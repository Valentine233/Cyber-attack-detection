[epoch1, step1]: loss 4.368342
[epoch1, step2]: loss 4.366392
[epoch1, step3]: loss 4.366363
[epoch1, step4]: loss 4.365426
[epoch1, step5]: loss 4.363655
[epoch1, step6]: loss 4.362785
[epoch1, step7]: loss 4.361229
[epoch1, step8]: loss 4.361299
[epoch1, step9]: loss 4.359336
[epoch1, step10]: loss 4.357742
[epoch1, step11]: loss 4.357336
[epoch1, step12]: loss 4.356789
[epoch1, step13]: loss 4.358582
[epoch1, step14]: loss 4.354644
[epoch1, step15]: loss 4.352667
[epoch1, step16]: loss 4.355222
[epoch1, step17]: loss 4.350612
[epoch1, step18]: loss 4.350690
[epoch1, step19]: loss 4.347589
[epoch1, step20]: loss 4.347311
[epoch1, step21]: loss 4.346605
[epoch1, step22]: loss 4.346395
[epoch1, step23]: loss 4.345376
[epoch1, step24]: loss 4.341597
[epoch1, step25]: loss 4.343616
[epoch1, step26]: loss 4.340724
[epoch1, step27]: loss 4.338294
[epoch1, step28]: loss 4.338631
[epoch1, step29]: loss 4.337493
[epoch1, step30]: loss 4.334543
[epoch1, step31]: loss 4.335706
[epoch1, step32]: loss 4.333745
[epoch1, step33]: loss 4.332808
[epoch1, step34]: loss 4.329812
[epoch1, step35]: loss 4.328583
[epoch1, step36]: loss 4.327005
[epoch1, step37]: loss 4.327258
[epoch1, step38]: loss 4.320753
[epoch1, step39]: loss 4.331122
[epoch1, step40]: loss 4.321240
[epoch1, step41]: loss 4.330140
[epoch1, step42]: loss 4.314451
[epoch1, step43]: loss 4.316362
[epoch1, step44]: loss 4.312766
[epoch1, step45]: loss 4.316841
[epoch1, step46]: loss 4.309443
[epoch1, step47]: loss 4.309568
[epoch1, step48]: loss 4.309294
[epoch1, step49]: loss 4.303055
[epoch1, step50]: loss 4.305865
[epoch1, step51]: loss 4.304048
[epoch1, step52]: loss 4.301865
[epoch1, step53]: loss 4.303846
[epoch1, step54]: loss 4.292354
[epoch1, step55]: loss 4.289430
[epoch1, step56]: loss 4.292734
[epoch1, step57]: loss 4.287461
[epoch1, step58]: loss 4.287814
[epoch1, step59]: loss 4.280028
[epoch1, step60]: loss 4.277695
[epoch1, step61]: loss 4.271220
[epoch1, step62]: loss 4.275511
[epoch1, step63]: loss 4.270713
[epoch1, step64]: loss 4.262258
[epoch1, step65]: loss 4.252111
[epoch1, step66]: loss 4.256680
[epoch1, step67]: loss 4.257781
[epoch1, step68]: loss 4.251534
[epoch1, step69]: loss 4.235170
[epoch1, step70]: loss 4.237537
[epoch1, step71]: loss 4.235014
[epoch1, step72]: loss 4.225346
[epoch1, step73]: loss 4.214150
[epoch1, step74]: loss 4.204526
[epoch1, step75]: loss 4.205320
[epoch1, step76]: loss 4.197478
[epoch1, step77]: loss 4.172859
[epoch1, step78]: loss 4.165441
[epoch1, step79]: loss 4.149331
[epoch1, step80]: loss 4.119399
[epoch1, step81]: loss 4.100164
[epoch1, step82]: loss 4.084236
[epoch1, step83]: loss 4.047894
[epoch1, step84]: loss 4.015175
[epoch1, step85]: loss 4.003577
[epoch1, step86]: loss 3.952371
[epoch1, step87]: loss 3.942605
[epoch1, step88]: loss 3.908302
[epoch1, step89]: loss 3.893488
[epoch1, step90]: loss 3.934327
[epoch1, step91]: loss 3.887180
[epoch1, step92]: loss 3.907957
[epoch1, step93]: loss 3.833878
[epoch1, step94]: loss 3.855277
[epoch1, step95]: loss 3.864289
[epoch1, step96]: loss 3.867634
[epoch1, step97]: loss 3.826358
[epoch1, step98]: loss 3.809455
[epoch1, step99]: loss 3.830760
[epoch1, step100]: loss 3.859166
[epoch1, step101]: loss 3.806608
[epoch1, step102]: loss 3.791846
[epoch1, step103]: loss 3.780864
[epoch1, step104]: loss 3.822208
[epoch1, step105]: loss 3.816965
[epoch1, step106]: loss 3.764826
[epoch1, step107]: loss 3.821059
[epoch1, step108]: loss 3.739693
[epoch1, step109]: loss 3.771178
[epoch1, step110]: loss 3.725959
[epoch1, step111]: loss 3.749639
[epoch1, step112]: loss 3.757960
[epoch1, step113]: loss 3.746035
[epoch1, step114]: loss 3.756957
[epoch1, step115]: loss 3.727944
[epoch1, step116]: loss 3.662070
[epoch1, step117]: loss 3.718397
[epoch1, step118]: loss 3.948720
[epoch1, step119]: loss 3.724518
[epoch1, step120]: loss 3.745941
[epoch1, step121]: loss 3.754887
[epoch1, step122]: loss 3.719710
[epoch1, step123]: loss 3.725410
[epoch1, step124]: loss 3.714334
[epoch1, step125]: loss 3.698850
[epoch1, step126]: loss 3.717403
[epoch1, step127]: loss 3.696347
[epoch1, step128]: loss 3.718031
[epoch1, step129]: loss 3.717166
[epoch1, step130]: loss 3.716852
[epoch1, step131]: loss 3.740186
[epoch1, step132]: loss 3.683649
[epoch1, step133]: loss 3.743684
[epoch1, step134]: loss 3.712433
[epoch1, step135]: loss 3.709169
[epoch1, step136]: loss 3.655533
[epoch1, step137]: loss 3.701649
[epoch1, step138]: loss 3.671184
[epoch1, step139]: loss 3.909360
[epoch1, step140]: loss 3.697412
[epoch1, step141]: loss 3.635188
[epoch1, step142]: loss 3.682714
[epoch1, step143]: loss 3.669641
[epoch1, step144]: loss 3.695633
[epoch1, step145]: loss 3.667141
[epoch1, step146]: loss 3.710648
[epoch1, step147]: loss 3.671913
[epoch1, step148]: loss 3.687978
[epoch1, step149]: loss 3.635937
[epoch1, step150]: loss 3.708932
[epoch1, step151]: loss 3.701073
[epoch1, step152]: loss 3.659438
[epoch1, step153]: loss 3.694228
[epoch1, step154]: loss 3.705383
[epoch1, step155]: loss 3.617834
[epoch1, step156]: loss 3.659618
[epoch1, step157]: loss 3.618254
[epoch1, step158]: loss 3.667265
[epoch1, step159]: loss 3.707590
[epoch1, step160]: loss 3.653805
[epoch1, step161]: loss 3.666201
[epoch1, step162]: loss 3.697714
[epoch1, step163]: loss 3.680568
[epoch1, step164]: loss 3.705418
[epoch1, step165]: loss 3.651706
[epoch1, step166]: loss 3.859521
[epoch1, step167]: loss 3.655454
[epoch1, step168]: loss 3.700196
[epoch1, step169]: loss 3.678927
[epoch1, step170]: loss 3.679125
[epoch1, step171]: loss 3.618204
[epoch1, step172]: loss 3.653489
[epoch1, step173]: loss 3.647986
[epoch1, step174]: loss 3.665025
[epoch1, step175]: loss 3.693739
[epoch1, step176]: loss 3.618337
[epoch1, step177]: loss 3.745730
[epoch1, step178]: loss 3.588933
[epoch1, step179]: loss 3.621470
[epoch1, step180]: loss 3.669186
[epoch1, step181]: loss 3.655386
[epoch1, step182]: loss 3.597384
[epoch1, step183]: loss 3.587806
[epoch1, step184]: loss 3.689427
[epoch1, step185]: loss 3.600281
[epoch1, step186]: loss 3.650875
[epoch1, step187]: loss 3.615470
[epoch1, step188]: loss 3.585307
[epoch1, step189]: loss 3.633880
[epoch1, step190]: loss 3.672618
[epoch1, step191]: loss 3.598014
[epoch1, step192]: loss 3.820544
[epoch1, step193]: loss 3.652620
[epoch1, step194]: loss 3.644002
[epoch1, step195]: loss 3.657012
[epoch1, step196]: loss 3.719682
[epoch1, step197]: loss 3.659212
[epoch1, step198]: loss 3.806499
[epoch1, step199]: loss 3.650181
[epoch1, step200]: loss 3.669165
[epoch1, step201]: loss 3.685572
[epoch1, step202]: loss 3.652504
[epoch1, step203]: loss 3.575567
[epoch1, step204]: loss 3.618008
[epoch1, step205]: loss 3.654364
[epoch1, step206]: loss 3.588997
[epoch1, step207]: loss 3.837796
[epoch1, step208]: loss 3.702523
[epoch1, step209]: loss 3.685261
[epoch1, step210]: loss 3.643013
[epoch1, step211]: loss 3.642141
[epoch1, step212]: loss 3.612971
[epoch1, step213]: loss 3.688079
[epoch1, step214]: loss 3.604353
[epoch1, step215]: loss 3.635839
[epoch1, step216]: loss 3.657100
[epoch1, step217]: loss 3.583685
[epoch1, step218]: loss 3.553107
[epoch1, step219]: loss 3.663516
[epoch1, step220]: loss 3.542068
[epoch1, step221]: loss 3.633469
[epoch1, step222]: loss 3.619042
[epoch1, step223]: loss 3.649297
[epoch1, step224]: loss 3.640290
[epoch1, step225]: loss 3.652115
[epoch1, step226]: loss 3.546576
[epoch1, step227]: loss 3.660822
[epoch1, step228]: loss 3.672188
[epoch1, step229]: loss 3.527841
[epoch1, step230]: loss 3.601606
[epoch1, step231]: loss 3.626631
[epoch1, step232]: loss 3.624519
[epoch1, step233]: loss 3.777113
[epoch1, step234]: loss 3.521263
[epoch1, step235]: loss 3.573998
[epoch1, step236]: loss 3.577739
[epoch1, step237]: loss 3.580937
[epoch1, step238]: loss 3.628503
[epoch1, step239]: loss 3.569899
[epoch1, step240]: loss 3.619471
[epoch1, step241]: loss 3.572677
[epoch1, step242]: loss 3.599933
[epoch1, step243]: loss 3.606154
[epoch1, step244]: loss 3.577916
[epoch1, step245]: loss 3.610251
[epoch1, step246]: loss 3.576619
[epoch1, step247]: loss 3.625229
[epoch1, step248]: loss 3.586757
[epoch1, step249]: loss 3.613575
[epoch1, step250]: loss 3.533495
[epoch1, step251]: loss 3.521643
[epoch1, step252]: loss 3.591587
[epoch1, step253]: loss 3.606013
[epoch1, step254]: loss 3.548311
[epoch1, step255]: loss 3.642376
[epoch1, step256]: loss 3.496233
[epoch1, step257]: loss 3.575269
[epoch1, step258]: loss 3.489138
[epoch1, step259]: loss 3.516850
[epoch1, step260]: loss 3.554962
[epoch1, step261]: loss 3.544875
[epoch1, step262]: loss 3.558056
[epoch1, step263]: loss 3.494617
[epoch1, step264]: loss 3.605667
[epoch1, step265]: loss 3.583249
[epoch1, step266]: loss 3.624532
[epoch1, step267]: loss 3.569328
[epoch1, step268]: loss 3.563406
[epoch1, step269]: loss 3.532529
[epoch1, step270]: loss 3.557910
[epoch1, step271]: loss 3.576841
[epoch1, step272]: loss 3.578953
[epoch1, step273]: loss 3.564003
[epoch1, step274]: loss 3.673311
[epoch1, step275]: loss 3.512173
[epoch1, step276]: loss 3.577204
[epoch1, step277]: loss 3.518715
[epoch1, step278]: loss 3.475062
[epoch1, step279]: loss 3.453296
[epoch1, step280]: loss 3.500608
[epoch1, step281]: loss 3.532366
[epoch1, step282]: loss 3.438452
[epoch1, step283]: loss 3.436183
[epoch1, step284]: loss 3.581532
[epoch1, step285]: loss 3.533512
[epoch1, step286]: loss 3.544567
[epoch1, step287]: loss 3.550684
[epoch1, step288]: loss 3.535404
[epoch1, step289]: loss 3.403429
[epoch1, step290]: loss 3.551126
[epoch1, step291]: loss 3.473531
[epoch1, step292]: loss 3.544348
[epoch1, step293]: loss 3.398954
[epoch1, step294]: loss 3.505500
[epoch1, step295]: loss 3.631469
[epoch1, step296]: loss 3.446321
[epoch1, step297]: loss 3.496623
[epoch1, step298]: loss 3.500650
[epoch1, step299]: loss 3.465123
[epoch1, step300]: loss 3.552337
[epoch1, step301]: loss 3.390073
[epoch1, step302]: loss 3.511113
[epoch1, step303]: loss 3.467709
[epoch1, step304]: loss 3.521832
[epoch1, step305]: loss 3.330175
[epoch1, step306]: loss 3.539875
[epoch1, step307]: loss 3.504020
[epoch1, step308]: loss 3.503258
[epoch1, step309]: loss 3.421569
[epoch1, step310]: loss 3.523414
[epoch1, step311]: loss 3.434496
[epoch1, step312]: loss 3.348697
[epoch1, step313]: loss 3.409810
[epoch1, step314]: loss 3.452348
[epoch1, step315]: loss 3.364444
[epoch1, step316]: loss 3.416375
[epoch1, step317]: loss 3.497784
[epoch1, step318]: loss 3.442398
[epoch1, step319]: loss 3.510322
[epoch1, step320]: loss 3.422180
[epoch1, step321]: loss 3.555632
[epoch1, step322]: loss 3.433324
[epoch1, step323]: loss 3.461524
[epoch1, step324]: loss 3.433153
[epoch1, step325]: loss 3.541369
[epoch1, step326]: loss 3.459843
[epoch1, step327]: loss 3.439268
[epoch1, step328]: loss 3.430950
[epoch1, step329]: loss 3.468529
[epoch1, step330]: loss 3.482059
[epoch1, step331]: loss 3.455602
[epoch1, step332]: loss 3.504053
[epoch1, step333]: loss 3.471605
[epoch1, step334]: loss 3.414888
[epoch1, step335]: loss 3.393108
[epoch1, step336]: loss 3.375704
[epoch1, step337]: loss 3.347552
[epoch1, step338]: loss 3.417220
[epoch1, step339]: loss 3.543136
[epoch1, step340]: loss 3.363101
[epoch1, step341]: loss 3.368775
[epoch1, step342]: loss 3.455050
[epoch1, step343]: loss 3.367540
[epoch1, step344]: loss 3.354462
[epoch1, step345]: loss 3.371980
[epoch1, step346]: loss 3.430242
[epoch1, step347]: loss 3.383216
[epoch1, step348]: loss 3.390488
[epoch1, step349]: loss 3.395133
[epoch1, step350]: loss 3.430150
[epoch1, step351]: loss 3.373437
[epoch1, step352]: loss 3.511545
[epoch1, step353]: loss 3.444195
[epoch1, step354]: loss 3.362304
[epoch1, step355]: loss 3.405924
[epoch1, step356]: loss 3.382174
[epoch1, step357]: loss 3.302855
[epoch1, step358]: loss 3.375758
[epoch1, step359]: loss 3.389762
[epoch1, step360]: loss 3.418537
[epoch1, step361]: loss 3.366802
[epoch1, step362]: loss 3.256636
[epoch1, step363]: loss 3.395951
[epoch1, step364]: loss 3.413103
[epoch1, step365]: loss 3.256641
[epoch1, step366]: loss 3.425185
[epoch1, step367]: loss 3.245400
[epoch1, step368]: loss 3.391759
[epoch1, step369]: loss 3.289461
[epoch1, step370]: loss 3.291642
[epoch1, step371]: loss 3.333981
[epoch1, step372]: loss 3.307539
[epoch1, step373]: loss 3.298425
[epoch1, step374]: loss 3.397765
[epoch1, step375]: loss 3.232139
[epoch1, step376]: loss 3.322100
[epoch1, step377]: loss 3.369939
[epoch1, step378]: loss 3.401655
[epoch1, step379]: loss 3.365995
[epoch1, step380]: loss 3.373857
[epoch1, step381]: loss 3.297019
[epoch1, step382]: loss 3.228495
[epoch1, step383]: loss 3.253355
[epoch1, step384]: loss 3.375828
[epoch1, step385]: loss 3.263483
[epoch1, step386]: loss 3.294439
[epoch1, step387]: loss 3.155538
[epoch1, step388]: loss 3.332409
[epoch1, step389]: loss 3.263572
[epoch1, step390]: loss 3.262547
[epoch1, step391]: loss 3.353766
[epoch1, step392]: loss 3.274785
[epoch1, step393]: loss 3.351007
[epoch1, step394]: loss 3.317916
[epoch1, step395]: loss 3.372356
[epoch1, step396]: loss 3.219239
[epoch1, step397]: loss 3.332099
[epoch1, step398]: loss 3.343824
[epoch1, step399]: loss 3.246869
[epoch1, step400]: loss 3.395684
[epoch1, step401]: loss 3.287538
[epoch1, step402]: loss 3.212128
[epoch1, step403]: loss 3.251552
[epoch1, step404]: loss 3.274300
[epoch1, step405]: loss 3.269879
[epoch1, step406]: loss 3.224432
[epoch1, step407]: loss 3.255644
[epoch1, step408]: loss 3.333906
[epoch1, step409]: loss 3.327087
[epoch1, step410]: loss 3.293391
[epoch1, step411]: loss 3.312291
[epoch1, step412]: loss 3.285465
[epoch1, step413]: loss 3.336478
[epoch1, step414]: loss 3.351557
[epoch1, step415]: loss 3.310403
[epoch1, step416]: loss 3.314731
[epoch1, step417]: loss 3.184932
[epoch1, step418]: loss 3.173172
[epoch1, step419]: loss 3.227136
[epoch1, step420]: loss 3.189106
[epoch1, step421]: loss 3.291426
[epoch1, step422]: loss 3.204139
[epoch1, step423]: loss 3.284318
[epoch1, step424]: loss 3.298371
[epoch1, step425]: loss 3.308249
[epoch1, step426]: loss 3.114740
[epoch1, step427]: loss 3.371094
[epoch1, step428]: loss 3.157428
[epoch1, step429]: loss 3.212256
[epoch1, step430]: loss 3.222796
[epoch1, step431]: loss 3.252234
[epoch1, step432]: loss 3.055356
[epoch1, step433]: loss 3.272102
[epoch1, step434]: loss 3.146262
[epoch1, step435]: loss 3.122948
[epoch1, step436]: loss 3.232839
[epoch1, step437]: loss 3.144807
[epoch1, step438]: loss 3.289993
[epoch1, step439]: loss 3.270990
[epoch1, step440]: loss 2.991465
[epoch1, step441]: loss 3.279194
[epoch1, step442]: loss 3.185920
[epoch1, step443]: loss 3.249547
[epoch1, step444]: loss 3.230257
[epoch1, step445]: loss 3.400188
[epoch1, step446]: loss 3.232459
[epoch1, step447]: loss 3.275260
[epoch1, step448]: loss 3.182892
[epoch1, step449]: loss 3.058197
[epoch1, step450]: loss 3.139086
[epoch1, step451]: loss 3.027269
[epoch1, step452]: loss 3.041517
[epoch1, step453]: loss 3.293289
[epoch1, step454]: loss 3.171678
[epoch1, step455]: loss 3.252926
[epoch1, step456]: loss 3.153028
[epoch1, step457]: loss 3.098549
[epoch1, step458]: loss 3.258468
[epoch1, step459]: loss 3.165428
[epoch1, step460]: loss 3.087957
[epoch1, step461]: loss 3.292328
[epoch1, step462]: loss 3.181992
[epoch1, step463]: loss 3.127896
[epoch1, step464]: loss 3.117567
[epoch1, step465]: loss 3.141042
[epoch1, step466]: loss 3.220976
[epoch1, step467]: loss 2.962882
[epoch1, step468]: loss 3.147285
[epoch1, step469]: loss 3.187710
[epoch1, step470]: loss 3.273166
[epoch1, step471]: loss 3.274488
[epoch1, step472]: loss 3.142417
[epoch1, step473]: loss 3.076044
[epoch1, step474]: loss 3.199943
[epoch1, step475]: loss 3.019494
[epoch1, step476]: loss 3.104043
[epoch1, step477]: loss 3.221272
[epoch1, step478]: loss 3.092627
[epoch1, step479]: loss 2.977594
[epoch1, step480]: loss 3.077161
[epoch1, step481]: loss 3.257843
[epoch1, step482]: loss 3.228758
[epoch1, step483]: loss 3.025323
[epoch1, step484]: loss 3.180032
[epoch1, step485]: loss 2.938299
[epoch1, step486]: loss 3.154280
[epoch1, step487]: loss 3.049099
[epoch1, step488]: loss 3.153501
[epoch1, step489]: loss 3.055642
[epoch1, step490]: loss 3.195175
[epoch1, step491]: loss 3.100358
[epoch1, step492]: loss 3.117446
[epoch1, step493]: loss 3.004493
[epoch1, step494]: loss 3.129387
[epoch1, step495]: loss 3.014226
[epoch1, step496]: loss 3.083577
[epoch1, step497]: loss 3.175773
[epoch1, step498]: loss 3.133479
[epoch1, step499]: loss 2.847782
[epoch1, step500]: loss 2.978738
[epoch1, step501]: loss 3.086246
[epoch1, step502]: loss 3.094983
[epoch1, step503]: loss 3.043539
[epoch1, step504]: loss 3.169680
[epoch1, step505]: loss 3.114096
[epoch1, step506]: loss 3.110355
[epoch1, step507]: loss 3.051188
[epoch1, step508]: loss 3.083500
[epoch1, step509]: loss 2.977166
[epoch1, step510]: loss 3.034298
[epoch1, step511]: loss 3.178496
[epoch1, step512]: loss 3.107721
[epoch1, step513]: loss 3.071082
[epoch1, step514]: loss 3.053989
[epoch1, step515]: loss 3.034576
[epoch1, step516]: loss 3.025446
[epoch1, step517]: loss 3.085574
[epoch1, step518]: loss 3.155598
[epoch1, step519]: loss 3.111819
[epoch1, step520]: loss 3.141164
[epoch1, step521]: loss 3.077474
[epoch1, step522]: loss 3.211994
[epoch1, step523]: loss 3.006835
[epoch1, step524]: loss 3.058955
[epoch1, step525]: loss 2.894840
[epoch1, step526]: loss 3.111333
[epoch1, step527]: loss 2.959835
[epoch1, step528]: loss 3.052648
[epoch1, step529]: loss 2.961904
[epoch1, step530]: loss 3.086626
[epoch1, step531]: loss 3.105641
[epoch1, step532]: loss 3.064747
[epoch1, step533]: loss 2.876887
[epoch1, step534]: loss 2.932725
[epoch1, step535]: loss 3.108057
[epoch1, step536]: loss 3.179119
[epoch1, step537]: loss 3.068216
[epoch1, step538]: loss 2.920566
[epoch1, step539]: loss 3.042636
[epoch1, step540]: loss 2.954759
[epoch1, step541]: loss 3.089846
[epoch1, step542]: loss 3.076336
[epoch1, step543]: loss 3.128061
[epoch1, step544]: loss 2.797197
[epoch1, step545]: loss 2.973070
[epoch1, step546]: loss 3.024942
[epoch1, step547]: loss 2.952252
[epoch1, step548]: loss 3.032749
[epoch1, step549]: loss 3.019358
[epoch1, step550]: loss 2.896789
[epoch1, step551]: loss 3.010212
[epoch1, step552]: loss 2.827986
[epoch1, step553]: loss 3.149911
[epoch1, step554]: loss 2.979442
[epoch1, step555]: loss 2.679173
[epoch1, step556]: loss 2.976044
[epoch1, step557]: loss 2.974885
[epoch1, step558]: loss 2.845316
[epoch1, step559]: loss 2.745015
[epoch1, step560]: loss 2.778100
[epoch1, step561]: loss 3.055200
[epoch1, step562]: loss 2.987260
[epoch1, step563]: loss 2.777843
[epoch1, step564]: loss 2.732162
[epoch1, step565]: loss 2.914352
[epoch1, step566]: loss 3.067138
[epoch1, step567]: loss 2.950189
[epoch1, step568]: loss 2.882075
[epoch1, step569]: loss 2.884053
[epoch1, step570]: loss 2.953872
[epoch1, step571]: loss 2.841194
[epoch1, step572]: loss 2.990422
[epoch1, step573]: loss 2.744180
[epoch1, step574]: loss 2.877750
[epoch1, step575]: loss 2.950562
[epoch1, step576]: loss 2.871639
[epoch1, step577]: loss 3.077598
[epoch1, step578]: loss 2.721511
[epoch1, step579]: loss 2.960701
[epoch1, step580]: loss 2.890110
[epoch1, step581]: loss 2.936508
[epoch1, step582]: loss 2.941413
[epoch1, step583]: loss 2.882861
[epoch1, step584]: loss 2.774010
[epoch1, step585]: loss 2.953348
[epoch1, step586]: loss 3.013339
[epoch1, step587]: loss 3.022969
[epoch1, step588]: loss 2.857687
[epoch1, step589]: loss 2.967015
[epoch1, step590]: loss 2.940387
[epoch1, step591]: loss 2.846070
[epoch1, step592]: loss 2.962362
[epoch1, step593]: loss 2.852308
[epoch1, step594]: loss 2.904705
[epoch1, step595]: loss 2.825074
[epoch1, step596]: loss 2.995347
[epoch1, step597]: loss 2.867497
[epoch1, step598]: loss 2.934274
[epoch1, step599]: loss 2.680126
[epoch1, step600]: loss 2.797117
[epoch1, step601]: loss 2.788946
[epoch1, step602]: loss 3.053081
[epoch1, step603]: loss 2.986784
[epoch1, step604]: loss 2.837943
[epoch1, step605]: loss 2.676792
[epoch1, step606]: loss 2.600558
[epoch1, step607]: loss 2.793236
[epoch1, step608]: loss 2.769984
[epoch1, step609]: loss 3.176378
[epoch1, step610]: loss 2.950535
[epoch1, step611]: loss 2.804532
[epoch1, step612]: loss 2.821801
[epoch1, step613]: loss 3.050111
[epoch1, step614]: loss 2.825481
[epoch1, step615]: loss 3.057207
[epoch1, step616]: loss 2.764493
[epoch1, step617]: loss 2.916542
[epoch1, step618]: loss 2.983397
[epoch1, step619]: loss 2.757604
[epoch1, step620]: loss 2.839361
[epoch1, step621]: loss 2.903650
[epoch1, step622]: loss 2.930744
[epoch1, step623]: loss 3.020955
[epoch1, step624]: loss 2.644588
[epoch1, step625]: loss 2.917887
[epoch1, step626]: loss 3.030611
[epoch1, step627]: loss 2.835675
[epoch1, step628]: loss 2.868417
[epoch1, step629]: loss 2.773841
[epoch1, step630]: loss 2.905617
[epoch1, step631]: loss 2.937378
[epoch1, step632]: loss 2.835218
[epoch1, step633]: loss 2.946408
[epoch1, step634]: loss 2.903052
[epoch1, step635]: loss 2.951973
[epoch1, step636]: loss 2.923243
[epoch1, step637]: loss 2.721137
[epoch1, step638]: loss 2.813242
[epoch1, step639]: loss 2.926977
[epoch1, step640]: loss 2.840225
[epoch1, step641]: loss 2.704401
[epoch1, step642]: loss 2.714577
[epoch1, step643]: loss 2.813871
[epoch1, step644]: loss 2.752499
[epoch1, step645]: loss 2.761815
[epoch1, step646]: loss 3.009081
[epoch1, step647]: loss 2.654676
[epoch1, step648]: loss 2.741813
[epoch1, step649]: loss 2.938794
[epoch1, step650]: loss 2.825611
[epoch1, step651]: loss 2.458709
[epoch1, step652]: loss 2.908148
[epoch1, step653]: loss 2.912757
[epoch1, step654]: loss 2.745758
[epoch1, step655]: loss 2.585284
[epoch1, step656]: loss 2.750896
[epoch1, step657]: loss 2.641203
[epoch1, step658]: loss 2.669516
[epoch1, step659]: loss 2.727078
[epoch1, step660]: loss 2.784192
[epoch1, step661]: loss 2.799909
[epoch1, step662]: loss 2.724256
[epoch1, step663]: loss 2.762896
[epoch1, step664]: loss 2.668628
[epoch1, step665]: loss 2.800460
[epoch1, step666]: loss 2.735583
[epoch1, step667]: loss 2.969906
[epoch1, step668]: loss 2.793635
[epoch1, step669]: loss 2.823460
[epoch1, step670]: loss 2.737210
[epoch1, step671]: loss 2.603821
[epoch1, step672]: loss 2.858777
[epoch1, step673]: loss 2.699197
[epoch1, step674]: loss 2.791752
[epoch1, step675]: loss 2.778730
[epoch1, step676]: loss 2.673815
[epoch1, step677]: loss 2.735297
[epoch1, step678]: loss 2.895415
[epoch1, step679]: loss 2.708499
[epoch1, step680]: loss 2.675141
[epoch1, step681]: loss 2.854813
[epoch1, step682]: loss 2.591266
[epoch1, step683]: loss 2.521343
[epoch1, step684]: loss 2.742085
[epoch1, step685]: loss 2.753793
[epoch1, step686]: loss 2.901470
[epoch1, step687]: loss 2.516421
[epoch1, step688]: loss 2.565873
[epoch1, step689]: loss 2.722739
[epoch1, step690]: loss 2.762045
[epoch1, step691]: loss 2.747963
[epoch1, step692]: loss 2.496724
[epoch1, step693]: loss 2.767265
[epoch1, step694]: loss 2.665005
[epoch1, step695]: loss 2.654553
[epoch1, step696]: loss 2.482138
[epoch1, step697]: loss 2.460513
[epoch1, step698]: loss 2.785757
[epoch1, step699]: loss 2.742064
[epoch1, step700]: loss 2.911441
[epoch1, step701]: loss 2.512138
[epoch1, step702]: loss 2.794637
[epoch1, step703]: loss 2.700382
[epoch1, step704]: loss 2.664771
[epoch1, step705]: loss 2.598224
[epoch1, step706]: loss 2.490524
[epoch1, step707]: loss 2.773116
[epoch1, step708]: loss 2.605643
[epoch1, step709]: loss 2.734625
[epoch1, step710]: loss 2.816840
[epoch1, step711]: loss 2.528638
[epoch1, step712]: loss 2.631134
[epoch1, step713]: loss 2.697119
[epoch1, step714]: loss 2.766801
[epoch1, step715]: loss 2.764007
[epoch1, step716]: loss 2.497095
[epoch1, step717]: loss 2.659002
[epoch1, step718]: loss 2.499712
[epoch1, step719]: loss 2.744852
[epoch1, step720]: loss 2.580421
[epoch1, step721]: loss 2.466897
[epoch1, step722]: loss 2.664036
[epoch1, step723]: loss 2.601095
[epoch1, step724]: loss 2.606873
[epoch1, step725]: loss 2.513025
[epoch1, step726]: loss 2.725800
[epoch1, step727]: loss 2.904612
[epoch1, step728]: loss 2.720836
[epoch1, step729]: loss 2.770387
[epoch1, step730]: loss 2.768167
[epoch1, step731]: loss 2.648554
[epoch1, step732]: loss 2.675690
[epoch1, step733]: loss 2.605453
[epoch1, step734]: loss 2.676101
[epoch1, step735]: loss 2.706285
[epoch1, step736]: loss 2.516677
[epoch1, step737]: loss 2.745329
[epoch1, step738]: loss 2.664922
[epoch1, step739]: loss 2.785235
[epoch1, step740]: loss 2.664302
[epoch1, step741]: loss 2.427078
[epoch1, step742]: loss 2.653062
[epoch1, step743]: loss 2.555771
[epoch1, step744]: loss 2.779903
[epoch1, step745]: loss 2.569738
[epoch1, step746]: loss 2.612679
[epoch1, step747]: loss 2.482976
[epoch1, step748]: loss 2.535169
[epoch1, step749]: loss 2.282355
[epoch1, step750]: loss 2.559635
[epoch1, step751]: loss 2.765926
[epoch1, step752]: loss 2.252798
[epoch1, step753]: loss 2.822295
[epoch1, step754]: loss 2.559297
[epoch1, step755]: loss 2.288321
[epoch1, step756]: loss 2.490862
[epoch1, step757]: loss 2.446621
[epoch1, step758]: loss 2.688426
[epoch1, step759]: loss 2.426981
[epoch1, step760]: loss 2.588890
[epoch1, step761]: loss 2.748063
[epoch1, step762]: loss 2.647334
[epoch1, step763]: loss 2.775405
[epoch1, step764]: loss 2.644091
[epoch1, step765]: loss 2.626982
[epoch1, step766]: loss 2.441743
[epoch1, step767]: loss 2.576242
[epoch1, step768]: loss 2.540311
[epoch1, step769]: loss 2.638830
[epoch1, step770]: loss 2.675675
[epoch1, step771]: loss 2.404026
[epoch1, step772]: loss 2.435196
[epoch1, step773]: loss 2.712040
[epoch1, step774]: loss 2.479428
[epoch1, step775]: loss 2.452969
[epoch1, step776]: loss 2.488496
[epoch1, step777]: loss 2.537455
[epoch1, step778]: loss 2.322175
[epoch1, step779]: loss 2.515860
[epoch1, step780]: loss 2.467306
[epoch1, step781]: loss 2.577955
[epoch1, step782]: loss 2.681527
[epoch1, step783]: loss 2.404991
[epoch1, step784]: loss 2.668232
[epoch1, step785]: loss 2.780535
[epoch1, step786]: loss 2.546903
[epoch1, step787]: loss 2.613931
[epoch1, step788]: loss 2.252726
[epoch1, step789]: loss 2.547977
[epoch1, step790]: loss 2.455324
[epoch1, step791]: loss 2.577989
[epoch1, step792]: loss 2.268034
[epoch1, step793]: loss 2.334957
[epoch1, step794]: loss 2.432814
[epoch1, step795]: loss 2.511222
[epoch1, step796]: loss 2.601197
[epoch1, step797]: loss 2.570459
[epoch1, step798]: loss 2.652749
[epoch1, step799]: loss 2.495640
[epoch1, step800]: loss 2.570248
[epoch1, step801]: loss 2.465467
[epoch1, step802]: loss 2.465017
[epoch1, step803]: loss 2.641203
[epoch1, step804]: loss 2.588941
[epoch1, step805]: loss 2.563319
[epoch1, step806]: loss 2.677526
[epoch1, step807]: loss 2.472213
[epoch1, step808]: loss 2.633270
[epoch1, step809]: loss 2.605044
[epoch1, step810]: loss 2.532199
[epoch1, step811]: loss 2.439936
[epoch1, step812]: loss 2.343282
[epoch1, step813]: loss 2.325157
[epoch1, step814]: loss 2.652248
[epoch1, step815]: loss 2.601143
[epoch1, step816]: loss 2.326128
[epoch1, step817]: loss 2.495405
[epoch1, step818]: loss 2.419930
[epoch1, step819]: loss 2.556702
[epoch1, step820]: loss 2.410239
[epoch1, step821]: loss 2.638555
[epoch1, step822]: loss 2.397452
[epoch1, step823]: loss 2.579241
[epoch1, step824]: loss 2.253386
[epoch1, step825]: loss 2.542734
[epoch1, step826]: loss 2.517162
[epoch1, step827]: loss 2.196722
[epoch1, step828]: loss 2.557527
[epoch1, step829]: loss 2.608091
[epoch1, step830]: loss 2.485885
[epoch1, step831]: loss 2.527200
[epoch1, step832]: loss 2.241642
[epoch1, step833]: loss 2.444193
[epoch1, step834]: loss 2.365629
[epoch1, step835]: loss 2.698106
[epoch1, step836]: loss 2.423766
[epoch1, step837]: loss 2.481015
[epoch1, step838]: loss 2.465553
[epoch1, step839]: loss 2.392390
[epoch1, step840]: loss 2.540802
[epoch1, step841]: loss 2.458072
[epoch1, step842]: loss 2.462326
[epoch1, step843]: loss 2.301208
[epoch1, step844]: loss 2.407368
[epoch1, step845]: loss 2.619036
[epoch1, step846]: loss 2.341384
[epoch1, step847]: loss 2.332974
[epoch1, step848]: loss 2.425748
[epoch1, step849]: loss 2.541430
[epoch1, step850]: loss 2.537516
[epoch1, step851]: loss 2.418967
[epoch1, step852]: loss 2.700492
[epoch1, step853]: loss 2.432803
[epoch1, step854]: loss 2.386607
[epoch1, step855]: loss 2.452152
[epoch1, step856]: loss 2.366018
[epoch1, step857]: loss 2.468197
[epoch1, step858]: loss 2.640142
[epoch1, step859]: loss 2.308911
[epoch1, step860]: loss 2.199518
[epoch1, step861]: loss 2.245339
[epoch1, step862]: loss 2.337430
[epoch1, step863]: loss 2.472216
[epoch1, step864]: loss 2.581587
[epoch1, step865]: loss 2.534148
[epoch1, step866]: loss 2.236987
[epoch1, step867]: loss 2.473988
[epoch1, step868]: loss 2.343814
[epoch1, step869]: loss 2.573866
[epoch1, step870]: loss 2.515351
[epoch1, step871]: loss 2.376528
[epoch1, step872]: loss 2.447016
[epoch1, step873]: loss 2.493089
[epoch1, step874]: loss 2.462993
[epoch1, step875]: loss 2.492994
[epoch1, step876]: loss 2.476825
[epoch1, step877]: loss 2.519308
[epoch1, step878]: loss 2.505111
[epoch1, step879]: loss 2.384418
[epoch1, step880]: loss 2.290375
[epoch1, step881]: loss 2.402236
[epoch1, step882]: loss 2.635854
[epoch1, step883]: loss 2.459976
[epoch1, step884]: loss 2.261756
[epoch1, step885]: loss 2.303490
[epoch1, step886]: loss 2.277317
[epoch1, step887]: loss 2.362489
[epoch1, step888]: loss 2.131857
[epoch1, step889]: loss 2.435814
[epoch1, step890]: loss 2.307582
[epoch1, step891]: loss 2.185098
[epoch1, step892]: loss 2.331341
[epoch1, step893]: loss 2.492438
[epoch1, step894]: loss 2.342546
[epoch1, step895]: loss 2.410025
[epoch1, step896]: loss 2.379607
[epoch1, step897]: loss 2.452096
[epoch1, step898]: loss 2.608681
[epoch1, step899]: loss 2.410550
[epoch1, step900]: loss 2.485185
[epoch1, step901]: loss 2.726573
[epoch1, step902]: loss 2.500686
[epoch1, step903]: loss 2.085470
[epoch1, step904]: loss 2.561328
[epoch1, step905]: loss 2.400786
[epoch1, step906]: loss 2.325756
[epoch1, step907]: loss 2.293294
[epoch1, step908]: loss 2.271196
[epoch1, step909]: loss 2.529941
[epoch1, step910]: loss 2.546184
[epoch1, step911]: loss 2.466675
[epoch1, step912]: loss 2.275251
[epoch1, step913]: loss 2.504557
[epoch1, step914]: loss 2.262250
[epoch1, step915]: loss 2.583492
[epoch1, step916]: loss 2.526959
[epoch1, step917]: loss 2.364081
[epoch1, step918]: loss 2.394311
[epoch1, step919]: loss 2.344473
[epoch1, step920]: loss 2.535056
[epoch1, step921]: loss 2.175113
[epoch1, step922]: loss 2.520264
[epoch1, step923]: loss 2.429229
[epoch1, step924]: loss 2.136196
[epoch1, step925]: loss 2.362636
[epoch1, step926]: loss 2.361349
[epoch1, step927]: loss 2.442918
[epoch1, step928]: loss 2.363120
[epoch1, step929]: loss 2.509346
[epoch1, step930]: loss 2.087648
[epoch1, step931]: loss 2.503112
[epoch1, step932]: loss 2.548090
[epoch1, step933]: loss 2.591285
[epoch1, step934]: loss 2.357388
[epoch1, step935]: loss 2.549785
[epoch1, step936]: loss 2.048955
[epoch1, step937]: loss 2.256886
[epoch1, step938]: loss 2.550625
[epoch1, step939]: loss 2.473837
[epoch1, step940]: loss 2.346902
[epoch1, step941]: loss 2.514305
[epoch1, step942]: loss 2.219617
[epoch1, step943]: loss 2.433815
[epoch1, step944]: loss 2.259911
[epoch1, step945]: loss 2.284697
[epoch1, step946]: loss 2.471955
[epoch1, step947]: loss 2.381104
[epoch1, step948]: loss 2.528757
[epoch1, step949]: loss 2.012025
[epoch1, step950]: loss 2.419019
[epoch1, step951]: loss 2.334513
[epoch1, step952]: loss 2.523160
[epoch1, step953]: loss 2.152444
[epoch1, step954]: loss 2.305368
[epoch1, step955]: loss 2.483291
[epoch1, step956]: loss 2.447187
[epoch1, step957]: loss 2.232100
[epoch1, step958]: loss 1.986750
[epoch1, step959]: loss 1.952376
[epoch1, step960]: loss 2.518124
[epoch1, step961]: loss 2.357482
[epoch1, step962]: loss 2.352077
[epoch1, step963]: loss 2.261150
[epoch1, step964]: loss 2.274023
[epoch1, step965]: loss 2.431926
[epoch1, step966]: loss 2.224093
[epoch1, step967]: loss 2.345179
[epoch1, step968]: loss 2.269737
[epoch1, step969]: loss 2.368675
[epoch1, step970]: loss 2.619125
[epoch1, step971]: loss 2.350104
[epoch1, step972]: loss 2.143961
[epoch1, step973]: loss 2.279202
[epoch1, step974]: loss 2.323394
[epoch1, step975]: loss 2.240315
[epoch1, step976]: loss 2.287436
[epoch1, step977]: loss 2.358662
[epoch1, step978]: loss 2.066639
[epoch1, step979]: loss 2.200972
[epoch1, step980]: loss 2.256922
[epoch1, step981]: loss 2.081245
[epoch1, step982]: loss 2.306726
[epoch1, step983]: loss 2.192510
[epoch1, step984]: loss 2.463101
[epoch1, step985]: loss 2.387007
[epoch1, step986]: loss 2.203671
[epoch1, step987]: loss 2.565182
[epoch1, step988]: loss 2.415135
[epoch1, step989]: loss 2.107581
[epoch1, step990]: loss 2.257283
[epoch1, step991]: loss 2.114483
[epoch1, step992]: loss 2.390883
[epoch1, step993]: loss 2.189204
[epoch1, step994]: loss 1.994820
[epoch1, step995]: loss 2.045851
[epoch1, step996]: loss 2.140847
[epoch1, step997]: loss 2.309100
[epoch1, step998]: loss 1.962186
[epoch1, step999]: loss 2.446847
[epoch1, step1000]: loss 2.347530
[epoch1, step1001]: loss 2.212690
[epoch1, step1002]: loss 2.293864
[epoch1, step1003]: loss 2.062234
[epoch1, step1004]: loss 2.267883
[epoch1, step1005]: loss 1.903943
[epoch1, step1006]: loss 2.441794
[epoch1, step1007]: loss 2.279189
[epoch1, step1008]: loss 1.799122
[epoch1, step1009]: loss 2.056234
[epoch1, step1010]: loss 2.193595
[epoch1, step1011]: loss 2.314214
[epoch1, step1012]: loss 2.269156
[epoch1, step1013]: loss 2.350569
[epoch1, step1014]: loss 1.987927
[epoch1, step1015]: loss 2.107457
[epoch1, step1016]: loss 2.295118
[epoch1, step1017]: loss 2.220733
[epoch1, step1018]: loss 2.328854
[epoch1, step1019]: loss 2.392861
[epoch1, step1020]: loss 2.241524
[epoch1, step1021]: loss 2.205083
[epoch1, step1022]: loss 2.320410
[epoch1, step1023]: loss 2.286077
[epoch1, step1024]: loss 1.991243
[epoch1, step1025]: loss 2.249190
[epoch1, step1026]: loss 2.337303
[epoch1, step1027]: loss 2.068698
[epoch1, step1028]: loss 2.216138
[epoch1, step1029]: loss 2.351232
[epoch1, step1030]: loss 2.343848
[epoch1, step1031]: loss 1.955180
[epoch1, step1032]: loss 2.184664
[epoch1, step1033]: loss 2.082900
[epoch1, step1034]: loss 2.211966
[epoch1, step1035]: loss 2.254662
[epoch1, step1036]: loss 2.250036
[epoch1, step1037]: loss 2.358950
[epoch1, step1038]: loss 2.380519
[epoch1, step1039]: loss 1.883763
[epoch1, step1040]: loss 2.295925
[epoch1, step1041]: loss 2.118241
[epoch1, step1042]: loss 2.299363
[epoch1, step1043]: loss 2.078223
[epoch1, step1044]: loss 1.911961
[epoch1, step1045]: loss 2.050963
[epoch1, step1046]: loss 2.380588
[epoch1, step1047]: loss 1.785227
[epoch1, step1048]: loss 2.424004
[epoch1, step1049]: loss 2.053888
[epoch1, step1050]: loss 2.289736
[epoch1, step1051]: loss 1.957468
[epoch1, step1052]: loss 1.918385
[epoch1, step1053]: loss 2.187879
[epoch1, step1054]: loss 1.837250
[epoch1, step1055]: loss 2.149410
[epoch1, step1056]: loss 1.970012
[epoch1, step1057]: loss 2.207531
[epoch1, step1058]: loss 2.225731
[epoch1, step1059]: loss 2.358820
[epoch1, step1060]: loss 2.201764
[epoch1, step1061]: loss 2.232424
[epoch1, step1062]: loss 2.282069
[epoch1, step1063]: loss 2.084556
[epoch1, step1064]: loss 1.905094
[epoch1, step1065]: loss 2.051985
[epoch1, step1066]: loss 2.120761
[epoch1, step1067]: loss 2.006435
[epoch1, step1068]: loss 2.146679
[epoch1, step1069]: loss 1.734090
[epoch1, step1070]: loss 2.242368
[epoch1, step1071]: loss 1.976655
[epoch1, step1072]: loss 2.226590
[epoch1, step1073]: loss 2.410946
[epoch1, step1074]: loss 2.190600
[epoch1, step1075]: loss 2.295409
[epoch1, step1076]: loss 2.165031
[epoch1, step1077]: loss 2.044115
[epoch1, step1078]: loss 2.253646
[epoch1, step1079]: loss 2.034858
[epoch1, step1080]: loss 2.171420
[epoch1, step1081]: loss 2.132516
[epoch1, step1082]: loss 2.180256
[epoch1, step1083]: loss 2.330747
[epoch1, step1084]: loss 2.165991
[epoch1, step1085]: loss 2.140946
[epoch1, step1086]: loss 2.288440
[epoch1, step1087]: loss 2.069055
[epoch1, step1088]: loss 2.041042
[epoch1, step1089]: loss 2.171515
[epoch1, step1090]: loss 2.084728
[epoch1, step1091]: loss 2.263351
[epoch1, step1092]: loss 2.073428
[epoch1, step1093]: loss 2.095946
[epoch1, step1094]: loss 1.912006
[epoch1, step1095]: loss 1.972465
[epoch1, step1096]: loss 2.087268
[epoch1, step1097]: loss 2.383138
[epoch1, step1098]: loss 2.350755
[epoch1, step1099]: loss 2.171105
[epoch1, step1100]: loss 1.787883
[epoch1, step1101]: loss 1.997571
[epoch1, step1102]: loss 2.160864
[epoch1, step1103]: loss 2.254711
[epoch1, step1104]: loss 2.025630
[epoch1, step1105]: loss 2.069728
[epoch1, step1106]: loss 2.283593
[epoch1, step1107]: loss 2.275464
[epoch1, step1108]: loss 1.843914
[epoch1, step1109]: loss 2.208593
[epoch1, step1110]: loss 2.207261
[epoch1, step1111]: loss 1.717314
[epoch1, step1112]: loss 2.354258
[epoch1, step1113]: loss 2.097503
[epoch1, step1114]: loss 1.900812
[epoch1, step1115]: loss 2.197891
[epoch1, step1116]: loss 1.825669
[epoch1, step1117]: loss 2.335791
[epoch1, step1118]: loss 1.974679
[epoch1, step1119]: loss 1.966494
[epoch1, step1120]: loss 2.174544
[epoch1, step1121]: loss 2.288322
[epoch1, step1122]: loss 2.166733
[epoch1, step1123]: loss 2.056501
[epoch1, step1124]: loss 2.206145
[epoch1, step1125]: loss 1.854734
[epoch1, step1126]: loss 2.173452
[epoch1, step1127]: loss 1.808730
[epoch1, step1128]: loss 2.011367
[epoch1, step1129]: loss 2.326908
[epoch1, step1130]: loss 1.869920
[epoch1, step1131]: loss 2.194273
[epoch1, step1132]: loss 2.102630
[epoch1, step1133]: loss 1.922248
[epoch1, step1134]: loss 2.151886
[epoch1, step1135]: loss 1.725425
[epoch1, step1136]: loss 1.835092
[epoch1, step1137]: loss 2.255257
[epoch1, step1138]: loss 1.988955
[epoch1, step1139]: loss 2.233620
[epoch1, step1140]: loss 2.062515
[epoch1, step1141]: loss 2.303757
[epoch1, step1142]: loss 2.100695
[epoch1, step1143]: loss 1.909858
[epoch1, step1144]: loss 1.919861
[epoch1, step1145]: loss 2.083462
[epoch1, step1146]: loss 1.963457
[epoch1, step1147]: loss 2.119059
[epoch1, step1148]: loss 2.287244
[epoch1, step1149]: loss 2.212747
[epoch1, step1150]: loss 2.137964
[epoch1, step1151]: loss 1.889934
[epoch1, step1152]: loss 1.869452
[epoch1, step1153]: loss 1.973346
[epoch1, step1154]: loss 2.107970
[epoch1, step1155]: loss 2.141420
[epoch1, step1156]: loss 1.970573
[epoch1, step1157]: loss 2.203191
[epoch1, step1158]: loss 1.986408
[epoch1, step1159]: loss 2.227876
[epoch1, step1160]: loss 2.211060
[epoch1, step1161]: loss 2.233287
[epoch1, step1162]: loss 1.749224
[epoch1, step1163]: loss 1.884259
[epoch1, step1164]: loss 1.822786
[epoch1, step1165]: loss 2.087828
[epoch1, step1166]: loss 2.006256
[epoch1, step1167]: loss 2.168078
[epoch1, step1168]: loss 1.889495
[epoch1, step1169]: loss 2.113906
[epoch1, step1170]: loss 2.260355
[epoch1, step1171]: loss 2.002093
[epoch1, step1172]: loss 1.901469
[epoch1, step1173]: loss 1.683117
[epoch1, step1174]: loss 2.418311
[epoch1, step1175]: loss 1.779080
[epoch1, step1176]: loss 1.899980
[epoch1, step1177]: loss 2.128270
[epoch1, step1178]: loss 2.056829
[epoch1, step1179]: loss 1.724589
[epoch1, step1180]: loss 1.303056
[epoch1, step1181]: loss 1.807699
[epoch1, step1182]: loss 2.036461
[epoch1, step1183]: loss 2.026879
[epoch1, step1184]: loss 1.844005
[epoch1, step1185]: loss 2.143574
[epoch1, step1186]: loss 1.719657
[epoch1, step1187]: loss 1.765919
[epoch1, step1188]: loss 1.947604
[epoch1, step1189]: loss 1.754528
[epoch1, step1190]: loss 2.137034
[epoch1, step1191]: loss 2.205803
[epoch1, step1192]: loss 1.917760
[epoch1, step1193]: loss 1.867927
[epoch1, step1194]: loss 2.234671
[epoch1, step1195]: loss 2.243764
[epoch1, step1196]: loss 1.767964
[epoch1, step1197]: loss 1.931410
[epoch1, step1198]: loss 1.992640
[epoch1, step1199]: loss 1.908231
[epoch1, step1200]: loss 1.984321
[epoch1, step1201]: loss 1.893807
[epoch1, step1202]: loss 1.903313
[epoch1, step1203]: loss 2.052223
[epoch1, step1204]: loss 2.072213
[epoch1, step1205]: loss 1.950208
[epoch1, step1206]: loss 2.098984
[epoch1, step1207]: loss 2.144238
[epoch1, step1208]: loss 2.164593
[epoch1, step1209]: loss 1.807492
[epoch1, step1210]: loss 2.105993
[epoch1, step1211]: loss 2.010394
[epoch1, step1212]: loss 2.026127
[epoch1, step1213]: loss 1.833212
[epoch1, step1214]: loss 1.970392
[epoch1, step1215]: loss 1.705096
[epoch1, step1216]: loss 2.118056
[epoch1, step1217]: loss 1.958472
[epoch1, step1218]: loss 1.834238
[epoch1, step1219]: loss 1.847556
[epoch1, step1220]: loss 1.925899
[epoch1, step1221]: loss 1.941779
[epoch1, step1222]: loss 1.956064
[epoch1, step1223]: loss 1.999256
[epoch1, step1224]: loss 1.810042
[epoch1, step1225]: loss 2.158619
[epoch1, step1226]: loss 2.098871
[epoch1, step1227]: loss 2.029868
[epoch1, step1228]: loss 1.746562
[epoch1, step1229]: loss 1.950642
[epoch1, step1230]: loss 1.968685
[epoch1, step1231]: loss 1.917422
[epoch1, step1232]: loss 2.020214
[epoch1, step1233]: loss 1.786320
[epoch1, step1234]: loss 1.588844
[epoch1, step1235]: loss 1.847959
[epoch1, step1236]: loss 1.854721
[epoch1, step1237]: loss 1.936940
[epoch1, step1238]: loss 2.046164
[epoch1, step1239]: loss 2.149843
[epoch1, step1240]: loss 1.754814
[epoch1, step1241]: loss 1.893400
[epoch1, step1242]: loss 1.603663
[epoch1, step1243]: loss 2.236013
[epoch1, step1244]: loss 1.889435
[epoch1, step1245]: loss 1.718361
[epoch1, step1246]: loss 2.094820
[epoch1, step1247]: loss 2.080498
[epoch1, step1248]: loss 2.078916
[epoch1, step1249]: loss 1.880744
[epoch1, step1250]: loss 2.155600
[epoch1, step1251]: loss 1.888753
[epoch1, step1252]: loss 1.753979
[epoch1, step1253]: loss 2.093956
[epoch1, step1254]: loss 2.122680
[epoch1, step1255]: loss 1.950251
[epoch1, step1256]: loss 2.110920
[epoch1, step1257]: loss 2.045166
[epoch1, step1258]: loss 2.048792
[epoch1, step1259]: loss 1.963629
[epoch1, step1260]: loss 2.154666
[epoch1, step1261]: loss 1.767380
[epoch1, step1262]: loss 1.911071
[epoch1, step1263]: loss 1.973686
[epoch1, step1264]: loss 1.612849
[epoch1, step1265]: loss 2.123017
[epoch1, step1266]: loss 1.769051
[epoch1, step1267]: loss 1.842084
[epoch1, step1268]: loss 1.618870
[epoch1, step1269]: loss 1.895117
[epoch1, step1270]: loss 1.553162
[epoch1, step1271]: loss 1.914876
[epoch1, step1272]: loss 1.938639
[epoch1, step1273]: loss 2.041020
[epoch1, step1274]: loss 1.867079
[epoch1, step1275]: loss 1.674163
[epoch1, step1276]: loss 1.847683
[epoch1, step1277]: loss 1.879735
[epoch1, step1278]: loss 1.879004
[epoch1, step1279]: loss 1.839841
[epoch1, step1280]: loss 1.923761
[epoch1, step1281]: loss 1.702259
[epoch1, step1282]: loss 1.454923
[epoch1, step1283]: loss 1.935841
[epoch1, step1284]: loss 2.027113
[epoch1, step1285]: loss 2.115365
[epoch1, step1286]: loss 2.014937
[epoch1, step1287]: loss 1.823760
[epoch1, step1288]: loss 1.922604
[epoch1, step1289]: loss 1.976505
[epoch1, step1290]: loss 1.736338
[epoch1, step1291]: loss 1.958525
[epoch1, step1292]: loss 1.582491
[epoch1, step1293]: loss 1.750498
[epoch1, step1294]: loss 1.846794
[epoch1, step1295]: loss 1.791469
[epoch1, step1296]: loss 2.117391
[epoch1, step1297]: loss 2.239266
[epoch1, step1298]: loss 1.610796
[epoch1, step1299]: loss 2.024817
[epoch1, step1300]: loss 1.821136
[epoch1, step1301]: loss 1.564219
[epoch1, step1302]: loss 1.865971
[epoch1, step1303]: loss 1.805301
[epoch1, step1304]: loss 1.336908
[epoch1, step1305]: loss 2.017735
[epoch1, step1306]: loss 2.056701
[epoch1, step1307]: loss 2.131065
[epoch1, step1308]: loss 1.357925
[epoch1, step1309]: loss 1.795282
[epoch1, step1310]: loss 1.805179
[epoch1, step1311]: loss 1.800824
[epoch1, step1312]: loss 1.784395
[epoch1, step1313]: loss 1.880444
[epoch1, step1314]: loss 1.968823
[epoch1, step1315]: loss 1.785534
[epoch1, step1316]: loss 1.996101
[epoch1, step1317]: loss 1.755613
[epoch1, step1318]: loss 1.837642
[epoch1, step1319]: loss 1.816841
[epoch1, step1320]: loss 1.738852
[epoch1, step1321]: loss 2.073513
[epoch1, step1322]: loss 1.886277
[epoch1, step1323]: loss 1.961192
[epoch1, step1324]: loss 1.892581
[epoch1, step1325]: loss 2.083340
[epoch1, step1326]: loss 1.787088
[epoch1, step1327]: loss 1.597206
[epoch1, step1328]: loss 1.569716
[epoch1, step1329]: loss 1.674283
[epoch1, step1330]: loss 2.005746
[epoch1, step1331]: loss 1.900666
[epoch1, step1332]: loss 1.599532
[epoch1, step1333]: loss 1.688859
[epoch1, step1334]: loss 1.993843
[epoch1, step1335]: loss 1.901018
[epoch1, step1336]: loss 1.961474
[epoch1, step1337]: loss 1.633978
[epoch1, step1338]: loss 1.861692
[epoch1, step1339]: loss 1.896617
[epoch1, step1340]: loss 1.347936
[epoch1, step1341]: loss 2.032669
[epoch1, step1342]: loss 2.244308
[epoch1, step1343]: loss 2.060534
[epoch1, step1344]: loss 2.024729
[epoch1, step1345]: loss 1.764814
[epoch1, step1346]: loss 1.777171
[epoch1, step1347]: loss 1.899843
[epoch1, step1348]: loss 1.760490
[epoch1, step1349]: loss 1.937233
[epoch1, step1350]: loss 1.473066
[epoch1, step1351]: loss 1.469988
[epoch1, step1352]: loss 2.172300
[epoch1, step1353]: loss 1.688556
[epoch1, step1354]: loss 1.855851
[epoch1, step1355]: loss 1.835480
[epoch1, step1356]: loss 1.708711
[epoch1, step1357]: loss 1.474327
[epoch1, step1358]: loss 1.617508
[epoch1, step1359]: loss 1.340532
[epoch1, step1360]: loss 1.609828
[epoch1, step1361]: loss 1.985278
[epoch1, step1362]: loss 2.052265
[epoch1, step1363]: loss 1.898313
[epoch1, step1364]: loss 1.723575
[epoch1, step1365]: loss 1.764403
[epoch1, step1366]: loss 1.872578
[epoch1, step1367]: loss 1.925558
[epoch1, step1368]: loss 1.624907
[epoch1, step1369]: loss 1.388556
[epoch1, step1370]: loss 1.720911
[epoch1, step1371]: loss 2.082073
[epoch1, step1372]: loss 1.471113
[epoch1, step1373]: loss 1.769143
[epoch1, step1374]: loss 1.979234
[epoch1, step1375]: loss 1.742656
[epoch1, step1376]: loss 1.650948
[epoch1, step1377]: loss 1.632328
[epoch1, step1378]: loss 1.678786
[epoch1, step1379]: loss 1.593087
[epoch1, step1380]: loss 1.539688
[epoch1, step1381]: loss 1.642429
[epoch1, step1382]: loss 1.813956
[epoch1, step1383]: loss 1.973175
[epoch1, step1384]: loss 1.665033
[epoch1, step1385]: loss 1.745422
[epoch1, step1386]: loss 1.693681
[epoch1, step1387]: loss 1.765763
[epoch1, step1388]: loss 1.830363
[epoch1, step1389]: loss 1.770474
[epoch1, step1390]: loss 2.205091
[epoch1, step1391]: loss 1.791341
[epoch1, step1392]: loss 1.909185
[epoch1, step1393]: loss 1.806973
[epoch1, step1394]: loss 1.817089
[epoch1, step1395]: loss 1.938463
[epoch1, step1396]: loss 1.888815
[epoch1, step1397]: loss 1.519397
[epoch1, step1398]: loss 2.126321
[epoch1, step1399]: loss 1.789032
[epoch1, step1400]: loss 1.697655
[epoch1, step1401]: loss 1.734339
[epoch1, step1402]: loss 1.907451
[epoch1, step1403]: loss 1.831630
[epoch1, step1404]: loss 1.455420
[epoch1, step1405]: loss 1.769987
[epoch1, step1406]: loss 1.643843
[epoch1, step1407]: loss 2.029513
[epoch1, step1408]: loss 1.985450
[epoch1, step1409]: loss 1.649069
[epoch1, step1410]: loss 1.924251
[epoch1, step1411]: loss 1.650824
[epoch1, step1412]: loss 1.741316
[epoch1, step1413]: loss 1.827357
[epoch1, step1414]: loss 1.366074
[epoch1, step1415]: loss 1.884885
[epoch1, step1416]: loss 1.483789
[epoch1, step1417]: loss 1.710491
[epoch1, step1418]: loss 1.940671
[epoch1, step1419]: loss 1.590745
[epoch1, step1420]: loss 2.022718
[epoch1, step1421]: loss 1.738966
[epoch1, step1422]: loss 1.619063
[epoch1, step1423]: loss 1.900592
[epoch1, step1424]: loss 1.596910
[epoch1, step1425]: loss 1.662349
[epoch1, step1426]: loss 1.853618
[epoch1, step1427]: loss 1.823885
[epoch1, step1428]: loss 1.608156
[epoch1, step1429]: loss 1.854010
[epoch1, step1430]: loss 1.632153
[epoch1, step1431]: loss 1.759615
[epoch1, step1432]: loss 2.059700
[epoch1, step1433]: loss 1.918823
[epoch1, step1434]: loss 1.763779
[epoch1, step1435]: loss 1.663680
[epoch1, step1436]: loss 1.922931
[epoch1, step1437]: loss 1.437541
[epoch1, step1438]: loss 1.489559
[epoch1, step1439]: loss 1.701519
[epoch1, step1440]: loss 1.755077
[epoch1, step1441]: loss 1.759380
[epoch1, step1442]: loss 1.696124
[epoch1, step1443]: loss 1.386598
[epoch1, step1444]: loss 1.774447
[epoch1, step1445]: loss 1.660780
[epoch1, step1446]: loss 1.782014
[epoch1, step1447]: loss 1.800900
[epoch1, step1448]: loss 1.838422
[epoch1, step1449]: loss 1.686486
[epoch1, step1450]: loss 1.957476
[epoch1, step1451]: loss 2.037456
[epoch1, step1452]: loss 1.403225
[epoch1, step1453]: loss 1.615499
[epoch1, step1454]: loss 1.627392
[epoch1, step1455]: loss 1.967352
[epoch1, step1456]: loss 1.587481
[epoch1, step1457]: loss 1.692282
[epoch1, step1458]: loss 1.432757
[epoch1, step1459]: loss 1.580600
[epoch1, step1460]: loss 1.662317
[epoch1, step1461]: loss 1.555775
[epoch1, step1462]: loss 1.215064
[epoch1, step1463]: loss 1.575308
[epoch1, step1464]: loss 1.188211
[epoch1, step1465]: loss 1.594111
[epoch1, step1466]: loss 1.815554
[epoch1, step1467]: loss 1.679157
[epoch1, step1468]: loss 1.880103
[epoch1, step1469]: loss 1.974561
[epoch1, step1470]: loss 1.483015
[epoch1, step1471]: loss 1.526792
[epoch1, step1472]: loss 1.570316
[epoch1, step1473]: loss 1.694410
[epoch1, step1474]: loss 1.587911
[epoch1, step1475]: loss 1.970865
[epoch1, step1476]: loss 1.397494
[epoch1, step1477]: loss 1.751693
[epoch1, step1478]: loss 1.810019
[epoch1, step1479]: loss 1.530262
[epoch1, step1480]: loss 1.876188
[epoch1, step1481]: loss 1.612134
[epoch1, step1482]: loss 1.487983
[epoch1, step1483]: loss 1.752307
[epoch1, step1484]: loss 1.204225
[epoch1, step1485]: loss 1.459018
[epoch1, step1486]: loss 1.540141
[epoch1, step1487]: loss 1.978757
[epoch1, step1488]: loss 1.432433
[epoch1, step1489]: loss 1.862703
[epoch1, step1490]: loss 1.825119
[epoch1, step1491]: loss 1.789399
[epoch1, step1492]: loss 1.599128
[epoch1, step1493]: loss 1.804948
[epoch1, step1494]: loss 2.062823
[epoch1, step1495]: loss 1.575327
[epoch1, step1496]: loss 1.887055
[epoch1, step1497]: loss 1.500298
[epoch1, step1498]: loss 1.612990
[epoch1, step1499]: loss 1.204505
[epoch1, step1500]: loss 1.503827
[epoch1, step1501]: loss 1.610125
[epoch1, step1502]: loss 1.186457
[epoch1, step1503]: loss 1.696352
[epoch1, step1504]: loss 1.694978
[epoch1, step1505]: loss 1.612904
[epoch1, step1506]: loss 1.634036
[epoch1, step1507]: loss 1.721482
[epoch1, step1508]: loss 1.834060
[epoch1, step1509]: loss 1.518685
[epoch1, step1510]: loss 1.611072
[epoch1, step1511]: loss 1.730614
[epoch1, step1512]: loss 2.003507
[epoch1, step1513]: loss 1.491052
[epoch1, step1514]: loss 1.849127
[epoch1, step1515]: loss 1.991830
[epoch1, step1516]: loss 1.713125
[epoch1, step1517]: loss 1.869307
[epoch1, step1518]: loss 1.622440
[epoch1, step1519]: loss 1.487319
[epoch1, step1520]: loss 1.963582
[epoch1, step1521]: loss 1.829457
[epoch1, step1522]: loss 1.393805
[epoch1, step1523]: loss 1.489693
[epoch1, step1524]: loss 1.216671
[epoch1, step1525]: loss 1.653144
[epoch1, step1526]: loss 1.613444
[epoch1, step1527]: loss 1.633773
[epoch1, step1528]: loss 1.688656
[epoch1, step1529]: loss 1.241213
[epoch1, step1530]: loss 1.738596
[epoch1, step1531]: loss 1.829503
[epoch1, step1532]: loss 1.630796
[epoch1, step1533]: loss 1.691817
[epoch1, step1534]: loss 1.816250
[epoch1, step1535]: loss 1.504072
[epoch1, step1536]: loss 1.456760
[epoch1, step1537]: loss 1.547545
[epoch1, step1538]: loss 1.827541
[epoch1, step1539]: loss 1.725715
[epoch1, step1540]: loss 1.479521
[epoch1, step1541]: loss 1.672815
[epoch1, step1542]: loss 1.782418
[epoch1, step1543]: loss 1.602153
[epoch1, step1544]: loss 1.435185
[epoch1, step1545]: loss 1.420465
[epoch1, step1546]: loss 1.708704
[epoch1, step1547]: loss 1.414908
[epoch1, step1548]: loss 1.720958
[epoch1, step1549]: loss 1.472018
[epoch1, step1550]: loss 1.512755
[epoch1, step1551]: loss 1.604767
[epoch1, step1552]: loss 1.596791
[epoch1, step1553]: loss 1.583379
[epoch1, step1554]: loss 1.684568
[epoch1, step1555]: loss 1.342108
[epoch1, step1556]: loss 1.311734
[epoch1, step1557]: loss 1.417817
[epoch1, step1558]: loss 1.847246
[epoch1, step1559]: loss 1.576961
[epoch1, step1560]: loss 1.622641
[epoch1, step1561]: loss 1.426969
[epoch1, step1562]: loss 1.631124
[epoch1, step1563]: loss 1.795196
[epoch1, step1564]: loss 1.625659
[epoch1, step1565]: loss 1.847473
[epoch1, step1566]: loss 1.358149
[epoch1, step1567]: loss 1.589911
[epoch1, step1568]: loss 1.584563
[epoch1, step1569]: loss 1.570695
[epoch1, step1570]: loss 1.907593
[epoch1, step1571]: loss 1.493627
[epoch1, step1572]: loss 1.602336
[epoch1, step1573]: loss 1.831860
[epoch1, step1574]: loss 1.154270
[epoch1, step1575]: loss 1.708374
[epoch1, step1576]: loss 1.492142
[epoch1, step1577]: loss 1.599753
[epoch1, step1578]: loss 1.280143
[epoch1, step1579]: loss 1.365154
[epoch1, step1580]: loss 1.690938
[epoch1, step1581]: loss 1.401464
[epoch1, step1582]: loss 1.397375
[epoch1, step1583]: loss 1.993741
[epoch1, step1584]: loss 1.585781
[epoch1, step1585]: loss 1.497653
[epoch1, step1586]: loss 1.727740
[epoch1, step1587]: loss 1.869821
[epoch1, step1588]: loss 1.838794
[epoch1, step1589]: loss 1.449571
[epoch1, step1590]: loss 1.283413
[epoch1, step1591]: loss 1.633273
[epoch1, step1592]: loss 1.833036
[epoch1, step1593]: loss 1.598678
[epoch1, step1594]: loss 1.426058
[epoch1, step1595]: loss 1.391604
[epoch1, step1596]: loss 1.684803
[epoch1, step1597]: loss 1.707587
[epoch1, step1598]: loss 1.332566
[epoch1, step1599]: loss 1.442410
[epoch1, step1600]: loss 1.774471
[epoch1, step1601]: loss 1.487282
[epoch1, step1602]: loss 1.813272
[epoch1, step1603]: loss 1.557495
[epoch1, step1604]: loss 1.416631
[epoch1, step1605]: loss 1.712308
[epoch1, step1606]: loss 1.898000
[epoch1, step1607]: loss 1.567513
[epoch1, step1608]: loss 1.405652
[epoch1, step1609]: loss 1.665863
[epoch1, step1610]: loss 1.357060
[epoch1, step1611]: loss 1.681115
[epoch1, step1612]: loss 1.433452
[epoch1, step1613]: loss 1.853777
[epoch1, step1614]: loss 1.412372
[epoch1, step1615]: loss 0.994012
[epoch1, step1616]: loss 1.817870
[epoch1, step1617]: loss 1.787626
[epoch1, step1618]: loss 2.021520
[epoch1, step1619]: loss 1.574920
[epoch1, step1620]: loss 1.403221
[epoch1, step1621]: loss 1.616690
[epoch1, step1622]: loss 1.556801
[epoch1, step1623]: loss 1.614542
[epoch1, step1624]: loss 1.672856
[epoch1, step1625]: loss 1.534111
[epoch1, step1626]: loss 1.689625
[epoch1, step1627]: loss 1.662418
[epoch1, step1628]: loss 1.303396
[epoch1, step1629]: loss 0.902749
[epoch1, step1630]: loss 1.605260
[epoch1, step1631]: loss 1.704036
[epoch1, step1632]: loss 1.426022
[epoch1, step1633]: loss 1.614535
[epoch1, step1634]: loss 1.418319
[epoch1, step1635]: loss 1.754525
[epoch1, step1636]: loss 1.239450
[epoch1, step1637]: loss 1.482044
[epoch1, step1638]: loss 1.392961
[epoch1, step1639]: loss 1.712547
[epoch1, step1640]: loss 1.676272
[epoch1, step1641]: loss 1.863753
[epoch1, step1642]: loss 1.567042
[epoch1, step1643]: loss 1.188796
[epoch1, step1644]: loss 1.647438
[epoch1, step1645]: loss 1.972855
[epoch1, step1646]: loss 1.449936
[epoch1, step1647]: loss 1.623418
[epoch1, step1648]: loss 1.780504
[epoch1, step1649]: loss 1.640528
[epoch1, step1650]: loss 1.488563
[epoch1, step1651]: loss 1.788058
[epoch1, step1652]: loss 1.639232
[epoch1, step1653]: loss 1.725965
[epoch1, step1654]: loss 1.124052
[epoch1, step1655]: loss 1.874853
[epoch1, step1656]: loss 1.277510
[epoch1, step1657]: loss 1.534643
[epoch1, step1658]: loss 1.677512
[epoch1, step1659]: loss 1.513377
[epoch1, step1660]: loss 1.766887
[epoch1, step1661]: loss 1.331417
[epoch1, step1662]: loss 1.686137
[epoch1, step1663]: loss 1.178609
[epoch1, step1664]: loss 1.703656
[epoch1, step1665]: loss 1.605618
[epoch1, step1666]: loss 1.666189
[epoch1, step1667]: loss 1.443236
[epoch1, step1668]: loss 1.423385
[epoch1, step1669]: loss 1.613440
[epoch1, step1670]: loss 1.338918
[epoch1, step1671]: loss 1.498967
[epoch1, step1672]: loss 1.852889
[epoch1, step1673]: loss 1.330627
[epoch1, step1674]: loss 1.457593
[epoch1, step1675]: loss 1.293881
[epoch1, step1676]: loss 1.921248
[epoch1, step1677]: loss 1.575105
[epoch1, step1678]: loss 1.432380
[epoch1, step1679]: loss 1.728430
[epoch1, step1680]: loss 1.626003
[epoch1, step1681]: loss 1.274806
[epoch1, step1682]: loss 1.631668
[epoch1, step1683]: loss 1.569381
[epoch1, step1684]: loss 1.618825
[epoch1, step1685]: loss 1.788666
[epoch1, step1686]: loss 1.795962
[epoch1, step1687]: loss 1.708987
[epoch1, step1688]: loss 1.702725
[epoch1, step1689]: loss 1.451566
[epoch1, step1690]: loss 1.600104
[epoch1, step1691]: loss 1.517974
[epoch1, step1692]: loss 1.879454
[epoch1, step1693]: loss 1.660989
[epoch1, step1694]: loss 1.893414
[epoch1, step1695]: loss 1.502470
[epoch1, step1696]: loss 1.624141
[epoch1, step1697]: loss 1.568321
[epoch1, step1698]: loss 1.788037
[epoch1, step1699]: loss 1.557430
[epoch1, step1700]: loss 1.411349
[epoch1, step1701]: loss 1.256393
[epoch1, step1702]: loss 1.572235
[epoch1, step1703]: loss 1.469915
[epoch1, step1704]: loss 1.752569
[epoch1, step1705]: loss 1.639629
[epoch1, step1706]: loss 1.550282
[epoch1, step1707]: loss 1.307909
[epoch1, step1708]: loss 1.455525
[epoch1, step1709]: loss 1.282426
[epoch1, step1710]: loss 1.585750
[epoch1, step1711]: loss 1.654152
[epoch1, step1712]: loss 1.677600
[epoch1, step1713]: loss 1.673314
[epoch1, step1714]: loss 1.676461
[epoch1, step1715]: loss 1.629014
[epoch1, step1716]: loss 1.551623
[epoch1, step1717]: loss 1.355878
[epoch1, step1718]: loss 1.838761
[epoch1, step1719]: loss 1.513397
[epoch1, step1720]: loss 1.836675
[epoch1, step1721]: loss 1.627271
[epoch1, step1722]: loss 1.397169
[epoch1, step1723]: loss 1.878002
[epoch1, step1724]: loss 1.131713
[epoch1, step1725]: loss 1.755759
[epoch1, step1726]: loss 1.580758
[epoch1, step1727]: loss 1.592958
[epoch1, step1728]: loss 1.489969
[epoch1, step1729]: loss 1.505807
[epoch1, step1730]: loss 1.600813
[epoch1, step1731]: loss 1.932784
[epoch1, step1732]: loss 1.829821
[epoch1, step1733]: loss 1.579047
[epoch1, step1734]: loss 1.650857
[epoch1, step1735]: loss 1.613467
[epoch1, step1736]: loss 1.441951
[epoch1, step1737]: loss 1.380710
[epoch1, step1738]: loss 1.403834
[epoch1, step1739]: loss 1.544543
[epoch1, step1740]: loss 1.315897
[epoch1, step1741]: loss 1.723063
[epoch1, step1742]: loss 1.230574
[epoch1, step1743]: loss 1.362749
[epoch1, step1744]: loss 1.572624
[epoch1, step1745]: loss 1.284431
[epoch1, step1746]: loss 1.867940
[epoch1, step1747]: loss 1.521113
[epoch1, step1748]: loss 1.612093
[epoch1, step1749]: loss 1.458178
[epoch1, step1750]: loss 0.994746
[epoch1, step1751]: loss 1.715655
[epoch1, step1752]: loss 1.528100
[epoch1, step1753]: loss 1.242632
[epoch1, step1754]: loss 1.221011
[epoch1, step1755]: loss 1.562931
[epoch1, step1756]: loss 1.393177
[epoch1, step1757]: loss 1.695277
[epoch1, step1758]: loss 1.375246
[epoch1, step1759]: loss 1.627619
[epoch1, step1760]: loss 1.488680
[epoch1, step1761]: loss 1.640360
[epoch1, step1762]: loss 1.147500
[epoch1, step1763]: loss 1.642127
[epoch1, step1764]: loss 1.491893
[epoch1, step1765]: loss 1.225276
[epoch1, step1766]: loss 1.617736
[epoch1, step1767]: loss 1.578846
[epoch1, step1768]: loss 1.480252
[epoch1, step1769]: loss 1.464550
[epoch1, step1770]: loss 1.598094
[epoch1, step1771]: loss 1.455187
[epoch1, step1772]: loss 1.459000
[epoch1, step1773]: loss 1.703948
[epoch1, step1774]: loss 1.263607
[epoch1, step1775]: loss 1.636570
[epoch1, step1776]: loss 1.405992
[epoch1, step1777]: loss 1.632734
[epoch1, step1778]: loss 1.576045
[epoch1, step1779]: loss 1.383906
[epoch1, step1780]: loss 1.464812
[epoch1, step1781]: loss 0.931795
[epoch1, step1782]: loss 1.648601
[epoch1, step1783]: loss 1.377815
[epoch1, step1784]: loss 1.748306
[epoch1, step1785]: loss 1.454001
[epoch1, step1786]: loss 1.678531
[epoch1, step1787]: loss 1.363311
[epoch1, step1788]: loss 1.644602
[epoch1, step1789]: loss 1.438938
[epoch1, step1790]: loss 1.555876
[epoch1, step1791]: loss 1.638663
[epoch1, step1792]: loss 1.485679
[epoch1, step1793]: loss 1.150610
[epoch1, step1794]: loss 1.798591
[epoch1, step1795]: loss 1.471291
[epoch1, step1796]: loss 1.530340
[epoch1, step1797]: loss 1.482758
[epoch1, step1798]: loss 1.324186
[epoch1, step1799]: loss 1.452770
[epoch1, step1800]: loss 1.512565
[epoch1, step1801]: loss 1.424511
[epoch1, step1802]: loss 1.704221
[epoch1, step1803]: loss 1.664021
[epoch1, step1804]: loss 1.501729
[epoch1, step1805]: loss 1.565433
[epoch1, step1806]: loss 1.510050
[epoch1, step1807]: loss 1.611244
[epoch1, step1808]: loss 1.792010
[epoch1, step1809]: loss 1.270474
[epoch1, step1810]: loss 1.389091
[epoch1, step1811]: loss 1.311982
[epoch1, step1812]: loss 1.712760
[epoch1, step1813]: loss 1.409303
[epoch1, step1814]: loss 1.301695
[epoch1, step1815]: loss 1.696763
[epoch1, step1816]: loss 1.704615
[epoch1, step1817]: loss 1.388494
[epoch1, step1818]: loss 1.300631
[epoch1, step1819]: loss 1.293626
[epoch1, step1820]: loss 1.102948
[epoch1, step1821]: loss 1.648400
[epoch1, step1822]: loss 1.783629
[epoch1, step1823]: loss 1.638373
[epoch1, step1824]: loss 1.668027
[epoch1, step1825]: loss 1.623294
[epoch1, step1826]: loss 1.628046
[epoch1, step1827]: loss 1.754145
[epoch1, step1828]: loss 1.355795
[epoch1, step1829]: loss 1.715025
[epoch1, step1830]: loss 1.462431
[epoch1, step1831]: loss 1.350677
[epoch1, step1832]: loss 1.588687
[epoch1, step1833]: loss 1.255339
[epoch1, step1834]: loss 1.323453
[epoch1, step1835]: loss 1.751637
[epoch1, step1836]: loss 1.412030
[epoch1, step1837]: loss 1.655166
[epoch1, step1838]: loss 1.645235
[epoch1, step1839]: loss 1.480361
[epoch1, step1840]: loss 1.559148
[epoch1, step1841]: loss 1.466253
[epoch1, step1842]: loss 1.422545
[epoch1, step1843]: loss 1.545933
[epoch1, step1844]: loss 1.614895
[epoch1, step1845]: loss 1.662795
[epoch1, step1846]: loss 1.329754
[epoch1, step1847]: loss 1.184521
[epoch1, step1848]: loss 1.934034
[epoch1, step1849]: loss 1.719761
[epoch1, step1850]: loss 1.381025
[epoch1, step1851]: loss 1.225478
[epoch1, step1852]: loss 1.630827
[epoch1, step1853]: loss 1.621354
[epoch1, step1854]: loss 1.799889
[epoch1, step1855]: loss 1.454404
[epoch1, step1856]: loss 1.532990
[epoch1, step1857]: loss 1.338077
[epoch1, step1858]: loss 1.177845
[epoch1, step1859]: loss 1.580241
[epoch1, step1860]: loss 1.456009
[epoch1, step1861]: loss 1.599840
[epoch1, step1862]: loss 1.418737
[epoch1, step1863]: loss 1.668802
[epoch1, step1864]: loss 1.138338
[epoch1, step1865]: loss 1.587338
[epoch1, step1866]: loss 1.471396
[epoch1, step1867]: loss 1.619888
[epoch1, step1868]: loss 1.342709
[epoch1, step1869]: loss 1.700270
[epoch1, step1870]: loss 1.508699
[epoch1, step1871]: loss 1.609742
[epoch1, step1872]: loss 1.345924
[epoch1, step1873]: loss 1.382547
[epoch1, step1874]: loss 1.466355
[epoch1, step1875]: loss 1.709845
[epoch1, step1876]: loss 1.669942
[epoch1, step1877]: loss 1.478379
[epoch1, step1878]: loss 1.372349
[epoch1, step1879]: loss 1.577918
[epoch1, step1880]: loss 1.023505
[epoch1, step1881]: loss 1.196370
[epoch1, step1882]: loss 1.478985
[epoch1, step1883]: loss 1.331333
[epoch1, step1884]: loss 1.780977
[epoch1, step1885]: loss 1.479939
[epoch1, step1886]: loss 1.111795
[epoch1, step1887]: loss 1.716753
[epoch1, step1888]: loss 1.664792
[epoch1, step1889]: loss 1.064059
[epoch1, step1890]: loss 1.510825
[epoch1, step1891]: loss 1.470546
[epoch1, step1892]: loss 1.671016
[epoch1, step1893]: loss 0.995080
[epoch1, step1894]: loss 1.394878
[epoch1, step1895]: loss 1.351327
[epoch1, step1896]: loss 1.404245
[epoch1, step1897]: loss 1.867274
[epoch1, step1898]: loss 1.435553
[epoch1, step1899]: loss 1.816403
[epoch1, step1900]: loss 1.583684
[epoch1, step1901]: loss 1.814567
[epoch1, step1902]: loss 1.178164
[epoch1, step1903]: loss 1.403651
[epoch1, step1904]: loss 1.597982
[epoch1, step1905]: loss 1.573673
[epoch1, step1906]: loss 1.731744
[epoch1, step1907]: loss 0.959740
[epoch1, step1908]: loss 1.251203
[epoch1, step1909]: loss 1.587539
[epoch1, step1910]: loss 1.349495
[epoch1, step1911]: loss 1.058011
[epoch1, step1912]: loss 1.340034
[epoch1, step1913]: loss 1.093960
[epoch1, step1914]: loss 1.755304
[epoch1, step1915]: loss 1.104697
[epoch1, step1916]: loss 1.543060
[epoch1, step1917]: loss 1.581327
[epoch1, step1918]: loss 1.362679
[epoch1, step1919]: loss 1.691126
[epoch1, step1920]: loss 1.596012
[epoch1, step1921]: loss 1.401312
[epoch1, step1922]: loss 1.475686
[epoch1, step1923]: loss 1.476708
[epoch1, step1924]: loss 1.229731
[epoch1, step1925]: loss 1.527798
[epoch1, step1926]: loss 1.740537
[epoch1, step1927]: loss 1.292227
[epoch1, step1928]: loss 1.479030
[epoch1, step1929]: loss 1.282242
[epoch1, step1930]: loss 1.701320
[epoch1, step1931]: loss 1.616269
[epoch1, step1932]: loss 1.218972
[epoch1, step1933]: loss 1.160030
[epoch1, step1934]: loss 1.459936
[epoch1, step1935]: loss 1.158028
[epoch1, step1936]: loss 1.649046
[epoch1, step1937]: loss 1.487397
[epoch1, step1938]: loss 1.362130
[epoch1, step1939]: loss 0.995880
[epoch1, step1940]: loss 1.378456
[epoch1, step1941]: loss 1.437311
[epoch1, step1942]: loss 1.452882
[epoch1, step1943]: loss 1.338809
[epoch1, step1944]: loss 1.488817
[epoch1, step1945]: loss 0.961206
[epoch1, step1946]: loss 1.459314
[epoch1, step1947]: loss 1.504928
[epoch1, step1948]: loss 1.644568
[epoch1, step1949]: loss 1.579420
[epoch1, step1950]: loss 1.093683
[epoch1, step1951]: loss 1.262791
[epoch1, step1952]: loss 1.330926
[epoch1, step1953]: loss 1.547747
[epoch1, step1954]: loss 1.423229
[epoch1, step1955]: loss 0.875241
[epoch1, step1956]: loss 1.616153
[epoch1, step1957]: loss 1.326639
[epoch1, step1958]: loss 1.451339
[epoch1, step1959]: loss 1.021525
[epoch1, step1960]: loss 1.643376
[epoch1, step1961]: loss 1.575696
[epoch1, step1962]: loss 1.596963
[epoch1, step1963]: loss 1.448882
[epoch1, step1964]: loss 1.321401
[epoch1, step1965]: loss 1.419269
[epoch1, step1966]: loss 1.374004
[epoch1, step1967]: loss 1.532129
[epoch1, step1968]: loss 1.411212
[epoch1, step1969]: loss 0.777963
[epoch1, step1970]: loss 1.227804
[epoch1, step1971]: loss 1.398784
[epoch1, step1972]: loss 1.276190
[epoch1, step1973]: loss 1.307352
[epoch1, step1974]: loss 1.432279
[epoch1, step1975]: loss 1.713153
[epoch1, step1976]: loss 1.614371
[epoch1, step1977]: loss 0.994274
[epoch1, step1978]: loss 1.544680
[epoch1, step1979]: loss 1.425799
[epoch1, step1980]: loss 1.619219
[epoch1, step1981]: loss 1.145441
[epoch1, step1982]: loss 1.435083
[epoch1, step1983]: loss 1.363414
[epoch1, step1984]: loss 1.327056
[epoch1, step1985]: loss 1.417020
[epoch1, step1986]: loss 1.288345
[epoch1, step1987]: loss 1.549675
[epoch1, step1988]: loss 1.630927
[epoch1, step1989]: loss 1.389763
[epoch1, step1990]: loss 1.428491
[epoch1, step1991]: loss 1.550659
[epoch1, step1992]: loss 1.413854
[epoch1, step1993]: loss 1.580418
[epoch1, step1994]: loss 1.743377
[epoch1, step1995]: loss 1.417884
[epoch1, step1996]: loss 1.279915
[epoch1, step1997]: loss 1.520485
[epoch1, step1998]: loss 1.459569
[epoch1, step1999]: loss 1.210908
[epoch1, step2000]: loss 1.045845
[epoch1, step2001]: loss 1.565462
[epoch1, step2002]: loss 1.780049
[epoch1, step2003]: loss 1.103208
[epoch1, step2004]: loss 1.209746
[epoch1, step2005]: loss 1.760809
[epoch1, step2006]: loss 1.520564
[epoch1, step2007]: loss 1.268256
[epoch1, step2008]: loss 1.179012
[epoch1, step2009]: loss 1.215667
[epoch1, step2010]: loss 0.963954
[epoch1, step2011]: loss 1.463458
[epoch1, step2012]: loss 1.188814
[epoch1, step2013]: loss 1.491885
[epoch1, step2014]: loss 1.662551
[epoch1, step2015]: loss 1.608199
[epoch1, step2016]: loss 1.520392
[epoch1, step2017]: loss 1.368079
[epoch1, step2018]: loss 1.222595
[epoch1, step2019]: loss 1.048373
[epoch1, step2020]: loss 1.479704
[epoch1, step2021]: loss 1.812301
[epoch1, step2022]: loss 0.978927
[epoch1, step2023]: loss 1.742593
[epoch1, step2024]: loss 1.216318
[epoch1, step2025]: loss 1.400010
[epoch1, step2026]: loss 1.404495
[epoch1, step2027]: loss 1.320499
[epoch1, step2028]: loss 1.267717
[epoch1, step2029]: loss 1.191272
[epoch1, step2030]: loss 1.349496
[epoch1, step2031]: loss 1.026437
[epoch1, step2032]: loss 1.568325
[epoch1, step2033]: loss 1.477187
[epoch1, step2034]: loss 1.150122
[epoch1, step2035]: loss 0.904465
[epoch1, step2036]: loss 1.253815
[epoch1, step2037]: loss 1.532719
[epoch1, step2038]: loss 1.661212
[epoch1, step2039]: loss 1.611172
[epoch1, step2040]: loss 1.458323
[epoch1, step2041]: loss 1.336757
[epoch1, step2042]: loss 1.320756
[epoch1, step2043]: loss 1.423496
[epoch1, step2044]: loss 1.803477
[epoch1, step2045]: loss 1.123796
[epoch1, step2046]: loss 1.271777
[epoch1, step2047]: loss 1.624853
[epoch1, step2048]: loss 1.190303
[epoch1, step2049]: loss 1.403536
[epoch1, step2050]: loss 1.347150
[epoch1, step2051]: loss 1.461468
[epoch1, step2052]: loss 1.615781
[epoch1, step2053]: loss 1.417266
[epoch1, step2054]: loss 1.365075
[epoch1, step2055]: loss 1.655312
[epoch1, step2056]: loss 1.330267
[epoch1, step2057]: loss 1.494999
[epoch1, step2058]: loss 1.163639
[epoch1, step2059]: loss 1.041553
[epoch1, step2060]: loss 1.507063
[epoch1, step2061]: loss 1.255726
[epoch1, step2062]: loss 1.147496
[epoch1, step2063]: loss 1.580162
[epoch1, step2064]: loss 1.046618
[epoch1, step2065]: loss 1.025592
[epoch1, step2066]: loss 1.368917
[epoch1, step2067]: loss 1.166721
[epoch1, step2068]: loss 1.463355
[epoch1, step2069]: loss 1.240337
[epoch1, step2070]: loss 1.097275
[epoch1, step2071]: loss 1.009137
[epoch1, step2072]: loss 1.512612
[epoch1, step2073]: loss 1.546215
[epoch1, step2074]: loss 1.510266
[epoch1, step2075]: loss 1.342982
[epoch1, step2076]: loss 1.547035
[epoch1, step2077]: loss 1.129891
[epoch1, step2078]: loss 1.290638
[epoch1, step2079]: loss 1.208326
[epoch1, step2080]: loss 1.297790
[epoch1, step2081]: loss 1.476210
[epoch1, step2082]: loss 1.346360
[epoch1, step2083]: loss 1.413306
[epoch1, step2084]: loss 1.274788
[epoch1, step2085]: loss 1.416589
[epoch1, step2086]: loss 1.412146
[epoch1, step2087]: loss 1.372702
[epoch1, step2088]: loss 1.533736
[epoch1, step2089]: loss 1.016400
[epoch1, step2090]: loss 1.463969
[epoch1, step2091]: loss 1.331096
[epoch1, step2092]: loss 1.175735
[epoch1, step2093]: loss 1.444980
[epoch1, step2094]: loss 1.432993
[epoch1, step2095]: loss 1.591536
[epoch1, step2096]: loss 1.296488
[epoch1, step2097]: loss 1.435440
[epoch1, step2098]: loss 1.522328
[epoch1, step2099]: loss 1.435320
[epoch1, step2100]: loss 0.857845
[epoch1, step2101]: loss 1.294324
[epoch1, step2102]: loss 1.373178
[epoch1, step2103]: loss 1.528311
[epoch1, step2104]: loss 1.405356
[epoch1, step2105]: loss 1.073038
[epoch1, step2106]: loss 1.136748
[epoch1, step2107]: loss 1.319343
[epoch1, step2108]: loss 1.337968
[epoch1, step2109]: loss 1.415335
[epoch1, step2110]: loss 1.382236
[epoch1, step2111]: loss 0.991371
[epoch1, step2112]: loss 1.431382
[epoch1, step2113]: loss 1.708633
[epoch1, step2114]: loss 1.257836
[epoch1, step2115]: loss 1.460684
[epoch1, step2116]: loss 1.323080
[epoch1, step2117]: loss 1.261823
[epoch1, step2118]: loss 1.499865
[epoch1, step2119]: loss 1.366120
[epoch1, step2120]: loss 1.368509
[epoch1, step2121]: loss 1.356587
[epoch1, step2122]: loss 1.098269
[epoch1, step2123]: loss 1.374220
[epoch1, step2124]: loss 1.384889
[epoch1, step2125]: loss 1.385470
[epoch1, step2126]: loss 1.504065
[epoch1, step2127]: loss 1.257926
[epoch1, step2128]: loss 1.203002
[epoch1, step2129]: loss 1.388580
[epoch1, step2130]: loss 1.023483
[epoch1, step2131]: loss 1.302564
[epoch1, step2132]: loss 1.451473
[epoch1, step2133]: loss 1.084985
[epoch1, step2134]: loss 1.603762
[epoch1, step2135]: loss 1.166954
[epoch1, step2136]: loss 1.093843
[epoch1, step2137]: loss 1.048014
[epoch1, step2138]: loss 1.348086
[epoch1, step2139]: loss 1.512073
[epoch1, step2140]: loss 1.333537
[epoch1, step2141]: loss 1.285419
[epoch1, step2142]: loss 1.437086
[epoch1, step2143]: loss 1.410413
[epoch1, step2144]: loss 0.903623
[epoch1, step2145]: loss 1.301258
[epoch1, step2146]: loss 1.186638
[epoch1, step2147]: loss 1.411265
[epoch1, step2148]: loss 1.090381
[epoch1, step2149]: loss 1.442971
[epoch1, step2150]: loss 1.264627
[epoch1, step2151]: loss 1.490742
[epoch1, step2152]: loss 1.225981
[epoch1, step2153]: loss 1.378100
[epoch1, step2154]: loss 1.416839
[epoch1, step2155]: loss 1.272516
[epoch1, step2156]: loss 1.417125
[epoch1, step2157]: loss 1.331856
[epoch1, step2158]: loss 0.966150
[epoch1, step2159]: loss 1.512557
[epoch1, step2160]: loss 1.274352
[epoch1, step2161]: loss 1.456851
[epoch1, step2162]: loss 1.194653
[epoch1, step2163]: loss 1.236912
[epoch1, step2164]: loss 0.867055
[epoch1, step2165]: loss 0.910338
[epoch1, step2166]: loss 1.311149
[epoch1, step2167]: loss 1.387971
[epoch1, step2168]: loss 1.649436
[epoch1, step2169]: loss 1.429157
[epoch1, step2170]: loss 1.302719
[epoch1, step2171]: loss 1.348957
[epoch1, step2172]: loss 1.415437
[epoch1, step2173]: loss 1.055272
[epoch1, step2174]: loss 1.207217
[epoch1, step2175]: loss 1.455833
[epoch1, step2176]: loss 1.423064
[epoch1, step2177]: loss 1.417182
[epoch1, step2178]: loss 1.321803
[epoch1, step2179]: loss 1.407606
[epoch1, step2180]: loss 1.440270
[epoch1, step2181]: loss 1.452215
[epoch1, step2182]: loss 1.376462
[epoch1, step2183]: loss 1.435335
[epoch1, step2184]: loss 1.558854
[epoch1, step2185]: loss 1.219561
[epoch1, step2186]: loss 1.641575
[epoch1, step2187]: loss 1.267335
[epoch1, step2188]: loss 1.469147
[epoch1, step2189]: loss 1.268964
[epoch1, step2190]: loss 1.218430
[epoch1, step2191]: loss 0.915899
[epoch1, step2192]: loss 1.512433
[epoch1, step2193]: loss 1.368823
[epoch1, step2194]: loss 1.291189
[epoch1, step2195]: loss 1.082197
[epoch1, step2196]: loss 0.853651
[epoch1, step2197]: loss 1.325267
[epoch1, step2198]: loss 1.297252
[epoch1, step2199]: loss 1.364356
[epoch1, step2200]: loss 1.200845
[epoch1, step2201]: loss 1.324674
[epoch1, step2202]: loss 1.405108
[epoch1, step2203]: loss 1.504673
[epoch1, step2204]: loss 1.432343
[epoch1, step2205]: loss 1.560486
[epoch1, step2206]: loss 1.139965
[epoch1, step2207]: loss 0.881848
[epoch1, step2208]: loss 1.084433
[epoch1, step2209]: loss 0.835845
[epoch1, step2210]: loss 1.438925
[epoch1, step2211]: loss 1.291353
[epoch1, step2212]: loss 1.112807
[epoch1, step2213]: loss 1.465497
[epoch1, step2214]: loss 1.456107
[epoch1, step2215]: loss 1.263244
[epoch1, step2216]: loss 1.050460
[epoch1, step2217]: loss 1.325793
[epoch1, step2218]: loss 1.266554
[epoch1, step2219]: loss 1.418277
[epoch1, step2220]: loss 1.471304
[epoch1, step2221]: loss 1.184657
[epoch1, step2222]: loss 1.614734
[epoch1, step2223]: loss 1.483838
[epoch1, step2224]: loss 1.235452
[epoch1, step2225]: loss 1.278611
[epoch1, step2226]: loss 1.409847
[epoch1, step2227]: loss 0.902662
[epoch1, step2228]: loss 1.343944
[epoch1, step2229]: loss 1.647810
[epoch1, step2230]: loss 1.093220
[epoch1, step2231]: loss 1.342081
[epoch1, step2232]: loss 1.193589
[epoch1, step2233]: loss 1.084584
[epoch1, step2234]: loss 1.273162
[epoch1, step2235]: loss 1.312874
[epoch1, step2236]: loss 1.322360
[epoch1, step2237]: loss 0.968539
[epoch1, step2238]: loss 1.338928
[epoch1, step2239]: loss 0.811375
[epoch1, step2240]: loss 1.169742
[epoch1, step2241]: loss 1.166906
[epoch1, step2242]: loss 1.095554
[epoch1, step2243]: loss 1.419399
[epoch1, step2244]: loss 1.373534
[epoch1, step2245]: loss 1.583893
[epoch1, step2246]: loss 1.378049
[epoch1, step2247]: loss 1.583902
[epoch1, step2248]: loss 1.293535
[epoch1, step2249]: loss 1.271638
[epoch1, step2250]: loss 1.496068
[epoch1, step2251]: loss 1.063561
[epoch1, step2252]: loss 1.311114
[epoch1, step2253]: loss 1.583066
[epoch1, step2254]: loss 1.231949
[epoch1, step2255]: loss 1.083686
[epoch1, step2256]: loss 1.432501
[epoch1, step2257]: loss 1.083669
[epoch1, step2258]: loss 1.322761
[epoch1, step2259]: loss 1.011516
[epoch1, step2260]: loss 1.217709
[epoch1, step2261]: loss 1.591548
[epoch1, step2262]: loss 1.373970
[epoch1, step2263]: loss 1.377207
[epoch1, step2264]: loss 1.500178
[epoch1, step2265]: loss 1.212722
[epoch1, step2266]: loss 1.280795
[epoch1, step2267]: loss 1.303937
[epoch1, step2268]: loss 1.398528
[epoch1, step2269]: loss 1.598121
[epoch1, step2270]: loss 1.344016
[epoch1, step2271]: loss 1.301417
[epoch1, step2272]: loss 1.238968
[epoch1, step2273]: loss 1.430199
[epoch1, step2274]: loss 1.242341
[epoch1, step2275]: loss 1.240403
[epoch1, step2276]: loss 1.452020
[epoch1, step2277]: loss 0.939218
[epoch1, step2278]: loss 1.175626
[epoch1, step2279]: loss 1.132567
[epoch1, step2280]: loss 1.058865
[epoch1, step2281]: loss 1.605272
[epoch1, step2282]: loss 0.995097
[epoch1, step2283]: loss 1.244146
[epoch1, step2284]: loss 1.192625
[epoch1, step2285]: loss 1.158516
[epoch1, step2286]: loss 1.376150
[epoch1, step2287]: loss 1.159287
[epoch1, step2288]: loss 0.813724
[epoch1, step2289]: loss 1.448622
[epoch1, step2290]: loss 1.577713
[epoch1, step2291]: loss 1.038922
[epoch1, step2292]: loss 1.409588
[epoch1, step2293]: loss 1.238500
[epoch1, step2294]: loss 1.302989
[epoch1, step2295]: loss 1.267114
[epoch1, step2296]: loss 1.133074
[epoch1, step2297]: loss 1.410535
[epoch1, step2298]: loss 1.583264
[epoch1, step2299]: loss 1.142610
[epoch1, step2300]: loss 1.497701
[epoch1, step2301]: loss 1.524258
[epoch1, step2302]: loss 1.522236
[epoch1, step2303]: loss 1.513849
[epoch1, step2304]: loss 1.161091
[epoch1, step2305]: loss 1.208098
[epoch1, step2306]: loss 1.240807
[epoch1, step2307]: loss 1.762332
[epoch1, step2308]: loss 1.172435
[epoch1, step2309]: loss 1.168803
[epoch1, step2310]: loss 1.154442
[epoch1, step2311]: loss 1.164829
[epoch1, step2312]: loss 1.664787
[epoch1, step2313]: loss 1.431195
[epoch1, step2314]: loss 1.316861
[epoch1, step2315]: loss 1.055427
[epoch1, step2316]: loss 1.566912
[epoch1, step2317]: loss 1.058540
[epoch1, step2318]: loss 0.936458
[epoch1, step2319]: loss 1.245441
[epoch1, step2320]: loss 1.295167
[epoch1, step2321]: loss 1.434719
[epoch1, step2322]: loss 1.212381
[epoch1, step2323]: loss 1.482478
[epoch1, step2324]: loss 1.176330
[epoch1, step2325]: loss 1.202494
[epoch1, step2326]: loss 1.416419
[epoch1, step2327]: loss 1.377552
[epoch1, step2328]: loss 1.699371
[epoch1, step2329]: loss 1.359495
[epoch1, step2330]: loss 1.042317
[epoch1, step2331]: loss 1.295985
[epoch1, step2332]: loss 1.114955
[epoch1, step2333]: loss 1.194402
[epoch1, step2334]: loss 1.323941
[epoch1, step2335]: loss 1.467741
[epoch1, step2336]: loss 1.311064
[epoch1, step2337]: loss 1.356901
[epoch1, step2338]: loss 1.501783
[epoch1, step2339]: loss 1.149734
[epoch1, step2340]: loss 1.177173
[epoch1, step2341]: loss 1.106466
[epoch1, step2342]: loss 1.128312
[epoch1, step2343]: loss 1.503434
[epoch1, step2344]: loss 1.150409
[epoch1, step2345]: loss 1.385990
[epoch1, step2346]: loss 1.343569
[epoch1, step2347]: loss 1.260565
[epoch1, step2348]: loss 1.092652
[epoch1, step2349]: loss 1.188868
[epoch1, step2350]: loss 1.317257
[epoch1, step2351]: loss 1.080112
[epoch1, step2352]: loss 1.341652
[epoch1, step2353]: loss 1.637977
[epoch1, step2354]: loss 1.289139
[epoch1, step2355]: loss 1.470557
[epoch1, step2356]: loss 1.382839
[epoch1, step2357]: loss 1.469490
[epoch1, step2358]: loss 1.433174
[epoch1, step2359]: loss 1.569982
[epoch1, step2360]: loss 1.423183
[epoch1, step2361]: loss 1.303837
[epoch1, step2362]: loss 1.350642
[epoch1, step2363]: loss 1.047597
[epoch1, step2364]: loss 1.348419
[epoch1, step2365]: loss 1.077592
[epoch1, step2366]: loss 1.595321
[epoch1, step2367]: loss 1.269087
[epoch1, step2368]: loss 1.189484
[epoch1, step2369]: loss 0.957687
[epoch1, step2370]: loss 1.074292
[epoch1, step2371]: loss 1.113138
[epoch1, step2372]: loss 1.384735
[epoch1, step2373]: loss 1.430445
[epoch1, step2374]: loss 1.220017
[epoch1, step2375]: loss 1.255613
[epoch1, step2376]: loss 1.187422
[epoch1, step2377]: loss 1.143473
[epoch1, step2378]: loss 1.289637
[epoch1, step2379]: loss 1.495733
[epoch1, step2380]: loss 1.124212
[epoch1, step2381]: loss 1.343141
[epoch1, step2382]: loss 1.464150
[epoch1, step2383]: loss 1.094921
[epoch1, step2384]: loss 1.206067
[epoch1, step2385]: loss 1.190966
[epoch1, step2386]: loss 1.274466
[epoch1, step2387]: loss 1.482975
[epoch1, step2388]: loss 1.391185
[epoch1, step2389]: loss 1.462483
[epoch1, step2390]: loss 1.361151
[epoch1, step2391]: loss 1.009106
[epoch1, step2392]: loss 1.220919
[epoch1, step2393]: loss 1.061091
[epoch1, step2394]: loss 1.405169
[epoch1, step2395]: loss 0.882562
[epoch1, step2396]: loss 1.301202
[epoch1, step2397]: loss 1.108079
[epoch1, step2398]: loss 1.103394
[epoch1, step2399]: loss 1.362795
[epoch1, step2400]: loss 1.164865
[epoch1, step2401]: loss 1.492066
[epoch1, step2402]: loss 1.046473
[epoch1, step2403]: loss 1.154812
[epoch1, step2404]: loss 1.093820
[epoch1, step2405]: loss 1.306476
[epoch1, step2406]: loss 1.535596
[epoch1, step2407]: loss 0.955064
[epoch1, step2408]: loss 0.761143
[epoch1, step2409]: loss 1.138427
[epoch1, step2410]: loss 1.155166
[epoch1, step2411]: loss 1.171178
[epoch1, step2412]: loss 1.517540
[epoch1, step2413]: loss 1.196546
[epoch1, step2414]: loss 1.287194
[epoch1, step2415]: loss 1.180742
[epoch1, step2416]: loss 0.784025
[epoch1, step2417]: loss 1.121094
[epoch1, step2418]: loss 1.306432
[epoch1, step2419]: loss 1.091359
[epoch1, step2420]: loss 1.341676
[epoch1, step2421]: loss 1.220841
[epoch1, step2422]: loss 1.374723
[epoch1, step2423]: loss 1.427879
[epoch1, step2424]: loss 1.145858
[epoch1, step2425]: loss 1.398708
[epoch1, step2426]: loss 1.160183
[epoch1, step2427]: loss 1.413232
[epoch1, step2428]: loss 1.064828
[epoch1, step2429]: loss 1.648582
[epoch1, step2430]: loss 0.898494
[epoch1, step2431]: loss 1.265480
[epoch1, step2432]: loss 1.022768
[epoch1, step2433]: loss 1.465575
[epoch1, step2434]: loss 1.051279
[epoch1, step2435]: loss 1.296596
[epoch1, step2436]: loss 1.222481
[epoch1, step2437]: loss 1.252815
[epoch1, step2438]: loss 1.042572
[epoch1, step2439]: loss 1.323525
[epoch1, step2440]: loss 1.396285
[epoch1, step2441]: loss 1.198098
[epoch1, step2442]: loss 1.004488
[epoch1, step2443]: loss 1.189333
[epoch1, step2444]: loss 1.384182
[epoch1, step2445]: loss 1.157530
[epoch1, step2446]: loss 0.809773
[epoch1, step2447]: loss 1.329103
[epoch1, step2448]: loss 1.535708
[epoch1, step2449]: loss 1.241110
[epoch1, step2450]: loss 1.250177
[epoch1, step2451]: loss 1.238023
[epoch1, step2452]: loss 1.404161
[epoch1, step2453]: loss 1.395368
[epoch1, step2454]: loss 1.372802
[epoch1, step2455]: loss 1.222782
[epoch1, step2456]: loss 1.389629
[epoch1, step2457]: loss 1.468697
[epoch1, step2458]: loss 1.129104
[epoch1, step2459]: loss 1.184477
[epoch1, step2460]: loss 1.246367
[epoch1, step2461]: loss 1.249964
[epoch1, step2462]: loss 1.153814
[epoch1, step2463]: loss 1.144145
[epoch1, step2464]: loss 1.432025
[epoch1, step2465]: loss 1.296970
[epoch1, step2466]: loss 1.271686
[epoch1, step2467]: loss 1.235755
[epoch1, step2468]: loss 1.185403
[epoch1, step2469]: loss 1.173111
[epoch1, step2470]: loss 1.213270
[epoch1, step2471]: loss 1.393530
[epoch1, step2472]: loss 1.385234
[epoch1, step2473]: loss 1.099477
[epoch1, step2474]: loss 1.294253
[epoch1, step2475]: loss 1.169335
[epoch1, step2476]: loss 1.138552
[epoch1, step2477]: loss 1.356087
[epoch1, step2478]: loss 1.400714
[epoch1, step2479]: loss 1.245676
[epoch1, step2480]: loss 1.285255
[epoch1, step2481]: loss 1.137276
[epoch1, step2482]: loss 1.085907
[epoch1, step2483]: loss 1.111866
[epoch1, step2484]: loss 0.987394
[epoch1, step2485]: loss 1.189229
[epoch1, step2486]: loss 1.315236
[epoch1, step2487]: loss 1.155012
[epoch1, step2488]: loss 0.957334
[epoch1, step2489]: loss 1.394764
[epoch1, step2490]: loss 1.179387
[epoch1, step2491]: loss 1.200640
[epoch1, step2492]: loss 1.305205
[epoch1, step2493]: loss 1.224299
[epoch1, step2494]: loss 1.209519
[epoch1, step2495]: loss 1.370354
[epoch1, step2496]: loss 1.212999
[epoch1, step2497]: loss 1.420354
[epoch1, step2498]: loss 1.192896
[epoch1, step2499]: loss 1.195785
[epoch1, step2500]: loss 1.530529
[epoch1, step2501]: loss 0.810057
[epoch1, step2502]: loss 1.077643
[epoch1, step2503]: loss 1.122891
[epoch1, step2504]: loss 1.158385
[epoch1, step2505]: loss 1.280265
[epoch1, step2506]: loss 1.131376
[epoch1, step2507]: loss 1.112506
[epoch1, step2508]: loss 1.298494
[epoch1, step2509]: loss 0.963574
[epoch1, step2510]: loss 1.031641
[epoch1, step2511]: loss 1.376876
[epoch1, step2512]: loss 1.023787
[epoch1, step2513]: loss 1.028946
[epoch1, step2514]: loss 1.368703
[epoch1, step2515]: loss 1.116966
[epoch1, step2516]: loss 1.240644
[epoch1, step2517]: loss 1.388484
[epoch1, step2518]: loss 1.370177
[epoch1, step2519]: loss 1.227446
[epoch1, step2520]: loss 1.196026
[epoch1, step2521]: loss 1.279797
[epoch1, step2522]: loss 1.202585
[epoch1, step2523]: loss 1.246153
[epoch1, step2524]: loss 1.142401
[epoch1, step2525]: loss 1.179523
[epoch1, step2526]: loss 1.211803
[epoch1, step2527]: loss 1.155661
[epoch1, step2528]: loss 1.252223
[epoch1, step2529]: loss 1.413130
[epoch1, step2530]: loss 1.131956
[epoch1, step2531]: loss 1.286898
[epoch1, step2532]: loss 1.233677
[epoch1, step2533]: loss 1.569649
[epoch1, step2534]: loss 1.224187
[epoch1, step2535]: loss 1.103652
[epoch1, step2536]: loss 0.977150
[epoch1, step2537]: loss 1.075247
[epoch1, step2538]: loss 0.881671
[epoch1, step2539]: loss 1.009701
[epoch1, step2540]: loss 1.559781
[epoch1, step2541]: loss 1.223701
[epoch1, step2542]: loss 0.923466
[epoch1, step2543]: loss 0.987699
[epoch1, step2544]: loss 1.229389
[epoch1, step2545]: loss 0.888798
[epoch1, step2546]: loss 1.142811
[epoch1, step2547]: loss 1.366049
[epoch1, step2548]: loss 1.353920
[epoch1, step2549]: loss 1.322466
[epoch1, step2550]: loss 1.434372
[epoch1, step2551]: loss 1.451967
[epoch1, step2552]: loss 1.131855
[epoch1, step2553]: loss 1.199252
[epoch1, step2554]: loss 0.959060
[epoch1, step2555]: loss 0.788056
[epoch1, step2556]: loss 1.172156
[epoch1, step2557]: loss 1.483892
[epoch1, step2558]: loss 1.116499
[epoch1, step2559]: loss 1.165603
[epoch1, step2560]: loss 1.259227
[epoch1, step2561]: loss 0.936100
[epoch1, step2562]: loss 1.279781
[epoch1, step2563]: loss 1.290929
[epoch1, step2564]: loss 1.202463
[epoch1, step2565]: loss 0.988909
[epoch1, step2566]: loss 0.972199
[epoch1, step2567]: loss 0.820950
[epoch1, step2568]: loss 0.626705
[epoch1, step2569]: loss 1.483364
[epoch1, step2570]: loss 1.124714
[epoch1, step2571]: loss 0.981926
[epoch1, step2572]: loss 1.314681
[epoch1, step2573]: loss 1.162717
[epoch1, step2574]: loss 1.560855
[epoch1, step2575]: loss 1.204787
[epoch1, step2576]: loss 1.210347
[epoch1, step2577]: loss 1.323902
[epoch1, step2578]: loss 1.043405
[epoch1, step2579]: loss 1.339366
[epoch1, step2580]: loss 1.244197
[epoch1, step2581]: loss 0.758787
[epoch1, step2582]: loss 1.251884
[epoch1, step2583]: loss 1.327170
[epoch1, step2584]: loss 1.232768
[epoch1, step2585]: loss 1.400406
[epoch1, step2586]: loss 1.202528
[epoch1, step2587]: loss 1.353142
[epoch1, step2588]: loss 1.128390
[epoch1, step2589]: loss 1.402257
[epoch1, step2590]: loss 1.092840
[epoch1, step2591]: loss 1.153197
[epoch1, step2592]: loss 1.091186
[epoch1, step2593]: loss 0.883108
[epoch1, step2594]: loss 1.125023
[epoch1, step2595]: loss 0.773560
[epoch1, step2596]: loss 1.228115
[epoch1, step2597]: loss 1.171869
[epoch1, step2598]: loss 1.218185
[epoch1, step2599]: loss 1.426131
[epoch1, step2600]: loss 1.045198
[epoch1, step2601]: loss 0.840939
[epoch1, step2602]: loss 1.183030
[epoch1, step2603]: loss 0.953593
[epoch1, step2604]: loss 1.207163
[epoch1, step2605]: loss 1.249293
[epoch1, step2606]: loss 1.264337
[epoch1, step2607]: loss 1.210830
[epoch1, step2608]: loss 1.375715
[epoch1, step2609]: loss 1.167357
[epoch1, step2610]: loss 1.285653
[epoch1, step2611]: loss 1.249896
[epoch1, step2612]: loss 0.668789
[epoch1, step2613]: loss 1.444587
[epoch1, step2614]: loss 1.175633
[epoch1, step2615]: loss 0.952364
[epoch1, step2616]: loss 1.148378
[epoch1, step2617]: loss 0.985894
[epoch1, step2618]: loss 0.923877
[epoch1, step2619]: loss 1.488593
[epoch1, step2620]: loss 0.993710
[epoch1, step2621]: loss 1.116622
[epoch1, step2622]: loss 1.210735
[epoch1, step2623]: loss 1.293785
[epoch1, step2624]: loss 1.048546
[epoch1, step2625]: loss 1.389606
[epoch1, step2626]: loss 1.229145
[epoch1, step2627]: loss 1.147050
[epoch1, step2628]: loss 1.084600
[epoch1, step2629]: loss 1.088567
[epoch1, step2630]: loss 1.304816
[epoch1, step2631]: loss 1.188330
[epoch1, step2632]: loss 0.969355
[epoch1, step2633]: loss 1.343235
[epoch1, step2634]: loss 1.159043
[epoch1, step2635]: loss 1.235210
[epoch1, step2636]: loss 1.259316
[epoch1, step2637]: loss 0.814019
[epoch1, step2638]: loss 0.907045
[epoch1, step2639]: loss 1.287516
[epoch1, step2640]: loss 1.040121
[epoch1, step2641]: loss 1.051849
[epoch1, step2642]: loss 1.222005
[epoch1, step2643]: loss 1.249252
[epoch1, step2644]: loss 1.367806
[epoch1, step2645]: loss 0.826079
[epoch1, step2646]: loss 1.216788
[epoch1, step2647]: loss 0.838828
[epoch1, step2648]: loss 0.857470
[epoch1, step2649]: loss 1.229971
[epoch1, step2650]: loss 0.955977
[epoch1, step2651]: loss 1.204580
[epoch1, step2652]: loss 1.298059
[epoch1, step2653]: loss 1.152121
[epoch1, step2654]: loss 1.294614
[epoch1, step2655]: loss 1.094346
[epoch1, step2656]: loss 1.079257
[epoch1, step2657]: loss 0.855898
[epoch1, step2658]: loss 1.395960
[epoch1, step2659]: loss 1.196174
[epoch1, step2660]: loss 1.247875
[epoch1, step2661]: loss 0.909444
[epoch1, step2662]: loss 1.242722
[epoch1, step2663]: loss 0.977580
[epoch1, step2664]: loss 1.171066
[epoch1, step2665]: loss 1.140832
[epoch1, step2666]: loss 0.804721
[epoch1, step2667]: loss 1.107932
[epoch1, step2668]: loss 1.336178
[epoch1, step2669]: loss 0.610980
[epoch1, step2670]: loss 0.925937
[epoch1, step2671]: loss 1.245069
[epoch1, step2672]: loss 1.019884
[epoch1, step2673]: loss 1.417304
[epoch1, step2674]: loss 1.217485
[epoch1, step2675]: loss 1.207932
[epoch1, step2676]: loss 0.943456
[epoch1, step2677]: loss 1.005136
[epoch1, step2678]: loss 1.042420
[epoch1, step2679]: loss 1.032303
[epoch1, step2680]: loss 1.222143
[epoch1, step2681]: loss 1.192862
[epoch1, step2682]: loss 1.335453
[epoch1, step2683]: loss 1.271993
[epoch1, step2684]: loss 1.189524
[epoch1, step2685]: loss 0.898219
[epoch1, step2686]: loss 1.063663
[epoch1, step2687]: loss 1.095914
[epoch1, step2688]: loss 1.265942
[epoch1, step2689]: loss 0.861898
[epoch1, step2690]: loss 0.975218
[epoch1, step2691]: loss 1.252960
[epoch1, step2692]: loss 1.060440
[epoch1, step2693]: loss 1.028673
[epoch1, step2694]: loss 1.285525
[epoch1, step2695]: loss 1.166620
[epoch1, step2696]: loss 1.054451
[epoch1, step2697]: loss 0.988815
[epoch1, step2698]: loss 1.168916
[epoch1, step2699]: loss 1.028330
[epoch1, step2700]: loss 1.212810
[epoch1, step2701]: loss 1.134258
[epoch1, step2702]: loss 1.099316
[epoch1, step2703]: loss 1.336916
[epoch1, step2704]: loss 1.132786
[epoch1, step2705]: loss 1.310822
[epoch1, step2706]: loss 1.290766
[epoch1, step2707]: loss 1.065176
[epoch1, step2708]: loss 1.302196
[epoch1, step2709]: loss 1.318615
[epoch1, step2710]: loss 1.119404
[epoch1, step2711]: loss 0.998371
[epoch1, step2712]: loss 1.043400
[epoch1, step2713]: loss 1.380347
[epoch1, step2714]: loss 1.213061
[epoch1, step2715]: loss 1.261776
[epoch1, step2716]: loss 0.940221
[epoch1, step2717]: loss 0.935630
[epoch1, step2718]: loss 1.429677
[epoch1, step2719]: loss 1.265103
[epoch1, step2720]: loss 1.058621
[epoch1, step2721]: loss 1.108011
[epoch1, step2722]: loss 1.179793
[epoch1, step2723]: loss 1.296679
[epoch1, step2724]: loss 1.484933
[epoch1, step2725]: loss 1.158020
[epoch1, step2726]: loss 1.449182
[epoch1, step2727]: loss 0.894654
[epoch1, step2728]: loss 1.444832
[epoch1, step2729]: loss 1.091405
[epoch1, step2730]: loss 1.009574
[epoch1, step2731]: loss 1.322926
[epoch1, step2732]: loss 1.157552
[epoch1, step2733]: loss 1.297458
[epoch1, step2734]: loss 1.108242
[epoch1, step2735]: loss 1.437769
[epoch1, step2736]: loss 1.039500
[epoch1, step2737]: loss 1.275966
[epoch1, step2738]: loss 1.144165
[epoch1, step2739]: loss 1.104941
[epoch1, step2740]: loss 0.957151
[epoch1, step2741]: loss 1.029573
[epoch1, step2742]: loss 1.307825
[epoch1, step2743]: loss 1.267255
[epoch1, step2744]: loss 0.837604
[epoch1, step2745]: loss 1.051645
[epoch1, step2746]: loss 0.754811
[epoch1, step2747]: loss 1.337792
[epoch1, step2748]: loss 1.077203
[epoch1, step2749]: loss 1.047650
[epoch1, step2750]: loss 1.173408
[epoch1, step2751]: loss 1.051399
[epoch1, step2752]: loss 0.942203
[epoch1, step2753]: loss 1.052997
[epoch1, step2754]: loss 1.310600
[epoch1, step2755]: loss 1.153491
[epoch1, step2756]: loss 1.130294
[epoch1, step2757]: loss 1.445371
[epoch1, step2758]: loss 1.015834
[epoch1, step2759]: loss 1.082903
[epoch1, step2760]: loss 0.971680
[epoch1, step2761]: loss 0.882176
[epoch1, step2762]: loss 1.137209
[epoch1, step2763]: loss 1.067926
[epoch1, step2764]: loss 1.208463
[epoch1, step2765]: loss 1.352584
[epoch1, step2766]: loss 1.133350
[epoch1, step2767]: loss 1.268301
[epoch1, step2768]: loss 1.150869
[epoch1, step2769]: loss 1.102903
[epoch1, step2770]: loss 0.901079
[epoch1, step2771]: loss 1.128539
[epoch1, step2772]: loss 0.944026
[epoch1, step2773]: loss 1.212027
[epoch1, step2774]: loss 1.381952
[epoch1, step2775]: loss 1.113195
[epoch1, step2776]: loss 1.006738
[epoch1, step2777]: loss 1.334646
[epoch1, step2778]: loss 0.733979
[epoch1, step2779]: loss 0.811579
[epoch1, step2780]: loss 0.960938
[epoch1, step2781]: loss 1.162258
[epoch1, step2782]: loss 0.882594
[epoch1, step2783]: loss 1.331122
[epoch1, step2784]: loss 1.049997
[epoch1, step2785]: loss 1.195061
[epoch1, step2786]: loss 1.218046
[epoch1, step2787]: loss 1.104138
[epoch1, step2788]: loss 1.150600
[epoch1, step2789]: loss 1.025025
[epoch1, step2790]: loss 1.171377
[epoch1, step2791]: loss 0.960402
[epoch1, step2792]: loss 0.875033
[epoch1, step2793]: loss 1.120569
[epoch1, step2794]: loss 1.131200
[epoch1, step2795]: loss 1.037866
[epoch1, step2796]: loss 1.194927
[epoch1, step2797]: loss 1.322462
[epoch1, step2798]: loss 1.286388
[epoch1, step2799]: loss 0.938142
[epoch1, step2800]: loss 0.585263
[epoch1, step2801]: loss 1.190374
[epoch1, step2802]: loss 1.146709
[epoch1, step2803]: loss 1.188406
[epoch1, step2804]: loss 1.302202
[epoch1, step2805]: loss 1.277227
[epoch1, step2806]: loss 1.284677
[epoch1, step2807]: loss 1.270805
[epoch1, step2808]: loss 1.321002
[epoch1, step2809]: loss 1.211022
[epoch1, step2810]: loss 1.265480
[epoch1, step2811]: loss 1.088191
[epoch1, step2812]: loss 0.841885
[epoch1, step2813]: loss 0.981997
[epoch1, step2814]: loss 1.201156
[epoch1, step2815]: loss 0.854607
[epoch1, step2816]: loss 0.939415
[epoch1, step2817]: loss 1.254796
[epoch1, step2818]: loss 1.352952
[epoch1, step2819]: loss 1.265071
[epoch1, step2820]: loss 1.015890
[epoch1, step2821]: loss 1.365832
[epoch1, step2822]: loss 1.193623
[epoch1, step2823]: loss 1.268940
[epoch1, step2824]: loss 1.236845
[epoch1, step2825]: loss 1.239126
[epoch1, step2826]: loss 0.859723
[epoch1, step2827]: loss 1.149695
[epoch1, step2828]: loss 1.336334
[epoch1, step2829]: loss 1.088393
[epoch1, step2830]: loss 1.220239
[epoch1, step2831]: loss 1.112695
[epoch1, step2832]: loss 1.317507
[epoch1, step2833]: loss 1.259657
[epoch1, step2834]: loss 1.296531
[epoch1, step2835]: loss 0.976910
[epoch1, step2836]: loss 1.230415
[epoch1, step2837]: loss 1.150027
[epoch1, step2838]: loss 1.009471
[epoch1, step2839]: loss 1.136020
[epoch1, step2840]: loss 0.455524
[epoch1, step2841]: loss 0.841048
[epoch1, step2842]: loss 1.017991
[epoch1, step2843]: loss 1.370760
[epoch1, step2844]: loss 1.361587
[epoch1, step2845]: loss 1.241062
[epoch1, step2846]: loss 0.881427
[epoch1, step2847]: loss 1.083615
[epoch1, step2848]: loss 1.305599
[epoch1, step2849]: loss 1.052124
[epoch1, step2850]: loss 1.147349
[epoch1, step2851]: loss 1.434945
[epoch1, step2852]: loss 1.404033
[epoch1, step2853]: loss 1.358365
[epoch1, step2854]: loss 1.106020
[epoch1, step2855]: loss 0.963613
[epoch1, step2856]: loss 1.234064
[epoch1, step2857]: loss 1.345597
[epoch1, step2858]: loss 1.110057
[epoch1, step2859]: loss 1.390122
[epoch1, step2860]: loss 1.149732
[epoch1, step2861]: loss 1.045671
[epoch1, step2862]: loss 1.304381
[epoch1, step2863]: loss 0.988780
[epoch1, step2864]: loss 1.051483
[epoch1, step2865]: loss 0.823356
[epoch1, step2866]: loss 1.310318
[epoch1, step2867]: loss 1.295568
[epoch1, step2868]: loss 1.068676
[epoch1, step2869]: loss 1.029049
[epoch1, step2870]: loss 1.056988
[epoch1, step2871]: loss 1.283284
[epoch1, step2872]: loss 1.297237
[epoch1, step2873]: loss 0.852084
[epoch1, step2874]: loss 1.244660
[epoch1, step2875]: loss 1.041464
[epoch1, step2876]: loss 1.316237
[epoch1, step2877]: loss 1.038071
[epoch1, step2878]: loss 1.170738
[epoch1, step2879]: loss 1.251064
[epoch1, step2880]: loss 1.193465
[epoch1, step2881]: loss 0.945686
[epoch1, step2882]: loss 1.278368
[epoch1, step2883]: loss 1.176949
[epoch1, step2884]: loss 1.208614
[epoch1, step2885]: loss 1.114809
[epoch1, step2886]: loss 0.923500
[epoch1, step2887]: loss 0.983484
[epoch1, step2888]: loss 1.208918
[epoch1, step2889]: loss 0.842642
[epoch1, step2890]: loss 1.118971
[epoch1, step2891]: loss 0.957507
[epoch1, step2892]: loss 1.355823
[epoch1, step2893]: loss 1.386857
[epoch1, step2894]: loss 1.027377
[epoch1, step2895]: loss 0.886405
[epoch1, step2896]: loss 1.135873
[epoch1, step2897]: loss 0.777044
[epoch1, step2898]: loss 0.851296
[epoch1, step2899]: loss 1.009822
[epoch1, step2900]: loss 1.267412
[epoch1, step2901]: loss 1.430366
[epoch1, step2902]: loss 1.199699
[epoch1, step2903]: loss 0.997451
[epoch1, step2904]: loss 0.977251
[epoch1, step2905]: loss 1.087477
[epoch1, step2906]: loss 0.689500
[epoch1, step2907]: loss 1.248056
[epoch1, step2908]: loss 1.030198
[epoch1, step2909]: loss 1.453661
[epoch1, step2910]: loss 0.954395
[epoch1, step2911]: loss 1.231384
[epoch1, step2912]: loss 0.966973
[epoch1, step2913]: loss 1.020369
[epoch1, step2914]: loss 0.883921
[epoch1, step2915]: loss 0.906007
[epoch1, step2916]: loss 0.826593
[epoch1, step2917]: loss 1.175593
[epoch1, step2918]: loss 1.057981
[epoch1, step2919]: loss 1.365298
[epoch1, step2920]: loss 1.163292
[epoch1, step2921]: loss 0.772236
[epoch1, step2922]: loss 1.358242
[epoch1, step2923]: loss 0.930977
[epoch1, step2924]: loss 0.976469
[epoch1, step2925]: loss 1.242392
[epoch1, step2926]: loss 1.136269
[epoch1, step2927]: loss 1.050715
[epoch1, step2928]: loss 1.171167
[epoch1, step2929]: loss 1.116911
[epoch1, step2930]: loss 1.254501
[epoch1, step2931]: loss 1.146663
[epoch1, step2932]: loss 0.806449
[epoch1, step2933]: loss 0.931319
[epoch1, step2934]: loss 0.983544
[epoch1, step2935]: loss 1.199145
[epoch1, step2936]: loss 1.239474
[epoch1, step2937]: loss 0.985439
[epoch1, step2938]: loss 1.153768
[epoch1, step2939]: loss 0.819526
[epoch1, step2940]: loss 0.928457
[epoch1, step2941]: loss 1.181178
[epoch1, step2942]: loss 1.132412
[epoch1, step2943]: loss 1.005678
[epoch1, step2944]: loss 1.079525
[epoch1, step2945]: loss 1.161992
[epoch1, step2946]: loss 1.246938
[epoch1, step2947]: loss 1.147819
[epoch1, step2948]: loss 1.139443
[epoch1, step2949]: loss 1.363320
[epoch1, step2950]: loss 0.851764
[epoch1, step2951]: loss 0.924273
[epoch1, step2952]: loss 1.356605
[epoch1, step2953]: loss 0.895606
[epoch1, step2954]: loss 1.127064
[epoch1, step2955]: loss 1.366395
[epoch1, step2956]: loss 1.169279
[epoch1, step2957]: loss 1.180446
[epoch1, step2958]: loss 1.087199
[epoch1, step2959]: loss 0.979681
[epoch1, step2960]: loss 1.079001
[epoch1, step2961]: loss 0.974448
[epoch1, step2962]: loss 1.194810
[epoch1, step2963]: loss 1.276132
[epoch1, step2964]: loss 1.132166
[epoch1, step2965]: loss 1.130394
[epoch1, step2966]: loss 1.138723
[epoch1, step2967]: loss 0.949367
[epoch1, step2968]: loss 0.854624
[epoch1, step2969]: loss 1.149172
[epoch1, step2970]: loss 1.084338
[epoch1, step2971]: loss 1.056852
[epoch1, step2972]: loss 1.169923
[epoch1, step2973]: loss 1.097170
[epoch1, step2974]: loss 1.080009
[epoch1, step2975]: loss 0.701252
[epoch1, step2976]: loss 1.126890
[epoch1, step2977]: loss 1.176059
[epoch1, step2978]: loss 1.324452
[epoch1, step2979]: loss 1.051058
[epoch1, step2980]: loss 1.171571
[epoch1, step2981]: loss 0.461691
[epoch1, step2982]: loss 1.195624
[epoch1, step2983]: loss 0.947942
[epoch1, step2984]: loss 1.282795
[epoch1, step2985]: loss 1.265190
[epoch1, step2986]: loss 1.008567
[epoch1, step2987]: loss 1.292999
[epoch1, step2988]: loss 0.728571
[epoch1, step2989]: loss 1.172149
[epoch1, step2990]: loss 1.088995
[epoch1, step2991]: loss 0.917452
[epoch1, step2992]: loss 1.022737
[epoch1, step2993]: loss 0.896270
[epoch1, step2994]: loss 0.933650
[epoch1, step2995]: loss 1.085888
[epoch1, step2996]: loss 1.010847
[epoch1, step2997]: loss 1.063010
[epoch1, step2998]: loss 1.334055
[epoch1, step2999]: loss 1.262066
[epoch1, step3000]: loss 1.348127
[epoch1, step3001]: loss 1.148173
[epoch1, step3002]: loss 1.312467
[epoch1, step3003]: loss 1.004262
[epoch1, step3004]: loss 1.005651
[epoch1, step3005]: loss 1.146046
[epoch1, step3006]: loss 1.253393
[epoch1, step3007]: loss 0.986477
[epoch1, step3008]: loss 1.118984
[epoch1, step3009]: loss 1.026023
[epoch1, step3010]: loss 1.029504
[epoch1, step3011]: loss 1.151845
[epoch1, step3012]: loss 1.224241
[epoch1, step3013]: loss 0.879113
[epoch1, step3014]: loss 1.065168
[epoch1, step3015]: loss 1.061723
[epoch1, step3016]: loss 1.045471
[epoch1, step3017]: loss 0.701496
[epoch1, step3018]: loss 1.324238
[epoch1, step3019]: loss 1.243061
[epoch1, step3020]: loss 0.797423
[epoch1, step3021]: loss 0.995350
[epoch1, step3022]: loss 0.927943
[epoch1, step3023]: loss 0.908622
[epoch1, step3024]: loss 1.446766
[epoch1, step3025]: loss 1.222064
[epoch1, step3026]: loss 1.020402
[epoch1, step3027]: loss 0.929151
[epoch1, step3028]: loss 1.179081
[epoch1, step3029]: loss 0.756054
[epoch1, step3030]: loss 1.057970
[epoch1, step3031]: loss 0.973642
[epoch1, step3032]: loss 1.136496
[epoch1, step3033]: loss 1.237573
[epoch1, step3034]: loss 0.914112
[epoch1, step3035]: loss 1.141827
[epoch1, step3036]: loss 0.725888
[epoch1, step3037]: loss 1.172881
[epoch1, step3038]: loss 1.113059
[epoch1, step3039]: loss 1.114552
[epoch1, step3040]: loss 1.227033
[epoch1, step3041]: loss 0.956476
[epoch1, step3042]: loss 0.902053
[epoch1, step3043]: loss 1.002234
[epoch1, step3044]: loss 0.962804
[epoch1, step3045]: loss 1.039313
[epoch1, step3046]: loss 1.195156
[epoch1, step3047]: loss 1.007516
[epoch1, step3048]: loss 1.163738
[epoch1, step3049]: loss 1.078332
[epoch1, step3050]: loss 1.153823
[epoch1, step3051]: loss 1.211748
[epoch1, step3052]: loss 0.943255
[epoch1, step3053]: loss 0.890604
[epoch1, step3054]: loss 1.041597
[epoch1, step3055]: loss 1.114640
[epoch1, step3056]: loss 0.952536
[epoch1, step3057]: loss 1.038334
[epoch1, step3058]: loss 1.000761
[epoch1, step3059]: loss 0.734237
[epoch1, step3060]: loss 1.236655
[epoch1, step3061]: loss 0.692747
[epoch1, step3062]: loss 0.747062
[epoch1, step3063]: loss 1.110464
[epoch1, step3064]: loss 1.136543
[epoch1, step3065]: loss 1.131467
[epoch1, step3066]: loss 1.237701
[epoch1, step3067]: loss 1.338958
[epoch1, step3068]: loss 1.069073
[epoch1, step3069]: loss 1.085575
[epoch1, step3070]: loss 0.755973
[epoch1, step3071]: loss 0.917589
[epoch1, step3072]: loss 1.113927
[epoch1, step3073]: loss 1.097242
[epoch1, step3074]: loss 1.039827
[epoch1, step3075]: loss 1.087836
[epoch1, step3076]: loss 1.078588

[epoch1]: avg loss 1.078588

[epoch2, step1]: loss 1.028166
[epoch2, step2]: loss 1.163491
[epoch2, step3]: loss 1.011011
[epoch2, step4]: loss 1.146613
[epoch2, step5]: loss 0.981188
[epoch2, step6]: loss 1.205182
[epoch2, step7]: loss 0.618539
[epoch2, step8]: loss 1.339220
[epoch2, step9]: loss 1.290463
[epoch2, step10]: loss 0.901893
[epoch2, step11]: loss 1.123721
[epoch2, step12]: loss 1.007560
[epoch2, step13]: loss 1.060813
[epoch2, step14]: loss 0.830101
[epoch2, step15]: loss 0.961048
[epoch2, step16]: loss 1.150879
[epoch2, step17]: loss 1.150962
[epoch2, step18]: loss 0.982066
[epoch2, step19]: loss 1.103596
[epoch2, step20]: loss 1.089854
[epoch2, step21]: loss 1.053079
[epoch2, step22]: loss 0.903195
[epoch2, step23]: loss 1.187790
[epoch2, step24]: loss 1.247646
[epoch2, step25]: loss 0.928207
[epoch2, step26]: loss 0.973168
[epoch2, step27]: loss 0.970103
[epoch2, step28]: loss 0.856289
[epoch2, step29]: loss 1.481516
[epoch2, step30]: loss 1.170030
[epoch2, step31]: loss 1.011141
[epoch2, step32]: loss 1.111445
[epoch2, step33]: loss 0.997624
[epoch2, step34]: loss 1.299492
[epoch2, step35]: loss 1.077476
[epoch2, step36]: loss 1.118227
[epoch2, step37]: loss 0.765596
[epoch2, step38]: loss 0.875799
[epoch2, step39]: loss 0.967829
[epoch2, step40]: loss 1.167314
[epoch2, step41]: loss 0.803129
[epoch2, step42]: loss 1.147006
[epoch2, step43]: loss 0.992491
[epoch2, step44]: loss 0.781505
[epoch2, step45]: loss 1.286561
[epoch2, step46]: loss 1.217558
[epoch2, step47]: loss 1.214586
[epoch2, step48]: loss 0.657850
[epoch2, step49]: loss 0.904394
[epoch2, step50]: loss 1.238895
[epoch2, step51]: loss 1.044588
[epoch2, step52]: loss 1.302678
[epoch2, step53]: loss 1.163198
[epoch2, step54]: loss 0.901732
[epoch2, step55]: loss 0.820682
[epoch2, step56]: loss 1.011319
[epoch2, step57]: loss 1.240265
[epoch2, step58]: loss 1.204744
[epoch2, step59]: loss 1.110051
[epoch2, step60]: loss 1.256733
[epoch2, step61]: loss 1.026470
[epoch2, step62]: loss 0.989808
[epoch2, step63]: loss 1.140572
[epoch2, step64]: loss 1.022787
[epoch2, step65]: loss 1.144227
[epoch2, step66]: loss 1.142914
[epoch2, step67]: loss 1.016851
[epoch2, step68]: loss 1.082985
[epoch2, step69]: loss 0.912921
[epoch2, step70]: loss 1.078086
[epoch2, step71]: loss 0.819072
[epoch2, step72]: loss 1.132580
[epoch2, step73]: loss 1.261511
[epoch2, step74]: loss 0.933512
[epoch2, step75]: loss 0.961500
[epoch2, step76]: loss 1.167017
[epoch2, step77]: loss 0.817547
[epoch2, step78]: loss 0.903944
[epoch2, step79]: loss 0.869140
[epoch2, step80]: loss 1.029806
[epoch2, step81]: loss 1.045139
[epoch2, step82]: loss 1.074262
[epoch2, step83]: loss 1.138287
[epoch2, step84]: loss 1.152491
[epoch2, step85]: loss 1.129859
[epoch2, step86]: loss 1.231211
[epoch2, step87]: loss 1.230887
[epoch2, step88]: loss 1.012509
[epoch2, step89]: loss 0.846570
[epoch2, step90]: loss 1.097558
[epoch2, step91]: loss 0.874953
[epoch2, step92]: loss 1.190503
[epoch2, step93]: loss 1.084701
[epoch2, step94]: loss 1.068201
[epoch2, step95]: loss 0.994986
[epoch2, step96]: loss 1.057545
[epoch2, step97]: loss 1.027082
[epoch2, step98]: loss 1.170813
[epoch2, step99]: loss 1.133825
[epoch2, step100]: loss 1.015833
[epoch2, step101]: loss 0.954178
[epoch2, step102]: loss 1.130197
[epoch2, step103]: loss 1.060032
[epoch2, step104]: loss 0.851245
[epoch2, step105]: loss 1.142688
[epoch2, step106]: loss 1.065488
[epoch2, step107]: loss 0.846426
[epoch2, step108]: loss 1.369360
[epoch2, step109]: loss 0.943645
[epoch2, step110]: loss 0.841871
[epoch2, step111]: loss 1.143650
[epoch2, step112]: loss 0.809867
[epoch2, step113]: loss 1.064903
[epoch2, step114]: loss 1.002294
[epoch2, step115]: loss 1.128872
[epoch2, step116]: loss 1.028554
[epoch2, step117]: loss 1.131225
[epoch2, step118]: loss 1.044702
[epoch2, step119]: loss 0.646810
[epoch2, step120]: loss 1.173868
[epoch2, step121]: loss 1.226600
[epoch2, step122]: loss 1.036361
[epoch2, step123]: loss 0.571386
[epoch2, step124]: loss 0.780627
[epoch2, step125]: loss 0.902242
[epoch2, step126]: loss 1.198203
[epoch2, step127]: loss 0.993437
[epoch2, step128]: loss 0.817868
[epoch2, step129]: loss 1.043437
[epoch2, step130]: loss 1.110932
[epoch2, step131]: loss 0.915321
[epoch2, step132]: loss 0.896596
[epoch2, step133]: loss 1.070803
[epoch2, step134]: loss 1.135006
[epoch2, step135]: loss 1.057474
[epoch2, step136]: loss 1.016621
[epoch2, step137]: loss 0.903935
[epoch2, step138]: loss 0.936266
[epoch2, step139]: loss 0.919287
[epoch2, step140]: loss 0.838756
[epoch2, step141]: loss 1.138664
[epoch2, step142]: loss 1.050570
[epoch2, step143]: loss 1.173756
[epoch2, step144]: loss 0.946909
[epoch2, step145]: loss 0.995093
[epoch2, step146]: loss 1.034695
[epoch2, step147]: loss 0.945760
[epoch2, step148]: loss 1.298338
[epoch2, step149]: loss 0.698914
[epoch2, step150]: loss 0.898520
[epoch2, step151]: loss 1.092678
[epoch2, step152]: loss 0.883828
[epoch2, step153]: loss 0.953156
[epoch2, step154]: loss 1.109710
[epoch2, step155]: loss 1.478837
[epoch2, step156]: loss 0.958126
[epoch2, step157]: loss 1.066710
[epoch2, step158]: loss 1.072351
[epoch2, step159]: loss 1.038545
[epoch2, step160]: loss 0.586754
[epoch2, step161]: loss 0.816272
[epoch2, step162]: loss 0.962788
[epoch2, step163]: loss 1.142426
[epoch2, step164]: loss 0.948394
[epoch2, step165]: loss 1.083731
[epoch2, step166]: loss 1.096366
[epoch2, step167]: loss 1.056485
[epoch2, step168]: loss 1.230246
[epoch2, step169]: loss 0.932749
[epoch2, step170]: loss 0.893865
[epoch2, step171]: loss 1.039058
[epoch2, step172]: loss 1.075091
[epoch2, step173]: loss 1.160957
[epoch2, step174]: loss 1.132149
[epoch2, step175]: loss 1.156455
[epoch2, step176]: loss 0.629318
[epoch2, step177]: loss 1.072070
[epoch2, step178]: loss 1.011963
[epoch2, step179]: loss 1.250944
[epoch2, step180]: loss 1.318682
[epoch2, step181]: loss 1.235179
[epoch2, step182]: loss 1.198211
[epoch2, step183]: loss 1.096150
[epoch2, step184]: loss 0.966602
[epoch2, step185]: loss 0.908052
[epoch2, step186]: loss 1.349757
[epoch2, step187]: loss 0.736829
[epoch2, step188]: loss 1.004415
[epoch2, step189]: loss 1.211314
[epoch2, step190]: loss 1.236511
[epoch2, step191]: loss 0.931954
[epoch2, step192]: loss 0.841429
[epoch2, step193]: loss 0.761599
[epoch2, step194]: loss 1.063882
[epoch2, step195]: loss 1.051316
[epoch2, step196]: loss 0.814218
[epoch2, step197]: loss 0.998116
[epoch2, step198]: loss 1.128150
[epoch2, step199]: loss 1.028599
[epoch2, step200]: loss 1.142734
[epoch2, step201]: loss 0.821229
[epoch2, step202]: loss 1.000296
[epoch2, step203]: loss 1.189185
[epoch2, step204]: loss 1.228333
[epoch2, step205]: loss 0.862527
[epoch2, step206]: loss 1.231491
[epoch2, step207]: loss 1.057924
[epoch2, step208]: loss 1.043829
[epoch2, step209]: loss 1.003335
[epoch2, step210]: loss 1.412096
[epoch2, step211]: loss 1.199069
[epoch2, step212]: loss 0.989141
[epoch2, step213]: loss 0.841462
[epoch2, step214]: loss 1.164116
[epoch2, step215]: loss 0.841209
[epoch2, step216]: loss 0.746010
[epoch2, step217]: loss 1.067945
[epoch2, step218]: loss 0.715128
[epoch2, step219]: loss 1.177561
[epoch2, step220]: loss 1.072674
[epoch2, step221]: loss 0.898837
[epoch2, step222]: loss 1.154485
[epoch2, step223]: loss 1.086865
[epoch2, step224]: loss 0.935393
[epoch2, step225]: loss 0.970746
[epoch2, step226]: loss 0.841296
[epoch2, step227]: loss 0.545260
[epoch2, step228]: loss 1.330058
[epoch2, step229]: loss 0.878686
[epoch2, step230]: loss 0.947610
[epoch2, step231]: loss 1.122888
[epoch2, step232]: loss 0.955676
[epoch2, step233]: loss 1.003910
[epoch2, step234]: loss 1.061592
[epoch2, step235]: loss 1.025601
[epoch2, step236]: loss 0.984615
[epoch2, step237]: loss 0.998057
[epoch2, step238]: loss 0.993063
[epoch2, step239]: loss 1.260454
[epoch2, step240]: loss 0.935892
[epoch2, step241]: loss 0.756788
[epoch2, step242]: loss 1.038602
[epoch2, step243]: loss 1.218169
[epoch2, step244]: loss 0.868916
[epoch2, step245]: loss 0.959247
[epoch2, step246]: loss 0.787116
[epoch2, step247]: loss 1.277951
[epoch2, step248]: loss 0.688911
[epoch2, step249]: loss 1.129993
[epoch2, step250]: loss 1.221409
[epoch2, step251]: loss 0.969843
[epoch2, step252]: loss 1.192879
[epoch2, step253]: loss 1.019230
[epoch2, step254]: loss 0.891631
[epoch2, step255]: loss 1.096391
[epoch2, step256]: loss 0.989600
[epoch2, step257]: loss 0.759344
[epoch2, step258]: loss 0.987006
[epoch2, step259]: loss 1.093984
[epoch2, step260]: loss 1.042686
[epoch2, step261]: loss 1.018672
[epoch2, step262]: loss 1.168844
[epoch2, step263]: loss 1.156703
[epoch2, step264]: loss 0.736470
[epoch2, step265]: loss 1.215215
[epoch2, step266]: loss 1.027941
[epoch2, step267]: loss 1.110219
[epoch2, step268]: loss 1.095828
[epoch2, step269]: loss 0.929242
[epoch2, step270]: loss 1.110552
[epoch2, step271]: loss 0.905110
[epoch2, step272]: loss 0.944121
[epoch2, step273]: loss 1.104585
[epoch2, step274]: loss 1.060514
[epoch2, step275]: loss 0.847682
[epoch2, step276]: loss 0.772519
[epoch2, step277]: loss 0.702441
[epoch2, step278]: loss 1.002925
[epoch2, step279]: loss 1.156784
[epoch2, step280]: loss 0.895779
[epoch2, step281]: loss 1.191437
[epoch2, step282]: loss 0.827000
[epoch2, step283]: loss 0.761190
[epoch2, step284]: loss 0.987099
[epoch2, step285]: loss 1.013837
[epoch2, step286]: loss 1.112997
[epoch2, step287]: loss 0.991905
[epoch2, step288]: loss 0.616761
[epoch2, step289]: loss 0.795497
[epoch2, step290]: loss 0.942345
[epoch2, step291]: loss 1.303163
[epoch2, step292]: loss 1.196274
[epoch2, step293]: loss 0.850241
[epoch2, step294]: loss 1.028709
[epoch2, step295]: loss 1.006642
[epoch2, step296]: loss 0.840117
[epoch2, step297]: loss 0.859257
[epoch2, step298]: loss 0.885650
[epoch2, step299]: loss 1.066306
[epoch2, step300]: loss 1.143745
[epoch2, step301]: loss 1.160946
[epoch2, step302]: loss 1.134013
[epoch2, step303]: loss 0.751822
[epoch2, step304]: loss 0.827808
[epoch2, step305]: loss 1.056883
[epoch2, step306]: loss 1.093819
[epoch2, step307]: loss 1.168356
[epoch2, step308]: loss 0.710730
[epoch2, step309]: loss 0.981001
[epoch2, step310]: loss 0.767848
[epoch2, step311]: loss 1.182838
[epoch2, step312]: loss 0.754634
[epoch2, step313]: loss 1.282022
[epoch2, step314]: loss 1.259321
[epoch2, step315]: loss 1.046023
[epoch2, step316]: loss 0.847203
[epoch2, step317]: loss 0.714219
[epoch2, step318]: loss 0.885653
[epoch2, step319]: loss 0.776675
[epoch2, step320]: loss 1.046094
[epoch2, step321]: loss 0.997707
[epoch2, step322]: loss 1.170465
[epoch2, step323]: loss 0.896647
[epoch2, step324]: loss 1.076933
[epoch2, step325]: loss 1.050806
[epoch2, step326]: loss 0.839437
[epoch2, step327]: loss 0.684857
[epoch2, step328]: loss 0.933910
[epoch2, step329]: loss 1.171256
[epoch2, step330]: loss 0.856374
[epoch2, step331]: loss 0.951337
[epoch2, step332]: loss 0.763348
[epoch2, step333]: loss 0.762290
[epoch2, step334]: loss 0.985679
[epoch2, step335]: loss 0.970780
[epoch2, step336]: loss 1.079908
[epoch2, step337]: loss 0.930765
[epoch2, step338]: loss 1.276778
[epoch2, step339]: loss 1.013531
[epoch2, step340]: loss 0.845022
[epoch2, step341]: loss 1.204625
[epoch2, step342]: loss 0.751053
[epoch2, step343]: loss 1.184177
[epoch2, step344]: loss 0.896027
[epoch2, step345]: loss 0.991317
[epoch2, step346]: loss 1.249866
[epoch2, step347]: loss 1.089070
[epoch2, step348]: loss 1.046192
[epoch2, step349]: loss 1.090403
[epoch2, step350]: loss 1.052878
[epoch2, step351]: loss 0.739626
[epoch2, step352]: loss 0.976183
[epoch2, step353]: loss 0.815486
[epoch2, step354]: loss 1.045902
[epoch2, step355]: loss 0.843814
[epoch2, step356]: loss 1.211363
[epoch2, step357]: loss 0.948246
[epoch2, step358]: loss 1.057616
[epoch2, step359]: loss 1.072609
[epoch2, step360]: loss 0.715386
[epoch2, step361]: loss 1.037337
[epoch2, step362]: loss 0.999521
[epoch2, step363]: loss 1.155500
[epoch2, step364]: loss 0.776547
[epoch2, step365]: loss 1.015918
[epoch2, step366]: loss 0.864163
[epoch2, step367]: loss 1.076420
[epoch2, step368]: loss 0.879827
[epoch2, step369]: loss 0.767633
[epoch2, step370]: loss 0.755625
[epoch2, step371]: loss 0.908374
[epoch2, step372]: loss 0.876602
[epoch2, step373]: loss 1.078503
[epoch2, step374]: loss 1.247652
[epoch2, step375]: loss 0.778795
[epoch2, step376]: loss 0.963962
[epoch2, step377]: loss 1.017032
[epoch2, step378]: loss 1.111266
[epoch2, step379]: loss 0.708686
[epoch2, step380]: loss 1.000694
[epoch2, step381]: loss 0.875255
[epoch2, step382]: loss 0.729336
[epoch2, step383]: loss 1.110142
[epoch2, step384]: loss 1.060003
[epoch2, step385]: loss 1.032278
[epoch2, step386]: loss 0.568347
[epoch2, step387]: loss 0.995906
[epoch2, step388]: loss 0.973645
[epoch2, step389]: loss 1.200467
[epoch2, step390]: loss 0.874593
[epoch2, step391]: loss 1.064139
[epoch2, step392]: loss 1.019769
[epoch2, step393]: loss 1.140956
[epoch2, step394]: loss 0.923322
[epoch2, step395]: loss 1.157945
[epoch2, step396]: loss 1.207754
[epoch2, step397]: loss 0.788560
[epoch2, step398]: loss 0.928920
[epoch2, step399]: loss 1.193696
[epoch2, step400]: loss 0.812812
[epoch2, step401]: loss 1.006029
[epoch2, step402]: loss 0.632342
[epoch2, step403]: loss 0.539043
[epoch2, step404]: loss 1.041061
[epoch2, step405]: loss 1.001357
[epoch2, step406]: loss 1.002205
[epoch2, step407]: loss 1.121740
[epoch2, step408]: loss 1.024788
[epoch2, step409]: loss 0.979998
[epoch2, step410]: loss 0.563867
[epoch2, step411]: loss 0.983686
[epoch2, step412]: loss 0.880499
[epoch2, step413]: loss 0.992089
[epoch2, step414]: loss 0.876991
[epoch2, step415]: loss 0.848486
[epoch2, step416]: loss 1.013955
[epoch2, step417]: loss 0.734388
[epoch2, step418]: loss 1.337689
[epoch2, step419]: loss 0.866024
[epoch2, step420]: loss 0.902094
[epoch2, step421]: loss 0.769956
[epoch2, step422]: loss 0.785327
[epoch2, step423]: loss 1.074883
[epoch2, step424]: loss 0.899816
[epoch2, step425]: loss 1.281059
[epoch2, step426]: loss 0.998620
[epoch2, step427]: loss 1.044345
[epoch2, step428]: loss 0.981143
[epoch2, step429]: loss 0.738945
[epoch2, step430]: loss 1.010563
[epoch2, step431]: loss 0.880026
[epoch2, step432]: loss 1.028809
[epoch2, step433]: loss 0.915161
[epoch2, step434]: loss 1.022480
[epoch2, step435]: loss 0.872564
[epoch2, step436]: loss 1.098578
[epoch2, step437]: loss 1.209972
[epoch2, step438]: loss 1.116713
[epoch2, step439]: loss 1.135685
[epoch2, step440]: loss 0.941116
[epoch2, step441]: loss 0.923951
[epoch2, step442]: loss 0.887165
[epoch2, step443]: loss 1.055591
[epoch2, step444]: loss 0.829079
[epoch2, step445]: loss 1.174704
[epoch2, step446]: loss 0.884199
[epoch2, step447]: loss 1.131705
[epoch2, step448]: loss 0.819604
[epoch2, step449]: loss 0.961729
[epoch2, step450]: loss 0.790955
[epoch2, step451]: loss 1.029740
[epoch2, step452]: loss 0.961571
[epoch2, step453]: loss 0.952419
[epoch2, step454]: loss 0.994401
[epoch2, step455]: loss 1.028015
[epoch2, step456]: loss 1.048211
[epoch2, step457]: loss 0.464047
[epoch2, step458]: loss 0.749785
[epoch2, step459]: loss 1.195977
[epoch2, step460]: loss 0.916468
[epoch2, step461]: loss 0.990035
[epoch2, step462]: loss 1.182321
[epoch2, step463]: loss 1.134882
[epoch2, step464]: loss 1.116588
[epoch2, step465]: loss 1.331832
[epoch2, step466]: loss 0.964532
[epoch2, step467]: loss 0.780365
[epoch2, step468]: loss 0.816222
[epoch2, step469]: loss 0.856945
[epoch2, step470]: loss 1.099767
[epoch2, step471]: loss 0.845778
[epoch2, step472]: loss 1.076519
[epoch2, step473]: loss 1.176684
[epoch2, step474]: loss 1.175080
[epoch2, step475]: loss 1.108261
[epoch2, step476]: loss 0.997720
[epoch2, step477]: loss 0.847628
[epoch2, step478]: loss 0.872231
[epoch2, step479]: loss 0.769713
[epoch2, step480]: loss 0.781316
[epoch2, step481]: loss 1.102087
[epoch2, step482]: loss 0.909624
[epoch2, step483]: loss 0.955586
[epoch2, step484]: loss 0.944081
[epoch2, step485]: loss 0.743006
[epoch2, step486]: loss 1.061182
[epoch2, step487]: loss 0.734295
[epoch2, step488]: loss 0.998575
[epoch2, step489]: loss 1.037366
[epoch2, step490]: loss 0.754920
[epoch2, step491]: loss 0.928370
[epoch2, step492]: loss 1.087681
[epoch2, step493]: loss 0.837175
[epoch2, step494]: loss 1.116468
[epoch2, step495]: loss 1.029386
[epoch2, step496]: loss 0.899052
[epoch2, step497]: loss 0.934721
[epoch2, step498]: loss 1.060123
[epoch2, step499]: loss 0.794637
[epoch2, step500]: loss 0.870346
[epoch2, step501]: loss 0.960993
[epoch2, step502]: loss 1.077779
[epoch2, step503]: loss 1.008948
[epoch2, step504]: loss 0.913989
[epoch2, step505]: loss 1.180540
[epoch2, step506]: loss 0.844146
[epoch2, step507]: loss 0.734726
[epoch2, step508]: loss 0.861012
[epoch2, step509]: loss 0.830605
[epoch2, step510]: loss 1.154489
[epoch2, step511]: loss 0.875534
[epoch2, step512]: loss 0.743127
[epoch2, step513]: loss 1.057430
[epoch2, step514]: loss 1.139682
[epoch2, step515]: loss 1.077103
[epoch2, step516]: loss 1.003272
[epoch2, step517]: loss 0.893522
[epoch2, step518]: loss 0.951205
[epoch2, step519]: loss 0.818348
[epoch2, step520]: loss 1.044370
[epoch2, step521]: loss 1.018241
[epoch2, step522]: loss 0.958833
[epoch2, step523]: loss 1.249380
[epoch2, step524]: loss 1.261294
[epoch2, step525]: loss 1.075124
[epoch2, step526]: loss 1.141674
[epoch2, step527]: loss 1.067126
[epoch2, step528]: loss 0.698872
[epoch2, step529]: loss 1.155082
[epoch2, step530]: loss 1.006168
[epoch2, step531]: loss 0.884667
[epoch2, step532]: loss 0.885729
[epoch2, step533]: loss 1.056484
[epoch2, step534]: loss 0.904733
[epoch2, step535]: loss 0.506208
[epoch2, step536]: loss 1.040898
[epoch2, step537]: loss 0.889053
[epoch2, step538]: loss 0.722887
[epoch2, step539]: loss 0.957359
[epoch2, step540]: loss 1.140415
[epoch2, step541]: loss 1.032025
[epoch2, step542]: loss 0.819236
[epoch2, step543]: loss 0.967057
[epoch2, step544]: loss 1.067126
[epoch2, step545]: loss 1.119836
[epoch2, step546]: loss 0.877881
[epoch2, step547]: loss 0.826738
[epoch2, step548]: loss 0.888559
[epoch2, step549]: loss 0.903627
[epoch2, step550]: loss 1.025553
[epoch2, step551]: loss 1.159288
[epoch2, step552]: loss 0.899965
[epoch2, step553]: loss 1.027152
[epoch2, step554]: loss 0.946213
[epoch2, step555]: loss 1.028457
[epoch2, step556]: loss 0.661777
[epoch2, step557]: loss 1.135111
[epoch2, step558]: loss 1.158872
[epoch2, step559]: loss 0.819991
[epoch2, step560]: loss 1.124225
[epoch2, step561]: loss 1.116861
[epoch2, step562]: loss 0.794799
[epoch2, step563]: loss 0.746841
[epoch2, step564]: loss 0.791669
[epoch2, step565]: loss 0.737448
[epoch2, step566]: loss 0.973076
[epoch2, step567]: loss 0.949278
[epoch2, step568]: loss 1.029621
[epoch2, step569]: loss 0.801695
[epoch2, step570]: loss 0.697041
[epoch2, step571]: loss 0.838859
[epoch2, step572]: loss 1.119704
[epoch2, step573]: loss 0.785522
[epoch2, step574]: loss 0.691045
[epoch2, step575]: loss 0.995869
[epoch2, step576]: loss 1.124545
[epoch2, step577]: loss 1.069309
[epoch2, step578]: loss 0.893426
[epoch2, step579]: loss 1.028184
[epoch2, step580]: loss 1.060057
[epoch2, step581]: loss 0.922892
[epoch2, step582]: loss 0.957499
[epoch2, step583]: loss 0.996520
[epoch2, step584]: loss 0.873427
[epoch2, step585]: loss 0.971207
[epoch2, step586]: loss 1.024823
[epoch2, step587]: loss 1.027448
[epoch2, step588]: loss 0.942416
[epoch2, step589]: loss 1.181356
[epoch2, step590]: loss 0.998545
[epoch2, step591]: loss 1.103068
[epoch2, step592]: loss 1.026331
[epoch2, step593]: loss 0.720257
[epoch2, step594]: loss 0.471154
[epoch2, step595]: loss 1.113894
[epoch2, step596]: loss 0.700993
[epoch2, step597]: loss 1.254778
[epoch2, step598]: loss 0.868373
[epoch2, step599]: loss 1.247251
[epoch2, step600]: loss 0.610623
[epoch2, step601]: loss 1.112860
[epoch2, step602]: loss 0.988476
[epoch2, step603]: loss 1.116079
[epoch2, step604]: loss 0.780336
[epoch2, step605]: loss 1.076507
[epoch2, step606]: loss 0.775838
[epoch2, step607]: loss 1.052594
[epoch2, step608]: loss 1.127911
[epoch2, step609]: loss 1.271752
[epoch2, step610]: loss 0.847296
[epoch2, step611]: loss 0.750882
[epoch2, step612]: loss 1.020636
[epoch2, step613]: loss 0.939239
[epoch2, step614]: loss 1.016448
[epoch2, step615]: loss 0.851176
[epoch2, step616]: loss 0.983034
[epoch2, step617]: loss 0.877768
[epoch2, step618]: loss 1.040260
[epoch2, step619]: loss 1.043604
[epoch2, step620]: loss 0.764108
[epoch2, step621]: loss 0.831548
[epoch2, step622]: loss 0.540601
[epoch2, step623]: loss 0.856306
[epoch2, step624]: loss 0.865687
[epoch2, step625]: loss 0.994037
[epoch2, step626]: loss 0.821147
[epoch2, step627]: loss 0.762873
[epoch2, step628]: loss 1.037724
[epoch2, step629]: loss 0.851352
[epoch2, step630]: loss 0.995771
[epoch2, step631]: loss 0.842906
[epoch2, step632]: loss 1.075596
[epoch2, step633]: loss 0.878820
[epoch2, step634]: loss 1.016619
[epoch2, step635]: loss 1.051424
[epoch2, step636]: loss 0.909917
[epoch2, step637]: loss 1.025277
[epoch2, step638]: loss 0.916601
[epoch2, step639]: loss 0.796097
[epoch2, step640]: loss 1.287907
[epoch2, step641]: loss 1.107118
[epoch2, step642]: loss 0.910425
[epoch2, step643]: loss 1.209040
[epoch2, step644]: loss 0.954673
[epoch2, step645]: loss 1.063273
[epoch2, step646]: loss 0.546105
[epoch2, step647]: loss 0.917654
[epoch2, step648]: loss 0.968877
[epoch2, step649]: loss 1.138883
[epoch2, step650]: loss 0.859757
[epoch2, step651]: loss 0.878099
[epoch2, step652]: loss 0.865658
[epoch2, step653]: loss 0.804170
[epoch2, step654]: loss 0.934574
[epoch2, step655]: loss 1.131814
[epoch2, step656]: loss 0.745974
[epoch2, step657]: loss 0.856984
[epoch2, step658]: loss 0.922181
[epoch2, step659]: loss 0.966641
[epoch2, step660]: loss 0.784477
[epoch2, step661]: loss 0.895663
[epoch2, step662]: loss 0.926025
[epoch2, step663]: loss 1.040575
[epoch2, step664]: loss 0.766002
[epoch2, step665]: loss 0.997248
[epoch2, step666]: loss 1.045547
[epoch2, step667]: loss 0.910496
[epoch2, step668]: loss 0.588349
[epoch2, step669]: loss 0.848919
[epoch2, step670]: loss 0.929768
[epoch2, step671]: loss 0.864120
[epoch2, step672]: loss 0.830768
[epoch2, step673]: loss 0.980315
[epoch2, step674]: loss 0.910951
[epoch2, step675]: loss 0.507926
[epoch2, step676]: loss 0.795366
[epoch2, step677]: loss 1.086209
[epoch2, step678]: loss 0.845569
[epoch2, step679]: loss 1.021617
[epoch2, step680]: loss 0.816737
[epoch2, step681]: loss 1.001086
[epoch2, step682]: loss 0.614786
[epoch2, step683]: loss 1.027635
[epoch2, step684]: loss 1.111034
[epoch2, step685]: loss 0.840640
[epoch2, step686]: loss 0.914180
[epoch2, step687]: loss 0.905160
[epoch2, step688]: loss 0.916174
[epoch2, step689]: loss 0.858872
[epoch2, step690]: loss 1.027335
[epoch2, step691]: loss 0.904942
[epoch2, step692]: loss 0.836093
[epoch2, step693]: loss 0.942837
[epoch2, step694]: loss 0.722483
[epoch2, step695]: loss 0.838804
[epoch2, step696]: loss 0.879331
[epoch2, step697]: loss 0.831405
[epoch2, step698]: loss 0.999238
[epoch2, step699]: loss 0.800430
[epoch2, step700]: loss 0.728610
[epoch2, step701]: loss 0.920304
[epoch2, step702]: loss 1.000313
[epoch2, step703]: loss 1.112422
[epoch2, step704]: loss 1.011924
[epoch2, step705]: loss 0.906794
[epoch2, step706]: loss 0.881105
[epoch2, step707]: loss 1.123993
[epoch2, step708]: loss 0.517733
[epoch2, step709]: loss 0.715720
[epoch2, step710]: loss 0.938497
[epoch2, step711]: loss 1.054301
[epoch2, step712]: loss 0.712364
[epoch2, step713]: loss 1.085734
[epoch2, step714]: loss 1.157494
[epoch2, step715]: loss 0.684431
[epoch2, step716]: loss 0.816379
[epoch2, step717]: loss 1.279344
[epoch2, step718]: loss 1.096764
[epoch2, step719]: loss 0.596720
[epoch2, step720]: loss 0.869075
[epoch2, step721]: loss 1.048844
[epoch2, step722]: loss 0.768972
[epoch2, step723]: loss 1.000222
[epoch2, step724]: loss 0.784470
[epoch2, step725]: loss 0.908788
[epoch2, step726]: loss 0.906813
[epoch2, step727]: loss 1.221611
[epoch2, step728]: loss 1.031699
[epoch2, step729]: loss 0.946631
[epoch2, step730]: loss 0.858205
[epoch2, step731]: loss 0.814134
[epoch2, step732]: loss 1.034122
[epoch2, step733]: loss 1.136306
[epoch2, step734]: loss 0.800743
[epoch2, step735]: loss 1.046726
[epoch2, step736]: loss 0.645041
[epoch2, step737]: loss 0.861378
[epoch2, step738]: loss 0.963194
[epoch2, step739]: loss 0.830793
[epoch2, step740]: loss 0.706320
[epoch2, step741]: loss 1.052756
[epoch2, step742]: loss 1.184687
[epoch2, step743]: loss 0.951912
[epoch2, step744]: loss 1.046059
[epoch2, step745]: loss 0.813711
[epoch2, step746]: loss 1.071001
[epoch2, step747]: loss 0.814087
[epoch2, step748]: loss 0.872006
[epoch2, step749]: loss 0.676682
[epoch2, step750]: loss 0.831602
[epoch2, step751]: loss 1.204848
[epoch2, step752]: loss 0.733094
[epoch2, step753]: loss 0.634651
[epoch2, step754]: loss 0.840654
[epoch2, step755]: loss 1.087460
[epoch2, step756]: loss 0.945016
[epoch2, step757]: loss 1.109440
[epoch2, step758]: loss 0.832848
[epoch2, step759]: loss 1.013643
[epoch2, step760]: loss 0.857287
[epoch2, step761]: loss 1.208475
[epoch2, step762]: loss 0.955546
[epoch2, step763]: loss 0.777912
[epoch2, step764]: loss 0.756938
[epoch2, step765]: loss 1.028885
[epoch2, step766]: loss 1.059839
[epoch2, step767]: loss 0.664001
[epoch2, step768]: loss 0.870196
[epoch2, step769]: loss 0.789354
[epoch2, step770]: loss 1.001554
[epoch2, step771]: loss 1.205214
[epoch2, step772]: loss 0.771053
[epoch2, step773]: loss 1.065935
[epoch2, step774]: loss 0.921999
[epoch2, step775]: loss 0.957178
[epoch2, step776]: loss 0.788058
[epoch2, step777]: loss 0.994847
[epoch2, step778]: loss 1.083200
[epoch2, step779]: loss 0.917827
[epoch2, step780]: loss 1.097752
[epoch2, step781]: loss 0.880376
[epoch2, step782]: loss 0.851846
[epoch2, step783]: loss 0.912043
[epoch2, step784]: loss 0.832729
[epoch2, step785]: loss 1.126956
[epoch2, step786]: loss 0.731918
[epoch2, step787]: loss 0.810533
[epoch2, step788]: loss 0.837355
[epoch2, step789]: loss 0.804062
[epoch2, step790]: loss 1.172900
[epoch2, step791]: loss 0.743330
[epoch2, step792]: loss 0.723630
[epoch2, step793]: loss 1.104073
[epoch2, step794]: loss 0.926932
[epoch2, step795]: loss 1.183412
[epoch2, step796]: loss 1.030240
[epoch2, step797]: loss 0.938399
[epoch2, step798]: loss 0.661054
[epoch2, step799]: loss 0.790532
[epoch2, step800]: loss 0.731950
[epoch2, step801]: loss 1.244773
[epoch2, step802]: loss 0.834345
[epoch2, step803]: loss 1.042740
[epoch2, step804]: loss 0.623889
[epoch2, step805]: loss 1.119214
[epoch2, step806]: loss 1.187136
[epoch2, step807]: loss 1.030946
[epoch2, step808]: loss 1.033232
[epoch2, step809]: loss 1.055672
[epoch2, step810]: loss 0.857506
[epoch2, step811]: loss 0.961071
[epoch2, step812]: loss 1.017452
[epoch2, step813]: loss 1.003617
[epoch2, step814]: loss 1.262428
[epoch2, step815]: loss 0.994161
[epoch2, step816]: loss 0.879074
[epoch2, step817]: loss 0.805788
[epoch2, step818]: loss 0.901588
[epoch2, step819]: loss 0.890055
[epoch2, step820]: loss 1.041867
[epoch2, step821]: loss 1.042761
[epoch2, step822]: loss 0.873719
[epoch2, step823]: loss 0.697665
[epoch2, step824]: loss 0.896322
[epoch2, step825]: loss 0.841197
[epoch2, step826]: loss 0.603907
[epoch2, step827]: loss 0.898972
[epoch2, step828]: loss 0.746960
[epoch2, step829]: loss 0.688074
[epoch2, step830]: loss 1.043057
[epoch2, step831]: loss 0.966426
[epoch2, step832]: loss 0.885749
[epoch2, step833]: loss 0.868227
[epoch2, step834]: loss 0.978181
[epoch2, step835]: loss 1.164477
[epoch2, step836]: loss 0.764558
[epoch2, step837]: loss 1.297161
[epoch2, step838]: loss 0.972737
[epoch2, step839]: loss 1.254104
[epoch2, step840]: loss 0.802878
[epoch2, step841]: loss 0.770763
[epoch2, step842]: loss 0.886535
[epoch2, step843]: loss 0.900715
[epoch2, step844]: loss 1.152340
[epoch2, step845]: loss 0.761576
[epoch2, step846]: loss 0.769297
[epoch2, step847]: loss 0.726957
[epoch2, step848]: loss 0.746020
[epoch2, step849]: loss 1.071789
[epoch2, step850]: loss 1.052551
[epoch2, step851]: loss 1.102497
[epoch2, step852]: loss 0.700645
[epoch2, step853]: loss 1.251440
[epoch2, step854]: loss 1.067210
[epoch2, step855]: loss 0.854961
[epoch2, step856]: loss 0.839391
[epoch2, step857]: loss 1.231300
[epoch2, step858]: loss 0.586088
[epoch2, step859]: loss 0.752541
[epoch2, step860]: loss 0.918499
[epoch2, step861]: loss 0.760428
[epoch2, step862]: loss 1.030647
[epoch2, step863]: loss 0.981089
[epoch2, step864]: loss 0.868069
[epoch2, step865]: loss 1.033194
[epoch2, step866]: loss 1.027716
[epoch2, step867]: loss 1.118609
[epoch2, step868]: loss 0.679008
[epoch2, step869]: loss 0.956965
[epoch2, step870]: loss 0.966823
[epoch2, step871]: loss 0.542881
[epoch2, step872]: loss 0.849342
[epoch2, step873]: loss 0.804341
[epoch2, step874]: loss 0.868171
[epoch2, step875]: loss 0.811608
[epoch2, step876]: loss 1.261363
[epoch2, step877]: loss 0.938566
[epoch2, step878]: loss 0.716084
[epoch2, step879]: loss 0.843660
[epoch2, step880]: loss 0.879777
[epoch2, step881]: loss 0.995624
[epoch2, step882]: loss 0.744037
[epoch2, step883]: loss 0.779537
[epoch2, step884]: loss 0.872729
[epoch2, step885]: loss 0.769185
[epoch2, step886]: loss 0.844873
[epoch2, step887]: loss 1.053815
[epoch2, step888]: loss 0.844968
[epoch2, step889]: loss 0.821457
[epoch2, step890]: loss 0.751484
[epoch2, step891]: loss 0.948205
[epoch2, step892]: loss 1.268155
[epoch2, step893]: loss 0.605482
[epoch2, step894]: loss 1.101669
[epoch2, step895]: loss 0.838671
[epoch2, step896]: loss 1.078691
[epoch2, step897]: loss 1.014997
[epoch2, step898]: loss 0.808950
[epoch2, step899]: loss 0.558548
[epoch2, step900]: loss 1.186671
[epoch2, step901]: loss 0.666470
[epoch2, step902]: loss 0.989525
[epoch2, step903]: loss 1.162258
[epoch2, step904]: loss 0.771645
[epoch2, step905]: loss 0.880312
[epoch2, step906]: loss 0.906353
[epoch2, step907]: loss 0.843163
[epoch2, step908]: loss 1.008787
[epoch2, step909]: loss 0.844193
[epoch2, step910]: loss 0.908150
[epoch2, step911]: loss 1.044575
[epoch2, step912]: loss 1.061440
[epoch2, step913]: loss 1.013658
[epoch2, step914]: loss 0.971211
[epoch2, step915]: loss 0.906750
[epoch2, step916]: loss 0.934478
[epoch2, step917]: loss 0.917689
[epoch2, step918]: loss 1.064294
[epoch2, step919]: loss 1.195045
[epoch2, step920]: loss 0.676915
[epoch2, step921]: loss 1.116629
[epoch2, step922]: loss 0.861454
[epoch2, step923]: loss 0.946215
[epoch2, step924]: loss 1.069180
[epoch2, step925]: loss 0.849703
[epoch2, step926]: loss 0.490090
[epoch2, step927]: loss 0.951859
[epoch2, step928]: loss 0.741108
[epoch2, step929]: loss 0.569679
[epoch2, step930]: loss 1.071286
[epoch2, step931]: loss 0.764492
[epoch2, step932]: loss 0.693882
[epoch2, step933]: loss 0.685012
[epoch2, step934]: loss 0.851979
[epoch2, step935]: loss 0.778822
[epoch2, step936]: loss 0.864151
[epoch2, step937]: loss 0.831329
[epoch2, step938]: loss 0.893612
[epoch2, step939]: loss 0.724949
[epoch2, step940]: loss 0.705984
[epoch2, step941]: loss 0.901098
[epoch2, step942]: loss 1.022138
[epoch2, step943]: loss 0.943640
[epoch2, step944]: loss 0.953759
[epoch2, step945]: loss 0.780201
[epoch2, step946]: loss 0.631351
[epoch2, step947]: loss 0.914728
[epoch2, step948]: loss 1.150408
[epoch2, step949]: loss 0.894419
[epoch2, step950]: loss 0.900798
[epoch2, step951]: loss 1.027977
[epoch2, step952]: loss 1.066603
[epoch2, step953]: loss 1.011870
[epoch2, step954]: loss 0.942493
[epoch2, step955]: loss 0.809421
[epoch2, step956]: loss 0.935783
[epoch2, step957]: loss 0.839743
[epoch2, step958]: loss 0.867702
[epoch2, step959]: loss 1.000605
[epoch2, step960]: loss 0.854007
[epoch2, step961]: loss 0.847110
[epoch2, step962]: loss 1.209877
[epoch2, step963]: loss 1.023228
[epoch2, step964]: loss 0.763046
[epoch2, step965]: loss 0.715393
[epoch2, step966]: loss 1.030085
[epoch2, step967]: loss 0.981597
[epoch2, step968]: loss 1.036392
[epoch2, step969]: loss 0.991490
[epoch2, step970]: loss 0.851630
[epoch2, step971]: loss 0.986753
[epoch2, step972]: loss 0.772175
[epoch2, step973]: loss 1.158182
[epoch2, step974]: loss 0.770612
[epoch2, step975]: loss 0.931184
[epoch2, step976]: loss 0.784137
[epoch2, step977]: loss 0.653248
[epoch2, step978]: loss 1.063470
[epoch2, step979]: loss 0.754257
[epoch2, step980]: loss 1.160569
[epoch2, step981]: loss 1.086046
[epoch2, step982]: loss 0.969356
[epoch2, step983]: loss 0.971852
[epoch2, step984]: loss 0.707659
[epoch2, step985]: loss 1.108702
[epoch2, step986]: loss 0.850075
[epoch2, step987]: loss 0.918910
[epoch2, step988]: loss 0.986747
[epoch2, step989]: loss 0.871512
[epoch2, step990]: loss 0.901175
[epoch2, step991]: loss 0.811555
[epoch2, step992]: loss 0.727510
[epoch2, step993]: loss 0.912188
[epoch2, step994]: loss 0.788545
[epoch2, step995]: loss 0.574050
[epoch2, step996]: loss 0.958297
[epoch2, step997]: loss 1.208727
[epoch2, step998]: loss 0.866544
[epoch2, step999]: loss 0.599095
[epoch2, step1000]: loss 0.987806
[epoch2, step1001]: loss 0.718573
[epoch2, step1002]: loss 0.532262
[epoch2, step1003]: loss 0.691971
[epoch2, step1004]: loss 0.858072
[epoch2, step1005]: loss 0.820120
[epoch2, step1006]: loss 1.104614
[epoch2, step1007]: loss 0.914967
[epoch2, step1008]: loss 0.752575
[epoch2, step1009]: loss 1.058986
[epoch2, step1010]: loss 0.851547
[epoch2, step1011]: loss 1.057794
[epoch2, step1012]: loss 0.878589
[epoch2, step1013]: loss 0.834501
[epoch2, step1014]: loss 0.923495
[epoch2, step1015]: loss 0.859906
[epoch2, step1016]: loss 0.693395
[epoch2, step1017]: loss 0.673399
[epoch2, step1018]: loss 0.878157
[epoch2, step1019]: loss 1.090872
[epoch2, step1020]: loss 0.954949
[epoch2, step1021]: loss 0.950559
[epoch2, step1022]: loss 0.820426
[epoch2, step1023]: loss 0.712725
[epoch2, step1024]: loss 0.868298
[epoch2, step1025]: loss 1.041215
[epoch2, step1026]: loss 0.536574
[epoch2, step1027]: loss 0.733968
[epoch2, step1028]: loss 1.213547
[epoch2, step1029]: loss 1.055173
[epoch2, step1030]: loss 0.724274
[epoch2, step1031]: loss 0.842675
[epoch2, step1032]: loss 0.635580
[epoch2, step1033]: loss 0.883742
[epoch2, step1034]: loss 0.969757
[epoch2, step1035]: loss 0.735534
[epoch2, step1036]: loss 0.514396
[epoch2, step1037]: loss 0.467482
[epoch2, step1038]: loss 0.724101
[epoch2, step1039]: loss 1.116156
[epoch2, step1040]: loss 1.171218
[epoch2, step1041]: loss 0.680419
[epoch2, step1042]: loss 0.902445
[epoch2, step1043]: loss 0.959163
[epoch2, step1044]: loss 0.755567
[epoch2, step1045]: loss 0.770642
[epoch2, step1046]: loss 0.673729
[epoch2, step1047]: loss 0.666831
[epoch2, step1048]: loss 0.834275
[epoch2, step1049]: loss 0.775622
[epoch2, step1050]: loss 0.969625
[epoch2, step1051]: loss 0.811609
[epoch2, step1052]: loss 1.019379
[epoch2, step1053]: loss 0.730722
[epoch2, step1054]: loss 0.865133
[epoch2, step1055]: loss 0.750511
[epoch2, step1056]: loss 0.549557
[epoch2, step1057]: loss 1.137082
[epoch2, step1058]: loss 0.591907
[epoch2, step1059]: loss 0.588028
[epoch2, step1060]: loss 0.960235
[epoch2, step1061]: loss 0.673019
[epoch2, step1062]: loss 1.256734
[epoch2, step1063]: loss 0.453399
[epoch2, step1064]: loss 0.847407
[epoch2, step1065]: loss 0.979409
[epoch2, step1066]: loss 0.765819
[epoch2, step1067]: loss 1.089883
[epoch2, step1068]: loss 0.973790
[epoch2, step1069]: loss 1.135953
[epoch2, step1070]: loss 0.732618
[epoch2, step1071]: loss 1.145711
[epoch2, step1072]: loss 0.853028
[epoch2, step1073]: loss 1.020865
[epoch2, step1074]: loss 0.591141
[epoch2, step1075]: loss 0.849424
[epoch2, step1076]: loss 0.887985
[epoch2, step1077]: loss 0.877386
[epoch2, step1078]: loss 1.016697
[epoch2, step1079]: loss 0.818505
[epoch2, step1080]: loss 0.852743
[epoch2, step1081]: loss 0.699262
[epoch2, step1082]: loss 1.037259
[epoch2, step1083]: loss 0.928820
[epoch2, step1084]: loss 1.025746
[epoch2, step1085]: loss 0.861847
[epoch2, step1086]: loss 0.813723
[epoch2, step1087]: loss 1.042107
[epoch2, step1088]: loss 1.015179
[epoch2, step1089]: loss 0.813786
[epoch2, step1090]: loss 1.174980
[epoch2, step1091]: loss 0.955148
[epoch2, step1092]: loss 0.840239
[epoch2, step1093]: loss 0.880978
[epoch2, step1094]: loss 0.771220
[epoch2, step1095]: loss 0.830503
[epoch2, step1096]: loss 1.064154
[epoch2, step1097]: loss 0.578463
[epoch2, step1098]: loss 1.127533
[epoch2, step1099]: loss 0.760035
[epoch2, step1100]: loss 0.813308
[epoch2, step1101]: loss 0.979540
[epoch2, step1102]: loss 0.877058
[epoch2, step1103]: loss 0.671720
[epoch2, step1104]: loss 0.939161
[epoch2, step1105]: loss 0.819741
[epoch2, step1106]: loss 0.975152
[epoch2, step1107]: loss 0.871978
[epoch2, step1108]: loss 0.773986
[epoch2, step1109]: loss 1.022616
[epoch2, step1110]: loss 0.705686
[epoch2, step1111]: loss 1.054214
[epoch2, step1112]: loss 1.134557
[epoch2, step1113]: loss 0.794974
[epoch2, step1114]: loss 1.076096
[epoch2, step1115]: loss 0.918775
[epoch2, step1116]: loss 1.078009
[epoch2, step1117]: loss 0.852409
[epoch2, step1118]: loss 0.844735
[epoch2, step1119]: loss 0.581652
[epoch2, step1120]: loss 1.016605
[epoch2, step1121]: loss 0.469324
[epoch2, step1122]: loss 0.457813
[epoch2, step1123]: loss 0.928576
[epoch2, step1124]: loss 0.483961
[epoch2, step1125]: loss 0.753444
[epoch2, step1126]: loss 0.865571
[epoch2, step1127]: loss 0.955484
[epoch2, step1128]: loss 0.698331
[epoch2, step1129]: loss 0.648511
[epoch2, step1130]: loss 0.840563
[epoch2, step1131]: loss 0.611113
[epoch2, step1132]: loss 1.026790
[epoch2, step1133]: loss 1.221552
[epoch2, step1134]: loss 0.894006
[epoch2, step1135]: loss 1.002173
[epoch2, step1136]: loss 0.910726
[epoch2, step1137]: loss 0.544796
[epoch2, step1138]: loss 1.043870
[epoch2, step1139]: loss 0.814109
[epoch2, step1140]: loss 0.878939
[epoch2, step1141]: loss 0.870065
[epoch2, step1142]: loss 0.956300
[epoch2, step1143]: loss 0.706706
[epoch2, step1144]: loss 1.134577
[epoch2, step1145]: loss 0.953423
[epoch2, step1146]: loss 1.018077
[epoch2, step1147]: loss 0.705314
[epoch2, step1148]: loss 0.691598
[epoch2, step1149]: loss 0.903718
[epoch2, step1150]: loss 0.661569
[epoch2, step1151]: loss 1.009029
[epoch2, step1152]: loss 0.955243
[epoch2, step1153]: loss 0.979294
[epoch2, step1154]: loss 0.867036
[epoch2, step1155]: loss 1.104784
[epoch2, step1156]: loss 0.813401
[epoch2, step1157]: loss 0.810108
[epoch2, step1158]: loss 1.140728
[epoch2, step1159]: loss 0.827372
[epoch2, step1160]: loss 0.938546
[epoch2, step1161]: loss 0.747481
[epoch2, step1162]: loss 0.952957
[epoch2, step1163]: loss 0.918446
[epoch2, step1164]: loss 0.787365
[epoch2, step1165]: loss 0.863144
[epoch2, step1166]: loss 1.060634
[epoch2, step1167]: loss 0.898177
[epoch2, step1168]: loss 0.587683
[epoch2, step1169]: loss 0.632532
[epoch2, step1170]: loss 1.033527
[epoch2, step1171]: loss 1.015895
[epoch2, step1172]: loss 1.108728
[epoch2, step1173]: loss 0.729383
[epoch2, step1174]: loss 0.799897
[epoch2, step1175]: loss 0.962001
[epoch2, step1176]: loss 1.016950
[epoch2, step1177]: loss 1.049353
[epoch2, step1178]: loss 0.904646
[epoch2, step1179]: loss 0.894895
[epoch2, step1180]: loss 0.853463
[epoch2, step1181]: loss 0.989998
[epoch2, step1182]: loss 1.000925
[epoch2, step1183]: loss 0.757750
[epoch2, step1184]: loss 0.520463
[epoch2, step1185]: loss 0.842212
[epoch2, step1186]: loss 0.901063
[epoch2, step1187]: loss 1.169353
[epoch2, step1188]: loss 1.059099
[epoch2, step1189]: loss 0.892746
[epoch2, step1190]: loss 0.888301
[epoch2, step1191]: loss 0.820514
[epoch2, step1192]: loss 0.717649
[epoch2, step1193]: loss 0.981880
[epoch2, step1194]: loss 0.706279
[epoch2, step1195]: loss 0.785487
[epoch2, step1196]: loss 0.895410
[epoch2, step1197]: loss 0.758064
[epoch2, step1198]: loss 0.929760
[epoch2, step1199]: loss 0.939329
[epoch2, step1200]: loss 0.939252
[epoch2, step1201]: loss 0.800484
[epoch2, step1202]: loss 0.959479
[epoch2, step1203]: loss 1.205495
[epoch2, step1204]: loss 0.735991
[epoch2, step1205]: loss 0.898711
[epoch2, step1206]: loss 0.813852
[epoch2, step1207]: loss 0.699438
[epoch2, step1208]: loss 0.748302
[epoch2, step1209]: loss 0.750449
[epoch2, step1210]: loss 1.038442
[epoch2, step1211]: loss 0.556493
[epoch2, step1212]: loss 0.968707
[epoch2, step1213]: loss 0.993647
[epoch2, step1214]: loss 0.901257
[epoch2, step1215]: loss 0.956620
[epoch2, step1216]: loss 0.762795
[epoch2, step1217]: loss 0.996815
[epoch2, step1218]: loss 0.796234
[epoch2, step1219]: loss 0.805683
[epoch2, step1220]: loss 0.952241
[epoch2, step1221]: loss 0.666760
[epoch2, step1222]: loss 0.873518
[epoch2, step1223]: loss 0.764254
[epoch2, step1224]: loss 1.100295
[epoch2, step1225]: loss 1.077762
[epoch2, step1226]: loss 0.775549
[epoch2, step1227]: loss 1.129968
[epoch2, step1228]: loss 0.850841
[epoch2, step1229]: loss 0.939761
[epoch2, step1230]: loss 0.615166
[epoch2, step1231]: loss 0.951494
[epoch2, step1232]: loss 0.784320
[epoch2, step1233]: loss 0.773558
[epoch2, step1234]: loss 1.045818
[epoch2, step1235]: loss 0.978316
[epoch2, step1236]: loss 0.887583
[epoch2, step1237]: loss 0.770199
[epoch2, step1238]: loss 0.995622
[epoch2, step1239]: loss 0.764546
[epoch2, step1240]: loss 0.795752
[epoch2, step1241]: loss 0.776285
[epoch2, step1242]: loss 0.806258
[epoch2, step1243]: loss 1.110821
[epoch2, step1244]: loss 0.949561
[epoch2, step1245]: loss 0.839972
[epoch2, step1246]: loss 0.606880
[epoch2, step1247]: loss 1.025703
[epoch2, step1248]: loss 0.643035
[epoch2, step1249]: loss 0.724911
[epoch2, step1250]: loss 0.903211
[epoch2, step1251]: loss 0.792334
[epoch2, step1252]: loss 1.002026
[epoch2, step1253]: loss 0.980226
[epoch2, step1254]: loss 0.811750
[epoch2, step1255]: loss 0.670904
[epoch2, step1256]: loss 0.967575
[epoch2, step1257]: loss 0.796610
[epoch2, step1258]: loss 0.800058
[epoch2, step1259]: loss 0.749571
[epoch2, step1260]: loss 0.704975
[epoch2, step1261]: loss 1.150067
[epoch2, step1262]: loss 0.922568
[epoch2, step1263]: loss 0.641136
[epoch2, step1264]: loss 0.311283
[epoch2, step1265]: loss 0.940497
[epoch2, step1266]: loss 0.932349
[epoch2, step1267]: loss 1.029696
[epoch2, step1268]: loss 0.663510
[epoch2, step1269]: loss 1.021186
[epoch2, step1270]: loss 0.988213
[epoch2, step1271]: loss 0.895365
[epoch2, step1272]: loss 1.082009
[epoch2, step1273]: loss 0.705242
[epoch2, step1274]: loss 1.011727
[epoch2, step1275]: loss 0.944268
[epoch2, step1276]: loss 0.890491
[epoch2, step1277]: loss 1.184942
[epoch2, step1278]: loss 0.777287
[epoch2, step1279]: loss 0.714730
[epoch2, step1280]: loss 0.693463
[epoch2, step1281]: loss 0.868321
[epoch2, step1282]: loss 0.953831
[epoch2, step1283]: loss 0.890595
[epoch2, step1284]: loss 0.703651
[epoch2, step1285]: loss 0.582402
[epoch2, step1286]: loss 0.933293
[epoch2, step1287]: loss 1.041498
[epoch2, step1288]: loss 0.919120
[epoch2, step1289]: loss 0.977001
[epoch2, step1290]: loss 0.643354
[epoch2, step1291]: loss 0.762825
[epoch2, step1292]: loss 0.876042
[epoch2, step1293]: loss 0.903078
[epoch2, step1294]: loss 1.038888
[epoch2, step1295]: loss 0.955512
[epoch2, step1296]: loss 0.939366
[epoch2, step1297]: loss 0.541963
[epoch2, step1298]: loss 0.930968
[epoch2, step1299]: loss 0.820258
[epoch2, step1300]: loss 1.063025
[epoch2, step1301]: loss 0.941303
[epoch2, step1302]: loss 0.966830
[epoch2, step1303]: loss 0.662702
[epoch2, step1304]: loss 0.776734
[epoch2, step1305]: loss 0.951334
[epoch2, step1306]: loss 0.791142
[epoch2, step1307]: loss 0.912177
[epoch2, step1308]: loss 0.786056
[epoch2, step1309]: loss 0.826285
[epoch2, step1310]: loss 0.875528
[epoch2, step1311]: loss 0.898915
[epoch2, step1312]: loss 0.650128
[epoch2, step1313]: loss 0.962304
[epoch2, step1314]: loss 0.695848
[epoch2, step1315]: loss 0.970462
[epoch2, step1316]: loss 1.015369
[epoch2, step1317]: loss 0.847395
[epoch2, step1318]: loss 1.074994
[epoch2, step1319]: loss 0.746120
[epoch2, step1320]: loss 0.872719
[epoch2, step1321]: loss 1.036306
[epoch2, step1322]: loss 0.617903
[epoch2, step1323]: loss 1.026745
[epoch2, step1324]: loss 1.110468
[epoch2, step1325]: loss 0.663463
[epoch2, step1326]: loss 0.844112
[epoch2, step1327]: loss 0.733545
[epoch2, step1328]: loss 0.809682
[epoch2, step1329]: loss 1.003968
[epoch2, step1330]: loss 0.639172
[epoch2, step1331]: loss 1.040692
[epoch2, step1332]: loss 0.807471
[epoch2, step1333]: loss 0.981033
[epoch2, step1334]: loss 0.925785
[epoch2, step1335]: loss 0.826090
[epoch2, step1336]: loss 0.730529
[epoch2, step1337]: loss 0.888964
[epoch2, step1338]: loss 1.068982
[epoch2, step1339]: loss 0.818917
[epoch2, step1340]: loss 0.953630
[epoch2, step1341]: loss 0.981038
[epoch2, step1342]: loss 0.833777
[epoch2, step1343]: loss 0.635960
[epoch2, step1344]: loss 0.794086
[epoch2, step1345]: loss 1.032277
[epoch2, step1346]: loss 0.923828
[epoch2, step1347]: loss 1.045301
[epoch2, step1348]: loss 1.041522
[epoch2, step1349]: loss 0.550746
[epoch2, step1350]: loss 1.003814
[epoch2, step1351]: loss 0.747904
[epoch2, step1352]: loss 0.706052
[epoch2, step1353]: loss 0.853352
[epoch2, step1354]: loss 1.111241
[epoch2, step1355]: loss 0.995739
[epoch2, step1356]: loss 0.700680
[epoch2, step1357]: loss 1.099001
[epoch2, step1358]: loss 0.607167
[epoch2, step1359]: loss 0.860385
[epoch2, step1360]: loss 0.847249
[epoch2, step1361]: loss 0.940022
[epoch2, step1362]: loss 0.750037
[epoch2, step1363]: loss 1.104429
[epoch2, step1364]: loss 0.856741
[epoch2, step1365]: loss 0.895415
[epoch2, step1366]: loss 0.770850
[epoch2, step1367]: loss 0.752124
[epoch2, step1368]: loss 0.861126
[epoch2, step1369]: loss 0.988879
[epoch2, step1370]: loss 1.009093
[epoch2, step1371]: loss 0.836873
[epoch2, step1372]: loss 0.938781
[epoch2, step1373]: loss 0.887818
[epoch2, step1374]: loss 0.916162
[epoch2, step1375]: loss 0.787305
[epoch2, step1376]: loss 0.963326
[epoch2, step1377]: loss 1.048304
[epoch2, step1378]: loss 0.879044
[epoch2, step1379]: loss 0.821257
[epoch2, step1380]: loss 0.850749
[epoch2, step1381]: loss 0.892615
[epoch2, step1382]: loss 0.804923
[epoch2, step1383]: loss 0.841409
[epoch2, step1384]: loss 0.425308
[epoch2, step1385]: loss 1.109712
[epoch2, step1386]: loss 0.814635
[epoch2, step1387]: loss 0.796692
[epoch2, step1388]: loss 0.865574
[epoch2, step1389]: loss 0.848220
[epoch2, step1390]: loss 0.965513
[epoch2, step1391]: loss 0.934821
[epoch2, step1392]: loss 0.964223
[epoch2, step1393]: loss 0.519065
[epoch2, step1394]: loss 0.686018
[epoch2, step1395]: loss 1.027286
[epoch2, step1396]: loss 0.810905
[epoch2, step1397]: loss 0.574549
[epoch2, step1398]: loss 0.658083
[epoch2, step1399]: loss 0.504232
[epoch2, step1400]: loss 1.082447
[epoch2, step1401]: loss 1.001989
[epoch2, step1402]: loss 0.521497
[epoch2, step1403]: loss 0.953697
[epoch2, step1404]: loss 1.027859
[epoch2, step1405]: loss 0.593426
[epoch2, step1406]: loss 0.769766
[epoch2, step1407]: loss 0.955727
[epoch2, step1408]: loss 0.983760
[epoch2, step1409]: loss 0.855539
[epoch2, step1410]: loss 0.929554
[epoch2, step1411]: loss 0.752871
[epoch2, step1412]: loss 0.470546
[epoch2, step1413]: loss 0.706049
[epoch2, step1414]: loss 0.904264
[epoch2, step1415]: loss 0.904394
[epoch2, step1416]: loss 0.910802
[epoch2, step1417]: loss 0.951622
[epoch2, step1418]: loss 0.797666
[epoch2, step1419]: loss 0.676601
[epoch2, step1420]: loss 0.891214
[epoch2, step1421]: loss 0.827312
[epoch2, step1422]: loss 1.054667
[epoch2, step1423]: loss 0.758349
[epoch2, step1424]: loss 0.761484
[epoch2, step1425]: loss 0.763548
[epoch2, step1426]: loss 0.684620
[epoch2, step1427]: loss 0.805237
[epoch2, step1428]: loss 0.778179
[epoch2, step1429]: loss 0.510430
[epoch2, step1430]: loss 0.997155
[epoch2, step1431]: loss 0.805811
[epoch2, step1432]: loss 1.015226
[epoch2, step1433]: loss 0.999051
[epoch2, step1434]: loss 0.985874
[epoch2, step1435]: loss 0.786535
[epoch2, step1436]: loss 0.728255
[epoch2, step1437]: loss 0.962856
[epoch2, step1438]: loss 0.809504
[epoch2, step1439]: loss 0.962513
[epoch2, step1440]: loss 1.026993
[epoch2, step1441]: loss 1.096186
[epoch2, step1442]: loss 0.661403
[epoch2, step1443]: loss 0.892758
[epoch2, step1444]: loss 0.793205
[epoch2, step1445]: loss 0.712471
[epoch2, step1446]: loss 1.001584
[epoch2, step1447]: loss 1.046322
[epoch2, step1448]: loss 0.731401
[epoch2, step1449]: loss 0.932692
[epoch2, step1450]: loss 0.948338
[epoch2, step1451]: loss 1.011478
[epoch2, step1452]: loss 0.726864
[epoch2, step1453]: loss 0.928823
[epoch2, step1454]: loss 0.720473
[epoch2, step1455]: loss 0.878425
[epoch2, step1456]: loss 0.924186
[epoch2, step1457]: loss 0.703760
[epoch2, step1458]: loss 0.946326
[epoch2, step1459]: loss 1.022016
[epoch2, step1460]: loss 0.742976
[epoch2, step1461]: loss 0.841920
[epoch2, step1462]: loss 0.755211
[epoch2, step1463]: loss 0.529731
[epoch2, step1464]: loss 0.775253
[epoch2, step1465]: loss 0.858092
[epoch2, step1466]: loss 0.861035
[epoch2, step1467]: loss 0.843310
[epoch2, step1468]: loss 1.076790
[epoch2, step1469]: loss 0.867543
[epoch2, step1470]: loss 0.578734
[epoch2, step1471]: loss 1.027512
[epoch2, step1472]: loss 1.035783
[epoch2, step1473]: loss 0.977303
[epoch2, step1474]: loss 0.753938
[epoch2, step1475]: loss 0.957286
[epoch2, step1476]: loss 0.947750
[epoch2, step1477]: loss 0.759854
[epoch2, step1478]: loss 0.539794
[epoch2, step1479]: loss 0.581548
[epoch2, step1480]: loss 0.673451
[epoch2, step1481]: loss 0.818443
[epoch2, step1482]: loss 1.018651
[epoch2, step1483]: loss 0.917191
[epoch2, step1484]: loss 0.755985
[epoch2, step1485]: loss 1.007084
[epoch2, step1486]: loss 0.637656
[epoch2, step1487]: loss 0.787657
[epoch2, step1488]: loss 0.688005
[epoch2, step1489]: loss 1.002986
[epoch2, step1490]: loss 0.663704
[epoch2, step1491]: loss 0.646086
[epoch2, step1492]: loss 0.683917
[epoch2, step1493]: loss 0.966302
[epoch2, step1494]: loss 0.705378
[epoch2, step1495]: loss 0.753009
[epoch2, step1496]: loss 0.798901
[epoch2, step1497]: loss 0.812007
[epoch2, step1498]: loss 0.690522
[epoch2, step1499]: loss 0.879745
[epoch2, step1500]: loss 0.750300
[epoch2, step1501]: loss 0.640334
[epoch2, step1502]: loss 0.567048
[epoch2, step1503]: loss 0.841350
[epoch2, step1504]: loss 0.516522
[epoch2, step1505]: loss 0.918238
[epoch2, step1506]: loss 1.000669
[epoch2, step1507]: loss 1.008350
[epoch2, step1508]: loss 0.678375
[epoch2, step1509]: loss 0.692481
[epoch2, step1510]: loss 0.712989
[epoch2, step1511]: loss 0.793043
[epoch2, step1512]: loss 0.785448
[epoch2, step1513]: loss 1.049245
[epoch2, step1514]: loss 0.606429
[epoch2, step1515]: loss 0.780397
[epoch2, step1516]: loss 0.772891
[epoch2, step1517]: loss 0.780345
[epoch2, step1518]: loss 0.725921
[epoch2, step1519]: loss 1.018273
[epoch2, step1520]: loss 0.945673
[epoch2, step1521]: loss 0.916864
[epoch2, step1522]: loss 1.008096
[epoch2, step1523]: loss 0.998589
[epoch2, step1524]: loss 0.876887
[epoch2, step1525]: loss 0.911878
[epoch2, step1526]: loss 0.784590
[epoch2, step1527]: loss 0.703613
[epoch2, step1528]: loss 0.578734
[epoch2, step1529]: loss 0.747170
[epoch2, step1530]: loss 0.864239
[epoch2, step1531]: loss 0.885055
[epoch2, step1532]: loss 0.615280
[epoch2, step1533]: loss 0.521849
[epoch2, step1534]: loss 0.884767
[epoch2, step1535]: loss 0.968860
[epoch2, step1536]: loss 1.132468
[epoch2, step1537]: loss 0.973391
[epoch2, step1538]: loss 0.739624
[epoch2, step1539]: loss 0.774227
[epoch2, step1540]: loss 0.936120
[epoch2, step1541]: loss 0.930764
[epoch2, step1542]: loss 0.612763
[epoch2, step1543]: loss 0.847837
[epoch2, step1544]: loss 1.056614
[epoch2, step1545]: loss 1.190937
[epoch2, step1546]: loss 0.690552
[epoch2, step1547]: loss 1.092031
[epoch2, step1548]: loss 0.764915
[epoch2, step1549]: loss 0.859856
[epoch2, step1550]: loss 0.852792
[epoch2, step1551]: loss 0.799776
[epoch2, step1552]: loss 0.634057
[epoch2, step1553]: loss 0.738765
[epoch2, step1554]: loss 0.942786
[epoch2, step1555]: loss 0.727518
[epoch2, step1556]: loss 0.800469
[epoch2, step1557]: loss 1.043858
[epoch2, step1558]: loss 0.947153
[epoch2, step1559]: loss 0.990869
[epoch2, step1560]: loss 0.816564
[epoch2, step1561]: loss 0.621506
[epoch2, step1562]: loss 0.881763
[epoch2, step1563]: loss 0.993193
[epoch2, step1564]: loss 0.590879
[epoch2, step1565]: loss 0.913990
[epoch2, step1566]: loss 0.837745
[epoch2, step1567]: loss 0.676776
[epoch2, step1568]: loss 0.893764
[epoch2, step1569]: loss 1.016034
[epoch2, step1570]: loss 0.814288
[epoch2, step1571]: loss 0.842074
[epoch2, step1572]: loss 1.031358
[epoch2, step1573]: loss 0.516850
[epoch2, step1574]: loss 0.834437
[epoch2, step1575]: loss 1.065705
[epoch2, step1576]: loss 0.913410
[epoch2, step1577]: loss 0.805593
[epoch2, step1578]: loss 0.835500
[epoch2, step1579]: loss 0.744651
[epoch2, step1580]: loss 0.722488
[epoch2, step1581]: loss 0.834710
[epoch2, step1582]: loss 0.870076
[epoch2, step1583]: loss 0.842503
[epoch2, step1584]: loss 0.864130
[epoch2, step1585]: loss 0.626774
[epoch2, step1586]: loss 0.776414
[epoch2, step1587]: loss 0.936008
[epoch2, step1588]: loss 0.937997
[epoch2, step1589]: loss 1.086721
[epoch2, step1590]: loss 0.717315
[epoch2, step1591]: loss 0.874281
[epoch2, step1592]: loss 0.559114
[epoch2, step1593]: loss 0.693187
[epoch2, step1594]: loss 0.913335
[epoch2, step1595]: loss 0.804515
[epoch2, step1596]: loss 1.073712
[epoch2, step1597]: loss 0.751669
[epoch2, step1598]: loss 0.708232
[epoch2, step1599]: loss 0.862941
[epoch2, step1600]: loss 0.555622
[epoch2, step1601]: loss 0.927546
[epoch2, step1602]: loss 0.842955
[epoch2, step1603]: loss 0.860879
[epoch2, step1604]: loss 1.044562
[epoch2, step1605]: loss 0.981956
[epoch2, step1606]: loss 1.004717
[epoch2, step1607]: loss 0.629685
[epoch2, step1608]: loss 0.859330
[epoch2, step1609]: loss 0.986725
[epoch2, step1610]: loss 0.913736
[epoch2, step1611]: loss 0.871487
[epoch2, step1612]: loss 0.542859
[epoch2, step1613]: loss 0.825738
[epoch2, step1614]: loss 0.679679
[epoch2, step1615]: loss 0.771264
[epoch2, step1616]: loss 0.738328
[epoch2, step1617]: loss 0.699854
[epoch2, step1618]: loss 0.758923
[epoch2, step1619]: loss 1.276237
[epoch2, step1620]: loss 0.971954
[epoch2, step1621]: loss 0.820971
[epoch2, step1622]: loss 0.617195
[epoch2, step1623]: loss 1.075608
[epoch2, step1624]: loss 0.847832
[epoch2, step1625]: loss 0.944427
[epoch2, step1626]: loss 0.789308
[epoch2, step1627]: loss 0.728816
[epoch2, step1628]: loss 0.448631
[epoch2, step1629]: loss 0.432786
[epoch2, step1630]: loss 0.595098
[epoch2, step1631]: loss 0.751971
[epoch2, step1632]: loss 0.779009
[epoch2, step1633]: loss 0.855165
[epoch2, step1634]: loss 0.805876
[epoch2, step1635]: loss 0.835173
[epoch2, step1636]: loss 0.713431
[epoch2, step1637]: loss 0.975941
[epoch2, step1638]: loss 0.823667
[epoch2, step1639]: loss 0.945081
[epoch2, step1640]: loss 0.780343
[epoch2, step1641]: loss 0.655482
[epoch2, step1642]: loss 0.784526
[epoch2, step1643]: loss 1.151729
[epoch2, step1644]: loss 0.770580
[epoch2, step1645]: loss 0.823153
[epoch2, step1646]: loss 0.879939
[epoch2, step1647]: loss 0.765593
[epoch2, step1648]: loss 0.761164
[epoch2, step1649]: loss 0.702692
[epoch2, step1650]: loss 0.850157
[epoch2, step1651]: loss 0.875112
[epoch2, step1652]: loss 0.555710
[epoch2, step1653]: loss 1.024027
[epoch2, step1654]: loss 1.000025
[epoch2, step1655]: loss 0.706107
[epoch2, step1656]: loss 0.867430
[epoch2, step1657]: loss 1.141667
[epoch2, step1658]: loss 0.778236
[epoch2, step1659]: loss 0.888499
[epoch2, step1660]: loss 0.750368
[epoch2, step1661]: loss 0.790777
[epoch2, step1662]: loss 0.863065
[epoch2, step1663]: loss 0.894462
[epoch2, step1664]: loss 1.097820
[epoch2, step1665]: loss 0.838001
[epoch2, step1666]: loss 0.782881
[epoch2, step1667]: loss 0.893900
[epoch2, step1668]: loss 0.875043
[epoch2, step1669]: loss 0.986046
[epoch2, step1670]: loss 0.950822
[epoch2, step1671]: loss 0.780840
[epoch2, step1672]: loss 0.454911
[epoch2, step1673]: loss 0.806567
[epoch2, step1674]: loss 0.991796
[epoch2, step1675]: loss 1.037744
[epoch2, step1676]: loss 0.622747
[epoch2, step1677]: loss 0.928457
[epoch2, step1678]: loss 0.840273
[epoch2, step1679]: loss 0.746447
[epoch2, step1680]: loss 0.842078
[epoch2, step1681]: loss 0.886032
[epoch2, step1682]: loss 0.590033
[epoch2, step1683]: loss 1.070977
[epoch2, step1684]: loss 0.703417
[epoch2, step1685]: loss 0.863370
[epoch2, step1686]: loss 0.970217
[epoch2, step1687]: loss 0.865415
[epoch2, step1688]: loss 0.940193
[epoch2, step1689]: loss 1.032987
[epoch2, step1690]: loss 0.830571
[epoch2, step1691]: loss 0.871803
[epoch2, step1692]: loss 0.815210
[epoch2, step1693]: loss 0.856518
[epoch2, step1694]: loss 0.786661
[epoch2, step1695]: loss 1.030192
[epoch2, step1696]: loss 0.864202
[epoch2, step1697]: loss 0.639101
[epoch2, step1698]: loss 0.627597
[epoch2, step1699]: loss 0.713231
[epoch2, step1700]: loss 0.924752
[epoch2, step1701]: loss 0.776039
[epoch2, step1702]: loss 0.969176
[epoch2, step1703]: loss 0.627169
[epoch2, step1704]: loss 0.972579
[epoch2, step1705]: loss 0.815413
[epoch2, step1706]: loss 0.924969
[epoch2, step1707]: loss 0.840479
[epoch2, step1708]: loss 0.750546
[epoch2, step1709]: loss 1.086113
[epoch2, step1710]: loss 0.931977
[epoch2, step1711]: loss 0.877877
[epoch2, step1712]: loss 0.900913
[epoch2, step1713]: loss 0.463716
[epoch2, step1714]: loss 1.009157
[epoch2, step1715]: loss 0.866252
[epoch2, step1716]: loss 0.840795
[epoch2, step1717]: loss 1.013916
[epoch2, step1718]: loss 0.874958
[epoch2, step1719]: loss 1.081317
[epoch2, step1720]: loss 1.010817
[epoch2, step1721]: loss 0.920611
[epoch2, step1722]: loss 0.782102
[epoch2, step1723]: loss 0.778744
[epoch2, step1724]: loss 0.698660
[epoch2, step1725]: loss 0.989363
[epoch2, step1726]: loss 0.750020
[epoch2, step1727]: loss 0.806496
[epoch2, step1728]: loss 0.817521
[epoch2, step1729]: loss 0.435663
[epoch2, step1730]: loss 0.850352
[epoch2, step1731]: loss 0.745230
[epoch2, step1732]: loss 0.812508
[epoch2, step1733]: loss 0.672095
[epoch2, step1734]: loss 0.583063
[epoch2, step1735]: loss 0.699043
[epoch2, step1736]: loss 0.814151
[epoch2, step1737]: loss 0.675339
[epoch2, step1738]: loss 0.833054
[epoch2, step1739]: loss 0.629111
[epoch2, step1740]: loss 0.786082
[epoch2, step1741]: loss 0.636789
[epoch2, step1742]: loss 0.687417
[epoch2, step1743]: loss 0.925703
[epoch2, step1744]: loss 0.725093
[epoch2, step1745]: loss 0.877965
[epoch2, step1746]: loss 0.711663
[epoch2, step1747]: loss 0.712589
[epoch2, step1748]: loss 0.749952
[epoch2, step1749]: loss 0.695949
[epoch2, step1750]: loss 0.874624
[epoch2, step1751]: loss 0.729180
[epoch2, step1752]: loss 1.059257
[epoch2, step1753]: loss 0.936662
[epoch2, step1754]: loss 0.950633
[epoch2, step1755]: loss 0.860352
[epoch2, step1756]: loss 0.821255
[epoch2, step1757]: loss 0.720999
[epoch2, step1758]: loss 0.736196
[epoch2, step1759]: loss 0.838849
[epoch2, step1760]: loss 0.964117
[epoch2, step1761]: loss 0.626590
[epoch2, step1762]: loss 0.803359
[epoch2, step1763]: loss 0.737947
[epoch2, step1764]: loss 0.793883
[epoch2, step1765]: loss 0.777665
[epoch2, step1766]: loss 0.914900
[epoch2, step1767]: loss 0.767107
[epoch2, step1768]: loss 0.725392
[epoch2, step1769]: loss 0.665930
[epoch2, step1770]: loss 0.654468
[epoch2, step1771]: loss 1.021728
[epoch2, step1772]: loss 0.848319
[epoch2, step1773]: loss 0.626521
[epoch2, step1774]: loss 0.886217
[epoch2, step1775]: loss 0.596505
[epoch2, step1776]: loss 0.697186
[epoch2, step1777]: loss 0.890853
[epoch2, step1778]: loss 0.765902
[epoch2, step1779]: loss 0.831230
[epoch2, step1780]: loss 0.645254
[epoch2, step1781]: loss 0.868616
[epoch2, step1782]: loss 1.015321
[epoch2, step1783]: loss 1.021793
[epoch2, step1784]: loss 0.544418
[epoch2, step1785]: loss 0.949495
[epoch2, step1786]: loss 0.877389
[epoch2, step1787]: loss 1.183532
[epoch2, step1788]: loss 0.944560
[epoch2, step1789]: loss 1.033605
[epoch2, step1790]: loss 0.951094
[epoch2, step1791]: loss 0.967028
[epoch2, step1792]: loss 0.962431
[epoch2, step1793]: loss 0.632380
[epoch2, step1794]: loss 0.559334
[epoch2, step1795]: loss 0.877499
[epoch2, step1796]: loss 0.679970
[epoch2, step1797]: loss 0.937174
[epoch2, step1798]: loss 0.747625
[epoch2, step1799]: loss 0.876896
[epoch2, step1800]: loss 0.716381
[epoch2, step1801]: loss 0.859385
[epoch2, step1802]: loss 1.013526
[epoch2, step1803]: loss 0.710279
[epoch2, step1804]: loss 0.814473
[epoch2, step1805]: loss 0.839057
[epoch2, step1806]: loss 0.705127
[epoch2, step1807]: loss 0.822527
[epoch2, step1808]: loss 1.044499
[epoch2, step1809]: loss 0.977336
[epoch2, step1810]: loss 0.976873
[epoch2, step1811]: loss 0.592448
[epoch2, step1812]: loss 0.691277
[epoch2, step1813]: loss 0.823839
[epoch2, step1814]: loss 0.833835
[epoch2, step1815]: loss 0.538912
[epoch2, step1816]: loss 0.915490
[epoch2, step1817]: loss 0.616500
[epoch2, step1818]: loss 0.813719
[epoch2, step1819]: loss 0.833032
[epoch2, step1820]: loss 0.782754
[epoch2, step1821]: loss 0.859647
[epoch2, step1822]: loss 0.772161
[epoch2, step1823]: loss 0.846782
[epoch2, step1824]: loss 0.596326
[epoch2, step1825]: loss 0.986779
[epoch2, step1826]: loss 0.986889
[epoch2, step1827]: loss 0.963894
[epoch2, step1828]: loss 0.456479
[epoch2, step1829]: loss 0.583534
[epoch2, step1830]: loss 0.744010
[epoch2, step1831]: loss 0.844413
[epoch2, step1832]: loss 0.960537
[epoch2, step1833]: loss 0.934857
[epoch2, step1834]: loss 0.667920
[epoch2, step1835]: loss 0.972330
[epoch2, step1836]: loss 0.723504
[epoch2, step1837]: loss 0.811471
[epoch2, step1838]: loss 0.971537
[epoch2, step1839]: loss 0.756638
[epoch2, step1840]: loss 0.742883
[epoch2, step1841]: loss 0.974090
[epoch2, step1842]: loss 0.837259
[epoch2, step1843]: loss 0.887955
[epoch2, step1844]: loss 0.424717
[epoch2, step1845]: loss 0.715071
[epoch2, step1846]: loss 0.377893
[epoch2, step1847]: loss 1.269881
[epoch2, step1848]: loss 1.002935
[epoch2, step1849]: loss 0.774669
[epoch2, step1850]: loss 0.458262
[epoch2, step1851]: loss 0.726438
[epoch2, step1852]: loss 0.947367
[epoch2, step1853]: loss 0.978255
[epoch2, step1854]: loss 0.943678
[epoch2, step1855]: loss 0.868425
[epoch2, step1856]: loss 0.739668
[epoch2, step1857]: loss 0.756351
[epoch2, step1858]: loss 0.615757
[epoch2, step1859]: loss 1.088920
[epoch2, step1860]: loss 0.610339
[epoch2, step1861]: loss 0.798792
[epoch2, step1862]: loss 0.836550
[epoch2, step1863]: loss 0.524316
[epoch2, step1864]: loss 0.596721
[epoch2, step1865]: loss 0.679781
[epoch2, step1866]: loss 0.657246
[epoch2, step1867]: loss 1.104213
[epoch2, step1868]: loss 0.884656
[epoch2, step1869]: loss 0.751445
[epoch2, step1870]: loss 0.684420
[epoch2, step1871]: loss 1.185697
[epoch2, step1872]: loss 0.684077
[epoch2, step1873]: loss 0.809172
[epoch2, step1874]: loss 0.824235
[epoch2, step1875]: loss 1.017860
[epoch2, step1876]: loss 0.683988
[epoch2, step1877]: loss 0.796245
[epoch2, step1878]: loss 0.978282
[epoch2, step1879]: loss 0.814638
[epoch2, step1880]: loss 1.007153
[epoch2, step1881]: loss 0.848362
[epoch2, step1882]: loss 0.516331
[epoch2, step1883]: loss 0.960495
[epoch2, step1884]: loss 0.776597
[epoch2, step1885]: loss 1.024448
[epoch2, step1886]: loss 1.155524
[epoch2, step1887]: loss 0.695483
[epoch2, step1888]: loss 0.865602
[epoch2, step1889]: loss 0.727856
[epoch2, step1890]: loss 0.792210
[epoch2, step1891]: loss 0.489029
[epoch2, step1892]: loss 0.739053
[epoch2, step1893]: loss 0.717642
[epoch2, step1894]: loss 0.789158
[epoch2, step1895]: loss 0.873709
[epoch2, step1896]: loss 0.505714
[epoch2, step1897]: loss 0.815122
[epoch2, step1898]: loss 0.788917
[epoch2, step1899]: loss 1.004679
[epoch2, step1900]: loss 0.815157
[epoch2, step1901]: loss 0.879712
[epoch2, step1902]: loss 0.652780
[epoch2, step1903]: loss 0.492740
[epoch2, step1904]: loss 0.830159
[epoch2, step1905]: loss 0.976155
[epoch2, step1906]: loss 0.846902
[epoch2, step1907]: loss 0.744276
[epoch2, step1908]: loss 0.792635
[epoch2, step1909]: loss 0.735853
[epoch2, step1910]: loss 1.012473
[epoch2, step1911]: loss 0.751377
[epoch2, step1912]: loss 0.855173
[epoch2, step1913]: loss 0.952072
[epoch2, step1914]: loss 0.861828
[epoch2, step1915]: loss 0.851375
[epoch2, step1916]: loss 0.752132
[epoch2, step1917]: loss 0.756479
[epoch2, step1918]: loss 0.665940
[epoch2, step1919]: loss 0.416736
[epoch2, step1920]: loss 1.081499
[epoch2, step1921]: loss 0.756027
[epoch2, step1922]: loss 0.930557
[epoch2, step1923]: loss 0.914216
[epoch2, step1924]: loss 0.849425
[epoch2, step1925]: loss 1.049535
[epoch2, step1926]: loss 0.602083
[epoch2, step1927]: loss 0.984694
[epoch2, step1928]: loss 0.589661
[epoch2, step1929]: loss 0.984688
[epoch2, step1930]: loss 0.757492
[epoch2, step1931]: loss 0.772683
[epoch2, step1932]: loss 0.693629
[epoch2, step1933]: loss 1.066747
[epoch2, step1934]: loss 0.942547
[epoch2, step1935]: loss 0.802916
[epoch2, step1936]: loss 0.992926
[epoch2, step1937]: loss 1.013530
[epoch2, step1938]: loss 0.930977
[epoch2, step1939]: loss 0.872868
[epoch2, step1940]: loss 1.016140
[epoch2, step1941]: loss 1.089609
[epoch2, step1942]: loss 0.539206
[epoch2, step1943]: loss 0.764357
[epoch2, step1944]: loss 0.664873
[epoch2, step1945]: loss 0.898045
[epoch2, step1946]: loss 0.796137
[epoch2, step1947]: loss 0.752528
[epoch2, step1948]: loss 0.748963
[epoch2, step1949]: loss 0.823483
[epoch2, step1950]: loss 0.837314
[epoch2, step1951]: loss 0.476446
[epoch2, step1952]: loss 0.899971
[epoch2, step1953]: loss 0.584297
[epoch2, step1954]: loss 0.953851
[epoch2, step1955]: loss 1.015193
[epoch2, step1956]: loss 0.813213
[epoch2, step1957]: loss 0.916265
[epoch2, step1958]: loss 0.679019
[epoch2, step1959]: loss 0.904951
[epoch2, step1960]: loss 0.527709
[epoch2, step1961]: loss 0.831900
[epoch2, step1962]: loss 0.840390
[epoch2, step1963]: loss 0.764797
[epoch2, step1964]: loss 0.935251
[epoch2, step1965]: loss 0.789362
[epoch2, step1966]: loss 0.518674
[epoch2, step1967]: loss 0.863598
[epoch2, step1968]: loss 0.312294
[epoch2, step1969]: loss 0.903535
[epoch2, step1970]: loss 0.779927
[epoch2, step1971]: loss 1.039853
[epoch2, step1972]: loss 0.718823
[epoch2, step1973]: loss 0.548149
[epoch2, step1974]: loss 0.804700
[epoch2, step1975]: loss 0.627364
[epoch2, step1976]: loss 0.779637
[epoch2, step1977]: loss 0.709380
[epoch2, step1978]: loss 0.938725
[epoch2, step1979]: loss 0.785453
[epoch2, step1980]: loss 0.893133
[epoch2, step1981]: loss 0.607966
[epoch2, step1982]: loss 0.872735
[epoch2, step1983]: loss 0.622462
[epoch2, step1984]: loss 0.912479
[epoch2, step1985]: loss 0.792640
[epoch2, step1986]: loss 0.720926
[epoch2, step1987]: loss 0.819564
[epoch2, step1988]: loss 0.770875
[epoch2, step1989]: loss 0.689546
[epoch2, step1990]: loss 0.962643
[epoch2, step1991]: loss 0.759360
[epoch2, step1992]: loss 1.087154
[epoch2, step1993]: loss 0.912680
[epoch2, step1994]: loss 0.525313
[epoch2, step1995]: loss 0.956267
[epoch2, step1996]: loss 0.741073
[epoch2, step1997]: loss 0.861749
[epoch2, step1998]: loss 0.774171
[epoch2, step1999]: loss 0.946737
[epoch2, step2000]: loss 0.927862
[epoch2, step2001]: loss 0.516080
[epoch2, step2002]: loss 0.860972
[epoch2, step2003]: loss 0.649061
[epoch2, step2004]: loss 1.018150
[epoch2, step2005]: loss 0.966015
[epoch2, step2006]: loss 0.738222
[epoch2, step2007]: loss 0.705895
[epoch2, step2008]: loss 0.720208
[epoch2, step2009]: loss 0.848718
[epoch2, step2010]: loss 0.649291
[epoch2, step2011]: loss 0.781582
[epoch2, step2012]: loss 1.112044
[epoch2, step2013]: loss 0.751138
[epoch2, step2014]: loss 0.663997
[epoch2, step2015]: loss 0.726596
[epoch2, step2016]: loss 1.069239
[epoch2, step2017]: loss 0.853986
[epoch2, step2018]: loss 1.009554
[epoch2, step2019]: loss 0.862357
[epoch2, step2020]: loss 0.771367
[epoch2, step2021]: loss 0.729127
[epoch2, step2022]: loss 0.944710
[epoch2, step2023]: loss 1.022419
[epoch2, step2024]: loss 0.872058
[epoch2, step2025]: loss 0.713647
[epoch2, step2026]: loss 0.657547
[epoch2, step2027]: loss 0.856820
[epoch2, step2028]: loss 1.011851
[epoch2, step2029]: loss 0.527670
[epoch2, step2030]: loss 0.879708
[epoch2, step2031]: loss 0.684552
[epoch2, step2032]: loss 0.839557
[epoch2, step2033]: loss 1.120627
[epoch2, step2034]: loss 0.893819
[epoch2, step2035]: loss 0.618037
[epoch2, step2036]: loss 0.936481
[epoch2, step2037]: loss 0.739705
[epoch2, step2038]: loss 0.940687
[epoch2, step2039]: loss 1.077807
[epoch2, step2040]: loss 0.648861
[epoch2, step2041]: loss 0.776876
[epoch2, step2042]: loss 0.626751
[epoch2, step2043]: loss 0.814978
[epoch2, step2044]: loss 0.657834
[epoch2, step2045]: loss 0.873363
[epoch2, step2046]: loss 0.781233
[epoch2, step2047]: loss 1.031377
[epoch2, step2048]: loss 0.745620
[epoch2, step2049]: loss 0.690132
[epoch2, step2050]: loss 0.764409
[epoch2, step2051]: loss 0.695329
[epoch2, step2052]: loss 0.882485
[epoch2, step2053]: loss 0.855903
[epoch2, step2054]: loss 0.484100
[epoch2, step2055]: loss 0.984076
[epoch2, step2056]: loss 0.891918
[epoch2, step2057]: loss 0.987245
[epoch2, step2058]: loss 0.843712
[epoch2, step2059]: loss 1.017804
[epoch2, step2060]: loss 0.865974
[epoch2, step2061]: loss 0.654951
[epoch2, step2062]: loss 0.542865
[epoch2, step2063]: loss 0.903554
[epoch2, step2064]: loss 0.673805
[epoch2, step2065]: loss 0.597130
[epoch2, step2066]: loss 0.904562
[epoch2, step2067]: loss 0.988480
[epoch2, step2068]: loss 0.968660
[epoch2, step2069]: loss 0.734879
[epoch2, step2070]: loss 0.596411
[epoch2, step2071]: loss 0.541079
[epoch2, step2072]: loss 1.007420
[epoch2, step2073]: loss 0.907586
[epoch2, step2074]: loss 1.086017
[epoch2, step2075]: loss 0.669462
[epoch2, step2076]: loss 0.815768
[epoch2, step2077]: loss 0.670006
[epoch2, step2078]: loss 0.746577
[epoch2, step2079]: loss 0.569960
[epoch2, step2080]: loss 1.088337
[epoch2, step2081]: loss 0.996582
[epoch2, step2082]: loss 0.862783
[epoch2, step2083]: loss 0.507945
[epoch2, step2084]: loss 0.833885
[epoch2, step2085]: loss 0.896511
[epoch2, step2086]: loss 0.632821
[epoch2, step2087]: loss 0.844438
[epoch2, step2088]: loss 0.873898
[epoch2, step2089]: loss 1.031278
[epoch2, step2090]: loss 0.750319
[epoch2, step2091]: loss 0.577189
[epoch2, step2092]: loss 1.030950
[epoch2, step2093]: loss 1.065455
[epoch2, step2094]: loss 1.002214
[epoch2, step2095]: loss 0.812707
[epoch2, step2096]: loss 0.707971
[epoch2, step2097]: loss 0.800977
[epoch2, step2098]: loss 0.957944
[epoch2, step2099]: loss 0.700569
[epoch2, step2100]: loss 1.047546
[epoch2, step2101]: loss 0.928868
[epoch2, step2102]: loss 0.979504
[epoch2, step2103]: loss 0.656764
[epoch2, step2104]: loss 1.021567
[epoch2, step2105]: loss 0.684616
[epoch2, step2106]: loss 0.835034
[epoch2, step2107]: loss 0.601050
[epoch2, step2108]: loss 0.716146
[epoch2, step2109]: loss 0.594849
[epoch2, step2110]: loss 0.833392
[epoch2, step2111]: loss 1.005435
[epoch2, step2112]: loss 0.451895
[epoch2, step2113]: loss 0.927809
[epoch2, step2114]: loss 0.588952
[epoch2, step2115]: loss 0.753051
[epoch2, step2116]: loss 0.651348
[epoch2, step2117]: loss 0.775142
[epoch2, step2118]: loss 0.982882
[epoch2, step2119]: loss 1.026889
[epoch2, step2120]: loss 0.833427
[epoch2, step2121]: loss 1.106535
[epoch2, step2122]: loss 0.785134
[epoch2, step2123]: loss 0.675046
[epoch2, step2124]: loss 0.779551
[epoch2, step2125]: loss 0.750547
[epoch2, step2126]: loss 0.810960
[epoch2, step2127]: loss 1.003540
[epoch2, step2128]: loss 0.858464
[epoch2, step2129]: loss 0.984377
[epoch2, step2130]: loss 0.870171
[epoch2, step2131]: loss 0.537788
[epoch2, step2132]: loss 0.552182
[epoch2, step2133]: loss 0.829144
[epoch2, step2134]: loss 0.736535
[epoch2, step2135]: loss 0.831608
[epoch2, step2136]: loss 0.662446
[epoch2, step2137]: loss 0.615326
[epoch2, step2138]: loss 0.659283
[epoch2, step2139]: loss 1.111688
[epoch2, step2140]: loss 0.587580
[epoch2, step2141]: loss 0.761185
[epoch2, step2142]: loss 0.739580
[epoch2, step2143]: loss 0.716794
[epoch2, step2144]: loss 0.861271
[epoch2, step2145]: loss 1.053027
[epoch2, step2146]: loss 0.925636
[epoch2, step2147]: loss 1.061761
[epoch2, step2148]: loss 0.617167
[epoch2, step2149]: loss 0.982693
[epoch2, step2150]: loss 0.442688
[epoch2, step2151]: loss 0.887158
[epoch2, step2152]: loss 0.862185
[epoch2, step2153]: loss 0.741598
[epoch2, step2154]: loss 0.490489
[epoch2, step2155]: loss 0.617169
[epoch2, step2156]: loss 0.979862
[epoch2, step2157]: loss 0.764720
[epoch2, step2158]: loss 0.930696
[epoch2, step2159]: loss 0.755341
[epoch2, step2160]: loss 1.018619
[epoch2, step2161]: loss 0.882297
[epoch2, step2162]: loss 1.049098
[epoch2, step2163]: loss 0.553384
[epoch2, step2164]: loss 0.821081
[epoch2, step2165]: loss 0.583802
[epoch2, step2166]: loss 0.962610
[epoch2, step2167]: loss 0.952859
[epoch2, step2168]: loss 0.816175
[epoch2, step2169]: loss 0.756183
[epoch2, step2170]: loss 0.833587
[epoch2, step2171]: loss 0.626086
[epoch2, step2172]: loss 1.002179
[epoch2, step2173]: loss 0.820572
[epoch2, step2174]: loss 1.140381
[epoch2, step2175]: loss 0.687854
[epoch2, step2176]: loss 0.997868
[epoch2, step2177]: loss 0.756421
[epoch2, step2178]: loss 0.789045
[epoch2, step2179]: loss 0.815379
[epoch2, step2180]: loss 0.620368
[epoch2, step2181]: loss 0.827983
[epoch2, step2182]: loss 0.739708
[epoch2, step2183]: loss 0.659284
[epoch2, step2184]: loss 0.995478
[epoch2, step2185]: loss 0.944923
[epoch2, step2186]: loss 0.773151
[epoch2, step2187]: loss 0.975366
[epoch2, step2188]: loss 0.530055
[epoch2, step2189]: loss 0.916251
[epoch2, step2190]: loss 1.122936
[epoch2, step2191]: loss 0.924660
[epoch2, step2192]: loss 0.596878
[epoch2, step2193]: loss 0.863571
[epoch2, step2194]: loss 0.744754
[epoch2, step2195]: loss 0.669112
[epoch2, step2196]: loss 0.711674
[epoch2, step2197]: loss 0.750718
[epoch2, step2198]: loss 0.901814
[epoch2, step2199]: loss 0.757754
[epoch2, step2200]: loss 0.580963
[epoch2, step2201]: loss 0.703764
[epoch2, step2202]: loss 0.835647
[epoch2, step2203]: loss 0.767760
[epoch2, step2204]: loss 0.901391
[epoch2, step2205]: loss 0.639692
[epoch2, step2206]: loss 0.800851
[epoch2, step2207]: loss 0.706345
[epoch2, step2208]: loss 1.035427
[epoch2, step2209]: loss 0.841047
[epoch2, step2210]: loss 0.603664
[epoch2, step2211]: loss 0.788743
[epoch2, step2212]: loss 0.935020
[epoch2, step2213]: loss 0.711710
[epoch2, step2214]: loss 0.885655
[epoch2, step2215]: loss 0.891189
[epoch2, step2216]: loss 0.942848
[epoch2, step2217]: loss 0.845606
[epoch2, step2218]: loss 0.605511
[epoch2, step2219]: loss 0.751491
[epoch2, step2220]: loss 0.312992
[epoch2, step2221]: loss 0.640907
[epoch2, step2222]: loss 0.919352
[epoch2, step2223]: loss 0.447610
[epoch2, step2224]: loss 0.978397
[epoch2, step2225]: loss 0.941903
[epoch2, step2226]: loss 1.044695
[epoch2, step2227]: loss 0.734689
[epoch2, step2228]: loss 0.473025
[epoch2, step2229]: loss 0.552289
[epoch2, step2230]: loss 0.488428
[epoch2, step2231]: loss 0.895787
[epoch2, step2232]: loss 0.526681
[epoch2, step2233]: loss 0.729282
[epoch2, step2234]: loss 0.497181
[epoch2, step2235]: loss 0.875873
[epoch2, step2236]: loss 0.547227
[epoch2, step2237]: loss 0.899757
[epoch2, step2238]: loss 0.829176
[epoch2, step2239]: loss 0.901615
[epoch2, step2240]: loss 0.775855
[epoch2, step2241]: loss 0.769831
[epoch2, step2242]: loss 0.614239
[epoch2, step2243]: loss 0.764087
[epoch2, step2244]: loss 0.864153
[epoch2, step2245]: loss 0.887308
[epoch2, step2246]: loss 1.163456
[epoch2, step2247]: loss 0.600665
[epoch2, step2248]: loss 0.851335
[epoch2, step2249]: loss 0.880394
[epoch2, step2250]: loss 0.955945
[epoch2, step2251]: loss 0.598590
[epoch2, step2252]: loss 0.823423
[epoch2, step2253]: loss 0.743975
[epoch2, step2254]: loss 0.679532
[epoch2, step2255]: loss 0.788703
[epoch2, step2256]: loss 0.819586
[epoch2, step2257]: loss 0.793953
[epoch2, step2258]: loss 0.779324
[epoch2, step2259]: loss 1.012013
[epoch2, step2260]: loss 0.767364
[epoch2, step2261]: loss 0.702123
[epoch2, step2262]: loss 0.872923
[epoch2, step2263]: loss 0.959202
[epoch2, step2264]: loss 0.745219
[epoch2, step2265]: loss 1.028905
[epoch2, step2266]: loss 0.754760
[epoch2, step2267]: loss 0.940483
[epoch2, step2268]: loss 0.735332
[epoch2, step2269]: loss 0.914810
[epoch2, step2270]: loss 0.956898
[epoch2, step2271]: loss 0.726508
[epoch2, step2272]: loss 0.927456
[epoch2, step2273]: loss 1.054121
[epoch2, step2274]: loss 0.327437
[epoch2, step2275]: loss 0.891214
[epoch2, step2276]: loss 0.908269
[epoch2, step2277]: loss 0.613055
[epoch2, step2278]: loss 0.946959
[epoch2, step2279]: loss 0.860050
[epoch2, step2280]: loss 0.914735
[epoch2, step2281]: loss 0.834158
[epoch2, step2282]: loss 0.781604
[epoch2, step2283]: loss 0.697879
[epoch2, step2284]: loss 0.249935
[epoch2, step2285]: loss 1.132913
[epoch2, step2286]: loss 0.607653
[epoch2, step2287]: loss 0.844458
[epoch2, step2288]: loss 0.931848
[epoch2, step2289]: loss 0.880062
[epoch2, step2290]: loss 0.808291
[epoch2, step2291]: loss 0.963816
[epoch2, step2292]: loss 0.861829
[epoch2, step2293]: loss 0.650976
[epoch2, step2294]: loss 0.537912
[epoch2, step2295]: loss 0.604056
[epoch2, step2296]: loss 0.547835
[epoch2, step2297]: loss 1.108171
[epoch2, step2298]: loss 0.850094
[epoch2, step2299]: loss 0.935755
[epoch2, step2300]: loss 0.759716
[epoch2, step2301]: loss 0.740883
[epoch2, step2302]: loss 0.619477
[epoch2, step2303]: loss 0.754461
[epoch2, step2304]: loss 0.723189
[epoch2, step2305]: loss 0.770489
[epoch2, step2306]: loss 0.592902
[epoch2, step2307]: loss 0.670802
[epoch2, step2308]: loss 0.782024
[epoch2, step2309]: loss 0.799031
[epoch2, step2310]: loss 0.858006
[epoch2, step2311]: loss 0.817696
[epoch2, step2312]: loss 0.820951
[epoch2, step2313]: loss 0.731186
[epoch2, step2314]: loss 0.816188
[epoch2, step2315]: loss 0.780827
[epoch2, step2316]: loss 0.647370
[epoch2, step2317]: loss 0.789768
[epoch2, step2318]: loss 0.714313
[epoch2, step2319]: loss 1.020013
[epoch2, step2320]: loss 0.760112
[epoch2, step2321]: loss 0.660321
[epoch2, step2322]: loss 0.977264
[epoch2, step2323]: loss 0.919140
[epoch2, step2324]: loss 0.983120
[epoch2, step2325]: loss 0.695496
[epoch2, step2326]: loss 0.757970
[epoch2, step2327]: loss 0.829239
[epoch2, step2328]: loss 0.807764
[epoch2, step2329]: loss 0.979063
[epoch2, step2330]: loss 0.672740
[epoch2, step2331]: loss 1.019312
[epoch2, step2332]: loss 0.755104
[epoch2, step2333]: loss 0.693213
[epoch2, step2334]: loss 0.591983
[epoch2, step2335]: loss 0.780319
[epoch2, step2336]: loss 0.894356
[epoch2, step2337]: loss 0.781422
[epoch2, step2338]: loss 0.994586
[epoch2, step2339]: loss 0.624862
[epoch2, step2340]: loss 0.787170
[epoch2, step2341]: loss 0.754984
[epoch2, step2342]: loss 0.977345
[epoch2, step2343]: loss 0.838358
[epoch2, step2344]: loss 0.904881
[epoch2, step2345]: loss 0.797278
[epoch2, step2346]: loss 0.745872
[epoch2, step2347]: loss 0.869219
[epoch2, step2348]: loss 0.827140
[epoch2, step2349]: loss 0.982890
[epoch2, step2350]: loss 0.716665
[epoch2, step2351]: loss 0.557925
[epoch2, step2352]: loss 0.541703
[epoch2, step2353]: loss 0.922960
[epoch2, step2354]: loss 0.672120
[epoch2, step2355]: loss 0.884569
[epoch2, step2356]: loss 0.923397
[epoch2, step2357]: loss 0.795230
[epoch2, step2358]: loss 0.956775
[epoch2, step2359]: loss 0.932503
[epoch2, step2360]: loss 0.783097
[epoch2, step2361]: loss 0.840106
[epoch2, step2362]: loss 0.811562
[epoch2, step2363]: loss 0.720735
[epoch2, step2364]: loss 0.770190
[epoch2, step2365]: loss 0.653818
[epoch2, step2366]: loss 0.725411
[epoch2, step2367]: loss 0.724342
[epoch2, step2368]: loss 0.674596
[epoch2, step2369]: loss 0.877254
[epoch2, step2370]: loss 0.957282
[epoch2, step2371]: loss 0.776763
[epoch2, step2372]: loss 0.966492
[epoch2, step2373]: loss 0.565650
[epoch2, step2374]: loss 0.655400
[epoch2, step2375]: loss 0.630164
[epoch2, step2376]: loss 0.826574
[epoch2, step2377]: loss 0.702438
[epoch2, step2378]: loss 0.564612
[epoch2, step2379]: loss 0.774581
[epoch2, step2380]: loss 0.675741
[epoch2, step2381]: loss 0.696157
[epoch2, step2382]: loss 0.882839
[epoch2, step2383]: loss 0.807795
[epoch2, step2384]: loss 0.795282
[epoch2, step2385]: loss 0.958774
[epoch2, step2386]: loss 0.783220
[epoch2, step2387]: loss 0.648862
[epoch2, step2388]: loss 0.690224
[epoch2, step2389]: loss 0.985467
[epoch2, step2390]: loss 0.776904
[epoch2, step2391]: loss 0.722870
[epoch2, step2392]: loss 0.808984
[epoch2, step2393]: loss 0.826732
[epoch2, step2394]: loss 0.872044
[epoch2, step2395]: loss 0.504494
[epoch2, step2396]: loss 0.880678
[epoch2, step2397]: loss 0.677288
[epoch2, step2398]: loss 0.874517
[epoch2, step2399]: loss 0.540256
[epoch2, step2400]: loss 0.802670
[epoch2, step2401]: loss 0.649803
[epoch2, step2402]: loss 0.864572
[epoch2, step2403]: loss 0.741481
[epoch2, step2404]: loss 0.428371
[epoch2, step2405]: loss 0.749891
[epoch2, step2406]: loss 0.720073
[epoch2, step2407]: loss 0.892020
[epoch2, step2408]: loss 0.751254
[epoch2, step2409]: loss 0.794214
[epoch2, step2410]: loss 0.690985
[epoch2, step2411]: loss 0.641735
[epoch2, step2412]: loss 0.715572
[epoch2, step2413]: loss 0.547268
[epoch2, step2414]: loss 0.851874
[epoch2, step2415]: loss 0.908678
[epoch2, step2416]: loss 0.715369
[epoch2, step2417]: loss 0.835585
[epoch2, step2418]: loss 0.617351
[epoch2, step2419]: loss 0.862913
[epoch2, step2420]: loss 0.582921
[epoch2, step2421]: loss 0.881149
[epoch2, step2422]: loss 0.825796
[epoch2, step2423]: loss 0.815681
[epoch2, step2424]: loss 0.718893
[epoch2, step2425]: loss 0.998774
[epoch2, step2426]: loss 0.811433
[epoch2, step2427]: loss 0.892635
[epoch2, step2428]: loss 0.627823
[epoch2, step2429]: loss 0.622809
[epoch2, step2430]: loss 0.750388
[epoch2, step2431]: loss 0.874393
[epoch2, step2432]: loss 0.650438
[epoch2, step2433]: loss 0.710704
[epoch2, step2434]: loss 0.846111
[epoch2, step2435]: loss 0.779980
[epoch2, step2436]: loss 0.703411
[epoch2, step2437]: loss 0.750632
[epoch2, step2438]: loss 0.833979
[epoch2, step2439]: loss 0.626879
[epoch2, step2440]: loss 0.665925
[epoch2, step2441]: loss 1.037591
[epoch2, step2442]: loss 0.974695
[epoch2, step2443]: loss 0.844404
[epoch2, step2444]: loss 0.916184
[epoch2, step2445]: loss 0.825113
[epoch2, step2446]: loss 0.961123
[epoch2, step2447]: loss 0.864041
[epoch2, step2448]: loss 0.710267
[epoch2, step2449]: loss 0.731654
[epoch2, step2450]: loss 0.779577
[epoch2, step2451]: loss 0.703308
[epoch2, step2452]: loss 0.784427
[epoch2, step2453]: loss 0.992463
[epoch2, step2454]: loss 0.957217
[epoch2, step2455]: loss 0.733387
[epoch2, step2456]: loss 0.704474
[epoch2, step2457]: loss 0.786789
[epoch2, step2458]: loss 0.625837
[epoch2, step2459]: loss 0.894620
[epoch2, step2460]: loss 0.457797
[epoch2, step2461]: loss 0.724820
[epoch2, step2462]: loss 0.647723
[epoch2, step2463]: loss 0.632752
[epoch2, step2464]: loss 0.784986
[epoch2, step2465]: loss 0.759267
[epoch2, step2466]: loss 0.806273
[epoch2, step2467]: loss 0.801367
[epoch2, step2468]: loss 0.553318
[epoch2, step2469]: loss 0.672136
[epoch2, step2470]: loss 0.603195
[epoch2, step2471]: loss 0.700482
[epoch2, step2472]: loss 0.392609
[epoch2, step2473]: loss 0.990429
[epoch2, step2474]: loss 1.055950
[epoch2, step2475]: loss 0.614327
[epoch2, step2476]: loss 0.740355
[epoch2, step2477]: loss 0.724207
[epoch2, step2478]: loss 0.759924
[epoch2, step2479]: loss 0.976438
[epoch2, step2480]: loss 0.429436
[epoch2, step2481]: loss 0.814755
[epoch2, step2482]: loss 0.646289
[epoch2, step2483]: loss 0.434047
[epoch2, step2484]: loss 0.553914
[epoch2, step2485]: loss 0.828666
[epoch2, step2486]: loss 0.986940
[epoch2, step2487]: loss 0.937296
[epoch2, step2488]: loss 1.037468
[epoch2, step2489]: loss 0.540782
[epoch2, step2490]: loss 0.672618
[epoch2, step2491]: loss 0.835409
[epoch2, step2492]: loss 0.715996
[epoch2, step2493]: loss 0.851602
[epoch2, step2494]: loss 0.998655
[epoch2, step2495]: loss 0.499122
[epoch2, step2496]: loss 0.746827
[epoch2, step2497]: loss 0.701829
[epoch2, step2498]: loss 0.785032
[epoch2, step2499]: loss 0.885505
[epoch2, step2500]: loss 0.708136
[epoch2, step2501]: loss 0.733897
[epoch2, step2502]: loss 0.765350
[epoch2, step2503]: loss 0.694308
[epoch2, step2504]: loss 0.694414
[epoch2, step2505]: loss 0.719505
[epoch2, step2506]: loss 1.023781
[epoch2, step2507]: loss 0.640443
[epoch2, step2508]: loss 0.796292
[epoch2, step2509]: loss 0.717927
[epoch2, step2510]: loss 0.988754
[epoch2, step2511]: loss 1.068645
[epoch2, step2512]: loss 0.792041
[epoch2, step2513]: loss 0.912187
[epoch2, step2514]: loss 0.755718
[epoch2, step2515]: loss 0.816486
[epoch2, step2516]: loss 0.581864
[epoch2, step2517]: loss 0.769101
[epoch2, step2518]: loss 0.779214
[epoch2, step2519]: loss 0.695658
[epoch2, step2520]: loss 0.859594
[epoch2, step2521]: loss 0.948974
[epoch2, step2522]: loss 0.780286
[epoch2, step2523]: loss 0.699703
[epoch2, step2524]: loss 1.037457
[epoch2, step2525]: loss 0.474581
[epoch2, step2526]: loss 0.799973
[epoch2, step2527]: loss 0.801389
[epoch2, step2528]: loss 0.886362
[epoch2, step2529]: loss 0.659773
[epoch2, step2530]: loss 0.652951
[epoch2, step2531]: loss 0.664189
[epoch2, step2532]: loss 0.908296
[epoch2, step2533]: loss 0.675653
[epoch2, step2534]: loss 0.944543
[epoch2, step2535]: loss 1.001030
[epoch2, step2536]: loss 0.510693
[epoch2, step2537]: loss 0.893327
[epoch2, step2538]: loss 0.771940
[epoch2, step2539]: loss 0.793167
[epoch2, step2540]: loss 0.633777
[epoch2, step2541]: loss 0.646702
[epoch2, step2542]: loss 0.683430
[epoch2, step2543]: loss 0.867736
[epoch2, step2544]: loss 0.607431
[epoch2, step2545]: loss 0.779296
[epoch2, step2546]: loss 0.974708
[epoch2, step2547]: loss 0.694934
[epoch2, step2548]: loss 0.860751
[epoch2, step2549]: loss 0.540298
[epoch2, step2550]: loss 0.862634
[epoch2, step2551]: loss 0.480423
[epoch2, step2552]: loss 0.966228
[epoch2, step2553]: loss 0.635536
[epoch2, step2554]: loss 0.651051
[epoch2, step2555]: loss 0.919609
[epoch2, step2556]: loss 1.044839
[epoch2, step2557]: loss 0.824689
[epoch2, step2558]: loss 0.891344
[epoch2, step2559]: loss 0.935547
[epoch2, step2560]: loss 0.790036
[epoch2, step2561]: loss 0.801981
[epoch2, step2562]: loss 0.691201
[epoch2, step2563]: loss 0.933875
[epoch2, step2564]: loss 0.923078
[epoch2, step2565]: loss 0.662313
[epoch2, step2566]: loss 0.657867
[epoch2, step2567]: loss 0.872191
[epoch2, step2568]: loss 0.512960
[epoch2, step2569]: loss 0.868116
[epoch2, step2570]: loss 0.715989
[epoch2, step2571]: loss 0.730188
[epoch2, step2572]: loss 0.532980
[epoch2, step2573]: loss 0.720892
[epoch2, step2574]: loss 0.716117
[epoch2, step2575]: loss 0.898343
[epoch2, step2576]: loss 0.813592
[epoch2, step2577]: loss 0.720954
[epoch2, step2578]: loss 1.001053
[epoch2, step2579]: loss 0.928358
[epoch2, step2580]: loss 0.843411
[epoch2, step2581]: loss 0.741380
[epoch2, step2582]: loss 0.980630
[epoch2, step2583]: loss 0.666667
[epoch2, step2584]: loss 0.642704
[epoch2, step2585]: loss 0.607052
[epoch2, step2586]: loss 0.767493
[epoch2, step2587]: loss 0.622003
[epoch2, step2588]: loss 0.843135
[epoch2, step2589]: loss 0.580866
[epoch2, step2590]: loss 1.050399
[epoch2, step2591]: loss 0.952202
[epoch2, step2592]: loss 0.837211
[epoch2, step2593]: loss 0.605942
[epoch2, step2594]: loss 0.963419
[epoch2, step2595]: loss 0.854224
[epoch2, step2596]: loss 0.861488
[epoch2, step2597]: loss 0.503449
[epoch2, step2598]: loss 0.670078
[epoch2, step2599]: loss 0.829130
[epoch2, step2600]: loss 0.783551
[epoch2, step2601]: loss 0.900153
[epoch2, step2602]: loss 0.666822
[epoch2, step2603]: loss 0.616821
[epoch2, step2604]: loss 0.695494
[epoch2, step2605]: loss 0.683519
[epoch2, step2606]: loss 0.795264
[epoch2, step2607]: loss 1.031285
[epoch2, step2608]: loss 0.628369
[epoch2, step2609]: loss 0.673434
[epoch2, step2610]: loss 0.805806
[epoch2, step2611]: loss 0.838303
[epoch2, step2612]: loss 0.709033
[epoch2, step2613]: loss 0.632565
[epoch2, step2614]: loss 0.726482
[epoch2, step2615]: loss 0.348308
[epoch2, step2616]: loss 0.789873
[epoch2, step2617]: loss 0.799858
[epoch2, step2618]: loss 0.839084
[epoch2, step2619]: loss 0.832308
[epoch2, step2620]: loss 0.728929
[epoch2, step2621]: loss 1.110792
[epoch2, step2622]: loss 0.609209
[epoch2, step2623]: loss 0.818877
[epoch2, step2624]: loss 0.758077
[epoch2, step2625]: loss 0.927550
[epoch2, step2626]: loss 0.787722
[epoch2, step2627]: loss 1.000388
[epoch2, step2628]: loss 0.459487
[epoch2, step2629]: loss 0.952187
[epoch2, step2630]: loss 0.727165
[epoch2, step2631]: loss 0.641281
[epoch2, step2632]: loss 0.681028
[epoch2, step2633]: loss 0.811692
[epoch2, step2634]: loss 0.915123
[epoch2, step2635]: loss 0.902764
[epoch2, step2636]: loss 0.737323
[epoch2, step2637]: loss 0.565874
[epoch2, step2638]: loss 0.776559
[epoch2, step2639]: loss 0.769879
[epoch2, step2640]: loss 0.991875
[epoch2, step2641]: loss 0.535983
[epoch2, step2642]: loss 0.869486
[epoch2, step2643]: loss 1.017995
[epoch2, step2644]: loss 0.694177
[epoch2, step2645]: loss 0.796366
[epoch2, step2646]: loss 0.886482
[epoch2, step2647]: loss 0.713238
[epoch2, step2648]: loss 0.862849
[epoch2, step2649]: loss 0.845297
[epoch2, step2650]: loss 0.772442
[epoch2, step2651]: loss 0.484439
[epoch2, step2652]: loss 0.656641
[epoch2, step2653]: loss 0.989301
[epoch2, step2654]: loss 0.656432
[epoch2, step2655]: loss 0.627350
[epoch2, step2656]: loss 0.405488
[epoch2, step2657]: loss 0.995770
[epoch2, step2658]: loss 0.746519
[epoch2, step2659]: loss 0.822006
[epoch2, step2660]: loss 0.940558
[epoch2, step2661]: loss 0.894604
[epoch2, step2662]: loss 0.809972
[epoch2, step2663]: loss 0.770794
[epoch2, step2664]: loss 0.770998
[epoch2, step2665]: loss 0.622600
[epoch2, step2666]: loss 0.887113
[epoch2, step2667]: loss 0.852406
[epoch2, step2668]: loss 0.920049
[epoch2, step2669]: loss 0.794535
[epoch2, step2670]: loss 0.446815
[epoch2, step2671]: loss 0.749588
[epoch2, step2672]: loss 0.663171
[epoch2, step2673]: loss 0.438607
[epoch2, step2674]: loss 0.871215
[epoch2, step2675]: loss 0.657992
[epoch2, step2676]: loss 0.847468
[epoch2, step2677]: loss 0.642726
[epoch2, step2678]: loss 0.704489
[epoch2, step2679]: loss 0.957587
[epoch2, step2680]: loss 0.949139
[epoch2, step2681]: loss 0.893894
[epoch2, step2682]: loss 0.957582
[epoch2, step2683]: loss 0.740001
[epoch2, step2684]: loss 0.491793
[epoch2, step2685]: loss 0.575071
[epoch2, step2686]: loss 0.453582
[epoch2, step2687]: loss 0.816652
[epoch2, step2688]: loss 0.926981
[epoch2, step2689]: loss 0.592807
[epoch2, step2690]: loss 0.775271
[epoch2, step2691]: loss 0.913864
[epoch2, step2692]: loss 0.752092
[epoch2, step2693]: loss 0.501770
[epoch2, step2694]: loss 0.653039
[epoch2, step2695]: loss 0.812597
[epoch2, step2696]: loss 0.814601
[epoch2, step2697]: loss 0.655225
[epoch2, step2698]: loss 0.534778
[epoch2, step2699]: loss 1.011842
[epoch2, step2700]: loss 0.618685
[epoch2, step2701]: loss 0.721782
[epoch2, step2702]: loss 0.862929
[epoch2, step2703]: loss 0.946713
[epoch2, step2704]: loss 0.899826
[epoch2, step2705]: loss 0.946725
[epoch2, step2706]: loss 0.590658
[epoch2, step2707]: loss 0.931460
[epoch2, step2708]: loss 0.700116
[epoch2, step2709]: loss 0.805301
[epoch2, step2710]: loss 0.717046
[epoch2, step2711]: loss 0.566213
[epoch2, step2712]: loss 0.910775
[epoch2, step2713]: loss 0.480840
[epoch2, step2714]: loss 1.111252
[epoch2, step2715]: loss 0.759989
[epoch2, step2716]: loss 0.724340
[epoch2, step2717]: loss 0.849359
[epoch2, step2718]: loss 0.796198
[epoch2, step2719]: loss 0.870286
[epoch2, step2720]: loss 0.878051
[epoch2, step2721]: loss 0.811148
[epoch2, step2722]: loss 0.683064
[epoch2, step2723]: loss 0.659089
[epoch2, step2724]: loss 0.719233
[epoch2, step2725]: loss 0.994387
[epoch2, step2726]: loss 0.439958
[epoch2, step2727]: loss 0.890832
[epoch2, step2728]: loss 0.932673
[epoch2, step2729]: loss 0.780980
[epoch2, step2730]: loss 0.868256
[epoch2, step2731]: loss 0.843632
[epoch2, step2732]: loss 0.858909
[epoch2, step2733]: loss 0.758789
[epoch2, step2734]: loss 0.906717
[epoch2, step2735]: loss 0.806351
[epoch2, step2736]: loss 0.866776
[epoch2, step2737]: loss 0.851113
[epoch2, step2738]: loss 0.849100
[epoch2, step2739]: loss 0.538995
[epoch2, step2740]: loss 0.846141
[epoch2, step2741]: loss 0.680273
[epoch2, step2742]: loss 0.979643
[epoch2, step2743]: loss 0.702144
[epoch2, step2744]: loss 0.974584
[epoch2, step2745]: loss 0.869867
[epoch2, step2746]: loss 0.888925
[epoch2, step2747]: loss 0.588095
[epoch2, step2748]: loss 0.922217
[epoch2, step2749]: loss 0.915406
[epoch2, step2750]: loss 0.533802
[epoch2, step2751]: loss 0.986293
[epoch2, step2752]: loss 0.484845
[epoch2, step2753]: loss 0.742731
[epoch2, step2754]: loss 0.672993
[epoch2, step2755]: loss 0.783429
[epoch2, step2756]: loss 0.717002
[epoch2, step2757]: loss 1.034057
[epoch2, step2758]: loss 0.687261
[epoch2, step2759]: loss 0.762153
[epoch2, step2760]: loss 0.571847
[epoch2, step2761]: loss 0.762514
[epoch2, step2762]: loss 0.623519
[epoch2, step2763]: loss 0.699514
[epoch2, step2764]: loss 0.904076
[epoch2, step2765]: loss 0.632891
[epoch2, step2766]: loss 0.692202
[epoch2, step2767]: loss 0.760095
[epoch2, step2768]: loss 0.859197
[epoch2, step2769]: loss 0.724895
[epoch2, step2770]: loss 0.799392
[epoch2, step2771]: loss 0.831947
[epoch2, step2772]: loss 0.616414
[epoch2, step2773]: loss 0.580165
[epoch2, step2774]: loss 0.876836
[epoch2, step2775]: loss 0.739932
[epoch2, step2776]: loss 0.704019
[epoch2, step2777]: loss 0.902449
[epoch2, step2778]: loss 0.924533
[epoch2, step2779]: loss 0.598021
[epoch2, step2780]: loss 0.652340
[epoch2, step2781]: loss 0.495703
[epoch2, step2782]: loss 0.804203
[epoch2, step2783]: loss 0.859261
[epoch2, step2784]: loss 0.907687
[epoch2, step2785]: loss 0.676401
[epoch2, step2786]: loss 0.712466
[epoch2, step2787]: loss 0.797709
[epoch2, step2788]: loss 0.643983
[epoch2, step2789]: loss 0.570291
[epoch2, step2790]: loss 0.734354
[epoch2, step2791]: loss 0.773794
[epoch2, step2792]: loss 0.944189
[epoch2, step2793]: loss 0.658405
[epoch2, step2794]: loss 0.931352
[epoch2, step2795]: loss 0.930293
[epoch2, step2796]: loss 0.809995
[epoch2, step2797]: loss 0.680820
[epoch2, step2798]: loss 0.917678
[epoch2, step2799]: loss 0.769969
[epoch2, step2800]: loss 0.839649
[epoch2, step2801]: loss 0.769673
[epoch2, step2802]: loss 0.362506
[epoch2, step2803]: loss 0.663851
[epoch2, step2804]: loss 0.749625
[epoch2, step2805]: loss 0.834897
[epoch2, step2806]: loss 0.812896
[epoch2, step2807]: loss 0.717959
[epoch2, step2808]: loss 0.732282
[epoch2, step2809]: loss 0.692720
[epoch2, step2810]: loss 0.930285
[epoch2, step2811]: loss 0.766064
[epoch2, step2812]: loss 0.981716
[epoch2, step2813]: loss 0.889579
[epoch2, step2814]: loss 0.530240
[epoch2, step2815]: loss 0.848963
[epoch2, step2816]: loss 0.922406
[epoch2, step2817]: loss 0.645905
[epoch2, step2818]: loss 0.958201
[epoch2, step2819]: loss 0.549470
[epoch2, step2820]: loss 0.638837
[epoch2, step2821]: loss 0.780220
[epoch2, step2822]: loss 0.761389
[epoch2, step2823]: loss 0.723399
[epoch2, step2824]: loss 1.007081
[epoch2, step2825]: loss 0.712459
[epoch2, step2826]: loss 0.890090
[epoch2, step2827]: loss 0.635576
[epoch2, step2828]: loss 0.516590
[epoch2, step2829]: loss 0.926743
[epoch2, step2830]: loss 0.809924
[epoch2, step2831]: loss 0.869111
[epoch2, step2832]: loss 0.815352
[epoch2, step2833]: loss 0.510715
[epoch2, step2834]: loss 1.044525
[epoch2, step2835]: loss 0.726319
[epoch2, step2836]: loss 0.800286
[epoch2, step2837]: loss 0.834372
[epoch2, step2838]: loss 0.695323
[epoch2, step2839]: loss 0.732366
[epoch2, step2840]: loss 0.799233
[epoch2, step2841]: loss 0.603890
[epoch2, step2842]: loss 0.674638
[epoch2, step2843]: loss 0.889357
[epoch2, step2844]: loss 0.785454
[epoch2, step2845]: loss 0.859111
[epoch2, step2846]: loss 0.604533
[epoch2, step2847]: loss 0.416034
[epoch2, step2848]: loss 0.625104
[epoch2, step2849]: loss 0.891662
[epoch2, step2850]: loss 0.841656
[epoch2, step2851]: loss 0.900172
[epoch2, step2852]: loss 0.962087
[epoch2, step2853]: loss 0.731977
[epoch2, step2854]: loss 0.771359
[epoch2, step2855]: loss 0.834081
[epoch2, step2856]: loss 0.749688
[epoch2, step2857]: loss 0.514087
[epoch2, step2858]: loss 0.675815
[epoch2, step2859]: loss 0.710185
[epoch2, step2860]: loss 0.947042
[epoch2, step2861]: loss 0.426229
[epoch2, step2862]: loss 0.792600
[epoch2, step2863]: loss 0.789104
[epoch2, step2864]: loss 0.715380
[epoch2, step2865]: loss 1.029147
[epoch2, step2866]: loss 0.674016
[epoch2, step2867]: loss 0.606822
[epoch2, step2868]: loss 0.808959
[epoch2, step2869]: loss 0.881871
[epoch2, step2870]: loss 0.758565
[epoch2, step2871]: loss 0.997315
[epoch2, step2872]: loss 0.887346
[epoch2, step2873]: loss 0.787039
[epoch2, step2874]: loss 0.701742
[epoch2, step2875]: loss 0.589313
[epoch2, step2876]: loss 0.594341
[epoch2, step2877]: loss 0.930164
[epoch2, step2878]: loss 0.811007
[epoch2, step2879]: loss 0.713029
[epoch2, step2880]: loss 0.312206
[epoch2, step2881]: loss 0.706990
[epoch2, step2882]: loss 0.304127
[epoch2, step2883]: loss 1.070864
[epoch2, step2884]: loss 0.510023
[epoch2, step2885]: loss 0.496009
[epoch2, step2886]: loss 0.848038
[epoch2, step2887]: loss 0.605229
[epoch2, step2888]: loss 0.729912
[epoch2, step2889]: loss 0.753389
[epoch2, step2890]: loss 0.929074
[epoch2, step2891]: loss 0.880026
[epoch2, step2892]: loss 0.637409
[epoch2, step2893]: loss 0.685829
[epoch2, step2894]: loss 0.664680
[epoch2, step2895]: loss 0.410039
[epoch2, step2896]: loss 0.732478
[epoch2, step2897]: loss 0.452405
[epoch2, step2898]: loss 0.670057
[epoch2, step2899]: loss 0.721298
[epoch2, step2900]: loss 0.723550
[epoch2, step2901]: loss 0.630421
[epoch2, step2902]: loss 0.750986
[epoch2, step2903]: loss 0.599659
[epoch2, step2904]: loss 0.742826
[epoch2, step2905]: loss 0.948641
[epoch2, step2906]: loss 0.764288
[epoch2, step2907]: loss 0.449947
[epoch2, step2908]: loss 0.341564
[epoch2, step2909]: loss 0.738565
[epoch2, step2910]: loss 0.610130
[epoch2, step2911]: loss 0.528358
[epoch2, step2912]: loss 0.679974
[epoch2, step2913]: loss 0.995401
[epoch2, step2914]: loss 0.495193
[epoch2, step2915]: loss 0.907264
[epoch2, step2916]: loss 0.772710
[epoch2, step2917]: loss 0.950080
[epoch2, step2918]: loss 0.614920
[epoch2, step2919]: loss 0.675669
[epoch2, step2920]: loss 0.882790
[epoch2, step2921]: loss 0.926798
[epoch2, step2922]: loss 0.779363
[epoch2, step2923]: loss 0.677225
[epoch2, step2924]: loss 0.427778
[epoch2, step2925]: loss 0.773839
[epoch2, step2926]: loss 0.782749
[epoch2, step2927]: loss 0.716318
[epoch2, step2928]: loss 0.909281
[epoch2, step2929]: loss 0.908729
[epoch2, step2930]: loss 0.857205
[epoch2, step2931]: loss 0.881240
[epoch2, step2932]: loss 0.791653
[epoch2, step2933]: loss 0.894786
[epoch2, step2934]: loss 0.695903
[epoch2, step2935]: loss 0.690854
[epoch2, step2936]: loss 0.984216
[epoch2, step2937]: loss 0.792439
[epoch2, step2938]: loss 0.540235
[epoch2, step2939]: loss 0.980438
[epoch2, step2940]: loss 0.633522
[epoch2, step2941]: loss 0.768282
[epoch2, step2942]: loss 0.528623
[epoch2, step2943]: loss 0.769027
[epoch2, step2944]: loss 0.896621
[epoch2, step2945]: loss 0.322313
[epoch2, step2946]: loss 0.778648
[epoch2, step2947]: loss 0.625293
[epoch2, step2948]: loss 0.561832
[epoch2, step2949]: loss 0.677231
[epoch2, step2950]: loss 0.845045
[epoch2, step2951]: loss 0.755773
[epoch2, step2952]: loss 1.022570
[epoch2, step2953]: loss 0.614967
[epoch2, step2954]: loss 0.805795
[epoch2, step2955]: loss 0.970082
[epoch2, step2956]: loss 0.949282
[epoch2, step2957]: loss 0.752505
[epoch2, step2958]: loss 0.889385
[epoch2, step2959]: loss 0.588051
[epoch2, step2960]: loss 0.498611
[epoch2, step2961]: loss 0.575273
[epoch2, step2962]: loss 0.561702
[epoch2, step2963]: loss 0.657060
[epoch2, step2964]: loss 0.653896
[epoch2, step2965]: loss 0.752797
[epoch2, step2966]: loss 0.719323
[epoch2, step2967]: loss 0.882576
[epoch2, step2968]: loss 0.864068
[epoch2, step2969]: loss 0.591361
[epoch2, step2970]: loss 0.894795
[epoch2, step2971]: loss 0.793698
[epoch2, step2972]: loss 1.041634
[epoch2, step2973]: loss 0.851708
[epoch2, step2974]: loss 0.704641
[epoch2, step2975]: loss 0.577174
[epoch2, step2976]: loss 0.882050
[epoch2, step2977]: loss 0.908200
[epoch2, step2978]: loss 0.792086
[epoch2, step2979]: loss 0.649380
[epoch2, step2980]: loss 0.923276
[epoch2, step2981]: loss 0.718916
[epoch2, step2982]: loss 0.453271
[epoch2, step2983]: loss 0.641632
[epoch2, step2984]: loss 0.623589
[epoch2, step2985]: loss 0.683761
[epoch2, step2986]: loss 0.856721
[epoch2, step2987]: loss 0.771226
[epoch2, step2988]: loss 0.917129
[epoch2, step2989]: loss 0.720425
[epoch2, step2990]: loss 0.877207
[epoch2, step2991]: loss 0.592256
[epoch2, step2992]: loss 0.852306
[epoch2, step2993]: loss 0.367409
[epoch2, step2994]: loss 0.543352
[epoch2, step2995]: loss 0.670982
[epoch2, step2996]: loss 0.672149
[epoch2, step2997]: loss 0.702940
[epoch2, step2998]: loss 0.555247
[epoch2, step2999]: loss 0.547682
[epoch2, step3000]: loss 0.600027
[epoch2, step3001]: loss 0.548598
[epoch2, step3002]: loss 0.598600
[epoch2, step3003]: loss 0.777810
[epoch2, step3004]: loss 0.954506
[epoch2, step3005]: loss 0.818721
[epoch2, step3006]: loss 0.825532
[epoch2, step3007]: loss 0.855772
[epoch2, step3008]: loss 0.965350
[epoch2, step3009]: loss 0.487945
[epoch2, step3010]: loss 0.793926
[epoch2, step3011]: loss 0.842562
[epoch2, step3012]: loss 0.646378
[epoch2, step3013]: loss 0.785948
[epoch2, step3014]: loss 0.930561
[epoch2, step3015]: loss 0.881475
[epoch2, step3016]: loss 0.402912
[epoch2, step3017]: loss 0.685373
[epoch2, step3018]: loss 0.787290
[epoch2, step3019]: loss 0.743511
[epoch2, step3020]: loss 0.819781
[epoch2, step3021]: loss 0.614559
[epoch2, step3022]: loss 0.833972
[epoch2, step3023]: loss 0.467819
[epoch2, step3024]: loss 0.622194
[epoch2, step3025]: loss 1.002806
[epoch2, step3026]: loss 0.906601
[epoch2, step3027]: loss 0.677075
[epoch2, step3028]: loss 0.731595
[epoch2, step3029]: loss 0.568100
[epoch2, step3030]: loss 0.740961
[epoch2, step3031]: loss 0.597554
[epoch2, step3032]: loss 0.867755
[epoch2, step3033]: loss 0.740947
[epoch2, step3034]: loss 0.454715
[epoch2, step3035]: loss 0.925686
[epoch2, step3036]: loss 0.459253
[epoch2, step3037]: loss 0.578748
[epoch2, step3038]: loss 0.791819
[epoch2, step3039]: loss 0.321064
[epoch2, step3040]: loss 0.645893
[epoch2, step3041]: loss 0.974187
[epoch2, step3042]: loss 0.933657
[epoch2, step3043]: loss 0.938707
[epoch2, step3044]: loss 0.529024
[epoch2, step3045]: loss 0.718105
[epoch2, step3046]: loss 0.795007
[epoch2, step3047]: loss 0.774511
[epoch2, step3048]: loss 0.556324
[epoch2, step3049]: loss 0.772051
[epoch2, step3050]: loss 0.878324
[epoch2, step3051]: loss 1.047084
[epoch2, step3052]: loss 0.846582
[epoch2, step3053]: loss 0.785907
[epoch2, step3054]: loss 0.764289
[epoch2, step3055]: loss 0.879598
[epoch2, step3056]: loss 0.913060
[epoch2, step3057]: loss 0.945134
[epoch2, step3058]: loss 0.912883
[epoch2, step3059]: loss 0.821481
[epoch2, step3060]: loss 0.931030
[epoch2, step3061]: loss 0.818870
[epoch2, step3062]: loss 0.780176
[epoch2, step3063]: loss 0.677275
[epoch2, step3064]: loss 0.868351
[epoch2, step3065]: loss 0.705532
[epoch2, step3066]: loss 0.659327
[epoch2, step3067]: loss 0.739848
[epoch2, step3068]: loss 0.610091
[epoch2, step3069]: loss 0.708997
[epoch2, step3070]: loss 0.817141
[epoch2, step3071]: loss 0.647223
[epoch2, step3072]: loss 0.743863
[epoch2, step3073]: loss 0.834830
[epoch2, step3074]: loss 0.426132
[epoch2, step3075]: loss 0.467952
[epoch2, step3076]: loss 0.635274

[epoch2]: avg loss 0.635274

[epoch3, step1]: loss 0.813745
[epoch3, step2]: loss 0.873142
[epoch3, step3]: loss 0.881432
[epoch3, step4]: loss 0.525789
[epoch3, step5]: loss 0.788295
[epoch3, step6]: loss 0.568978
[epoch3, step7]: loss 0.402996
[epoch3, step8]: loss 0.572266
[epoch3, step9]: loss 0.783521
[epoch3, step10]: loss 0.745248
[epoch3, step11]: loss 0.852112
[epoch3, step12]: loss 0.447926
[epoch3, step13]: loss 0.318272
[epoch3, step14]: loss 0.543978
[epoch3, step15]: loss 0.910913
[epoch3, step16]: loss 0.649083
[epoch3, step17]: loss 0.537199
[epoch3, step18]: loss 0.394714
[epoch3, step19]: loss 0.994674
[epoch3, step20]: loss 0.765149
[epoch3, step21]: loss 0.894668
[epoch3, step22]: loss 0.713650
[epoch3, step23]: loss 0.612907
[epoch3, step24]: loss 0.372308
[epoch3, step25]: loss 0.906185
[epoch3, step26]: loss 0.723585
[epoch3, step27]: loss 0.708603
[epoch3, step28]: loss 0.890952
[epoch3, step29]: loss 0.632487
[epoch3, step30]: loss 0.705214
[epoch3, step31]: loss 0.711690
[epoch3, step32]: loss 0.736118
[epoch3, step33]: loss 0.934439
[epoch3, step34]: loss 0.776355
[epoch3, step35]: loss 0.830746
[epoch3, step36]: loss 0.816011
[epoch3, step37]: loss 0.673162
[epoch3, step38]: loss 0.681715
[epoch3, step39]: loss 0.670993
[epoch3, step40]: loss 0.702771
[epoch3, step41]: loss 0.969072
[epoch3, step42]: loss 0.668525
[epoch3, step43]: loss 0.513160
[epoch3, step44]: loss 0.742161
[epoch3, step45]: loss 0.714452
[epoch3, step46]: loss 0.760866
[epoch3, step47]: loss 0.832374
[epoch3, step48]: loss 0.522154
[epoch3, step49]: loss 0.961177
[epoch3, step50]: loss 0.844252
[epoch3, step51]: loss 0.719504
[epoch3, step52]: loss 0.896839
[epoch3, step53]: loss 0.932496
[epoch3, step54]: loss 0.767639
[epoch3, step55]: loss 0.929486
[epoch3, step56]: loss 0.821260
[epoch3, step57]: loss 0.887932
[epoch3, step58]: loss 0.620770
[epoch3, step59]: loss 0.464315
[epoch3, step60]: loss 0.973417
[epoch3, step61]: loss 0.598994
[epoch3, step62]: loss 0.708706
[epoch3, step63]: loss 0.464656
[epoch3, step64]: loss 0.743780
[epoch3, step65]: loss 0.760900
[epoch3, step66]: loss 0.704251
[epoch3, step67]: loss 0.770483
[epoch3, step68]: loss 0.660555
[epoch3, step69]: loss 0.994986
[epoch3, step70]: loss 0.430689
[epoch3, step71]: loss 0.766154
[epoch3, step72]: loss 0.442204
[epoch3, step73]: loss 0.708221
[epoch3, step74]: loss 0.648548
[epoch3, step75]: loss 0.752180
[epoch3, step76]: loss 0.735686
[epoch3, step77]: loss 0.585498
[epoch3, step78]: loss 0.875593
[epoch3, step79]: loss 0.741060
[epoch3, step80]: loss 0.585246
[epoch3, step81]: loss 0.678113
[epoch3, step82]: loss 0.802270
[epoch3, step83]: loss 0.386373
[epoch3, step84]: loss 0.915850
[epoch3, step85]: loss 0.634845
[epoch3, step86]: loss 0.845589
[epoch3, step87]: loss 0.757282
[epoch3, step88]: loss 0.754310
[epoch3, step89]: loss 0.874830
[epoch3, step90]: loss 0.593260
[epoch3, step91]: loss 0.466344
[epoch3, step92]: loss 0.707469
[epoch3, step93]: loss 0.724587
[epoch3, step94]: loss 0.903405
[epoch3, step95]: loss 0.458039
[epoch3, step96]: loss 0.966920
[epoch3, step97]: loss 0.744591
[epoch3, step98]: loss 0.861414
[epoch3, step99]: loss 0.860697
[epoch3, step100]: loss 0.934803
[epoch3, step101]: loss 0.907051
[epoch3, step102]: loss 0.671803
[epoch3, step103]: loss 0.688679
[epoch3, step104]: loss 0.808609
[epoch3, step105]: loss 0.903390
[epoch3, step106]: loss 0.818542
[epoch3, step107]: loss 0.851225
[epoch3, step108]: loss 0.785352
[epoch3, step109]: loss 0.871852
[epoch3, step110]: loss 0.791611
[epoch3, step111]: loss 0.692902
[epoch3, step112]: loss 0.444311
[epoch3, step113]: loss 0.854756
[epoch3, step114]: loss 0.993418
[epoch3, step115]: loss 0.464310
[epoch3, step116]: loss 0.800965
[epoch3, step117]: loss 0.882159
[epoch3, step118]: loss 0.694217
[epoch3, step119]: loss 0.650397
[epoch3, step120]: loss 0.842832
[epoch3, step121]: loss 0.722740
[epoch3, step122]: loss 0.664862
[epoch3, step123]: loss 0.653692
[epoch3, step124]: loss 0.634724
[epoch3, step125]: loss 0.845568
[epoch3, step126]: loss 0.694431
[epoch3, step127]: loss 0.742452
[epoch3, step128]: loss 0.634510
[epoch3, step129]: loss 0.594067
[epoch3, step130]: loss 0.975232
[epoch3, step131]: loss 0.714537
[epoch3, step132]: loss 0.763313
[epoch3, step133]: loss 0.802549
[epoch3, step134]: loss 0.300814
[epoch3, step135]: loss 0.553684
[epoch3, step136]: loss 0.893818
[epoch3, step137]: loss 0.712202
[epoch3, step138]: loss 0.718669
[epoch3, step139]: loss 0.567063
[epoch3, step140]: loss 0.739730
[epoch3, step141]: loss 0.816616
[epoch3, step142]: loss 0.571002
[epoch3, step143]: loss 0.823223
[epoch3, step144]: loss 0.696951
[epoch3, step145]: loss 0.856483
[epoch3, step146]: loss 0.586381
[epoch3, step147]: loss 0.735039
[epoch3, step148]: loss 0.287373
[epoch3, step149]: loss 0.681802
[epoch3, step150]: loss 0.994265
[epoch3, step151]: loss 0.917061
[epoch3, step152]: loss 0.446496
[epoch3, step153]: loss 0.496306
[epoch3, step154]: loss 0.793806
[epoch3, step155]: loss 0.907379
[epoch3, step156]: loss 0.784286
[epoch3, step157]: loss 0.603509
[epoch3, step158]: loss 0.697908
[epoch3, step159]: loss 0.743228
[epoch3, step160]: loss 0.877473
[epoch3, step161]: loss 0.923625
[epoch3, step162]: loss 0.491170
[epoch3, step163]: loss 0.831737
[epoch3, step164]: loss 0.593218
[epoch3, step165]: loss 0.759827
[epoch3, step166]: loss 0.533770
[epoch3, step167]: loss 0.670147
[epoch3, step168]: loss 0.876952
[epoch3, step169]: loss 0.533376
[epoch3, step170]: loss 0.512825
[epoch3, step171]: loss 1.012092
[epoch3, step172]: loss 0.812720
[epoch3, step173]: loss 0.885597
[epoch3, step174]: loss 0.797130
[epoch3, step175]: loss 0.889246
[epoch3, step176]: loss 0.919623
[epoch3, step177]: loss 0.573820
[epoch3, step178]: loss 0.844376
[epoch3, step179]: loss 0.603217
[epoch3, step180]: loss 0.821592
[epoch3, step181]: loss 0.686622
[epoch3, step182]: loss 0.977454
[epoch3, step183]: loss 0.802522
[epoch3, step184]: loss 0.749088
[epoch3, step185]: loss 0.783834
[epoch3, step186]: loss 0.784806
[epoch3, step187]: loss 1.015473
[epoch3, step188]: loss 0.346312
[epoch3, step189]: loss 0.489492
[epoch3, step190]: loss 0.888928
[epoch3, step191]: loss 0.834679
[epoch3, step192]: loss 0.927490
[epoch3, step193]: loss 0.907042
[epoch3, step194]: loss 0.438700
[epoch3, step195]: loss 0.746269
[epoch3, step196]: loss 0.852032
[epoch3, step197]: loss 0.527376
[epoch3, step198]: loss 0.691617
[epoch3, step199]: loss 0.860349
[epoch3, step200]: loss 0.727309
[epoch3, step201]: loss 0.763908
[epoch3, step202]: loss 0.728141
[epoch3, step203]: loss 0.388942
[epoch3, step204]: loss 0.389341
[epoch3, step205]: loss 0.741657
[epoch3, step206]: loss 0.804340
[epoch3, step207]: loss 0.723167
[epoch3, step208]: loss 0.644901
[epoch3, step209]: loss 0.759912
[epoch3, step210]: loss 0.415490
[epoch3, step211]: loss 0.611139
[epoch3, step212]: loss 0.833847
[epoch3, step213]: loss 0.683411
[epoch3, step214]: loss 0.723168
[epoch3, step215]: loss 0.462859
[epoch3, step216]: loss 0.669875
[epoch3, step217]: loss 0.854159
[epoch3, step218]: loss 0.753546
[epoch3, step219]: loss 0.440554
[epoch3, step220]: loss 0.501892
[epoch3, step221]: loss 0.353426
[epoch3, step222]: loss 0.675066
[epoch3, step223]: loss 0.723246
[epoch3, step224]: loss 0.943608
[epoch3, step225]: loss 0.900186
[epoch3, step226]: loss 0.673521
[epoch3, step227]: loss 0.570418
[epoch3, step228]: loss 0.812158
[epoch3, step229]: loss 0.906011
[epoch3, step230]: loss 0.812390
[epoch3, step231]: loss 0.782817
[epoch3, step232]: loss 0.802160
[epoch3, step233]: loss 0.902504
[epoch3, step234]: loss 0.310413
[epoch3, step235]: loss 0.735403
[epoch3, step236]: loss 0.966495
[epoch3, step237]: loss 0.614044
[epoch3, step238]: loss 0.976066
[epoch3, step239]: loss 0.643787
[epoch3, step240]: loss 0.753420
[epoch3, step241]: loss 0.715145
[epoch3, step242]: loss 0.805192
[epoch3, step243]: loss 0.511928
[epoch3, step244]: loss 0.660038
[epoch3, step245]: loss 0.790714
[epoch3, step246]: loss 0.912383
[epoch3, step247]: loss 0.977057
[epoch3, step248]: loss 0.674704
[epoch3, step249]: loss 0.312378
[epoch3, step250]: loss 0.553899
[epoch3, step251]: loss 0.830673
[epoch3, step252]: loss 0.515416
[epoch3, step253]: loss 0.443118
[epoch3, step254]: loss 0.450706
[epoch3, step255]: loss 0.887896
[epoch3, step256]: loss 0.710024
[epoch3, step257]: loss 0.712739
[epoch3, step258]: loss 0.840704
[epoch3, step259]: loss 0.567839
[epoch3, step260]: loss 0.826108
[epoch3, step261]: loss 0.821713
[epoch3, step262]: loss 0.993123
[epoch3, step263]: loss 0.489124
[epoch3, step264]: loss 0.538593
[epoch3, step265]: loss 0.712757
[epoch3, step266]: loss 0.603228
[epoch3, step267]: loss 0.557892
[epoch3, step268]: loss 0.937835
[epoch3, step269]: loss 0.617308
[epoch3, step270]: loss 0.693173
[epoch3, step271]: loss 0.866253
[epoch3, step272]: loss 0.700273
[epoch3, step273]: loss 0.464012
[epoch3, step274]: loss 0.617490
[epoch3, step275]: loss 0.683216
[epoch3, step276]: loss 0.504807
[epoch3, step277]: loss 0.897752
[epoch3, step278]: loss 0.650722
[epoch3, step279]: loss 0.803257
[epoch3, step280]: loss 0.678252
[epoch3, step281]: loss 0.943468
[epoch3, step282]: loss 0.830548
[epoch3, step283]: loss 0.682590
[epoch3, step284]: loss 0.748732
[epoch3, step285]: loss 1.033936
[epoch3, step286]: loss 0.689174
[epoch3, step287]: loss 0.943275
[epoch3, step288]: loss 0.281886
[epoch3, step289]: loss 0.701928
[epoch3, step290]: loss 0.759481
[epoch3, step291]: loss 0.844000
[epoch3, step292]: loss 0.769420
[epoch3, step293]: loss 0.939303
[epoch3, step294]: loss 0.768981
[epoch3, step295]: loss 0.775438
[epoch3, step296]: loss 0.810960
[epoch3, step297]: loss 0.592028
[epoch3, step298]: loss 0.769143
[epoch3, step299]: loss 0.748359
[epoch3, step300]: loss 0.847154
[epoch3, step301]: loss 0.730986
[epoch3, step302]: loss 0.861998
[epoch3, step303]: loss 0.657120
[epoch3, step304]: loss 0.726852
[epoch3, step305]: loss 0.745294
[epoch3, step306]: loss 0.720681
[epoch3, step307]: loss 0.540456
[epoch3, step308]: loss 0.595024
[epoch3, step309]: loss 0.718339
[epoch3, step310]: loss 0.606178
[epoch3, step311]: loss 0.824638
[epoch3, step312]: loss 0.462640
[epoch3, step313]: loss 0.802038
[epoch3, step314]: loss 0.796017
[epoch3, step315]: loss 0.594262
[epoch3, step316]: loss 0.772076
[epoch3, step317]: loss 0.704048
[epoch3, step318]: loss 0.528749
[epoch3, step319]: loss 0.635581
[epoch3, step320]: loss 0.618159
[epoch3, step321]: loss 0.757432
[epoch3, step322]: loss 1.024393
[epoch3, step323]: loss 0.718090
[epoch3, step324]: loss 0.706183
[epoch3, step325]: loss 0.862875
[epoch3, step326]: loss 0.604648
[epoch3, step327]: loss 0.588629
[epoch3, step328]: loss 0.807827
[epoch3, step329]: loss 0.461705
[epoch3, step330]: loss 0.840561
[epoch3, step331]: loss 0.788607
[epoch3, step332]: loss 0.626659
[epoch3, step333]: loss 0.357813
[epoch3, step334]: loss 0.532978
[epoch3, step335]: loss 0.610355
[epoch3, step336]: loss 0.711756
[epoch3, step337]: loss 0.782759
[epoch3, step338]: loss 0.387070
[epoch3, step339]: loss 0.618305
[epoch3, step340]: loss 0.700700
[epoch3, step341]: loss 0.756227
[epoch3, step342]: loss 0.832397
[epoch3, step343]: loss 0.737996
[epoch3, step344]: loss 0.372217
[epoch3, step345]: loss 0.581112
[epoch3, step346]: loss 0.794713
[epoch3, step347]: loss 0.795743
[epoch3, step348]: loss 0.882869
[epoch3, step349]: loss 0.779599
[epoch3, step350]: loss 0.739606
[epoch3, step351]: loss 0.605717
[epoch3, step352]: loss 0.220002
[epoch3, step353]: loss 0.779047
[epoch3, step354]: loss 0.895748
[epoch3, step355]: loss 1.035815
[epoch3, step356]: loss 0.793099
[epoch3, step357]: loss 0.777427
[epoch3, step358]: loss 1.024396
[epoch3, step359]: loss 0.676542
[epoch3, step360]: loss 0.899826
[epoch3, step361]: loss 0.634188
[epoch3, step362]: loss 0.391811
[epoch3, step363]: loss 0.627716
[epoch3, step364]: loss 0.518379
[epoch3, step365]: loss 0.855478
[epoch3, step366]: loss 0.533501
[epoch3, step367]: loss 0.843273
[epoch3, step368]: loss 0.621224
[epoch3, step369]: loss 0.963618
[epoch3, step370]: loss 0.781033
[epoch3, step371]: loss 0.638691
[epoch3, step372]: loss 0.815610
[epoch3, step373]: loss 0.919051
[epoch3, step374]: loss 0.649384
[epoch3, step375]: loss 0.697496
[epoch3, step376]: loss 0.793537
[epoch3, step377]: loss 0.739039
[epoch3, step378]: loss 0.715038
[epoch3, step379]: loss 0.992533
[epoch3, step380]: loss 0.795913
[epoch3, step381]: loss 0.781508
[epoch3, step382]: loss 0.831384
[epoch3, step383]: loss 0.620293
[epoch3, step384]: loss 0.723730
[epoch3, step385]: loss 0.659270
[epoch3, step386]: loss 0.798299
[epoch3, step387]: loss 0.784540
[epoch3, step388]: loss 0.757142
[epoch3, step389]: loss 0.738107
[epoch3, step390]: loss 0.785423
[epoch3, step391]: loss 0.789425
[epoch3, step392]: loss 0.327114
[epoch3, step393]: loss 0.774529
[epoch3, step394]: loss 0.835912
[epoch3, step395]: loss 0.790365
[epoch3, step396]: loss 0.733567
[epoch3, step397]: loss 1.079874
[epoch3, step398]: loss 0.627724
[epoch3, step399]: loss 0.743816
[epoch3, step400]: loss 0.802394
[epoch3, step401]: loss 0.750653
[epoch3, step402]: loss 0.714519
[epoch3, step403]: loss 0.578746
[epoch3, step404]: loss 0.791221
[epoch3, step405]: loss 0.920533
[epoch3, step406]: loss 0.737214
[epoch3, step407]: loss 0.844916
[epoch3, step408]: loss 0.773788
[epoch3, step409]: loss 0.890927
[epoch3, step410]: loss 0.732371
[epoch3, step411]: loss 0.795190
[epoch3, step412]: loss 0.739336
[epoch3, step413]: loss 0.519136
[epoch3, step414]: loss 0.659045
[epoch3, step415]: loss 0.500371
[epoch3, step416]: loss 0.676056
[epoch3, step417]: loss 0.464466
[epoch3, step418]: loss 0.780332
[epoch3, step419]: loss 0.547820
[epoch3, step420]: loss 0.535630
[epoch3, step421]: loss 0.714731
[epoch3, step422]: loss 0.747418
[epoch3, step423]: loss 0.924459
[epoch3, step424]: loss 0.707360
[epoch3, step425]: loss 0.689477
[epoch3, step426]: loss 0.859277
[epoch3, step427]: loss 0.658337
[epoch3, step428]: loss 0.347760
[epoch3, step429]: loss 0.883470
[epoch3, step430]: loss 0.639636
[epoch3, step431]: loss 0.792372
[epoch3, step432]: loss 0.839103
[epoch3, step433]: loss 0.675012
[epoch3, step434]: loss 0.631839
[epoch3, step435]: loss 0.935248
[epoch3, step436]: loss 0.699158
[epoch3, step437]: loss 0.661492
[epoch3, step438]: loss 0.559052
[epoch3, step439]: loss 0.685322
[epoch3, step440]: loss 0.927561
[epoch3, step441]: loss 0.826364
[epoch3, step442]: loss 0.572361
[epoch3, step443]: loss 1.024122
[epoch3, step444]: loss 0.640879
[epoch3, step445]: loss 0.560024
[epoch3, step446]: loss 0.473083
[epoch3, step447]: loss 0.769734
[epoch3, step448]: loss 0.648336
[epoch3, step449]: loss 0.899688
[epoch3, step450]: loss 0.473301
[epoch3, step451]: loss 0.736040
[epoch3, step452]: loss 0.784547
[epoch3, step453]: loss 0.775116
[epoch3, step454]: loss 0.701364
[epoch3, step455]: loss 0.457653
[epoch3, step456]: loss 0.465403
[epoch3, step457]: loss 0.334854
[epoch3, step458]: loss 0.596105
[epoch3, step459]: loss 0.611960
[epoch3, step460]: loss 0.840939
[epoch3, step461]: loss 0.786074
[epoch3, step462]: loss 0.936595
[epoch3, step463]: loss 0.501246
[epoch3, step464]: loss 0.245578
[epoch3, step465]: loss 0.586728
[epoch3, step466]: loss 0.887224
[epoch3, step467]: loss 0.591796
[epoch3, step468]: loss 0.508710
[epoch3, step469]: loss 0.836092
[epoch3, step470]: loss 0.682975
[epoch3, step471]: loss 0.771837
[epoch3, step472]: loss 0.923700
[epoch3, step473]: loss 0.664453
[epoch3, step474]: loss 0.581011
[epoch3, step475]: loss 0.423111
[epoch3, step476]: loss 0.593442
[epoch3, step477]: loss 0.576838
[epoch3, step478]: loss 0.542574
[epoch3, step479]: loss 0.809330
[epoch3, step480]: loss 0.831148
[epoch3, step481]: loss 0.553119
[epoch3, step482]: loss 0.811583
[epoch3, step483]: loss 0.982797
[epoch3, step484]: loss 0.595675
[epoch3, step485]: loss 1.074937
[epoch3, step486]: loss 0.642002
[epoch3, step487]: loss 0.747077
[epoch3, step488]: loss 0.738528
[epoch3, step489]: loss 0.708594
[epoch3, step490]: loss 0.943630
[epoch3, step491]: loss 0.321879
[epoch3, step492]: loss 0.680189
[epoch3, step493]: loss 0.743560
[epoch3, step494]: loss 0.614653
[epoch3, step495]: loss 0.784458
[epoch3, step496]: loss 0.760199
[epoch3, step497]: loss 0.789382
[epoch3, step498]: loss 0.815714
[epoch3, step499]: loss 0.943488
[epoch3, step500]: loss 0.817163
[epoch3, step501]: loss 0.768259
[epoch3, step502]: loss 0.800657
[epoch3, step503]: loss 0.551982
[epoch3, step504]: loss 0.721925
[epoch3, step505]: loss 0.807278
[epoch3, step506]: loss 0.494081
[epoch3, step507]: loss 0.364452
[epoch3, step508]: loss 0.814656
[epoch3, step509]: loss 0.918340
[epoch3, step510]: loss 0.417476
[epoch3, step511]: loss 0.620617
[epoch3, step512]: loss 0.942148
[epoch3, step513]: loss 0.744856
[epoch3, step514]: loss 0.895420
[epoch3, step515]: loss 0.767526
[epoch3, step516]: loss 0.314432
[epoch3, step517]: loss 0.833112
[epoch3, step518]: loss 0.926880
[epoch3, step519]: loss 0.794801
[epoch3, step520]: loss 0.937699
[epoch3, step521]: loss 0.757987
[epoch3, step522]: loss 0.280286
[epoch3, step523]: loss 0.485225
[epoch3, step524]: loss 0.791592
[epoch3, step525]: loss 0.628220
[epoch3, step526]: loss 0.775779
[epoch3, step527]: loss 0.627226
[epoch3, step528]: loss 0.715152
[epoch3, step529]: loss 0.909032
[epoch3, step530]: loss 0.597073
[epoch3, step531]: loss 0.800483
[epoch3, step532]: loss 0.920034
[epoch3, step533]: loss 0.839704
[epoch3, step534]: loss 0.637012
[epoch3, step535]: loss 0.750570
[epoch3, step536]: loss 0.841197
[epoch3, step537]: loss 0.503190
[epoch3, step538]: loss 0.753839
[epoch3, step539]: loss 0.998147
[epoch3, step540]: loss 0.765161
[epoch3, step541]: loss 0.698918
[epoch3, step542]: loss 0.460795
[epoch3, step543]: loss 0.564950
[epoch3, step544]: loss 0.505765
[epoch3, step545]: loss 0.611724
[epoch3, step546]: loss 0.742085
[epoch3, step547]: loss 0.736966
[epoch3, step548]: loss 0.696580
[epoch3, step549]: loss 0.812583
[epoch3, step550]: loss 0.780412
[epoch3, step551]: loss 0.623316
[epoch3, step552]: loss 0.730921
[epoch3, step553]: loss 0.561882
[epoch3, step554]: loss 0.944875
[epoch3, step555]: loss 0.631702
[epoch3, step556]: loss 0.741420
[epoch3, step557]: loss 0.626968
[epoch3, step558]: loss 0.877328
[epoch3, step559]: loss 0.673984
[epoch3, step560]: loss 0.580817
[epoch3, step561]: loss 0.851266
[epoch3, step562]: loss 0.415812
[epoch3, step563]: loss 0.498152
[epoch3, step564]: loss 0.396723
[epoch3, step565]: loss 0.795474
[epoch3, step566]: loss 0.831809
[epoch3, step567]: loss 0.517900
[epoch3, step568]: loss 0.788381
[epoch3, step569]: loss 0.693277
[epoch3, step570]: loss 0.629274
[epoch3, step571]: loss 1.043115
[epoch3, step572]: loss 0.719069
[epoch3, step573]: loss 0.773818
[epoch3, step574]: loss 0.689547
[epoch3, step575]: loss 0.511586
[epoch3, step576]: loss 0.559178
[epoch3, step577]: loss 1.023222
[epoch3, step578]: loss 0.637131
[epoch3, step579]: loss 0.773615
[epoch3, step580]: loss 0.531275
[epoch3, step581]: loss 0.815242
[epoch3, step582]: loss 0.655775
[epoch3, step583]: loss 0.549063
[epoch3, step584]: loss 0.816900
[epoch3, step585]: loss 0.569789
[epoch3, step586]: loss 0.385556
[epoch3, step587]: loss 0.780959
[epoch3, step588]: loss 0.604127
[epoch3, step589]: loss 0.600394
[epoch3, step590]: loss 0.696454
[epoch3, step591]: loss 0.666862
[epoch3, step592]: loss 1.003719
[epoch3, step593]: loss 0.779603
[epoch3, step594]: loss 0.407479
[epoch3, step595]: loss 0.943902
[epoch3, step596]: loss 0.874799
[epoch3, step597]: loss 0.773686
[epoch3, step598]: loss 0.691848
[epoch3, step599]: loss 0.862876
[epoch3, step600]: loss 0.481944
[epoch3, step601]: loss 0.927520
[epoch3, step602]: loss 0.615267
[epoch3, step603]: loss 0.832411
[epoch3, step604]: loss 0.646122
[epoch3, step605]: loss 0.722226
[epoch3, step606]: loss 0.708578
[epoch3, step607]: loss 0.999980
[epoch3, step608]: loss 0.545315
[epoch3, step609]: loss 0.883992
[epoch3, step610]: loss 0.677923
[epoch3, step611]: loss 0.709898
[epoch3, step612]: loss 0.613037
[epoch3, step613]: loss 0.836790
[epoch3, step614]: loss 0.502433
[epoch3, step615]: loss 0.564268
[epoch3, step616]: loss 0.403515
[epoch3, step617]: loss 0.708069
[epoch3, step618]: loss 0.408533
[epoch3, step619]: loss 0.783047
[epoch3, step620]: loss 0.899180
[epoch3, step621]: loss 0.516785
[epoch3, step622]: loss 1.016266
[epoch3, step623]: loss 0.698244
[epoch3, step624]: loss 0.620892
[epoch3, step625]: loss 0.879911
[epoch3, step626]: loss 0.965342
[epoch3, step627]: loss 1.026494
[epoch3, step628]: loss 0.631272
[epoch3, step629]: loss 0.483768
[epoch3, step630]: loss 0.892597
[epoch3, step631]: loss 0.862017
[epoch3, step632]: loss 0.640787
[epoch3, step633]: loss 0.642534
[epoch3, step634]: loss 0.687843
[epoch3, step635]: loss 0.593271
[epoch3, step636]: loss 0.770610
[epoch3, step637]: loss 0.574534
[epoch3, step638]: loss 0.632773
[epoch3, step639]: loss 0.628584
[epoch3, step640]: loss 0.857698
[epoch3, step641]: loss 0.577030
[epoch3, step642]: loss 0.664346
[epoch3, step643]: loss 0.589208
[epoch3, step644]: loss 0.694434
[epoch3, step645]: loss 0.644146
[epoch3, step646]: loss 0.647808
[epoch3, step647]: loss 0.713911
[epoch3, step648]: loss 0.675157
[epoch3, step649]: loss 0.529206
[epoch3, step650]: loss 0.872469
[epoch3, step651]: loss 0.669799
[epoch3, step652]: loss 0.861265
[epoch3, step653]: loss 0.591551
[epoch3, step654]: loss 0.755254
[epoch3, step655]: loss 0.824821
[epoch3, step656]: loss 0.772372
[epoch3, step657]: loss 0.882633
[epoch3, step658]: loss 0.484358
[epoch3, step659]: loss 0.470143
[epoch3, step660]: loss 0.775864
[epoch3, step661]: loss 0.550241
[epoch3, step662]: loss 0.502345
[epoch3, step663]: loss 0.701079
[epoch3, step664]: loss 0.592665
[epoch3, step665]: loss 0.643930
[epoch3, step666]: loss 0.637335
[epoch3, step667]: loss 0.797679
[epoch3, step668]: loss 0.574208
[epoch3, step669]: loss 0.588642
[epoch3, step670]: loss 0.732779
[epoch3, step671]: loss 0.612421
[epoch3, step672]: loss 0.736238
[epoch3, step673]: loss 0.820205
[epoch3, step674]: loss 0.623089
[epoch3, step675]: loss 0.693582
[epoch3, step676]: loss 1.016095
[epoch3, step677]: loss 1.003822
[epoch3, step678]: loss 0.726707
[epoch3, step679]: loss 0.939586
[epoch3, step680]: loss 0.513525
[epoch3, step681]: loss 0.751130
[epoch3, step682]: loss 0.648424
[epoch3, step683]: loss 0.759091
[epoch3, step684]: loss 0.736775
[epoch3, step685]: loss 0.609711
[epoch3, step686]: loss 0.612252
[epoch3, step687]: loss 0.769935
[epoch3, step688]: loss 0.474877
[epoch3, step689]: loss 0.881855
[epoch3, step690]: loss 0.784199
[epoch3, step691]: loss 0.720142
[epoch3, step692]: loss 0.695318
[epoch3, step693]: loss 0.689663
[epoch3, step694]: loss 0.518756
[epoch3, step695]: loss 0.619495
[epoch3, step696]: loss 1.006416
[epoch3, step697]: loss 0.643345
[epoch3, step698]: loss 0.849920
[epoch3, step699]: loss 0.713359
[epoch3, step700]: loss 0.607012
[epoch3, step701]: loss 0.785108
[epoch3, step702]: loss 0.714856
[epoch3, step703]: loss 0.675953
[epoch3, step704]: loss 0.842093
[epoch3, step705]: loss 0.784325
[epoch3, step706]: loss 0.355032
[epoch3, step707]: loss 0.660217
[epoch3, step708]: loss 0.551397
[epoch3, step709]: loss 0.760690
[epoch3, step710]: loss 0.333472
[epoch3, step711]: loss 0.484829
[epoch3, step712]: loss 0.834528
[epoch3, step713]: loss 0.740292
[epoch3, step714]: loss 0.637554
[epoch3, step715]: loss 0.917803
[epoch3, step716]: loss 0.647284
[epoch3, step717]: loss 0.571251
[epoch3, step718]: loss 0.756747
[epoch3, step719]: loss 0.786139
[epoch3, step720]: loss 0.795482
[epoch3, step721]: loss 0.325153
[epoch3, step722]: loss 0.798974
[epoch3, step723]: loss 0.602335
[epoch3, step724]: loss 0.519508
[epoch3, step725]: loss 0.513921
[epoch3, step726]: loss 0.815048
[epoch3, step727]: loss 0.651583
[epoch3, step728]: loss 0.856786
[epoch3, step729]: loss 0.687103
[epoch3, step730]: loss 0.635200
[epoch3, step731]: loss 0.786964
[epoch3, step732]: loss 0.737594
[epoch3, step733]: loss 0.782996
[epoch3, step734]: loss 0.920380
[epoch3, step735]: loss 0.792318
[epoch3, step736]: loss 0.831954
[epoch3, step737]: loss 0.640087
[epoch3, step738]: loss 0.631425
[epoch3, step739]: loss 0.744394
[epoch3, step740]: loss 0.748222
[epoch3, step741]: loss 0.815935
[epoch3, step742]: loss 0.734485
[epoch3, step743]: loss 0.719335
[epoch3, step744]: loss 0.622289
[epoch3, step745]: loss 0.806790
[epoch3, step746]: loss 1.021549
[epoch3, step747]: loss 0.634401
[epoch3, step748]: loss 0.463334
[epoch3, step749]: loss 0.849979
[epoch3, step750]: loss 0.716731
[epoch3, step751]: loss 0.584113
[epoch3, step752]: loss 0.779557
[epoch3, step753]: loss 0.572822
[epoch3, step754]: loss 0.473451
[epoch3, step755]: loss 0.617712
[epoch3, step756]: loss 0.930406
[epoch3, step757]: loss 0.800184
[epoch3, step758]: loss 0.800841
[epoch3, step759]: loss 0.831081
[epoch3, step760]: loss 0.602139
[epoch3, step761]: loss 0.902686
[epoch3, step762]: loss 0.672126
[epoch3, step763]: loss 0.750898
[epoch3, step764]: loss 0.722016
[epoch3, step765]: loss 0.824823
[epoch3, step766]: loss 0.717741
[epoch3, step767]: loss 0.668134
[epoch3, step768]: loss 0.502236
[epoch3, step769]: loss 0.766430
[epoch3, step770]: loss 0.405981
[epoch3, step771]: loss 0.874661
[epoch3, step772]: loss 0.721768
[epoch3, step773]: loss 0.582730
[epoch3, step774]: loss 0.585125
[epoch3, step775]: loss 0.546716
[epoch3, step776]: loss 0.676484
[epoch3, step777]: loss 0.914519
[epoch3, step778]: loss 1.022218
[epoch3, step779]: loss 0.617769
[epoch3, step780]: loss 1.002128
[epoch3, step781]: loss 0.613321
[epoch3, step782]: loss 0.786055
[epoch3, step783]: loss 0.442910
[epoch3, step784]: loss 0.874454
[epoch3, step785]: loss 0.691848
[epoch3, step786]: loss 0.693430
[epoch3, step787]: loss 0.707408
[epoch3, step788]: loss 0.603918
[epoch3, step789]: loss 0.622690
[epoch3, step790]: loss 0.448738
[epoch3, step791]: loss 0.780802
[epoch3, step792]: loss 0.633082
[epoch3, step793]: loss 0.673227
[epoch3, step794]: loss 0.867266
[epoch3, step795]: loss 0.677813
[epoch3, step796]: loss 0.530113
[epoch3, step797]: loss 0.625591
[epoch3, step798]: loss 0.812060
[epoch3, step799]: loss 0.583130
[epoch3, step800]: loss 0.781550
[epoch3, step801]: loss 0.613201
[epoch3, step802]: loss 0.606275
[epoch3, step803]: loss 0.700445
[epoch3, step804]: loss 0.817784
[epoch3, step805]: loss 0.757411
[epoch3, step806]: loss 0.469158
[epoch3, step807]: loss 0.727991
[epoch3, step808]: loss 0.607461
[epoch3, step809]: loss 0.861709
[epoch3, step810]: loss 0.795600
[epoch3, step811]: loss 0.901426
[epoch3, step812]: loss 0.713624
[epoch3, step813]: loss 0.576956
[epoch3, step814]: loss 0.716017
[epoch3, step815]: loss 0.874104
[epoch3, step816]: loss 0.401098
[epoch3, step817]: loss 0.459468
[epoch3, step818]: loss 0.976733
[epoch3, step819]: loss 0.657365
[epoch3, step820]: loss 0.722230
[epoch3, step821]: loss 0.723836
[epoch3, step822]: loss 0.682521
[epoch3, step823]: loss 0.696936
[epoch3, step824]: loss 0.624169
[epoch3, step825]: loss 0.635480
[epoch3, step826]: loss 0.827571
[epoch3, step827]: loss 0.985300
[epoch3, step828]: loss 0.832114
[epoch3, step829]: loss 0.801682
[epoch3, step830]: loss 0.725053
[epoch3, step831]: loss 0.877614
[epoch3, step832]: loss 0.816956
[epoch3, step833]: loss 0.799835
[epoch3, step834]: loss 0.714813
[epoch3, step835]: loss 0.536348
[epoch3, step836]: loss 0.619519
[epoch3, step837]: loss 0.551304
[epoch3, step838]: loss 0.633119
[epoch3, step839]: loss 0.805997
[epoch3, step840]: loss 0.722148
[epoch3, step841]: loss 0.640392
[epoch3, step842]: loss 0.632411
[epoch3, step843]: loss 0.633806
[epoch3, step844]: loss 0.926708
[epoch3, step845]: loss 0.696059
[epoch3, step846]: loss 0.855790
[epoch3, step847]: loss 0.594096
[epoch3, step848]: loss 0.467628
[epoch3, step849]: loss 0.688326
[epoch3, step850]: loss 0.687290
[epoch3, step851]: loss 0.654758
[epoch3, step852]: loss 0.705099
[epoch3, step853]: loss 0.831845
[epoch3, step854]: loss 0.799760
[epoch3, step855]: loss 0.322093
[epoch3, step856]: loss 0.698614
[epoch3, step857]: loss 0.800769
[epoch3, step858]: loss 0.797381
[epoch3, step859]: loss 0.764014
[epoch3, step860]: loss 0.866163
[epoch3, step861]: loss 0.327419
[epoch3, step862]: loss 0.791052
[epoch3, step863]: loss 0.951784
[epoch3, step864]: loss 0.701369
[epoch3, step865]: loss 0.719592
[epoch3, step866]: loss 0.700192
[epoch3, step867]: loss 0.604176
[epoch3, step868]: loss 0.785440
[epoch3, step869]: loss 0.908876
[epoch3, step870]: loss 0.598958
[epoch3, step871]: loss 0.830729
[epoch3, step872]: loss 0.667966
[epoch3, step873]: loss 0.644902
[epoch3, step874]: loss 0.696039
[epoch3, step875]: loss 0.264946
[epoch3, step876]: loss 0.690580
[epoch3, step877]: loss 0.823871
[epoch3, step878]: loss 0.911347
[epoch3, step879]: loss 0.523385
[epoch3, step880]: loss 0.301427
[epoch3, step881]: loss 0.433973
[epoch3, step882]: loss 0.695759
[epoch3, step883]: loss 0.458574
[epoch3, step884]: loss 0.880634
[epoch3, step885]: loss 0.436216
[epoch3, step886]: loss 0.621253
[epoch3, step887]: loss 0.626710
[epoch3, step888]: loss 0.920616
[epoch3, step889]: loss 0.668339
[epoch3, step890]: loss 0.888830
[epoch3, step891]: loss 0.620723
[epoch3, step892]: loss 0.850696
[epoch3, step893]: loss 0.624744
[epoch3, step894]: loss 0.381276
[epoch3, step895]: loss 0.338184
[epoch3, step896]: loss 0.691243
[epoch3, step897]: loss 0.468324
[epoch3, step898]: loss 0.590262
[epoch3, step899]: loss 1.063610
[epoch3, step900]: loss 0.788968
[epoch3, step901]: loss 0.658487
[epoch3, step902]: loss 0.724344
[epoch3, step903]: loss 0.864019
[epoch3, step904]: loss 0.658657
[epoch3, step905]: loss 0.646230
[epoch3, step906]: loss 0.743920
[epoch3, step907]: loss 0.851539
[epoch3, step908]: loss 0.473438
[epoch3, step909]: loss 0.670412
[epoch3, step910]: loss 0.893820
[epoch3, step911]: loss 0.772881
[epoch3, step912]: loss 0.819626
[epoch3, step913]: loss 0.876692
[epoch3, step914]: loss 0.498605
[epoch3, step915]: loss 0.580327
[epoch3, step916]: loss 0.850873
[epoch3, step917]: loss 0.690349
[epoch3, step918]: loss 0.713850
[epoch3, step919]: loss 0.657253
[epoch3, step920]: loss 0.672887
[epoch3, step921]: loss 0.710778
[epoch3, step922]: loss 0.700264
[epoch3, step923]: loss 0.814418
[epoch3, step924]: loss 0.934269
[epoch3, step925]: loss 0.670360
[epoch3, step926]: loss 0.707882
[epoch3, step927]: loss 0.723039
[epoch3, step928]: loss 0.345312
[epoch3, step929]: loss 0.714562
[epoch3, step930]: loss 0.835869
[epoch3, step931]: loss 0.556839
[epoch3, step932]: loss 0.779713
[epoch3, step933]: loss 0.719685
[epoch3, step934]: loss 0.762239
[epoch3, step935]: loss 0.812162
[epoch3, step936]: loss 0.927654
[epoch3, step937]: loss 0.766971
[epoch3, step938]: loss 0.755521
[epoch3, step939]: loss 0.729916
[epoch3, step940]: loss 0.626411
[epoch3, step941]: loss 0.609773
[epoch3, step942]: loss 0.872201
[epoch3, step943]: loss 0.540957
[epoch3, step944]: loss 0.793984
[epoch3, step945]: loss 0.806196
[epoch3, step946]: loss 0.734452
[epoch3, step947]: loss 0.461917
[epoch3, step948]: loss 0.676822
[epoch3, step949]: loss 0.842156
[epoch3, step950]: loss 0.746871
[epoch3, step951]: loss 0.684656
[epoch3, step952]: loss 0.571391
[epoch3, step953]: loss 0.979982
[epoch3, step954]: loss 0.417400
[epoch3, step955]: loss 0.784781
[epoch3, step956]: loss 0.786782
[epoch3, step957]: loss 0.850923
[epoch3, step958]: loss 0.592858
[epoch3, step959]: loss 0.786331
[epoch3, step960]: loss 0.816351
[epoch3, step961]: loss 0.796767
[epoch3, step962]: loss 0.548733
[epoch3, step963]: loss 0.888844
[epoch3, step964]: loss 0.829536
[epoch3, step965]: loss 0.638184
[epoch3, step966]: loss 0.668874
[epoch3, step967]: loss 0.811751
[epoch3, step968]: loss 0.714508
[epoch3, step969]: loss 0.575327
[epoch3, step970]: loss 0.620419
[epoch3, step971]: loss 1.013558
[epoch3, step972]: loss 0.869978
[epoch3, step973]: loss 0.729586
[epoch3, step974]: loss 0.489815
[epoch3, step975]: loss 0.670843
[epoch3, step976]: loss 0.472107
[epoch3, step977]: loss 0.819139
[epoch3, step978]: loss 0.662880
[epoch3, step979]: loss 0.561748
[epoch3, step980]: loss 0.957803
[epoch3, step981]: loss 0.738708
[epoch3, step982]: loss 0.963880
[epoch3, step983]: loss 0.632983
[epoch3, step984]: loss 0.548617
[epoch3, step985]: loss 0.699079
[epoch3, step986]: loss 0.699173
[epoch3, step987]: loss 0.386138
[epoch3, step988]: loss 0.719924
[epoch3, step989]: loss 0.472561
[epoch3, step990]: loss 0.447432
[epoch3, step991]: loss 0.907629
[epoch3, step992]: loss 0.493624
[epoch3, step993]: loss 0.404172
[epoch3, step994]: loss 0.532387
[epoch3, step995]: loss 0.376246
[epoch3, step996]: loss 0.239309
[epoch3, step997]: loss 0.839833
[epoch3, step998]: loss 0.606218
[epoch3, step999]: loss 0.458402
[epoch3, step1000]: loss 0.739876
[epoch3, step1001]: loss 0.757323
[epoch3, step1002]: loss 0.477065
[epoch3, step1003]: loss 0.669813
[epoch3, step1004]: loss 0.932832
[epoch3, step1005]: loss 0.824466
[epoch3, step1006]: loss 0.587073
[epoch3, step1007]: loss 0.595965
[epoch3, step1008]: loss 0.611247
[epoch3, step1009]: loss 0.409832
[epoch3, step1010]: loss 0.612412
[epoch3, step1011]: loss 0.616382
[epoch3, step1012]: loss 0.597139
[epoch3, step1013]: loss 0.636676
[epoch3, step1014]: loss 0.877227
[epoch3, step1015]: loss 0.777005
[epoch3, step1016]: loss 0.606982
[epoch3, step1017]: loss 0.763502
[epoch3, step1018]: loss 0.681076
[epoch3, step1019]: loss 0.433001
[epoch3, step1020]: loss 0.676926
[epoch3, step1021]: loss 0.589837
[epoch3, step1022]: loss 0.712021
[epoch3, step1023]: loss 0.679834
[epoch3, step1024]: loss 0.876941
[epoch3, step1025]: loss 0.624923
[epoch3, step1026]: loss 0.601513
[epoch3, step1027]: loss 0.632291
[epoch3, step1028]: loss 0.384654
[epoch3, step1029]: loss 0.394492
[epoch3, step1030]: loss 0.904618
[epoch3, step1031]: loss 0.644394
[epoch3, step1032]: loss 0.674797
[epoch3, step1033]: loss 0.832175
[epoch3, step1034]: loss 0.469712
[epoch3, step1035]: loss 0.773264
[epoch3, step1036]: loss 0.767295
[epoch3, step1037]: loss 0.626790
[epoch3, step1038]: loss 0.713528
[epoch3, step1039]: loss 0.623326
[epoch3, step1040]: loss 0.677631
[epoch3, step1041]: loss 0.617577
[epoch3, step1042]: loss 0.860371
[epoch3, step1043]: loss 0.885564
[epoch3, step1044]: loss 0.606844
[epoch3, step1045]: loss 0.286459
[epoch3, step1046]: loss 0.720427
[epoch3, step1047]: loss 0.625584
[epoch3, step1048]: loss 0.747972
[epoch3, step1049]: loss 0.661788
[epoch3, step1050]: loss 0.766060
[epoch3, step1051]: loss 0.758839
[epoch3, step1052]: loss 0.824051
[epoch3, step1053]: loss 0.702140
[epoch3, step1054]: loss 0.847976
[epoch3, step1055]: loss 0.730308
[epoch3, step1056]: loss 0.526255
[epoch3, step1057]: loss 0.618923
[epoch3, step1058]: loss 0.946718
[epoch3, step1059]: loss 0.451614
[epoch3, step1060]: loss 0.764417
[epoch3, step1061]: loss 0.560018
[epoch3, step1062]: loss 0.839108
[epoch3, step1063]: loss 0.781003
[epoch3, step1064]: loss 0.894665
[epoch3, step1065]: loss 0.730696
[epoch3, step1066]: loss 0.792018
[epoch3, step1067]: loss 0.812113
[epoch3, step1068]: loss 0.506634
[epoch3, step1069]: loss 0.636602
[epoch3, step1070]: loss 0.694296
[epoch3, step1071]: loss 0.810256
[epoch3, step1072]: loss 0.558133
[epoch3, step1073]: loss 0.726844
[epoch3, step1074]: loss 0.645129
[epoch3, step1075]: loss 0.923822
[epoch3, step1076]: loss 0.815652
[epoch3, step1077]: loss 0.623585
[epoch3, step1078]: loss 0.455076
[epoch3, step1079]: loss 0.767530
[epoch3, step1080]: loss 0.531464
[epoch3, step1081]: loss 0.732278
[epoch3, step1082]: loss 0.382204
[epoch3, step1083]: loss 0.601048
[epoch3, step1084]: loss 0.962470
[epoch3, step1085]: loss 0.745756
[epoch3, step1086]: loss 0.723043
[epoch3, step1087]: loss 0.951815
[epoch3, step1088]: loss 0.637897
[epoch3, step1089]: loss 0.812368
[epoch3, step1090]: loss 0.742487
[epoch3, step1091]: loss 0.863214
[epoch3, step1092]: loss 0.707989
[epoch3, step1093]: loss 0.752781
[epoch3, step1094]: loss 0.741924
[epoch3, step1095]: loss 0.464853
[epoch3, step1096]: loss 0.546129
[epoch3, step1097]: loss 0.644280
[epoch3, step1098]: loss 0.682061
[epoch3, step1099]: loss 0.643935
[epoch3, step1100]: loss 0.686045
[epoch3, step1101]: loss 0.565087
[epoch3, step1102]: loss 0.760197
[epoch3, step1103]: loss 0.586384
[epoch3, step1104]: loss 0.666934
[epoch3, step1105]: loss 0.723827
[epoch3, step1106]: loss 0.530422
[epoch3, step1107]: loss 0.818629
[epoch3, step1108]: loss 0.359701
[epoch3, step1109]: loss 0.568846
[epoch3, step1110]: loss 0.824403
[epoch3, step1111]: loss 0.627303
[epoch3, step1112]: loss 0.842594
[epoch3, step1113]: loss 0.710711
[epoch3, step1114]: loss 0.725429
[epoch3, step1115]: loss 0.487173
[epoch3, step1116]: loss 0.614123
[epoch3, step1117]: loss 0.654167
[epoch3, step1118]: loss 0.665957
[epoch3, step1119]: loss 0.708225
[epoch3, step1120]: loss 0.581588
[epoch3, step1121]: loss 0.545904
[epoch3, step1122]: loss 0.686089
[epoch3, step1123]: loss 0.743189
[epoch3, step1124]: loss 0.695031
[epoch3, step1125]: loss 0.314233
[epoch3, step1126]: loss 0.564246
[epoch3, step1127]: loss 0.673540
[epoch3, step1128]: loss 0.655443
[epoch3, step1129]: loss 0.677330
[epoch3, step1130]: loss 0.778571
[epoch3, step1131]: loss 0.498290
[epoch3, step1132]: loss 0.805421
[epoch3, step1133]: loss 0.676837
[epoch3, step1134]: loss 0.676499
[epoch3, step1135]: loss 0.572823
[epoch3, step1136]: loss 0.776808
[epoch3, step1137]: loss 0.688247
[epoch3, step1138]: loss 0.838283
[epoch3, step1139]: loss 0.804375
[epoch3, step1140]: loss 0.720922
[epoch3, step1141]: loss 0.569490
[epoch3, step1142]: loss 0.746463
[epoch3, step1143]: loss 0.490553
[epoch3, step1144]: loss 0.587226
[epoch3, step1145]: loss 0.670164
[epoch3, step1146]: loss 0.416358
[epoch3, step1147]: loss 0.500988
[epoch3, step1148]: loss 0.542388
[epoch3, step1149]: loss 0.428174
[epoch3, step1150]: loss 0.712329
[epoch3, step1151]: loss 0.778857
[epoch3, step1152]: loss 0.617224
[epoch3, step1153]: loss 0.432707
[epoch3, step1154]: loss 0.601844
[epoch3, step1155]: loss 0.635113
[epoch3, step1156]: loss 0.579146
[epoch3, step1157]: loss 0.853626
[epoch3, step1158]: loss 0.947090
[epoch3, step1159]: loss 0.696074
[epoch3, step1160]: loss 0.678361
[epoch3, step1161]: loss 0.688845
[epoch3, step1162]: loss 0.565581
[epoch3, step1163]: loss 0.674383
[epoch3, step1164]: loss 0.571306
[epoch3, step1165]: loss 0.687239
[epoch3, step1166]: loss 0.814859
[epoch3, step1167]: loss 0.744144
[epoch3, step1168]: loss 0.711770
[epoch3, step1169]: loss 0.400300
[epoch3, step1170]: loss 0.659433
[epoch3, step1171]: loss 0.689965
[epoch3, step1172]: loss 0.597862
[epoch3, step1173]: loss 0.847747
[epoch3, step1174]: loss 0.713351
[epoch3, step1175]: loss 0.828234
[epoch3, step1176]: loss 0.526963
[epoch3, step1177]: loss 0.747722
[epoch3, step1178]: loss 0.383434
[epoch3, step1179]: loss 0.589968
[epoch3, step1180]: loss 0.696300
[epoch3, step1181]: loss 0.650597
[epoch3, step1182]: loss 0.777469
[epoch3, step1183]: loss 0.981614
[epoch3, step1184]: loss 0.614189
[epoch3, step1185]: loss 0.746837
[epoch3, step1186]: loss 0.597189
[epoch3, step1187]: loss 0.561148
[epoch3, step1188]: loss 0.903151
[epoch3, step1189]: loss 0.774210
[epoch3, step1190]: loss 0.705617
[epoch3, step1191]: loss 0.689503
[epoch3, step1192]: loss 0.783844
[epoch3, step1193]: loss 0.651896
[epoch3, step1194]: loss 0.791158
[epoch3, step1195]: loss 0.849993
[epoch3, step1196]: loss 0.701018
[epoch3, step1197]: loss 0.846417
[epoch3, step1198]: loss 0.825230
[epoch3, step1199]: loss 0.782344
[epoch3, step1200]: loss 0.859530
[epoch3, step1201]: loss 0.884778
[epoch3, step1202]: loss 0.751829
[epoch3, step1203]: loss 0.506185
[epoch3, step1204]: loss 0.819912
[epoch3, step1205]: loss 0.705946
[epoch3, step1206]: loss 0.587930
[epoch3, step1207]: loss 0.689505
[epoch3, step1208]: loss 0.641106
[epoch3, step1209]: loss 0.939566
[epoch3, step1210]: loss 0.706968
[epoch3, step1211]: loss 1.037941
[epoch3, step1212]: loss 0.587133
[epoch3, step1213]: loss 0.775570
[epoch3, step1214]: loss 0.775407
[epoch3, step1215]: loss 0.842035
[epoch3, step1216]: loss 0.767509
[epoch3, step1217]: loss 0.450779
[epoch3, step1218]: loss 0.726643
[epoch3, step1219]: loss 0.677098
[epoch3, step1220]: loss 0.453090
[epoch3, step1221]: loss 0.632520
[epoch3, step1222]: loss 0.665369
[epoch3, step1223]: loss 0.556182
[epoch3, step1224]: loss 0.712156
[epoch3, step1225]: loss 1.054287
[epoch3, step1226]: loss 0.709151
[epoch3, step1227]: loss 0.502803
[epoch3, step1228]: loss 0.782471
[epoch3, step1229]: loss 0.724593
[epoch3, step1230]: loss 0.720930
[epoch3, step1231]: loss 0.630715
[epoch3, step1232]: loss 0.436066
[epoch3, step1233]: loss 0.423494
[epoch3, step1234]: loss 0.666577
[epoch3, step1235]: loss 0.809866
[epoch3, step1236]: loss 0.543811
[epoch3, step1237]: loss 0.492662
[epoch3, step1238]: loss 0.686526
[epoch3, step1239]: loss 0.705649
[epoch3, step1240]: loss 0.822362
[epoch3, step1241]: loss 0.606825
[epoch3, step1242]: loss 0.884961
[epoch3, step1243]: loss 0.847739
[epoch3, step1244]: loss 0.699971
[epoch3, step1245]: loss 0.591824
[epoch3, step1246]: loss 0.999980
[epoch3, step1247]: loss 0.984825
[epoch3, step1248]: loss 0.469948
[epoch3, step1249]: loss 0.726350
[epoch3, step1250]: loss 0.603043
[epoch3, step1251]: loss 0.689689
[epoch3, step1252]: loss 0.654531
[epoch3, step1253]: loss 0.463202
[epoch3, step1254]: loss 0.491162
[epoch3, step1255]: loss 0.794575
[epoch3, step1256]: loss 0.982151
[epoch3, step1257]: loss 0.687869
[epoch3, step1258]: loss 1.094251
[epoch3, step1259]: loss 0.554479
[epoch3, step1260]: loss 0.584395
[epoch3, step1261]: loss 0.632407
[epoch3, step1262]: loss 0.325483
[epoch3, step1263]: loss 0.640391
[epoch3, step1264]: loss 0.803648
[epoch3, step1265]: loss 0.401207
[epoch3, step1266]: loss 0.831256
[epoch3, step1267]: loss 0.586588
[epoch3, step1268]: loss 0.420725
[epoch3, step1269]: loss 0.602186
[epoch3, step1270]: loss 0.660143
[epoch3, step1271]: loss 0.397841
[epoch3, step1272]: loss 0.622462
[epoch3, step1273]: loss 0.427892
[epoch3, step1274]: loss 0.329517
[epoch3, step1275]: loss 0.746294
[epoch3, step1276]: loss 0.631368
[epoch3, step1277]: loss 0.797535
[epoch3, step1278]: loss 0.778407
[epoch3, step1279]: loss 0.907919
[epoch3, step1280]: loss 0.764244
[epoch3, step1281]: loss 0.393557
[epoch3, step1282]: loss 0.911395
[epoch3, step1283]: loss 0.555219
[epoch3, step1284]: loss 0.678213
[epoch3, step1285]: loss 0.633713
[epoch3, step1286]: loss 0.713169
[epoch3, step1287]: loss 0.732842
[epoch3, step1288]: loss 0.718622
[epoch3, step1289]: loss 0.675992
[epoch3, step1290]: loss 0.587683
[epoch3, step1291]: loss 0.683852
[epoch3, step1292]: loss 0.839692
[epoch3, step1293]: loss 0.874236
[epoch3, step1294]: loss 0.253785
[epoch3, step1295]: loss 0.747263
[epoch3, step1296]: loss 0.690088
[epoch3, step1297]: loss 0.889275
[epoch3, step1298]: loss 0.623044
[epoch3, step1299]: loss 0.953737
[epoch3, step1300]: loss 0.898279
[epoch3, step1301]: loss 0.942358
[epoch3, step1302]: loss 0.793677
[epoch3, step1303]: loss 0.824871
[epoch3, step1304]: loss 0.722206
[epoch3, step1305]: loss 0.816930
[epoch3, step1306]: loss 0.494304
[epoch3, step1307]: loss 0.716601
[epoch3, step1308]: loss 0.686858
[epoch3, step1309]: loss 0.841080
[epoch3, step1310]: loss 0.534138
[epoch3, step1311]: loss 0.574653
[epoch3, step1312]: loss 0.667730
[epoch3, step1313]: loss 0.855500
[epoch3, step1314]: loss 0.564928
[epoch3, step1315]: loss 0.731965
[epoch3, step1316]: loss 0.455311
[epoch3, step1317]: loss 0.686418
[epoch3, step1318]: loss 0.755703
[epoch3, step1319]: loss 0.636376
[epoch3, step1320]: loss 0.512998
[epoch3, step1321]: loss 0.791782
[epoch3, step1322]: loss 0.840050
[epoch3, step1323]: loss 0.635239
[epoch3, step1324]: loss 0.606559
[epoch3, step1325]: loss 0.823182
[epoch3, step1326]: loss 0.569272
[epoch3, step1327]: loss 0.513484
[epoch3, step1328]: loss 0.503510
[epoch3, step1329]: loss 0.829911
[epoch3, step1330]: loss 0.774668
[epoch3, step1331]: loss 0.886279
[epoch3, step1332]: loss 0.487447
[epoch3, step1333]: loss 0.649652
[epoch3, step1334]: loss 0.834626
[epoch3, step1335]: loss 0.729454
[epoch3, step1336]: loss 0.410879
[epoch3, step1337]: loss 0.795517
[epoch3, step1338]: loss 0.522211
[epoch3, step1339]: loss 0.698748
[epoch3, step1340]: loss 0.793871
[epoch3, step1341]: loss 0.778651
[epoch3, step1342]: loss 0.436745
[epoch3, step1343]: loss 0.691896
[epoch3, step1344]: loss 0.949109
[epoch3, step1345]: loss 0.659592
[epoch3, step1346]: loss 0.737927
[epoch3, step1347]: loss 0.727149
[epoch3, step1348]: loss 0.685787
[epoch3, step1349]: loss 0.821267
[epoch3, step1350]: loss 0.613504
[epoch3, step1351]: loss 0.587857
[epoch3, step1352]: loss 0.754117
[epoch3, step1353]: loss 0.813075
[epoch3, step1354]: loss 0.316947
[epoch3, step1355]: loss 0.682528
[epoch3, step1356]: loss 0.415941
[epoch3, step1357]: loss 0.683966
[epoch3, step1358]: loss 0.656322
[epoch3, step1359]: loss 0.795007
[epoch3, step1360]: loss 0.611373
[epoch3, step1361]: loss 0.875837
[epoch3, step1362]: loss 0.647834
[epoch3, step1363]: loss 0.637178
[epoch3, step1364]: loss 0.553986
[epoch3, step1365]: loss 0.744052
[epoch3, step1366]: loss 0.740375
[epoch3, step1367]: loss 0.714358
[epoch3, step1368]: loss 0.375350
[epoch3, step1369]: loss 0.788821
[epoch3, step1370]: loss 0.601037
[epoch3, step1371]: loss 0.686632
[epoch3, step1372]: loss 0.400789
[epoch3, step1373]: loss 0.663639
[epoch3, step1374]: loss 0.668224
[epoch3, step1375]: loss 0.810734
[epoch3, step1376]: loss 0.727242
[epoch3, step1377]: loss 0.674634
[epoch3, step1378]: loss 0.555680
[epoch3, step1379]: loss 0.777802
[epoch3, step1380]: loss 0.702925
[epoch3, step1381]: loss 0.422396
[epoch3, step1382]: loss 0.498591
[epoch3, step1383]: loss 0.997339
[epoch3, step1384]: loss 0.800477
[epoch3, step1385]: loss 0.638418
[epoch3, step1386]: loss 0.663836
[epoch3, step1387]: loss 0.571698
[epoch3, step1388]: loss 0.650905
[epoch3, step1389]: loss 0.597667
[epoch3, step1390]: loss 0.525050
[epoch3, step1391]: loss 0.865649
[epoch3, step1392]: loss 0.511022
[epoch3, step1393]: loss 0.797374
[epoch3, step1394]: loss 0.778807
[epoch3, step1395]: loss 1.070619
[epoch3, step1396]: loss 0.498714
[epoch3, step1397]: loss 0.560662
[epoch3, step1398]: loss 0.792975
[epoch3, step1399]: loss 0.818346
[epoch3, step1400]: loss 0.596040
[epoch3, step1401]: loss 0.635964
[epoch3, step1402]: loss 0.660834
[epoch3, step1403]: loss 0.695042
[epoch3, step1404]: loss 0.520248
[epoch3, step1405]: loss 0.816796
[epoch3, step1406]: loss 0.695569
[epoch3, step1407]: loss 0.506236
[epoch3, step1408]: loss 0.781709
[epoch3, step1409]: loss 0.664318
[epoch3, step1410]: loss 0.435819
[epoch3, step1411]: loss 0.629506
[epoch3, step1412]: loss 0.515081
[epoch3, step1413]: loss 0.937789
[epoch3, step1414]: loss 0.860274
[epoch3, step1415]: loss 0.839156
[epoch3, step1416]: loss 0.680266
[epoch3, step1417]: loss 1.024268
[epoch3, step1418]: loss 0.775684
[epoch3, step1419]: loss 0.628170
[epoch3, step1420]: loss 0.470894
[epoch3, step1421]: loss 0.678854
[epoch3, step1422]: loss 0.828715
[epoch3, step1423]: loss 0.862406
[epoch3, step1424]: loss 0.630614
[epoch3, step1425]: loss 0.611249
[epoch3, step1426]: loss 0.682343
[epoch3, step1427]: loss 0.566906
[epoch3, step1428]: loss 0.644779
[epoch3, step1429]: loss 0.695419
[epoch3, step1430]: loss 0.871888
[epoch3, step1431]: loss 0.497044
[epoch3, step1432]: loss 0.611395
[epoch3, step1433]: loss 0.838250
[epoch3, step1434]: loss 0.640036
[epoch3, step1435]: loss 0.641360
[epoch3, step1436]: loss 0.487404
[epoch3, step1437]: loss 0.666290
[epoch3, step1438]: loss 0.879877
[epoch3, step1439]: loss 0.248299
[epoch3, step1440]: loss 0.825090
[epoch3, step1441]: loss 0.736760
[epoch3, step1442]: loss 0.655271
[epoch3, step1443]: loss 0.664319
[epoch3, step1444]: loss 0.623779
[epoch3, step1445]: loss 0.663641
[epoch3, step1446]: loss 0.863394
[epoch3, step1447]: loss 0.875171
[epoch3, step1448]: loss 0.799903
[epoch3, step1449]: loss 0.707687
[epoch3, step1450]: loss 0.843745
[epoch3, step1451]: loss 0.786148
[epoch3, step1452]: loss 0.939316
[epoch3, step1453]: loss 0.558690
[epoch3, step1454]: loss 0.699156
[epoch3, step1455]: loss 0.889492
[epoch3, step1456]: loss 0.636850
[epoch3, step1457]: loss 0.623872
[epoch3, step1458]: loss 0.331656
[epoch3, step1459]: loss 0.412589
[epoch3, step1460]: loss 0.710394
[epoch3, step1461]: loss 0.764643
[epoch3, step1462]: loss 0.464576
[epoch3, step1463]: loss 0.751801
[epoch3, step1464]: loss 0.437787
[epoch3, step1465]: loss 0.686395
[epoch3, step1466]: loss 0.941928
[epoch3, step1467]: loss 0.852014
[epoch3, step1468]: loss 0.944989
[epoch3, step1469]: loss 0.633681
[epoch3, step1470]: loss 0.655265
[epoch3, step1471]: loss 0.888590
[epoch3, step1472]: loss 0.837149
[epoch3, step1473]: loss 0.581516
[epoch3, step1474]: loss 0.719713
[epoch3, step1475]: loss 0.550550
[epoch3, step1476]: loss 0.822163
[epoch3, step1477]: loss 0.620142
[epoch3, step1478]: loss 0.801707
[epoch3, step1479]: loss 0.646928
[epoch3, step1480]: loss 0.716777
[epoch3, step1481]: loss 0.642691
[epoch3, step1482]: loss 0.457851
[epoch3, step1483]: loss 0.556004
[epoch3, step1484]: loss 0.705003
[epoch3, step1485]: loss 0.550595
[epoch3, step1486]: loss 0.882460
[epoch3, step1487]: loss 0.797310
[epoch3, step1488]: loss 0.804008
[epoch3, step1489]: loss 0.500847
[epoch3, step1490]: loss 0.841684
[epoch3, step1491]: loss 0.663911
[epoch3, step1492]: loss 0.806127
[epoch3, step1493]: loss 0.749896
[epoch3, step1494]: loss 0.864941
[epoch3, step1495]: loss 0.617318
[epoch3, step1496]: loss 0.573500
[epoch3, step1497]: loss 0.574376
[epoch3, step1498]: loss 0.617628
[epoch3, step1499]: loss 0.634663
[epoch3, step1500]: loss 0.574314
[epoch3, step1501]: loss 0.470118
[epoch3, step1502]: loss 0.707985
[epoch3, step1503]: loss 0.745574
[epoch3, step1504]: loss 0.689003
[epoch3, step1505]: loss 0.836175
[epoch3, step1506]: loss 0.451714
[epoch3, step1507]: loss 0.689461
[epoch3, step1508]: loss 0.697591
[epoch3, step1509]: loss 0.549709
[epoch3, step1510]: loss 0.577337
[epoch3, step1511]: loss 0.661183
[epoch3, step1512]: loss 0.594151
[epoch3, step1513]: loss 0.777899
[epoch3, step1514]: loss 0.365406
[epoch3, step1515]: loss 0.723475
[epoch3, step1516]: loss 0.699160
[epoch3, step1517]: loss 0.710374
[epoch3, step1518]: loss 0.759377
[epoch3, step1519]: loss 0.653123
[epoch3, step1520]: loss 0.528015
[epoch3, step1521]: loss 0.521531
[epoch3, step1522]: loss 0.796628
[epoch3, step1523]: loss 0.606674
[epoch3, step1524]: loss 0.669927
[epoch3, step1525]: loss 0.730518
[epoch3, step1526]: loss 0.584031
[epoch3, step1527]: loss 0.718268
[epoch3, step1528]: loss 0.621698
[epoch3, step1529]: loss 0.609491
[epoch3, step1530]: loss 0.604297
[epoch3, step1531]: loss 0.809662
[epoch3, step1532]: loss 0.777859
[epoch3, step1533]: loss 0.897851
[epoch3, step1534]: loss 0.908035
[epoch3, step1535]: loss 0.643349
[epoch3, step1536]: loss 0.589364
[epoch3, step1537]: loss 0.703813
[epoch3, step1538]: loss 0.944730
[epoch3, step1539]: loss 0.578484
[epoch3, step1540]: loss 0.588973
[epoch3, step1541]: loss 0.671334
[epoch3, step1542]: loss 0.957185
[epoch3, step1543]: loss 0.634010
[epoch3, step1544]: loss 0.519021
[epoch3, step1545]: loss 0.849937
[epoch3, step1546]: loss 0.688006
[epoch3, step1547]: loss 0.937279
[epoch3, step1548]: loss 0.569826
[epoch3, step1549]: loss 0.527457
[epoch3, step1550]: loss 0.619874
[epoch3, step1551]: loss 0.861378
[epoch3, step1552]: loss 0.517913
[epoch3, step1553]: loss 0.516454
[epoch3, step1554]: loss 0.441508
[epoch3, step1555]: loss 0.470881
[epoch3, step1556]: loss 0.815177
[epoch3, step1557]: loss 0.411295
[epoch3, step1558]: loss 0.818502
[epoch3, step1559]: loss 0.560131
[epoch3, step1560]: loss 0.694398
[epoch3, step1561]: loss 0.870083
[epoch3, step1562]: loss 0.798433
[epoch3, step1563]: loss 0.977905
[epoch3, step1564]: loss 0.442577
[epoch3, step1565]: loss 0.231190
[epoch3, step1566]: loss 0.378771
[epoch3, step1567]: loss 0.699997
[epoch3, step1568]: loss 0.667429
[epoch3, step1569]: loss 0.890467
[epoch3, step1570]: loss 0.393139
[epoch3, step1571]: loss 0.841075
[epoch3, step1572]: loss 0.600290
[epoch3, step1573]: loss 0.371768
[epoch3, step1574]: loss 0.887046
[epoch3, step1575]: loss 0.675587
[epoch3, step1576]: loss 0.690336
[epoch3, step1577]: loss 0.672380
[epoch3, step1578]: loss 0.463425
[epoch3, step1579]: loss 0.849502
[epoch3, step1580]: loss 0.685204
[epoch3, step1581]: loss 0.851892
[epoch3, step1582]: loss 0.693729
[epoch3, step1583]: loss 0.702204
[epoch3, step1584]: loss 0.814967
[epoch3, step1585]: loss 0.624193
[epoch3, step1586]: loss 0.851228
[epoch3, step1587]: loss 0.876034
[epoch3, step1588]: loss 0.833520
[epoch3, step1589]: loss 0.718853
[epoch3, step1590]: loss 0.462340
[epoch3, step1591]: loss 0.638894
[epoch3, step1592]: loss 0.729894
[epoch3, step1593]: loss 0.746136
[epoch3, step1594]: loss 0.775515
[epoch3, step1595]: loss 0.805680
[epoch3, step1596]: loss 0.663318
[epoch3, step1597]: loss 0.445624
[epoch3, step1598]: loss 0.611861
[epoch3, step1599]: loss 0.439385
[epoch3, step1600]: loss 0.631305
[epoch3, step1601]: loss 0.746147
[epoch3, step1602]: loss 0.741504
[epoch3, step1603]: loss 0.792615
[epoch3, step1604]: loss 0.986398
[epoch3, step1605]: loss 0.506837
[epoch3, step1606]: loss 0.582798
[epoch3, step1607]: loss 0.729818
[epoch3, step1608]: loss 0.725461
[epoch3, step1609]: loss 0.645988
[epoch3, step1610]: loss 0.655646
[epoch3, step1611]: loss 0.618621
[epoch3, step1612]: loss 0.838269
[epoch3, step1613]: loss 0.579810
[epoch3, step1614]: loss 0.619255
[epoch3, step1615]: loss 0.788381
[epoch3, step1616]: loss 0.797508
[epoch3, step1617]: loss 0.764603
[epoch3, step1618]: loss 0.920345
[epoch3, step1619]: loss 0.794375
[epoch3, step1620]: loss 0.729382
[epoch3, step1621]: loss 0.941816
[epoch3, step1622]: loss 0.674460
[epoch3, step1623]: loss 1.032677
[epoch3, step1624]: loss 0.604791
[epoch3, step1625]: loss 0.879464
[epoch3, step1626]: loss 0.876481
[epoch3, step1627]: loss 0.614652
[epoch3, step1628]: loss 0.849979
[epoch3, step1629]: loss 1.060490
[epoch3, step1630]: loss 0.530954
[epoch3, step1631]: loss 0.700423
[epoch3, step1632]: loss 0.740759
[epoch3, step1633]: loss 0.540335
[epoch3, step1634]: loss 0.682389
[epoch3, step1635]: loss 0.741999
[epoch3, step1636]: loss 0.565504
[epoch3, step1637]: loss 0.747856
[epoch3, step1638]: loss 0.752008
[epoch3, step1639]: loss 0.573122
[epoch3, step1640]: loss 0.810133
[epoch3, step1641]: loss 0.755011
[epoch3, step1642]: loss 0.664026
[epoch3, step1643]: loss 0.592456
[epoch3, step1644]: loss 0.693419
[epoch3, step1645]: loss 0.567978
[epoch3, step1646]: loss 0.644142
[epoch3, step1647]: loss 0.435348
[epoch3, step1648]: loss 0.742159
[epoch3, step1649]: loss 0.805338
[epoch3, step1650]: loss 0.801756
[epoch3, step1651]: loss 0.824516
[epoch3, step1652]: loss 0.654738
[epoch3, step1653]: loss 0.426885
[epoch3, step1654]: loss 0.350580
[epoch3, step1655]: loss 0.887343
[epoch3, step1656]: loss 0.737174
[epoch3, step1657]: loss 0.872893
[epoch3, step1658]: loss 0.649502
[epoch3, step1659]: loss 0.400625
[epoch3, step1660]: loss 0.473667
[epoch3, step1661]: loss 0.544600
[epoch3, step1662]: loss 0.740232
[epoch3, step1663]: loss 0.835595
[epoch3, step1664]: loss 0.646756
[epoch3, step1665]: loss 0.863106
[epoch3, step1666]: loss 1.022926
[epoch3, step1667]: loss 0.893036
[epoch3, step1668]: loss 0.384514
[epoch3, step1669]: loss 0.633916
[epoch3, step1670]: loss 0.708964
[epoch3, step1671]: loss 0.694023
[epoch3, step1672]: loss 0.646533
[epoch3, step1673]: loss 0.500773
[epoch3, step1674]: loss 0.255288
[epoch3, step1675]: loss 0.655049
[epoch3, step1676]: loss 0.662042
[epoch3, step1677]: loss 0.353521
[epoch3, step1678]: loss 0.566784
[epoch3, step1679]: loss 0.745167
[epoch3, step1680]: loss 0.461833
[epoch3, step1681]: loss 0.836342
[epoch3, step1682]: loss 0.454386
[epoch3, step1683]: loss 0.584625
[epoch3, step1684]: loss 0.824982
[epoch3, step1685]: loss 0.671396
[epoch3, step1686]: loss 0.743076
[epoch3, step1687]: loss 0.598968
[epoch3, step1688]: loss 0.679016
[epoch3, step1689]: loss 0.682838
[epoch3, step1690]: loss 0.762024
[epoch3, step1691]: loss 0.705977
[epoch3, step1692]: loss 0.610511
[epoch3, step1693]: loss 0.602766
[epoch3, step1694]: loss 0.601607
[epoch3, step1695]: loss 0.735194
[epoch3, step1696]: loss 0.764248
[epoch3, step1697]: loss 0.808941
[epoch3, step1698]: loss 0.601883
[epoch3, step1699]: loss 0.611469
[epoch3, step1700]: loss 0.682355
[epoch3, step1701]: loss 0.695864
[epoch3, step1702]: loss 0.779745
[epoch3, step1703]: loss 0.715680
[epoch3, step1704]: loss 0.642029
[epoch3, step1705]: loss 1.102551
[epoch3, step1706]: loss 0.617375
[epoch3, step1707]: loss 0.656069
[epoch3, step1708]: loss 0.587928
[epoch3, step1709]: loss 0.535578
[epoch3, step1710]: loss 0.656395
[epoch3, step1711]: loss 0.747381
[epoch3, step1712]: loss 0.865376
[epoch3, step1713]: loss 0.920442
[epoch3, step1714]: loss 0.663541
[epoch3, step1715]: loss 0.865899
[epoch3, step1716]: loss 0.684067
[epoch3, step1717]: loss 0.569137
[epoch3, step1718]: loss 0.922969
[epoch3, step1719]: loss 0.504148
[epoch3, step1720]: loss 0.809632
[epoch3, step1721]: loss 0.871333
[epoch3, step1722]: loss 0.688715
[epoch3, step1723]: loss 0.845616
[epoch3, step1724]: loss 0.535903
[epoch3, step1725]: loss 1.039114
[epoch3, step1726]: loss 0.505589
[epoch3, step1727]: loss 0.929498
[epoch3, step1728]: loss 0.753462
[epoch3, step1729]: loss 0.730270
[epoch3, step1730]: loss 0.787129
[epoch3, step1731]: loss 0.557118
[epoch3, step1732]: loss 0.696100
[epoch3, step1733]: loss 0.644309
[epoch3, step1734]: loss 0.634655
[epoch3, step1735]: loss 0.803552
[epoch3, step1736]: loss 0.347190
[epoch3, step1737]: loss 0.764746
[epoch3, step1738]: loss 0.709602
[epoch3, step1739]: loss 0.456959
[epoch3, step1740]: loss 0.832996
[epoch3, step1741]: loss 0.621294
[epoch3, step1742]: loss 0.430906
[epoch3, step1743]: loss 0.960544
[epoch3, step1744]: loss 0.286497
[epoch3, step1745]: loss 0.620307
[epoch3, step1746]: loss 0.144501
[epoch3, step1747]: loss 0.774504
[epoch3, step1748]: loss 0.664985
[epoch3, step1749]: loss 0.583590
[epoch3, step1750]: loss 0.654750
[epoch3, step1751]: loss 0.645589
[epoch3, step1752]: loss 0.873205
[epoch3, step1753]: loss 0.619911
[epoch3, step1754]: loss 0.510626
[epoch3, step1755]: loss 0.504648
[epoch3, step1756]: loss 0.728229
[epoch3, step1757]: loss 0.706133
[epoch3, step1758]: loss 0.636608
[epoch3, step1759]: loss 0.541315
[epoch3, step1760]: loss 0.945187
[epoch3, step1761]: loss 0.719558
[epoch3, step1762]: loss 0.484570
[epoch3, step1763]: loss 0.784797
[epoch3, step1764]: loss 0.538464
[epoch3, step1765]: loss 0.775054
[epoch3, step1766]: loss 0.598011
[epoch3, step1767]: loss 0.838241
[epoch3, step1768]: loss 0.584880
[epoch3, step1769]: loss 0.765647
[epoch3, step1770]: loss 0.488880
[epoch3, step1771]: loss 0.333729
[epoch3, step1772]: loss 0.854548
[epoch3, step1773]: loss 0.620803
[epoch3, step1774]: loss 0.811464
[epoch3, step1775]: loss 0.716652
[epoch3, step1776]: loss 0.982279
[epoch3, step1777]: loss 0.629387
[epoch3, step1778]: loss 0.899451
[epoch3, step1779]: loss 0.587473
[epoch3, step1780]: loss 0.742923
[epoch3, step1781]: loss 0.820592
[epoch3, step1782]: loss 0.644935
[epoch3, step1783]: loss 0.422872
[epoch3, step1784]: loss 0.796600
[epoch3, step1785]: loss 0.613797
[epoch3, step1786]: loss 0.705344
[epoch3, step1787]: loss 0.748270
[epoch3, step1788]: loss 0.672690
[epoch3, step1789]: loss 0.706523
[epoch3, step1790]: loss 0.563795
[epoch3, step1791]: loss 0.336304
[epoch3, step1792]: loss 0.568844
[epoch3, step1793]: loss 0.742434
[epoch3, step1794]: loss 0.706058
[epoch3, step1795]: loss 0.915197
[epoch3, step1796]: loss 0.509212
[epoch3, step1797]: loss 0.328263
[epoch3, step1798]: loss 0.960441
[epoch3, step1799]: loss 0.702964
[epoch3, step1800]: loss 0.671628
[epoch3, step1801]: loss 0.278269
[epoch3, step1802]: loss 0.602536
[epoch3, step1803]: loss 0.664542
[epoch3, step1804]: loss 0.655584
[epoch3, step1805]: loss 0.632469
[epoch3, step1806]: loss 0.709527
[epoch3, step1807]: loss 0.719696
[epoch3, step1808]: loss 0.727456
[epoch3, step1809]: loss 0.688079
[epoch3, step1810]: loss 0.819388
[epoch3, step1811]: loss 0.882057
[epoch3, step1812]: loss 0.775901
[epoch3, step1813]: loss 0.667745
[epoch3, step1814]: loss 0.736868
[epoch3, step1815]: loss 0.420350
[epoch3, step1816]: loss 0.532969
[epoch3, step1817]: loss 0.964532
[epoch3, step1818]: loss 0.709938
[epoch3, step1819]: loss 0.872739
[epoch3, step1820]: loss 0.799313
[epoch3, step1821]: loss 0.605916
[epoch3, step1822]: loss 0.845320
[epoch3, step1823]: loss 0.585075
[epoch3, step1824]: loss 0.610134
[epoch3, step1825]: loss 0.682176
[epoch3, step1826]: loss 0.424683
[epoch3, step1827]: loss 0.526292
[epoch3, step1828]: loss 0.660017
[epoch3, step1829]: loss 0.719687
[epoch3, step1830]: loss 0.780192
[epoch3, step1831]: loss 0.677428
[epoch3, step1832]: loss 0.571547
[epoch3, step1833]: loss 0.597126
[epoch3, step1834]: loss 0.992318
[epoch3, step1835]: loss 0.561436
[epoch3, step1836]: loss 0.710370
[epoch3, step1837]: loss 0.310738
[epoch3, step1838]: loss 0.557396
[epoch3, step1839]: loss 0.520753
[epoch3, step1840]: loss 0.813608
[epoch3, step1841]: loss 0.757637
[epoch3, step1842]: loss 0.782800
[epoch3, step1843]: loss 0.705766
[epoch3, step1844]: loss 0.976964
[epoch3, step1845]: loss 0.648214
[epoch3, step1846]: loss 0.701202
[epoch3, step1847]: loss 0.370772
[epoch3, step1848]: loss 0.609509
[epoch3, step1849]: loss 0.754101
[epoch3, step1850]: loss 0.723459
[epoch3, step1851]: loss 0.638827
[epoch3, step1852]: loss 1.079941
[epoch3, step1853]: loss 0.662895
[epoch3, step1854]: loss 0.589912
[epoch3, step1855]: loss 0.892537
[epoch3, step1856]: loss 0.542028
[epoch3, step1857]: loss 0.617772
[epoch3, step1858]: loss 0.669079
[epoch3, step1859]: loss 0.652626
[epoch3, step1860]: loss 0.505971
[epoch3, step1861]: loss 0.637216
[epoch3, step1862]: loss 0.647507
[epoch3, step1863]: loss 0.567546
[epoch3, step1864]: loss 0.405426
[epoch3, step1865]: loss 0.797725
[epoch3, step1866]: loss 0.784761
[epoch3, step1867]: loss 0.661897
[epoch3, step1868]: loss 0.666830
[epoch3, step1869]: loss 0.539087
[epoch3, step1870]: loss 0.804010
[epoch3, step1871]: loss 0.842783
[epoch3, step1872]: loss 0.337915
[epoch3, step1873]: loss 0.537919
[epoch3, step1874]: loss 0.844802
[epoch3, step1875]: loss 0.281023
[epoch3, step1876]: loss 0.911459
[epoch3, step1877]: loss 0.447347
[epoch3, step1878]: loss 0.710386
[epoch3, step1879]: loss 0.280357
[epoch3, step1880]: loss 0.580175
[epoch3, step1881]: loss 0.678639
[epoch3, step1882]: loss 0.666234
[epoch3, step1883]: loss 0.673090
[epoch3, step1884]: loss 0.740431
[epoch3, step1885]: loss 0.550998
[epoch3, step1886]: loss 0.574357
[epoch3, step1887]: loss 0.987925
[epoch3, step1888]: loss 0.588903
[epoch3, step1889]: loss 0.711466
[epoch3, step1890]: loss 0.500503
[epoch3, step1891]: loss 0.676116
[epoch3, step1892]: loss 0.392094
[epoch3, step1893]: loss 0.544327
[epoch3, step1894]: loss 0.879332
[epoch3, step1895]: loss 0.630231
[epoch3, step1896]: loss 0.574306
[epoch3, step1897]: loss 0.780363
[epoch3, step1898]: loss 0.761874
[epoch3, step1899]: loss 0.655470
[epoch3, step1900]: loss 0.692703
[epoch3, step1901]: loss 0.561088
[epoch3, step1902]: loss 0.516913
[epoch3, step1903]: loss 0.696756
[epoch3, step1904]: loss 0.671179
[epoch3, step1905]: loss 0.867497
[epoch3, step1906]: loss 0.804826
[epoch3, step1907]: loss 0.500599
[epoch3, step1908]: loss 0.588319
[epoch3, step1909]: loss 0.705730
[epoch3, step1910]: loss 0.877143
[epoch3, step1911]: loss 0.466606
[epoch3, step1912]: loss 0.411502
[epoch3, step1913]: loss 0.613509
[epoch3, step1914]: loss 0.785803
[epoch3, step1915]: loss 0.915353
[epoch3, step1916]: loss 0.402210
[epoch3, step1917]: loss 0.736175
[epoch3, step1918]: loss 0.732871
[epoch3, step1919]: loss 0.877598
[epoch3, step1920]: loss 0.689588
[epoch3, step1921]: loss 0.704189
[epoch3, step1922]: loss 0.367128
[epoch3, step1923]: loss 0.796023
[epoch3, step1924]: loss 0.520801
[epoch3, step1925]: loss 0.735165
[epoch3, step1926]: loss 0.452890
[epoch3, step1927]: loss 0.801756
[epoch3, step1928]: loss 0.803581
[epoch3, step1929]: loss 0.686243
[epoch3, step1930]: loss 0.506426
[epoch3, step1931]: loss 0.884303
[epoch3, step1932]: loss 0.701527
[epoch3, step1933]: loss 0.717542
[epoch3, step1934]: loss 0.550975
[epoch3, step1935]: loss 0.862290
[epoch3, step1936]: loss 0.562589
[epoch3, step1937]: loss 0.781597
[epoch3, step1938]: loss 0.859166
[epoch3, step1939]: loss 0.327563
[epoch3, step1940]: loss 0.728664
[epoch3, step1941]: loss 0.524223
[epoch3, step1942]: loss 0.526463
[epoch3, step1943]: loss 0.929709
[epoch3, step1944]: loss 0.561966
[epoch3, step1945]: loss 0.773943
[epoch3, step1946]: loss 0.782026
[epoch3, step1947]: loss 0.740379
[epoch3, step1948]: loss 0.586905
[epoch3, step1949]: loss 0.651166
[epoch3, step1950]: loss 0.551868
[epoch3, step1951]: loss 0.462434
[epoch3, step1952]: loss 0.646043
[epoch3, step1953]: loss 0.808678
[epoch3, step1954]: loss 0.719327
[epoch3, step1955]: loss 0.652088
[epoch3, step1956]: loss 0.809388
[epoch3, step1957]: loss 0.552619
[epoch3, step1958]: loss 0.743114
[epoch3, step1959]: loss 0.517386
[epoch3, step1960]: loss 0.631959
[epoch3, step1961]: loss 0.455604
[epoch3, step1962]: loss 0.963820
[epoch3, step1963]: loss 0.808524
[epoch3, step1964]: loss 0.601431
[epoch3, step1965]: loss 0.813147
[epoch3, step1966]: loss 0.712737
[epoch3, step1967]: loss 0.713254
[epoch3, step1968]: loss 0.808442
[epoch3, step1969]: loss 0.723835
[epoch3, step1970]: loss 0.742600
[epoch3, step1971]: loss 0.421132
[epoch3, step1972]: loss 0.664994
[epoch3, step1973]: loss 0.857800
[epoch3, step1974]: loss 0.797158
[epoch3, step1975]: loss 0.844099
[epoch3, step1976]: loss 0.788728
[epoch3, step1977]: loss 0.819691
[epoch3, step1978]: loss 0.507330
[epoch3, step1979]: loss 0.608778
[epoch3, step1980]: loss 0.767090
[epoch3, step1981]: loss 0.344771
[epoch3, step1982]: loss 0.886024
[epoch3, step1983]: loss 0.831764
[epoch3, step1984]: loss 0.790970
[epoch3, step1985]: loss 0.388206
[epoch3, step1986]: loss 0.666732
[epoch3, step1987]: loss 0.337046
[epoch3, step1988]: loss 0.824092
[epoch3, step1989]: loss 0.476525
[epoch3, step1990]: loss 0.417479
[epoch3, step1991]: loss 0.814249
[epoch3, step1992]: loss 0.761500
[epoch3, step1993]: loss 0.787865
[epoch3, step1994]: loss 0.756703
[epoch3, step1995]: loss 0.694068
[epoch3, step1996]: loss 0.675383
[epoch3, step1997]: loss 0.855137
[epoch3, step1998]: loss 0.436544
[epoch3, step1999]: loss 0.624837
[epoch3, step2000]: loss 0.842493
[epoch3, step2001]: loss 0.643611
[epoch3, step2002]: loss 0.757603
[epoch3, step2003]: loss 0.482239
[epoch3, step2004]: loss 0.624553
[epoch3, step2005]: loss 0.935297
[epoch3, step2006]: loss 0.723293
[epoch3, step2007]: loss 0.682214
[epoch3, step2008]: loss 0.982699
[epoch3, step2009]: loss 0.807931
[epoch3, step2010]: loss 0.510623
[epoch3, step2011]: loss 0.690685
[epoch3, step2012]: loss 0.603806
[epoch3, step2013]: loss 0.608247
[epoch3, step2014]: loss 0.738126
[epoch3, step2015]: loss 0.746337
[epoch3, step2016]: loss 0.689900
[epoch3, step2017]: loss 0.577525
[epoch3, step2018]: loss 0.645850
[epoch3, step2019]: loss 0.651248
[epoch3, step2020]: loss 0.582291
[epoch3, step2021]: loss 0.641809
[epoch3, step2022]: loss 0.389962
[epoch3, step2023]: loss 0.626920
[epoch3, step2024]: loss 0.694871
[epoch3, step2025]: loss 0.496639
[epoch3, step2026]: loss 0.915626
[epoch3, step2027]: loss 0.791515
[epoch3, step2028]: loss 0.886476
[epoch3, step2029]: loss 0.790845
[epoch3, step2030]: loss 0.638817
[epoch3, step2031]: loss 0.640605
[epoch3, step2032]: loss 0.769381
[epoch3, step2033]: loss 0.672427
[epoch3, step2034]: loss 0.491949
[epoch3, step2035]: loss 0.564231
[epoch3, step2036]: loss 0.698355
[epoch3, step2037]: loss 0.438192
[epoch3, step2038]: loss 0.524161
[epoch3, step2039]: loss 0.730728
[epoch3, step2040]: loss 0.474159
[epoch3, step2041]: loss 0.602993
[epoch3, step2042]: loss 0.639783
[epoch3, step2043]: loss 0.414192
[epoch3, step2044]: loss 0.769624
[epoch3, step2045]: loss 0.418694
[epoch3, step2046]: loss 0.725588
[epoch3, step2047]: loss 0.572892
[epoch3, step2048]: loss 0.757388
[epoch3, step2049]: loss 0.466926
[epoch3, step2050]: loss 0.436492
[epoch3, step2051]: loss 0.463099
[epoch3, step2052]: loss 0.685570
[epoch3, step2053]: loss 0.627416
[epoch3, step2054]: loss 0.540567
[epoch3, step2055]: loss 0.802423
[epoch3, step2056]: loss 0.561263
[epoch3, step2057]: loss 0.786023
[epoch3, step2058]: loss 0.448906
[epoch3, step2059]: loss 0.744186
[epoch3, step2060]: loss 0.548393
[epoch3, step2061]: loss 0.547609
[epoch3, step2062]: loss 0.547047
[epoch3, step2063]: loss 0.798043
[epoch3, step2064]: loss 0.530874
[epoch3, step2065]: loss 0.661422
[epoch3, step2066]: loss 0.708519
[epoch3, step2067]: loss 0.734787
[epoch3, step2068]: loss 0.559603
[epoch3, step2069]: loss 0.671116
[epoch3, step2070]: loss 0.563239
[epoch3, step2071]: loss 0.729794
[epoch3, step2072]: loss 0.699287
[epoch3, step2073]: loss 0.472759
[epoch3, step2074]: loss 0.618265
[epoch3, step2075]: loss 0.671986
[epoch3, step2076]: loss 0.600401
[epoch3, step2077]: loss 0.642169
[epoch3, step2078]: loss 0.705508
[epoch3, step2079]: loss 0.724583
[epoch3, step2080]: loss 0.623312
[epoch3, step2081]: loss 0.646860
[epoch3, step2082]: loss 0.839593
[epoch3, step2083]: loss 0.649774
[epoch3, step2084]: loss 0.711076
[epoch3, step2085]: loss 0.928814
[epoch3, step2086]: loss 0.460731
[epoch3, step2087]: loss 0.599845
[epoch3, step2088]: loss 0.829771
[epoch3, step2089]: loss 0.442513
[epoch3, step2090]: loss 0.562540
[epoch3, step2091]: loss 0.592447
[epoch3, step2092]: loss 0.342509
[epoch3, step2093]: loss 0.738740
[epoch3, step2094]: loss 0.705283
[epoch3, step2095]: loss 0.967245
[epoch3, step2096]: loss 0.762178
[epoch3, step2097]: loss 0.546456
[epoch3, step2098]: loss 0.457038
[epoch3, step2099]: loss 0.428813
[epoch3, step2100]: loss 0.734827
[epoch3, step2101]: loss 0.789059
[epoch3, step2102]: loss 0.789848
[epoch3, step2103]: loss 0.449176
[epoch3, step2104]: loss 0.589019
[epoch3, step2105]: loss 0.626089
[epoch3, step2106]: loss 0.921877
[epoch3, step2107]: loss 0.687687
[epoch3, step2108]: loss 0.695735
[epoch3, step2109]: loss 0.562793
[epoch3, step2110]: loss 0.828592
[epoch3, step2111]: loss 0.297113
[epoch3, step2112]: loss 0.455466
[epoch3, step2113]: loss 0.594906
[epoch3, step2114]: loss 0.506765
[epoch3, step2115]: loss 0.812895
[epoch3, step2116]: loss 0.778840
[epoch3, step2117]: loss 0.753504
[epoch3, step2118]: loss 0.608570
[epoch3, step2119]: loss 0.741456
[epoch3, step2120]: loss 0.601736
[epoch3, step2121]: loss 0.880743
[epoch3, step2122]: loss 0.903269
[epoch3, step2123]: loss 0.798810
[epoch3, step2124]: loss 0.653059
[epoch3, step2125]: loss 0.608181
[epoch3, step2126]: loss 0.569609
[epoch3, step2127]: loss 0.672458
[epoch3, step2128]: loss 0.447332
[epoch3, step2129]: loss 0.575416
[epoch3, step2130]: loss 0.552607
[epoch3, step2131]: loss 0.844202
[epoch3, step2132]: loss 0.387987
[epoch3, step2133]: loss 0.635416
[epoch3, step2134]: loss 0.496145
[epoch3, step2135]: loss 0.892245
[epoch3, step2136]: loss 0.716031
[epoch3, step2137]: loss 0.548067
[epoch3, step2138]: loss 0.672394
[epoch3, step2139]: loss 0.685607
[epoch3, step2140]: loss 0.569439
[epoch3, step2141]: loss 0.505834
[epoch3, step2142]: loss 0.679194
[epoch3, step2143]: loss 0.470190
[epoch3, step2144]: loss 0.650695
[epoch3, step2145]: loss 0.536159
[epoch3, step2146]: loss 0.519550
[epoch3, step2147]: loss 0.680646
[epoch3, step2148]: loss 0.646117
[epoch3, step2149]: loss 0.322669
[epoch3, step2150]: loss 0.477307
[epoch3, step2151]: loss 0.656883
[epoch3, step2152]: loss 0.737405
[epoch3, step2153]: loss 0.613942
[epoch3, step2154]: loss 0.630193
[epoch3, step2155]: loss 0.587790
[epoch3, step2156]: loss 0.595668
[epoch3, step2157]: loss 0.428337
[epoch3, step2158]: loss 0.438604
[epoch3, step2159]: loss 0.837303
[epoch3, step2160]: loss 0.691334
[epoch3, step2161]: loss 0.691362
[epoch3, step2162]: loss 0.786959
[epoch3, step2163]: loss 0.660734
[epoch3, step2164]: loss 0.742436
[epoch3, step2165]: loss 0.582025
[epoch3, step2166]: loss 0.568668
[epoch3, step2167]: loss 0.605111
[epoch3, step2168]: loss 0.690384
[epoch3, step2169]: loss 0.745907
[epoch3, step2170]: loss 0.556689
[epoch3, step2171]: loss 0.550582
[epoch3, step2172]: loss 0.775191
[epoch3, step2173]: loss 0.601811
[epoch3, step2174]: loss 0.636756
[epoch3, step2175]: loss 0.780043
[epoch3, step2176]: loss 0.704595
[epoch3, step2177]: loss 0.460072
[epoch3, step2178]: loss 0.547679
[epoch3, step2179]: loss 0.615718
[epoch3, step2180]: loss 0.599946
[epoch3, step2181]: loss 0.852903
[epoch3, step2182]: loss 0.621515
[epoch3, step2183]: loss 0.344923
[epoch3, step2184]: loss 0.778707
[epoch3, step2185]: loss 0.475399
[epoch3, step2186]: loss 0.593957
[epoch3, step2187]: loss 0.486320
[epoch3, step2188]: loss 0.867697
[epoch3, step2189]: loss 0.818014
[epoch3, step2190]: loss 0.370515
[epoch3, step2191]: loss 0.891688
[epoch3, step2192]: loss 0.610858
[epoch3, step2193]: loss 0.571957
[epoch3, step2194]: loss 0.539109
[epoch3, step2195]: loss 0.589365
[epoch3, step2196]: loss 0.621726
[epoch3, step2197]: loss 0.472437
[epoch3, step2198]: loss 0.732658
[epoch3, step2199]: loss 0.753386
[epoch3, step2200]: loss 0.822867
[epoch3, step2201]: loss 0.564417
[epoch3, step2202]: loss 0.907666
[epoch3, step2203]: loss 0.472472
[epoch3, step2204]: loss 0.681101
[epoch3, step2205]: loss 0.751314
[epoch3, step2206]: loss 0.688546
[epoch3, step2207]: loss 0.789473
[epoch3, step2208]: loss 0.892811
[epoch3, step2209]: loss 0.582057
[epoch3, step2210]: loss 0.527229
[epoch3, step2211]: loss 0.387880
[epoch3, step2212]: loss 0.617644
[epoch3, step2213]: loss 0.829158
[epoch3, step2214]: loss 0.554618
[epoch3, step2215]: loss 0.478073
[epoch3, step2216]: loss 0.542331
[epoch3, step2217]: loss 0.886253
[epoch3, step2218]: loss 0.653869
[epoch3, step2219]: loss 0.737763
[epoch3, step2220]: loss 0.582546
[epoch3, step2221]: loss 0.779672
[epoch3, step2222]: loss 0.987960
[epoch3, step2223]: loss 0.301946
[epoch3, step2224]: loss 0.665584
[epoch3, step2225]: loss 0.517129
[epoch3, step2226]: loss 0.689198
[epoch3, step2227]: loss 0.700552
[epoch3, step2228]: loss 0.808963
[epoch3, step2229]: loss 0.700261
[epoch3, step2230]: loss 0.473946
[epoch3, step2231]: loss 0.455510
[epoch3, step2232]: loss 0.800657
[epoch3, step2233]: loss 0.478958
[epoch3, step2234]: loss 0.670066
[epoch3, step2235]: loss 0.701793
[epoch3, step2236]: loss 0.618727
[epoch3, step2237]: loss 0.614018
[epoch3, step2238]: loss 0.440653
[epoch3, step2239]: loss 0.848063
[epoch3, step2240]: loss 0.796425
[epoch3, step2241]: loss 0.617818
[epoch3, step2242]: loss 0.686509
[epoch3, step2243]: loss 0.602371
[epoch3, step2244]: loss 0.848904
[epoch3, step2245]: loss 0.822570
[epoch3, step2246]: loss 0.701214
[epoch3, step2247]: loss 0.783598
[epoch3, step2248]: loss 0.953852
[epoch3, step2249]: loss 0.213840
[epoch3, step2250]: loss 0.600469
[epoch3, step2251]: loss 0.644281
[epoch3, step2252]: loss 0.782060
[epoch3, step2253]: loss 0.573210
[epoch3, step2254]: loss 0.431185
[epoch3, step2255]: loss 0.983373
[epoch3, step2256]: loss 0.265624
[epoch3, step2257]: loss 0.718801
[epoch3, step2258]: loss 0.359178
[epoch3, step2259]: loss 0.238789
[epoch3, step2260]: loss 0.511908
[epoch3, step2261]: loss 0.721462
[epoch3, step2262]: loss 0.791677
[epoch3, step2263]: loss 0.527626
[epoch3, step2264]: loss 0.785071
[epoch3, step2265]: loss 0.437686
[epoch3, step2266]: loss 0.734439
[epoch3, step2267]: loss 0.565369
[epoch3, step2268]: loss 0.719965
[epoch3, step2269]: loss 0.966667
[epoch3, step2270]: loss 0.695544
[epoch3, step2271]: loss 0.525859
[epoch3, step2272]: loss 0.604142
[epoch3, step2273]: loss 0.845186
[epoch3, step2274]: loss 0.560377
[epoch3, step2275]: loss 0.520691
[epoch3, step2276]: loss 0.712079
[epoch3, step2277]: loss 0.606281
[epoch3, step2278]: loss 0.734690
[epoch3, step2279]: loss 0.798520
[epoch3, step2280]: loss 0.874443
[epoch3, step2281]: loss 0.568736
[epoch3, step2282]: loss 0.803631
[epoch3, step2283]: loss 0.518731
[epoch3, step2284]: loss 0.513269
[epoch3, step2285]: loss 0.813726
[epoch3, step2286]: loss 0.760116
[epoch3, step2287]: loss 0.620855
[epoch3, step2288]: loss 0.719303
[epoch3, step2289]: loss 0.516533
[epoch3, step2290]: loss 0.760739
[epoch3, step2291]: loss 0.493058
[epoch3, step2292]: loss 0.751175
[epoch3, step2293]: loss 0.728297
[epoch3, step2294]: loss 0.458270
[epoch3, step2295]: loss 0.724225
[epoch3, step2296]: loss 0.421558
[epoch3, step2297]: loss 0.844379
[epoch3, step2298]: loss 0.610182
[epoch3, step2299]: loss 0.671509
[epoch3, step2300]: loss 0.766771
[epoch3, step2301]: loss 0.549622
[epoch3, step2302]: loss 0.685280
[epoch3, step2303]: loss 0.641162
[epoch3, step2304]: loss 0.651980
[epoch3, step2305]: loss 0.638552
[epoch3, step2306]: loss 0.686329
[epoch3, step2307]: loss 0.705055
[epoch3, step2308]: loss 0.822919
[epoch3, step2309]: loss 0.654635
[epoch3, step2310]: loss 0.790119
[epoch3, step2311]: loss 0.563582
[epoch3, step2312]: loss 0.580420
[epoch3, step2313]: loss 0.644217
[epoch3, step2314]: loss 0.808247
[epoch3, step2315]: loss 0.545857
[epoch3, step2316]: loss 0.663359
[epoch3, step2317]: loss 0.793295
[epoch3, step2318]: loss 0.465968
[epoch3, step2319]: loss 0.708588
[epoch3, step2320]: loss 0.436489
[epoch3, step2321]: loss 0.528068
[epoch3, step2322]: loss 0.759012
[epoch3, step2323]: loss 0.451145
[epoch3, step2324]: loss 0.645549
[epoch3, step2325]: loss 0.503076
[epoch3, step2326]: loss 0.857428
[epoch3, step2327]: loss 0.709272
[epoch3, step2328]: loss 0.610220
[epoch3, step2329]: loss 0.533563
[epoch3, step2330]: loss 0.818385
[epoch3, step2331]: loss 0.503386
[epoch3, step2332]: loss 0.816601
[epoch3, step2333]: loss 0.599522
[epoch3, step2334]: loss 0.788900
[epoch3, step2335]: loss 0.519934
[epoch3, step2336]: loss 0.576905
[epoch3, step2337]: loss 0.737555
[epoch3, step2338]: loss 0.647493
[epoch3, step2339]: loss 0.688427
[epoch3, step2340]: loss 0.645670
[epoch3, step2341]: loss 0.531601
[epoch3, step2342]: loss 0.796711
[epoch3, step2343]: loss 0.432752
[epoch3, step2344]: loss 0.847279
[epoch3, step2345]: loss 0.781235
[epoch3, step2346]: loss 0.970600
[epoch3, step2347]: loss 0.603720
[epoch3, step2348]: loss 0.863688
[epoch3, step2349]: loss 0.697685
[epoch3, step2350]: loss 0.712152
[epoch3, step2351]: loss 0.722440
[epoch3, step2352]: loss 0.616948
[epoch3, step2353]: loss 0.637719
[epoch3, step2354]: loss 0.374349
[epoch3, step2355]: loss 0.839870
[epoch3, step2356]: loss 0.401157
[epoch3, step2357]: loss 0.540324
[epoch3, step2358]: loss 0.848204
[epoch3, step2359]: loss 0.504772
[epoch3, step2360]: loss 0.617493
[epoch3, step2361]: loss 0.429398
[epoch3, step2362]: loss 0.768212
[epoch3, step2363]: loss 0.686093
[epoch3, step2364]: loss 0.646240
[epoch3, step2365]: loss 0.712819
[epoch3, step2366]: loss 0.481081
[epoch3, step2367]: loss 0.652101
[epoch3, step2368]: loss 0.586513
[epoch3, step2369]: loss 0.772534
[epoch3, step2370]: loss 0.864754
[epoch3, step2371]: loss 0.185107
[epoch3, step2372]: loss 0.753201
[epoch3, step2373]: loss 0.517488
[epoch3, step2374]: loss 0.211885
[epoch3, step2375]: loss 0.532356
[epoch3, step2376]: loss 0.518415
[epoch3, step2377]: loss 0.691415
[epoch3, step2378]: loss 0.857489
[epoch3, step2379]: loss 0.700293
[epoch3, step2380]: loss 0.635606
[epoch3, step2381]: loss 0.454743
[epoch3, step2382]: loss 0.655376
[epoch3, step2383]: loss 0.611953
[epoch3, step2384]: loss 0.582598
[epoch3, step2385]: loss 0.433046
[epoch3, step2386]: loss 0.593710
[epoch3, step2387]: loss 0.482761
[epoch3, step2388]: loss 0.594454
[epoch3, step2389]: loss 0.641213
[epoch3, step2390]: loss 0.804469
[epoch3, step2391]: loss 0.652330
[epoch3, step2392]: loss 0.721508
[epoch3, step2393]: loss 0.714311
[epoch3, step2394]: loss 0.666389
[epoch3, step2395]: loss 0.447289
[epoch3, step2396]: loss 0.809041
[epoch3, step2397]: loss 0.536399
[epoch3, step2398]: loss 0.830786
[epoch3, step2399]: loss 0.401941
[epoch3, step2400]: loss 0.797202
[epoch3, step2401]: loss 0.663271
[epoch3, step2402]: loss 0.579112
[epoch3, step2403]: loss 0.743988
[epoch3, step2404]: loss 0.732509
[epoch3, step2405]: loss 0.628562
[epoch3, step2406]: loss 0.487556
[epoch3, step2407]: loss 0.486684
[epoch3, step2408]: loss 0.424854
[epoch3, step2409]: loss 0.509497
[epoch3, step2410]: loss 0.589209
[epoch3, step2411]: loss 0.745458
[epoch3, step2412]: loss 0.703371
[epoch3, step2413]: loss 0.509426
[epoch3, step2414]: loss 0.536212
[epoch3, step2415]: loss 0.602268
[epoch3, step2416]: loss 0.411969
[epoch3, step2417]: loss 0.475875
[epoch3, step2418]: loss 0.695814
[epoch3, step2419]: loss 0.630436
[epoch3, step2420]: loss 0.733777
[epoch3, step2421]: loss 0.779992
[epoch3, step2422]: loss 0.666333
[epoch3, step2423]: loss 0.721109
[epoch3, step2424]: loss 0.451918
[epoch3, step2425]: loss 0.558179
[epoch3, step2426]: loss 0.409430
[epoch3, step2427]: loss 0.577831
[epoch3, step2428]: loss 0.860789
[epoch3, step2429]: loss 0.235269
[epoch3, step2430]: loss 0.466068
[epoch3, step2431]: loss 0.904949
[epoch3, step2432]: loss 0.700958
[epoch3, step2433]: loss 0.618997
[epoch3, step2434]: loss 0.660660
[epoch3, step2435]: loss 0.864244
[epoch3, step2436]: loss 0.555738
[epoch3, step2437]: loss 0.608068
[epoch3, step2438]: loss 0.802821
[epoch3, step2439]: loss 0.869464
[epoch3, step2440]: loss 0.531066
[epoch3, step2441]: loss 0.703656
[epoch3, step2442]: loss 0.556498
[epoch3, step2443]: loss 0.634748
[epoch3, step2444]: loss 0.955842
[epoch3, step2445]: loss 0.586210
[epoch3, step2446]: loss 0.776213
[epoch3, step2447]: loss 0.873819
[epoch3, step2448]: loss 0.680158
[epoch3, step2449]: loss 0.512875
[epoch3, step2450]: loss 0.729784
[epoch3, step2451]: loss 0.546197
[epoch3, step2452]: loss 0.383090
[epoch3, step2453]: loss 0.742079
[epoch3, step2454]: loss 0.826244
[epoch3, step2455]: loss 0.598623
[epoch3, step2456]: loss 0.547160
[epoch3, step2457]: loss 0.725109
[epoch3, step2458]: loss 0.681055
[epoch3, step2459]: loss 0.347027
[epoch3, step2460]: loss 0.401111
[epoch3, step2461]: loss 0.795009
[epoch3, step2462]: loss 0.791106
[epoch3, step2463]: loss 0.573924
[epoch3, step2464]: loss 0.693276
[epoch3, step2465]: loss 0.600255
[epoch3, step2466]: loss 0.430626
[epoch3, step2467]: loss 0.574762
[epoch3, step2468]: loss 0.563175
[epoch3, step2469]: loss 0.821014
[epoch3, step2470]: loss 0.650222
[epoch3, step2471]: loss 0.621177
[epoch3, step2472]: loss 0.665300
[epoch3, step2473]: loss 0.574772
[epoch3, step2474]: loss 0.680506
[epoch3, step2475]: loss 0.493333
[epoch3, step2476]: loss 0.683757
[epoch3, step2477]: loss 0.656646
[epoch3, step2478]: loss 0.793696
[epoch3, step2479]: loss 0.505381
[epoch3, step2480]: loss 0.735151
[epoch3, step2481]: loss 0.523987
[epoch3, step2482]: loss 0.626033
[epoch3, step2483]: loss 0.393507
[epoch3, step2484]: loss 0.707006
[epoch3, step2485]: loss 0.653205
[epoch3, step2486]: loss 0.489178
[epoch3, step2487]: loss 0.753964
[epoch3, step2488]: loss 0.758653
[epoch3, step2489]: loss 0.477300
[epoch3, step2490]: loss 0.543209
[epoch3, step2491]: loss 0.660821
[epoch3, step2492]: loss 0.733712
[epoch3, step2493]: loss 0.579086
[epoch3, step2494]: loss 0.787379
[epoch3, step2495]: loss 0.537425
[epoch3, step2496]: loss 0.722672
[epoch3, step2497]: loss 0.482767
[epoch3, step2498]: loss 0.684685
[epoch3, step2499]: loss 0.758229
[epoch3, step2500]: loss 0.591335
[epoch3, step2501]: loss 0.802561
[epoch3, step2502]: loss 0.696899
[epoch3, step2503]: loss 0.641617
[epoch3, step2504]: loss 0.786889
[epoch3, step2505]: loss 0.684990
[epoch3, step2506]: loss 0.634315
[epoch3, step2507]: loss 0.578632
[epoch3, step2508]: loss 0.533176
[epoch3, step2509]: loss 0.435507
[epoch3, step2510]: loss 0.540150
[epoch3, step2511]: loss 0.698021
[epoch3, step2512]: loss 0.489641
[epoch3, step2513]: loss 0.624351
[epoch3, step2514]: loss 0.749579
[epoch3, step2515]: loss 0.580192
[epoch3, step2516]: loss 0.649166
[epoch3, step2517]: loss 0.660255
[epoch3, step2518]: loss 0.818398
[epoch3, step2519]: loss 0.692594
[epoch3, step2520]: loss 0.626568
[epoch3, step2521]: loss 0.941195
[epoch3, step2522]: loss 0.452644
[epoch3, step2523]: loss 0.606798
[epoch3, step2524]: loss 0.428603
[epoch3, step2525]: loss 0.562983
[epoch3, step2526]: loss 0.470284
[epoch3, step2527]: loss 0.562733
[epoch3, step2528]: loss 0.534133
[epoch3, step2529]: loss 0.755685
[epoch3, step2530]: loss 0.773775
[epoch3, step2531]: loss 0.447867
[epoch3, step2532]: loss 0.529814
[epoch3, step2533]: loss 0.699013
[epoch3, step2534]: loss 0.683028
[epoch3, step2535]: loss 0.382692
[epoch3, step2536]: loss 0.695694
[epoch3, step2537]: loss 0.516667
[epoch3, step2538]: loss 0.950062
[epoch3, step2539]: loss 0.677005
[epoch3, step2540]: loss 0.723705
[epoch3, step2541]: loss 0.577196
[epoch3, step2542]: loss 0.802287
[epoch3, step2543]: loss 0.729363
[epoch3, step2544]: loss 0.693365
[epoch3, step2545]: loss 0.559679
[epoch3, step2546]: loss 0.787624
[epoch3, step2547]: loss 0.763806
[epoch3, step2548]: loss 0.687010
[epoch3, step2549]: loss 0.471241
[epoch3, step2550]: loss 0.610010
[epoch3, step2551]: loss 0.618332
[epoch3, step2552]: loss 0.770374
[epoch3, step2553]: loss 0.436431
[epoch3, step2554]: loss 0.341881
[epoch3, step2555]: loss 0.653933
[epoch3, step2556]: loss 0.806622
[epoch3, step2557]: loss 0.781535
[epoch3, step2558]: loss 0.831975
[epoch3, step2559]: loss 0.698338
[epoch3, step2560]: loss 0.659914
[epoch3, step2561]: loss 0.598142
[epoch3, step2562]: loss 0.871590
[epoch3, step2563]: loss 0.608611
[epoch3, step2564]: loss 0.671475
[epoch3, step2565]: loss 0.577214
[epoch3, step2566]: loss 0.543456
[epoch3, step2567]: loss 0.673546
[epoch3, step2568]: loss 0.660239
[epoch3, step2569]: loss 0.764223
[epoch3, step2570]: loss 0.208250
[epoch3, step2571]: loss 0.714475
[epoch3, step2572]: loss 0.747683
[epoch3, step2573]: loss 0.717154
[epoch3, step2574]: loss 0.458403
[epoch3, step2575]: loss 0.611997
[epoch3, step2576]: loss 0.567796
[epoch3, step2577]: loss 0.338643
[epoch3, step2578]: loss 0.848219
[epoch3, step2579]: loss 0.914768
[epoch3, step2580]: loss 0.760656
[epoch3, step2581]: loss 0.822707
[epoch3, step2582]: loss 0.590866
[epoch3, step2583]: loss 0.695479
[epoch3, step2584]: loss 0.800041
[epoch3, step2585]: loss 0.460894
[epoch3, step2586]: loss 0.663227
[epoch3, step2587]: loss 0.576860
[epoch3, step2588]: loss 0.675541
[epoch3, step2589]: loss 0.555965
[epoch3, step2590]: loss 0.764768
[epoch3, step2591]: loss 0.633599
[epoch3, step2592]: loss 0.559300
[epoch3, step2593]: loss 0.484964
[epoch3, step2594]: loss 0.668576
[epoch3, step2595]: loss 0.579423
[epoch3, step2596]: loss 0.679638
[epoch3, step2597]: loss 0.751389
[epoch3, step2598]: loss 0.623809
[epoch3, step2599]: loss 0.812420
[epoch3, step2600]: loss 0.503378
[epoch3, step2601]: loss 0.473098
[epoch3, step2602]: loss 0.349094
[epoch3, step2603]: loss 0.468939
[epoch3, step2604]: loss 0.758352
[epoch3, step2605]: loss 0.721840
[epoch3, step2606]: loss 0.557451
[epoch3, step2607]: loss 0.333850
[epoch3, step2608]: loss 0.622036
[epoch3, step2609]: loss 0.773576
[epoch3, step2610]: loss 0.410312
[epoch3, step2611]: loss 0.781546
[epoch3, step2612]: loss 0.652076
[epoch3, step2613]: loss 0.725871
[epoch3, step2614]: loss 0.646485
[epoch3, step2615]: loss 0.777501
[epoch3, step2616]: loss 0.535570
[epoch3, step2617]: loss 0.161896
[epoch3, step2618]: loss 0.825919
[epoch3, step2619]: loss 0.645354
[epoch3, step2620]: loss 0.755953
[epoch3, step2621]: loss 0.814545
[epoch3, step2622]: loss 0.681104
[epoch3, step2623]: loss 0.770299
[epoch3, step2624]: loss 0.487160
[epoch3, step2625]: loss 0.789219
[epoch3, step2626]: loss 0.909284
[epoch3, step2627]: loss 0.770972
[epoch3, step2628]: loss 0.793707
[epoch3, step2629]: loss 0.460435
[epoch3, step2630]: loss 0.443640
[epoch3, step2631]: loss 0.665627
[epoch3, step2632]: loss 0.806345
[epoch3, step2633]: loss 0.443507
[epoch3, step2634]: loss 0.684456
[epoch3, step2635]: loss 0.818412
[epoch3, step2636]: loss 0.609441
[epoch3, step2637]: loss 0.645768
[epoch3, step2638]: loss 0.862169
[epoch3, step2639]: loss 0.709698
[epoch3, step2640]: loss 0.588343
[epoch3, step2641]: loss 0.420182
[epoch3, step2642]: loss 0.448368
[epoch3, step2643]: loss 0.613628
[epoch3, step2644]: loss 0.637491
[epoch3, step2645]: loss 0.740651
[epoch3, step2646]: loss 0.535315
[epoch3, step2647]: loss 0.904791
[epoch3, step2648]: loss 0.846678
[epoch3, step2649]: loss 0.720499
[epoch3, step2650]: loss 0.514098
[epoch3, step2651]: loss 0.894828
[epoch3, step2652]: loss 0.505346
[epoch3, step2653]: loss 0.819053
[epoch3, step2654]: loss 0.589983
[epoch3, step2655]: loss 0.731067
[epoch3, step2656]: loss 0.802255
[epoch3, step2657]: loss 0.642311
[epoch3, step2658]: loss 0.649465
[epoch3, step2659]: loss 0.592898
[epoch3, step2660]: loss 0.394277
[epoch3, step2661]: loss 0.642297
[epoch3, step2662]: loss 0.405104
[epoch3, step2663]: loss 0.799461
[epoch3, step2664]: loss 0.626058
[epoch3, step2665]: loss 0.658791
[epoch3, step2666]: loss 0.351699
[epoch3, step2667]: loss 0.846594
[epoch3, step2668]: loss 0.578340
[epoch3, step2669]: loss 0.375385
[epoch3, step2670]: loss 0.704993
[epoch3, step2671]: loss 0.583026
[epoch3, step2672]: loss 0.816527
[epoch3, step2673]: loss 0.745334
[epoch3, step2674]: loss 0.678409
[epoch3, step2675]: loss 0.827525
[epoch3, step2676]: loss 0.773108
[epoch3, step2677]: loss 0.662760
[epoch3, step2678]: loss 0.426564
[epoch3, step2679]: loss 0.721705
[epoch3, step2680]: loss 0.629915
[epoch3, step2681]: loss 0.651589
[epoch3, step2682]: loss 0.791364
[epoch3, step2683]: loss 0.580798
[epoch3, step2684]: loss 0.399849
[epoch3, step2685]: loss 0.869325
[epoch3, step2686]: loss 0.772475
[epoch3, step2687]: loss 0.638179
[epoch3, step2688]: loss 0.704759
[epoch3, step2689]: loss 0.675030
[epoch3, step2690]: loss 0.704544
[epoch3, step2691]: loss 0.686498
[epoch3, step2692]: loss 0.761071
[epoch3, step2693]: loss 0.453919
[epoch3, step2694]: loss 0.765273
[epoch3, step2695]: loss 0.767558
[epoch3, step2696]: loss 0.498227
[epoch3, step2697]: loss 0.906788
[epoch3, step2698]: loss 0.607084
[epoch3, step2699]: loss 0.612307
[epoch3, step2700]: loss 0.598003
[epoch3, step2701]: loss 0.653907
[epoch3, step2702]: loss 0.616872
[epoch3, step2703]: loss 0.594430
[epoch3, step2704]: loss 0.550482
[epoch3, step2705]: loss 0.620680
[epoch3, step2706]: loss 0.485274
[epoch3, step2707]: loss 0.507725
[epoch3, step2708]: loss 0.555306
[epoch3, step2709]: loss 0.940694
[epoch3, step2710]: loss 0.679970
[epoch3, step2711]: loss 0.776049
[epoch3, step2712]: loss 0.423859
[epoch3, step2713]: loss 0.709098
[epoch3, step2714]: loss 0.421288
[epoch3, step2715]: loss 0.659907
[epoch3, step2716]: loss 0.472718
[epoch3, step2717]: loss 0.893689
[epoch3, step2718]: loss 0.616856
[epoch3, step2719]: loss 0.580406
[epoch3, step2720]: loss 0.510593
[epoch3, step2721]: loss 0.299816
[epoch3, step2722]: loss 0.669136
[epoch3, step2723]: loss 0.590808
[epoch3, step2724]: loss 0.665958
[epoch3, step2725]: loss 0.595929
[epoch3, step2726]: loss 0.633702
[epoch3, step2727]: loss 0.795941
[epoch3, step2728]: loss 0.548996
[epoch3, step2729]: loss 0.598126
[epoch3, step2730]: loss 0.595037
[epoch3, step2731]: loss 0.723935
[epoch3, step2732]: loss 0.682029
[epoch3, step2733]: loss 0.776151
[epoch3, step2734]: loss 0.511814
[epoch3, step2735]: loss 0.544163
[epoch3, step2736]: loss 0.494187
[epoch3, step2737]: loss 0.780901
[epoch3, step2738]: loss 0.549864
[epoch3, step2739]: loss 0.503024
[epoch3, step2740]: loss 0.269690
[epoch3, step2741]: loss 0.738161
[epoch3, step2742]: loss 0.788101
[epoch3, step2743]: loss 0.792875
[epoch3, step2744]: loss 0.731390
[epoch3, step2745]: loss 0.722348
[epoch3, step2746]: loss 0.518644
[epoch3, step2747]: loss 0.718774
[epoch3, step2748]: loss 0.540948
[epoch3, step2749]: loss 0.430201
[epoch3, step2750]: loss 0.763413
[epoch3, step2751]: loss 0.371135
[epoch3, step2752]: loss 0.789733
[epoch3, step2753]: loss 0.642934
[epoch3, step2754]: loss 0.772888
[epoch3, step2755]: loss 0.502483
[epoch3, step2756]: loss 0.703682
[epoch3, step2757]: loss 0.495706
[epoch3, step2758]: loss 0.750128
[epoch3, step2759]: loss 0.718007
[epoch3, step2760]: loss 0.765890
[epoch3, step2761]: loss 0.763017
[epoch3, step2762]: loss 0.715999
[epoch3, step2763]: loss 0.662598
[epoch3, step2764]: loss 0.753505
[epoch3, step2765]: loss 0.310795
[epoch3, step2766]: loss 0.827333
[epoch3, step2767]: loss 0.800777
[epoch3, step2768]: loss 0.742991
[epoch3, step2769]: loss 0.557986
[epoch3, step2770]: loss 0.646328
[epoch3, step2771]: loss 0.842728
[epoch3, step2772]: loss 0.739298
[epoch3, step2773]: loss 0.503210
[epoch3, step2774]: loss 0.845907
[epoch3, step2775]: loss 0.729292
[epoch3, step2776]: loss 0.639827
[epoch3, step2777]: loss 0.703064
[epoch3, step2778]: loss 0.649634
[epoch3, step2779]: loss 0.769752
[epoch3, step2780]: loss 0.739719
[epoch3, step2781]: loss 0.905431
[epoch3, step2782]: loss 0.688192
[epoch3, step2783]: loss 0.643786
[epoch3, step2784]: loss 0.430910
[epoch3, step2785]: loss 0.645600
[epoch3, step2786]: loss 0.851348
[epoch3, step2787]: loss 0.738848
[epoch3, step2788]: loss 0.751692
[epoch3, step2789]: loss 0.842492
[epoch3, step2790]: loss 0.327367
[epoch3, step2791]: loss 0.554748
[epoch3, step2792]: loss 0.469512
[epoch3, step2793]: loss 0.522034
[epoch3, step2794]: loss 0.367308
[epoch3, step2795]: loss 0.606755
[epoch3, step2796]: loss 0.648904
[epoch3, step2797]: loss 0.486503
[epoch3, step2798]: loss 0.758370
[epoch3, step2799]: loss 0.684575
[epoch3, step2800]: loss 0.734249
[epoch3, step2801]: loss 0.754530
[epoch3, step2802]: loss 0.738970
[epoch3, step2803]: loss 0.748782
[epoch3, step2804]: loss 0.771794
[epoch3, step2805]: loss 0.707181
[epoch3, step2806]: loss 0.220465
[epoch3, step2807]: loss 0.493923
[epoch3, step2808]: loss 0.605483
[epoch3, step2809]: loss 0.666623
[epoch3, step2810]: loss 0.725828
[epoch3, step2811]: loss 0.468091
[epoch3, step2812]: loss 0.618954
[epoch3, step2813]: loss 0.596346
[epoch3, step2814]: loss 0.720525
[epoch3, step2815]: loss 0.472524
[epoch3, step2816]: loss 0.756092
[epoch3, step2817]: loss 0.540210
[epoch3, step2818]: loss 0.617689
[epoch3, step2819]: loss 0.761734
[epoch3, step2820]: loss 0.376256
[epoch3, step2821]: loss 0.421108
[epoch3, step2822]: loss 0.421777
[epoch3, step2823]: loss 0.837082
[epoch3, step2824]: loss 0.641622
[epoch3, step2825]: loss 0.617683
[epoch3, step2826]: loss 0.549342
[epoch3, step2827]: loss 0.899792
[epoch3, step2828]: loss 0.606491
[epoch3, step2829]: loss 0.604493
[epoch3, step2830]: loss 0.573711
[epoch3, step2831]: loss 0.548233
[epoch3, step2832]: loss 0.326239
[epoch3, step2833]: loss 0.837607
[epoch3, step2834]: loss 0.690768
[epoch3, step2835]: loss 0.668453
[epoch3, step2836]: loss 0.723482
[epoch3, step2837]: loss 0.516911
[epoch3, step2838]: loss 0.756341
[epoch3, step2839]: loss 0.679757
[epoch3, step2840]: loss 0.488939
[epoch3, step2841]: loss 0.617851
[epoch3, step2842]: loss 0.756457
[epoch3, step2843]: loss 0.659306
[epoch3, step2844]: loss 0.485423
[epoch3, step2845]: loss 0.456672
[epoch3, step2846]: loss 0.541810
[epoch3, step2847]: loss 0.642151
[epoch3, step2848]: loss 0.798660
[epoch3, step2849]: loss 0.472392
[epoch3, step2850]: loss 0.605193
[epoch3, step2851]: loss 0.714919
[epoch3, step2852]: loss 0.611022
[epoch3, step2853]: loss 0.557055
[epoch3, step2854]: loss 0.507271
[epoch3, step2855]: loss 0.562920
[epoch3, step2856]: loss 0.724780
[epoch3, step2857]: loss 0.191447
[epoch3, step2858]: loss 0.563714
[epoch3, step2859]: loss 0.605465
[epoch3, step2860]: loss 0.842067
[epoch3, step2861]: loss 0.681543
[epoch3, step2862]: loss 0.770356
[epoch3, step2863]: loss 0.645577
[epoch3, step2864]: loss 0.503381
[epoch3, step2865]: loss 0.451048
[epoch3, step2866]: loss 0.598672
[epoch3, step2867]: loss 0.726306
[epoch3, step2868]: loss 0.755775
[epoch3, step2869]: loss 0.520252
[epoch3, step2870]: loss 0.640630
[epoch3, step2871]: loss 0.773844
[epoch3, step2872]: loss 0.427704
[epoch3, step2873]: loss 0.441230
[epoch3, step2874]: loss 0.614863
[epoch3, step2875]: loss 0.801417
[epoch3, step2876]: loss 0.459728
[epoch3, step2877]: loss 0.778882
[epoch3, step2878]: loss 0.574670
[epoch3, step2879]: loss 0.499130
[epoch3, step2880]: loss 0.465106
[epoch3, step2881]: loss 0.640004
[epoch3, step2882]: loss 0.668152
[epoch3, step2883]: loss 0.748119
[epoch3, step2884]: loss 0.826199
[epoch3, step2885]: loss 0.551751
[epoch3, step2886]: loss 0.695851
[epoch3, step2887]: loss 0.687095
[epoch3, step2888]: loss 0.712109
[epoch3, step2889]: loss 0.695776
[epoch3, step2890]: loss 0.701104
[epoch3, step2891]: loss 0.748905
[epoch3, step2892]: loss 0.333997
[epoch3, step2893]: loss 0.650883
[epoch3, step2894]: loss 0.689448
[epoch3, step2895]: loss 0.632295
[epoch3, step2896]: loss 0.656431
[epoch3, step2897]: loss 0.792829
[epoch3, step2898]: loss 0.506176
[epoch3, step2899]: loss 0.718226
[epoch3, step2900]: loss 0.785331
[epoch3, step2901]: loss 0.745264
[epoch3, step2902]: loss 0.656170
[epoch3, step2903]: loss 0.620570
[epoch3, step2904]: loss 0.668522
[epoch3, step2905]: loss 0.579046
[epoch3, step2906]: loss 0.684417
[epoch3, step2907]: loss 0.771375
[epoch3, step2908]: loss 0.728883
[epoch3, step2909]: loss 0.604420
[epoch3, step2910]: loss 0.761648
[epoch3, step2911]: loss 0.855474
[epoch3, step2912]: loss 0.522788
[epoch3, step2913]: loss 0.750639
[epoch3, step2914]: loss 0.554305
[epoch3, step2915]: loss 0.603163
[epoch3, step2916]: loss 0.413767
[epoch3, step2917]: loss 0.941659
[epoch3, step2918]: loss 0.612333
[epoch3, step2919]: loss 0.694805
[epoch3, step2920]: loss 0.650695
[epoch3, step2921]: loss 0.579537
[epoch3, step2922]: loss 0.929034
[epoch3, step2923]: loss 0.972859
[epoch3, step2924]: loss 0.807596
[epoch3, step2925]: loss 0.651889
[epoch3, step2926]: loss 0.686484
[epoch3, step2927]: loss 0.568326
[epoch3, step2928]: loss 0.812060
[epoch3, step2929]: loss 0.723635
[epoch3, step2930]: loss 0.926874
[epoch3, step2931]: loss 0.551787
[epoch3, step2932]: loss 0.499583
[epoch3, step2933]: loss 0.390312
[epoch3, step2934]: loss 0.757137
[epoch3, step2935]: loss 1.020871
[epoch3, step2936]: loss 0.627564
[epoch3, step2937]: loss 0.554705
[epoch3, step2938]: loss 0.615121
[epoch3, step2939]: loss 0.551531
[epoch3, step2940]: loss 0.719228
[epoch3, step2941]: loss 0.646738
[epoch3, step2942]: loss 0.444540
[epoch3, step2943]: loss 0.577458
[epoch3, step2944]: loss 0.576322
[epoch3, step2945]: loss 0.832991
[epoch3, step2946]: loss 0.501882
[epoch3, step2947]: loss 0.757354
[epoch3, step2948]: loss 0.892064
[epoch3, step2949]: loss 0.599249
[epoch3, step2950]: loss 0.574404
[epoch3, step2951]: loss 0.647766
[epoch3, step2952]: loss 0.650294
[epoch3, step2953]: loss 0.636096
[epoch3, step2954]: loss 0.511216
[epoch3, step2955]: loss 0.306516
[epoch3, step2956]: loss 0.788635
[epoch3, step2957]: loss 0.789811
[epoch3, step2958]: loss 0.636537
[epoch3, step2959]: loss 0.468831
[epoch3, step2960]: loss 0.649461
[epoch3, step2961]: loss 0.780454
[epoch3, step2962]: loss 0.558695
[epoch3, step2963]: loss 0.809111
[epoch3, step2964]: loss 0.568710
[epoch3, step2965]: loss 0.423087
[epoch3, step2966]: loss 0.419305
[epoch3, step2967]: loss 0.699597
[epoch3, step2968]: loss 0.605420
[epoch3, step2969]: loss 0.799435
[epoch3, step2970]: loss 0.799022
[epoch3, step2971]: loss 0.623565
[epoch3, step2972]: loss 0.457329
[epoch3, step2973]: loss 0.810052
[epoch3, step2974]: loss 0.873521
[epoch3, step2975]: loss 0.771684
[epoch3, step2976]: loss 0.713915
[epoch3, step2977]: loss 0.719909
[epoch3, step2978]: loss 0.772439
[epoch3, step2979]: loss 0.813493
[epoch3, step2980]: loss 0.882091
[epoch3, step2981]: loss 0.482740
[epoch3, step2982]: loss 0.698554
[epoch3, step2983]: loss 0.659768
[epoch3, step2984]: loss 0.501925
[epoch3, step2985]: loss 0.562021
[epoch3, step2986]: loss 0.747522
[epoch3, step2987]: loss 0.651493
[epoch3, step2988]: loss 0.573549
[epoch3, step2989]: loss 0.851085
[epoch3, step2990]: loss 0.573896
[epoch3, step2991]: loss 0.449868
[epoch3, step2992]: loss 0.535366
[epoch3, step2993]: loss 0.702992
[epoch3, step2994]: loss 0.724903
[epoch3, step2995]: loss 0.402471
[epoch3, step2996]: loss 0.704072
[epoch3, step2997]: loss 0.623408
[epoch3, step2998]: loss 0.698953
[epoch3, step2999]: loss 0.699984
[epoch3, step3000]: loss 0.630318
[epoch3, step3001]: loss 0.700225
[epoch3, step3002]: loss 0.520416
[epoch3, step3003]: loss 0.872081
[epoch3, step3004]: loss 0.635539
[epoch3, step3005]: loss 0.644202
[epoch3, step3006]: loss 0.735535
[epoch3, step3007]: loss 0.688605
[epoch3, step3008]: loss 0.656601
[epoch3, step3009]: loss 0.811390
[epoch3, step3010]: loss 0.820264
[epoch3, step3011]: loss 0.809532
[epoch3, step3012]: loss 0.386077
[epoch3, step3013]: loss 0.752233
[epoch3, step3014]: loss 0.537400
[epoch3, step3015]: loss 0.672613
[epoch3, step3016]: loss 0.683529
[epoch3, step3017]: loss 0.559328
[epoch3, step3018]: loss 0.560271
[epoch3, step3019]: loss 0.755435
[epoch3, step3020]: loss 0.395119
[epoch3, step3021]: loss 0.808306
[epoch3, step3022]: loss 0.663726
[epoch3, step3023]: loss 0.343510
[epoch3, step3024]: loss 0.613768
[epoch3, step3025]: loss 0.410487
[epoch3, step3026]: loss 0.587771
[epoch3, step3027]: loss 0.684731
[epoch3, step3028]: loss 0.705886
[epoch3, step3029]: loss 0.670830
[epoch3, step3030]: loss 0.543852
[epoch3, step3031]: loss 0.702705
[epoch3, step3032]: loss 0.764954
[epoch3, step3033]: loss 0.791273
[epoch3, step3034]: loss 0.446434
[epoch3, step3035]: loss 0.545067
[epoch3, step3036]: loss 0.707911
[epoch3, step3037]: loss 0.733773
[epoch3, step3038]: loss 0.261963
[epoch3, step3039]: loss 0.752729
[epoch3, step3040]: loss 0.451391
[epoch3, step3041]: loss 0.879763
[epoch3, step3042]: loss 0.797383
[epoch3, step3043]: loss 0.620321
[epoch3, step3044]: loss 0.729470
[epoch3, step3045]: loss 0.719467
[epoch3, step3046]: loss 0.454737
[epoch3, step3047]: loss 0.478696
[epoch3, step3048]: loss 0.732425
[epoch3, step3049]: loss 0.544242
[epoch3, step3050]: loss 0.808154
[epoch3, step3051]: loss 0.430443
[epoch3, step3052]: loss 0.953523
[epoch3, step3053]: loss 0.957567
[epoch3, step3054]: loss 0.920247
[epoch3, step3055]: loss 0.774598
[epoch3, step3056]: loss 0.686358
[epoch3, step3057]: loss 0.607483
[epoch3, step3058]: loss 0.596756
[epoch3, step3059]: loss 0.610621
[epoch3, step3060]: loss 0.556016
[epoch3, step3061]: loss 0.345171
[epoch3, step3062]: loss 0.875695
[epoch3, step3063]: loss 0.641621
[epoch3, step3064]: loss 0.656357
[epoch3, step3065]: loss 0.453763
[epoch3, step3066]: loss 0.700865
[epoch3, step3067]: loss 0.434874
[epoch3, step3068]: loss 0.542976
[epoch3, step3069]: loss 0.591909
[epoch3, step3070]: loss 0.534632
[epoch3, step3071]: loss 0.586063
[epoch3, step3072]: loss 0.566919
[epoch3, step3073]: loss 0.576887
[epoch3, step3074]: loss 0.777377
[epoch3, step3075]: loss 0.824462
[epoch3, step3076]: loss 0.487455

[epoch3]: avg loss 0.487455

[epoch4, step1]: loss 0.441010
[epoch4, step2]: loss 0.551812
[epoch4, step3]: loss 0.735272
[epoch4, step4]: loss 0.729987
[epoch4, step5]: loss 0.532569
[epoch4, step6]: loss 0.777464
[epoch4, step7]: loss 0.466716
[epoch4, step8]: loss 0.504785
[epoch4, step9]: loss 0.840804
[epoch4, step10]: loss 0.681393
[epoch4, step11]: loss 0.598369
[epoch4, step12]: loss 0.644360
[epoch4, step13]: loss 0.609603
[epoch4, step14]: loss 0.538995
[epoch4, step15]: loss 0.891018
[epoch4, step16]: loss 0.563982
[epoch4, step17]: loss 0.467185
[epoch4, step18]: loss 0.283802
[epoch4, step19]: loss 0.466608
[epoch4, step20]: loss 0.816794
[epoch4, step21]: loss 0.500238
[epoch4, step22]: loss 0.463780
[epoch4, step23]: loss 0.748566
[epoch4, step24]: loss 0.789052
[epoch4, step25]: loss 0.709801
[epoch4, step26]: loss 0.639262
[epoch4, step27]: loss 0.436157
[epoch4, step28]: loss 0.643511
[epoch4, step29]: loss 0.861979
[epoch4, step30]: loss 0.565964
[epoch4, step31]: loss 0.508533
[epoch4, step32]: loss 0.537898
[epoch4, step33]: loss 0.716374
[epoch4, step34]: loss 0.483654
[epoch4, step35]: loss 1.065119
[epoch4, step36]: loss 0.434883
[epoch4, step37]: loss 0.417376
[epoch4, step38]: loss 0.766582
[epoch4, step39]: loss 0.371614
[epoch4, step40]: loss 0.491257
[epoch4, step41]: loss 0.644863
[epoch4, step42]: loss 0.486724
[epoch4, step43]: loss 0.460283
[epoch4, step44]: loss 1.010745
[epoch4, step45]: loss 0.513382
[epoch4, step46]: loss 0.516049
[epoch4, step47]: loss 0.837094
[epoch4, step48]: loss 0.838255
[epoch4, step49]: loss 0.571534
[epoch4, step50]: loss 0.751097
[epoch4, step51]: loss 0.518245
[epoch4, step52]: loss 0.706222
[epoch4, step53]: loss 0.541228
[epoch4, step54]: loss 0.693050
[epoch4, step55]: loss 0.633830
[epoch4, step56]: loss 0.578666
[epoch4, step57]: loss 0.466831
[epoch4, step58]: loss 0.658386
[epoch4, step59]: loss 0.709977
[epoch4, step60]: loss 0.459911
[epoch4, step61]: loss 0.753810
[epoch4, step62]: loss 0.829477
[epoch4, step63]: loss 0.621895
[epoch4, step64]: loss 0.499040
[epoch4, step65]: loss 0.487595
[epoch4, step66]: loss 0.461732
[epoch4, step67]: loss 0.560945
[epoch4, step68]: loss 0.329909
[epoch4, step69]: loss 0.667958
[epoch4, step70]: loss 0.552318
[epoch4, step71]: loss 0.348969
[epoch4, step72]: loss 0.551512
[epoch4, step73]: loss 0.681223
[epoch4, step74]: loss 0.590369
[epoch4, step75]: loss 0.863359
[epoch4, step76]: loss 0.749250
[epoch4, step77]: loss 0.585911
[epoch4, step78]: loss 0.688295
[epoch4, step79]: loss 0.758414
[epoch4, step80]: loss 0.675823
[epoch4, step81]: loss 0.600736
[epoch4, step82]: loss 0.645722
[epoch4, step83]: loss 0.730011
[epoch4, step84]: loss 0.816840
[epoch4, step85]: loss 0.823777
[epoch4, step86]: loss 0.778417
[epoch4, step87]: loss 0.655226
[epoch4, step88]: loss 0.605470
[epoch4, step89]: loss 0.591390
[epoch4, step90]: loss 0.807674
[epoch4, step91]: loss 0.649580
[epoch4, step92]: loss 0.605965
[epoch4, step93]: loss 0.380191
[epoch4, step94]: loss 0.839432
[epoch4, step95]: loss 0.641498
[epoch4, step96]: loss 0.456836
[epoch4, step97]: loss 0.544776
[epoch4, step98]: loss 0.546340
[epoch4, step99]: loss 0.574630
[epoch4, step100]: loss 0.430013
[epoch4, step101]: loss 0.469117
[epoch4, step102]: loss 0.843965
[epoch4, step103]: loss 0.890569
[epoch4, step104]: loss 0.666262
[epoch4, step105]: loss 0.657178
[epoch4, step106]: loss 0.611676
[epoch4, step107]: loss 0.573545
[epoch4, step108]: loss 0.622245
[epoch4, step109]: loss 0.598428
[epoch4, step110]: loss 0.629666
[epoch4, step111]: loss 0.684352
[epoch4, step112]: loss 0.634198
[epoch4, step113]: loss 0.669001
[epoch4, step114]: loss 0.727695
[epoch4, step115]: loss 0.567401
[epoch4, step116]: loss 0.665825
[epoch4, step117]: loss 0.683225
[epoch4, step118]: loss 0.669239
[epoch4, step119]: loss 0.654906
[epoch4, step120]: loss 0.732962
[epoch4, step121]: loss 0.760607
[epoch4, step122]: loss 0.736993
[epoch4, step123]: loss 0.633025
[epoch4, step124]: loss 0.583764
[epoch4, step125]: loss 0.389539
[epoch4, step126]: loss 0.603874
[epoch4, step127]: loss 0.679381
[epoch4, step128]: loss 0.619248
[epoch4, step129]: loss 0.551406
[epoch4, step130]: loss 0.863530
[epoch4, step131]: loss 0.469897
[epoch4, step132]: loss 0.802494
[epoch4, step133]: loss 0.635463
[epoch4, step134]: loss 0.710778
[epoch4, step135]: loss 0.754784
[epoch4, step136]: loss 0.617660
[epoch4, step137]: loss 0.543300
[epoch4, step138]: loss 0.486732
[epoch4, step139]: loss 0.537339
[epoch4, step140]: loss 0.633746
[epoch4, step141]: loss 0.748862
[epoch4, step142]: loss 0.561230
[epoch4, step143]: loss 0.711710
[epoch4, step144]: loss 0.454369
[epoch4, step145]: loss 0.692932
[epoch4, step146]: loss 0.634548
[epoch4, step147]: loss 0.930685
[epoch4, step148]: loss 0.746688
[epoch4, step149]: loss 0.176067
[epoch4, step150]: loss 0.573630
[epoch4, step151]: loss 0.667521
[epoch4, step152]: loss 0.919129
[epoch4, step153]: loss 0.709406
[epoch4, step154]: loss 0.658249
[epoch4, step155]: loss 0.673593
[epoch4, step156]: loss 0.795205
[epoch4, step157]: loss 0.515171
[epoch4, step158]: loss 0.705693
[epoch4, step159]: loss 0.662611
[epoch4, step160]: loss 0.815165
[epoch4, step161]: loss 0.577059
[epoch4, step162]: loss 0.503045
[epoch4, step163]: loss 0.473215
[epoch4, step164]: loss 0.807454
[epoch4, step165]: loss 0.484590
[epoch4, step166]: loss 0.628515
[epoch4, step167]: loss 0.844447
[epoch4, step168]: loss 0.475890
[epoch4, step169]: loss 0.565388
[epoch4, step170]: loss 0.596286
[epoch4, step171]: loss 0.959326
[epoch4, step172]: loss 0.570534
[epoch4, step173]: loss 0.738126
[epoch4, step174]: loss 0.418655
[epoch4, step175]: loss 0.709995
[epoch4, step176]: loss 0.743251
[epoch4, step177]: loss 0.445098
[epoch4, step178]: loss 0.581064
[epoch4, step179]: loss 0.590187
[epoch4, step180]: loss 0.873615
[epoch4, step181]: loss 0.889101
[epoch4, step182]: loss 0.860983
[epoch4, step183]: loss 0.705668
[epoch4, step184]: loss 0.417440
[epoch4, step185]: loss 0.477783
[epoch4, step186]: loss 0.372909
[epoch4, step187]: loss 0.583678
[epoch4, step188]: loss 0.435006
[epoch4, step189]: loss 0.450274
[epoch4, step190]: loss 0.455440
[epoch4, step191]: loss 0.765811
[epoch4, step192]: loss 0.615360
[epoch4, step193]: loss 0.684184
[epoch4, step194]: loss 0.466416
[epoch4, step195]: loss 0.600551
[epoch4, step196]: loss 0.616819
[epoch4, step197]: loss 0.602777
[epoch4, step198]: loss 0.822109
[epoch4, step199]: loss 0.796178
[epoch4, step200]: loss 0.800770
[epoch4, step201]: loss 0.687853
[epoch4, step202]: loss 0.629203
[epoch4, step203]: loss 0.658354
[epoch4, step204]: loss 0.315799
[epoch4, step205]: loss 0.689160
[epoch4, step206]: loss 0.635069
[epoch4, step207]: loss 0.698998
[epoch4, step208]: loss 0.672361
[epoch4, step209]: loss 0.651156
[epoch4, step210]: loss 0.699864
[epoch4, step211]: loss 0.733757
[epoch4, step212]: loss 0.550832
[epoch4, step213]: loss 0.670133
[epoch4, step214]: loss 0.734689
[epoch4, step215]: loss 0.841128
[epoch4, step216]: loss 0.562319
[epoch4, step217]: loss 0.513306
[epoch4, step218]: loss 0.495696
[epoch4, step219]: loss 0.731030
[epoch4, step220]: loss 0.666714
[epoch4, step221]: loss 0.758551
[epoch4, step222]: loss 0.430853
[epoch4, step223]: loss 0.538458
[epoch4, step224]: loss 0.450631
[epoch4, step225]: loss 0.765891
[epoch4, step226]: loss 0.444447
[epoch4, step227]: loss 0.762341
[epoch4, step228]: loss 0.686296
[epoch4, step229]: loss 0.722589
[epoch4, step230]: loss 0.784714
[epoch4, step231]: loss 0.668344
[epoch4, step232]: loss 0.622829
[epoch4, step233]: loss 0.532772
[epoch4, step234]: loss 0.442135
[epoch4, step235]: loss 0.696066
[epoch4, step236]: loss 0.536217
[epoch4, step237]: loss 0.699262
[epoch4, step238]: loss 0.651011
[epoch4, step239]: loss 0.594842
[epoch4, step240]: loss 0.730417
[epoch4, step241]: loss 0.471671
[epoch4, step242]: loss 0.590998
[epoch4, step243]: loss 0.883062
[epoch4, step244]: loss 0.688959
[epoch4, step245]: loss 0.701609
[epoch4, step246]: loss 0.682782
[epoch4, step247]: loss 0.730738
[epoch4, step248]: loss 0.599694
[epoch4, step249]: loss 0.711820
[epoch4, step250]: loss 0.625575
[epoch4, step251]: loss 0.525547
[epoch4, step252]: loss 0.734737
[epoch4, step253]: loss 0.614260
[epoch4, step254]: loss 0.491787
[epoch4, step255]: loss 0.643058
[epoch4, step256]: loss 0.689567
[epoch4, step257]: loss 0.729695
[epoch4, step258]: loss 0.824379
[epoch4, step259]: loss 0.672710
[epoch4, step260]: loss 0.510844
[epoch4, step261]: loss 0.914269
[epoch4, step262]: loss 0.939464
[epoch4, step263]: loss 0.530909
[epoch4, step264]: loss 1.001813
[epoch4, step265]: loss 0.770711
[epoch4, step266]: loss 0.554961
[epoch4, step267]: loss 0.478517
[epoch4, step268]: loss 0.790712
[epoch4, step269]: loss 0.816020
[epoch4, step270]: loss 0.411913
[epoch4, step271]: loss 0.522461
[epoch4, step272]: loss 0.630460
[epoch4, step273]: loss 0.437400
[epoch4, step274]: loss 0.641431
[epoch4, step275]: loss 0.409175
[epoch4, step276]: loss 0.557565
[epoch4, step277]: loss 0.690513
[epoch4, step278]: loss 0.494011
[epoch4, step279]: loss 0.730229
[epoch4, step280]: loss 0.696109
[epoch4, step281]: loss 0.725036
[epoch4, step282]: loss 0.667168
[epoch4, step283]: loss 0.686959
[epoch4, step284]: loss 0.499608
[epoch4, step285]: loss 0.706978
[epoch4, step286]: loss 0.618300
[epoch4, step287]: loss 0.462181
[epoch4, step288]: loss 0.720840
[epoch4, step289]: loss 0.716267
[epoch4, step290]: loss 0.732879
[epoch4, step291]: loss 0.609164
[epoch4, step292]: loss 0.642219
[epoch4, step293]: loss 0.440637
[epoch4, step294]: loss 0.689090
[epoch4, step295]: loss 0.457093
[epoch4, step296]: loss 0.595784
[epoch4, step297]: loss 0.680103
[epoch4, step298]: loss 0.514381
[epoch4, step299]: loss 0.465196
[epoch4, step300]: loss 0.677296
[epoch4, step301]: loss 0.599425
[epoch4, step302]: loss 0.814527
[epoch4, step303]: loss 0.423242
[epoch4, step304]: loss 0.525518
[epoch4, step305]: loss 0.435650
[epoch4, step306]: loss 0.568230
[epoch4, step307]: loss 0.840776
[epoch4, step308]: loss 0.645939
[epoch4, step309]: loss 0.775774
[epoch4, step310]: loss 0.776591
[epoch4, step311]: loss 0.487942
[epoch4, step312]: loss 0.607744
[epoch4, step313]: loss 0.615730
[epoch4, step314]: loss 0.532032
[epoch4, step315]: loss 0.760441
[epoch4, step316]: loss 0.535017
[epoch4, step317]: loss 0.303336
[epoch4, step318]: loss 0.578048
[epoch4, step319]: loss 0.770853
[epoch4, step320]: loss 0.524208
[epoch4, step321]: loss 0.710278
[epoch4, step322]: loss 0.561720
[epoch4, step323]: loss 0.739595
[epoch4, step324]: loss 0.521285
[epoch4, step325]: loss 0.557855
[epoch4, step326]: loss 0.645417
[epoch4, step327]: loss 0.710740
[epoch4, step328]: loss 0.504701
[epoch4, step329]: loss 0.581952
[epoch4, step330]: loss 0.676611
[epoch4, step331]: loss 0.405470
[epoch4, step332]: loss 0.662991
[epoch4, step333]: loss 0.526450
[epoch4, step334]: loss 0.447889
[epoch4, step335]: loss 0.642406
[epoch4, step336]: loss 0.616184
[epoch4, step337]: loss 0.653291
[epoch4, step338]: loss 0.421659
[epoch4, step339]: loss 0.791108
[epoch4, step340]: loss 0.707735
[epoch4, step341]: loss 0.558889
[epoch4, step342]: loss 0.523438
[epoch4, step343]: loss 0.582075
[epoch4, step344]: loss 0.643887
[epoch4, step345]: loss 0.781810
[epoch4, step346]: loss 0.472680
[epoch4, step347]: loss 0.745814
[epoch4, step348]: loss 0.360802
[epoch4, step349]: loss 0.782762
[epoch4, step350]: loss 0.428446
[epoch4, step351]: loss 0.634297
[epoch4, step352]: loss 0.731535
[epoch4, step353]: loss 0.628693
[epoch4, step354]: loss 0.447513
[epoch4, step355]: loss 0.848420
[epoch4, step356]: loss 0.762811
[epoch4, step357]: loss 0.385258
[epoch4, step358]: loss 0.695850
[epoch4, step359]: loss 0.480905
[epoch4, step360]: loss 0.834712
[epoch4, step361]: loss 0.510512
[epoch4, step362]: loss 0.604877
[epoch4, step363]: loss 0.708533
[epoch4, step364]: loss 0.593246
[epoch4, step365]: loss 0.763068
[epoch4, step366]: loss 0.883565
[epoch4, step367]: loss 0.330103
[epoch4, step368]: loss 0.764099
[epoch4, step369]: loss 0.486262
[epoch4, step370]: loss 0.787228
[epoch4, step371]: loss 0.590568
[epoch4, step372]: loss 0.774286
[epoch4, step373]: loss 0.793573
[epoch4, step374]: loss 0.672633
[epoch4, step375]: loss 0.586034
[epoch4, step376]: loss 0.434775
[epoch4, step377]: loss 0.500402
[epoch4, step378]: loss 0.682620
[epoch4, step379]: loss 0.619976
[epoch4, step380]: loss 0.744554
[epoch4, step381]: loss 0.359728
[epoch4, step382]: loss 0.715303
[epoch4, step383]: loss 0.404090
[epoch4, step384]: loss 0.840034
[epoch4, step385]: loss 0.511364
[epoch4, step386]: loss 0.699149
[epoch4, step387]: loss 0.589820
[epoch4, step388]: loss 0.444674
[epoch4, step389]: loss 0.681380
[epoch4, step390]: loss 0.742655
[epoch4, step391]: loss 0.820697
[epoch4, step392]: loss 0.512825
[epoch4, step393]: loss 0.741197
[epoch4, step394]: loss 0.695994
[epoch4, step395]: loss 0.547479
[epoch4, step396]: loss 0.764846
[epoch4, step397]: loss 0.614350
[epoch4, step398]: loss 0.399926
[epoch4, step399]: loss 0.583049
[epoch4, step400]: loss 0.704147
[epoch4, step401]: loss 0.778266
[epoch4, step402]: loss 0.892747
[epoch4, step403]: loss 0.604156
[epoch4, step404]: loss 0.436235
[epoch4, step405]: loss 0.593462
[epoch4, step406]: loss 0.664778
[epoch4, step407]: loss 0.550803
[epoch4, step408]: loss 0.598538
[epoch4, step409]: loss 0.537046
[epoch4, step410]: loss 0.511250
[epoch4, step411]: loss 0.568492
[epoch4, step412]: loss 0.723360
[epoch4, step413]: loss 0.593309
[epoch4, step414]: loss 0.440938
[epoch4, step415]: loss 0.659205
[epoch4, step416]: loss 0.840918
[epoch4, step417]: loss 0.678208
[epoch4, step418]: loss 0.477322
[epoch4, step419]: loss 0.610218
[epoch4, step420]: loss 0.650228
[epoch4, step421]: loss 0.346461
[epoch4, step422]: loss 0.648461
[epoch4, step423]: loss 0.660059
[epoch4, step424]: loss 0.721513
[epoch4, step425]: loss 0.852968
[epoch4, step426]: loss 0.659583
[epoch4, step427]: loss 0.652144
[epoch4, step428]: loss 0.468976
[epoch4, step429]: loss 0.428285
[epoch4, step430]: loss 0.563779
[epoch4, step431]: loss 0.307318
[epoch4, step432]: loss 0.647165
[epoch4, step433]: loss 0.921804
[epoch4, step434]: loss 0.526632
[epoch4, step435]: loss 0.575592
[epoch4, step436]: loss 0.661592
[epoch4, step437]: loss 0.584570
[epoch4, step438]: loss 0.829926
[epoch4, step439]: loss 0.441324
[epoch4, step440]: loss 0.636992
[epoch4, step441]: loss 0.740825
[epoch4, step442]: loss 0.324728
[epoch4, step443]: loss 0.608108
[epoch4, step444]: loss 0.493752
[epoch4, step445]: loss 0.864285
[epoch4, step446]: loss 0.351286
[epoch4, step447]: loss 0.461832
[epoch4, step448]: loss 0.583066
[epoch4, step449]: loss 0.400673
[epoch4, step450]: loss 0.810641
[epoch4, step451]: loss 0.828175
[epoch4, step452]: loss 0.607170
[epoch4, step453]: loss 0.488551
[epoch4, step454]: loss 0.589410
[epoch4, step455]: loss 0.611006
[epoch4, step456]: loss 0.640432
[epoch4, step457]: loss 0.459940
[epoch4, step458]: loss 0.723812
[epoch4, step459]: loss 0.526733
[epoch4, step460]: loss 0.717970
[epoch4, step461]: loss 0.593584
[epoch4, step462]: loss 0.659510
[epoch4, step463]: loss 0.637730
[epoch4, step464]: loss 0.417614
[epoch4, step465]: loss 0.652956
[epoch4, step466]: loss 0.669191
[epoch4, step467]: loss 0.455815
[epoch4, step468]: loss 0.771141
[epoch4, step469]: loss 0.634706
[epoch4, step470]: loss 0.443357
[epoch4, step471]: loss 0.278179
[epoch4, step472]: loss 0.748453
[epoch4, step473]: loss 0.670251
[epoch4, step474]: loss 0.739675
[epoch4, step475]: loss 0.370841
[epoch4, step476]: loss 0.718956
[epoch4, step477]: loss 0.509542
[epoch4, step478]: loss 0.687422
[epoch4, step479]: loss 0.825833
[epoch4, step480]: loss 0.770691
[epoch4, step481]: loss 0.496469
[epoch4, step482]: loss 0.828191
[epoch4, step483]: loss 0.504569
[epoch4, step484]: loss 0.633461
[epoch4, step485]: loss 0.732486
[epoch4, step486]: loss 0.589078
[epoch4, step487]: loss 0.576254
[epoch4, step488]: loss 0.556991
[epoch4, step489]: loss 0.544399
[epoch4, step490]: loss 0.355985
[epoch4, step491]: loss 0.635372
[epoch4, step492]: loss 0.869240
[epoch4, step493]: loss 0.683256
[epoch4, step494]: loss 0.769255
[epoch4, step495]: loss 0.720472
[epoch4, step496]: loss 0.663528
[epoch4, step497]: loss 0.642245
[epoch4, step498]: loss 0.521616
[epoch4, step499]: loss 0.696802
[epoch4, step500]: loss 0.612458
[epoch4, step501]: loss 0.341877
[epoch4, step502]: loss 0.637600
[epoch4, step503]: loss 0.502241
[epoch4, step504]: loss 0.461812
[epoch4, step505]: loss 0.760925
[epoch4, step506]: loss 0.541400
[epoch4, step507]: loss 0.566984
[epoch4, step508]: loss 0.495935
[epoch4, step509]: loss 0.667831
[epoch4, step510]: loss 0.545871
[epoch4, step511]: loss 0.618357
[epoch4, step512]: loss 0.640109
[epoch4, step513]: loss 0.661843
[epoch4, step514]: loss 0.755777
[epoch4, step515]: loss 0.503661
[epoch4, step516]: loss 0.759419
[epoch4, step517]: loss 0.818242
[epoch4, step518]: loss 0.675323
[epoch4, step519]: loss 0.638297
[epoch4, step520]: loss 0.650516
[epoch4, step521]: loss 0.594968
[epoch4, step522]: loss 0.701778
[epoch4, step523]: loss 0.655803
[epoch4, step524]: loss 0.566499
[epoch4, step525]: loss 0.605334
[epoch4, step526]: loss 0.692859
[epoch4, step527]: loss 0.658712
[epoch4, step528]: loss 0.336605
[epoch4, step529]: loss 0.624441
[epoch4, step530]: loss 0.517090
[epoch4, step531]: loss 0.679890
[epoch4, step532]: loss 0.687301
[epoch4, step533]: loss 0.921762
[epoch4, step534]: loss 0.852561
[epoch4, step535]: loss 0.805517
[epoch4, step536]: loss 0.792522
[epoch4, step537]: loss 0.541787
[epoch4, step538]: loss 0.487954
[epoch4, step539]: loss 0.703064
[epoch4, step540]: loss 0.860780
[epoch4, step541]: loss 0.607975
[epoch4, step542]: loss 0.473134
[epoch4, step543]: loss 0.727475
[epoch4, step544]: loss 0.726723
[epoch4, step545]: loss 0.516443
[epoch4, step546]: loss 0.482383
[epoch4, step547]: loss 0.725387
[epoch4, step548]: loss 0.813030
[epoch4, step549]: loss 0.588861
[epoch4, step550]: loss 0.575978
[epoch4, step551]: loss 0.499716
[epoch4, step552]: loss 0.376517
[epoch4, step553]: loss 0.419667
[epoch4, step554]: loss 0.408183
[epoch4, step555]: loss 0.793035
[epoch4, step556]: loss 0.179475
[epoch4, step557]: loss 0.732646
[epoch4, step558]: loss 0.823002
[epoch4, step559]: loss 0.692835
[epoch4, step560]: loss 0.464350
[epoch4, step561]: loss 0.394094
[epoch4, step562]: loss 1.037240
[epoch4, step563]: loss 0.760468
[epoch4, step564]: loss 0.554773
[epoch4, step565]: loss 0.617694
[epoch4, step566]: loss 0.880901
[epoch4, step567]: loss 0.801349
[epoch4, step568]: loss 0.174874
[epoch4, step569]: loss 0.689513
[epoch4, step570]: loss 0.556797
[epoch4, step571]: loss 0.940479
[epoch4, step572]: loss 0.691475
[epoch4, step573]: loss 0.675387
[epoch4, step574]: loss 0.834542
[epoch4, step575]: loss 0.718845
[epoch4, step576]: loss 0.705478
[epoch4, step577]: loss 0.487347
[epoch4, step578]: loss 0.604921
[epoch4, step579]: loss 0.707304
[epoch4, step580]: loss 0.255289
[epoch4, step581]: loss 0.835550
[epoch4, step582]: loss 0.808963
[epoch4, step583]: loss 0.645709
[epoch4, step584]: loss 0.608410
[epoch4, step585]: loss 0.764679
[epoch4, step586]: loss 0.855261
[epoch4, step587]: loss 0.405325
[epoch4, step588]: loss 0.454519
[epoch4, step589]: loss 0.391237
[epoch4, step590]: loss 0.804657
[epoch4, step591]: loss 0.634538
[epoch4, step592]: loss 0.648244
[epoch4, step593]: loss 0.400577
[epoch4, step594]: loss 0.685172
[epoch4, step595]: loss 0.515706
[epoch4, step596]: loss 0.402520
[epoch4, step597]: loss 0.652020
[epoch4, step598]: loss 0.768679
[epoch4, step599]: loss 0.542735
[epoch4, step600]: loss 0.739504
[epoch4, step601]: loss 0.538467
[epoch4, step602]: loss 0.607670
[epoch4, step603]: loss 0.236111
[epoch4, step604]: loss 0.714276
[epoch4, step605]: loss 0.587740
[epoch4, step606]: loss 0.523503
[epoch4, step607]: loss 0.727225
[epoch4, step608]: loss 0.600488
[epoch4, step609]: loss 0.638395
[epoch4, step610]: loss 0.556892
[epoch4, step611]: loss 0.659095
[epoch4, step612]: loss 0.579160
[epoch4, step613]: loss 0.569926
[epoch4, step614]: loss 0.541697
[epoch4, step615]: loss 0.750349
[epoch4, step616]: loss 0.594866
[epoch4, step617]: loss 0.850671
[epoch4, step618]: loss 0.747045
[epoch4, step619]: loss 0.348528
[epoch4, step620]: loss 0.716395
[epoch4, step621]: loss 0.540248
[epoch4, step622]: loss 0.744325
[epoch4, step623]: loss 0.521943
[epoch4, step624]: loss 0.409668
[epoch4, step625]: loss 0.733690
[epoch4, step626]: loss 0.764565
[epoch4, step627]: loss 0.675910
[epoch4, step628]: loss 0.365871
[epoch4, step629]: loss 0.872763
[epoch4, step630]: loss 0.595095
[epoch4, step631]: loss 0.636518
[epoch4, step632]: loss 0.659575
[epoch4, step633]: loss 0.636469
[epoch4, step634]: loss 0.463400
[epoch4, step635]: loss 0.753859
[epoch4, step636]: loss 0.522494
[epoch4, step637]: loss 0.531610
[epoch4, step638]: loss 0.675271
[epoch4, step639]: loss 0.616631
[epoch4, step640]: loss 0.397483
[epoch4, step641]: loss 0.821355
[epoch4, step642]: loss 0.708657
[epoch4, step643]: loss 0.555426
[epoch4, step644]: loss 0.488322
[epoch4, step645]: loss 0.494736
[epoch4, step646]: loss 0.737746
[epoch4, step647]: loss 0.569896
[epoch4, step648]: loss 0.313456
[epoch4, step649]: loss 0.789417
[epoch4, step650]: loss 0.508017
[epoch4, step651]: loss 0.614202
[epoch4, step652]: loss 0.753788
[epoch4, step653]: loss 0.640399
[epoch4, step654]: loss 0.610809
[epoch4, step655]: loss 0.707001
[epoch4, step656]: loss 0.772136
[epoch4, step657]: loss 0.612179
[epoch4, step658]: loss 0.579610
[epoch4, step659]: loss 0.634414
[epoch4, step660]: loss 0.477073
[epoch4, step661]: loss 0.717504
[epoch4, step662]: loss 0.573026
[epoch4, step663]: loss 0.769190
[epoch4, step664]: loss 0.709287
[epoch4, step665]: loss 0.684450
[epoch4, step666]: loss 0.712809
[epoch4, step667]: loss 0.705165
[epoch4, step668]: loss 0.643740
[epoch4, step669]: loss 0.552649
[epoch4, step670]: loss 0.757007
[epoch4, step671]: loss 0.501575
[epoch4, step672]: loss 0.603958
[epoch4, step673]: loss 0.767606
[epoch4, step674]: loss 0.851342
[epoch4, step675]: loss 0.363949
[epoch4, step676]: loss 0.698617
[epoch4, step677]: loss 0.514990
[epoch4, step678]: loss 0.502142
[epoch4, step679]: loss 0.700455
[epoch4, step680]: loss 0.666898
[epoch4, step681]: loss 0.816124
[epoch4, step682]: loss 0.873248
[epoch4, step683]: loss 0.741574
[epoch4, step684]: loss 0.619899
[epoch4, step685]: loss 0.584793
[epoch4, step686]: loss 0.472556
[epoch4, step687]: loss 0.314382
[epoch4, step688]: loss 0.869492
[epoch4, step689]: loss 0.714773
[epoch4, step690]: loss 0.621181
[epoch4, step691]: loss 0.781289
[epoch4, step692]: loss 0.600686
[epoch4, step693]: loss 0.800125
[epoch4, step694]: loss 0.293117
[epoch4, step695]: loss 0.314020
[epoch4, step696]: loss 0.459051
[epoch4, step697]: loss 0.876866
[epoch4, step698]: loss 0.639417
[epoch4, step699]: loss 0.575493
[epoch4, step700]: loss 0.829553
[epoch4, step701]: loss 0.435288
[epoch4, step702]: loss 0.639039
[epoch4, step703]: loss 0.410464
[epoch4, step704]: loss 0.340391
[epoch4, step705]: loss 0.555061
[epoch4, step706]: loss 0.655096
[epoch4, step707]: loss 0.468725
[epoch4, step708]: loss 0.527986
[epoch4, step709]: loss 0.351534
[epoch4, step710]: loss 0.760484
[epoch4, step711]: loss 0.690820
[epoch4, step712]: loss 0.628861
[epoch4, step713]: loss 0.715916
[epoch4, step714]: loss 0.599867
[epoch4, step715]: loss 0.676988
[epoch4, step716]: loss 0.309889
[epoch4, step717]: loss 0.394486
[epoch4, step718]: loss 0.675762
[epoch4, step719]: loss 0.536405
[epoch4, step720]: loss 0.778853
[epoch4, step721]: loss 0.428598
[epoch4, step722]: loss 0.932417
[epoch4, step723]: loss 0.365979
[epoch4, step724]: loss 0.490398
[epoch4, step725]: loss 0.688359
[epoch4, step726]: loss 0.541065
[epoch4, step727]: loss 0.646165
[epoch4, step728]: loss 0.429016
[epoch4, step729]: loss 0.703513
[epoch4, step730]: loss 0.696712
[epoch4, step731]: loss 0.686890
[epoch4, step732]: loss 0.613320
[epoch4, step733]: loss 0.502551
[epoch4, step734]: loss 0.567743
[epoch4, step735]: loss 0.353985
[epoch4, step736]: loss 0.601980
[epoch4, step737]: loss 0.753156
[epoch4, step738]: loss 0.929728
[epoch4, step739]: loss 0.595772
[epoch4, step740]: loss 0.543282
[epoch4, step741]: loss 0.365919
[epoch4, step742]: loss 0.543954
[epoch4, step743]: loss 0.754657
[epoch4, step744]: loss 0.595629
[epoch4, step745]: loss 0.531237
[epoch4, step746]: loss 0.848178
[epoch4, step747]: loss 0.746075
[epoch4, step748]: loss 0.404181
[epoch4, step749]: loss 0.685806
[epoch4, step750]: loss 0.439809
[epoch4, step751]: loss 0.537977
[epoch4, step752]: loss 0.645074
[epoch4, step753]: loss 0.671543
[epoch4, step754]: loss 0.763151
[epoch4, step755]: loss 0.741989
[epoch4, step756]: loss 0.406620
[epoch4, step757]: loss 0.382257
[epoch4, step758]: loss 0.554907
[epoch4, step759]: loss 0.639268
[epoch4, step760]: loss 0.658115
[epoch4, step761]: loss 0.280643
[epoch4, step762]: loss 0.385822
[epoch4, step763]: loss 0.872253
[epoch4, step764]: loss 0.813671
[epoch4, step765]: loss 0.852871
[epoch4, step766]: loss 0.722257
[epoch4, step767]: loss 0.424762
[epoch4, step768]: loss 0.594495
[epoch4, step769]: loss 0.700233
[epoch4, step770]: loss 0.497958
[epoch4, step771]: loss 0.414924
[epoch4, step772]: loss 0.545916
[epoch4, step773]: loss 0.571740
[epoch4, step774]: loss 0.577798
[epoch4, step775]: loss 0.452321
[epoch4, step776]: loss 0.635167
[epoch4, step777]: loss 0.840710
[epoch4, step778]: loss 0.532908
[epoch4, step779]: loss 0.798273
[epoch4, step780]: loss 0.547022
[epoch4, step781]: loss 0.160523
[epoch4, step782]: loss 0.464287
[epoch4, step783]: loss 0.451937
[epoch4, step784]: loss 0.502881
[epoch4, step785]: loss 0.773090
[epoch4, step786]: loss 0.625056
[epoch4, step787]: loss 0.652330
[epoch4, step788]: loss 0.716462
[epoch4, step789]: loss 0.652921
[epoch4, step790]: loss 0.393735
[epoch4, step791]: loss 0.712224
[epoch4, step792]: loss 0.549592
[epoch4, step793]: loss 0.804565
[epoch4, step794]: loss 0.463624
[epoch4, step795]: loss 0.736194
[epoch4, step796]: loss 0.480045
[epoch4, step797]: loss 0.775467
[epoch4, step798]: loss 0.885895
[epoch4, step799]: loss 0.763535
[epoch4, step800]: loss 0.382026
[epoch4, step801]: loss 0.676139
[epoch4, step802]: loss 0.425178
[epoch4, step803]: loss 0.678136
[epoch4, step804]: loss 0.823545
[epoch4, step805]: loss 0.558690
[epoch4, step806]: loss 0.700564
[epoch4, step807]: loss 0.587460
[epoch4, step808]: loss 0.669988
[epoch4, step809]: loss 0.563772
[epoch4, step810]: loss 0.677342
[epoch4, step811]: loss 0.676151
[epoch4, step812]: loss 0.429056
[epoch4, step813]: loss 0.732909
[epoch4, step814]: loss 0.633728
[epoch4, step815]: loss 0.662974
[epoch4, step816]: loss 0.577653
[epoch4, step817]: loss 0.673324
[epoch4, step818]: loss 0.722335
[epoch4, step819]: loss 0.563411
[epoch4, step820]: loss 0.685305
[epoch4, step821]: loss 0.731290
[epoch4, step822]: loss 0.562743
[epoch4, step823]: loss 0.713846
[epoch4, step824]: loss 0.548047
[epoch4, step825]: loss 0.459934
[epoch4, step826]: loss 0.336953
[epoch4, step827]: loss 0.700249
[epoch4, step828]: loss 0.535439
[epoch4, step829]: loss 0.772623
[epoch4, step830]: loss 0.799771
[epoch4, step831]: loss 0.997222
[epoch4, step832]: loss 0.706683
[epoch4, step833]: loss 0.739209
[epoch4, step834]: loss 0.756652
[epoch4, step835]: loss 0.574002
[epoch4, step836]: loss 0.899630
[epoch4, step837]: loss 0.638900
[epoch4, step838]: loss 0.808267
[epoch4, step839]: loss 0.690192
[epoch4, step840]: loss 0.599694
[epoch4, step841]: loss 0.668597
[epoch4, step842]: loss 0.725371
[epoch4, step843]: loss 0.681300
[epoch4, step844]: loss 0.380949
[epoch4, step845]: loss 0.585578
[epoch4, step846]: loss 0.660456
[epoch4, step847]: loss 0.467961
[epoch4, step848]: loss 0.508526
[epoch4, step849]: loss 0.576719
[epoch4, step850]: loss 0.745165
[epoch4, step851]: loss 0.851922
[epoch4, step852]: loss 0.687203
[epoch4, step853]: loss 0.695263
[epoch4, step854]: loss 0.378850
[epoch4, step855]: loss 0.696362
[epoch4, step856]: loss 0.909269
[epoch4, step857]: loss 0.311290
[epoch4, step858]: loss 0.752904
[epoch4, step859]: loss 0.735960
[epoch4, step860]: loss 0.796914
[epoch4, step861]: loss 0.558689
[epoch4, step862]: loss 0.684271
[epoch4, step863]: loss 0.502791
[epoch4, step864]: loss 0.718407
[epoch4, step865]: loss 0.912477
[epoch4, step866]: loss 0.477972
[epoch4, step867]: loss 0.680956
[epoch4, step868]: loss 0.709243
[epoch4, step869]: loss 0.682890
[epoch4, step870]: loss 0.734938
[epoch4, step871]: loss 0.620635
[epoch4, step872]: loss 0.482891
[epoch4, step873]: loss 0.701051
[epoch4, step874]: loss 0.921880
[epoch4, step875]: loss 0.681780
[epoch4, step876]: loss 0.648974
[epoch4, step877]: loss 0.327655
[epoch4, step878]: loss 0.689309
[epoch4, step879]: loss 0.462061
[epoch4, step880]: loss 0.559673
[epoch4, step881]: loss 0.522245
[epoch4, step882]: loss 0.684177
[epoch4, step883]: loss 0.800457
[epoch4, step884]: loss 0.488601
[epoch4, step885]: loss 0.855223
[epoch4, step886]: loss 0.547535
[epoch4, step887]: loss 0.601503
[epoch4, step888]: loss 0.756692
[epoch4, step889]: loss 0.698371
[epoch4, step890]: loss 0.463043
[epoch4, step891]: loss 0.662112
[epoch4, step892]: loss 0.598740
[epoch4, step893]: loss 0.440075
[epoch4, step894]: loss 0.620748
[epoch4, step895]: loss 0.602289
[epoch4, step896]: loss 0.671400
[epoch4, step897]: loss 0.721774
[epoch4, step898]: loss 0.516390
[epoch4, step899]: loss 0.419964
[epoch4, step900]: loss 0.651341
[epoch4, step901]: loss 0.569472
[epoch4, step902]: loss 0.385946
[epoch4, step903]: loss 0.515012
[epoch4, step904]: loss 0.609965
[epoch4, step905]: loss 0.438647
[epoch4, step906]: loss 0.503210
[epoch4, step907]: loss 0.307771
[epoch4, step908]: loss 0.499766
[epoch4, step909]: loss 0.687551
[epoch4, step910]: loss 0.594875
[epoch4, step911]: loss 0.454110
[epoch4, step912]: loss 0.467850
[epoch4, step913]: loss 0.661192
[epoch4, step914]: loss 0.506994
[epoch4, step915]: loss 0.816355
[epoch4, step916]: loss 0.615714
[epoch4, step917]: loss 0.723544
[epoch4, step918]: loss 0.948773
[epoch4, step919]: loss 0.548323
[epoch4, step920]: loss 0.678169
[epoch4, step921]: loss 0.836781
[epoch4, step922]: loss 0.622270
[epoch4, step923]: loss 0.440390
[epoch4, step924]: loss 0.690973
[epoch4, step925]: loss 0.695010
[epoch4, step926]: loss 0.689429
[epoch4, step927]: loss 0.891562
[epoch4, step928]: loss 0.869021
[epoch4, step929]: loss 0.403861
[epoch4, step930]: loss 0.828141
[epoch4, step931]: loss 0.534212
[epoch4, step932]: loss 0.832253
[epoch4, step933]: loss 0.701173
[epoch4, step934]: loss 0.714195
[epoch4, step935]: loss 0.796629
[epoch4, step936]: loss 0.577363
[epoch4, step937]: loss 0.600580
[epoch4, step938]: loss 0.814601
[epoch4, step939]: loss 0.646865
[epoch4, step940]: loss 0.730430
[epoch4, step941]: loss 0.497511
[epoch4, step942]: loss 0.629046
[epoch4, step943]: loss 0.611155
[epoch4, step944]: loss 0.402236
[epoch4, step945]: loss 0.614603
[epoch4, step946]: loss 0.451791
[epoch4, step947]: loss 0.718631
[epoch4, step948]: loss 0.550006
[epoch4, step949]: loss 0.690351
[epoch4, step950]: loss 0.827875
[epoch4, step951]: loss 0.437124
[epoch4, step952]: loss 0.504172
[epoch4, step953]: loss 0.729077
[epoch4, step954]: loss 0.692578
[epoch4, step955]: loss 0.576309
[epoch4, step956]: loss 0.292210
[epoch4, step957]: loss 0.730273
[epoch4, step958]: loss 0.540236
[epoch4, step959]: loss 0.656395
[epoch4, step960]: loss 0.642484
[epoch4, step961]: loss 0.454661
[epoch4, step962]: loss 0.679807
[epoch4, step963]: loss 0.782399
[epoch4, step964]: loss 0.667177
[epoch4, step965]: loss 0.784671
[epoch4, step966]: loss 0.763217
[epoch4, step967]: loss 0.401051
[epoch4, step968]: loss 0.602081
[epoch4, step969]: loss 0.569482
[epoch4, step970]: loss 0.305495
[epoch4, step971]: loss 0.759312
[epoch4, step972]: loss 0.566763
[epoch4, step973]: loss 0.777391
[epoch4, step974]: loss 0.622067
[epoch4, step975]: loss 0.669077
[epoch4, step976]: loss 0.675887
[epoch4, step977]: loss 0.716286
[epoch4, step978]: loss 0.570981
[epoch4, step979]: loss 0.676484
[epoch4, step980]: loss 0.796654
[epoch4, step981]: loss 0.921509
[epoch4, step982]: loss 0.603374
[epoch4, step983]: loss 0.661672
[epoch4, step984]: loss 0.717055
[epoch4, step985]: loss 0.649648
[epoch4, step986]: loss 0.754007
[epoch4, step987]: loss 0.768698
[epoch4, step988]: loss 0.393159
[epoch4, step989]: loss 0.642154
[epoch4, step990]: loss 0.876573
[epoch4, step991]: loss 0.562141
[epoch4, step992]: loss 0.736839
[epoch4, step993]: loss 0.250311
[epoch4, step994]: loss 0.516857
[epoch4, step995]: loss 0.391008
[epoch4, step996]: loss 0.500469
[epoch4, step997]: loss 0.486548
[epoch4, step998]: loss 0.573878
[epoch4, step999]: loss 0.559901
[epoch4, step1000]: loss 0.456477
[epoch4, step1001]: loss 0.511335
[epoch4, step1002]: loss 0.599577
[epoch4, step1003]: loss 0.701335
[epoch4, step1004]: loss 0.676884
[epoch4, step1005]: loss 0.759715
[epoch4, step1006]: loss 0.626106
[epoch4, step1007]: loss 0.679303
[epoch4, step1008]: loss 0.509001
[epoch4, step1009]: loss 0.718934
[epoch4, step1010]: loss 0.826931
[epoch4, step1011]: loss 0.214981
[epoch4, step1012]: loss 0.653866
[epoch4, step1013]: loss 0.261525
[epoch4, step1014]: loss 0.777867
[epoch4, step1015]: loss 0.710203
[epoch4, step1016]: loss 0.598989
[epoch4, step1017]: loss 0.443739
[epoch4, step1018]: loss 0.519175
[epoch4, step1019]: loss 0.517148
[epoch4, step1020]: loss 0.768616
[epoch4, step1021]: loss 0.448512
[epoch4, step1022]: loss 0.584865
[epoch4, step1023]: loss 0.634948
[epoch4, step1024]: loss 0.611620
[epoch4, step1025]: loss 0.557389
[epoch4, step1026]: loss 0.767236
[epoch4, step1027]: loss 0.654563
[epoch4, step1028]: loss 0.634113
[epoch4, step1029]: loss 0.685664
[epoch4, step1030]: loss 0.497414
[epoch4, step1031]: loss 0.625504
[epoch4, step1032]: loss 0.504403
[epoch4, step1033]: loss 0.650175
[epoch4, step1034]: loss 0.807027
[epoch4, step1035]: loss 0.573512
[epoch4, step1036]: loss 0.574738
[epoch4, step1037]: loss 0.483018
[epoch4, step1038]: loss 0.823142
[epoch4, step1039]: loss 0.696534
[epoch4, step1040]: loss 0.783260
[epoch4, step1041]: loss 0.701641
[epoch4, step1042]: loss 0.490650
[epoch4, step1043]: loss 0.407270
[epoch4, step1044]: loss 0.619547
[epoch4, step1045]: loss 0.632630
[epoch4, step1046]: loss 0.765342
[epoch4, step1047]: loss 0.713981
[epoch4, step1048]: loss 0.540648
[epoch4, step1049]: loss 0.773262
[epoch4, step1050]: loss 0.513746
[epoch4, step1051]: loss 0.808176
[epoch4, step1052]: loss 0.637558
[epoch4, step1053]: loss 0.629205
[epoch4, step1054]: loss 0.694489
[epoch4, step1055]: loss 0.613813
[epoch4, step1056]: loss 0.580535
[epoch4, step1057]: loss 0.448743
[epoch4, step1058]: loss 0.685407
[epoch4, step1059]: loss 0.215034
[epoch4, step1060]: loss 0.690433
[epoch4, step1061]: loss 0.552597
[epoch4, step1062]: loss 0.620557
[epoch4, step1063]: loss 0.642528
[epoch4, step1064]: loss 0.713442
[epoch4, step1065]: loss 0.775149
[epoch4, step1066]: loss 0.637543
[epoch4, step1067]: loss 0.803017
[epoch4, step1068]: loss 0.540417
[epoch4, step1069]: loss 0.802583
[epoch4, step1070]: loss 0.756540
[epoch4, step1071]: loss 0.535141
[epoch4, step1072]: loss 0.573927
[epoch4, step1073]: loss 0.456633
[epoch4, step1074]: loss 0.628164
[epoch4, step1075]: loss 0.615214
[epoch4, step1076]: loss 0.301388
[epoch4, step1077]: loss 0.832383
[epoch4, step1078]: loss 0.771102
[epoch4, step1079]: loss 0.416309
[epoch4, step1080]: loss 0.324252
[epoch4, step1081]: loss 0.881688
[epoch4, step1082]: loss 0.581530
[epoch4, step1083]: loss 0.592801
[epoch4, step1084]: loss 0.333413
[epoch4, step1085]: loss 0.585922
[epoch4, step1086]: loss 0.745409
[epoch4, step1087]: loss 0.776419
[epoch4, step1088]: loss 0.312845
[epoch4, step1089]: loss 0.666861
[epoch4, step1090]: loss 0.777788
[epoch4, step1091]: loss 0.436711
[epoch4, step1092]: loss 0.900611
[epoch4, step1093]: loss 0.696373
[epoch4, step1094]: loss 0.623959
[epoch4, step1095]: loss 0.409275
[epoch4, step1096]: loss 0.637287
[epoch4, step1097]: loss 0.728222
[epoch4, step1098]: loss 0.566637
[epoch4, step1099]: loss 0.398252
[epoch4, step1100]: loss 0.866787
[epoch4, step1101]: loss 0.631193
[epoch4, step1102]: loss 0.513008
[epoch4, step1103]: loss 0.280585
[epoch4, step1104]: loss 0.614320
[epoch4, step1105]: loss 0.745602
[epoch4, step1106]: loss 0.519610
[epoch4, step1107]: loss 0.879995
[epoch4, step1108]: loss 0.517217
[epoch4, step1109]: loss 0.489901
[epoch4, step1110]: loss 0.329966
[epoch4, step1111]: loss 0.456858
[epoch4, step1112]: loss 0.770713
[epoch4, step1113]: loss 0.716988
[epoch4, step1114]: loss 0.955593
[epoch4, step1115]: loss 0.331672
[epoch4, step1116]: loss 0.544286
[epoch4, step1117]: loss 0.683606
[epoch4, step1118]: loss 0.572326
[epoch4, step1119]: loss 0.733034
[epoch4, step1120]: loss 0.694340
[epoch4, step1121]: loss 0.761940
[epoch4, step1122]: loss 0.514729
[epoch4, step1123]: loss 0.651257
[epoch4, step1124]: loss 0.753782
[epoch4, step1125]: loss 0.755808
[epoch4, step1126]: loss 0.445750
[epoch4, step1127]: loss 0.764325
[epoch4, step1128]: loss 0.735759
[epoch4, step1129]: loss 0.708668
[epoch4, step1130]: loss 0.743099
[epoch4, step1131]: loss 0.358736
[epoch4, step1132]: loss 0.710249
[epoch4, step1133]: loss 0.718659
[epoch4, step1134]: loss 0.309648
[epoch4, step1135]: loss 0.678994
[epoch4, step1136]: loss 0.862398
[epoch4, step1137]: loss 0.817909
[epoch4, step1138]: loss 0.393954
[epoch4, step1139]: loss 0.509587
[epoch4, step1140]: loss 0.482965
[epoch4, step1141]: loss 0.402114
[epoch4, step1142]: loss 0.694705
[epoch4, step1143]: loss 0.275015
[epoch4, step1144]: loss 0.661085
[epoch4, step1145]: loss 0.836307
[epoch4, step1146]: loss 0.594050
[epoch4, step1147]: loss 0.588363
[epoch4, step1148]: loss 0.734518
[epoch4, step1149]: loss 0.528889
[epoch4, step1150]: loss 0.731432
[epoch4, step1151]: loss 0.835348
[epoch4, step1152]: loss 0.509986
[epoch4, step1153]: loss 0.442003
[epoch4, step1154]: loss 0.726711
[epoch4, step1155]: loss 0.732484
[epoch4, step1156]: loss 0.646901
[epoch4, step1157]: loss 0.520114
[epoch4, step1158]: loss 0.893643
[epoch4, step1159]: loss 0.617923
[epoch4, step1160]: loss 0.826261
[epoch4, step1161]: loss 0.403475
[epoch4, step1162]: loss 0.672778
[epoch4, step1163]: loss 0.367436
[epoch4, step1164]: loss 0.482962
[epoch4, step1165]: loss 0.751292
[epoch4, step1166]: loss 0.569669
[epoch4, step1167]: loss 0.302074
[epoch4, step1168]: loss 0.222206
[epoch4, step1169]: loss 0.709478
[epoch4, step1170]: loss 0.803689
[epoch4, step1171]: loss 0.574763
[epoch4, step1172]: loss 0.375620
[epoch4, step1173]: loss 0.615466
[epoch4, step1174]: loss 0.590309
[epoch4, step1175]: loss 0.550089
[epoch4, step1176]: loss 0.565049
[epoch4, step1177]: loss 0.578772
[epoch4, step1178]: loss 0.512299
[epoch4, step1179]: loss 0.694912
[epoch4, step1180]: loss 0.433951
[epoch4, step1181]: loss 0.529994
[epoch4, step1182]: loss 0.951949
[epoch4, step1183]: loss 0.827898
[epoch4, step1184]: loss 0.568393
[epoch4, step1185]: loss 0.544192
[epoch4, step1186]: loss 0.285466
[epoch4, step1187]: loss 0.746028
[epoch4, step1188]: loss 0.724992
[epoch4, step1189]: loss 0.714171
[epoch4, step1190]: loss 0.357527
[epoch4, step1191]: loss 0.630921
[epoch4, step1192]: loss 0.666542
[epoch4, step1193]: loss 0.316234
[epoch4, step1194]: loss 0.671715
[epoch4, step1195]: loss 0.460567
[epoch4, step1196]: loss 0.657190
[epoch4, step1197]: loss 0.609709
[epoch4, step1198]: loss 0.721059
[epoch4, step1199]: loss 0.724763
[epoch4, step1200]: loss 0.384522
[epoch4, step1201]: loss 0.753254
[epoch4, step1202]: loss 0.500959
[epoch4, step1203]: loss 0.517075
[epoch4, step1204]: loss 0.832698
[epoch4, step1205]: loss 0.715049
[epoch4, step1206]: loss 0.842571
[epoch4, step1207]: loss 0.411930
[epoch4, step1208]: loss 0.547684
[epoch4, step1209]: loss 0.800758
[epoch4, step1210]: loss 0.934238
[epoch4, step1211]: loss 0.867340
[epoch4, step1212]: loss 0.638777
[epoch4, step1213]: loss 0.545941
[epoch4, step1214]: loss 0.617155
[epoch4, step1215]: loss 0.624645
[epoch4, step1216]: loss 0.716510
[epoch4, step1217]: loss 0.587717
[epoch4, step1218]: loss 0.629485
[epoch4, step1219]: loss 0.786368
[epoch4, step1220]: loss 0.547609
[epoch4, step1221]: loss 0.549636
[epoch4, step1222]: loss 0.427834
[epoch4, step1223]: loss 0.418874
[epoch4, step1224]: loss 0.572526
[epoch4, step1225]: loss 0.653306
[epoch4, step1226]: loss 0.525611
[epoch4, step1227]: loss 0.619741
[epoch4, step1228]: loss 0.558749
[epoch4, step1229]: loss 0.492524
[epoch4, step1230]: loss 0.580656
[epoch4, step1231]: loss 0.282261
[epoch4, step1232]: loss 0.659401
[epoch4, step1233]: loss 0.823546
[epoch4, step1234]: loss 0.733325
[epoch4, step1235]: loss 0.777463
[epoch4, step1236]: loss 0.708485
[epoch4, step1237]: loss 0.734984
[epoch4, step1238]: loss 0.667429
[epoch4, step1239]: loss 0.544385
[epoch4, step1240]: loss 0.830343
[epoch4, step1241]: loss 0.510386
[epoch4, step1242]: loss 0.633554
[epoch4, step1243]: loss 0.642974
[epoch4, step1244]: loss 0.797820
[epoch4, step1245]: loss 0.754181
[epoch4, step1246]: loss 0.738953
[epoch4, step1247]: loss 0.472407
[epoch4, step1248]: loss 0.783423
[epoch4, step1249]: loss 0.567198
[epoch4, step1250]: loss 0.496349
[epoch4, step1251]: loss 0.353319
[epoch4, step1252]: loss 0.755386
[epoch4, step1253]: loss 0.431888
[epoch4, step1254]: loss 0.608053
[epoch4, step1255]: loss 0.506184
[epoch4, step1256]: loss 0.527359
[epoch4, step1257]: loss 0.572856
[epoch4, step1258]: loss 0.715558
[epoch4, step1259]: loss 0.834104
[epoch4, step1260]: loss 0.459720
[epoch4, step1261]: loss 0.535389
[epoch4, step1262]: loss 0.589050
[epoch4, step1263]: loss 0.812127
[epoch4, step1264]: loss 0.571145
[epoch4, step1265]: loss 0.551423
[epoch4, step1266]: loss 0.508947
[epoch4, step1267]: loss 0.753598
[epoch4, step1268]: loss 0.565495
[epoch4, step1269]: loss 0.885698
[epoch4, step1270]: loss 0.434126
[epoch4, step1271]: loss 0.629171
[epoch4, step1272]: loss 0.753192
[epoch4, step1273]: loss 0.757448
[epoch4, step1274]: loss 0.428020
[epoch4, step1275]: loss 0.791869
[epoch4, step1276]: loss 0.431002
[epoch4, step1277]: loss 0.392439
[epoch4, step1278]: loss 0.614906
[epoch4, step1279]: loss 0.624424
[epoch4, step1280]: loss 0.508814
[epoch4, step1281]: loss 0.402894
[epoch4, step1282]: loss 0.504684
[epoch4, step1283]: loss 0.466125
[epoch4, step1284]: loss 0.822751
[epoch4, step1285]: loss 0.616970
[epoch4, step1286]: loss 0.599622
[epoch4, step1287]: loss 0.691960
[epoch4, step1288]: loss 0.306636
[epoch4, step1289]: loss 0.440390
[epoch4, step1290]: loss 0.411543
[epoch4, step1291]: loss 0.347293
[epoch4, step1292]: loss 0.234306
[epoch4, step1293]: loss 0.622989
[epoch4, step1294]: loss 0.558453
[epoch4, step1295]: loss 0.859555
[epoch4, step1296]: loss 0.604895
[epoch4, step1297]: loss 0.543649
[epoch4, step1298]: loss 0.538825
[epoch4, step1299]: loss 0.760999
[epoch4, step1300]: loss 0.763496
[epoch4, step1301]: loss 0.467488
[epoch4, step1302]: loss 0.391737
[epoch4, step1303]: loss 0.602980
[epoch4, step1304]: loss 0.595088
[epoch4, step1305]: loss 0.556208
[epoch4, step1306]: loss 0.694622
[epoch4, step1307]: loss 0.824360
[epoch4, step1308]: loss 0.397611
[epoch4, step1309]: loss 0.650558
[epoch4, step1310]: loss 0.441327
[epoch4, step1311]: loss 0.527033
[epoch4, step1312]: loss 0.786505
[epoch4, step1313]: loss 0.488853
[epoch4, step1314]: loss 0.583019
[epoch4, step1315]: loss 0.485721
[epoch4, step1316]: loss 0.450922
[epoch4, step1317]: loss 0.725051
[epoch4, step1318]: loss 0.599383
[epoch4, step1319]: loss 0.742966
[epoch4, step1320]: loss 0.767431
[epoch4, step1321]: loss 0.524618
[epoch4, step1322]: loss 0.700098
[epoch4, step1323]: loss 0.498726
[epoch4, step1324]: loss 0.600949
[epoch4, step1325]: loss 0.680158
[epoch4, step1326]: loss 0.557485
[epoch4, step1327]: loss 0.385057
[epoch4, step1328]: loss 0.473907
[epoch4, step1329]: loss 0.486506
[epoch4, step1330]: loss 0.471743
[epoch4, step1331]: loss 0.519654
[epoch4, step1332]: loss 0.562556
[epoch4, step1333]: loss 0.495880
[epoch4, step1334]: loss 0.792150
[epoch4, step1335]: loss 0.501530
[epoch4, step1336]: loss 0.887917
[epoch4, step1337]: loss 0.309166
[epoch4, step1338]: loss 0.646375
[epoch4, step1339]: loss 0.561777
[epoch4, step1340]: loss 0.326830
[epoch4, step1341]: loss 0.203343
[epoch4, step1342]: loss 0.742016
[epoch4, step1343]: loss 0.584125
[epoch4, step1344]: loss 0.736984
[epoch4, step1345]: loss 0.538271
[epoch4, step1346]: loss 0.573381
[epoch4, step1347]: loss 0.510905
[epoch4, step1348]: loss 0.390452
[epoch4, step1349]: loss 0.779288
[epoch4, step1350]: loss 0.536678
[epoch4, step1351]: loss 0.329269
[epoch4, step1352]: loss 0.324965
[epoch4, step1353]: loss 0.407255
[epoch4, step1354]: loss 0.683559
[epoch4, step1355]: loss 0.533582
[epoch4, step1356]: loss 0.795503
[epoch4, step1357]: loss 0.576569
[epoch4, step1358]: loss 0.349568
[epoch4, step1359]: loss 0.536861
[epoch4, step1360]: loss 0.466756
[epoch4, step1361]: loss 0.643437
[epoch4, step1362]: loss 0.883705
[epoch4, step1363]: loss 0.530502
[epoch4, step1364]: loss 0.732240
[epoch4, step1365]: loss 0.586433
[epoch4, step1366]: loss 0.593918
[epoch4, step1367]: loss 0.614534
[epoch4, step1368]: loss 0.557485
[epoch4, step1369]: loss 0.809684
[epoch4, step1370]: loss 0.633131
[epoch4, step1371]: loss 0.776695
[epoch4, step1372]: loss 0.631962
[epoch4, step1373]: loss 0.439894
[epoch4, step1374]: loss 0.475647
[epoch4, step1375]: loss 0.689302
[epoch4, step1376]: loss 0.764124
[epoch4, step1377]: loss 0.790057
[epoch4, step1378]: loss 0.410436
[epoch4, step1379]: loss 0.796240
[epoch4, step1380]: loss 0.645696
[epoch4, step1381]: loss 0.622581
[epoch4, step1382]: loss 0.658164
[epoch4, step1383]: loss 0.504000
[epoch4, step1384]: loss 0.632075
[epoch4, step1385]: loss 0.461104
[epoch4, step1386]: loss 0.493780
[epoch4, step1387]: loss 0.671675
[epoch4, step1388]: loss 0.378867
[epoch4, step1389]: loss 0.586566
[epoch4, step1390]: loss 0.784331
[epoch4, step1391]: loss 0.441382
[epoch4, step1392]: loss 0.632271
[epoch4, step1393]: loss 0.574139
[epoch4, step1394]: loss 0.598487
[epoch4, step1395]: loss 0.741632
[epoch4, step1396]: loss 0.426970
[epoch4, step1397]: loss 0.474699
[epoch4, step1398]: loss 0.460894
[epoch4, step1399]: loss 0.687692
[epoch4, step1400]: loss 0.616993
[epoch4, step1401]: loss 0.773646
[epoch4, step1402]: loss 0.793707
[epoch4, step1403]: loss 0.276745
[epoch4, step1404]: loss 0.683418
[epoch4, step1405]: loss 0.735684
[epoch4, step1406]: loss 0.582588
[epoch4, step1407]: loss 0.629567
[epoch4, step1408]: loss 0.778121
[epoch4, step1409]: loss 0.323068
[epoch4, step1410]: loss 0.293886
[epoch4, step1411]: loss 0.581462
[epoch4, step1412]: loss 0.445559
[epoch4, step1413]: loss 0.629184
[epoch4, step1414]: loss 0.382717
[epoch4, step1415]: loss 0.669414
[epoch4, step1416]: loss 0.646056
[epoch4, step1417]: loss 0.574144
[epoch4, step1418]: loss 0.672454
[epoch4, step1419]: loss 0.601895
[epoch4, step1420]: loss 0.666327
[epoch4, step1421]: loss 0.491467
[epoch4, step1422]: loss 0.717328
[epoch4, step1423]: loss 0.745146
[epoch4, step1424]: loss 0.392372
[epoch4, step1425]: loss 0.578939
[epoch4, step1426]: loss 0.827948
[epoch4, step1427]: loss 0.632523
[epoch4, step1428]: loss 0.675557
[epoch4, step1429]: loss 0.795215
[epoch4, step1430]: loss 0.789692
[epoch4, step1431]: loss 0.549927
[epoch4, step1432]: loss 0.757328
[epoch4, step1433]: loss 0.618184
[epoch4, step1434]: loss 0.478517
[epoch4, step1435]: loss 0.662712
[epoch4, step1436]: loss 0.587132
[epoch4, step1437]: loss 0.755928
[epoch4, step1438]: loss 0.371801
[epoch4, step1439]: loss 0.754698
[epoch4, step1440]: loss 0.343375
[epoch4, step1441]: loss 0.541676
[epoch4, step1442]: loss 0.797295
[epoch4, step1443]: loss 0.589628
[epoch4, step1444]: loss 0.487764
[epoch4, step1445]: loss 0.629103
[epoch4, step1446]: loss 0.511985
[epoch4, step1447]: loss 0.861451
[epoch4, step1448]: loss 0.628674
[epoch4, step1449]: loss 0.490652
[epoch4, step1450]: loss 0.662395
[epoch4, step1451]: loss 0.434883
[epoch4, step1452]: loss 0.671421
[epoch4, step1453]: loss 0.631410
[epoch4, step1454]: loss 0.406226
[epoch4, step1455]: loss 0.225339
[epoch4, step1456]: loss 0.565181
[epoch4, step1457]: loss 0.579455
[epoch4, step1458]: loss 0.844983
[epoch4, step1459]: loss 0.663171
[epoch4, step1460]: loss 0.461189
[epoch4, step1461]: loss 0.479189
[epoch4, step1462]: loss 0.475612
[epoch4, step1463]: loss 0.576511
[epoch4, step1464]: loss 0.459870
[epoch4, step1465]: loss 0.661181
[epoch4, step1466]: loss 0.769075
[epoch4, step1467]: loss 0.961083
[epoch4, step1468]: loss 0.695556
[epoch4, step1469]: loss 0.597858
[epoch4, step1470]: loss 0.557387
[epoch4, step1471]: loss 0.563516
[epoch4, step1472]: loss 0.570216
[epoch4, step1473]: loss 0.731847
[epoch4, step1474]: loss 0.722185
[epoch4, step1475]: loss 0.644171
[epoch4, step1476]: loss 0.306581
[epoch4, step1477]: loss 0.634898
[epoch4, step1478]: loss 0.684118
[epoch4, step1479]: loss 0.537183
[epoch4, step1480]: loss 0.434174
[epoch4, step1481]: loss 0.899457
[epoch4, step1482]: loss 0.693777
[epoch4, step1483]: loss 0.458487
[epoch4, step1484]: loss 0.491473
[epoch4, step1485]: loss 0.630465
[epoch4, step1486]: loss 0.535988
[epoch4, step1487]: loss 0.709277
[epoch4, step1488]: loss 0.851369
[epoch4, step1489]: loss 0.686032
[epoch4, step1490]: loss 0.866555
[epoch4, step1491]: loss 0.712819
[epoch4, step1492]: loss 0.679683
[epoch4, step1493]: loss 0.687299
[epoch4, step1494]: loss 0.682418
[epoch4, step1495]: loss 0.665050
[epoch4, step1496]: loss 0.497251
[epoch4, step1497]: loss 0.606616
[epoch4, step1498]: loss 0.482597
[epoch4, step1499]: loss 0.535865
[epoch4, step1500]: loss 0.629163
[epoch4, step1501]: loss 0.708586
[epoch4, step1502]: loss 0.703023
[epoch4, step1503]: loss 0.649385
[epoch4, step1504]: loss 0.461669
[epoch4, step1505]: loss 0.528353
[epoch4, step1506]: loss 0.507300
[epoch4, step1507]: loss 0.668311
[epoch4, step1508]: loss 0.614575
[epoch4, step1509]: loss 0.765018
[epoch4, step1510]: loss 0.235723
[epoch4, step1511]: loss 0.661544
[epoch4, step1512]: loss 0.745064
[epoch4, step1513]: loss 0.384714
[epoch4, step1514]: loss 0.611427
[epoch4, step1515]: loss 0.886117
[epoch4, step1516]: loss 0.822592
[epoch4, step1517]: loss 0.500101
[epoch4, step1518]: loss 0.886707
[epoch4, step1519]: loss 0.789300
[epoch4, step1520]: loss 0.658180
[epoch4, step1521]: loss 0.472160
[epoch4, step1522]: loss 0.473752
[epoch4, step1523]: loss 0.633851
[epoch4, step1524]: loss 0.728473
[epoch4, step1525]: loss 0.286176
[epoch4, step1526]: loss 0.422690
[epoch4, step1527]: loss 0.653315
[epoch4, step1528]: loss 0.480872
[epoch4, step1529]: loss 0.589611
[epoch4, step1530]: loss 0.462579
[epoch4, step1531]: loss 0.680989
[epoch4, step1532]: loss 0.553266
[epoch4, step1533]: loss 0.593352
[epoch4, step1534]: loss 0.455543
[epoch4, step1535]: loss 0.746279
[epoch4, step1536]: loss 0.696050
[epoch4, step1537]: loss 0.648694
[epoch4, step1538]: loss 0.650840
[epoch4, step1539]: loss 0.763630
[epoch4, step1540]: loss 0.640159
[epoch4, step1541]: loss 0.628041
[epoch4, step1542]: loss 0.231104
[epoch4, step1543]: loss 0.647593
[epoch4, step1544]: loss 0.782507
[epoch4, step1545]: loss 0.827108
[epoch4, step1546]: loss 0.671579
[epoch4, step1547]: loss 0.583598
[epoch4, step1548]: loss 0.561499
[epoch4, step1549]: loss 0.620352
[epoch4, step1550]: loss 0.505789
[epoch4, step1551]: loss 0.564018
[epoch4, step1552]: loss 0.561814
[epoch4, step1553]: loss 0.439308
[epoch4, step1554]: loss 0.651058
[epoch4, step1555]: loss 0.537713
[epoch4, step1556]: loss 0.312919
[epoch4, step1557]: loss 0.692235
[epoch4, step1558]: loss 0.632208
[epoch4, step1559]: loss 0.428177
[epoch4, step1560]: loss 0.666288
[epoch4, step1561]: loss 0.510208
[epoch4, step1562]: loss 0.687285
[epoch4, step1563]: loss 0.238407
[epoch4, step1564]: loss 0.227867
[epoch4, step1565]: loss 0.218244
[epoch4, step1566]: loss 0.785922
[epoch4, step1567]: loss 0.650071
[epoch4, step1568]: loss 0.407253
[epoch4, step1569]: loss 0.371554
[epoch4, step1570]: loss 0.733229
[epoch4, step1571]: loss 0.593551
[epoch4, step1572]: loss 0.652981
[epoch4, step1573]: loss 0.324005
[epoch4, step1574]: loss 0.541717
[epoch4, step1575]: loss 0.528438
[epoch4, step1576]: loss 0.569315
[epoch4, step1577]: loss 0.763670
[epoch4, step1578]: loss 0.814158
[epoch4, step1579]: loss 0.596569
[epoch4, step1580]: loss 0.567626
[epoch4, step1581]: loss 0.492928
[epoch4, step1582]: loss 0.668523
[epoch4, step1583]: loss 0.457930
[epoch4, step1584]: loss 0.788137
[epoch4, step1585]: loss 0.701657
[epoch4, step1586]: loss 0.402705
[epoch4, step1587]: loss 0.606385
[epoch4, step1588]: loss 0.800955
[epoch4, step1589]: loss 0.503901
[epoch4, step1590]: loss 0.695182
[epoch4, step1591]: loss 0.602953
[epoch4, step1592]: loss 0.788108
[epoch4, step1593]: loss 0.292083
[epoch4, step1594]: loss 0.280273
[epoch4, step1595]: loss 0.615648
[epoch4, step1596]: loss 0.651927
[epoch4, step1597]: loss 0.626733
[epoch4, step1598]: loss 0.647813
[epoch4, step1599]: loss 0.692955
[epoch4, step1600]: loss 0.784790
[epoch4, step1601]: loss 0.379507
[epoch4, step1602]: loss 0.448241
[epoch4, step1603]: loss 0.803459
[epoch4, step1604]: loss 0.605088
[epoch4, step1605]: loss 0.499449
[epoch4, step1606]: loss 0.433562
[epoch4, step1607]: loss 0.758554
[epoch4, step1608]: loss 0.624671
[epoch4, step1609]: loss 0.874608
[epoch4, step1610]: loss 0.818497
[epoch4, step1611]: loss 0.820654
[epoch4, step1612]: loss 0.339398
[epoch4, step1613]: loss 0.833502
[epoch4, step1614]: loss 0.789657
[epoch4, step1615]: loss 0.608528
[epoch4, step1616]: loss 0.674256
[epoch4, step1617]: loss 0.538868
[epoch4, step1618]: loss 0.772695
[epoch4, step1619]: loss 0.696415
[epoch4, step1620]: loss 0.443749
[epoch4, step1621]: loss 0.755356
[epoch4, step1622]: loss 0.416143
[epoch4, step1623]: loss 0.558014
[epoch4, step1624]: loss 0.609632
[epoch4, step1625]: loss 0.526463
[epoch4, step1626]: loss 0.689521
[epoch4, step1627]: loss 0.329093
[epoch4, step1628]: loss 0.629372
[epoch4, step1629]: loss 0.785667
[epoch4, step1630]: loss 0.593469
[epoch4, step1631]: loss 0.540013
[epoch4, step1632]: loss 0.708769
[epoch4, step1633]: loss 0.430996
[epoch4, step1634]: loss 0.867683
[epoch4, step1635]: loss 0.586889
[epoch4, step1636]: loss 0.278530
[epoch4, step1637]: loss 0.662152
[epoch4, step1638]: loss 0.596120
[epoch4, step1639]: loss 0.770397
[epoch4, step1640]: loss 0.423659
[epoch4, step1641]: loss 0.758777
[epoch4, step1642]: loss 0.665842
[epoch4, step1643]: loss 0.669398
[epoch4, step1644]: loss 0.607223
[epoch4, step1645]: loss 0.607521
[epoch4, step1646]: loss 0.746664
[epoch4, step1647]: loss 0.462960
[epoch4, step1648]: loss 0.728407
[epoch4, step1649]: loss 0.673211
[epoch4, step1650]: loss 0.647267
[epoch4, step1651]: loss 0.510554
[epoch4, step1652]: loss 0.468176
[epoch4, step1653]: loss 0.787080
[epoch4, step1654]: loss 0.857487
[epoch4, step1655]: loss 0.550852
[epoch4, step1656]: loss 0.645544
[epoch4, step1657]: loss 0.437501
[epoch4, step1658]: loss 0.630652
[epoch4, step1659]: loss 0.522438
[epoch4, step1660]: loss 0.430207
[epoch4, step1661]: loss 0.642311
[epoch4, step1662]: loss 0.837130
[epoch4, step1663]: loss 0.530890
[epoch4, step1664]: loss 0.390694
[epoch4, step1665]: loss 0.616346
[epoch4, step1666]: loss 0.590666
[epoch4, step1667]: loss 0.714534
[epoch4, step1668]: loss 0.706097
[epoch4, step1669]: loss 0.219816
[epoch4, step1670]: loss 0.580905
[epoch4, step1671]: loss 0.601941
[epoch4, step1672]: loss 0.597644
[epoch4, step1673]: loss 0.718555
[epoch4, step1674]: loss 0.766848
[epoch4, step1675]: loss 0.256507
[epoch4, step1676]: loss 0.622014
[epoch4, step1677]: loss 0.559062
[epoch4, step1678]: loss 0.377264
[epoch4, step1679]: loss 0.827114
[epoch4, step1680]: loss 0.607221
[epoch4, step1681]: loss 0.667440
[epoch4, step1682]: loss 0.640576
[epoch4, step1683]: loss 0.259544
[epoch4, step1684]: loss 0.484204
[epoch4, step1685]: loss 0.261661
[epoch4, step1686]: loss 0.400987
[epoch4, step1687]: loss 0.508446
[epoch4, step1688]: loss 0.578404
[epoch4, step1689]: loss 0.699842
[epoch4, step1690]: loss 0.700435
[epoch4, step1691]: loss 0.614679
[epoch4, step1692]: loss 0.219187
[epoch4, step1693]: loss 0.756624
[epoch4, step1694]: loss 0.697620
[epoch4, step1695]: loss 0.795636
[epoch4, step1696]: loss 0.212051
[epoch4, step1697]: loss 0.843578
[epoch4, step1698]: loss 0.428476
[epoch4, step1699]: loss 0.454692
[epoch4, step1700]: loss 0.701317
[epoch4, step1701]: loss 0.718240
[epoch4, step1702]: loss 0.197305
[epoch4, step1703]: loss 0.682477
[epoch4, step1704]: loss 0.585961
[epoch4, step1705]: loss 0.752767
[epoch4, step1706]: loss 0.778041
[epoch4, step1707]: loss 0.673362
[epoch4, step1708]: loss 0.508454
[epoch4, step1709]: loss 0.443351
[epoch4, step1710]: loss 0.524877
[epoch4, step1711]: loss 0.681158
[epoch4, step1712]: loss 0.798420
[epoch4, step1713]: loss 0.732072
[epoch4, step1714]: loss 0.769675
[epoch4, step1715]: loss 0.506197
[epoch4, step1716]: loss 0.304298
[epoch4, step1717]: loss 0.641197
[epoch4, step1718]: loss 0.473902
[epoch4, step1719]: loss 0.663171
[epoch4, step1720]: loss 0.607979
[epoch4, step1721]: loss 0.489811
[epoch4, step1722]: loss 0.745601
[epoch4, step1723]: loss 0.460738
[epoch4, step1724]: loss 0.561677
[epoch4, step1725]: loss 0.611240
[epoch4, step1726]: loss 0.525713
[epoch4, step1727]: loss 0.499229
[epoch4, step1728]: loss 0.305599
[epoch4, step1729]: loss 0.473924
[epoch4, step1730]: loss 0.835709
[epoch4, step1731]: loss 0.579423
[epoch4, step1732]: loss 0.687313
[epoch4, step1733]: loss 0.384056
[epoch4, step1734]: loss 0.532643
[epoch4, step1735]: loss 0.514141
[epoch4, step1736]: loss 0.568084
[epoch4, step1737]: loss 0.595203
[epoch4, step1738]: loss 0.778657
[epoch4, step1739]: loss 0.610022
[epoch4, step1740]: loss 0.561885
[epoch4, step1741]: loss 0.585837
[epoch4, step1742]: loss 0.313098
[epoch4, step1743]: loss 0.592251
[epoch4, step1744]: loss 0.715231
[epoch4, step1745]: loss 0.795949
[epoch4, step1746]: loss 0.769427
[epoch4, step1747]: loss 0.812666
[epoch4, step1748]: loss 0.697539
[epoch4, step1749]: loss 0.508371
[epoch4, step1750]: loss 0.567434
[epoch4, step1751]: loss 0.660815
[epoch4, step1752]: loss 0.767792
[epoch4, step1753]: loss 0.552337
[epoch4, step1754]: loss 0.750732
[epoch4, step1755]: loss 0.679678
[epoch4, step1756]: loss 0.606734
[epoch4, step1757]: loss 0.547673
[epoch4, step1758]: loss 0.506658
[epoch4, step1759]: loss 0.761191
[epoch4, step1760]: loss 0.263824
[epoch4, step1761]: loss 0.580359
[epoch4, step1762]: loss 0.579593
[epoch4, step1763]: loss 0.632607
[epoch4, step1764]: loss 0.739859
[epoch4, step1765]: loss 0.655345
[epoch4, step1766]: loss 0.647115
[epoch4, step1767]: loss 0.791648
[epoch4, step1768]: loss 0.648107
[epoch4, step1769]: loss 0.613035
[epoch4, step1770]: loss 0.753973
[epoch4, step1771]: loss 0.470022
[epoch4, step1772]: loss 0.748180
[epoch4, step1773]: loss 0.716155
[epoch4, step1774]: loss 0.679830
[epoch4, step1775]: loss 0.339496
[epoch4, step1776]: loss 0.776880
[epoch4, step1777]: loss 0.936566
[epoch4, step1778]: loss 0.393062
[epoch4, step1779]: loss 0.457636
[epoch4, step1780]: loss 0.544553
[epoch4, step1781]: loss 0.627668
[epoch4, step1782]: loss 0.839159
[epoch4, step1783]: loss 0.349123
[epoch4, step1784]: loss 0.570645
[epoch4, step1785]: loss 0.848597
[epoch4, step1786]: loss 0.754896
[epoch4, step1787]: loss 0.742961
[epoch4, step1788]: loss 0.736306
[epoch4, step1789]: loss 0.697716
[epoch4, step1790]: loss 0.738807
[epoch4, step1791]: loss 0.844635
[epoch4, step1792]: loss 0.646152
[epoch4, step1793]: loss 0.305317
[epoch4, step1794]: loss 0.559978
[epoch4, step1795]: loss 0.674521
[epoch4, step1796]: loss 0.490553
[epoch4, step1797]: loss 0.297801
[epoch4, step1798]: loss 0.603321
[epoch4, step1799]: loss 0.321004
[epoch4, step1800]: loss 0.350242
[epoch4, step1801]: loss 0.712119
[epoch4, step1802]: loss 0.266400
[epoch4, step1803]: loss 0.434815
[epoch4, step1804]: loss 0.754065
[epoch4, step1805]: loss 0.575233
[epoch4, step1806]: loss 0.347702
[epoch4, step1807]: loss 0.504473
[epoch4, step1808]: loss 0.552132
[epoch4, step1809]: loss 0.651552
[epoch4, step1810]: loss 0.499876
[epoch4, step1811]: loss 0.659368
[epoch4, step1812]: loss 0.548889
[epoch4, step1813]: loss 0.512058
[epoch4, step1814]: loss 0.699285
[epoch4, step1815]: loss 0.568344
[epoch4, step1816]: loss 0.343862
[epoch4, step1817]: loss 0.633582
[epoch4, step1818]: loss 0.565003
[epoch4, step1819]: loss 0.690910
[epoch4, step1820]: loss 0.525270
[epoch4, step1821]: loss 0.617830
[epoch4, step1822]: loss 0.629543
[epoch4, step1823]: loss 0.601690
[epoch4, step1824]: loss 0.542924
[epoch4, step1825]: loss 0.539268
[epoch4, step1826]: loss 0.648564
[epoch4, step1827]: loss 0.589575
[epoch4, step1828]: loss 0.177474
[epoch4, step1829]: loss 0.605855
[epoch4, step1830]: loss 0.592178
[epoch4, step1831]: loss 0.742693
[epoch4, step1832]: loss 0.467136
[epoch4, step1833]: loss 0.513022
[epoch4, step1834]: loss 0.412601
[epoch4, step1835]: loss 0.652519
[epoch4, step1836]: loss 0.354354
[epoch4, step1837]: loss 0.665725
[epoch4, step1838]: loss 0.204083
[epoch4, step1839]: loss 0.678161
[epoch4, step1840]: loss 0.510312
[epoch4, step1841]: loss 0.592694
[epoch4, step1842]: loss 0.675892
[epoch4, step1843]: loss 0.542916
[epoch4, step1844]: loss 0.377895
[epoch4, step1845]: loss 0.695035
[epoch4, step1846]: loss 0.684223
[epoch4, step1847]: loss 0.591295
[epoch4, step1848]: loss 0.629464
[epoch4, step1849]: loss 0.525655
[epoch4, step1850]: loss 0.718736
[epoch4, step1851]: loss 0.731168
[epoch4, step1852]: loss 0.693852
[epoch4, step1853]: loss 0.644639
[epoch4, step1854]: loss 0.442782
[epoch4, step1855]: loss 0.573697
[epoch4, step1856]: loss 0.427652
[epoch4, step1857]: loss 0.458075
[epoch4, step1858]: loss 0.700875
[epoch4, step1859]: loss 0.619675
[epoch4, step1860]: loss 0.790500
[epoch4, step1861]: loss 0.604192
[epoch4, step1862]: loss 0.618546
[epoch4, step1863]: loss 0.855671
[epoch4, step1864]: loss 0.760588
[epoch4, step1865]: loss 0.829411
[epoch4, step1866]: loss 0.782436
[epoch4, step1867]: loss 0.422396
[epoch4, step1868]: loss 0.361721
[epoch4, step1869]: loss 0.648759
[epoch4, step1870]: loss 0.795104
[epoch4, step1871]: loss 0.533264
[epoch4, step1872]: loss 0.785597
[epoch4, step1873]: loss 0.576055
[epoch4, step1874]: loss 0.762764
[epoch4, step1875]: loss 0.530546
[epoch4, step1876]: loss 0.525093
[epoch4, step1877]: loss 0.752102
[epoch4, step1878]: loss 0.740099
[epoch4, step1879]: loss 0.547017
[epoch4, step1880]: loss 0.587336
[epoch4, step1881]: loss 0.730957
[epoch4, step1882]: loss 0.738225
[epoch4, step1883]: loss 0.518486
[epoch4, step1884]: loss 0.601443
[epoch4, step1885]: loss 0.379966
[epoch4, step1886]: loss 0.612080
[epoch4, step1887]: loss 0.785996
[epoch4, step1888]: loss 0.775471
[epoch4, step1889]: loss 0.576886
[epoch4, step1890]: loss 0.620101
[epoch4, step1891]: loss 0.637124
[epoch4, step1892]: loss 0.331268
[epoch4, step1893]: loss 0.473366
[epoch4, step1894]: loss 0.620658
[epoch4, step1895]: loss 0.443347
[epoch4, step1896]: loss 0.796934
[epoch4, step1897]: loss 0.608420
[epoch4, step1898]: loss 0.600155
[epoch4, step1899]: loss 0.772453
[epoch4, step1900]: loss 0.821515
[epoch4, step1901]: loss 0.659414
[epoch4, step1902]: loss 0.571274
[epoch4, step1903]: loss 0.711599
[epoch4, step1904]: loss 0.610702
[epoch4, step1905]: loss 0.613705
[epoch4, step1906]: loss 0.765104
[epoch4, step1907]: loss 0.698336
[epoch4, step1908]: loss 0.510341
[epoch4, step1909]: loss 0.821472
[epoch4, step1910]: loss 0.345270
[epoch4, step1911]: loss 0.451678
[epoch4, step1912]: loss 0.602201
[epoch4, step1913]: loss 0.559932
[epoch4, step1914]: loss 0.522204
[epoch4, step1915]: loss 0.723275
[epoch4, step1916]: loss 0.697271
[epoch4, step1917]: loss 0.850215
[epoch4, step1918]: loss 0.761636
[epoch4, step1919]: loss 0.636146
[epoch4, step1920]: loss 0.308664
[epoch4, step1921]: loss 0.492598
[epoch4, step1922]: loss 0.197415
[epoch4, step1923]: loss 0.434342
[epoch4, step1924]: loss 0.549451
[epoch4, step1925]: loss 0.666488
[epoch4, step1926]: loss 0.433540
[epoch4, step1927]: loss 0.738056
[epoch4, step1928]: loss 0.396566
[epoch4, step1929]: loss 0.694244
[epoch4, step1930]: loss 0.487019
[epoch4, step1931]: loss 0.598406
[epoch4, step1932]: loss 0.396449
[epoch4, step1933]: loss 0.441454
[epoch4, step1934]: loss 0.450214
[epoch4, step1935]: loss 0.795419
[epoch4, step1936]: loss 0.590937
[epoch4, step1937]: loss 0.497407
[epoch4, step1938]: loss 0.736732
[epoch4, step1939]: loss 0.758991
[epoch4, step1940]: loss 0.559961
[epoch4, step1941]: loss 0.574654
[epoch4, step1942]: loss 0.703672
[epoch4, step1943]: loss 0.705431
[epoch4, step1944]: loss 0.652940
[epoch4, step1945]: loss 0.733834
[epoch4, step1946]: loss 0.723231
[epoch4, step1947]: loss 0.128286
[epoch4, step1948]: loss 0.661723
[epoch4, step1949]: loss 0.639940
[epoch4, step1950]: loss 0.690257
[epoch4, step1951]: loss 0.622807
[epoch4, step1952]: loss 0.744914
[epoch4, step1953]: loss 0.625061
[epoch4, step1954]: loss 0.602601
[epoch4, step1955]: loss 0.649691
[epoch4, step1956]: loss 0.742132
[epoch4, step1957]: loss 0.666722
[epoch4, step1958]: loss 0.277368
[epoch4, step1959]: loss 0.834923
[epoch4, step1960]: loss 0.694819
[epoch4, step1961]: loss 0.775060
[epoch4, step1962]: loss 0.824038
[epoch4, step1963]: loss 0.306982
[epoch4, step1964]: loss 0.756384
[epoch4, step1965]: loss 0.689940
[epoch4, step1966]: loss 0.561393
[epoch4, step1967]: loss 0.849522
[epoch4, step1968]: loss 0.464262
[epoch4, step1969]: loss 0.640212
[epoch4, step1970]: loss 0.492319
[epoch4, step1971]: loss 0.761352
[epoch4, step1972]: loss 0.426436
[epoch4, step1973]: loss 0.511939
[epoch4, step1974]: loss 0.849028
[epoch4, step1975]: loss 0.656750
[epoch4, step1976]: loss 0.376997
[epoch4, step1977]: loss 0.423816
[epoch4, step1978]: loss 0.309634
[epoch4, step1979]: loss 0.763161
[epoch4, step1980]: loss 0.520837
[epoch4, step1981]: loss 0.574960
[epoch4, step1982]: loss 0.514107
[epoch4, step1983]: loss 0.567931
[epoch4, step1984]: loss 0.632858
[epoch4, step1985]: loss 0.806398
[epoch4, step1986]: loss 0.796750
[epoch4, step1987]: loss 0.594900
[epoch4, step1988]: loss 0.779785
[epoch4, step1989]: loss 0.416147
[epoch4, step1990]: loss 0.517364
[epoch4, step1991]: loss 0.406311
[epoch4, step1992]: loss 0.581834
[epoch4, step1993]: loss 0.617812
[epoch4, step1994]: loss 0.694373
[epoch4, step1995]: loss 0.620241
[epoch4, step1996]: loss 0.594517
[epoch4, step1997]: loss 0.472766
[epoch4, step1998]: loss 0.561253
[epoch4, step1999]: loss 0.770115
[epoch4, step2000]: loss 0.885003
[epoch4, step2001]: loss 0.598023
[epoch4, step2002]: loss 0.723233
[epoch4, step2003]: loss 0.519300
[epoch4, step2004]: loss 0.460703
[epoch4, step2005]: loss 0.584270
[epoch4, step2006]: loss 0.740113
[epoch4, step2007]: loss 0.381254
[epoch4, step2008]: loss 0.653418
[epoch4, step2009]: loss 0.925998
[epoch4, step2010]: loss 0.676550
[epoch4, step2011]: loss 0.528741
[epoch4, step2012]: loss 0.596810
[epoch4, step2013]: loss 0.705315
[epoch4, step2014]: loss 0.680740
[epoch4, step2015]: loss 0.894724
[epoch4, step2016]: loss 0.565957
[epoch4, step2017]: loss 0.355118
[epoch4, step2018]: loss 0.624638
[epoch4, step2019]: loss 0.710165
[epoch4, step2020]: loss 0.642654
[epoch4, step2021]: loss 0.503752
[epoch4, step2022]: loss 0.555629
[epoch4, step2023]: loss 0.496346
[epoch4, step2024]: loss 0.641688
[epoch4, step2025]: loss 0.787916
[epoch4, step2026]: loss 0.589817
[epoch4, step2027]: loss 0.470682
[epoch4, step2028]: loss 0.701946
[epoch4, step2029]: loss 0.623071
[epoch4, step2030]: loss 0.346155
[epoch4, step2031]: loss 0.433679
[epoch4, step2032]: loss 0.702331
[epoch4, step2033]: loss 0.710002
[epoch4, step2034]: loss 0.631865
[epoch4, step2035]: loss 0.202057
[epoch4, step2036]: loss 0.612516
[epoch4, step2037]: loss 0.416521
[epoch4, step2038]: loss 0.769621
[epoch4, step2039]: loss 0.951066
[epoch4, step2040]: loss 0.749989
[epoch4, step2041]: loss 0.543534
[epoch4, step2042]: loss 0.697711
[epoch4, step2043]: loss 0.646214
[epoch4, step2044]: loss 0.602128
[epoch4, step2045]: loss 0.432884
[epoch4, step2046]: loss 0.623904
[epoch4, step2047]: loss 0.350129
[epoch4, step2048]: loss 0.605284
[epoch4, step2049]: loss 0.559721
[epoch4, step2050]: loss 0.911771
[epoch4, step2051]: loss 0.603825
[epoch4, step2052]: loss 0.542489
[epoch4, step2053]: loss 0.748989
[epoch4, step2054]: loss 0.839508
[epoch4, step2055]: loss 0.782577
[epoch4, step2056]: loss 0.743227
[epoch4, step2057]: loss 0.738825
[epoch4, step2058]: loss 0.678014
[epoch4, step2059]: loss 0.491229
[epoch4, step2060]: loss 0.376915
[epoch4, step2061]: loss 0.590070
[epoch4, step2062]: loss 0.723073
[epoch4, step2063]: loss 0.764436
[epoch4, step2064]: loss 0.657355
[epoch4, step2065]: loss 0.778173
[epoch4, step2066]: loss 0.594832
[epoch4, step2067]: loss 0.323824
[epoch4, step2068]: loss 0.506510
[epoch4, step2069]: loss 0.561915
[epoch4, step2070]: loss 0.662715
[epoch4, step2071]: loss 0.633973
[epoch4, step2072]: loss 0.696569
[epoch4, step2073]: loss 0.488921
[epoch4, step2074]: loss 0.589135
[epoch4, step2075]: loss 0.640103
[epoch4, step2076]: loss 0.738598
[epoch4, step2077]: loss 0.521117
[epoch4, step2078]: loss 0.713321
[epoch4, step2079]: loss 0.591899
[epoch4, step2080]: loss 0.658922
[epoch4, step2081]: loss 0.853673
[epoch4, step2082]: loss 0.528116
[epoch4, step2083]: loss 0.741772
[epoch4, step2084]: loss 0.535902
[epoch4, step2085]: loss 0.694702
[epoch4, step2086]: loss 0.453167
[epoch4, step2087]: loss 0.321522
[epoch4, step2088]: loss 0.438999
[epoch4, step2089]: loss 0.456459
[epoch4, step2090]: loss 0.623988
[epoch4, step2091]: loss 0.726710
[epoch4, step2092]: loss 0.790780
[epoch4, step2093]: loss 0.735353
[epoch4, step2094]: loss 0.530033
[epoch4, step2095]: loss 0.774328
[epoch4, step2096]: loss 0.764224
[epoch4, step2097]: loss 0.828474
[epoch4, step2098]: loss 0.398405
[epoch4, step2099]: loss 0.789345
[epoch4, step2100]: loss 0.511137
[epoch4, step2101]: loss 0.632880
[epoch4, step2102]: loss 0.643460
[epoch4, step2103]: loss 0.450479
[epoch4, step2104]: loss 0.597742
[epoch4, step2105]: loss 0.473714
[epoch4, step2106]: loss 0.408263
[epoch4, step2107]: loss 0.476098
[epoch4, step2108]: loss 0.640740
[epoch4, step2109]: loss 0.406738
[epoch4, step2110]: loss 0.743329
[epoch4, step2111]: loss 0.563461
[epoch4, step2112]: loss 0.607136
[epoch4, step2113]: loss 0.649914
[epoch4, step2114]: loss 0.559300
[epoch4, step2115]: loss 0.722930
[epoch4, step2116]: loss 0.660445
[epoch4, step2117]: loss 0.707304
[epoch4, step2118]: loss 0.536081
[epoch4, step2119]: loss 0.807704
[epoch4, step2120]: loss 0.649759
[epoch4, step2121]: loss 0.333855
[epoch4, step2122]: loss 0.631055
[epoch4, step2123]: loss 0.520781
[epoch4, step2124]: loss 0.451140
[epoch4, step2125]: loss 0.763861
[epoch4, step2126]: loss 0.599065
[epoch4, step2127]: loss 0.477093
[epoch4, step2128]: loss 0.345793
[epoch4, step2129]: loss 0.490961
[epoch4, step2130]: loss 0.705811
[epoch4, step2131]: loss 0.296324
[epoch4, step2132]: loss 0.641078
[epoch4, step2133]: loss 0.730483
[epoch4, step2134]: loss 0.365165
[epoch4, step2135]: loss 0.620560
[epoch4, step2136]: loss 0.468161
[epoch4, step2137]: loss 0.749475
[epoch4, step2138]: loss 0.796754
[epoch4, step2139]: loss 0.576820
[epoch4, step2140]: loss 0.794701
[epoch4, step2141]: loss 0.735429
[epoch4, step2142]: loss 0.631471
[epoch4, step2143]: loss 0.518647
[epoch4, step2144]: loss 0.571506
[epoch4, step2145]: loss 0.573453
[epoch4, step2146]: loss 0.824376
[epoch4, step2147]: loss 0.532519
[epoch4, step2148]: loss 0.431652
[epoch4, step2149]: loss 0.695808
[epoch4, step2150]: loss 0.307393
[epoch4, step2151]: loss 0.770187
[epoch4, step2152]: loss 0.750083
[epoch4, step2153]: loss 0.580399
[epoch4, step2154]: loss 0.634794
[epoch4, step2155]: loss 0.733539
[epoch4, step2156]: loss 0.572061
[epoch4, step2157]: loss 0.839675
[epoch4, step2158]: loss 0.383614
[epoch4, step2159]: loss 0.608162
[epoch4, step2160]: loss 0.787362
[epoch4, step2161]: loss 0.421934
[epoch4, step2162]: loss 0.514514
[epoch4, step2163]: loss 0.567362
[epoch4, step2164]: loss 0.538663
[epoch4, step2165]: loss 0.691144
[epoch4, step2166]: loss 0.685765
[epoch4, step2167]: loss 0.738117
[epoch4, step2168]: loss 0.696955
[epoch4, step2169]: loss 0.611881
[epoch4, step2170]: loss 0.474248
[epoch4, step2171]: loss 0.550858
[epoch4, step2172]: loss 0.515924
[epoch4, step2173]: loss 0.517495
[epoch4, step2174]: loss 0.665591
[epoch4, step2175]: loss 0.744272
[epoch4, step2176]: loss 0.785510
[epoch4, step2177]: loss 0.601056
[epoch4, step2178]: loss 0.685381
[epoch4, step2179]: loss 0.679039
[epoch4, step2180]: loss 0.721382
[epoch4, step2181]: loss 0.474074
[epoch4, step2182]: loss 0.360740
[epoch4, step2183]: loss 0.603124
[epoch4, step2184]: loss 0.414503
[epoch4, step2185]: loss 0.549251
[epoch4, step2186]: loss 0.359906
[epoch4, step2187]: loss 0.626161
[epoch4, step2188]: loss 0.660121
[epoch4, step2189]: loss 0.417108
[epoch4, step2190]: loss 0.748576
[epoch4, step2191]: loss 0.570286
[epoch4, step2192]: loss 0.748629
[epoch4, step2193]: loss 0.605067
[epoch4, step2194]: loss 0.659758
[epoch4, step2195]: loss 0.669730
[epoch4, step2196]: loss 0.644362
[epoch4, step2197]: loss 0.671503
[epoch4, step2198]: loss 0.506058
[epoch4, step2199]: loss 0.646643
[epoch4, step2200]: loss 0.834791
[epoch4, step2201]: loss 0.344306
[epoch4, step2202]: loss 0.477755
[epoch4, step2203]: loss 0.695945
[epoch4, step2204]: loss 0.693975
[epoch4, step2205]: loss 0.384083
[epoch4, step2206]: loss 0.557094
[epoch4, step2207]: loss 0.812959
[epoch4, step2208]: loss 0.441242
[epoch4, step2209]: loss 0.606638
[epoch4, step2210]: loss 0.792660
[epoch4, step2211]: loss 0.518233
[epoch4, step2212]: loss 0.294291
[epoch4, step2213]: loss 0.474949
[epoch4, step2214]: loss 0.609212
[epoch4, step2215]: loss 0.791620
[epoch4, step2216]: loss 0.669357
[epoch4, step2217]: loss 0.679976
[epoch4, step2218]: loss 0.596211
[epoch4, step2219]: loss 0.512094
[epoch4, step2220]: loss 0.524307
[epoch4, step2221]: loss 0.646775
[epoch4, step2222]: loss 0.582984
[epoch4, step2223]: loss 0.676198
[epoch4, step2224]: loss 0.665826
[epoch4, step2225]: loss 0.689147
[epoch4, step2226]: loss 0.565553
[epoch4, step2227]: loss 0.328560
[epoch4, step2228]: loss 0.714389
[epoch4, step2229]: loss 0.753054
[epoch4, step2230]: loss 0.609375
[epoch4, step2231]: loss 0.601118
[epoch4, step2232]: loss 0.471115
[epoch4, step2233]: loss 0.676307
[epoch4, step2234]: loss 0.820443
[epoch4, step2235]: loss 0.728593
[epoch4, step2236]: loss 0.587967
[epoch4, step2237]: loss 0.579059
[epoch4, step2238]: loss 0.546353
[epoch4, step2239]: loss 0.766435
[epoch4, step2240]: loss 0.343615
[epoch4, step2241]: loss 0.728559
[epoch4, step2242]: loss 0.439091
[epoch4, step2243]: loss 0.271089
[epoch4, step2244]: loss 0.536837
[epoch4, step2245]: loss 0.531797
[epoch4, step2246]: loss 0.637750
[epoch4, step2247]: loss 0.518287
[epoch4, step2248]: loss 0.594636
[epoch4, step2249]: loss 0.561669
[epoch4, step2250]: loss 0.604700
[epoch4, step2251]: loss 0.677069
[epoch4, step2252]: loss 0.571800
[epoch4, step2253]: loss 0.653589
[epoch4, step2254]: loss 0.514339
[epoch4, step2255]: loss 0.804656
[epoch4, step2256]: loss 0.389902
[epoch4, step2257]: loss 0.559413
[epoch4, step2258]: loss 0.613188
[epoch4, step2259]: loss 0.614303
[epoch4, step2260]: loss 0.488529
[epoch4, step2261]: loss 0.677569
[epoch4, step2262]: loss 0.492041
[epoch4, step2263]: loss 0.648333
[epoch4, step2264]: loss 0.825302
[epoch4, step2265]: loss 0.672580
[epoch4, step2266]: loss 0.476936
[epoch4, step2267]: loss 0.438117
[epoch4, step2268]: loss 0.807780
[epoch4, step2269]: loss 0.782867
[epoch4, step2270]: loss 0.760368
[epoch4, step2271]: loss 0.735763
[epoch4, step2272]: loss 0.585834
[epoch4, step2273]: loss 0.576680
[epoch4, step2274]: loss 0.629553
[epoch4, step2275]: loss 0.645671
[epoch4, step2276]: loss 0.468696
[epoch4, step2277]: loss 0.615709
[epoch4, step2278]: loss 0.688097
[epoch4, step2279]: loss 0.609384
[epoch4, step2280]: loss 0.327419
[epoch4, step2281]: loss 0.512661
[epoch4, step2282]: loss 0.584264
[epoch4, step2283]: loss 0.684209
[epoch4, step2284]: loss 0.209084
[epoch4, step2285]: loss 0.772272
[epoch4, step2286]: loss 0.700994
[epoch4, step2287]: loss 0.754949
[epoch4, step2288]: loss 0.671821
[epoch4, step2289]: loss 0.675689
[epoch4, step2290]: loss 0.754017
[epoch4, step2291]: loss 0.457975
[epoch4, step2292]: loss 0.696372
[epoch4, step2293]: loss 0.594057
[epoch4, step2294]: loss 0.625364
[epoch4, step2295]: loss 0.558541
[epoch4, step2296]: loss 0.592009
[epoch4, step2297]: loss 0.583598
[epoch4, step2298]: loss 0.612039
[epoch4, step2299]: loss 0.826701
[epoch4, step2300]: loss 0.684882
[epoch4, step2301]: loss 0.585232
[epoch4, step2302]: loss 0.567729
[epoch4, step2303]: loss 0.592825
[epoch4, step2304]: loss 0.666936
[epoch4, step2305]: loss 0.496277
[epoch4, step2306]: loss 0.769716
[epoch4, step2307]: loss 0.793419
[epoch4, step2308]: loss 0.576785
[epoch4, step2309]: loss 0.553272
[epoch4, step2310]: loss 0.503277
[epoch4, step2311]: loss 0.740395
[epoch4, step2312]: loss 0.664466
[epoch4, step2313]: loss 0.697002
[epoch4, step2314]: loss 0.656645
[epoch4, step2315]: loss 0.612885
[epoch4, step2316]: loss 0.587196
[epoch4, step2317]: loss 0.633863
[epoch4, step2318]: loss 0.635516
[epoch4, step2319]: loss 0.423854
[epoch4, step2320]: loss 0.988509
[epoch4, step2321]: loss 0.727871
[epoch4, step2322]: loss 0.643620
[epoch4, step2323]: loss 0.449556
[epoch4, step2324]: loss 0.723911
[epoch4, step2325]: loss 0.536724
[epoch4, step2326]: loss 0.605759
[epoch4, step2327]: loss 0.759045
[epoch4, step2328]: loss 0.800161
[epoch4, step2329]: loss 0.681905
[epoch4, step2330]: loss 0.639841
[epoch4, step2331]: loss 0.668541
[epoch4, step2332]: loss 0.478345
[epoch4, step2333]: loss 0.481232
[epoch4, step2334]: loss 0.843734
[epoch4, step2335]: loss 0.772812
[epoch4, step2336]: loss 0.743815
[epoch4, step2337]: loss 0.682773
[epoch4, step2338]: loss 0.521619
[epoch4, step2339]: loss 0.508830
[epoch4, step2340]: loss 0.705545
[epoch4, step2341]: loss 0.510870
[epoch4, step2342]: loss 0.542477
[epoch4, step2343]: loss 0.564645
[epoch4, step2344]: loss 0.561259
[epoch4, step2345]: loss 0.567036
[epoch4, step2346]: loss 0.893103
[epoch4, step2347]: loss 0.523486
[epoch4, step2348]: loss 0.838365
[epoch4, step2349]: loss 0.804738
[epoch4, step2350]: loss 0.456789
[epoch4, step2351]: loss 0.540676
[epoch4, step2352]: loss 0.824744
[epoch4, step2353]: loss 0.655639
[epoch4, step2354]: loss 0.479641
[epoch4, step2355]: loss 0.544895
[epoch4, step2356]: loss 0.425445
[epoch4, step2357]: loss 0.560675
[epoch4, step2358]: loss 0.794162
[epoch4, step2359]: loss 0.474093
[epoch4, step2360]: loss 0.424505
[epoch4, step2361]: loss 0.537574
[epoch4, step2362]: loss 0.576349
[epoch4, step2363]: loss 0.674936
[epoch4, step2364]: loss 0.289196
[epoch4, step2365]: loss 0.618084
[epoch4, step2366]: loss 0.506001
[epoch4, step2367]: loss 0.636620
[epoch4, step2368]: loss 0.685353
[epoch4, step2369]: loss 0.793349
[epoch4, step2370]: loss 0.599223
[epoch4, step2371]: loss 0.688332
[epoch4, step2372]: loss 0.558743
[epoch4, step2373]: loss 0.647408
[epoch4, step2374]: loss 0.488481
[epoch4, step2375]: loss 0.764056
[epoch4, step2376]: loss 0.518685
[epoch4, step2377]: loss 0.773279
[epoch4, step2378]: loss 0.732970
[epoch4, step2379]: loss 0.792549
[epoch4, step2380]: loss 0.660122
[epoch4, step2381]: loss 0.597621
[epoch4, step2382]: loss 0.418949
[epoch4, step2383]: loss 0.563863
[epoch4, step2384]: loss 0.777235
[epoch4, step2385]: loss 0.431256
[epoch4, step2386]: loss 0.429461
[epoch4, step2387]: loss 0.695731
[epoch4, step2388]: loss 0.335953
[epoch4, step2389]: loss 0.436031
[epoch4, step2390]: loss 0.331384
[epoch4, step2391]: loss 0.574837
[epoch4, step2392]: loss 0.584351
[epoch4, step2393]: loss 0.633878
[epoch4, step2394]: loss 0.793825
[epoch4, step2395]: loss 0.408857
[epoch4, step2396]: loss 0.800347
[epoch4, step2397]: loss 0.550899
[epoch4, step2398]: loss 0.682499
[epoch4, step2399]: loss 0.697434
[epoch4, step2400]: loss 0.700553
[epoch4, step2401]: loss 0.719247
[epoch4, step2402]: loss 0.211024
[epoch4, step2403]: loss 0.831506
[epoch4, step2404]: loss 0.444334
[epoch4, step2405]: loss 0.733209
[epoch4, step2406]: loss 0.567359
[epoch4, step2407]: loss 0.749129
[epoch4, step2408]: loss 0.655492
[epoch4, step2409]: loss 0.479667
[epoch4, step2410]: loss 1.007697
[epoch4, step2411]: loss 0.601903
[epoch4, step2412]: loss 0.643816
[epoch4, step2413]: loss 0.353187
[epoch4, step2414]: loss 0.500812
[epoch4, step2415]: loss 0.680690
[epoch4, step2416]: loss 0.701766
[epoch4, step2417]: loss 0.647251
[epoch4, step2418]: loss 0.613016
[epoch4, step2419]: loss 0.396767
[epoch4, step2420]: loss 0.489608
[epoch4, step2421]: loss 0.671943
[epoch4, step2422]: loss 0.183767
[epoch4, step2423]: loss 0.703451
[epoch4, step2424]: loss 0.561411
[epoch4, step2425]: loss 0.552340
[epoch4, step2426]: loss 0.773201
[epoch4, step2427]: loss 0.773412
[epoch4, step2428]: loss 0.546511
[epoch4, step2429]: loss 0.595428
[epoch4, step2430]: loss 0.703641
[epoch4, step2431]: loss 0.684210
[epoch4, step2432]: loss 0.769396
[epoch4, step2433]: loss 0.713582
[epoch4, step2434]: loss 0.733049
[epoch4, step2435]: loss 0.377421
[epoch4, step2436]: loss 0.746714
[epoch4, step2437]: loss 0.726683
[epoch4, step2438]: loss 0.422510
[epoch4, step2439]: loss 0.691164
[epoch4, step2440]: loss 0.418056
[epoch4, step2441]: loss 0.528645
[epoch4, step2442]: loss 0.324013
[epoch4, step2443]: loss 0.693447
[epoch4, step2444]: loss 0.536274
[epoch4, step2445]: loss 0.569002
[epoch4, step2446]: loss 0.507524
[epoch4, step2447]: loss 0.742903
[epoch4, step2448]: loss 0.389167
[epoch4, step2449]: loss 0.557113
[epoch4, step2450]: loss 0.839291
[epoch4, step2451]: loss 0.484332
[epoch4, step2452]: loss 0.831083
[epoch4, step2453]: loss 0.704093
[epoch4, step2454]: loss 0.685244
[epoch4, step2455]: loss 0.779269
[epoch4, step2456]: loss 0.757136
[epoch4, step2457]: loss 0.576133
[epoch4, step2458]: loss 0.738246
[epoch4, step2459]: loss 0.557780
[epoch4, step2460]: loss 0.631532
[epoch4, step2461]: loss 0.283123
[epoch4, step2462]: loss 0.634159
[epoch4, step2463]: loss 0.786894
[epoch4, step2464]: loss 0.471788
[epoch4, step2465]: loss 0.674776
[epoch4, step2466]: loss 0.399660
[epoch4, step2467]: loss 0.582092
[epoch4, step2468]: loss 0.719908
[epoch4, step2469]: loss 0.627570
[epoch4, step2470]: loss 0.355929
[epoch4, step2471]: loss 0.422779
[epoch4, step2472]: loss 0.766829
[epoch4, step2473]: loss 0.480018
[epoch4, step2474]: loss 0.598700
[epoch4, step2475]: loss 0.281370
[epoch4, step2476]: loss 0.456203
[epoch4, step2477]: loss 0.597145
[epoch4, step2478]: loss 0.549027
[epoch4, step2479]: loss 0.484891
[epoch4, step2480]: loss 0.488432
[epoch4, step2481]: loss 0.552960
[epoch4, step2482]: loss 0.650790
[epoch4, step2483]: loss 0.534691
[epoch4, step2484]: loss 0.627739
[epoch4, step2485]: loss 0.635397
[epoch4, step2486]: loss 0.567463
[epoch4, step2487]: loss 0.913796
[epoch4, step2488]: loss 0.496966
[epoch4, step2489]: loss 0.633850
[epoch4, step2490]: loss 0.743581
[epoch4, step2491]: loss 0.501702
[epoch4, step2492]: loss 0.757486
[epoch4, step2493]: loss 0.568753
[epoch4, step2494]: loss 0.548083
[epoch4, step2495]: loss 0.466563
[epoch4, step2496]: loss 0.569681
[epoch4, step2497]: loss 0.611310
[epoch4, step2498]: loss 0.693310
[epoch4, step2499]: loss 0.513793
[epoch4, step2500]: loss 0.647270
[epoch4, step2501]: loss 0.732709
[epoch4, step2502]: loss 0.639734
[epoch4, step2503]: loss 0.242119
[epoch4, step2504]: loss 0.546000
[epoch4, step2505]: loss 0.878361
[epoch4, step2506]: loss 0.808035
[epoch4, step2507]: loss 0.365646
[epoch4, step2508]: loss 0.834435
[epoch4, step2509]: loss 0.749563
[epoch4, step2510]: loss 0.608657
[epoch4, step2511]: loss 0.700891
[epoch4, step2512]: loss 0.709102
[epoch4, step2513]: loss 0.639268
[epoch4, step2514]: loss 0.635155
[epoch4, step2515]: loss 0.436566
[epoch4, step2516]: loss 0.661577
[epoch4, step2517]: loss 0.660171
[epoch4, step2518]: loss 0.537186
[epoch4, step2519]: loss 0.732028
[epoch4, step2520]: loss 0.492471
[epoch4, step2521]: loss 0.673240
[epoch4, step2522]: loss 0.627469
[epoch4, step2523]: loss 0.647534
[epoch4, step2524]: loss 0.352093
[epoch4, step2525]: loss 0.732516
[epoch4, step2526]: loss 0.543831
[epoch4, step2527]: loss 0.596069
[epoch4, step2528]: loss 0.607708
[epoch4, step2529]: loss 0.766709
[epoch4, step2530]: loss 0.741222
[epoch4, step2531]: loss 0.559733
[epoch4, step2532]: loss 0.612341
[epoch4, step2533]: loss 0.544305
[epoch4, step2534]: loss 0.855815
[epoch4, step2535]: loss 0.669445
[epoch4, step2536]: loss 0.671679
[epoch4, step2537]: loss 0.725550
[epoch4, step2538]: loss 0.457923
[epoch4, step2539]: loss 0.601336
[epoch4, step2540]: loss 0.568338
[epoch4, step2541]: loss 0.256254
[epoch4, step2542]: loss 0.504761
[epoch4, step2543]: loss 0.620262
[epoch4, step2544]: loss 0.507009
[epoch4, step2545]: loss 0.291068
[epoch4, step2546]: loss 0.762613
[epoch4, step2547]: loss 0.262884
[epoch4, step2548]: loss 0.452897
[epoch4, step2549]: loss 0.440523
[epoch4, step2550]: loss 0.554274
[epoch4, step2551]: loss 0.655984
[epoch4, step2552]: loss 0.467381
[epoch4, step2553]: loss 0.308071
[epoch4, step2554]: loss 0.337174
[epoch4, step2555]: loss 0.510142
[epoch4, step2556]: loss 0.406425
[epoch4, step2557]: loss 0.486265
[epoch4, step2558]: loss 0.660630
[epoch4, step2559]: loss 0.772814
[epoch4, step2560]: loss 0.781488
[epoch4, step2561]: loss 0.779058
[epoch4, step2562]: loss 0.760501
[epoch4, step2563]: loss 0.535143
[epoch4, step2564]: loss 0.430492
[epoch4, step2565]: loss 0.623719
[epoch4, step2566]: loss 0.259899
[epoch4, step2567]: loss 0.570029
[epoch4, step2568]: loss 0.877824
[epoch4, step2569]: loss 0.412655
[epoch4, step2570]: loss 0.529233
[epoch4, step2571]: loss 0.625905
[epoch4, step2572]: loss 0.409559
[epoch4, step2573]: loss 0.181197
[epoch4, step2574]: loss 0.480934
[epoch4, step2575]: loss 0.777422
[epoch4, step2576]: loss 0.571748
[epoch4, step2577]: loss 0.449466
[epoch4, step2578]: loss 0.704869
[epoch4, step2579]: loss 0.643307
[epoch4, step2580]: loss 0.235695
[epoch4, step2581]: loss 0.591001
[epoch4, step2582]: loss 0.600161
[epoch4, step2583]: loss 0.470790
[epoch4, step2584]: loss 0.643437
[epoch4, step2585]: loss 0.783051
[epoch4, step2586]: loss 0.469742
[epoch4, step2587]: loss 0.463795
[epoch4, step2588]: loss 0.723599
[epoch4, step2589]: loss 0.526437
[epoch4, step2590]: loss 0.404748
[epoch4, step2591]: loss 0.722591
[epoch4, step2592]: loss 0.790607
[epoch4, step2593]: loss 0.598451
[epoch4, step2594]: loss 0.593734
[epoch4, step2595]: loss 0.816990
[epoch4, step2596]: loss 0.529467
[epoch4, step2597]: loss 0.412203
[epoch4, step2598]: loss 0.749896
[epoch4, step2599]: loss 0.697021
[epoch4, step2600]: loss 0.371965
[epoch4, step2601]: loss 0.619750
[epoch4, step2602]: loss 0.551807
[epoch4, step2603]: loss 0.606969
[epoch4, step2604]: loss 0.684769
[epoch4, step2605]: loss 0.533414
[epoch4, step2606]: loss 0.584864
[epoch4, step2607]: loss 0.610918
[epoch4, step2608]: loss 0.635190
[epoch4, step2609]: loss 0.482096
[epoch4, step2610]: loss 0.636084
[epoch4, step2611]: loss 0.789278
[epoch4, step2612]: loss 0.368556
[epoch4, step2613]: loss 0.577386
[epoch4, step2614]: loss 0.586350
[epoch4, step2615]: loss 0.531721
[epoch4, step2616]: loss 0.648558
[epoch4, step2617]: loss 0.557073
[epoch4, step2618]: loss 0.531578
[epoch4, step2619]: loss 0.792428
[epoch4, step2620]: loss 0.406166
[epoch4, step2621]: loss 0.539523
[epoch4, step2622]: loss 0.481491
[epoch4, step2623]: loss 0.521246
[epoch4, step2624]: loss 0.480894
[epoch4, step2625]: loss 0.516615
[epoch4, step2626]: loss 0.614493
[epoch4, step2627]: loss 0.342372
[epoch4, step2628]: loss 0.543443
[epoch4, step2629]: loss 0.755479
[epoch4, step2630]: loss 0.749575
[epoch4, step2631]: loss 0.570287
[epoch4, step2632]: loss 0.471554
[epoch4, step2633]: loss 0.791077
[epoch4, step2634]: loss 0.333869
[epoch4, step2635]: loss 0.602834
[epoch4, step2636]: loss 0.766407
[epoch4, step2637]: loss 0.900412
[epoch4, step2638]: loss 0.551940
[epoch4, step2639]: loss 0.614153
[epoch4, step2640]: loss 0.764018
[epoch4, step2641]: loss 0.617877
[epoch4, step2642]: loss 0.586519
[epoch4, step2643]: loss 0.769574
[epoch4, step2644]: loss 0.582960
[epoch4, step2645]: loss 0.483512
[epoch4, step2646]: loss 0.626049
[epoch4, step2647]: loss 0.522945
[epoch4, step2648]: loss 0.381038
[epoch4, step2649]: loss 0.660102
[epoch4, step2650]: loss 0.479852
[epoch4, step2651]: loss 0.532733
[epoch4, step2652]: loss 0.533476
[epoch4, step2653]: loss 0.626451
[epoch4, step2654]: loss 0.351899
[epoch4, step2655]: loss 0.497932
[epoch4, step2656]: loss 0.585260
[epoch4, step2657]: loss 0.638521
[epoch4, step2658]: loss 0.597594
[epoch4, step2659]: loss 0.437072
[epoch4, step2660]: loss 0.592959
[epoch4, step2661]: loss 0.853553
[epoch4, step2662]: loss 0.771393
[epoch4, step2663]: loss 0.638818
[epoch4, step2664]: loss 0.663536
[epoch4, step2665]: loss 0.375880
[epoch4, step2666]: loss 0.291027
[epoch4, step2667]: loss 0.574159
[epoch4, step2668]: loss 0.813399
[epoch4, step2669]: loss 0.298780
[epoch4, step2670]: loss 0.298529
[epoch4, step2671]: loss 0.654823
[epoch4, step2672]: loss 0.709851
[epoch4, step2673]: loss 0.490674
[epoch4, step2674]: loss 0.510517
[epoch4, step2675]: loss 0.825042
[epoch4, step2676]: loss 0.377939
[epoch4, step2677]: loss 0.472898
[epoch4, step2678]: loss 0.248643
[epoch4, step2679]: loss 0.523906
[epoch4, step2680]: loss 0.476424
[epoch4, step2681]: loss 0.915786
[epoch4, step2682]: loss 0.839344
[epoch4, step2683]: loss 0.567218
[epoch4, step2684]: loss 0.445555
[epoch4, step2685]: loss 0.669930
[epoch4, step2686]: loss 0.328732
[epoch4, step2687]: loss 0.317944
[epoch4, step2688]: loss 0.548868
[epoch4, step2689]: loss 0.536253
[epoch4, step2690]: loss 0.298918
[epoch4, step2691]: loss 0.726163
[epoch4, step2692]: loss 0.670688
[epoch4, step2693]: loss 0.579386
[epoch4, step2694]: loss 0.414225
[epoch4, step2695]: loss 0.583210
[epoch4, step2696]: loss 0.569208
[epoch4, step2697]: loss 0.442451
[epoch4, step2698]: loss 0.498689
[epoch4, step2699]: loss 0.664594
[epoch4, step2700]: loss 0.779085
[epoch4, step2701]: loss 0.450877
[epoch4, step2702]: loss 0.628373
[epoch4, step2703]: loss 0.616745
[epoch4, step2704]: loss 0.316386
[epoch4, step2705]: loss 0.897888
[epoch4, step2706]: loss 0.680075
[epoch4, step2707]: loss 0.424723
[epoch4, step2708]: loss 0.602282
[epoch4, step2709]: loss 0.635458
[epoch4, step2710]: loss 0.615287
[epoch4, step2711]: loss 0.555156
[epoch4, step2712]: loss 0.620469
[epoch4, step2713]: loss 0.451999
[epoch4, step2714]: loss 0.772500
[epoch4, step2715]: loss 0.649888
[epoch4, step2716]: loss 0.395961
[epoch4, step2717]: loss 0.597116
[epoch4, step2718]: loss 0.633314
[epoch4, step2719]: loss 0.510247
[epoch4, step2720]: loss 0.478564
[epoch4, step2721]: loss 0.735630
[epoch4, step2722]: loss 0.374942
[epoch4, step2723]: loss 0.413674
[epoch4, step2724]: loss 0.638188
[epoch4, step2725]: loss 0.732324
[epoch4, step2726]: loss 0.710421
[epoch4, step2727]: loss 0.681623
[epoch4, step2728]: loss 0.681472
[epoch4, step2729]: loss 0.773840
[epoch4, step2730]: loss 0.570069
[epoch4, step2731]: loss 0.450323
[epoch4, step2732]: loss 0.439824
[epoch4, step2733]: loss 0.425166
[epoch4, step2734]: loss 0.607749
[epoch4, step2735]: loss 0.537234
[epoch4, step2736]: loss 0.680438
[epoch4, step2737]: loss 0.520355
[epoch4, step2738]: loss 0.654134
[epoch4, step2739]: loss 0.495310
[epoch4, step2740]: loss 0.604967
[epoch4, step2741]: loss 0.690504
[epoch4, step2742]: loss 0.490054
[epoch4, step2743]: loss 0.595093
[epoch4, step2744]: loss 0.577500
[epoch4, step2745]: loss 0.413863
[epoch4, step2746]: loss 0.765908
[epoch4, step2747]: loss 0.675340
[epoch4, step2748]: loss 0.410924
[epoch4, step2749]: loss 0.521799
[epoch4, step2750]: loss 0.618073
[epoch4, step2751]: loss 0.384562
[epoch4, step2752]: loss 0.331737
[epoch4, step2753]: loss 0.454771
[epoch4, step2754]: loss 0.781225
[epoch4, step2755]: loss 0.463117
[epoch4, step2756]: loss 0.629587
[epoch4, step2757]: loss 0.405463
[epoch4, step2758]: loss 0.519676
[epoch4, step2759]: loss 0.876812
[epoch4, step2760]: loss 0.790930
[epoch4, step2761]: loss 0.610763
[epoch4, step2762]: loss 0.745640
[epoch4, step2763]: loss 0.790104
[epoch4, step2764]: loss 0.395566
[epoch4, step2765]: loss 0.418188
[epoch4, step2766]: loss 0.527981
[epoch4, step2767]: loss 0.456258
[epoch4, step2768]: loss 0.566091
[epoch4, step2769]: loss 0.311518
[epoch4, step2770]: loss 0.659881
[epoch4, step2771]: loss 0.657272
[epoch4, step2772]: loss 0.559466
[epoch4, step2773]: loss 0.584555
[epoch4, step2774]: loss 0.877208
[epoch4, step2775]: loss 0.253679
[epoch4, step2776]: loss 0.569091
[epoch4, step2777]: loss 0.704938
[epoch4, step2778]: loss 0.615412
[epoch4, step2779]: loss 0.578113
[epoch4, step2780]: loss 0.596010
[epoch4, step2781]: loss 0.726446
[epoch4, step2782]: loss 0.513735
[epoch4, step2783]: loss 0.625950
[epoch4, step2784]: loss 0.431415
[epoch4, step2785]: loss 0.731307
[epoch4, step2786]: loss 0.750884
[epoch4, step2787]: loss 0.784086
[epoch4, step2788]: loss 0.683270
[epoch4, step2789]: loss 0.372500
[epoch4, step2790]: loss 0.572900
[epoch4, step2791]: loss 0.567603
[epoch4, step2792]: loss 0.536218
[epoch4, step2793]: loss 0.601717
[epoch4, step2794]: loss 0.436017
[epoch4, step2795]: loss 0.748519
[epoch4, step2796]: loss 0.666884
[epoch4, step2797]: loss 0.486888
[epoch4, step2798]: loss 0.575645
[epoch4, step2799]: loss 0.577862
[epoch4, step2800]: loss 0.791451
[epoch4, step2801]: loss 0.742596
[epoch4, step2802]: loss 0.742333
[epoch4, step2803]: loss 0.471533
[epoch4, step2804]: loss 0.741353
[epoch4, step2805]: loss 0.808991
[epoch4, step2806]: loss 0.616333
[epoch4, step2807]: loss 0.608398
[epoch4, step2808]: loss 0.200442
[epoch4, step2809]: loss 0.650843
[epoch4, step2810]: loss 0.307665
[epoch4, step2811]: loss 0.607430
[epoch4, step2812]: loss 0.780268
[epoch4, step2813]: loss 0.566254
[epoch4, step2814]: loss 0.305773
[epoch4, step2815]: loss 0.690676
[epoch4, step2816]: loss 0.519309
[epoch4, step2817]: loss 0.756366
[epoch4, step2818]: loss 0.492209
[epoch4, step2819]: loss 0.765694
[epoch4, step2820]: loss 0.644634
[epoch4, step2821]: loss 0.606694
[epoch4, step2822]: loss 0.652812
[epoch4, step2823]: loss 0.542118
[epoch4, step2824]: loss 0.798942
[epoch4, step2825]: loss 0.436846
[epoch4, step2826]: loss 0.405260
[epoch4, step2827]: loss 0.641517
[epoch4, step2828]: loss 0.634345
[epoch4, step2829]: loss 0.443395
[epoch4, step2830]: loss 0.391043
[epoch4, step2831]: loss 0.524487
[epoch4, step2832]: loss 0.669691
[epoch4, step2833]: loss 0.574898
[epoch4, step2834]: loss 0.544727
[epoch4, step2835]: loss 0.394883
[epoch4, step2836]: loss 0.491850
[epoch4, step2837]: loss 0.520557
[epoch4, step2838]: loss 0.678999
[epoch4, step2839]: loss 0.781162
[epoch4, step2840]: loss 0.340694
[epoch4, step2841]: loss 0.740459
[epoch4, step2842]: loss 0.487324
[epoch4, step2843]: loss 0.540955
[epoch4, step2844]: loss 0.507194
[epoch4, step2845]: loss 0.759400
[epoch4, step2846]: loss 0.454027
[epoch4, step2847]: loss 0.981131
[epoch4, step2848]: loss 0.569351
[epoch4, step2849]: loss 0.754851
[epoch4, step2850]: loss 0.586300
[epoch4, step2851]: loss 0.734205
[epoch4, step2852]: loss 0.541661
[epoch4, step2853]: loss 0.705626
[epoch4, step2854]: loss 0.656866
[epoch4, step2855]: loss 0.635891
[epoch4, step2856]: loss 0.630555
[epoch4, step2857]: loss 0.445449
[epoch4, step2858]: loss 0.858281
[epoch4, step2859]: loss 0.667383
[epoch4, step2860]: loss 0.459640
[epoch4, step2861]: loss 0.322549
[epoch4, step2862]: loss 0.768173
[epoch4, step2863]: loss 0.484447
[epoch4, step2864]: loss 0.354553
[epoch4, step2865]: loss 0.760003
[epoch4, step2866]: loss 0.517196
[epoch4, step2867]: loss 0.258875
[epoch4, step2868]: loss 0.613069
[epoch4, step2869]: loss 0.853198
[epoch4, step2870]: loss 0.793918
[epoch4, step2871]: loss 0.550713
[epoch4, step2872]: loss 0.732564
[epoch4, step2873]: loss 0.573305
[epoch4, step2874]: loss 0.598416
[epoch4, step2875]: loss 0.638454
[epoch4, step2876]: loss 0.700043
[epoch4, step2877]: loss 0.530615
[epoch4, step2878]: loss 0.775922
[epoch4, step2879]: loss 0.658091
[epoch4, step2880]: loss 0.626417
[epoch4, step2881]: loss 0.801937
[epoch4, step2882]: loss 0.626375
[epoch4, step2883]: loss 0.725953
[epoch4, step2884]: loss 0.559503
[epoch4, step2885]: loss 0.663553
[epoch4, step2886]: loss 0.393015
[epoch4, step2887]: loss 0.715878
[epoch4, step2888]: loss 0.561325
[epoch4, step2889]: loss 0.595359
[epoch4, step2890]: loss 0.414915
[epoch4, step2891]: loss 0.769725
[epoch4, step2892]: loss 0.306279
[epoch4, step2893]: loss 0.433063
[epoch4, step2894]: loss 0.339287
[epoch4, step2895]: loss 0.391765
[epoch4, step2896]: loss 0.572654
[epoch4, step2897]: loss 0.699671
[epoch4, step2898]: loss 0.368832
[epoch4, step2899]: loss 0.712085
[epoch4, step2900]: loss 0.564931
[epoch4, step2901]: loss 0.689926
[epoch4, step2902]: loss 0.750505
[epoch4, step2903]: loss 0.343044
[epoch4, step2904]: loss 0.454913
[epoch4, step2905]: loss 0.400085
[epoch4, step2906]: loss 0.607050
[epoch4, step2907]: loss 0.748959
[epoch4, step2908]: loss 0.762547
[epoch4, step2909]: loss 0.230709
[epoch4, step2910]: loss 0.402522
[epoch4, step2911]: loss 0.555616
[epoch4, step2912]: loss 0.517212
[epoch4, step2913]: loss 0.625348
[epoch4, step2914]: loss 0.800031
[epoch4, step2915]: loss 0.641905
[epoch4, step2916]: loss 0.421631
[epoch4, step2917]: loss 0.552914
[epoch4, step2918]: loss 0.557194
[epoch4, step2919]: loss 0.506281
[epoch4, step2920]: loss 0.425256
[epoch4, step2921]: loss 0.526805
[epoch4, step2922]: loss 0.540150
[epoch4, step2923]: loss 0.278349
[epoch4, step2924]: loss 0.604196
[epoch4, step2925]: loss 0.604536
[epoch4, step2926]: loss 0.319547
[epoch4, step2927]: loss 0.700894
[epoch4, step2928]: loss 0.768014
[epoch4, step2929]: loss 0.522134
[epoch4, step2930]: loss 0.230383
[epoch4, step2931]: loss 0.309200
[epoch4, step2932]: loss 0.309443
[epoch4, step2933]: loss 0.302112
[epoch4, step2934]: loss 0.624342
[epoch4, step2935]: loss 0.462061
[epoch4, step2936]: loss 0.332410
[epoch4, step2937]: loss 0.434194
[epoch4, step2938]: loss 0.740349
[epoch4, step2939]: loss 0.579324
[epoch4, step2940]: loss 0.693703
[epoch4, step2941]: loss 0.710589
[epoch4, step2942]: loss 0.648173
[epoch4, step2943]: loss 0.554973
[epoch4, step2944]: loss 0.663011
[epoch4, step2945]: loss 0.534797
[epoch4, step2946]: loss 0.813621
[epoch4, step2947]: loss 0.577040
[epoch4, step2948]: loss 0.622035
[epoch4, step2949]: loss 0.724609
[epoch4, step2950]: loss 0.507156
[epoch4, step2951]: loss 0.545981
[epoch4, step2952]: loss 0.547270
[epoch4, step2953]: loss 0.513451
[epoch4, step2954]: loss 0.304661
[epoch4, step2955]: loss 0.520703
[epoch4, step2956]: loss 0.578148
[epoch4, step2957]: loss 0.476044
[epoch4, step2958]: loss 0.684314
[epoch4, step2959]: loss 0.872040
[epoch4, step2960]: loss 0.741484
[epoch4, step2961]: loss 0.455049
[epoch4, step2962]: loss 0.878355
[epoch4, step2963]: loss 0.279374
[epoch4, step2964]: loss 0.551852
[epoch4, step2965]: loss 0.774648
[epoch4, step2966]: loss 0.606792
[epoch4, step2967]: loss 0.709623
[epoch4, step2968]: loss 0.406978
[epoch4, step2969]: loss 0.720613
[epoch4, step2970]: loss 0.646390
[epoch4, step2971]: loss 0.741540
[epoch4, step2972]: loss 0.346970
[epoch4, step2973]: loss 0.783711
[epoch4, step2974]: loss 0.532486
[epoch4, step2975]: loss 0.613111
[epoch4, step2976]: loss 0.532146
[epoch4, step2977]: loss 0.823000
[epoch4, step2978]: loss 0.612231
[epoch4, step2979]: loss 0.308964
[epoch4, step2980]: loss 0.716676
[epoch4, step2981]: loss 0.502475
[epoch4, step2982]: loss 0.519750
[epoch4, step2983]: loss 0.477522
[epoch4, step2984]: loss 0.703855
[epoch4, step2985]: loss 0.549027
[epoch4, step2986]: loss 0.509842
[epoch4, step2987]: loss 0.466487
[epoch4, step2988]: loss 0.549625
[epoch4, step2989]: loss 0.710999
[epoch4, step2990]: loss 0.672723
[epoch4, step2991]: loss 0.435224
[epoch4, step2992]: loss 0.532495
[epoch4, step2993]: loss 0.680806
[epoch4, step2994]: loss 0.520458
[epoch4, step2995]: loss 0.588692
[epoch4, step2996]: loss 0.754907
[epoch4, step2997]: loss 0.559930
[epoch4, step2998]: loss 0.442336
[epoch4, step2999]: loss 0.799405
[epoch4, step3000]: loss 0.554732
[epoch4, step3001]: loss 0.573609
[epoch4, step3002]: loss 0.891984
[epoch4, step3003]: loss 0.473871
[epoch4, step3004]: loss 0.491534
[epoch4, step3005]: loss 0.686953
[epoch4, step3006]: loss 0.535391
[epoch4, step3007]: loss 0.536817
[epoch4, step3008]: loss 0.460455
[epoch4, step3009]: loss 0.855625
[epoch4, step3010]: loss 0.552916
[epoch4, step3011]: loss 0.706514
[epoch4, step3012]: loss 0.364554
[epoch4, step3013]: loss 0.785403
[epoch4, step3014]: loss 0.515317
[epoch4, step3015]: loss 0.709711
[epoch4, step3016]: loss 0.629944
[epoch4, step3017]: loss 0.681978
[epoch4, step3018]: loss 0.427382
[epoch4, step3019]: loss 0.493412
[epoch4, step3020]: loss 0.591276
[epoch4, step3021]: loss 0.552853
[epoch4, step3022]: loss 0.636495
[epoch4, step3023]: loss 0.452415
[epoch4, step3024]: loss 0.567701
[epoch4, step3025]: loss 0.661296
[epoch4, step3026]: loss 0.721130
[epoch4, step3027]: loss 0.210921
[epoch4, step3028]: loss 0.593312
[epoch4, step3029]: loss 0.449909
[epoch4, step3030]: loss 0.775728
[epoch4, step3031]: loss 0.604059
[epoch4, step3032]: loss 0.610120
[epoch4, step3033]: loss 0.573924
[epoch4, step3034]: loss 0.789798
[epoch4, step3035]: loss 0.725172
[epoch4, step3036]: loss 0.579785
[epoch4, step3037]: loss 0.813995
[epoch4, step3038]: loss 0.722320
[epoch4, step3039]: loss 0.473362
[epoch4, step3040]: loss 0.689630
[epoch4, step3041]: loss 0.650765
[epoch4, step3042]: loss 0.647481
[epoch4, step3043]: loss 0.486836
[epoch4, step3044]: loss 0.499205
[epoch4, step3045]: loss 0.877543
[epoch4, step3046]: loss 0.517701
[epoch4, step3047]: loss 0.615932
[epoch4, step3048]: loss 0.461430
[epoch4, step3049]: loss 0.850368
[epoch4, step3050]: loss 0.679780
[epoch4, step3051]: loss 0.360380
[epoch4, step3052]: loss 0.833684
[epoch4, step3053]: loss 0.715380
[epoch4, step3054]: loss 0.344381
[epoch4, step3055]: loss 0.303345
[epoch4, step3056]: loss 0.565633
[epoch4, step3057]: loss 0.411057
[epoch4, step3058]: loss 0.587749
[epoch4, step3059]: loss 0.709240
[epoch4, step3060]: loss 0.465950
[epoch4, step3061]: loss 0.497071
[epoch4, step3062]: loss 0.367109
[epoch4, step3063]: loss 0.550406
[epoch4, step3064]: loss 0.570544
[epoch4, step3065]: loss 0.779801
[epoch4, step3066]: loss 0.589478
[epoch4, step3067]: loss 0.662439
[epoch4, step3068]: loss 0.549945
[epoch4, step3069]: loss 0.518449
[epoch4, step3070]: loss 0.300910
[epoch4, step3071]: loss 0.502373
[epoch4, step3072]: loss 0.760682
[epoch4, step3073]: loss 0.618404
[epoch4, step3074]: loss 0.708543
[epoch4, step3075]: loss 0.391585
[epoch4, step3076]: loss 0.426233

[epoch4]: avg loss 0.426233

[epoch5, step1]: loss 0.749063
[epoch5, step2]: loss 0.686308
[epoch5, step3]: loss 0.897992
[epoch5, step4]: loss 0.582143
[epoch5, step5]: loss 0.709022
[epoch5, step6]: loss 0.530347
[epoch5, step7]: loss 0.600130
[epoch5, step8]: loss 0.765231
[epoch5, step9]: loss 0.566078
[epoch5, step10]: loss 0.496884
[epoch5, step11]: loss 0.381505
[epoch5, step12]: loss 0.477226
[epoch5, step13]: loss 0.681803
[epoch5, step14]: loss 0.802138
[epoch5, step15]: loss 0.618711
[epoch5, step16]: loss 0.697045
[epoch5, step17]: loss 0.517733
[epoch5, step18]: loss 0.498174
[epoch5, step19]: loss 0.440586
[epoch5, step20]: loss 0.669945
[epoch5, step21]: loss 0.646538
[epoch5, step22]: loss 0.782294
[epoch5, step23]: loss 0.607015
[epoch5, step24]: loss 0.685579
[epoch5, step25]: loss 0.471762
[epoch5, step26]: loss 0.718572
[epoch5, step27]: loss 0.692728
[epoch5, step28]: loss 0.727622
[epoch5, step29]: loss 0.902797
[epoch5, step30]: loss 0.586706
[epoch5, step31]: loss 0.423249
[epoch5, step32]: loss 0.693235
[epoch5, step33]: loss 0.685800
[epoch5, step34]: loss 0.517833
[epoch5, step35]: loss 0.610690
[epoch5, step36]: loss 0.529005
[epoch5, step37]: loss 0.585600
[epoch5, step38]: loss 0.572016
[epoch5, step39]: loss 0.828572
[epoch5, step40]: loss 0.544016
[epoch5, step41]: loss 0.632088
[epoch5, step42]: loss 0.711588
[epoch5, step43]: loss 0.335728
[epoch5, step44]: loss 0.728668
[epoch5, step45]: loss 0.483095
[epoch5, step46]: loss 0.870452
[epoch5, step47]: loss 0.589996
[epoch5, step48]: loss 0.443643
[epoch5, step49]: loss 0.729071
[epoch5, step50]: loss 0.861287
[epoch5, step51]: loss 0.325136
[epoch5, step52]: loss 0.528684
[epoch5, step53]: loss 0.619982
[epoch5, step54]: loss 0.202379
[epoch5, step55]: loss 0.496113
[epoch5, step56]: loss 0.548990
[epoch5, step57]: loss 0.509384
[epoch5, step58]: loss 0.672127
[epoch5, step59]: loss 0.482817
[epoch5, step60]: loss 0.448829
[epoch5, step61]: loss 0.610974
[epoch5, step62]: loss 0.426140
[epoch5, step63]: loss 0.691641
[epoch5, step64]: loss 0.514021
[epoch5, step65]: loss 0.343027
[epoch5, step66]: loss 0.764021
[epoch5, step67]: loss 0.676508
[epoch5, step68]: loss 0.611466
[epoch5, step69]: loss 0.542633
[epoch5, step70]: loss 0.479416
[epoch5, step71]: loss 0.417508
[epoch5, step72]: loss 0.675636
[epoch5, step73]: loss 0.537387
[epoch5, step74]: loss 0.812647
[epoch5, step75]: loss 0.464963
[epoch5, step76]: loss 0.730666
[epoch5, step77]: loss 0.523068
[epoch5, step78]: loss 0.555212
[epoch5, step79]: loss 0.437200
[epoch5, step80]: loss 0.673500
[epoch5, step81]: loss 0.399546
[epoch5, step82]: loss 0.657248
[epoch5, step83]: loss 0.626368
[epoch5, step84]: loss 0.879570
[epoch5, step85]: loss 0.667118
[epoch5, step86]: loss 0.547060
[epoch5, step87]: loss 0.558967
[epoch5, step88]: loss 0.761207
[epoch5, step89]: loss 0.583406
[epoch5, step90]: loss 0.616874
[epoch5, step91]: loss 0.555725
[epoch5, step92]: loss 0.613140
[epoch5, step93]: loss 0.858755
[epoch5, step94]: loss 0.434586
[epoch5, step95]: loss 0.543161
[epoch5, step96]: loss 0.449413
[epoch5, step97]: loss 0.689919
[epoch5, step98]: loss 0.626646
[epoch5, step99]: loss 0.588231
[epoch5, step100]: loss 0.626628
[epoch5, step101]: loss 0.731365
[epoch5, step102]: loss 0.499184
[epoch5, step103]: loss 0.711416
[epoch5, step104]: loss 0.742433
[epoch5, step105]: loss 0.397605
[epoch5, step106]: loss 0.550296
[epoch5, step107]: loss 0.155333
[epoch5, step108]: loss 0.685787
[epoch5, step109]: loss 0.476055
[epoch5, step110]: loss 0.594010
[epoch5, step111]: loss 0.828042
[epoch5, step112]: loss 0.442363
[epoch5, step113]: loss 0.699369
[epoch5, step114]: loss 0.529172
[epoch5, step115]: loss 0.593249
[epoch5, step116]: loss 0.700932
[epoch5, step117]: loss 0.424507
[epoch5, step118]: loss 0.626232
[epoch5, step119]: loss 0.730540
[epoch5, step120]: loss 0.472036
[epoch5, step121]: loss 0.709913
[epoch5, step122]: loss 0.330296
[epoch5, step123]: loss 0.552971
[epoch5, step124]: loss 0.473947
[epoch5, step125]: loss 0.743154
[epoch5, step126]: loss 0.613707
[epoch5, step127]: loss 0.684807
[epoch5, step128]: loss 0.562091
[epoch5, step129]: loss 0.536005
[epoch5, step130]: loss 0.375403
[epoch5, step131]: loss 0.496105
[epoch5, step132]: loss 0.786345
[epoch5, step133]: loss 0.681794
[epoch5, step134]: loss 0.574927
[epoch5, step135]: loss 0.740247
[epoch5, step136]: loss 0.206894
[epoch5, step137]: loss 0.566492
[epoch5, step138]: loss 0.448758
[epoch5, step139]: loss 0.757864
[epoch5, step140]: loss 0.552652
[epoch5, step141]: loss 0.664780
[epoch5, step142]: loss 0.841832
[epoch5, step143]: loss 0.584609
[epoch5, step144]: loss 0.612609
[epoch5, step145]: loss 0.373352
[epoch5, step146]: loss 0.732192
[epoch5, step147]: loss 0.776721
[epoch5, step148]: loss 0.534059
[epoch5, step149]: loss 0.708986
[epoch5, step150]: loss 0.386712
[epoch5, step151]: loss 0.681736
[epoch5, step152]: loss 0.570444
[epoch5, step153]: loss 0.663446
[epoch5, step154]: loss 0.639487
[epoch5, step155]: loss 0.720502
[epoch5, step156]: loss 0.403623
[epoch5, step157]: loss 0.536317
[epoch5, step158]: loss 0.729584
[epoch5, step159]: loss 0.694139
[epoch5, step160]: loss 0.540591
[epoch5, step161]: loss 0.404571
[epoch5, step162]: loss 0.833412
[epoch5, step163]: loss 0.274609
[epoch5, step164]: loss 0.767077
[epoch5, step165]: loss 0.452898
[epoch5, step166]: loss 0.223046
[epoch5, step167]: loss 0.675347
[epoch5, step168]: loss 0.692454
[epoch5, step169]: loss 0.460025
[epoch5, step170]: loss 0.697427
[epoch5, step171]: loss 0.704925
[epoch5, step172]: loss 0.666977
[epoch5, step173]: loss 0.531215
[epoch5, step174]: loss 0.412199
[epoch5, step175]: loss 0.518285
[epoch5, step176]: loss 0.478082
[epoch5, step177]: loss 0.488743
[epoch5, step178]: loss 0.622530
[epoch5, step179]: loss 0.613347
[epoch5, step180]: loss 0.640850
[epoch5, step181]: loss 0.437246
[epoch5, step182]: loss 0.566246
[epoch5, step183]: loss 0.583079
[epoch5, step184]: loss 0.617117
[epoch5, step185]: loss 0.652309
[epoch5, step186]: loss 0.508357
[epoch5, step187]: loss 0.499133
[epoch5, step188]: loss 0.736096
[epoch5, step189]: loss 0.562946
[epoch5, step190]: loss 0.472125
[epoch5, step191]: loss 0.550513
[epoch5, step192]: loss 0.700058
[epoch5, step193]: loss 0.773258
[epoch5, step194]: loss 0.664104
[epoch5, step195]: loss 0.748222
[epoch5, step196]: loss 0.617414
[epoch5, step197]: loss 0.702215
[epoch5, step198]: loss 0.744493
[epoch5, step199]: loss 0.458434
[epoch5, step200]: loss 0.530782
[epoch5, step201]: loss 0.299342
[epoch5, step202]: loss 0.742898
[epoch5, step203]: loss 0.572038
[epoch5, step204]: loss 0.422948
[epoch5, step205]: loss 0.409389
[epoch5, step206]: loss 0.362048
[epoch5, step207]: loss 0.357817
[epoch5, step208]: loss 0.630676
[epoch5, step209]: loss 0.398341
[epoch5, step210]: loss 0.896446
[epoch5, step211]: loss 0.487777
[epoch5, step212]: loss 0.669270
[epoch5, step213]: loss 0.547898
[epoch5, step214]: loss 0.387696
[epoch5, step215]: loss 0.774270
[epoch5, step216]: loss 0.472119
[epoch5, step217]: loss 0.401332
[epoch5, step218]: loss 0.525241
[epoch5, step219]: loss 0.744928
[epoch5, step220]: loss 0.714574
[epoch5, step221]: loss 0.518961
[epoch5, step222]: loss 0.621877
[epoch5, step223]: loss 0.523382
[epoch5, step224]: loss 0.594958
[epoch5, step225]: loss 0.412743
[epoch5, step226]: loss 0.632499
[epoch5, step227]: loss 0.733549
[epoch5, step228]: loss 0.298089
[epoch5, step229]: loss 0.732897
[epoch5, step230]: loss 0.727729
[epoch5, step231]: loss 0.632369
[epoch5, step232]: loss 0.709183
[epoch5, step233]: loss 0.569349
[epoch5, step234]: loss 0.563613
[epoch5, step235]: loss 0.637021
[epoch5, step236]: loss 0.658847
[epoch5, step237]: loss 0.492749
[epoch5, step238]: loss 0.447389
[epoch5, step239]: loss 0.690911
[epoch5, step240]: loss 0.793225
[epoch5, step241]: loss 0.497854
[epoch5, step242]: loss 0.384722
[epoch5, step243]: loss 0.594931
[epoch5, step244]: loss 0.672310
[epoch5, step245]: loss 0.427989
[epoch5, step246]: loss 0.947242
[epoch5, step247]: loss 0.372694
[epoch5, step248]: loss 0.765331
[epoch5, step249]: loss 0.651014
[epoch5, step250]: loss 0.381986
[epoch5, step251]: loss 0.491313
[epoch5, step252]: loss 0.540727
[epoch5, step253]: loss 0.696370
[epoch5, step254]: loss 0.583248
[epoch5, step255]: loss 0.762297
[epoch5, step256]: loss 0.695170
[epoch5, step257]: loss 0.523908
[epoch5, step258]: loss 0.683147
[epoch5, step259]: loss 0.456585
[epoch5, step260]: loss 0.396989
[epoch5, step261]: loss 0.749099
[epoch5, step262]: loss 0.368706
[epoch5, step263]: loss 0.924175
[epoch5, step264]: loss 0.750651
[epoch5, step265]: loss 0.292911
[epoch5, step266]: loss 0.558324
[epoch5, step267]: loss 0.410545
[epoch5, step268]: loss 0.687760
[epoch5, step269]: loss 0.539165
[epoch5, step270]: loss 0.511436
[epoch5, step271]: loss 0.672882
[epoch5, step272]: loss 0.392922
[epoch5, step273]: loss 0.741685
[epoch5, step274]: loss 0.509293
[epoch5, step275]: loss 0.851969
[epoch5, step276]: loss 0.807183
[epoch5, step277]: loss 0.853785
[epoch5, step278]: loss 0.395756
[epoch5, step279]: loss 0.611781
[epoch5, step280]: loss 0.360289
[epoch5, step281]: loss 0.593604
[epoch5, step282]: loss 0.614492
[epoch5, step283]: loss 0.398038
[epoch5, step284]: loss 0.510162
[epoch5, step285]: loss 0.432641
[epoch5, step286]: loss 0.660552
[epoch5, step287]: loss 0.676074
[epoch5, step288]: loss 0.622400
[epoch5, step289]: loss 0.833612
[epoch5, step290]: loss 0.532894
[epoch5, step291]: loss 0.468432
[epoch5, step292]: loss 0.424581
[epoch5, step293]: loss 0.557723
[epoch5, step294]: loss 0.600529
[epoch5, step295]: loss 0.528693
[epoch5, step296]: loss 0.639999
[epoch5, step297]: loss 0.662603
[epoch5, step298]: loss 0.668662
[epoch5, step299]: loss 0.512456
[epoch5, step300]: loss 0.430105
[epoch5, step301]: loss 0.722306
[epoch5, step302]: loss 0.719181
[epoch5, step303]: loss 0.722113
[epoch5, step304]: loss 0.494308
[epoch5, step305]: loss 0.641615
[epoch5, step306]: loss 0.623696
[epoch5, step307]: loss 0.649296
[epoch5, step308]: loss 0.703105
[epoch5, step309]: loss 0.715539
[epoch5, step310]: loss 0.547167
[epoch5, step311]: loss 0.438916
[epoch5, step312]: loss 0.647999
[epoch5, step313]: loss 0.684921
[epoch5, step314]: loss 0.705913
[epoch5, step315]: loss 0.536144
[epoch5, step316]: loss 0.705502
[epoch5, step317]: loss 0.455217
[epoch5, step318]: loss 0.606698
[epoch5, step319]: loss 0.607110
[epoch5, step320]: loss 0.754781
[epoch5, step321]: loss 0.549921
[epoch5, step322]: loss 0.466302
[epoch5, step323]: loss 0.677519
[epoch5, step324]: loss 0.342765
[epoch5, step325]: loss 0.809060
[epoch5, step326]: loss 0.595183
[epoch5, step327]: loss 0.699693
[epoch5, step328]: loss 0.719778
[epoch5, step329]: loss 0.566007
[epoch5, step330]: loss 0.616116
[epoch5, step331]: loss 0.604924
[epoch5, step332]: loss 0.679610
[epoch5, step333]: loss 0.567872
[epoch5, step334]: loss 0.605554
[epoch5, step335]: loss 0.270511
[epoch5, step336]: loss 0.454844
[epoch5, step337]: loss 0.717520
[epoch5, step338]: loss 0.586102
[epoch5, step339]: loss 0.590128
[epoch5, step340]: loss 0.716176
[epoch5, step341]: loss 0.466402
[epoch5, step342]: loss 0.686404
[epoch5, step343]: loss 0.546780
[epoch5, step344]: loss 0.674238
[epoch5, step345]: loss 0.594932
[epoch5, step346]: loss 0.851134
[epoch5, step347]: loss 0.532458
[epoch5, step348]: loss 0.416118
[epoch5, step349]: loss 0.684131
[epoch5, step350]: loss 0.549849
[epoch5, step351]: loss 0.674622
[epoch5, step352]: loss 0.624015
[epoch5, step353]: loss 0.381465
[epoch5, step354]: loss 0.794571
[epoch5, step355]: loss 0.683072
[epoch5, step356]: loss 0.797140
[epoch5, step357]: loss 0.587024
[epoch5, step358]: loss 0.435708
[epoch5, step359]: loss 0.733318
[epoch5, step360]: loss 0.708088
[epoch5, step361]: loss 0.444538
[epoch5, step362]: loss 0.373377
[epoch5, step363]: loss 0.674181
[epoch5, step364]: loss 0.524572
[epoch5, step365]: loss 0.530398
[epoch5, step366]: loss 0.719352
[epoch5, step367]: loss 0.585182
[epoch5, step368]: loss 0.728937
[epoch5, step369]: loss 0.522794
[epoch5, step370]: loss 0.625936
[epoch5, step371]: loss 0.708739
[epoch5, step372]: loss 0.474068
[epoch5, step373]: loss 0.556530
[epoch5, step374]: loss 0.533120
[epoch5, step375]: loss 0.298175
[epoch5, step376]: loss 0.734627
[epoch5, step377]: loss 0.435670
[epoch5, step378]: loss 0.641628
[epoch5, step379]: loss 0.770131
[epoch5, step380]: loss 0.443722
[epoch5, step381]: loss 0.538671
[epoch5, step382]: loss 0.537022
[epoch5, step383]: loss 0.565089
[epoch5, step384]: loss 0.899021
[epoch5, step385]: loss 0.679365
[epoch5, step386]: loss 0.730933
[epoch5, step387]: loss 0.771432
[epoch5, step388]: loss 0.820700
[epoch5, step389]: loss 0.673459
[epoch5, step390]: loss 0.840760
[epoch5, step391]: loss 0.618178
[epoch5, step392]: loss 0.539240
[epoch5, step393]: loss 0.554662
[epoch5, step394]: loss 0.347929
[epoch5, step395]: loss 0.587608
[epoch5, step396]: loss 0.596932
[epoch5, step397]: loss 0.437169
[epoch5, step398]: loss 0.235149
[epoch5, step399]: loss 0.618375
[epoch5, step400]: loss 0.287572
[epoch5, step401]: loss 0.532329
[epoch5, step402]: loss 0.616669
[epoch5, step403]: loss 0.655771
[epoch5, step404]: loss 0.426300
[epoch5, step405]: loss 0.271184
[epoch5, step406]: loss 0.538822
[epoch5, step407]: loss 0.539325
[epoch5, step408]: loss 0.687678
[epoch5, step409]: loss 0.324057
[epoch5, step410]: loss 0.646797
[epoch5, step411]: loss 0.731763
[epoch5, step412]: loss 0.662577
[epoch5, step413]: loss 0.682483
[epoch5, step414]: loss 0.562792
[epoch5, step415]: loss 0.636964
[epoch5, step416]: loss 0.637614
[epoch5, step417]: loss 0.717593
[epoch5, step418]: loss 0.542471
[epoch5, step419]: loss 0.656155
[epoch5, step420]: loss 0.601450
[epoch5, step421]: loss 0.616301
[epoch5, step422]: loss 0.633099
[epoch5, step423]: loss 0.444460
[epoch5, step424]: loss 0.735320
[epoch5, step425]: loss 0.647578
[epoch5, step426]: loss 0.637199
[epoch5, step427]: loss 0.630874
[epoch5, step428]: loss 0.475136
[epoch5, step429]: loss 0.570388
[epoch5, step430]: loss 0.678368
[epoch5, step431]: loss 0.423208
[epoch5, step432]: loss 0.355243
[epoch5, step433]: loss 0.483564
[epoch5, step434]: loss 0.815473
[epoch5, step435]: loss 0.399922
[epoch5, step436]: loss 0.539492
[epoch5, step437]: loss 0.292912
[epoch5, step438]: loss 0.436052
[epoch5, step439]: loss 0.687252
[epoch5, step440]: loss 0.694657
[epoch5, step441]: loss 0.459410
[epoch5, step442]: loss 0.592692
[epoch5, step443]: loss 0.649629
[epoch5, step444]: loss 0.586948
[epoch5, step445]: loss 0.557667
[epoch5, step446]: loss 0.480012
[epoch5, step447]: loss 0.468491
[epoch5, step448]: loss 0.526303
[epoch5, step449]: loss 0.501112
[epoch5, step450]: loss 0.539791
[epoch5, step451]: loss 0.558311
[epoch5, step452]: loss 0.606462
[epoch5, step453]: loss 0.464863
[epoch5, step454]: loss 0.650348
[epoch5, step455]: loss 0.641067
[epoch5, step456]: loss 0.618307
[epoch5, step457]: loss 0.702621
[epoch5, step458]: loss 0.442717
[epoch5, step459]: loss 0.725101
[epoch5, step460]: loss 0.407511
[epoch5, step461]: loss 0.700396
[epoch5, step462]: loss 0.731629
[epoch5, step463]: loss 0.739442
[epoch5, step464]: loss 0.755157
[epoch5, step465]: loss 0.585896
[epoch5, step466]: loss 0.482001
[epoch5, step467]: loss 0.435921
[epoch5, step468]: loss 0.653155
[epoch5, step469]: loss 0.580754
[epoch5, step470]: loss 0.357119
[epoch5, step471]: loss 0.503934
[epoch5, step472]: loss 0.508621
[epoch5, step473]: loss 0.344610
[epoch5, step474]: loss 0.530731
[epoch5, step475]: loss 0.410504
[epoch5, step476]: loss 0.474928
[epoch5, step477]: loss 0.595325
[epoch5, step478]: loss 0.570135
[epoch5, step479]: loss 0.450786
[epoch5, step480]: loss 0.670277
[epoch5, step481]: loss 0.425230
[epoch5, step482]: loss 0.645024
[epoch5, step483]: loss 0.449318
[epoch5, step484]: loss 0.570434
[epoch5, step485]: loss 0.458631
[epoch5, step486]: loss 0.451699
[epoch5, step487]: loss 0.653814
[epoch5, step488]: loss 0.616688
[epoch5, step489]: loss 0.604410
[epoch5, step490]: loss 0.416547
[epoch5, step491]: loss 0.730200
[epoch5, step492]: loss 0.460941
[epoch5, step493]: loss 0.555776
[epoch5, step494]: loss 0.469284
[epoch5, step495]: loss 0.448290
[epoch5, step496]: loss 0.517332
[epoch5, step497]: loss 0.454002
[epoch5, step498]: loss 0.419327
[epoch5, step499]: loss 0.552955
[epoch5, step500]: loss 0.770384
[epoch5, step501]: loss 0.607769
[epoch5, step502]: loss 0.579763
[epoch5, step503]: loss 0.532346
[epoch5, step504]: loss 0.599405
[epoch5, step505]: loss 0.333940
[epoch5, step506]: loss 0.505508
[epoch5, step507]: loss 0.678711
[epoch5, step508]: loss 0.404750
[epoch5, step509]: loss 0.332617
[epoch5, step510]: loss 0.805336
[epoch5, step511]: loss 0.659028
[epoch5, step512]: loss 0.683075
[epoch5, step513]: loss 0.666035
[epoch5, step514]: loss 0.495479
[epoch5, step515]: loss 0.699054
[epoch5, step516]: loss 0.733417
[epoch5, step517]: loss 0.374324
[epoch5, step518]: loss 0.287040
[epoch5, step519]: loss 0.538573
[epoch5, step520]: loss 0.807453
[epoch5, step521]: loss 0.498262
[epoch5, step522]: loss 0.691297
[epoch5, step523]: loss 0.616223
[epoch5, step524]: loss 0.764741
[epoch5, step525]: loss 0.626060
[epoch5, step526]: loss 0.579746
[epoch5, step527]: loss 0.369316
[epoch5, step528]: loss 0.374203
[epoch5, step529]: loss 0.588881
[epoch5, step530]: loss 0.688466
[epoch5, step531]: loss 0.496133
[epoch5, step532]: loss 0.479092
[epoch5, step533]: loss 0.200467
[epoch5, step534]: loss 0.761983
[epoch5, step535]: loss 0.542747
[epoch5, step536]: loss 0.544046
[epoch5, step537]: loss 0.497019
[epoch5, step538]: loss 0.592743
[epoch5, step539]: loss 0.484519
[epoch5, step540]: loss 0.723580
[epoch5, step541]: loss 0.438133
[epoch5, step542]: loss 0.440539
[epoch5, step543]: loss 0.851634
[epoch5, step544]: loss 0.737253
[epoch5, step545]: loss 0.621610
[epoch5, step546]: loss 0.491065
[epoch5, step547]: loss 0.563441
[epoch5, step548]: loss 0.542259
[epoch5, step549]: loss 0.619588
[epoch5, step550]: loss 0.565252
[epoch5, step551]: loss 0.213742
[epoch5, step552]: loss 0.570416
[epoch5, step553]: loss 0.825710
[epoch5, step554]: loss 0.394798
[epoch5, step555]: loss 0.480993
[epoch5, step556]: loss 0.630427
[epoch5, step557]: loss 0.608622
[epoch5, step558]: loss 0.370222
[epoch5, step559]: loss 0.760982
[epoch5, step560]: loss 0.700350
[epoch5, step561]: loss 0.682243
[epoch5, step562]: loss 0.483896
[epoch5, step563]: loss 0.605702
[epoch5, step564]: loss 0.677397
[epoch5, step565]: loss 0.494092
[epoch5, step566]: loss 0.407083
[epoch5, step567]: loss 0.491250
[epoch5, step568]: loss 0.697676
[epoch5, step569]: loss 0.782211
[epoch5, step570]: loss 0.777420
[epoch5, step571]: loss 0.729832
[epoch5, step572]: loss 0.539337
[epoch5, step573]: loss 0.281605
[epoch5, step574]: loss 0.679217
[epoch5, step575]: loss 0.616423
[epoch5, step576]: loss 0.407798
[epoch5, step577]: loss 0.537109
[epoch5, step578]: loss 0.584464
[epoch5, step579]: loss 0.689285
[epoch5, step580]: loss 0.564745
[epoch5, step581]: loss 0.746833
[epoch5, step582]: loss 0.558383
[epoch5, step583]: loss 0.431749
[epoch5, step584]: loss 0.771495
[epoch5, step585]: loss 0.604096
[epoch5, step586]: loss 0.682703
[epoch5, step587]: loss 0.351716
[epoch5, step588]: loss 0.415198
[epoch5, step589]: loss 0.383739
[epoch5, step590]: loss 0.495603
[epoch5, step591]: loss 0.634401
[epoch5, step592]: loss 0.598665
[epoch5, step593]: loss 0.813949
[epoch5, step594]: loss 0.433925
[epoch5, step595]: loss 0.661757
[epoch5, step596]: loss 0.538501
[epoch5, step597]: loss 0.453289
[epoch5, step598]: loss 0.649279
[epoch5, step599]: loss 0.430024
[epoch5, step600]: loss 0.707135
[epoch5, step601]: loss 0.381184
[epoch5, step602]: loss 0.295479
[epoch5, step603]: loss 0.445078
[epoch5, step604]: loss 0.571566
[epoch5, step605]: loss 0.716599
[epoch5, step606]: loss 0.558325
[epoch5, step607]: loss 0.633641
[epoch5, step608]: loss 0.725900
[epoch5, step609]: loss 0.765991
[epoch5, step610]: loss 0.601544
[epoch5, step611]: loss 0.356697
[epoch5, step612]: loss 0.620805
[epoch5, step613]: loss 0.430129
[epoch5, step614]: loss 0.461736
[epoch5, step615]: loss 0.703805
[epoch5, step616]: loss 0.357564
[epoch5, step617]: loss 0.734649
[epoch5, step618]: loss 0.694949
[epoch5, step619]: loss 0.766340
[epoch5, step620]: loss 0.563356
[epoch5, step621]: loss 0.674553
[epoch5, step622]: loss 0.513520
[epoch5, step623]: loss 0.742288
[epoch5, step624]: loss 0.710661
[epoch5, step625]: loss 0.602634
[epoch5, step626]: loss 0.585894
[epoch5, step627]: loss 0.441107
[epoch5, step628]: loss 0.479979
[epoch5, step629]: loss 0.462293
[epoch5, step630]: loss 0.351611
[epoch5, step631]: loss 0.498126
[epoch5, step632]: loss 0.806229
[epoch5, step633]: loss 0.574798
[epoch5, step634]: loss 0.798012
[epoch5, step635]: loss 0.648641
[epoch5, step636]: loss 0.368855
[epoch5, step637]: loss 0.456261
[epoch5, step638]: loss 0.707384
[epoch5, step639]: loss 0.711189
[epoch5, step640]: loss 0.497367
[epoch5, step641]: loss 0.382979
[epoch5, step642]: loss 0.269386
[epoch5, step643]: loss 0.516081
[epoch5, step644]: loss 0.716800
[epoch5, step645]: loss 0.510212
[epoch5, step646]: loss 0.625917
[epoch5, step647]: loss 0.636063
[epoch5, step648]: loss 0.440679
[epoch5, step649]: loss 0.627157
[epoch5, step650]: loss 0.838343
[epoch5, step651]: loss 0.562525
[epoch5, step652]: loss 0.490155
[epoch5, step653]: loss 0.500471
[epoch5, step654]: loss 0.583619
[epoch5, step655]: loss 0.642317
[epoch5, step656]: loss 0.604140
[epoch5, step657]: loss 0.391589
[epoch5, step658]: loss 0.744387
[epoch5, step659]: loss 0.460563
[epoch5, step660]: loss 0.534534
[epoch5, step661]: loss 0.330650
[epoch5, step662]: loss 0.310230
[epoch5, step663]: loss 0.498446
[epoch5, step664]: loss 0.290148
[epoch5, step665]: loss 0.413963
[epoch5, step666]: loss 0.302980
[epoch5, step667]: loss 0.598937
[epoch5, step668]: loss 0.727509
[epoch5, step669]: loss 0.562665
[epoch5, step670]: loss 0.610560
[epoch5, step671]: loss 0.595509
[epoch5, step672]: loss 0.505036
[epoch5, step673]: loss 0.653379
[epoch5, step674]: loss 0.570703
[epoch5, step675]: loss 0.278761
[epoch5, step676]: loss 0.719692
[epoch5, step677]: loss 0.487096
[epoch5, step678]: loss 0.647639
[epoch5, step679]: loss 0.376557
[epoch5, step680]: loss 0.625138
[epoch5, step681]: loss 0.470073
[epoch5, step682]: loss 0.663517
[epoch5, step683]: loss 0.523539
[epoch5, step684]: loss 0.495156
[epoch5, step685]: loss 0.505346
[epoch5, step686]: loss 0.504222
[epoch5, step687]: loss 0.766559
[epoch5, step688]: loss 0.599932
[epoch5, step689]: loss 0.415066
[epoch5, step690]: loss 0.288085
[epoch5, step691]: loss 0.710380
[epoch5, step692]: loss 0.715341
[epoch5, step693]: loss 0.419348
[epoch5, step694]: loss 0.583928
[epoch5, step695]: loss 0.515020
[epoch5, step696]: loss 0.503394
[epoch5, step697]: loss 0.683516
[epoch5, step698]: loss 0.576312
[epoch5, step699]: loss 0.557891
[epoch5, step700]: loss 0.375453
[epoch5, step701]: loss 0.237332
[epoch5, step702]: loss 0.739896
[epoch5, step703]: loss 0.720718
[epoch5, step704]: loss 0.603247
[epoch5, step705]: loss 0.566006
[epoch5, step706]: loss 0.572427
[epoch5, step707]: loss 0.680040
[epoch5, step708]: loss 0.614391
[epoch5, step709]: loss 0.673935
[epoch5, step710]: loss 0.488827
[epoch5, step711]: loss 0.785369
[epoch5, step712]: loss 0.371748
[epoch5, step713]: loss 0.600068
[epoch5, step714]: loss 0.733855
[epoch5, step715]: loss 0.592619
[epoch5, step716]: loss 0.479894
[epoch5, step717]: loss 0.456514
[epoch5, step718]: loss 0.652758
[epoch5, step719]: loss 0.643135
[epoch5, step720]: loss 0.657976
[epoch5, step721]: loss 0.733934
[epoch5, step722]: loss 0.767387
[epoch5, step723]: loss 0.601970
[epoch5, step724]: loss 0.304478
[epoch5, step725]: loss 0.805460
[epoch5, step726]: loss 0.196665
[epoch5, step727]: loss 0.355357
[epoch5, step728]: loss 0.525402
[epoch5, step729]: loss 0.698666
[epoch5, step730]: loss 0.498463
[epoch5, step731]: loss 0.904666
[epoch5, step732]: loss 0.594724
[epoch5, step733]: loss 0.701536
[epoch5, step734]: loss 0.753775
[epoch5, step735]: loss 0.584962
[epoch5, step736]: loss 0.534215
[epoch5, step737]: loss 0.537040
[epoch5, step738]: loss 0.574700
[epoch5, step739]: loss 0.697175
[epoch5, step740]: loss 0.429072
[epoch5, step741]: loss 0.624810
[epoch5, step742]: loss 0.333913
[epoch5, step743]: loss 0.490268
[epoch5, step744]: loss 0.692528
[epoch5, step745]: loss 0.511616
[epoch5, step746]: loss 0.644981
[epoch5, step747]: loss 0.423205
[epoch5, step748]: loss 0.866033
[epoch5, step749]: loss 0.302564
[epoch5, step750]: loss 0.622809
[epoch5, step751]: loss 0.641442
[epoch5, step752]: loss 0.506882
[epoch5, step753]: loss 0.535491
[epoch5, step754]: loss 0.551415
[epoch5, step755]: loss 0.519504
[epoch5, step756]: loss 0.821435
[epoch5, step757]: loss 0.534663
[epoch5, step758]: loss 0.733561
[epoch5, step759]: loss 0.601300
[epoch5, step760]: loss 0.719549
[epoch5, step761]: loss 0.636061
[epoch5, step762]: loss 0.704497
[epoch5, step763]: loss 0.435233
[epoch5, step764]: loss 0.524851
[epoch5, step765]: loss 0.761658
[epoch5, step766]: loss 0.592858
[epoch5, step767]: loss 0.483687
[epoch5, step768]: loss 0.582297
[epoch5, step769]: loss 0.727664
[epoch5, step770]: loss 0.684386
[epoch5, step771]: loss 0.555551
[epoch5, step772]: loss 0.661655
[epoch5, step773]: loss 0.367182
[epoch5, step774]: loss 0.607659
[epoch5, step775]: loss 0.538294
[epoch5, step776]: loss 0.336996
[epoch5, step777]: loss 0.647665
[epoch5, step778]: loss 0.521248
[epoch5, step779]: loss 0.508435
[epoch5, step780]: loss 0.534737
[epoch5, step781]: loss 0.408005
[epoch5, step782]: loss 0.458913
[epoch5, step783]: loss 0.647787
[epoch5, step784]: loss 0.471358
[epoch5, step785]: loss 0.754071
[epoch5, step786]: loss 0.625910
[epoch5, step787]: loss 0.568503
[epoch5, step788]: loss 0.551202
[epoch5, step789]: loss 0.438250
[epoch5, step790]: loss 0.569520
[epoch5, step791]: loss 0.648989
[epoch5, step792]: loss 0.618412
[epoch5, step793]: loss 0.664474
[epoch5, step794]: loss 0.478319
[epoch5, step795]: loss 0.663123
[epoch5, step796]: loss 0.450000
[epoch5, step797]: loss 0.478072
[epoch5, step798]: loss 0.468645
[epoch5, step799]: loss 0.695958
[epoch5, step800]: loss 0.722580
[epoch5, step801]: loss 0.605364
[epoch5, step802]: loss 0.672686
[epoch5, step803]: loss 0.551459
[epoch5, step804]: loss 0.814228
[epoch5, step805]: loss 0.645213
[epoch5, step806]: loss 0.391746
[epoch5, step807]: loss 0.465638
[epoch5, step808]: loss 0.848674
[epoch5, step809]: loss 0.878351
[epoch5, step810]: loss 0.550773
[epoch5, step811]: loss 0.684542
[epoch5, step812]: loss 0.232723
[epoch5, step813]: loss 0.776561
[epoch5, step814]: loss 0.453726
[epoch5, step815]: loss 0.446511
[epoch5, step816]: loss 0.459139
[epoch5, step817]: loss 0.556550
[epoch5, step818]: loss 0.850194
[epoch5, step819]: loss 0.304336
[epoch5, step820]: loss 0.722341
[epoch5, step821]: loss 0.363859
[epoch5, step822]: loss 0.417484
[epoch5, step823]: loss 0.662192
[epoch5, step824]: loss 0.639068
[epoch5, step825]: loss 0.462311
[epoch5, step826]: loss 0.492413
[epoch5, step827]: loss 0.508812
[epoch5, step828]: loss 0.595853
[epoch5, step829]: loss 0.646740
[epoch5, step830]: loss 0.731220
[epoch5, step831]: loss 0.680352
[epoch5, step832]: loss 0.628513
[epoch5, step833]: loss 0.504587
[epoch5, step834]: loss 0.304727
[epoch5, step835]: loss 0.757772
[epoch5, step836]: loss 0.338413
[epoch5, step837]: loss 0.609150
[epoch5, step838]: loss 0.736615
[epoch5, step839]: loss 0.645991
[epoch5, step840]: loss 0.655894
[epoch5, step841]: loss 0.716606
[epoch5, step842]: loss 0.697562
[epoch5, step843]: loss 0.814420
[epoch5, step844]: loss 0.676847
[epoch5, step845]: loss 0.536404
[epoch5, step846]: loss 0.507859
[epoch5, step847]: loss 0.382356
[epoch5, step848]: loss 0.416379
[epoch5, step849]: loss 0.699377
[epoch5, step850]: loss 0.477397
[epoch5, step851]: loss 0.368397
[epoch5, step852]: loss 0.517279
[epoch5, step853]: loss 0.459084
[epoch5, step854]: loss 0.437376
[epoch5, step855]: loss 0.389112
[epoch5, step856]: loss 0.595072
[epoch5, step857]: loss 0.310145
[epoch5, step858]: loss 0.600493
[epoch5, step859]: loss 0.646529
[epoch5, step860]: loss 0.559749
[epoch5, step861]: loss 0.706122
[epoch5, step862]: loss 0.518599
[epoch5, step863]: loss 0.784483
[epoch5, step864]: loss 0.647726
[epoch5, step865]: loss 0.523889
[epoch5, step866]: loss 0.426450
[epoch5, step867]: loss 0.758834
[epoch5, step868]: loss 0.688724
[epoch5, step869]: loss 0.740655
[epoch5, step870]: loss 0.347652
[epoch5, step871]: loss 0.668106
[epoch5, step872]: loss 0.556224
[epoch5, step873]: loss 0.784460
[epoch5, step874]: loss 0.610303
[epoch5, step875]: loss 0.414950
[epoch5, step876]: loss 0.583902
[epoch5, step877]: loss 0.709364
[epoch5, step878]: loss 0.616614
[epoch5, step879]: loss 0.422431
[epoch5, step880]: loss 0.708264
[epoch5, step881]: loss 0.471453
[epoch5, step882]: loss 0.514289
[epoch5, step883]: loss 0.274408
[epoch5, step884]: loss 0.734772
[epoch5, step885]: loss 0.760127
[epoch5, step886]: loss 0.593052
[epoch5, step887]: loss 0.536720
[epoch5, step888]: loss 0.626473
[epoch5, step889]: loss 0.550634
[epoch5, step890]: loss 0.623087
[epoch5, step891]: loss 0.692554
[epoch5, step892]: loss 0.600088
[epoch5, step893]: loss 0.577803
[epoch5, step894]: loss 0.618329
[epoch5, step895]: loss 0.583667
[epoch5, step896]: loss 0.482763
[epoch5, step897]: loss 0.788251
[epoch5, step898]: loss 0.579486
[epoch5, step899]: loss 0.398555
[epoch5, step900]: loss 0.560221
[epoch5, step901]: loss 0.504299
[epoch5, step902]: loss 0.413609
[epoch5, step903]: loss 0.536613
[epoch5, step904]: loss 0.401380
[epoch5, step905]: loss 0.488077
[epoch5, step906]: loss 0.610386
[epoch5, step907]: loss 0.550381
[epoch5, step908]: loss 0.748854
[epoch5, step909]: loss 0.246050
[epoch5, step910]: loss 0.275740
[epoch5, step911]: loss 0.632628
[epoch5, step912]: loss 0.372764
[epoch5, step913]: loss 0.558679
[epoch5, step914]: loss 0.387705
[epoch5, step915]: loss 0.403003
[epoch5, step916]: loss 0.575056
[epoch5, step917]: loss 0.770405
[epoch5, step918]: loss 0.570301
[epoch5, step919]: loss 0.363056
[epoch5, step920]: loss 0.635700
[epoch5, step921]: loss 0.611854
[epoch5, step922]: loss 0.732778
[epoch5, step923]: loss 0.485443
[epoch5, step924]: loss 0.362500
[epoch5, step925]: loss 0.586626
[epoch5, step926]: loss 0.733288
[epoch5, step927]: loss 0.682130
[epoch5, step928]: loss 0.682017
[epoch5, step929]: loss 0.771297
[epoch5, step930]: loss 0.477635
[epoch5, step931]: loss 0.445820
[epoch5, step932]: loss 0.637562
[epoch5, step933]: loss 0.611629
[epoch5, step934]: loss 0.745254
[epoch5, step935]: loss 0.364088
[epoch5, step936]: loss 0.349109
[epoch5, step937]: loss 0.537258
[epoch5, step938]: loss 0.464232
[epoch5, step939]: loss 0.268003
[epoch5, step940]: loss 0.569206
[epoch5, step941]: loss 0.628959
[epoch5, step942]: loss 0.601978
[epoch5, step943]: loss 0.328318
[epoch5, step944]: loss 0.690020
[epoch5, step945]: loss 0.556199
[epoch5, step946]: loss 0.660545
[epoch5, step947]: loss 0.620293
[epoch5, step948]: loss 0.320605
[epoch5, step949]: loss 0.687256
[epoch5, step950]: loss 0.548402
[epoch5, step951]: loss 0.854534
[epoch5, step952]: loss 0.718769
[epoch5, step953]: loss 0.487713
[epoch5, step954]: loss 0.587738
[epoch5, step955]: loss 0.619881
[epoch5, step956]: loss 0.745601
[epoch5, step957]: loss 0.684946
[epoch5, step958]: loss 0.308392
[epoch5, step959]: loss 0.479707
[epoch5, step960]: loss 0.647860
[epoch5, step961]: loss 0.676015
[epoch5, step962]: loss 0.665946
[epoch5, step963]: loss 0.800693
[epoch5, step964]: loss 0.629274
[epoch5, step965]: loss 0.658057
[epoch5, step966]: loss 0.775691
[epoch5, step967]: loss 0.970166
[epoch5, step968]: loss 0.863357
[epoch5, step969]: loss 0.750021
[epoch5, step970]: loss 0.300389
[epoch5, step971]: loss 0.817240
[epoch5, step972]: loss 0.459338
[epoch5, step973]: loss 0.505657
[epoch5, step974]: loss 0.413542
[epoch5, step975]: loss 0.476920
[epoch5, step976]: loss 0.697685
[epoch5, step977]: loss 0.672276
[epoch5, step978]: loss 0.641469
[epoch5, step979]: loss 0.634378
[epoch5, step980]: loss 0.676764
[epoch5, step981]: loss 0.657853
[epoch5, step982]: loss 0.481655
[epoch5, step983]: loss 0.724821
[epoch5, step984]: loss 0.699719
[epoch5, step985]: loss 0.711469
[epoch5, step986]: loss 0.658929
[epoch5, step987]: loss 0.644095
[epoch5, step988]: loss 0.477895
[epoch5, step989]: loss 0.602923
[epoch5, step990]: loss 0.417136
[epoch5, step991]: loss 0.476973
[epoch5, step992]: loss 0.467573
[epoch5, step993]: loss 0.546760
[epoch5, step994]: loss 0.618506
[epoch5, step995]: loss 0.616364
[epoch5, step996]: loss 0.868171
[epoch5, step997]: loss 0.601882
[epoch5, step998]: loss 0.442429
[epoch5, step999]: loss 0.513551
[epoch5, step1000]: loss 0.905273
[epoch5, step1001]: loss 0.579689
[epoch5, step1002]: loss 0.520804
[epoch5, step1003]: loss 0.576833
[epoch5, step1004]: loss 0.414692
[epoch5, step1005]: loss 0.473016
[epoch5, step1006]: loss 0.609275
[epoch5, step1007]: loss 0.651273
[epoch5, step1008]: loss 0.559298
[epoch5, step1009]: loss 0.799655
[epoch5, step1010]: loss 0.547613
[epoch5, step1011]: loss 0.384378
[epoch5, step1012]: loss 0.432859
[epoch5, step1013]: loss 0.677765
[epoch5, step1014]: loss 0.347134
[epoch5, step1015]: loss 0.839967
[epoch5, step1016]: loss 0.463098
[epoch5, step1017]: loss 0.735316
[epoch5, step1018]: loss 0.455775
[epoch5, step1019]: loss 0.204702
[epoch5, step1020]: loss 0.403556
[epoch5, step1021]: loss 0.531480
[epoch5, step1022]: loss 0.761428
[epoch5, step1023]: loss 0.784339
[epoch5, step1024]: loss 0.488414
[epoch5, step1025]: loss 0.634048
[epoch5, step1026]: loss 0.448577
[epoch5, step1027]: loss 0.339751
[epoch5, step1028]: loss 0.632127
[epoch5, step1029]: loss 0.827822
[epoch5, step1030]: loss 0.258999
[epoch5, step1031]: loss 0.616731
[epoch5, step1032]: loss 0.520201
[epoch5, step1033]: loss 0.526806
[epoch5, step1034]: loss 0.450356
[epoch5, step1035]: loss 0.431961
[epoch5, step1036]: loss 0.536166
[epoch5, step1037]: loss 0.619524
[epoch5, step1038]: loss 0.434570
[epoch5, step1039]: loss 0.688229
[epoch5, step1040]: loss 0.552362
[epoch5, step1041]: loss 0.606501
[epoch5, step1042]: loss 0.446670
[epoch5, step1043]: loss 0.681560
[epoch5, step1044]: loss 0.516180
[epoch5, step1045]: loss 0.629771
[epoch5, step1046]: loss 0.618642
[epoch5, step1047]: loss 0.277508
[epoch5, step1048]: loss 0.307398
[epoch5, step1049]: loss 0.565845
[epoch5, step1050]: loss 0.519948
[epoch5, step1051]: loss 0.412791
[epoch5, step1052]: loss 0.641540
[epoch5, step1053]: loss 0.633953
[epoch5, step1054]: loss 0.346500
[epoch5, step1055]: loss 0.609726
[epoch5, step1056]: loss 0.510446
[epoch5, step1057]: loss 0.426785
[epoch5, step1058]: loss 0.568864
[epoch5, step1059]: loss 0.438305
[epoch5, step1060]: loss 0.491758
[epoch5, step1061]: loss 0.347638
[epoch5, step1062]: loss 0.486961
[epoch5, step1063]: loss 0.543145
[epoch5, step1064]: loss 0.272438
[epoch5, step1065]: loss 0.508313
[epoch5, step1066]: loss 0.529453
[epoch5, step1067]: loss 0.700078
[epoch5, step1068]: loss 0.645653
[epoch5, step1069]: loss 0.401438
[epoch5, step1070]: loss 0.609612
[epoch5, step1071]: loss 0.501208
[epoch5, step1072]: loss 0.667126
[epoch5, step1073]: loss 0.479344
[epoch5, step1074]: loss 0.525501
[epoch5, step1075]: loss 0.639198
[epoch5, step1076]: loss 0.507888
[epoch5, step1077]: loss 0.572956
[epoch5, step1078]: loss 0.782618
[epoch5, step1079]: loss 0.557345
[epoch5, step1080]: loss 0.752077
[epoch5, step1081]: loss 0.540986
[epoch5, step1082]: loss 0.548794
[epoch5, step1083]: loss 0.801540
[epoch5, step1084]: loss 0.564885
[epoch5, step1085]: loss 0.560323
[epoch5, step1086]: loss 0.437120
[epoch5, step1087]: loss 0.561770
[epoch5, step1088]: loss 0.730719
[epoch5, step1089]: loss 0.525614
[epoch5, step1090]: loss 0.274068
[epoch5, step1091]: loss 0.318474
[epoch5, step1092]: loss 0.221484
[epoch5, step1093]: loss 0.535945
[epoch5, step1094]: loss 0.778023
[epoch5, step1095]: loss 0.741866
[epoch5, step1096]: loss 0.393895
[epoch5, step1097]: loss 0.685700
[epoch5, step1098]: loss 0.906840
[epoch5, step1099]: loss 0.701828
[epoch5, step1100]: loss 0.621960
[epoch5, step1101]: loss 0.630302
[epoch5, step1102]: loss 0.546734
[epoch5, step1103]: loss 0.601474
[epoch5, step1104]: loss 0.553643
[epoch5, step1105]: loss 0.552651
[epoch5, step1106]: loss 0.540438
[epoch5, step1107]: loss 0.873322
[epoch5, step1108]: loss 0.371579
[epoch5, step1109]: loss 0.593141
[epoch5, step1110]: loss 0.435171
[epoch5, step1111]: loss 0.461315
[epoch5, step1112]: loss 0.607110
[epoch5, step1113]: loss 0.529474
[epoch5, step1114]: loss 0.411632
[epoch5, step1115]: loss 0.615728
[epoch5, step1116]: loss 0.719065
[epoch5, step1117]: loss 0.673812
[epoch5, step1118]: loss 0.467198
[epoch5, step1119]: loss 0.737051
[epoch5, step1120]: loss 0.672766
[epoch5, step1121]: loss 0.550649
[epoch5, step1122]: loss 0.627481
[epoch5, step1123]: loss 0.483899
[epoch5, step1124]: loss 0.870333
[epoch5, step1125]: loss 0.672693
[epoch5, step1126]: loss 0.491524
[epoch5, step1127]: loss 0.575801
[epoch5, step1128]: loss 0.513003
[epoch5, step1129]: loss 0.466001
[epoch5, step1130]: loss 0.702085
[epoch5, step1131]: loss 0.502115
[epoch5, step1132]: loss 0.515064
[epoch5, step1133]: loss 0.643149
[epoch5, step1134]: loss 0.649503
[epoch5, step1135]: loss 0.718641
[epoch5, step1136]: loss 0.588405
[epoch5, step1137]: loss 0.709692
[epoch5, step1138]: loss 0.573917
[epoch5, step1139]: loss 0.663937
[epoch5, step1140]: loss 0.632431
[epoch5, step1141]: loss 0.285150
[epoch5, step1142]: loss 0.753848
[epoch5, step1143]: loss 0.445581
[epoch5, step1144]: loss 0.477444
[epoch5, step1145]: loss 0.545982
[epoch5, step1146]: loss 0.516680
[epoch5, step1147]: loss 0.585175
[epoch5, step1148]: loss 0.583410
[epoch5, step1149]: loss 0.567030
[epoch5, step1150]: loss 0.418488
[epoch5, step1151]: loss 0.678999
[epoch5, step1152]: loss 0.680821
[epoch5, step1153]: loss 0.784534
[epoch5, step1154]: loss 0.381755
[epoch5, step1155]: loss 0.396837
[epoch5, step1156]: loss 0.751182
[epoch5, step1157]: loss 0.452911
[epoch5, step1158]: loss 0.521911
[epoch5, step1159]: loss 0.432489
[epoch5, step1160]: loss 0.468776
[epoch5, step1161]: loss 0.588814
[epoch5, step1162]: loss 0.793973
[epoch5, step1163]: loss 0.414568
[epoch5, step1164]: loss 0.489494
[epoch5, step1165]: loss 0.727741
[epoch5, step1166]: loss 0.702599
[epoch5, step1167]: loss 0.545344
[epoch5, step1168]: loss 0.493330
[epoch5, step1169]: loss 0.837182
[epoch5, step1170]: loss 0.605049
[epoch5, step1171]: loss 0.485419
[epoch5, step1172]: loss 0.783347
[epoch5, step1173]: loss 0.675837
[epoch5, step1174]: loss 0.411437
[epoch5, step1175]: loss 0.759365
[epoch5, step1176]: loss 0.729613
[epoch5, step1177]: loss 0.522337
[epoch5, step1178]: loss 0.332833
[epoch5, step1179]: loss 0.508055
[epoch5, step1180]: loss 0.476260
[epoch5, step1181]: loss 0.675309
[epoch5, step1182]: loss 0.746343
[epoch5, step1183]: loss 0.281193
[epoch5, step1184]: loss 0.547350
[epoch5, step1185]: loss 0.628261
[epoch5, step1186]: loss 0.548495
[epoch5, step1187]: loss 0.516378
[epoch5, step1188]: loss 0.687233
[epoch5, step1189]: loss 0.559767
[epoch5, step1190]: loss 0.438490
[epoch5, step1191]: loss 0.733027
[epoch5, step1192]: loss 0.778502
[epoch5, step1193]: loss 0.511554
[epoch5, step1194]: loss 0.799242
[epoch5, step1195]: loss 0.567113
[epoch5, step1196]: loss 0.567061
[epoch5, step1197]: loss 0.528915
[epoch5, step1198]: loss 0.703976
[epoch5, step1199]: loss 0.255487
[epoch5, step1200]: loss 0.646806
[epoch5, step1201]: loss 0.559467
[epoch5, step1202]: loss 0.807115
[epoch5, step1203]: loss 0.641320
[epoch5, step1204]: loss 0.626352
[epoch5, step1205]: loss 0.580912
[epoch5, step1206]: loss 0.700527
[epoch5, step1207]: loss 0.630269
[epoch5, step1208]: loss 0.376998
[epoch5, step1209]: loss 0.434249
[epoch5, step1210]: loss 0.747745
[epoch5, step1211]: loss 0.561318
[epoch5, step1212]: loss 0.632105
[epoch5, step1213]: loss 0.573748
[epoch5, step1214]: loss 0.645774
[epoch5, step1215]: loss 0.623518
[epoch5, step1216]: loss 0.586223
[epoch5, step1217]: loss 0.624418
[epoch5, step1218]: loss 0.595848
[epoch5, step1219]: loss 0.415445
[epoch5, step1220]: loss 0.414449
[epoch5, step1221]: loss 0.671391
[epoch5, step1222]: loss 0.701253
[epoch5, step1223]: loss 0.559089
[epoch5, step1224]: loss 0.446694
[epoch5, step1225]: loss 0.462759
[epoch5, step1226]: loss 0.603934
[epoch5, step1227]: loss 0.480723
[epoch5, step1228]: loss 0.674962
[epoch5, step1229]: loss 0.583384
[epoch5, step1230]: loss 0.277813
[epoch5, step1231]: loss 0.689621
[epoch5, step1232]: loss 0.615597
[epoch5, step1233]: loss 0.723010
[epoch5, step1234]: loss 0.494780
[epoch5, step1235]: loss 0.752602
[epoch5, step1236]: loss 0.697018
[epoch5, step1237]: loss 0.755304
[epoch5, step1238]: loss 0.664690
[epoch5, step1239]: loss 0.509290
[epoch5, step1240]: loss 0.812145
[epoch5, step1241]: loss 0.562611
[epoch5, step1242]: loss 0.554250
[epoch5, step1243]: loss 0.619228
[epoch5, step1244]: loss 0.669105
[epoch5, step1245]: loss 0.558514
[epoch5, step1246]: loss 0.609849
[epoch5, step1247]: loss 0.612434
[epoch5, step1248]: loss 0.287183
[epoch5, step1249]: loss 0.730777
[epoch5, step1250]: loss 0.405483
[epoch5, step1251]: loss 0.625214
[epoch5, step1252]: loss 0.475323
[epoch5, step1253]: loss 0.524418
[epoch5, step1254]: loss 0.762020
[epoch5, step1255]: loss 0.476106
[epoch5, step1256]: loss 0.567159
[epoch5, step1257]: loss 0.526091
[epoch5, step1258]: loss 0.708815
[epoch5, step1259]: loss 0.722971
[epoch5, step1260]: loss 0.603061
[epoch5, step1261]: loss 0.505705
[epoch5, step1262]: loss 0.662602
[epoch5, step1263]: loss 0.511694
[epoch5, step1264]: loss 0.362631
[epoch5, step1265]: loss 0.579684
[epoch5, step1266]: loss 0.455072
[epoch5, step1267]: loss 0.555214
[epoch5, step1268]: loss 0.331570
[epoch5, step1269]: loss 0.645165
[epoch5, step1270]: loss 0.538287
[epoch5, step1271]: loss 0.639394
[epoch5, step1272]: loss 0.415301
[epoch5, step1273]: loss 0.328566
[epoch5, step1274]: loss 0.672206
[epoch5, step1275]: loss 0.609771
[epoch5, step1276]: loss 0.560467
[epoch5, step1277]: loss 0.394414
[epoch5, step1278]: loss 0.703143
[epoch5, step1279]: loss 0.429602
[epoch5, step1280]: loss 0.581854
[epoch5, step1281]: loss 0.492773
[epoch5, step1282]: loss 0.738877
[epoch5, step1283]: loss 0.706441
[epoch5, step1284]: loss 0.593783
[epoch5, step1285]: loss 0.667780
[epoch5, step1286]: loss 0.586322
[epoch5, step1287]: loss 0.342114
[epoch5, step1288]: loss 0.758220
[epoch5, step1289]: loss 0.782903
[epoch5, step1290]: loss 0.229127
[epoch5, step1291]: loss 0.603151
[epoch5, step1292]: loss 0.724550
[epoch5, step1293]: loss 0.353644
[epoch5, step1294]: loss 0.500426
[epoch5, step1295]: loss 0.756652
[epoch5, step1296]: loss 0.841069
[epoch5, step1297]: loss 0.547858
[epoch5, step1298]: loss 0.826198
[epoch5, step1299]: loss 0.409934
[epoch5, step1300]: loss 0.202605
[epoch5, step1301]: loss 0.560630
[epoch5, step1302]: loss 0.670477
[epoch5, step1303]: loss 0.601138
[epoch5, step1304]: loss 0.501068
[epoch5, step1305]: loss 0.476900
[epoch5, step1306]: loss 0.591335
[epoch5, step1307]: loss 0.519249
[epoch5, step1308]: loss 0.740601
[epoch5, step1309]: loss 0.455441
[epoch5, step1310]: loss 0.672206
[epoch5, step1311]: loss 0.597708
[epoch5, step1312]: loss 0.319849
[epoch5, step1313]: loss 0.501387
[epoch5, step1314]: loss 0.672135
[epoch5, step1315]: loss 0.676248
[epoch5, step1316]: loss 0.562963
[epoch5, step1317]: loss 0.706866
[epoch5, step1318]: loss 0.630826
[epoch5, step1319]: loss 0.673123
[epoch5, step1320]: loss 0.599519
[epoch5, step1321]: loss 0.472370
[epoch5, step1322]: loss 0.609068
[epoch5, step1323]: loss 0.385371
[epoch5, step1324]: loss 0.195269
[epoch5, step1325]: loss 0.473735
[epoch5, step1326]: loss 0.303344
[epoch5, step1327]: loss 0.527434
[epoch5, step1328]: loss 0.521306
[epoch5, step1329]: loss 0.523531
[epoch5, step1330]: loss 0.572875
[epoch5, step1331]: loss 0.585993
[epoch5, step1332]: loss 0.581121
[epoch5, step1333]: loss 0.546573
[epoch5, step1334]: loss 0.623793
[epoch5, step1335]: loss 0.351987
[epoch5, step1336]: loss 0.438645
[epoch5, step1337]: loss 0.581664
[epoch5, step1338]: loss 0.565800
[epoch5, step1339]: loss 0.688610
[epoch5, step1340]: loss 0.773444
[epoch5, step1341]: loss 0.513438
[epoch5, step1342]: loss 0.540982
[epoch5, step1343]: loss 0.395517
[epoch5, step1344]: loss 0.883699
[epoch5, step1345]: loss 0.381948
[epoch5, step1346]: loss 0.525389
[epoch5, step1347]: loss 0.577369
[epoch5, step1348]: loss 0.455253
[epoch5, step1349]: loss 0.386343
[epoch5, step1350]: loss 0.357910
[epoch5, step1351]: loss 0.481153
[epoch5, step1352]: loss 0.316237
[epoch5, step1353]: loss 0.592493
[epoch5, step1354]: loss 0.734017
[epoch5, step1355]: loss 0.448386
[epoch5, step1356]: loss 0.621327
[epoch5, step1357]: loss 0.617305
[epoch5, step1358]: loss 0.772697
[epoch5, step1359]: loss 0.504299
[epoch5, step1360]: loss 0.896399
[epoch5, step1361]: loss 0.345201
[epoch5, step1362]: loss 0.633841
[epoch5, step1363]: loss 0.585918
[epoch5, step1364]: loss 0.637356
[epoch5, step1365]: loss 0.419436
[epoch5, step1366]: loss 0.544683
[epoch5, step1367]: loss 0.743606
[epoch5, step1368]: loss 0.542946
[epoch5, step1369]: loss 0.649751
[epoch5, step1370]: loss 0.505486
[epoch5, step1371]: loss 0.723624
[epoch5, step1372]: loss 0.356148
[epoch5, step1373]: loss 0.356390
[epoch5, step1374]: loss 0.304949
[epoch5, step1375]: loss 0.498580
[epoch5, step1376]: loss 0.645840
[epoch5, step1377]: loss 0.561530
[epoch5, step1378]: loss 0.695956
[epoch5, step1379]: loss 0.366226
[epoch5, step1380]: loss 0.511183
[epoch5, step1381]: loss 0.496910
[epoch5, step1382]: loss 0.625117
[epoch5, step1383]: loss 0.439811
[epoch5, step1384]: loss 0.520424
[epoch5, step1385]: loss 0.594116
[epoch5, step1386]: loss 0.131133
[epoch5, step1387]: loss 0.695493
[epoch5, step1388]: loss 0.596630
[epoch5, step1389]: loss 0.667128
[epoch5, step1390]: loss 0.399453
[epoch5, step1391]: loss 0.390667
[epoch5, step1392]: loss 0.531253
[epoch5, step1393]: loss 0.775014
[epoch5, step1394]: loss 0.562317
[epoch5, step1395]: loss 0.503509
[epoch5, step1396]: loss 0.384048
[epoch5, step1397]: loss 0.615963
[epoch5, step1398]: loss 0.539911
[epoch5, step1399]: loss 0.674786
[epoch5, step1400]: loss 0.567041
[epoch5, step1401]: loss 0.476002
[epoch5, step1402]: loss 0.492308
[epoch5, step1403]: loss 0.628065
[epoch5, step1404]: loss 0.440736
[epoch5, step1405]: loss 0.574954
[epoch5, step1406]: loss 0.506139
[epoch5, step1407]: loss 0.506115
[epoch5, step1408]: loss 0.416493
[epoch5, step1409]: loss 0.626962
[epoch5, step1410]: loss 0.740415
[epoch5, step1411]: loss 0.563633
[epoch5, step1412]: loss 0.719757
[epoch5, step1413]: loss 0.810467
[epoch5, step1414]: loss 0.598833
[epoch5, step1415]: loss 0.551803
[epoch5, step1416]: loss 0.794224
[epoch5, step1417]: loss 0.587740
[epoch5, step1418]: loss 0.781275
[epoch5, step1419]: loss 0.497745
[epoch5, step1420]: loss 0.493445
[epoch5, step1421]: loss 0.565930
[epoch5, step1422]: loss 0.213408
[epoch5, step1423]: loss 0.450630
[epoch5, step1424]: loss 0.762175
[epoch5, step1425]: loss 0.791469
[epoch5, step1426]: loss 0.759366
[epoch5, step1427]: loss 0.408453
[epoch5, step1428]: loss 0.822643
[epoch5, step1429]: loss 0.354303
[epoch5, step1430]: loss 0.323982
[epoch5, step1431]: loss 0.347946
[epoch5, step1432]: loss 0.260910
[epoch5, step1433]: loss 0.591810
[epoch5, step1434]: loss 0.519250
[epoch5, step1435]: loss 0.470025
[epoch5, step1436]: loss 0.679212
[epoch5, step1437]: loss 0.492588
[epoch5, step1438]: loss 0.760062
[epoch5, step1439]: loss 0.493095
[epoch5, step1440]: loss 0.717747
[epoch5, step1441]: loss 0.501971
[epoch5, step1442]: loss 0.723156
[epoch5, step1443]: loss 0.305007
[epoch5, step1444]: loss 0.755591
[epoch5, step1445]: loss 0.453329
[epoch5, step1446]: loss 0.668460
[epoch5, step1447]: loss 0.678123
[epoch5, step1448]: loss 0.497074
[epoch5, step1449]: loss 0.205719
[epoch5, step1450]: loss 0.626706
[epoch5, step1451]: loss 0.468954
[epoch5, step1452]: loss 0.547863
[epoch5, step1453]: loss 0.413015
[epoch5, step1454]: loss 0.627550
[epoch5, step1455]: loss 0.655876
[epoch5, step1456]: loss 0.603195
[epoch5, step1457]: loss 0.439618
[epoch5, step1458]: loss 0.754642
[epoch5, step1459]: loss 0.560530
[epoch5, step1460]: loss 0.642779
[epoch5, step1461]: loss 0.512577
[epoch5, step1462]: loss 0.377894
[epoch5, step1463]: loss 0.675131
[epoch5, step1464]: loss 0.398531
[epoch5, step1465]: loss 0.710951
[epoch5, step1466]: loss 0.619192
[epoch5, step1467]: loss 0.727467
[epoch5, step1468]: loss 0.531006
[epoch5, step1469]: loss 0.616127
[epoch5, step1470]: loss 0.531600
[epoch5, step1471]: loss 0.685890
[epoch5, step1472]: loss 0.677085
[epoch5, step1473]: loss 0.533338
[epoch5, step1474]: loss 0.680670
[epoch5, step1475]: loss 0.727895
[epoch5, step1476]: loss 0.512656
[epoch5, step1477]: loss 0.666103
[epoch5, step1478]: loss 0.594105
[epoch5, step1479]: loss 0.778988
[epoch5, step1480]: loss 0.543293
[epoch5, step1481]: loss 0.455245
[epoch5, step1482]: loss 0.467299
[epoch5, step1483]: loss 0.465250
[epoch5, step1484]: loss 0.637772
[epoch5, step1485]: loss 0.733068
[epoch5, step1486]: loss 0.618560
[epoch5, step1487]: loss 0.792207
[epoch5, step1488]: loss 0.547760
[epoch5, step1489]: loss 0.668567
[epoch5, step1490]: loss 0.550929
[epoch5, step1491]: loss 0.586800
[epoch5, step1492]: loss 0.742309
[epoch5, step1493]: loss 0.541358
[epoch5, step1494]: loss 0.609151
[epoch5, step1495]: loss 0.742574
[epoch5, step1496]: loss 0.647586
[epoch5, step1497]: loss 0.413493
[epoch5, step1498]: loss 0.433966
[epoch5, step1499]: loss 0.246313
[epoch5, step1500]: loss 0.420158
[epoch5, step1501]: loss 0.610341
[epoch5, step1502]: loss 0.375132
[epoch5, step1503]: loss 0.914161
[epoch5, step1504]: loss 0.615989
[epoch5, step1505]: loss 0.470268
[epoch5, step1506]: loss 0.341854
[epoch5, step1507]: loss 0.410115
[epoch5, step1508]: loss 0.787143
[epoch5, step1509]: loss 0.525111
[epoch5, step1510]: loss 0.684816
[epoch5, step1511]: loss 0.461264
[epoch5, step1512]: loss 0.625732
[epoch5, step1513]: loss 0.508256
[epoch5, step1514]: loss 0.536048
[epoch5, step1515]: loss 0.548340
[epoch5, step1516]: loss 0.629368
[epoch5, step1517]: loss 0.277828
[epoch5, step1518]: loss 0.480759
[epoch5, step1519]: loss 0.417844
[epoch5, step1520]: loss 0.617714
[epoch5, step1521]: loss 0.579661
[epoch5, step1522]: loss 0.483176
[epoch5, step1523]: loss 0.493812
[epoch5, step1524]: loss 0.562366
[epoch5, step1525]: loss 0.499002
[epoch5, step1526]: loss 0.401746
[epoch5, step1527]: loss 0.577603
[epoch5, step1528]: loss 0.305666
[epoch5, step1529]: loss 0.596658
[epoch5, step1530]: loss 0.716495
[epoch5, step1531]: loss 0.542078
[epoch5, step1532]: loss 0.516791
[epoch5, step1533]: loss 0.717754
[epoch5, step1534]: loss 0.745035
[epoch5, step1535]: loss 0.646735
[epoch5, step1536]: loss 0.674931
[epoch5, step1537]: loss 0.614394
[epoch5, step1538]: loss 0.562151
[epoch5, step1539]: loss 0.583053
[epoch5, step1540]: loss 0.772415
[epoch5, step1541]: loss 0.886875
[epoch5, step1542]: loss 0.863857
[epoch5, step1543]: loss 0.596567
[epoch5, step1544]: loss 0.807540
[epoch5, step1545]: loss 0.456569
[epoch5, step1546]: loss 0.667246
[epoch5, step1547]: loss 0.718718
[epoch5, step1548]: loss 0.690777
[epoch5, step1549]: loss 0.693341
[epoch5, step1550]: loss 0.467324
[epoch5, step1551]: loss 0.485727
[epoch5, step1552]: loss 0.296736
[epoch5, step1553]: loss 0.479441
[epoch5, step1554]: loss 0.586613
[epoch5, step1555]: loss 0.638564
[epoch5, step1556]: loss 0.491652
[epoch5, step1557]: loss 0.799360
[epoch5, step1558]: loss 0.591007
[epoch5, step1559]: loss 0.260421
[epoch5, step1560]: loss 0.549409
[epoch5, step1561]: loss 0.656139
[epoch5, step1562]: loss 0.755524
[epoch5, step1563]: loss 0.514030
[epoch5, step1564]: loss 0.726119
[epoch5, step1565]: loss 0.683143
[epoch5, step1566]: loss 0.653701
[epoch5, step1567]: loss 0.494688
[epoch5, step1568]: loss 0.628229
[epoch5, step1569]: loss 0.371762
[epoch5, step1570]: loss 0.436541
[epoch5, step1571]: loss 0.498837
[epoch5, step1572]: loss 0.590703
[epoch5, step1573]: loss 0.830366
[epoch5, step1574]: loss 0.668613
[epoch5, step1575]: loss 0.567502
[epoch5, step1576]: loss 0.625216
[epoch5, step1577]: loss 0.512660
[epoch5, step1578]: loss 0.453090
[epoch5, step1579]: loss 0.532175
[epoch5, step1580]: loss 0.504757
[epoch5, step1581]: loss 0.566747
[epoch5, step1582]: loss 0.647917
[epoch5, step1583]: loss 0.582849
[epoch5, step1584]: loss 0.440773
[epoch5, step1585]: loss 0.741168
[epoch5, step1586]: loss 0.662198
[epoch5, step1587]: loss 0.691771
[epoch5, step1588]: loss 0.679137
[epoch5, step1589]: loss 0.495986
[epoch5, step1590]: loss 0.481392
[epoch5, step1591]: loss 0.395006
[epoch5, step1592]: loss 0.594723
[epoch5, step1593]: loss 0.687980
[epoch5, step1594]: loss 0.469551
[epoch5, step1595]: loss 0.644023
[epoch5, step1596]: loss 0.672948
[epoch5, step1597]: loss 0.664621
[epoch5, step1598]: loss 0.854318
[epoch5, step1599]: loss 0.596450
[epoch5, step1600]: loss 0.638728
[epoch5, step1601]: loss 0.654253
[epoch5, step1602]: loss 0.649841
[epoch5, step1603]: loss 0.689261
[epoch5, step1604]: loss 0.609459
[epoch5, step1605]: loss 0.688419
[epoch5, step1606]: loss 0.475283
[epoch5, step1607]: loss 0.625960
[epoch5, step1608]: loss 0.595208
[epoch5, step1609]: loss 0.382210
[epoch5, step1610]: loss 0.662950
[epoch5, step1611]: loss 0.451371
[epoch5, step1612]: loss 0.606923
[epoch5, step1613]: loss 0.573121
[epoch5, step1614]: loss 0.391924
[epoch5, step1615]: loss 0.650372
[epoch5, step1616]: loss 0.422436
[epoch5, step1617]: loss 0.416971
[epoch5, step1618]: loss 0.591596
[epoch5, step1619]: loss 0.670194
[epoch5, step1620]: loss 0.392682
[epoch5, step1621]: loss 0.586756
[epoch5, step1622]: loss 0.655776
[epoch5, step1623]: loss 0.488754
[epoch5, step1624]: loss 0.357592
[epoch5, step1625]: loss 0.647390
[epoch5, step1626]: loss 0.663104
[epoch5, step1627]: loss 0.498858
[epoch5, step1628]: loss 0.558711
[epoch5, step1629]: loss 0.266800
[epoch5, step1630]: loss 0.438989
[epoch5, step1631]: loss 0.575820
[epoch5, step1632]: loss 0.488029
[epoch5, step1633]: loss 0.648537
[epoch5, step1634]: loss 0.575390
[epoch5, step1635]: loss 0.450033
[epoch5, step1636]: loss 0.627790
[epoch5, step1637]: loss 0.690284
[epoch5, step1638]: loss 0.469177
[epoch5, step1639]: loss 0.793678
[epoch5, step1640]: loss 0.489157
[epoch5, step1641]: loss 0.424990
[epoch5, step1642]: loss 0.751188
[epoch5, step1643]: loss 0.381696
[epoch5, step1644]: loss 0.511703
[epoch5, step1645]: loss 0.467739
[epoch5, step1646]: loss 0.609664
[epoch5, step1647]: loss 0.504649
[epoch5, step1648]: loss 0.637953
[epoch5, step1649]: loss 0.705882
[epoch5, step1650]: loss 0.597745
[epoch5, step1651]: loss 0.638135
[epoch5, step1652]: loss 0.616827
[epoch5, step1653]: loss 0.594373
[epoch5, step1654]: loss 0.857190
[epoch5, step1655]: loss 0.538605
[epoch5, step1656]: loss 0.475354
[epoch5, step1657]: loss 0.521238
[epoch5, step1658]: loss 0.414193
[epoch5, step1659]: loss 0.782892
[epoch5, step1660]: loss 0.655566
[epoch5, step1661]: loss 0.541068
[epoch5, step1662]: loss 0.637872
[epoch5, step1663]: loss 0.605046
[epoch5, step1664]: loss 0.524050
[epoch5, step1665]: loss 0.695480
[epoch5, step1666]: loss 0.522085
[epoch5, step1667]: loss 0.776665
[epoch5, step1668]: loss 0.558226
[epoch5, step1669]: loss 0.454722
[epoch5, step1670]: loss 0.704283
[epoch5, step1671]: loss 0.367125
[epoch5, step1672]: loss 0.583858
[epoch5, step1673]: loss 0.720348
[epoch5, step1674]: loss 0.673990
[epoch5, step1675]: loss 0.505385
[epoch5, step1676]: loss 0.583120
[epoch5, step1677]: loss 0.449206
[epoch5, step1678]: loss 0.416189
[epoch5, step1679]: loss 0.672901
[epoch5, step1680]: loss 0.663889
[epoch5, step1681]: loss 0.730573
[epoch5, step1682]: loss 0.833984
[epoch5, step1683]: loss 0.152433
[epoch5, step1684]: loss 0.571303
[epoch5, step1685]: loss 0.681014
[epoch5, step1686]: loss 0.578107
[epoch5, step1687]: loss 0.359745
[epoch5, step1688]: loss 0.532672
[epoch5, step1689]: loss 0.807785
[epoch5, step1690]: loss 0.634123
[epoch5, step1691]: loss 0.463903
[epoch5, step1692]: loss 0.687896
[epoch5, step1693]: loss 0.679172
[epoch5, step1694]: loss 0.620696
[epoch5, step1695]: loss 0.561791
[epoch5, step1696]: loss 0.632902
[epoch5, step1697]: loss 0.565371
[epoch5, step1698]: loss 0.556747
[epoch5, step1699]: loss 0.553723
[epoch5, step1700]: loss 0.482712
[epoch5, step1701]: loss 0.649432
[epoch5, step1702]: loss 0.389007
[epoch5, step1703]: loss 0.573567
[epoch5, step1704]: loss 0.307829
[epoch5, step1705]: loss 0.671159
[epoch5, step1706]: loss 0.324925
[epoch5, step1707]: loss 0.558287
[epoch5, step1708]: loss 0.483618
[epoch5, step1709]: loss 0.486388
[epoch5, step1710]: loss 0.765538
[epoch5, step1711]: loss 0.665540
[epoch5, step1712]: loss 0.412262
[epoch5, step1713]: loss 0.457752
[epoch5, step1714]: loss 0.170116
[epoch5, step1715]: loss 0.626089
[epoch5, step1716]: loss 0.373357
[epoch5, step1717]: loss 0.826815
[epoch5, step1718]: loss 0.651002
[epoch5, step1719]: loss 0.515164
[epoch5, step1720]: loss 0.394761
[epoch5, step1721]: loss 0.519328
[epoch5, step1722]: loss 0.596473
[epoch5, step1723]: loss 0.717077
[epoch5, step1724]: loss 0.504808
[epoch5, step1725]: loss 0.652825
[epoch5, step1726]: loss 0.186076
[epoch5, step1727]: loss 0.601180
[epoch5, step1728]: loss 0.481425
[epoch5, step1729]: loss 0.810296
[epoch5, step1730]: loss 0.705623
[epoch5, step1731]: loss 0.653755
[epoch5, step1732]: loss 0.646938
[epoch5, step1733]: loss 0.514242
[epoch5, step1734]: loss 0.417907
[epoch5, step1735]: loss 0.751277
[epoch5, step1736]: loss 0.569596
[epoch5, step1737]: loss 0.619941
[epoch5, step1738]: loss 0.210885
[epoch5, step1739]: loss 0.715322
[epoch5, step1740]: loss 0.633603
[epoch5, step1741]: loss 0.767847
[epoch5, step1742]: loss 0.572347
[epoch5, step1743]: loss 0.372683
[epoch5, step1744]: loss 0.484478
[epoch5, step1745]: loss 0.392102
[epoch5, step1746]: loss 0.295692
[epoch5, step1747]: loss 0.681657
[epoch5, step1748]: loss 0.689604
[epoch5, step1749]: loss 0.484232
[epoch5, step1750]: loss 0.448668
[epoch5, step1751]: loss 0.542542
[epoch5, step1752]: loss 0.451059
[epoch5, step1753]: loss 0.451144
[epoch5, step1754]: loss 0.456434
[epoch5, step1755]: loss 0.508748
[epoch5, step1756]: loss 0.678191
[epoch5, step1757]: loss 0.564152
[epoch5, step1758]: loss 0.655185
[epoch5, step1759]: loss 0.414450
[epoch5, step1760]: loss 0.417221
[epoch5, step1761]: loss 0.538693
[epoch5, step1762]: loss 0.826090
[epoch5, step1763]: loss 0.515355
[epoch5, step1764]: loss 0.588325
[epoch5, step1765]: loss 0.554484
[epoch5, step1766]: loss 0.589317
[epoch5, step1767]: loss 0.241799
[epoch5, step1768]: loss 0.264563
[epoch5, step1769]: loss 0.706136
[epoch5, step1770]: loss 0.650838
[epoch5, step1771]: loss 0.393600
[epoch5, step1772]: loss 0.505345
[epoch5, step1773]: loss 0.754636
[epoch5, step1774]: loss 0.798717
[epoch5, step1775]: loss 0.454927
[epoch5, step1776]: loss 0.426069
[epoch5, step1777]: loss 0.553773
[epoch5, step1778]: loss 0.619121
[epoch5, step1779]: loss 0.611825
[epoch5, step1780]: loss 0.558142
[epoch5, step1781]: loss 0.545054
[epoch5, step1782]: loss 0.473682
[epoch5, step1783]: loss 0.344500
[epoch5, step1784]: loss 0.730926
[epoch5, step1785]: loss 0.568402
[epoch5, step1786]: loss 0.786861
[epoch5, step1787]: loss 0.866802
[epoch5, step1788]: loss 0.193391
[epoch5, step1789]: loss 0.472735
[epoch5, step1790]: loss 0.478211
[epoch5, step1791]: loss 0.567197
[epoch5, step1792]: loss 0.348701
[epoch5, step1793]: loss 0.710235
[epoch5, step1794]: loss 0.327947
[epoch5, step1795]: loss 0.418460
[epoch5, step1796]: loss 0.817468
[epoch5, step1797]: loss 0.709792
[epoch5, step1798]: loss 0.934789
[epoch5, step1799]: loss 0.592047
[epoch5, step1800]: loss 0.607278
[epoch5, step1801]: loss 0.266770
[epoch5, step1802]: loss 0.580878
[epoch5, step1803]: loss 0.473541
[epoch5, step1804]: loss 0.623921
[epoch5, step1805]: loss 0.448356
[epoch5, step1806]: loss 0.509423
[epoch5, step1807]: loss 0.639799
[epoch5, step1808]: loss 0.406786
[epoch5, step1809]: loss 0.510535
[epoch5, step1810]: loss 0.474146
[epoch5, step1811]: loss 0.721300
[epoch5, step1812]: loss 0.544462
[epoch5, step1813]: loss 0.569596
[epoch5, step1814]: loss 0.593596
[epoch5, step1815]: loss 0.630438
[epoch5, step1816]: loss 0.550198
[epoch5, step1817]: loss 0.687196
[epoch5, step1818]: loss 0.595730
[epoch5, step1819]: loss 0.473327
[epoch5, step1820]: loss 0.833480
[epoch5, step1821]: loss 0.302904
[epoch5, step1822]: loss 0.494564
[epoch5, step1823]: loss 0.480591
[epoch5, step1824]: loss 0.691956
[epoch5, step1825]: loss 0.569543
[epoch5, step1826]: loss 0.661945
[epoch5, step1827]: loss 0.583682
[epoch5, step1828]: loss 0.549278
[epoch5, step1829]: loss 0.580934
[epoch5, step1830]: loss 0.650585
[epoch5, step1831]: loss 0.405577
[epoch5, step1832]: loss 0.578136
[epoch5, step1833]: loss 0.659454
[epoch5, step1834]: loss 0.705805
[epoch5, step1835]: loss 0.448683
[epoch5, step1836]: loss 0.570539
[epoch5, step1837]: loss 0.552342
[epoch5, step1838]: loss 0.595573
[epoch5, step1839]: loss 0.519814
[epoch5, step1840]: loss 0.507645
[epoch5, step1841]: loss 0.737344
[epoch5, step1842]: loss 0.384134
[epoch5, step1843]: loss 0.655771
[epoch5, step1844]: loss 0.582680
[epoch5, step1845]: loss 0.715588
[epoch5, step1846]: loss 0.726606
[epoch5, step1847]: loss 0.364822
[epoch5, step1848]: loss 0.678294
[epoch5, step1849]: loss 0.480964
[epoch5, step1850]: loss 0.776850
[epoch5, step1851]: loss 0.659326
[epoch5, step1852]: loss 0.670153
[epoch5, step1853]: loss 0.581498
[epoch5, step1854]: loss 0.770456
[epoch5, step1855]: loss 0.767822
[epoch5, step1856]: loss 0.635945
[epoch5, step1857]: loss 0.473654
[epoch5, step1858]: loss 0.872308
[epoch5, step1859]: loss 0.545986
[epoch5, step1860]: loss 0.668666
[epoch5, step1861]: loss 0.307287
[epoch5, step1862]: loss 0.720599
[epoch5, step1863]: loss 0.637359
[epoch5, step1864]: loss 0.513448
[epoch5, step1865]: loss 0.609577
[epoch5, step1866]: loss 0.754893
[epoch5, step1867]: loss 0.816294
[epoch5, step1868]: loss 0.590485
[epoch5, step1869]: loss 0.690705
[epoch5, step1870]: loss 0.881859
[epoch5, step1871]: loss 0.559161
[epoch5, step1872]: loss 0.557803
[epoch5, step1873]: loss 0.574756
[epoch5, step1874]: loss 0.748643
[epoch5, step1875]: loss 0.345948
[epoch5, step1876]: loss 0.544693
[epoch5, step1877]: loss 0.693365
[epoch5, step1878]: loss 0.443273
[epoch5, step1879]: loss 0.644019
[epoch5, step1880]: loss 0.660380
[epoch5, step1881]: loss 0.456686
[epoch5, step1882]: loss 0.657320
[epoch5, step1883]: loss 0.313942
[epoch5, step1884]: loss 0.342427
[epoch5, step1885]: loss 0.517653
[epoch5, step1886]: loss 0.721559
[epoch5, step1887]: loss 0.649192
[epoch5, step1888]: loss 0.766687
[epoch5, step1889]: loss 0.696865
[epoch5, step1890]: loss 0.440660
[epoch5, step1891]: loss 0.267084
[epoch5, step1892]: loss 0.484282
[epoch5, step1893]: loss 0.709508
[epoch5, step1894]: loss 0.571022
[epoch5, step1895]: loss 0.626043
[epoch5, step1896]: loss 0.607435
[epoch5, step1897]: loss 0.498751
[epoch5, step1898]: loss 0.840013
[epoch5, step1899]: loss 0.612862
[epoch5, step1900]: loss 0.347523
[epoch5, step1901]: loss 0.452258
[epoch5, step1902]: loss 0.558988
[epoch5, step1903]: loss 0.531447
[epoch5, step1904]: loss 0.534608
[epoch5, step1905]: loss 0.640732
[epoch5, step1906]: loss 0.594717
[epoch5, step1907]: loss 0.509896
[epoch5, step1908]: loss 0.504645
[epoch5, step1909]: loss 0.601892
[epoch5, step1910]: loss 0.532315
[epoch5, step1911]: loss 0.862449
[epoch5, step1912]: loss 0.444395
[epoch5, step1913]: loss 0.270556
[epoch5, step1914]: loss 0.301303
[epoch5, step1915]: loss 0.298719
[epoch5, step1916]: loss 0.825662
[epoch5, step1917]: loss 0.575205
[epoch5, step1918]: loss 0.486428
[epoch5, step1919]: loss 0.545467
[epoch5, step1920]: loss 0.661543
[epoch5, step1921]: loss 0.512788
[epoch5, step1922]: loss 0.597729
[epoch5, step1923]: loss 0.764730
[epoch5, step1924]: loss 0.647895
[epoch5, step1925]: loss 0.474708
[epoch5, step1926]: loss 0.500538
[epoch5, step1927]: loss 0.778583
[epoch5, step1928]: loss 0.619110
[epoch5, step1929]: loss 0.833926
[epoch5, step1930]: loss 0.792689
[epoch5, step1931]: loss 0.502907
[epoch5, step1932]: loss 0.738976
[epoch5, step1933]: loss 0.594155
[epoch5, step1934]: loss 0.594791
[epoch5, step1935]: loss 0.702220
[epoch5, step1936]: loss 0.417924
[epoch5, step1937]: loss 0.696503
[epoch5, step1938]: loss 0.640675
[epoch5, step1939]: loss 0.517820
[epoch5, step1940]: loss 0.452710
[epoch5, step1941]: loss 0.644703
[epoch5, step1942]: loss 0.393828
[epoch5, step1943]: loss 0.449329
[epoch5, step1944]: loss 0.395098
[epoch5, step1945]: loss 0.376544
[epoch5, step1946]: loss 0.711371
[epoch5, step1947]: loss 0.244469
[epoch5, step1948]: loss 0.321309
[epoch5, step1949]: loss 0.619542
[epoch5, step1950]: loss 0.596671
[epoch5, step1951]: loss 0.656521
[epoch5, step1952]: loss 0.713063
[epoch5, step1953]: loss 0.341552
[epoch5, step1954]: loss 0.723398
[epoch5, step1955]: loss 0.766705
[epoch5, step1956]: loss 0.597408
[epoch5, step1957]: loss 0.539337
[epoch5, step1958]: loss 0.867179
[epoch5, step1959]: loss 0.857586
[epoch5, step1960]: loss 0.740631
[epoch5, step1961]: loss 0.701089
[epoch5, step1962]: loss 0.464180
[epoch5, step1963]: loss 0.745123
[epoch5, step1964]: loss 0.765961
[epoch5, step1965]: loss 0.288767
[epoch5, step1966]: loss 0.572721
[epoch5, step1967]: loss 0.701043
[epoch5, step1968]: loss 0.490617
[epoch5, step1969]: loss 0.519655
[epoch5, step1970]: loss 0.598999
[epoch5, step1971]: loss 0.331586
[epoch5, step1972]: loss 0.652564
[epoch5, step1973]: loss 0.392686
[epoch5, step1974]: loss 0.726092
[epoch5, step1975]: loss 0.524323
[epoch5, step1976]: loss 0.630178
[epoch5, step1977]: loss 0.512018
[epoch5, step1978]: loss 0.658187
[epoch5, step1979]: loss 0.657440
[epoch5, step1980]: loss 0.339414
[epoch5, step1981]: loss 0.634275
[epoch5, step1982]: loss 0.536193
[epoch5, step1983]: loss 0.799794
[epoch5, step1984]: loss 0.396360
[epoch5, step1985]: loss 0.722469
[epoch5, step1986]: loss 0.519424
[epoch5, step1987]: loss 0.780757
[epoch5, step1988]: loss 0.701968
[epoch5, step1989]: loss 0.299346
[epoch5, step1990]: loss 0.343886
[epoch5, step1991]: loss 0.514910
[epoch5, step1992]: loss 0.501902
[epoch5, step1993]: loss 0.728148
[epoch5, step1994]: loss 0.826773
[epoch5, step1995]: loss 0.427115
[epoch5, step1996]: loss 0.629117
[epoch5, step1997]: loss 0.646646
[epoch5, step1998]: loss 0.728583
[epoch5, step1999]: loss 0.537272
[epoch5, step2000]: loss 0.554951
[epoch5, step2001]: loss 0.762055
[epoch5, step2002]: loss 0.450238
[epoch5, step2003]: loss 0.541240
[epoch5, step2004]: loss 0.450395
[epoch5, step2005]: loss 0.291464
[epoch5, step2006]: loss 0.568546
[epoch5, step2007]: loss 0.370179
[epoch5, step2008]: loss 0.438410
[epoch5, step2009]: loss 0.616355
[epoch5, step2010]: loss 0.533956
[epoch5, step2011]: loss 0.370028
[epoch5, step2012]: loss 0.526253
[epoch5, step2013]: loss 0.449182
[epoch5, step2014]: loss 0.652974
[epoch5, step2015]: loss 0.445974
[epoch5, step2016]: loss 0.360970
[epoch5, step2017]: loss 0.793227
[epoch5, step2018]: loss 0.825931
[epoch5, step2019]: loss 0.611033
[epoch5, step2020]: loss 0.700043
[epoch5, step2021]: loss 0.548420
[epoch5, step2022]: loss 0.526411
[epoch5, step2023]: loss 0.313894
[epoch5, step2024]: loss 0.678605
[epoch5, step2025]: loss 0.718622
[epoch5, step2026]: loss 0.550748
[epoch5, step2027]: loss 0.379086
[epoch5, step2028]: loss 0.350251
[epoch5, step2029]: loss 0.761708
[epoch5, step2030]: loss 0.747689
[epoch5, step2031]: loss 0.603932
[epoch5, step2032]: loss 0.491554
[epoch5, step2033]: loss 0.227344
[epoch5, step2034]: loss 0.676903
[epoch5, step2035]: loss 0.198299
[epoch5, step2036]: loss 0.750213
[epoch5, step2037]: loss 0.532090
[epoch5, step2038]: loss 0.550907
[epoch5, step2039]: loss 0.391720
[epoch5, step2040]: loss 0.524977
[epoch5, step2041]: loss 0.633759
[epoch5, step2042]: loss 0.671520
[epoch5, step2043]: loss 0.504330
[epoch5, step2044]: loss 0.595796
[epoch5, step2045]: loss 0.629605
[epoch5, step2046]: loss 0.704497
[epoch5, step2047]: loss 0.623631
[epoch5, step2048]: loss 0.543655
[epoch5, step2049]: loss 0.643860
[epoch5, step2050]: loss 0.594474
[epoch5, step2051]: loss 0.576049
[epoch5, step2052]: loss 0.484037
[epoch5, step2053]: loss 0.524122
[epoch5, step2054]: loss 0.848961
[epoch5, step2055]: loss 0.541125
[epoch5, step2056]: loss 0.418202
[epoch5, step2057]: loss 0.611616
[epoch5, step2058]: loss 0.555410
[epoch5, step2059]: loss 0.458733
[epoch5, step2060]: loss 0.721100
[epoch5, step2061]: loss 0.667305
[epoch5, step2062]: loss 0.467075
[epoch5, step2063]: loss 0.333882
[epoch5, step2064]: loss 0.489841
[epoch5, step2065]: loss 0.191552
[epoch5, step2066]: loss 0.476143
[epoch5, step2067]: loss 0.613908
[epoch5, step2068]: loss 0.734496
[epoch5, step2069]: loss 0.419957
[epoch5, step2070]: loss 0.563921
[epoch5, step2071]: loss 0.411241
[epoch5, step2072]: loss 0.653868
[epoch5, step2073]: loss 0.616811
[epoch5, step2074]: loss 0.577486
[epoch5, step2075]: loss 0.411738
[epoch5, step2076]: loss 0.388682
[epoch5, step2077]: loss 0.543906
[epoch5, step2078]: loss 0.525379
[epoch5, step2079]: loss 0.753249
[epoch5, step2080]: loss 0.597366
[epoch5, step2081]: loss 0.601232
[epoch5, step2082]: loss 0.343182
[epoch5, step2083]: loss 0.458401
[epoch5, step2084]: loss 0.674210
[epoch5, step2085]: loss 0.607586
[epoch5, step2086]: loss 0.603560
[epoch5, step2087]: loss 0.803596
[epoch5, step2088]: loss 0.407426
[epoch5, step2089]: loss 0.445779
[epoch5, step2090]: loss 0.421220
[epoch5, step2091]: loss 0.639382
[epoch5, step2092]: loss 0.747535
[epoch5, step2093]: loss 0.640562
[epoch5, step2094]: loss 0.499785
[epoch5, step2095]: loss 0.579668
[epoch5, step2096]: loss 0.677239
[epoch5, step2097]: loss 0.495578
[epoch5, step2098]: loss 0.564149
[epoch5, step2099]: loss 0.740237
[epoch5, step2100]: loss 0.658702
[epoch5, step2101]: loss 0.633366
[epoch5, step2102]: loss 0.619408
[epoch5, step2103]: loss 0.672549
[epoch5, step2104]: loss 0.461857
[epoch5, step2105]: loss 0.398714
[epoch5, step2106]: loss 0.486850
[epoch5, step2107]: loss 0.687194
[epoch5, step2108]: loss 0.397039
[epoch5, step2109]: loss 0.488800
[epoch5, step2110]: loss 0.402451
[epoch5, step2111]: loss 0.529681
[epoch5, step2112]: loss 0.587519
[epoch5, step2113]: loss 0.674502
[epoch5, step2114]: loss 0.613271
[epoch5, step2115]: loss 0.540949
[epoch5, step2116]: loss 0.410331
[epoch5, step2117]: loss 0.434334
[epoch5, step2118]: loss 0.596195
[epoch5, step2119]: loss 0.799802
[epoch5, step2120]: loss 0.672364
[epoch5, step2121]: loss 0.543714
[epoch5, step2122]: loss 0.493678
[epoch5, step2123]: loss 0.775845
[epoch5, step2124]: loss 0.574558
[epoch5, step2125]: loss 0.568135
[epoch5, step2126]: loss 0.881547
[epoch5, step2127]: loss 0.644543
[epoch5, step2128]: loss 0.681878
[epoch5, step2129]: loss 0.602902
[epoch5, step2130]: loss 0.654104
[epoch5, step2131]: loss 0.599331
[epoch5, step2132]: loss 0.605987
[epoch5, step2133]: loss 0.740453
[epoch5, step2134]: loss 0.556972
[epoch5, step2135]: loss 0.867791
[epoch5, step2136]: loss 0.372444
[epoch5, step2137]: loss 0.663990
[epoch5, step2138]: loss 0.678859
[epoch5, step2139]: loss 0.572968
[epoch5, step2140]: loss 0.645197
[epoch5, step2141]: loss 0.415965
[epoch5, step2142]: loss 0.474542
[epoch5, step2143]: loss 0.622687
[epoch5, step2144]: loss 0.800288
[epoch5, step2145]: loss 0.749113
[epoch5, step2146]: loss 0.730341
[epoch5, step2147]: loss 0.621166
[epoch5, step2148]: loss 0.646401
[epoch5, step2149]: loss 0.611643
[epoch5, step2150]: loss 0.627590
[epoch5, step2151]: loss 0.618912
[epoch5, step2152]: loss 0.580601
[epoch5, step2153]: loss 0.526151
[epoch5, step2154]: loss 0.586298
[epoch5, step2155]: loss 0.454140
[epoch5, step2156]: loss 0.571302
[epoch5, step2157]: loss 0.402888
[epoch5, step2158]: loss 0.591753
[epoch5, step2159]: loss 0.663543
[epoch5, step2160]: loss 0.621958
[epoch5, step2161]: loss 0.495185
[epoch5, step2162]: loss 0.584901
[epoch5, step2163]: loss 0.592077
[epoch5, step2164]: loss 0.414905
[epoch5, step2165]: loss 0.638019
[epoch5, step2166]: loss 0.323082
[epoch5, step2167]: loss 0.589123
[epoch5, step2168]: loss 0.564351
[epoch5, step2169]: loss 0.511960
[epoch5, step2170]: loss 0.644468
[epoch5, step2171]: loss 0.727085
[epoch5, step2172]: loss 0.669295
[epoch5, step2173]: loss 0.587094
[epoch5, step2174]: loss 0.505474
[epoch5, step2175]: loss 0.562234
[epoch5, step2176]: loss 0.645484
[epoch5, step2177]: loss 0.277184
[epoch5, step2178]: loss 0.509820
[epoch5, step2179]: loss 0.314200
[epoch5, step2180]: loss 0.753129
[epoch5, step2181]: loss 0.500742
[epoch5, step2182]: loss 0.562323
[epoch5, step2183]: loss 0.606488
[epoch5, step2184]: loss 0.594252
[epoch5, step2185]: loss 0.488685
[epoch5, step2186]: loss 0.621378
[epoch5, step2187]: loss 0.690069
[epoch5, step2188]: loss 0.650558
[epoch5, step2189]: loss 0.499934
[epoch5, step2190]: loss 0.491178
[epoch5, step2191]: loss 0.527736
[epoch5, step2192]: loss 0.626195
[epoch5, step2193]: loss 0.341709
[epoch5, step2194]: loss 0.681882
[epoch5, step2195]: loss 0.476019
[epoch5, step2196]: loss 0.569193
[epoch5, step2197]: loss 0.320944
[epoch5, step2198]: loss 0.689126
[epoch5, step2199]: loss 0.851462
[epoch5, step2200]: loss 0.502658
[epoch5, step2201]: loss 0.593851
[epoch5, step2202]: loss 0.704361
[epoch5, step2203]: loss 0.745953
[epoch5, step2204]: loss 0.586349
[epoch5, step2205]: loss 0.708454
[epoch5, step2206]: loss 0.670677
[epoch5, step2207]: loss 0.342835
[epoch5, step2208]: loss 0.138717
[epoch5, step2209]: loss 0.536854
[epoch5, step2210]: loss 0.645452
[epoch5, step2211]: loss 0.756200
[epoch5, step2212]: loss 0.590430
[epoch5, step2213]: loss 0.556348
[epoch5, step2214]: loss 0.442067
[epoch5, step2215]: loss 0.539317
[epoch5, step2216]: loss 0.530789
[epoch5, step2217]: loss 0.454431
[epoch5, step2218]: loss 0.568490
[epoch5, step2219]: loss 0.370138
[epoch5, step2220]: loss 0.840612
[epoch5, step2221]: loss 0.712579
[epoch5, step2222]: loss 0.734876
[epoch5, step2223]: loss 0.657088
[epoch5, step2224]: loss 0.478171
[epoch5, step2225]: loss 0.561175
[epoch5, step2226]: loss 0.248450
[epoch5, step2227]: loss 0.807919
[epoch5, step2228]: loss 0.565807
[epoch5, step2229]: loss 0.724759
[epoch5, step2230]: loss 0.520039
[epoch5, step2231]: loss 0.586957
[epoch5, step2232]: loss 0.683142
[epoch5, step2233]: loss 0.572115
[epoch5, step2234]: loss 0.660160
[epoch5, step2235]: loss 0.774954
[epoch5, step2236]: loss 0.377207
[epoch5, step2237]: loss 0.712882
[epoch5, step2238]: loss 0.558879
[epoch5, step2239]: loss 0.697241
[epoch5, step2240]: loss 0.631504
[epoch5, step2241]: loss 0.657001
[epoch5, step2242]: loss 0.676069
[epoch5, step2243]: loss 0.346651
[epoch5, step2244]: loss 0.387523
[epoch5, step2245]: loss 0.828622
[epoch5, step2246]: loss 0.625814
[epoch5, step2247]: loss 0.597644
[epoch5, step2248]: loss 0.393355
[epoch5, step2249]: loss 0.770901
[epoch5, step2250]: loss 0.413494
[epoch5, step2251]: loss 0.595093
[epoch5, step2252]: loss 0.644542
[epoch5, step2253]: loss 0.518539
[epoch5, step2254]: loss 0.415953
[epoch5, step2255]: loss 0.566472
[epoch5, step2256]: loss 0.632964
[epoch5, step2257]: loss 0.582803
[epoch5, step2258]: loss 0.646397
[epoch5, step2259]: loss 0.782206
[epoch5, step2260]: loss 0.605072
[epoch5, step2261]: loss 0.772862
[epoch5, step2262]: loss 0.647950
[epoch5, step2263]: loss 0.776952
[epoch5, step2264]: loss 0.472100
[epoch5, step2265]: loss 0.483742
[epoch5, step2266]: loss 0.526219
[epoch5, step2267]: loss 0.625736
[epoch5, step2268]: loss 0.684637
[epoch5, step2269]: loss 0.561165
[epoch5, step2270]: loss 0.559285
[epoch5, step2271]: loss 0.426035
[epoch5, step2272]: loss 0.672846
[epoch5, step2273]: loss 0.733034
[epoch5, step2274]: loss 0.706376
[epoch5, step2275]: loss 0.575160
[epoch5, step2276]: loss 0.676389
[epoch5, step2277]: loss 0.476777
[epoch5, step2278]: loss 0.703367
[epoch5, step2279]: loss 0.606048
[epoch5, step2280]: loss 0.673530
[epoch5, step2281]: loss 0.605453
[epoch5, step2282]: loss 0.509801
[epoch5, step2283]: loss 0.482291
[epoch5, step2284]: loss 0.743735
[epoch5, step2285]: loss 0.542267
[epoch5, step2286]: loss 0.268943
[epoch5, step2287]: loss 0.468257
[epoch5, step2288]: loss 0.457423
[epoch5, step2289]: loss 0.531448
[epoch5, step2290]: loss 0.617094
[epoch5, step2291]: loss 0.656422
[epoch5, step2292]: loss 0.779496
[epoch5, step2293]: loss 0.724604
[epoch5, step2294]: loss 0.604685
[epoch5, step2295]: loss 0.411752
[epoch5, step2296]: loss 0.422525
[epoch5, step2297]: loss 0.771458
[epoch5, step2298]: loss 0.518086
[epoch5, step2299]: loss 0.580342
[epoch5, step2300]: loss 0.556689
[epoch5, step2301]: loss 0.595509
[epoch5, step2302]: loss 0.791365
[epoch5, step2303]: loss 0.409487
[epoch5, step2304]: loss 0.443952
[epoch5, step2305]: loss 0.722112
[epoch5, step2306]: loss 0.561827
[epoch5, step2307]: loss 0.840575
[epoch5, step2308]: loss 0.508450
[epoch5, step2309]: loss 0.454595
[epoch5, step2310]: loss 0.427885
[epoch5, step2311]: loss 0.666933
[epoch5, step2312]: loss 0.697411
[epoch5, step2313]: loss 0.419987
[epoch5, step2314]: loss 0.443142
[epoch5, step2315]: loss 0.732802
[epoch5, step2316]: loss 0.596055
[epoch5, step2317]: loss 0.551772
[epoch5, step2318]: loss 0.680696
[epoch5, step2319]: loss 0.561163
[epoch5, step2320]: loss 0.654117
[epoch5, step2321]: loss 0.468565
[epoch5, step2322]: loss 0.608714
[epoch5, step2323]: loss 0.490207
[epoch5, step2324]: loss 0.569450
[epoch5, step2325]: loss 0.594365
[epoch5, step2326]: loss 0.473286
[epoch5, step2327]: loss 0.807285
[epoch5, step2328]: loss 0.453426
[epoch5, step2329]: loss 0.692367
[epoch5, step2330]: loss 0.390837
[epoch5, step2331]: loss 0.853855
[epoch5, step2332]: loss 0.719053
[epoch5, step2333]: loss 0.494826
[epoch5, step2334]: loss 0.646757
[epoch5, step2335]: loss 0.739190
[epoch5, step2336]: loss 0.512270
[epoch5, step2337]: loss 0.456454
[epoch5, step2338]: loss 0.541821
[epoch5, step2339]: loss 0.675868
[epoch5, step2340]: loss 0.164842
[epoch5, step2341]: loss 0.471822
[epoch5, step2342]: loss 0.585501
[epoch5, step2343]: loss 0.532800
[epoch5, step2344]: loss 0.567994
[epoch5, step2345]: loss 0.564317
[epoch5, step2346]: loss 0.717245
[epoch5, step2347]: loss 0.607470
[epoch5, step2348]: loss 0.777787
[epoch5, step2349]: loss 0.778821
[epoch5, step2350]: loss 0.740480
[epoch5, step2351]: loss 0.374120
[epoch5, step2352]: loss 0.506080
[epoch5, step2353]: loss 0.659100
[epoch5, step2354]: loss 0.415212
[epoch5, step2355]: loss 0.416267
[epoch5, step2356]: loss 0.618522
[epoch5, step2357]: loss 0.445242
[epoch5, step2358]: loss 0.340742
[epoch5, step2359]: loss 0.522696
[epoch5, step2360]: loss 0.442258
[epoch5, step2361]: loss 0.614334
[epoch5, step2362]: loss 0.639070
[epoch5, step2363]: loss 0.629038
[epoch5, step2364]: loss 0.661649
[epoch5, step2365]: loss 0.492664
[epoch5, step2366]: loss 0.728444
[epoch5, step2367]: loss 0.706599
[epoch5, step2368]: loss 0.413130
[epoch5, step2369]: loss 0.527155
[epoch5, step2370]: loss 0.456801
[epoch5, step2371]: loss 0.458673
[epoch5, step2372]: loss 0.353447
[epoch5, step2373]: loss 0.561119
[epoch5, step2374]: loss 0.624305
[epoch5, step2375]: loss 0.433359
[epoch5, step2376]: loss 0.292091
[epoch5, step2377]: loss 0.582478
[epoch5, step2378]: loss 0.783795
[epoch5, step2379]: loss 0.399246
[epoch5, step2380]: loss 0.646438
[epoch5, step2381]: loss 0.656330
[epoch5, step2382]: loss 0.875881
[epoch5, step2383]: loss 0.341222
[epoch5, step2384]: loss 0.757289
[epoch5, step2385]: loss 0.704257
[epoch5, step2386]: loss 0.303044
[epoch5, step2387]: loss 0.542440
[epoch5, step2388]: loss 0.648382
[epoch5, step2389]: loss 0.772501
[epoch5, step2390]: loss 0.583186
[epoch5, step2391]: loss 0.805484
[epoch5, step2392]: loss 0.621503
[epoch5, step2393]: loss 0.570882
[epoch5, step2394]: loss 0.577542
[epoch5, step2395]: loss 0.490222
[epoch5, step2396]: loss 0.360002
[epoch5, step2397]: loss 0.243112
[epoch5, step2398]: loss 0.519853
[epoch5, step2399]: loss 0.521026
[epoch5, step2400]: loss 0.505060
[epoch5, step2401]: loss 0.471720
[epoch5, step2402]: loss 0.791512
[epoch5, step2403]: loss 0.590684
[epoch5, step2404]: loss 0.620872
[epoch5, step2405]: loss 0.641778
[epoch5, step2406]: loss 0.563567
[epoch5, step2407]: loss 0.696312
[epoch5, step2408]: loss 0.414098
[epoch5, step2409]: loss 0.614016
[epoch5, step2410]: loss 0.679841
[epoch5, step2411]: loss 0.724116
[epoch5, step2412]: loss 0.579911
[epoch5, step2413]: loss 0.510989
[epoch5, step2414]: loss 0.377665
[epoch5, step2415]: loss 0.355551
[epoch5, step2416]: loss 0.346912
[epoch5, step2417]: loss 0.603079
[epoch5, step2418]: loss 0.610451
[epoch5, step2419]: loss 0.630397
[epoch5, step2420]: loss 0.812619
[epoch5, step2421]: loss 0.641698
[epoch5, step2422]: loss 0.639213
[epoch5, step2423]: loss 0.460101
[epoch5, step2424]: loss 0.557098
[epoch5, step2425]: loss 0.620781
[epoch5, step2426]: loss 0.706156
[epoch5, step2427]: loss 0.516887
[epoch5, step2428]: loss 0.713346
[epoch5, step2429]: loss 0.544962
[epoch5, step2430]: loss 0.673225
[epoch5, step2431]: loss 0.287695
[epoch5, step2432]: loss 0.666095
[epoch5, step2433]: loss 0.291192
[epoch5, step2434]: loss 0.500157
[epoch5, step2435]: loss 0.555903
[epoch5, step2436]: loss 0.591859
[epoch5, step2437]: loss 0.490829
[epoch5, step2438]: loss 0.543278
[epoch5, step2439]: loss 0.609419
[epoch5, step2440]: loss 0.453223
[epoch5, step2441]: loss 0.465178
[epoch5, step2442]: loss 0.624419
[epoch5, step2443]: loss 0.415593
[epoch5, step2444]: loss 0.835727
[epoch5, step2445]: loss 0.580619
[epoch5, step2446]: loss 0.503775
[epoch5, step2447]: loss 0.556109
[epoch5, step2448]: loss 0.352360
[epoch5, step2449]: loss 0.624956
[epoch5, step2450]: loss 0.612396
[epoch5, step2451]: loss 0.454164
[epoch5, step2452]: loss 0.534316
[epoch5, step2453]: loss 0.127327
[epoch5, step2454]: loss 0.624755
[epoch5, step2455]: loss 0.498503
[epoch5, step2456]: loss 0.509641
[epoch5, step2457]: loss 0.733977
[epoch5, step2458]: loss 0.680975
[epoch5, step2459]: loss 0.580777
[epoch5, step2460]: loss 0.428836
[epoch5, step2461]: loss 0.619432
[epoch5, step2462]: loss 0.617518
[epoch5, step2463]: loss 0.391882
[epoch5, step2464]: loss 0.715364
[epoch5, step2465]: loss 0.508773
[epoch5, step2466]: loss 0.710100
[epoch5, step2467]: loss 0.304644
[epoch5, step2468]: loss 0.481043
[epoch5, step2469]: loss 0.734533
[epoch5, step2470]: loss 0.436212
[epoch5, step2471]: loss 0.219361
[epoch5, step2472]: loss 0.388108
[epoch5, step2473]: loss 0.575378
[epoch5, step2474]: loss 0.468173
[epoch5, step2475]: loss 0.611957
[epoch5, step2476]: loss 0.644169
[epoch5, step2477]: loss 0.592154
[epoch5, step2478]: loss 0.629173
[epoch5, step2479]: loss 0.567948
[epoch5, step2480]: loss 0.545066
[epoch5, step2481]: loss 0.588049
[epoch5, step2482]: loss 0.675543
[epoch5, step2483]: loss 0.322929
[epoch5, step2484]: loss 0.320582
[epoch5, step2485]: loss 0.684239
[epoch5, step2486]: loss 0.668779
[epoch5, step2487]: loss 0.457853
[epoch5, step2488]: loss 0.678484
[epoch5, step2489]: loss 0.650928
[epoch5, step2490]: loss 0.676265
[epoch5, step2491]: loss 0.456868
[epoch5, step2492]: loss 0.546586
[epoch5, step2493]: loss 0.584693
[epoch5, step2494]: loss 0.581661
[epoch5, step2495]: loss 0.377475
[epoch5, step2496]: loss 0.880579
[epoch5, step2497]: loss 0.562412
[epoch5, step2498]: loss 0.690694
[epoch5, step2499]: loss 0.621559
[epoch5, step2500]: loss 0.564136
[epoch5, step2501]: loss 0.427564
[epoch5, step2502]: loss 0.417914
[epoch5, step2503]: loss 0.427172
[epoch5, step2504]: loss 0.511522
[epoch5, step2505]: loss 0.395653
[epoch5, step2506]: loss 0.470557
[epoch5, step2507]: loss 0.767483
[epoch5, step2508]: loss 0.704973
[epoch5, step2509]: loss 0.592240
[epoch5, step2510]: loss 0.419207
[epoch5, step2511]: loss 0.323979
[epoch5, step2512]: loss 0.459996
[epoch5, step2513]: loss 0.599261
[epoch5, step2514]: loss 0.725281
[epoch5, step2515]: loss 0.460658
[epoch5, step2516]: loss 0.335516
[epoch5, step2517]: loss 0.793422
[epoch5, step2518]: loss 0.683908
[epoch5, step2519]: loss 0.464129
[epoch5, step2520]: loss 0.432012
[epoch5, step2521]: loss 0.555984
[epoch5, step2522]: loss 0.602628
[epoch5, step2523]: loss 0.498551
[epoch5, step2524]: loss 0.347327
[epoch5, step2525]: loss 0.511410
[epoch5, step2526]: loss 0.439728
[epoch5, step2527]: loss 0.534842
[epoch5, step2528]: loss 0.611173
[epoch5, step2529]: loss 0.600722
[epoch5, step2530]: loss 0.596355
[epoch5, step2531]: loss 0.427608
[epoch5, step2532]: loss 0.620359
[epoch5, step2533]: loss 0.680293
[epoch5, step2534]: loss 0.563593
[epoch5, step2535]: loss 0.402821
[epoch5, step2536]: loss 0.658293
[epoch5, step2537]: loss 0.594538
[epoch5, step2538]: loss 0.662995
[epoch5, step2539]: loss 0.443992
[epoch5, step2540]: loss 0.179079
[epoch5, step2541]: loss 0.446494
[epoch5, step2542]: loss 0.643366
[epoch5, step2543]: loss 0.561925
[epoch5, step2544]: loss 0.400183
[epoch5, step2545]: loss 0.635755
[epoch5, step2546]: loss 0.285192
[epoch5, step2547]: loss 0.725976
[epoch5, step2548]: loss 0.463796
[epoch5, step2549]: loss 0.702066
[epoch5, step2550]: loss 0.795854
[epoch5, step2551]: loss 0.498250
[epoch5, step2552]: loss 0.531542
[epoch5, step2553]: loss 0.537159
[epoch5, step2554]: loss 0.673359
[epoch5, step2555]: loss 0.610775
[epoch5, step2556]: loss 0.490267
[epoch5, step2557]: loss 0.646548
[epoch5, step2558]: loss 0.285618
[epoch5, step2559]: loss 0.752839
[epoch5, step2560]: loss 0.571189
[epoch5, step2561]: loss 0.333517
[epoch5, step2562]: loss 0.605426
[epoch5, step2563]: loss 0.895481
[epoch5, step2564]: loss 0.424383
[epoch5, step2565]: loss 0.653384
[epoch5, step2566]: loss 0.545817
[epoch5, step2567]: loss 0.445920
[epoch5, step2568]: loss 0.442379
[epoch5, step2569]: loss 0.701061
[epoch5, step2570]: loss 0.669641
[epoch5, step2571]: loss 0.595896
[epoch5, step2572]: loss 0.637931
[epoch5, step2573]: loss 0.352851
[epoch5, step2574]: loss 0.680248
[epoch5, step2575]: loss 0.361477
[epoch5, step2576]: loss 0.407449
[epoch5, step2577]: loss 0.724668
[epoch5, step2578]: loss 0.674531
[epoch5, step2579]: loss 0.615550
[epoch5, step2580]: loss 0.532062
[epoch5, step2581]: loss 0.644692
[epoch5, step2582]: loss 0.446392
[epoch5, step2583]: loss 0.588429
[epoch5, step2584]: loss 0.265097
[epoch5, step2585]: loss 0.607256
[epoch5, step2586]: loss 0.307888
[epoch5, step2587]: loss 0.713704
[epoch5, step2588]: loss 0.781037
[epoch5, step2589]: loss 0.783439
[epoch5, step2590]: loss 0.683664
[epoch5, step2591]: loss 0.520376
[epoch5, step2592]: loss 0.487778
[epoch5, step2593]: loss 0.590673
[epoch5, step2594]: loss 0.358398
[epoch5, step2595]: loss 0.780709
[epoch5, step2596]: loss 0.632769
[epoch5, step2597]: loss 0.540084
[epoch5, step2598]: loss 0.652269
[epoch5, step2599]: loss 0.819160
[epoch5, step2600]: loss 0.490180
[epoch5, step2601]: loss 0.767832
[epoch5, step2602]: loss 0.420448
[epoch5, step2603]: loss 0.635871
[epoch5, step2604]: loss 0.617714
[epoch5, step2605]: loss 0.500599
[epoch5, step2606]: loss 0.597730
[epoch5, step2607]: loss 0.657910
[epoch5, step2608]: loss 0.503847
[epoch5, step2609]: loss 0.173310
[epoch5, step2610]: loss 0.478912
[epoch5, step2611]: loss 0.502023
[epoch5, step2612]: loss 0.671559
[epoch5, step2613]: loss 0.734344
[epoch5, step2614]: loss 0.688574
[epoch5, step2615]: loss 0.681448
[epoch5, step2616]: loss 0.389576
[epoch5, step2617]: loss 0.161829
[epoch5, step2618]: loss 0.525192
[epoch5, step2619]: loss 0.519665
[epoch5, step2620]: loss 0.718334
[epoch5, step2621]: loss 0.301595
[epoch5, step2622]: loss 0.755730
[epoch5, step2623]: loss 0.668105
[epoch5, step2624]: loss 0.457562
[epoch5, step2625]: loss 0.460554
[epoch5, step2626]: loss 0.823281
[epoch5, step2627]: loss 0.792299
[epoch5, step2628]: loss 0.708693
[epoch5, step2629]: loss 0.817463
[epoch5, step2630]: loss 0.397817
[epoch5, step2631]: loss 0.748629
[epoch5, step2632]: loss 0.619617
[epoch5, step2633]: loss 0.541228
[epoch5, step2634]: loss 0.428350
[epoch5, step2635]: loss 0.571675
[epoch5, step2636]: loss 0.599833
[epoch5, step2637]: loss 0.489923
[epoch5, step2638]: loss 0.664329
[epoch5, step2639]: loss 0.423702
[epoch5, step2640]: loss 0.452546
[epoch5, step2641]: loss 0.662020
[epoch5, step2642]: loss 0.647361
[epoch5, step2643]: loss 0.629706
[epoch5, step2644]: loss 0.476199
[epoch5, step2645]: loss 0.568102
[epoch5, step2646]: loss 0.387221
[epoch5, step2647]: loss 0.358508
[epoch5, step2648]: loss 0.751648
[epoch5, step2649]: loss 0.427676
[epoch5, step2650]: loss 0.570766
[epoch5, step2651]: loss 0.440509
[epoch5, step2652]: loss 0.549017
[epoch5, step2653]: loss 0.440520
[epoch5, step2654]: loss 0.492848
[epoch5, step2655]: loss 0.645131
[epoch5, step2656]: loss 0.538473
[epoch5, step2657]: loss 0.545665
[epoch5, step2658]: loss 0.699986
[epoch5, step2659]: loss 0.702672
[epoch5, step2660]: loss 0.556238
[epoch5, step2661]: loss 0.667333
[epoch5, step2662]: loss 0.512500
[epoch5, step2663]: loss 0.678351
[epoch5, step2664]: loss 0.747483
[epoch5, step2665]: loss 0.472472
[epoch5, step2666]: loss 0.444660
[epoch5, step2667]: loss 0.574309
[epoch5, step2668]: loss 0.584273
[epoch5, step2669]: loss 0.540101
[epoch5, step2670]: loss 0.539838
[epoch5, step2671]: loss 0.486164
[epoch5, step2672]: loss 0.457829
[epoch5, step2673]: loss 0.266597
[epoch5, step2674]: loss 0.700364
[epoch5, step2675]: loss 0.510861
[epoch5, step2676]: loss 0.487143
[epoch5, step2677]: loss 0.479845
[epoch5, step2678]: loss 0.661938
[epoch5, step2679]: loss 0.543020
[epoch5, step2680]: loss 0.659150
[epoch5, step2681]: loss 0.722010
[epoch5, step2682]: loss 0.792058
[epoch5, step2683]: loss 0.505889
[epoch5, step2684]: loss 0.575240
[epoch5, step2685]: loss 0.745643
[epoch5, step2686]: loss 0.330449
[epoch5, step2687]: loss 0.529780
[epoch5, step2688]: loss 0.776331
[epoch5, step2689]: loss 0.577620
[epoch5, step2690]: loss 0.538745
[epoch5, step2691]: loss 0.441178
[epoch5, step2692]: loss 0.705966
[epoch5, step2693]: loss 0.842777
[epoch5, step2694]: loss 0.444558
[epoch5, step2695]: loss 0.737459
[epoch5, step2696]: loss 0.537361
[epoch5, step2697]: loss 0.596003
[epoch5, step2698]: loss 0.475613
[epoch5, step2699]: loss 0.327192
[epoch5, step2700]: loss 0.281525
[epoch5, step2701]: loss 0.590862
[epoch5, step2702]: loss 0.461820
[epoch5, step2703]: loss 0.619002
[epoch5, step2704]: loss 0.471579
[epoch5, step2705]: loss 0.336589
[epoch5, step2706]: loss 0.607867
[epoch5, step2707]: loss 0.255182
[epoch5, step2708]: loss 0.510625
[epoch5, step2709]: loss 0.558967
[epoch5, step2710]: loss 0.672557
[epoch5, step2711]: loss 0.583890
[epoch5, step2712]: loss 0.576398
[epoch5, step2713]: loss 0.615271
[epoch5, step2714]: loss 0.578831
[epoch5, step2715]: loss 0.640727
[epoch5, step2716]: loss 0.435560
[epoch5, step2717]: loss 0.654227
[epoch5, step2718]: loss 0.590977
[epoch5, step2719]: loss 0.573908
[epoch5, step2720]: loss 0.665825
[epoch5, step2721]: loss 0.760694
[epoch5, step2722]: loss 0.670519
[epoch5, step2723]: loss 0.634207
[epoch5, step2724]: loss 0.756130
[epoch5, step2725]: loss 0.340855
[epoch5, step2726]: loss 0.519566
[epoch5, step2727]: loss 0.738136
[epoch5, step2728]: loss 0.508868
[epoch5, step2729]: loss 0.405613
[epoch5, step2730]: loss 0.429823
[epoch5, step2731]: loss 0.436886
[epoch5, step2732]: loss 0.577590
[epoch5, step2733]: loss 0.401184
[epoch5, step2734]: loss 0.542746
[epoch5, step2735]: loss 0.529252
[epoch5, step2736]: loss 0.712905
[epoch5, step2737]: loss 0.790507
[epoch5, step2738]: loss 0.557885
[epoch5, step2739]: loss 0.718763
[epoch5, step2740]: loss 0.450543
[epoch5, step2741]: loss 0.606891
[epoch5, step2742]: loss 0.745489
[epoch5, step2743]: loss 0.562535
[epoch5, step2744]: loss 0.705422
[epoch5, step2745]: loss 0.560236
[epoch5, step2746]: loss 0.435492
[epoch5, step2747]: loss 0.532903
[epoch5, step2748]: loss 0.397359
[epoch5, step2749]: loss 0.620591
[epoch5, step2750]: loss 0.817778
[epoch5, step2751]: loss 0.540588
[epoch5, step2752]: loss 0.331650
[epoch5, step2753]: loss 0.721518
[epoch5, step2754]: loss 0.620948
[epoch5, step2755]: loss 0.430389
[epoch5, step2756]: loss 0.617827
[epoch5, step2757]: loss 0.722542
[epoch5, step2758]: loss 0.561105
[epoch5, step2759]: loss 0.303531
[epoch5, step2760]: loss 0.628700
[epoch5, step2761]: loss 0.417930
[epoch5, step2762]: loss 0.614301
[epoch5, step2763]: loss 0.610552
[epoch5, step2764]: loss 0.466590
[epoch5, step2765]: loss 0.588550
[epoch5, step2766]: loss 0.234198
[epoch5, step2767]: loss 0.699475
[epoch5, step2768]: loss 0.566783
[epoch5, step2769]: loss 0.526679
[epoch5, step2770]: loss 0.481257
[epoch5, step2771]: loss 0.620378
[epoch5, step2772]: loss 0.420546
[epoch5, step2773]: loss 0.536816
[epoch5, step2774]: loss 0.625280
[epoch5, step2775]: loss 0.559860
[epoch5, step2776]: loss 0.637200
[epoch5, step2777]: loss 0.612205
[epoch5, step2778]: loss 0.212992
[epoch5, step2779]: loss 0.703935
[epoch5, step2780]: loss 0.864396
[epoch5, step2781]: loss 0.614056
[epoch5, step2782]: loss 0.676220
[epoch5, step2783]: loss 0.503447
[epoch5, step2784]: loss 0.670279
[epoch5, step2785]: loss 0.523900
[epoch5, step2786]: loss 0.550607
[epoch5, step2787]: loss 0.594764
[epoch5, step2788]: loss 0.720492
[epoch5, step2789]: loss 0.439805
[epoch5, step2790]: loss 0.375076
[epoch5, step2791]: loss 0.544451
[epoch5, step2792]: loss 0.648112
[epoch5, step2793]: loss 0.666842
[epoch5, step2794]: loss 0.599929
[epoch5, step2795]: loss 0.344286
[epoch5, step2796]: loss 0.484564
[epoch5, step2797]: loss 0.711383
[epoch5, step2798]: loss 0.561696
[epoch5, step2799]: loss 0.713064
[epoch5, step2800]: loss 0.501269
[epoch5, step2801]: loss 0.580237
[epoch5, step2802]: loss 0.466060
[epoch5, step2803]: loss 0.710606
[epoch5, step2804]: loss 0.634094
[epoch5, step2805]: loss 0.656286
[epoch5, step2806]: loss 0.560482
[epoch5, step2807]: loss 0.571858
[epoch5, step2808]: loss 0.789980
[epoch5, step2809]: loss 0.317917
[epoch5, step2810]: loss 0.676860
[epoch5, step2811]: loss 0.462433
[epoch5, step2812]: loss 0.592027
[epoch5, step2813]: loss 0.650731
[epoch5, step2814]: loss 0.724529
[epoch5, step2815]: loss 0.458868
[epoch5, step2816]: loss 0.422456
[epoch5, step2817]: loss 0.548282
[epoch5, step2818]: loss 0.433322
[epoch5, step2819]: loss 0.759729
[epoch5, step2820]: loss 0.501895
[epoch5, step2821]: loss 0.561931
[epoch5, step2822]: loss 0.619757
[epoch5, step2823]: loss 0.496098
[epoch5, step2824]: loss 0.528961
[epoch5, step2825]: loss 0.567216
[epoch5, step2826]: loss 0.638940
[epoch5, step2827]: loss 0.542024
[epoch5, step2828]: loss 0.673639
[epoch5, step2829]: loss 0.503526
[epoch5, step2830]: loss 0.459441
[epoch5, step2831]: loss 0.740857
[epoch5, step2832]: loss 0.724861
[epoch5, step2833]: loss 0.600997
[epoch5, step2834]: loss 0.591603
[epoch5, step2835]: loss 0.677960
[epoch5, step2836]: loss 0.529927
[epoch5, step2837]: loss 0.539220
[epoch5, step2838]: loss 0.561288
[epoch5, step2839]: loss 0.448019
[epoch5, step2840]: loss 0.614333
[epoch5, step2841]: loss 0.667919
[epoch5, step2842]: loss 0.570827
[epoch5, step2843]: loss 0.493352
[epoch5, step2844]: loss 0.609274
[epoch5, step2845]: loss 0.642081
[epoch5, step2846]: loss 0.616369
[epoch5, step2847]: loss 0.646570
[epoch5, step2848]: loss 0.533110
[epoch5, step2849]: loss 0.683688
[epoch5, step2850]: loss 0.423876
[epoch5, step2851]: loss 0.360380
[epoch5, step2852]: loss 0.440288
[epoch5, step2853]: loss 0.672562
[epoch5, step2854]: loss 0.363988
[epoch5, step2855]: loss 0.648737
[epoch5, step2856]: loss 0.624061
[epoch5, step2857]: loss 0.529974
[epoch5, step2858]: loss 0.466330
[epoch5, step2859]: loss 0.576818
[epoch5, step2860]: loss 0.754393
[epoch5, step2861]: loss 0.637651
[epoch5, step2862]: loss 0.668037
[epoch5, step2863]: loss 0.293232
[epoch5, step2864]: loss 0.504703
[epoch5, step2865]: loss 0.453011
[epoch5, step2866]: loss 0.575447
[epoch5, step2867]: loss 0.683522
[epoch5, step2868]: loss 0.614155
[epoch5, step2869]: loss 0.606667
[epoch5, step2870]: loss 0.592883
[epoch5, step2871]: loss 0.532872
[epoch5, step2872]: loss 0.566328
[epoch5, step2873]: loss 0.472639
[epoch5, step2874]: loss 0.279599
[epoch5, step2875]: loss 0.413407
[epoch5, step2876]: loss 0.584051
[epoch5, step2877]: loss 0.592440
[epoch5, step2878]: loss 0.373206
[epoch5, step2879]: loss 0.847475
[epoch5, step2880]: loss 0.476567
[epoch5, step2881]: loss 0.486190
[epoch5, step2882]: loss 0.450858
[epoch5, step2883]: loss 0.578175
[epoch5, step2884]: loss 0.624129
[epoch5, step2885]: loss 0.641097
[epoch5, step2886]: loss 0.517287
[epoch5, step2887]: loss 0.835024
[epoch5, step2888]: loss 0.345204
[epoch5, step2889]: loss 0.723177
[epoch5, step2890]: loss 0.652633
[epoch5, step2891]: loss 0.527784
[epoch5, step2892]: loss 0.600006
[epoch5, step2893]: loss 0.599928
[epoch5, step2894]: loss 0.470459
[epoch5, step2895]: loss 0.383504
[epoch5, step2896]: loss 0.554205
[epoch5, step2897]: loss 0.600804
[epoch5, step2898]: loss 0.734790
[epoch5, step2899]: loss 0.543036
[epoch5, step2900]: loss 0.480446
[epoch5, step2901]: loss 0.408470
[epoch5, step2902]: loss 0.730917
[epoch5, step2903]: loss 0.704614
[epoch5, step2904]: loss 0.475869
[epoch5, step2905]: loss 0.753127
[epoch5, step2906]: loss 0.469788
[epoch5, step2907]: loss 0.685021
[epoch5, step2908]: loss 0.533237
[epoch5, step2909]: loss 0.366361
[epoch5, step2910]: loss 0.532010
[epoch5, step2911]: loss 0.489605
[epoch5, step2912]: loss 0.626912
[epoch5, step2913]: loss 0.460996
[epoch5, step2914]: loss 0.576024
[epoch5, step2915]: loss 0.466030
[epoch5, step2916]: loss 0.578940
[epoch5, step2917]: loss 0.421100
[epoch5, step2918]: loss 0.247892
[epoch5, step2919]: loss 0.313868
[epoch5, step2920]: loss 0.765657
[epoch5, step2921]: loss 0.582123
[epoch5, step2922]: loss 0.565584
[epoch5, step2923]: loss 0.528570
[epoch5, step2924]: loss 0.812722
[epoch5, step2925]: loss 0.503158
[epoch5, step2926]: loss 0.548722
[epoch5, step2927]: loss 0.419354
[epoch5, step2928]: loss 0.666137
[epoch5, step2929]: loss 0.568634
[epoch5, step2930]: loss 0.684863
[epoch5, step2931]: loss 0.372553
[epoch5, step2932]: loss 0.446319
[epoch5, step2933]: loss 0.523343
[epoch5, step2934]: loss 0.732004
[epoch5, step2935]: loss 0.538108
[epoch5, step2936]: loss 0.377691
[epoch5, step2937]: loss 0.667915
[epoch5, step2938]: loss 0.868196
[epoch5, step2939]: loss 0.280894
[epoch5, step2940]: loss 0.458284
[epoch5, step2941]: loss 0.533032
[epoch5, step2942]: loss 0.451082
[epoch5, step2943]: loss 0.776879
[epoch5, step2944]: loss 0.826057
[epoch5, step2945]: loss 0.529779
[epoch5, step2946]: loss 0.494428
[epoch5, step2947]: loss 0.851421
[epoch5, step2948]: loss 0.498740
[epoch5, step2949]: loss 0.557190
[epoch5, step2950]: loss 0.547876
[epoch5, step2951]: loss 0.297681
[epoch5, step2952]: loss 0.317021
[epoch5, step2953]: loss 0.628745
[epoch5, step2954]: loss 0.664424
[epoch5, step2955]: loss 0.641179
[epoch5, step2956]: loss 0.536977
[epoch5, step2957]: loss 0.350624
[epoch5, step2958]: loss 0.675307
[epoch5, step2959]: loss 0.309855
[epoch5, step2960]: loss 0.432818
[epoch5, step2961]: loss 0.667414
[epoch5, step2962]: loss 0.607841
[epoch5, step2963]: loss 0.830556
[epoch5, step2964]: loss 0.452266
[epoch5, step2965]: loss 0.424635
[epoch5, step2966]: loss 0.520300
[epoch5, step2967]: loss 0.563501
[epoch5, step2968]: loss 0.628430
[epoch5, step2969]: loss 0.783637
[epoch5, step2970]: loss 0.558476
[epoch5, step2971]: loss 0.600799
[epoch5, step2972]: loss 0.682211
[epoch5, step2973]: loss 0.551839
[epoch5, step2974]: loss 0.369173
[epoch5, step2975]: loss 0.549545
[epoch5, step2976]: loss 0.418925
[epoch5, step2977]: loss 0.555191
[epoch5, step2978]: loss 0.577597
[epoch5, step2979]: loss 0.391397
[epoch5, step2980]: loss 0.342105
[epoch5, step2981]: loss 0.661936
[epoch5, step2982]: loss 0.545493
[epoch5, step2983]: loss 0.730095
[epoch5, step2984]: loss 0.589382
[epoch5, step2985]: loss 0.589722
[epoch5, step2986]: loss 0.506800
[epoch5, step2987]: loss 0.358292
[epoch5, step2988]: loss 0.508942
[epoch5, step2989]: loss 0.495041
[epoch5, step2990]: loss 0.390882
[epoch5, step2991]: loss 0.452243
[epoch5, step2992]: loss 0.360094
[epoch5, step2993]: loss 0.647993
[epoch5, step2994]: loss 0.573957
[epoch5, step2995]: loss 0.341502
[epoch5, step2996]: loss 0.436191
[epoch5, step2997]: loss 0.827359
[epoch5, step2998]: loss 0.377406
[epoch5, step2999]: loss 0.845310
[epoch5, step3000]: loss 0.728318
[epoch5, step3001]: loss 0.752742
[epoch5, step3002]: loss 0.526455
[epoch5, step3003]: loss 0.697863
[epoch5, step3004]: loss 0.647856
[epoch5, step3005]: loss 0.587909
[epoch5, step3006]: loss 0.715713
[epoch5, step3007]: loss 0.484065
[epoch5, step3008]: loss 0.667636
[epoch5, step3009]: loss 0.535722
[epoch5, step3010]: loss 0.586674
[epoch5, step3011]: loss 0.530196
[epoch5, step3012]: loss 0.308053
[epoch5, step3013]: loss 0.226420
[epoch5, step3014]: loss 0.562753
[epoch5, step3015]: loss 0.731241
[epoch5, step3016]: loss 0.646391
[epoch5, step3017]: loss 0.331070
[epoch5, step3018]: loss 0.494040
[epoch5, step3019]: loss 0.606527
[epoch5, step3020]: loss 0.450350
[epoch5, step3021]: loss 0.619196
[epoch5, step3022]: loss 0.750926
[epoch5, step3023]: loss 0.474234
[epoch5, step3024]: loss 0.525574
[epoch5, step3025]: loss 0.398982
[epoch5, step3026]: loss 0.835222
[epoch5, step3027]: loss 0.728977
[epoch5, step3028]: loss 0.467200
[epoch5, step3029]: loss 0.598764
[epoch5, step3030]: loss 0.436359
[epoch5, step3031]: loss 0.594426
[epoch5, step3032]: loss 0.660589
[epoch5, step3033]: loss 0.523624
[epoch5, step3034]: loss 0.593515
[epoch5, step3035]: loss 0.221582
[epoch5, step3036]: loss 0.419749
[epoch5, step3037]: loss 0.653910
[epoch5, step3038]: loss 0.404775
[epoch5, step3039]: loss 0.621253
[epoch5, step3040]: loss 0.746666
[epoch5, step3041]: loss 0.462090
[epoch5, step3042]: loss 0.653630
[epoch5, step3043]: loss 0.583603
[epoch5, step3044]: loss 0.615230
[epoch5, step3045]: loss 0.451402
[epoch5, step3046]: loss 0.374236
[epoch5, step3047]: loss 0.525032
[epoch5, step3048]: loss 0.758850
[epoch5, step3049]: loss 0.569687
[epoch5, step3050]: loss 0.167363
[epoch5, step3051]: loss 0.356600
[epoch5, step3052]: loss 0.642387
[epoch5, step3053]: loss 0.608028
[epoch5, step3054]: loss 0.471579
[epoch5, step3055]: loss 0.469590
[epoch5, step3056]: loss 0.223424
[epoch5, step3057]: loss 0.444839
[epoch5, step3058]: loss 0.321267
[epoch5, step3059]: loss 0.545246
[epoch5, step3060]: loss 0.641827
[epoch5, step3061]: loss 0.604156
[epoch5, step3062]: loss 0.439454
[epoch5, step3063]: loss 0.354310
[epoch5, step3064]: loss 0.660349
[epoch5, step3065]: loss 0.504709
[epoch5, step3066]: loss 0.664144
[epoch5, step3067]: loss 0.774108
[epoch5, step3068]: loss 0.667440
[epoch5, step3069]: loss 0.375509
[epoch5, step3070]: loss 0.605517
[epoch5, step3071]: loss 0.623829
[epoch5, step3072]: loss 0.477266
[epoch5, step3073]: loss 0.730093
[epoch5, step3074]: loss 0.859455
[epoch5, step3075]: loss 0.617612
[epoch5, step3076]: loss 0.747451

[epoch5]: avg loss 0.747451

[epoch6, step1]: loss 0.583377
[epoch6, step2]: loss 0.556498
[epoch6, step3]: loss 0.626744
[epoch6, step4]: loss 0.368846
[epoch6, step5]: loss 0.259251
[epoch6, step6]: loss 0.806549
[epoch6, step7]: loss 0.641526
[epoch6, step8]: loss 0.521538
[epoch6, step9]: loss 0.537340
[epoch6, step10]: loss 0.543566
[epoch6, step11]: loss 0.537113
[epoch6, step12]: loss 0.804216
[epoch6, step13]: loss 0.433813
[epoch6, step14]: loss 0.510349
[epoch6, step15]: loss 0.569844
[epoch6, step16]: loss 0.455030
[epoch6, step17]: loss 0.547526
[epoch6, step18]: loss 0.457228
[epoch6, step19]: loss 0.756467
[epoch6, step20]: loss 0.327887
[epoch6, step21]: loss 0.309948
[epoch6, step22]: loss 0.667987
[epoch6, step23]: loss 0.828009
[epoch6, step24]: loss 0.657767
[epoch6, step25]: loss 0.578529
[epoch6, step26]: loss 0.428809
[epoch6, step27]: loss 0.684035
[epoch6, step28]: loss 0.481427
[epoch6, step29]: loss 0.568987
[epoch6, step30]: loss 0.633596
[epoch6, step31]: loss 0.567042
[epoch6, step32]: loss 0.341615
[epoch6, step33]: loss 0.590119
[epoch6, step34]: loss 0.473086
[epoch6, step35]: loss 0.622659
[epoch6, step36]: loss 0.711025
[epoch6, step37]: loss 0.804121
[epoch6, step38]: loss 0.651255
[epoch6, step39]: loss 0.394895
[epoch6, step40]: loss 0.636608
[epoch6, step41]: loss 0.318920
[epoch6, step42]: loss 0.741155
[epoch6, step43]: loss 0.557629
[epoch6, step44]: loss 0.249050
[epoch6, step45]: loss 0.616793
[epoch6, step46]: loss 0.437484
[epoch6, step47]: loss 0.775453
[epoch6, step48]: loss 0.518105
[epoch6, step49]: loss 0.573267
[epoch6, step50]: loss 0.424915
[epoch6, step51]: loss 0.603578
[epoch6, step52]: loss 0.385058
[epoch6, step53]: loss 0.566760
[epoch6, step54]: loss 0.513490
[epoch6, step55]: loss 0.736822
[epoch6, step56]: loss 0.544394
[epoch6, step57]: loss 0.580363
[epoch6, step58]: loss 0.710505
[epoch6, step59]: loss 0.539480
[epoch6, step60]: loss 0.818472
[epoch6, step61]: loss 0.499566
[epoch6, step62]: loss 0.403973
[epoch6, step63]: loss 0.547742
[epoch6, step64]: loss 0.249412
[epoch6, step65]: loss 0.756412
[epoch6, step66]: loss 0.749287
[epoch6, step67]: loss 0.418194
[epoch6, step68]: loss 0.690251
[epoch6, step69]: loss 0.513051
[epoch6, step70]: loss 0.409267
[epoch6, step71]: loss 0.380011
[epoch6, step72]: loss 0.480225
[epoch6, step73]: loss 0.583874
[epoch6, step74]: loss 0.299244
[epoch6, step75]: loss 0.621673
[epoch6, step76]: loss 0.574542
[epoch6, step77]: loss 0.634131
[epoch6, step78]: loss 0.329783
[epoch6, step79]: loss 0.277685
[epoch6, step80]: loss 0.426126
[epoch6, step81]: loss 0.605324
[epoch6, step82]: loss 0.536771
[epoch6, step83]: loss 0.567368
[epoch6, step84]: loss 0.506412
[epoch6, step85]: loss 0.612044
[epoch6, step86]: loss 0.926080
[epoch6, step87]: loss 0.440600
[epoch6, step88]: loss 0.534218
[epoch6, step89]: loss 0.589250
[epoch6, step90]: loss 0.405741
[epoch6, step91]: loss 0.431418
[epoch6, step92]: loss 0.519366
[epoch6, step93]: loss 0.446465
[epoch6, step94]: loss 0.682308
[epoch6, step95]: loss 0.839400
[epoch6, step96]: loss 0.561750
[epoch6, step97]: loss 0.628217
[epoch6, step98]: loss 0.432206
[epoch6, step99]: loss 0.467775
[epoch6, step100]: loss 0.360557
[epoch6, step101]: loss 0.522298
[epoch6, step102]: loss 0.877872
[epoch6, step103]: loss 0.573031
[epoch6, step104]: loss 0.670353
[epoch6, step105]: loss 0.356266
[epoch6, step106]: loss 0.349144
[epoch6, step107]: loss 0.501309
[epoch6, step108]: loss 0.538718
[epoch6, step109]: loss 0.611505
[epoch6, step110]: loss 0.430072
[epoch6, step111]: loss 0.328607
[epoch6, step112]: loss 0.573258
[epoch6, step113]: loss 0.625610
[epoch6, step114]: loss 0.275337
[epoch6, step115]: loss 0.662005
[epoch6, step116]: loss 0.675668
[epoch6, step117]: loss 0.465642
[epoch6, step118]: loss 0.450916
[epoch6, step119]: loss 0.451537
[epoch6, step120]: loss 0.751315
[epoch6, step121]: loss 0.695833
[epoch6, step122]: loss 0.592643
[epoch6, step123]: loss 0.359512
[epoch6, step124]: loss 0.507177
[epoch6, step125]: loss 0.622551
[epoch6, step126]: loss 0.511549
[epoch6, step127]: loss 0.674683
[epoch6, step128]: loss 0.456507
[epoch6, step129]: loss 0.455284
[epoch6, step130]: loss 0.703187
[epoch6, step131]: loss 0.599431
[epoch6, step132]: loss 0.639031
[epoch6, step133]: loss 0.797519
[epoch6, step134]: loss 0.430510
[epoch6, step135]: loss 0.445369
[epoch6, step136]: loss 0.580590
[epoch6, step137]: loss 0.504499
[epoch6, step138]: loss 0.226814
[epoch6, step139]: loss 0.534095
[epoch6, step140]: loss 0.640334
[epoch6, step141]: loss 0.495191
[epoch6, step142]: loss 0.485511
[epoch6, step143]: loss 0.763386
[epoch6, step144]: loss 0.634958
[epoch6, step145]: loss 0.477452
[epoch6, step146]: loss 0.574930
[epoch6, step147]: loss 0.631615
[epoch6, step148]: loss 0.577810
[epoch6, step149]: loss 0.421879
[epoch6, step150]: loss 0.467015
[epoch6, step151]: loss 0.552426
[epoch6, step152]: loss 0.638447
[epoch6, step153]: loss 0.446125
[epoch6, step154]: loss 0.564458
[epoch6, step155]: loss 0.432055
[epoch6, step156]: loss 0.501347
[epoch6, step157]: loss 0.397953
[epoch6, step158]: loss 0.630278
[epoch6, step159]: loss 0.511277
[epoch6, step160]: loss 0.641032
[epoch6, step161]: loss 0.537164
[epoch6, step162]: loss 0.622009
[epoch6, step163]: loss 0.578798
[epoch6, step164]: loss 0.610074
[epoch6, step165]: loss 0.560027
[epoch6, step166]: loss 0.640126
[epoch6, step167]: loss 0.575449
[epoch6, step168]: loss 0.563755
[epoch6, step169]: loss 0.625355
[epoch6, step170]: loss 0.377445
[epoch6, step171]: loss 0.527111
[epoch6, step172]: loss 0.607795
[epoch6, step173]: loss 0.311386
[epoch6, step174]: loss 0.352409
[epoch6, step175]: loss 0.566518
[epoch6, step176]: loss 0.898033
[epoch6, step177]: loss 0.515868
[epoch6, step178]: loss 0.592392
[epoch6, step179]: loss 0.713529
[epoch6, step180]: loss 0.659044
[epoch6, step181]: loss 0.552129
[epoch6, step182]: loss 0.441337
[epoch6, step183]: loss 0.260110
[epoch6, step184]: loss 0.624425
[epoch6, step185]: loss 0.532776
[epoch6, step186]: loss 0.429597
[epoch6, step187]: loss 0.622155
[epoch6, step188]: loss 0.532048
[epoch6, step189]: loss 0.569550
[epoch6, step190]: loss 0.678330
[epoch6, step191]: loss 0.815222
[epoch6, step192]: loss 0.560202
[epoch6, step193]: loss 0.659982
[epoch6, step194]: loss 0.766676
[epoch6, step195]: loss 0.388229
[epoch6, step196]: loss 0.664923
[epoch6, step197]: loss 0.812392
[epoch6, step198]: loss 0.611866
[epoch6, step199]: loss 0.809331
[epoch6, step200]: loss 0.298716
[epoch6, step201]: loss 0.316199
[epoch6, step202]: loss 0.558571
[epoch6, step203]: loss 0.661237
[epoch6, step204]: loss 0.426410
[epoch6, step205]: loss 0.641601
[epoch6, step206]: loss 0.732449
[epoch6, step207]: loss 0.555217
[epoch6, step208]: loss 0.621624
[epoch6, step209]: loss 0.717911
[epoch6, step210]: loss 0.736222
[epoch6, step211]: loss 0.657398
[epoch6, step212]: loss 0.508977
[epoch6, step213]: loss 0.712541
[epoch6, step214]: loss 0.824330
[epoch6, step215]: loss 0.643037
[epoch6, step216]: loss 0.527534
[epoch6, step217]: loss 0.648627
[epoch6, step218]: loss 0.451865
[epoch6, step219]: loss 0.845636
[epoch6, step220]: loss 0.655358
[epoch6, step221]: loss 0.492599
[epoch6, step222]: loss 0.459554
[epoch6, step223]: loss 0.437200
[epoch6, step224]: loss 0.586896
[epoch6, step225]: loss 0.636942
[epoch6, step226]: loss 0.604059
[epoch6, step227]: loss 0.675418
[epoch6, step228]: loss 0.569410
[epoch6, step229]: loss 0.420574
[epoch6, step230]: loss 0.481939
[epoch6, step231]: loss 0.586930
[epoch6, step232]: loss 0.431189
[epoch6, step233]: loss 0.641811
[epoch6, step234]: loss 0.415334
[epoch6, step235]: loss 0.460000
[epoch6, step236]: loss 0.331487
[epoch6, step237]: loss 0.375177
[epoch6, step238]: loss 0.563145
[epoch6, step239]: loss 0.621290
[epoch6, step240]: loss 0.684940
[epoch6, step241]: loss 0.723500
[epoch6, step242]: loss 0.510232
[epoch6, step243]: loss 0.695741
[epoch6, step244]: loss 0.495122
[epoch6, step245]: loss 0.590005
[epoch6, step246]: loss 0.311381
[epoch6, step247]: loss 0.763780
[epoch6, step248]: loss 0.616345
[epoch6, step249]: loss 0.750997
[epoch6, step250]: loss 0.474419
[epoch6, step251]: loss 0.723230
[epoch6, step252]: loss 0.427706
[epoch6, step253]: loss 0.397899
[epoch6, step254]: loss 0.452412
[epoch6, step255]: loss 0.594433
[epoch6, step256]: loss 0.432079
[epoch6, step257]: loss 0.609237
[epoch6, step258]: loss 0.559120
[epoch6, step259]: loss 0.793262
[epoch6, step260]: loss 0.595250
[epoch6, step261]: loss 0.623562
[epoch6, step262]: loss 0.551903
[epoch6, step263]: loss 0.259200
[epoch6, step264]: loss 0.755971
[epoch6, step265]: loss 0.506933
[epoch6, step266]: loss 0.430588
[epoch6, step267]: loss 0.541207
[epoch6, step268]: loss 0.582199
[epoch6, step269]: loss 0.812875
[epoch6, step270]: loss 0.550490
[epoch6, step271]: loss 0.665181
[epoch6, step272]: loss 0.444371
[epoch6, step273]: loss 0.480919
[epoch6, step274]: loss 0.288363
[epoch6, step275]: loss 0.693124
[epoch6, step276]: loss 0.766195
[epoch6, step277]: loss 0.609981
[epoch6, step278]: loss 0.264620
[epoch6, step279]: loss 0.379874
[epoch6, step280]: loss 0.308964
[epoch6, step281]: loss 0.575574
[epoch6, step282]: loss 0.514987
[epoch6, step283]: loss 0.658497
[epoch6, step284]: loss 0.550168
[epoch6, step285]: loss 0.609118
[epoch6, step286]: loss 0.344037
[epoch6, step287]: loss 0.419975
[epoch6, step288]: loss 0.434717
[epoch6, step289]: loss 0.503921
[epoch6, step290]: loss 0.741611
[epoch6, step291]: loss 0.532512
[epoch6, step292]: loss 0.519066
[epoch6, step293]: loss 0.526405
[epoch6, step294]: loss 0.558266
[epoch6, step295]: loss 0.701240
[epoch6, step296]: loss 0.548298
[epoch6, step297]: loss 0.763276
[epoch6, step298]: loss 0.633846
[epoch6, step299]: loss 0.446263
[epoch6, step300]: loss 0.480810
[epoch6, step301]: loss 0.324630
[epoch6, step302]: loss 0.762159
[epoch6, step303]: loss 0.594468
[epoch6, step304]: loss 0.169598
[epoch6, step305]: loss 0.529329
[epoch6, step306]: loss 0.605610
[epoch6, step307]: loss 0.535235
[epoch6, step308]: loss 0.390787
[epoch6, step309]: loss 0.643616
[epoch6, step310]: loss 0.715674
[epoch6, step311]: loss 0.797154
[epoch6, step312]: loss 0.752703
[epoch6, step313]: loss 0.460868
[epoch6, step314]: loss 0.732249
[epoch6, step315]: loss 0.732301
[epoch6, step316]: loss 0.688294
[epoch6, step317]: loss 0.484759
[epoch6, step318]: loss 0.722546
[epoch6, step319]: loss 0.574419
[epoch6, step320]: loss 0.423024
[epoch6, step321]: loss 0.644631
[epoch6, step322]: loss 0.809862
[epoch6, step323]: loss 0.583872
[epoch6, step324]: loss 0.607123
[epoch6, step325]: loss 0.561787
[epoch6, step326]: loss 0.584313
[epoch6, step327]: loss 0.478380
[epoch6, step328]: loss 0.612627
[epoch6, step329]: loss 0.585202
[epoch6, step330]: loss 0.629129
[epoch6, step331]: loss 0.604504
[epoch6, step332]: loss 0.428843
[epoch6, step333]: loss 0.653842
[epoch6, step334]: loss 0.427584
[epoch6, step335]: loss 0.607320
[epoch6, step336]: loss 0.542360
[epoch6, step337]: loss 0.575985
[epoch6, step338]: loss 0.655832
[epoch6, step339]: loss 0.375004
[epoch6, step340]: loss 0.311530
[epoch6, step341]: loss 0.576652
[epoch6, step342]: loss 0.615684
[epoch6, step343]: loss 0.575459
[epoch6, step344]: loss 0.617743
[epoch6, step345]: loss 0.504216
[epoch6, step346]: loss 0.536222
[epoch6, step347]: loss 0.670106
[epoch6, step348]: loss 0.518341
[epoch6, step349]: loss 0.478185
[epoch6, step350]: loss 0.580086
[epoch6, step351]: loss 0.608851
[epoch6, step352]: loss 0.652257
[epoch6, step353]: loss 0.727063
[epoch6, step354]: loss 0.639732
[epoch6, step355]: loss 0.611473
[epoch6, step356]: loss 0.283986
[epoch6, step357]: loss 0.633056
[epoch6, step358]: loss 0.442611
[epoch6, step359]: loss 0.430371
[epoch6, step360]: loss 0.708447
[epoch6, step361]: loss 0.647393
[epoch6, step362]: loss 0.177727
[epoch6, step363]: loss 0.650145
[epoch6, step364]: loss 0.620793
[epoch6, step365]: loss 0.520757
[epoch6, step366]: loss 0.663453
[epoch6, step367]: loss 0.443788
[epoch6, step368]: loss 0.610835
[epoch6, step369]: loss 0.353328
[epoch6, step370]: loss 0.477930
[epoch6, step371]: loss 0.609778
[epoch6, step372]: loss 0.487171
[epoch6, step373]: loss 0.630626
[epoch6, step374]: loss 0.389754
[epoch6, step375]: loss 0.349002
[epoch6, step376]: loss 0.495167
[epoch6, step377]: loss 0.575006
[epoch6, step378]: loss 0.759304
[epoch6, step379]: loss 0.527330
[epoch6, step380]: loss 0.365996
[epoch6, step381]: loss 0.676767
[epoch6, step382]: loss 0.587093
[epoch6, step383]: loss 0.351382
[epoch6, step384]: loss 0.500387
[epoch6, step385]: loss 0.693381
[epoch6, step386]: loss 0.742551
[epoch6, step387]: loss 0.531775
[epoch6, step388]: loss 0.538708
[epoch6, step389]: loss 0.485818
[epoch6, step390]: loss 0.824558
[epoch6, step391]: loss 0.545555
[epoch6, step392]: loss 0.580861
[epoch6, step393]: loss 0.369807
[epoch6, step394]: loss 0.597163
[epoch6, step395]: loss 0.530036
[epoch6, step396]: loss 0.412098
[epoch6, step397]: loss 0.703452
[epoch6, step398]: loss 0.716718
[epoch6, step399]: loss 0.551609
[epoch6, step400]: loss 0.312991
[epoch6, step401]: loss 0.573705
[epoch6, step402]: loss 0.723531
[epoch6, step403]: loss 0.459839
[epoch6, step404]: loss 0.397018
[epoch6, step405]: loss 0.316034
[epoch6, step406]: loss 0.501274
[epoch6, step407]: loss 0.758464
[epoch6, step408]: loss 0.579506
[epoch6, step409]: loss 0.440448
[epoch6, step410]: loss 0.781850
[epoch6, step411]: loss 0.500851
[epoch6, step412]: loss 0.393896
[epoch6, step413]: loss 0.450370
[epoch6, step414]: loss 0.699209
[epoch6, step415]: loss 0.449293
[epoch6, step416]: loss 0.529567
[epoch6, step417]: loss 0.294705
[epoch6, step418]: loss 0.328242
[epoch6, step419]: loss 0.367849
[epoch6, step420]: loss 0.520721
[epoch6, step421]: loss 0.741192
[epoch6, step422]: loss 0.459349
[epoch6, step423]: loss 0.481167
[epoch6, step424]: loss 0.178762
[epoch6, step425]: loss 0.506253
[epoch6, step426]: loss 0.612381
[epoch6, step427]: loss 0.615658
[epoch6, step428]: loss 0.422795
[epoch6, step429]: loss 0.541821
[epoch6, step430]: loss 0.554131
[epoch6, step431]: loss 0.425562
[epoch6, step432]: loss 0.418565
[epoch6, step433]: loss 0.588118
[epoch6, step434]: loss 0.704803
[epoch6, step435]: loss 0.528819
[epoch6, step436]: loss 0.759342
[epoch6, step437]: loss 0.542818
[epoch6, step438]: loss 0.524049
[epoch6, step439]: loss 0.623441
[epoch6, step440]: loss 0.465799
[epoch6, step441]: loss 0.625860
[epoch6, step442]: loss 0.876693
[epoch6, step443]: loss 0.536428
[epoch6, step444]: loss 0.701278
[epoch6, step445]: loss 0.655414
[epoch6, step446]: loss 0.553126
[epoch6, step447]: loss 0.567757
[epoch6, step448]: loss 0.285762
[epoch6, step449]: loss 0.542489
[epoch6, step450]: loss 0.564307
[epoch6, step451]: loss 0.671894
[epoch6, step452]: loss 0.693140
[epoch6, step453]: loss 0.410724
[epoch6, step454]: loss 0.545247
[epoch6, step455]: loss 0.600882
[epoch6, step456]: loss 0.566654
[epoch6, step457]: loss 0.376496
[epoch6, step458]: loss 0.583125
[epoch6, step459]: loss 0.730776
[epoch6, step460]: loss 0.414235
[epoch6, step461]: loss 0.315097
[epoch6, step462]: loss 0.799514
[epoch6, step463]: loss 0.601756
[epoch6, step464]: loss 0.852393
[epoch6, step465]: loss 0.635258
[epoch6, step466]: loss 0.562520
[epoch6, step467]: loss 0.548970
[epoch6, step468]: loss 0.457075
[epoch6, step469]: loss 0.534523
[epoch6, step470]: loss 0.639454
[epoch6, step471]: loss 0.792807
[epoch6, step472]: loss 0.620234
[epoch6, step473]: loss 0.663441
[epoch6, step474]: loss 0.539558
[epoch6, step475]: loss 0.597841
[epoch6, step476]: loss 0.751912
[epoch6, step477]: loss 0.581947
[epoch6, step478]: loss 0.569144
[epoch6, step479]: loss 0.261611
[epoch6, step480]: loss 0.429209
[epoch6, step481]: loss 0.699954
[epoch6, step482]: loss 0.638110
[epoch6, step483]: loss 0.634383
[epoch6, step484]: loss 0.464800
[epoch6, step485]: loss 0.658714
[epoch6, step486]: loss 0.584849
[epoch6, step487]: loss 0.691206
[epoch6, step488]: loss 0.739332
[epoch6, step489]: loss 0.281670
[epoch6, step490]: loss 0.763788
[epoch6, step491]: loss 0.728554
[epoch6, step492]: loss 0.395293
[epoch6, step493]: loss 0.648894
[epoch6, step494]: loss 0.348341
[epoch6, step495]: loss 0.370484
[epoch6, step496]: loss 0.355830
[epoch6, step497]: loss 0.556960
[epoch6, step498]: loss 0.689158
[epoch6, step499]: loss 0.468499
[epoch6, step500]: loss 0.575428
[epoch6, step501]: loss 0.806150
[epoch6, step502]: loss 0.746155
[epoch6, step503]: loss 0.540932
[epoch6, step504]: loss 0.350894
[epoch6, step505]: loss 0.568486
[epoch6, step506]: loss 0.565215
[epoch6, step507]: loss 0.533714
[epoch6, step508]: loss 0.396775
[epoch6, step509]: loss 0.650096
[epoch6, step510]: loss 0.468843
[epoch6, step511]: loss 0.626370
[epoch6, step512]: loss 0.567586
[epoch6, step513]: loss 0.715474
[epoch6, step514]: loss 0.309850
[epoch6, step515]: loss 0.747672
[epoch6, step516]: loss 0.501981
[epoch6, step517]: loss 0.387804
[epoch6, step518]: loss 0.559370
[epoch6, step519]: loss 0.360480
[epoch6, step520]: loss 0.567053
[epoch6, step521]: loss 0.539380
[epoch6, step522]: loss 0.473835
[epoch6, step523]: loss 0.409690
[epoch6, step524]: loss 0.499543
[epoch6, step525]: loss 0.486069
[epoch6, step526]: loss 0.406946
[epoch6, step527]: loss 0.534389
[epoch6, step528]: loss 0.529367
[epoch6, step529]: loss 0.586316
[epoch6, step530]: loss 0.472316
[epoch6, step531]: loss 0.562799
[epoch6, step532]: loss 0.734981
[epoch6, step533]: loss 0.661470
[epoch6, step534]: loss 0.198700
[epoch6, step535]: loss 0.741356
[epoch6, step536]: loss 0.524530
[epoch6, step537]: loss 0.734818
[epoch6, step538]: loss 0.473754
[epoch6, step539]: loss 0.571408
[epoch6, step540]: loss 0.681230
[epoch6, step541]: loss 0.526000
[epoch6, step542]: loss 0.475869
[epoch6, step543]: loss 0.396696
[epoch6, step544]: loss 0.590884
[epoch6, step545]: loss 0.674172
[epoch6, step546]: loss 0.596684
[epoch6, step547]: loss 0.616856
[epoch6, step548]: loss 0.631440
[epoch6, step549]: loss 0.642935
[epoch6, step550]: loss 0.548185
[epoch6, step551]: loss 0.781744
[epoch6, step552]: loss 0.616456
[epoch6, step553]: loss 0.785448
[epoch6, step554]: loss 0.562190
[epoch6, step555]: loss 0.434842
[epoch6, step556]: loss 0.514873
[epoch6, step557]: loss 0.283648
[epoch6, step558]: loss 0.471544
[epoch6, step559]: loss 0.554836
[epoch6, step560]: loss 0.320091
[epoch6, step561]: loss 0.570622
[epoch6, step562]: loss 0.702432
[epoch6, step563]: loss 0.667513
[epoch6, step564]: loss 0.743397
[epoch6, step565]: loss 0.327218
[epoch6, step566]: loss 0.666543
[epoch6, step567]: loss 0.609353
[epoch6, step568]: loss 0.693107
[epoch6, step569]: loss 0.585082
[epoch6, step570]: loss 0.494026
[epoch6, step571]: loss 0.315533
[epoch6, step572]: loss 0.604714
[epoch6, step573]: loss 0.593528
[epoch6, step574]: loss 0.444844
[epoch6, step575]: loss 0.551347
[epoch6, step576]: loss 0.624275
[epoch6, step577]: loss 0.383403
[epoch6, step578]: loss 0.495172
[epoch6, step579]: loss 0.532667
[epoch6, step580]: loss 0.470880
[epoch6, step581]: loss 0.462190
[epoch6, step582]: loss 0.523945
[epoch6, step583]: loss 0.590214
[epoch6, step584]: loss 0.683892
[epoch6, step585]: loss 0.769399
[epoch6, step586]: loss 0.372179
[epoch6, step587]: loss 0.697820
[epoch6, step588]: loss 0.800787
[epoch6, step589]: loss 0.681260
[epoch6, step590]: loss 0.576084
[epoch6, step591]: loss 0.696622
[epoch6, step592]: loss 0.493841
[epoch6, step593]: loss 0.659869
[epoch6, step594]: loss 0.431835
[epoch6, step595]: loss 0.595373
[epoch6, step596]: loss 0.558120
[epoch6, step597]: loss 0.434330
[epoch6, step598]: loss 0.781530
[epoch6, step599]: loss 0.322460
[epoch6, step600]: loss 0.755197
[epoch6, step601]: loss 0.728524
[epoch6, step602]: loss 0.490709
[epoch6, step603]: loss 0.471470
[epoch6, step604]: loss 0.495961
[epoch6, step605]: loss 0.503839
[epoch6, step606]: loss 0.588625
[epoch6, step607]: loss 0.578602
[epoch6, step608]: loss 0.345263
[epoch6, step609]: loss 0.551509
[epoch6, step610]: loss 0.736789
[epoch6, step611]: loss 0.758719
[epoch6, step612]: loss 0.324646
[epoch6, step613]: loss 0.300480
[epoch6, step614]: loss 0.536874
[epoch6, step615]: loss 0.775180
[epoch6, step616]: loss 0.324894
[epoch6, step617]: loss 0.574628
[epoch6, step618]: loss 0.691181
[epoch6, step619]: loss 0.345190
[epoch6, step620]: loss 0.497361
[epoch6, step621]: loss 0.438988
[epoch6, step622]: loss 0.424374
[epoch6, step623]: loss 0.380751
[epoch6, step624]: loss 0.446527
[epoch6, step625]: loss 0.469152
[epoch6, step626]: loss 0.572484
[epoch6, step627]: loss 0.960290
[epoch6, step628]: loss 0.259169
[epoch6, step629]: loss 0.414333
[epoch6, step630]: loss 0.531634
[epoch6, step631]: loss 0.643648
[epoch6, step632]: loss 0.797068
[epoch6, step633]: loss 0.497394
[epoch6, step634]: loss 0.548020
[epoch6, step635]: loss 0.530905
[epoch6, step636]: loss 0.501420
[epoch6, step637]: loss 0.542008
[epoch6, step638]: loss 0.546939
[epoch6, step639]: loss 0.441821
[epoch6, step640]: loss 0.715258
[epoch6, step641]: loss 0.359506
[epoch6, step642]: loss 0.263154
[epoch6, step643]: loss 0.595976
[epoch6, step644]: loss 0.863533
[epoch6, step645]: loss 0.473883
[epoch6, step646]: loss 0.449946
[epoch6, step647]: loss 0.680185
[epoch6, step648]: loss 0.748591
[epoch6, step649]: loss 0.385457
[epoch6, step650]: loss 0.472958
[epoch6, step651]: loss 0.695544
[epoch6, step652]: loss 0.647290
[epoch6, step653]: loss 0.514226
[epoch6, step654]: loss 0.625346
[epoch6, step655]: loss 0.784111
[epoch6, step656]: loss 0.514625
[epoch6, step657]: loss 0.463270
[epoch6, step658]: loss 0.468216
[epoch6, step659]: loss 0.613015
[epoch6, step660]: loss 0.669427
[epoch6, step661]: loss 0.548082
[epoch6, step662]: loss 0.737598
[epoch6, step663]: loss 0.731275
[epoch6, step664]: loss 0.635101
[epoch6, step665]: loss 0.525179
[epoch6, step666]: loss 0.632108
[epoch6, step667]: loss 0.663237
[epoch6, step668]: loss 0.748459
[epoch6, step669]: loss 0.210556
[epoch6, step670]: loss 0.438674
[epoch6, step671]: loss 0.642361
[epoch6, step672]: loss 0.392570
[epoch6, step673]: loss 0.769741
[epoch6, step674]: loss 0.329875
[epoch6, step675]: loss 0.413119
[epoch6, step676]: loss 0.330645
[epoch6, step677]: loss 0.560691
[epoch6, step678]: loss 0.562510
[epoch6, step679]: loss 0.579503
[epoch6, step680]: loss 0.459569
[epoch6, step681]: loss 0.674041
[epoch6, step682]: loss 0.553451
[epoch6, step683]: loss 0.610829
[epoch6, step684]: loss 0.662093
[epoch6, step685]: loss 0.488835
[epoch6, step686]: loss 0.416619
[epoch6, step687]: loss 0.328788
[epoch6, step688]: loss 0.374838
[epoch6, step689]: loss 0.492077
[epoch6, step690]: loss 0.657183
[epoch6, step691]: loss 0.680881
[epoch6, step692]: loss 0.611124
[epoch6, step693]: loss 0.549237
[epoch6, step694]: loss 0.558456
[epoch6, step695]: loss 0.508906
[epoch6, step696]: loss 0.515278
[epoch6, step697]: loss 0.379798
[epoch6, step698]: loss 0.634652
[epoch6, step699]: loss 0.410546
[epoch6, step700]: loss 0.545647
[epoch6, step701]: loss 0.431181
[epoch6, step702]: loss 0.519991
[epoch6, step703]: loss 0.489111
[epoch6, step704]: loss 0.708259
[epoch6, step705]: loss 0.332543
[epoch6, step706]: loss 0.469465
[epoch6, step707]: loss 0.621154
[epoch6, step708]: loss 0.512260
[epoch6, step709]: loss 0.684424
[epoch6, step710]: loss 0.683572
[epoch6, step711]: loss 0.397623
[epoch6, step712]: loss 0.635285
[epoch6, step713]: loss 0.629531
[epoch6, step714]: loss 0.847140
[epoch6, step715]: loss 0.400680
[epoch6, step716]: loss 0.311868
[epoch6, step717]: loss 0.449354
[epoch6, step718]: loss 0.659361
[epoch6, step719]: loss 0.546358
[epoch6, step720]: loss 0.597746
[epoch6, step721]: loss 0.277621
[epoch6, step722]: loss 0.754200
[epoch6, step723]: loss 0.390100
[epoch6, step724]: loss 0.560950
[epoch6, step725]: loss 0.383650
[epoch6, step726]: loss 0.722895
[epoch6, step727]: loss 0.634165
[epoch6, step728]: loss 0.437105
[epoch6, step729]: loss 0.586446
[epoch6, step730]: loss 0.781505
[epoch6, step731]: loss 0.732687
[epoch6, step732]: loss 0.410787
[epoch6, step733]: loss 0.642972
[epoch6, step734]: loss 0.417468
[epoch6, step735]: loss 0.773617
[epoch6, step736]: loss 0.668499
[epoch6, step737]: loss 0.723263
[epoch6, step738]: loss 0.482517
[epoch6, step739]: loss 0.782885
[epoch6, step740]: loss 0.278017
[epoch6, step741]: loss 0.728173
[epoch6, step742]: loss 0.741690
[epoch6, step743]: loss 0.659068
[epoch6, step744]: loss 0.784209
[epoch6, step745]: loss 0.975372
[epoch6, step746]: loss 0.714113
[epoch6, step747]: loss 0.664514
[epoch6, step748]: loss 0.644259
[epoch6, step749]: loss 0.656175
[epoch6, step750]: loss 0.316291
[epoch6, step751]: loss 0.518911
[epoch6, step752]: loss 0.489042
[epoch6, step753]: loss 0.684511
[epoch6, step754]: loss 0.441749
[epoch6, step755]: loss 0.504238
[epoch6, step756]: loss 0.566505
[epoch6, step757]: loss 0.699294
[epoch6, step758]: loss 0.635285
[epoch6, step759]: loss 0.713897
[epoch6, step760]: loss 0.402328
[epoch6, step761]: loss 0.522785
[epoch6, step762]: loss 0.655057
[epoch6, step763]: loss 0.680367
[epoch6, step764]: loss 0.478970
[epoch6, step765]: loss 0.490965
[epoch6, step766]: loss 0.769716
[epoch6, step767]: loss 0.467759
[epoch6, step768]: loss 0.464436
[epoch6, step769]: loss 0.791934
[epoch6, step770]: loss 0.461389
[epoch6, step771]: loss 0.482108
[epoch6, step772]: loss 0.497020
[epoch6, step773]: loss 0.690588
[epoch6, step774]: loss 0.708082
[epoch6, step775]: loss 0.683722
[epoch6, step776]: loss 0.554588
[epoch6, step777]: loss 0.302736
[epoch6, step778]: loss 0.257481
[epoch6, step779]: loss 0.533376
[epoch6, step780]: loss 0.251122
[epoch6, step781]: loss 0.620037
[epoch6, step782]: loss 0.205639
[epoch6, step783]: loss 0.769058
[epoch6, step784]: loss 0.707064
[epoch6, step785]: loss 0.819703
[epoch6, step786]: loss 0.359375
[epoch6, step787]: loss 0.555874
[epoch6, step788]: loss 0.771082
[epoch6, step789]: loss 0.759971
[epoch6, step790]: loss 0.645081
[epoch6, step791]: loss 0.360071
[epoch6, step792]: loss 0.683184
[epoch6, step793]: loss 0.541132
[epoch6, step794]: loss 0.674558
[epoch6, step795]: loss 0.478148
[epoch6, step796]: loss 0.402341
[epoch6, step797]: loss 0.750451
[epoch6, step798]: loss 0.493879
[epoch6, step799]: loss 0.562186
[epoch6, step800]: loss 0.511513
[epoch6, step801]: loss 0.405317
[epoch6, step802]: loss 0.705231
[epoch6, step803]: loss 0.342595
[epoch6, step804]: loss 0.344898
[epoch6, step805]: loss 0.478735
[epoch6, step806]: loss 0.460226
[epoch6, step807]: loss 0.272951
[epoch6, step808]: loss 0.522334
[epoch6, step809]: loss 0.723510
[epoch6, step810]: loss 0.453443
[epoch6, step811]: loss 0.643426
[epoch6, step812]: loss 0.200929
[epoch6, step813]: loss 0.512295
[epoch6, step814]: loss 0.702168
[epoch6, step815]: loss 0.689021
[epoch6, step816]: loss 0.601038
[epoch6, step817]: loss 0.262132
[epoch6, step818]: loss 0.423589
[epoch6, step819]: loss 0.708201
[epoch6, step820]: loss 0.601802
[epoch6, step821]: loss 0.484590
[epoch6, step822]: loss 0.536407
[epoch6, step823]: loss 0.564408
[epoch6, step824]: loss 0.664222
[epoch6, step825]: loss 0.626432
[epoch6, step826]: loss 0.462554
[epoch6, step827]: loss 0.539710
[epoch6, step828]: loss 0.526445
[epoch6, step829]: loss 0.611024
[epoch6, step830]: loss 0.643982
[epoch6, step831]: loss 0.533986
[epoch6, step832]: loss 0.485937
[epoch6, step833]: loss 0.923609
[epoch6, step834]: loss 0.582474
[epoch6, step835]: loss 0.611426
[epoch6, step836]: loss 0.519166
[epoch6, step837]: loss 0.586173
[epoch6, step838]: loss 0.535194
[epoch6, step839]: loss 0.641892
[epoch6, step840]: loss 0.654121
[epoch6, step841]: loss 0.523843
[epoch6, step842]: loss 0.421591
[epoch6, step843]: loss 0.594878
[epoch6, step844]: loss 0.587433
[epoch6, step845]: loss 0.283620
[epoch6, step846]: loss 0.447747
[epoch6, step847]: loss 0.483374
[epoch6, step848]: loss 0.551367
[epoch6, step849]: loss 0.750194
[epoch6, step850]: loss 0.563331
[epoch6, step851]: loss 0.660061
[epoch6, step852]: loss 0.340195
[epoch6, step853]: loss 0.423939
[epoch6, step854]: loss 0.552550
[epoch6, step855]: loss 0.431609
[epoch6, step856]: loss 0.510442
[epoch6, step857]: loss 0.475609
[epoch6, step858]: loss 0.699838
[epoch6, step859]: loss 0.596272
[epoch6, step860]: loss 0.250926
[epoch6, step861]: loss 0.574193
[epoch6, step862]: loss 0.308054
[epoch6, step863]: loss 0.342415
[epoch6, step864]: loss 0.730215
[epoch6, step865]: loss 0.440859
[epoch6, step866]: loss 0.310881
[epoch6, step867]: loss 0.692254
[epoch6, step868]: loss 0.618360
[epoch6, step869]: loss 0.554942
[epoch6, step870]: loss 0.776532
[epoch6, step871]: loss 0.471854
[epoch6, step872]: loss 0.652227
[epoch6, step873]: loss 0.517281
[epoch6, step874]: loss 0.681226
[epoch6, step875]: loss 0.737048
[epoch6, step876]: loss 0.429711
[epoch6, step877]: loss 0.351858
[epoch6, step878]: loss 0.424473
[epoch6, step879]: loss 0.677216
[epoch6, step880]: loss 0.616757
[epoch6, step881]: loss 0.578173
[epoch6, step882]: loss 0.530534
[epoch6, step883]: loss 0.426334
[epoch6, step884]: loss 0.670774
[epoch6, step885]: loss 0.374148
[epoch6, step886]: loss 0.643336
[epoch6, step887]: loss 0.551998
[epoch6, step888]: loss 0.446590
[epoch6, step889]: loss 0.373819
[epoch6, step890]: loss 0.581033
[epoch6, step891]: loss 0.443679
[epoch6, step892]: loss 0.515487
[epoch6, step893]: loss 0.592308
[epoch6, step894]: loss 0.389135
[epoch6, step895]: loss 0.683590
[epoch6, step896]: loss 0.592203
[epoch6, step897]: loss 0.749643
[epoch6, step898]: loss 0.453013
[epoch6, step899]: loss 0.396694
[epoch6, step900]: loss 0.700215
[epoch6, step901]: loss 0.543889
[epoch6, step902]: loss 0.656575
[epoch6, step903]: loss 0.457822
[epoch6, step904]: loss 0.451242
[epoch6, step905]: loss 0.393448
[epoch6, step906]: loss 0.729308
[epoch6, step907]: loss 0.442524
[epoch6, step908]: loss 0.623076
[epoch6, step909]: loss 0.600271
[epoch6, step910]: loss 0.536413
[epoch6, step911]: loss 0.524619
[epoch6, step912]: loss 0.548791
[epoch6, step913]: loss 0.571538
[epoch6, step914]: loss 0.523855
[epoch6, step915]: loss 0.747970
[epoch6, step916]: loss 0.642224
[epoch6, step917]: loss 0.341626
[epoch6, step918]: loss 0.488832
[epoch6, step919]: loss 0.598703
[epoch6, step920]: loss 0.469850
[epoch6, step921]: loss 0.612676
[epoch6, step922]: loss 0.643252
[epoch6, step923]: loss 0.457232
[epoch6, step924]: loss 0.640557
[epoch6, step925]: loss 0.442255
[epoch6, step926]: loss 0.689552
[epoch6, step927]: loss 0.610164
[epoch6, step928]: loss 0.501971
[epoch6, step929]: loss 0.635332
[epoch6, step930]: loss 0.476767
[epoch6, step931]: loss 0.340834
[epoch6, step932]: loss 0.687178
[epoch6, step933]: loss 0.646191
[epoch6, step934]: loss 0.716708
[epoch6, step935]: loss 0.513543
[epoch6, step936]: loss 0.553522
[epoch6, step937]: loss 0.627610
[epoch6, step938]: loss 0.305985
[epoch6, step939]: loss 0.605594
[epoch6, step940]: loss 0.851607
[epoch6, step941]: loss 0.301626
[epoch6, step942]: loss 0.594013
[epoch6, step943]: loss 0.423350
[epoch6, step944]: loss 0.605006
[epoch6, step945]: loss 0.506395
[epoch6, step946]: loss 0.687047
[epoch6, step947]: loss 0.682362
[epoch6, step948]: loss 0.662530
[epoch6, step949]: loss 0.480037
[epoch6, step950]: loss 0.611338
[epoch6, step951]: loss 0.593695
[epoch6, step952]: loss 0.649302
[epoch6, step953]: loss 0.451634
[epoch6, step954]: loss 0.717641
[epoch6, step955]: loss 0.395428
[epoch6, step956]: loss 0.482451
[epoch6, step957]: loss 0.321950
[epoch6, step958]: loss 0.482000
[epoch6, step959]: loss 0.697533
[epoch6, step960]: loss 0.749060
[epoch6, step961]: loss 0.572590
[epoch6, step962]: loss 0.576732
[epoch6, step963]: loss 0.395902
[epoch6, step964]: loss 0.434122
[epoch6, step965]: loss 0.560656
[epoch6, step966]: loss 0.610204
[epoch6, step967]: loss 0.323238
[epoch6, step968]: loss 0.465709
[epoch6, step969]: loss 0.613316
[epoch6, step970]: loss 0.385874
[epoch6, step971]: loss 0.426063
[epoch6, step972]: loss 0.391348
[epoch6, step973]: loss 0.738547
[epoch6, step974]: loss 0.763659
[epoch6, step975]: loss 0.526104
[epoch6, step976]: loss 0.280564
[epoch6, step977]: loss 0.623701
[epoch6, step978]: loss 0.526820
[epoch6, step979]: loss 0.419420
[epoch6, step980]: loss 0.414778
[epoch6, step981]: loss 0.709404
[epoch6, step982]: loss 0.520874
[epoch6, step983]: loss 0.408486
[epoch6, step984]: loss 0.658119
[epoch6, step985]: loss 0.654168
[epoch6, step986]: loss 0.410895
[epoch6, step987]: loss 0.338632
[epoch6, step988]: loss 0.724507
[epoch6, step989]: loss 0.622120
[epoch6, step990]: loss 0.463814
[epoch6, step991]: loss 0.499616
[epoch6, step992]: loss 0.403507
[epoch6, step993]: loss 0.292343
[epoch6, step994]: loss 0.686554
[epoch6, step995]: loss 0.410162
[epoch6, step996]: loss 0.412064
[epoch6, step997]: loss 0.723065
[epoch6, step998]: loss 0.423947
[epoch6, step999]: loss 0.510190
[epoch6, step1000]: loss 0.778232
[epoch6, step1001]: loss 0.801569
[epoch6, step1002]: loss 0.604218
[epoch6, step1003]: loss 0.572282
[epoch6, step1004]: loss 0.476354
[epoch6, step1005]: loss 0.659008
[epoch6, step1006]: loss 0.753126
[epoch6, step1007]: loss 0.721739
[epoch6, step1008]: loss 0.669649
[epoch6, step1009]: loss 0.386496
[epoch6, step1010]: loss 0.635642
[epoch6, step1011]: loss 0.521436
[epoch6, step1012]: loss 0.496933
[epoch6, step1013]: loss 0.802247
[epoch6, step1014]: loss 0.400799
[epoch6, step1015]: loss 0.657634
[epoch6, step1016]: loss 0.304994
[epoch6, step1017]: loss 0.498365
[epoch6, step1018]: loss 0.486562
[epoch6, step1019]: loss 0.523998
[epoch6, step1020]: loss 0.591530
[epoch6, step1021]: loss 0.515435
[epoch6, step1022]: loss 0.238297
[epoch6, step1023]: loss 0.527174
[epoch6, step1024]: loss 0.499213
[epoch6, step1025]: loss 0.615407
[epoch6, step1026]: loss 0.528006
[epoch6, step1027]: loss 0.619792
[epoch6, step1028]: loss 0.437427
[epoch6, step1029]: loss 0.592232
[epoch6, step1030]: loss 0.630531
[epoch6, step1031]: loss 0.550855
[epoch6, step1032]: loss 0.538889
[epoch6, step1033]: loss 0.496820
[epoch6, step1034]: loss 0.440446
[epoch6, step1035]: loss 0.506628
[epoch6, step1036]: loss 0.614763
[epoch6, step1037]: loss 0.692561
[epoch6, step1038]: loss 0.591064
[epoch6, step1039]: loss 0.428263
[epoch6, step1040]: loss 0.551574
[epoch6, step1041]: loss 0.576066
[epoch6, step1042]: loss 0.525531
[epoch6, step1043]: loss 0.293168
[epoch6, step1044]: loss 0.714936
[epoch6, step1045]: loss 0.601726
[epoch6, step1046]: loss 0.640428
[epoch6, step1047]: loss 0.782433
[epoch6, step1048]: loss 0.561538
[epoch6, step1049]: loss 0.657735
[epoch6, step1050]: loss 0.754652
[epoch6, step1051]: loss 0.600792
[epoch6, step1052]: loss 0.389939
[epoch6, step1053]: loss 0.614863
[epoch6, step1054]: loss 0.589059
[epoch6, step1055]: loss 0.535920
[epoch6, step1056]: loss 0.506489
[epoch6, step1057]: loss 0.685193
[epoch6, step1058]: loss 0.840586
[epoch6, step1059]: loss 0.683294
[epoch6, step1060]: loss 0.608605
[epoch6, step1061]: loss 0.348422
[epoch6, step1062]: loss 0.660820
[epoch6, step1063]: loss 0.630409
[epoch6, step1064]: loss 0.578262
[epoch6, step1065]: loss 0.513495
[epoch6, step1066]: loss 0.551533
[epoch6, step1067]: loss 0.538685
[epoch6, step1068]: loss 0.368021
[epoch6, step1069]: loss 0.678253
[epoch6, step1070]: loss 0.543468
[epoch6, step1071]: loss 0.643047
[epoch6, step1072]: loss 0.454568
[epoch6, step1073]: loss 0.494028
[epoch6, step1074]: loss 0.435556
[epoch6, step1075]: loss 0.482081
[epoch6, step1076]: loss 0.577937
[epoch6, step1077]: loss 0.643890
[epoch6, step1078]: loss 0.533735
[epoch6, step1079]: loss 0.482152
[epoch6, step1080]: loss 0.411564
[epoch6, step1081]: loss 0.532610
[epoch6, step1082]: loss 0.572810
[epoch6, step1083]: loss 0.548487
[epoch6, step1084]: loss 0.798359
[epoch6, step1085]: loss 0.581565
[epoch6, step1086]: loss 0.637190
[epoch6, step1087]: loss 0.639017
[epoch6, step1088]: loss 0.341753
[epoch6, step1089]: loss 0.433021
[epoch6, step1090]: loss 0.623978
[epoch6, step1091]: loss 0.704068
[epoch6, step1092]: loss 0.392985
[epoch6, step1093]: loss 0.596299
[epoch6, step1094]: loss 0.354766
[epoch6, step1095]: loss 0.413540
[epoch6, step1096]: loss 0.435516
[epoch6, step1097]: loss 0.568503
[epoch6, step1098]: loss 0.505624
[epoch6, step1099]: loss 0.593624
[epoch6, step1100]: loss 0.384740
[epoch6, step1101]: loss 0.607353
[epoch6, step1102]: loss 0.665715
[epoch6, step1103]: loss 0.746777
[epoch6, step1104]: loss 0.413596
[epoch6, step1105]: loss 0.654602
[epoch6, step1106]: loss 0.784511
[epoch6, step1107]: loss 0.581872
[epoch6, step1108]: loss 0.275043
[epoch6, step1109]: loss 0.473915
[epoch6, step1110]: loss 0.468445
[epoch6, step1111]: loss 0.684578
[epoch6, step1112]: loss 0.445321
[epoch6, step1113]: loss 0.641026
[epoch6, step1114]: loss 0.671731
[epoch6, step1115]: loss 0.812342
[epoch6, step1116]: loss 0.241816
[epoch6, step1117]: loss 0.556080
[epoch6, step1118]: loss 0.334986
[epoch6, step1119]: loss 0.502552
[epoch6, step1120]: loss 0.358767
[epoch6, step1121]: loss 0.424605
[epoch6, step1122]: loss 0.565958
[epoch6, step1123]: loss 0.794571
[epoch6, step1124]: loss 0.482899
[epoch6, step1125]: loss 0.511196
[epoch6, step1126]: loss 0.576662
[epoch6, step1127]: loss 0.601975
[epoch6, step1128]: loss 0.580767
[epoch6, step1129]: loss 0.372395
[epoch6, step1130]: loss 0.567441
[epoch6, step1131]: loss 0.631791
[epoch6, step1132]: loss 0.389464
[epoch6, step1133]: loss 0.736264
[epoch6, step1134]: loss 0.169958
[epoch6, step1135]: loss 0.371689
[epoch6, step1136]: loss 0.453991
[epoch6, step1137]: loss 0.743647
[epoch6, step1138]: loss 0.707338
[epoch6, step1139]: loss 0.479302
[epoch6, step1140]: loss 0.283965
[epoch6, step1141]: loss 0.836105
[epoch6, step1142]: loss 0.568632
[epoch6, step1143]: loss 0.718506
[epoch6, step1144]: loss 0.698395
[epoch6, step1145]: loss 0.526045
[epoch6, step1146]: loss 0.542684
[epoch6, step1147]: loss 0.771237
[epoch6, step1148]: loss 0.589506
[epoch6, step1149]: loss 0.442103
[epoch6, step1150]: loss 0.621068
[epoch6, step1151]: loss 0.377070
[epoch6, step1152]: loss 0.646483
[epoch6, step1153]: loss 0.564506
[epoch6, step1154]: loss 0.532719
[epoch6, step1155]: loss 0.513265
[epoch6, step1156]: loss 0.692253
[epoch6, step1157]: loss 0.572321
[epoch6, step1158]: loss 0.499584
[epoch6, step1159]: loss 0.357630
[epoch6, step1160]: loss 0.565733
[epoch6, step1161]: loss 0.566103
[epoch6, step1162]: loss 0.597857
[epoch6, step1163]: loss 0.176508
[epoch6, step1164]: loss 0.600881
[epoch6, step1165]: loss 0.743209
[epoch6, step1166]: loss 0.716411
[epoch6, step1167]: loss 0.478587
[epoch6, step1168]: loss 0.587350
[epoch6, step1169]: loss 0.454238
[epoch6, step1170]: loss 0.631266
[epoch6, step1171]: loss 0.668123
[epoch6, step1172]: loss 0.621458
[epoch6, step1173]: loss 0.657808
[epoch6, step1174]: loss 0.639357
[epoch6, step1175]: loss 0.754669
[epoch6, step1176]: loss 0.647190
[epoch6, step1177]: loss 0.645509
[epoch6, step1178]: loss 0.277124
[epoch6, step1179]: loss 0.428003
[epoch6, step1180]: loss 0.432974
[epoch6, step1181]: loss 0.292714
[epoch6, step1182]: loss 0.501530
[epoch6, step1183]: loss 0.674361
[epoch6, step1184]: loss 0.533916
[epoch6, step1185]: loss 0.666729
[epoch6, step1186]: loss 0.780434
[epoch6, step1187]: loss 0.710353
[epoch6, step1188]: loss 0.452084
[epoch6, step1189]: loss 0.488366
[epoch6, step1190]: loss 0.591345
[epoch6, step1191]: loss 0.531533
[epoch6, step1192]: loss 0.454727
[epoch6, step1193]: loss 0.487375
[epoch6, step1194]: loss 0.327273
[epoch6, step1195]: loss 0.341044
[epoch6, step1196]: loss 0.474208
[epoch6, step1197]: loss 0.390382
[epoch6, step1198]: loss 0.398739
[epoch6, step1199]: loss 0.492246
[epoch6, step1200]: loss 0.435766
[epoch6, step1201]: loss 0.598014
[epoch6, step1202]: loss 0.687574
[epoch6, step1203]: loss 0.414972
[epoch6, step1204]: loss 0.576642
[epoch6, step1205]: loss 0.683311
[epoch6, step1206]: loss 0.395867
[epoch6, step1207]: loss 0.695700
[epoch6, step1208]: loss 0.698372
[epoch6, step1209]: loss 0.583857
[epoch6, step1210]: loss 0.460846
[epoch6, step1211]: loss 0.645222
[epoch6, step1212]: loss 0.729119
[epoch6, step1213]: loss 0.658766
[epoch6, step1214]: loss 0.517238
[epoch6, step1215]: loss 0.654834
[epoch6, step1216]: loss 0.471133
[epoch6, step1217]: loss 0.595132
[epoch6, step1218]: loss 0.560746
[epoch6, step1219]: loss 0.451334
[epoch6, step1220]: loss 0.570692
[epoch6, step1221]: loss 0.352737
[epoch6, step1222]: loss 0.692218
[epoch6, step1223]: loss 0.492563
[epoch6, step1224]: loss 0.570948
[epoch6, step1225]: loss 0.343497
[epoch6, step1226]: loss 0.712204
[epoch6, step1227]: loss 0.607352
[epoch6, step1228]: loss 0.646813
[epoch6, step1229]: loss 0.623841
[epoch6, step1230]: loss 0.607411
[epoch6, step1231]: loss 0.710498
[epoch6, step1232]: loss 0.470025
[epoch6, step1233]: loss 0.588786
[epoch6, step1234]: loss 0.642319
[epoch6, step1235]: loss 0.586997
[epoch6, step1236]: loss 0.609619
[epoch6, step1237]: loss 0.609319
[epoch6, step1238]: loss 0.417763
[epoch6, step1239]: loss 0.517302
[epoch6, step1240]: loss 0.806337
[epoch6, step1241]: loss 0.259324
[epoch6, step1242]: loss 0.322513
[epoch6, step1243]: loss 0.626747
[epoch6, step1244]: loss 0.485113
[epoch6, step1245]: loss 0.653063
[epoch6, step1246]: loss 0.305996
[epoch6, step1247]: loss 0.773819
[epoch6, step1248]: loss 0.571185
[epoch6, step1249]: loss 0.605255
[epoch6, step1250]: loss 0.676427
[epoch6, step1251]: loss 0.565835
[epoch6, step1252]: loss 0.454315
[epoch6, step1253]: loss 0.797364
[epoch6, step1254]: loss 0.552678
[epoch6, step1255]: loss 0.218917
[epoch6, step1256]: loss 0.596121
[epoch6, step1257]: loss 0.329573
[epoch6, step1258]: loss 0.377662
[epoch6, step1259]: loss 0.393655
[epoch6, step1260]: loss 0.539234
[epoch6, step1261]: loss 0.681545
[epoch6, step1262]: loss 0.591991
[epoch6, step1263]: loss 0.286332
[epoch6, step1264]: loss 0.700654
[epoch6, step1265]: loss 0.652261
[epoch6, step1266]: loss 0.293405
[epoch6, step1267]: loss 0.600738
[epoch6, step1268]: loss 0.358393
[epoch6, step1269]: loss 0.547565
[epoch6, step1270]: loss 0.684539
[epoch6, step1271]: loss 0.676252
[epoch6, step1272]: loss 0.575496
[epoch6, step1273]: loss 0.649131
[epoch6, step1274]: loss 0.499507
[epoch6, step1275]: loss 0.508956
[epoch6, step1276]: loss 0.638312
[epoch6, step1277]: loss 0.787741
[epoch6, step1278]: loss 0.712510
[epoch6, step1279]: loss 0.692861
[epoch6, step1280]: loss 0.555025
[epoch6, step1281]: loss 0.464536
[epoch6, step1282]: loss 0.665738
[epoch6, step1283]: loss 0.660022
[epoch6, step1284]: loss 0.761557
[epoch6, step1285]: loss 0.618102
[epoch6, step1286]: loss 0.182524
[epoch6, step1287]: loss 0.394694
[epoch6, step1288]: loss 0.259177
[epoch6, step1289]: loss 0.552393
[epoch6, step1290]: loss 0.515636
[epoch6, step1291]: loss 0.722281
[epoch6, step1292]: loss 0.454845
[epoch6, step1293]: loss 0.588373
[epoch6, step1294]: loss 0.433690
[epoch6, step1295]: loss 0.627310
[epoch6, step1296]: loss 0.605026
[epoch6, step1297]: loss 0.533381
[epoch6, step1298]: loss 0.466346
[epoch6, step1299]: loss 0.608027
[epoch6, step1300]: loss 0.470659
[epoch6, step1301]: loss 0.655290
[epoch6, step1302]: loss 0.618668
[epoch6, step1303]: loss 0.332119
[epoch6, step1304]: loss 0.609955
[epoch6, step1305]: loss 0.842291
[epoch6, step1306]: loss 0.583210
[epoch6, step1307]: loss 0.470819
[epoch6, step1308]: loss 0.758099
[epoch6, step1309]: loss 0.382062
[epoch6, step1310]: loss 0.591806
[epoch6, step1311]: loss 0.643646
[epoch6, step1312]: loss 0.537287
[epoch6, step1313]: loss 0.575237
[epoch6, step1314]: loss 0.594524
[epoch6, step1315]: loss 0.652845
[epoch6, step1316]: loss 0.454693
[epoch6, step1317]: loss 0.675585
[epoch6, step1318]: loss 0.392111
[epoch6, step1319]: loss 0.418659
[epoch6, step1320]: loss 0.779068
[epoch6, step1321]: loss 0.445260
[epoch6, step1322]: loss 0.624555
[epoch6, step1323]: loss 0.332326
[epoch6, step1324]: loss 0.583426
[epoch6, step1325]: loss 0.554098
[epoch6, step1326]: loss 0.657319
[epoch6, step1327]: loss 0.589770
[epoch6, step1328]: loss 0.400545
[epoch6, step1329]: loss 0.493952
[epoch6, step1330]: loss 0.670568
[epoch6, step1331]: loss 0.496829
[epoch6, step1332]: loss 0.598986
[epoch6, step1333]: loss 0.404280
[epoch6, step1334]: loss 0.430432
[epoch6, step1335]: loss 0.398588
[epoch6, step1336]: loss 0.775725
[epoch6, step1337]: loss 0.611018
[epoch6, step1338]: loss 0.748263
[epoch6, step1339]: loss 0.604363
[epoch6, step1340]: loss 0.589526
[epoch6, step1341]: loss 0.565981
[epoch6, step1342]: loss 0.590186
[epoch6, step1343]: loss 0.374976
[epoch6, step1344]: loss 0.654165
[epoch6, step1345]: loss 0.476384
[epoch6, step1346]: loss 0.608407
[epoch6, step1347]: loss 0.575050
[epoch6, step1348]: loss 0.377181
[epoch6, step1349]: loss 0.530035
[epoch6, step1350]: loss 0.466330
[epoch6, step1351]: loss 0.613021
[epoch6, step1352]: loss 0.659249
[epoch6, step1353]: loss 0.522611
[epoch6, step1354]: loss 0.317008
[epoch6, step1355]: loss 0.483355
[epoch6, step1356]: loss 0.646473
[epoch6, step1357]: loss 0.651890
[epoch6, step1358]: loss 0.591353
[epoch6, step1359]: loss 0.551853
[epoch6, step1360]: loss 0.409254
[epoch6, step1361]: loss 0.719094
[epoch6, step1362]: loss 0.693314
[epoch6, step1363]: loss 0.494431
[epoch6, step1364]: loss 0.569647
[epoch6, step1365]: loss 0.833470
[epoch6, step1366]: loss 0.577791
[epoch6, step1367]: loss 0.619050
[epoch6, step1368]: loss 0.638763
[epoch6, step1369]: loss 0.557319
[epoch6, step1370]: loss 0.717269
[epoch6, step1371]: loss 0.523834
[epoch6, step1372]: loss 0.695571
[epoch6, step1373]: loss 0.616601
[epoch6, step1374]: loss 0.634563
[epoch6, step1375]: loss 0.730763
[epoch6, step1376]: loss 0.774585
[epoch6, step1377]: loss 0.567015
[epoch6, step1378]: loss 0.457931
[epoch6, step1379]: loss 0.607235
[epoch6, step1380]: loss 0.474609
[epoch6, step1381]: loss 0.394831
[epoch6, step1382]: loss 0.580587
[epoch6, step1383]: loss 0.853645
[epoch6, step1384]: loss 0.412271
[epoch6, step1385]: loss 0.551593
[epoch6, step1386]: loss 0.821757
[epoch6, step1387]: loss 0.491301
[epoch6, step1388]: loss 0.236039
[epoch6, step1389]: loss 0.487431
[epoch6, step1390]: loss 0.557051
[epoch6, step1391]: loss 0.769097
[epoch6, step1392]: loss 0.436771
[epoch6, step1393]: loss 0.432274
[epoch6, step1394]: loss 0.510943
[epoch6, step1395]: loss 0.566676
[epoch6, step1396]: loss 0.602815
[epoch6, step1397]: loss 0.654541
[epoch6, step1398]: loss 0.615128
[epoch6, step1399]: loss 0.627691
[epoch6, step1400]: loss 0.187173
[epoch6, step1401]: loss 0.757833
[epoch6, step1402]: loss 0.757160
[epoch6, step1403]: loss 0.643061
[epoch6, step1404]: loss 0.603605
[epoch6, step1405]: loss 0.513351
[epoch6, step1406]: loss 0.419132
[epoch6, step1407]: loss 0.708111
[epoch6, step1408]: loss 0.676961
[epoch6, step1409]: loss 0.300139
[epoch6, step1410]: loss 0.545474
[epoch6, step1411]: loss 0.624361
[epoch6, step1412]: loss 0.556749
[epoch6, step1413]: loss 0.585673
[epoch6, step1414]: loss 0.686936
[epoch6, step1415]: loss 0.605768
[epoch6, step1416]: loss 0.382854
[epoch6, step1417]: loss 0.701533
[epoch6, step1418]: loss 0.412281
[epoch6, step1419]: loss 0.500777
[epoch6, step1420]: loss 0.513573
[epoch6, step1421]: loss 0.428923
[epoch6, step1422]: loss 0.689961
[epoch6, step1423]: loss 0.538308
[epoch6, step1424]: loss 0.568792
[epoch6, step1425]: loss 0.625524
[epoch6, step1426]: loss 0.509125
[epoch6, step1427]: loss 0.599831
[epoch6, step1428]: loss 0.508790
[epoch6, step1429]: loss 0.818429
[epoch6, step1430]: loss 0.535925
[epoch6, step1431]: loss 0.596328
[epoch6, step1432]: loss 0.281126
[epoch6, step1433]: loss 0.466679
[epoch6, step1434]: loss 0.673492
[epoch6, step1435]: loss 0.633457
[epoch6, step1436]: loss 0.850348
[epoch6, step1437]: loss 0.580557
[epoch6, step1438]: loss 0.681001
[epoch6, step1439]: loss 0.290761
[epoch6, step1440]: loss 0.742801
[epoch6, step1441]: loss 0.670994
[epoch6, step1442]: loss 0.528849
[epoch6, step1443]: loss 0.739398
[epoch6, step1444]: loss 0.642233
[epoch6, step1445]: loss 0.432785
[epoch6, step1446]: loss 0.470258
[epoch6, step1447]: loss 0.545095
[epoch6, step1448]: loss 0.496235
[epoch6, step1449]: loss 0.608868
[epoch6, step1450]: loss 0.439592
[epoch6, step1451]: loss 0.270835
[epoch6, step1452]: loss 0.622851
[epoch6, step1453]: loss 0.583061
[epoch6, step1454]: loss 0.674671
[epoch6, step1455]: loss 0.656025
[epoch6, step1456]: loss 0.811277
[epoch6, step1457]: loss 0.336174
[epoch6, step1458]: loss 0.557665
[epoch6, step1459]: loss 0.546971
[epoch6, step1460]: loss 0.490921
[epoch6, step1461]: loss 0.607145
[epoch6, step1462]: loss 0.548909
[epoch6, step1463]: loss 0.355557
[epoch6, step1464]: loss 0.551640
[epoch6, step1465]: loss 0.667774
[epoch6, step1466]: loss 0.742276
[epoch6, step1467]: loss 0.661674
[epoch6, step1468]: loss 0.676139
[epoch6, step1469]: loss 0.558419
[epoch6, step1470]: loss 0.756977
[epoch6, step1471]: loss 0.357830
[epoch6, step1472]: loss 0.539215
[epoch6, step1473]: loss 0.582204
[epoch6, step1474]: loss 0.439482
[epoch6, step1475]: loss 0.644041
[epoch6, step1476]: loss 0.212836
[epoch6, step1477]: loss 0.422455
[epoch6, step1478]: loss 0.670138
[epoch6, step1479]: loss 0.859646
[epoch6, step1480]: loss 0.451050
[epoch6, step1481]: loss 0.756821
[epoch6, step1482]: loss 0.477708
[epoch6, step1483]: loss 0.608761
[epoch6, step1484]: loss 0.572146
[epoch6, step1485]: loss 0.585166
[epoch6, step1486]: loss 0.629483
[epoch6, step1487]: loss 0.562945
[epoch6, step1488]: loss 0.541853
[epoch6, step1489]: loss 0.589918
[epoch6, step1490]: loss 0.478077
[epoch6, step1491]: loss 0.611549
[epoch6, step1492]: loss 0.532490
[epoch6, step1493]: loss 0.224723
[epoch6, step1494]: loss 0.696863
[epoch6, step1495]: loss 0.518332
[epoch6, step1496]: loss 0.628966
[epoch6, step1497]: loss 0.500309
[epoch6, step1498]: loss 0.376979
[epoch6, step1499]: loss 0.472564
[epoch6, step1500]: loss 0.686286
[epoch6, step1501]: loss 0.618706
[epoch6, step1502]: loss 0.365416
[epoch6, step1503]: loss 0.651057
[epoch6, step1504]: loss 0.726663
[epoch6, step1505]: loss 0.535208
[epoch6, step1506]: loss 0.721351
[epoch6, step1507]: loss 0.544469
[epoch6, step1508]: loss 0.618262
[epoch6, step1509]: loss 0.702868
[epoch6, step1510]: loss 0.512769
[epoch6, step1511]: loss 0.625262
[epoch6, step1512]: loss 0.667170
[epoch6, step1513]: loss 0.702627
[epoch6, step1514]: loss 0.598689
[epoch6, step1515]: loss 0.763238
[epoch6, step1516]: loss 0.583932
[epoch6, step1517]: loss 0.420802
[epoch6, step1518]: loss 0.537627
[epoch6, step1519]: loss 0.647328
[epoch6, step1520]: loss 0.614608
[epoch6, step1521]: loss 0.719195
[epoch6, step1522]: loss 0.401026
[epoch6, step1523]: loss 0.547721
[epoch6, step1524]: loss 0.547779
[epoch6, step1525]: loss 0.389384
[epoch6, step1526]: loss 0.520371
[epoch6, step1527]: loss 0.362502
[epoch6, step1528]: loss 0.333343
[epoch6, step1529]: loss 0.447352
[epoch6, step1530]: loss 0.289441
[epoch6, step1531]: loss 0.416902
[epoch6, step1532]: loss 0.623818
[epoch6, step1533]: loss 0.486206
[epoch6, step1534]: loss 0.292128
[epoch6, step1535]: loss 0.622845
[epoch6, step1536]: loss 0.708721
[epoch6, step1537]: loss 0.599455
[epoch6, step1538]: loss 0.815880
[epoch6, step1539]: loss 0.527374
[epoch6, step1540]: loss 0.516877
[epoch6, step1541]: loss 0.628570
[epoch6, step1542]: loss 0.625409
[epoch6, step1543]: loss 0.778388
[epoch6, step1544]: loss 0.798009
[epoch6, step1545]: loss 0.751440
[epoch6, step1546]: loss 0.586706
[epoch6, step1547]: loss 0.483948
[epoch6, step1548]: loss 0.633379
[epoch6, step1549]: loss 0.682344
[epoch6, step1550]: loss 0.543217
[epoch6, step1551]: loss 0.592642
[epoch6, step1552]: loss 0.365033
[epoch6, step1553]: loss 0.781787
[epoch6, step1554]: loss 0.549978
[epoch6, step1555]: loss 0.561776
[epoch6, step1556]: loss 0.700320
[epoch6, step1557]: loss 0.490897
[epoch6, step1558]: loss 0.619635
[epoch6, step1559]: loss 0.762833
[epoch6, step1560]: loss 0.604576
[epoch6, step1561]: loss 0.400753
[epoch6, step1562]: loss 0.530975
[epoch6, step1563]: loss 0.595768
[epoch6, step1564]: loss 0.473685
[epoch6, step1565]: loss 0.647353
[epoch6, step1566]: loss 0.366595
[epoch6, step1567]: loss 0.484759
[epoch6, step1568]: loss 0.512685
[epoch6, step1569]: loss 0.526737
[epoch6, step1570]: loss 0.649742
[epoch6, step1571]: loss 0.489304
[epoch6, step1572]: loss 0.329765
[epoch6, step1573]: loss 0.493289
[epoch6, step1574]: loss 0.591096
[epoch6, step1575]: loss 0.538403
[epoch6, step1576]: loss 0.298353
[epoch6, step1577]: loss 0.586813
[epoch6, step1578]: loss 0.411228
[epoch6, step1579]: loss 0.526999
[epoch6, step1580]: loss 0.485262
[epoch6, step1581]: loss 0.464411
[epoch6, step1582]: loss 0.623799
[epoch6, step1583]: loss 0.590716
[epoch6, step1584]: loss 0.527983
[epoch6, step1585]: loss 0.512837
[epoch6, step1586]: loss 0.762612
[epoch6, step1587]: loss 0.380564
[epoch6, step1588]: loss 0.516823
[epoch6, step1589]: loss 0.655681
[epoch6, step1590]: loss 0.393439
[epoch6, step1591]: loss 0.626680
[epoch6, step1592]: loss 0.617241
[epoch6, step1593]: loss 0.829583
[epoch6, step1594]: loss 0.506436
[epoch6, step1595]: loss 0.443491
[epoch6, step1596]: loss 0.492634
[epoch6, step1597]: loss 0.452985
[epoch6, step1598]: loss 0.566757
[epoch6, step1599]: loss 0.643840
[epoch6, step1600]: loss 0.516238
[epoch6, step1601]: loss 0.643000
[epoch6, step1602]: loss 0.627881
[epoch6, step1603]: loss 0.625830
[epoch6, step1604]: loss 0.232524
[epoch6, step1605]: loss 0.362336
[epoch6, step1606]: loss 0.408542
[epoch6, step1607]: loss 0.560125
[epoch6, step1608]: loss 0.585116
[epoch6, step1609]: loss 0.734258
[epoch6, step1610]: loss 0.492676
[epoch6, step1611]: loss 0.484113
[epoch6, step1612]: loss 0.506426
[epoch6, step1613]: loss 0.507735
[epoch6, step1614]: loss 0.534905
[epoch6, step1615]: loss 0.684768
[epoch6, step1616]: loss 0.489260
[epoch6, step1617]: loss 0.596673
[epoch6, step1618]: loss 0.576238
[epoch6, step1619]: loss 0.491906
[epoch6, step1620]: loss 0.271119
[epoch6, step1621]: loss 0.806536
[epoch6, step1622]: loss 0.680854
[epoch6, step1623]: loss 0.790750
[epoch6, step1624]: loss 0.481984
[epoch6, step1625]: loss 0.409453
[epoch6, step1626]: loss 0.408257
[epoch6, step1627]: loss 0.560044
[epoch6, step1628]: loss 0.240460
[epoch6, step1629]: loss 0.529976
[epoch6, step1630]: loss 0.740453
[epoch6, step1631]: loss 0.702511
[epoch6, step1632]: loss 0.458085
[epoch6, step1633]: loss 0.780349
[epoch6, step1634]: loss 0.503005
[epoch6, step1635]: loss 0.480915
[epoch6, step1636]: loss 0.544768
[epoch6, step1637]: loss 0.464029
[epoch6, step1638]: loss 0.264435
[epoch6, step1639]: loss 0.546829
[epoch6, step1640]: loss 0.732203
[epoch6, step1641]: loss 0.806762
[epoch6, step1642]: loss 0.611078
[epoch6, step1643]: loss 0.543940
[epoch6, step1644]: loss 0.480250
[epoch6, step1645]: loss 0.548758
[epoch6, step1646]: loss 0.700741
[epoch6, step1647]: loss 0.526035
[epoch6, step1648]: loss 0.638191
[epoch6, step1649]: loss 0.505957
[epoch6, step1650]: loss 0.498050
[epoch6, step1651]: loss 0.648978
[epoch6, step1652]: loss 0.657012
[epoch6, step1653]: loss 0.800678
[epoch6, step1654]: loss 0.395707
[epoch6, step1655]: loss 0.718544
[epoch6, step1656]: loss 0.451666
[epoch6, step1657]: loss 0.666271
[epoch6, step1658]: loss 0.471047
[epoch6, step1659]: loss 0.491869
[epoch6, step1660]: loss 0.496797
[epoch6, step1661]: loss 0.442917
[epoch6, step1662]: loss 0.152943
[epoch6, step1663]: loss 0.348976
[epoch6, step1664]: loss 0.543374
[epoch6, step1665]: loss 0.543843
[epoch6, step1666]: loss 0.704096
[epoch6, step1667]: loss 0.465112
[epoch6, step1668]: loss 0.409321
[epoch6, step1669]: loss 0.555284
[epoch6, step1670]: loss 0.517347
[epoch6, step1671]: loss 0.324483
[epoch6, step1672]: loss 0.391003
[epoch6, step1673]: loss 0.662972
[epoch6, step1674]: loss 0.578090
[epoch6, step1675]: loss 0.637353
[epoch6, step1676]: loss 0.398185
[epoch6, step1677]: loss 0.745380
[epoch6, step1678]: loss 0.760899
[epoch6, step1679]: loss 0.648820
[epoch6, step1680]: loss 0.506031
[epoch6, step1681]: loss 0.655002
[epoch6, step1682]: loss 0.362562
[epoch6, step1683]: loss 0.503474
[epoch6, step1684]: loss 0.397330
[epoch6, step1685]: loss 0.594961
[epoch6, step1686]: loss 0.557826
[epoch6, step1687]: loss 0.529799
[epoch6, step1688]: loss 0.696759
[epoch6, step1689]: loss 0.719132
[epoch6, step1690]: loss 0.307169
[epoch6, step1691]: loss 0.483952
[epoch6, step1692]: loss 0.674361
[epoch6, step1693]: loss 0.447200
[epoch6, step1694]: loss 0.375898
[epoch6, step1695]: loss 0.766499
[epoch6, step1696]: loss 0.533812
[epoch6, step1697]: loss 0.491660
[epoch6, step1698]: loss 0.558014
[epoch6, step1699]: loss 0.507127
[epoch6, step1700]: loss 0.678044
[epoch6, step1701]: loss 0.556495
[epoch6, step1702]: loss 0.410852
[epoch6, step1703]: loss 0.277002
[epoch6, step1704]: loss 0.579074
[epoch6, step1705]: loss 0.461804
[epoch6, step1706]: loss 0.820617
[epoch6, step1707]: loss 0.185570
[epoch6, step1708]: loss 0.636451
[epoch6, step1709]: loss 0.534278
[epoch6, step1710]: loss 0.551874
[epoch6, step1711]: loss 0.595783
[epoch6, step1712]: loss 0.583610
[epoch6, step1713]: loss 0.317305
[epoch6, step1714]: loss 0.608630
[epoch6, step1715]: loss 0.362532
[epoch6, step1716]: loss 0.507613
[epoch6, step1717]: loss 0.490836
[epoch6, step1718]: loss 0.462592
[epoch6, step1719]: loss 0.550874
[epoch6, step1720]: loss 0.400910
[epoch6, step1721]: loss 0.556981
[epoch6, step1722]: loss 0.482152
[epoch6, step1723]: loss 0.491039
[epoch6, step1724]: loss 0.442293
[epoch6, step1725]: loss 0.540665
[epoch6, step1726]: loss 0.508426
[epoch6, step1727]: loss 0.318978
[epoch6, step1728]: loss 0.608570
[epoch6, step1729]: loss 0.635995
[epoch6, step1730]: loss 0.509395
[epoch6, step1731]: loss 0.429362
[epoch6, step1732]: loss 0.408708
[epoch6, step1733]: loss 0.651786
[epoch6, step1734]: loss 0.517097
[epoch6, step1735]: loss 0.594331
[epoch6, step1736]: loss 0.727442
[epoch6, step1737]: loss 0.499111
[epoch6, step1738]: loss 0.610576
[epoch6, step1739]: loss 0.440321
[epoch6, step1740]: loss 0.570756
[epoch6, step1741]: loss 0.454993
[epoch6, step1742]: loss 0.404128
[epoch6, step1743]: loss 0.627462
[epoch6, step1744]: loss 0.640544
[epoch6, step1745]: loss 0.377957
[epoch6, step1746]: loss 0.781264
[epoch6, step1747]: loss 0.480143
[epoch6, step1748]: loss 0.524659
[epoch6, step1749]: loss 0.544749
[epoch6, step1750]: loss 0.581700
[epoch6, step1751]: loss 0.634678
[epoch6, step1752]: loss 0.815689
[epoch6, step1753]: loss 0.598893
[epoch6, step1754]: loss 0.508509
[epoch6, step1755]: loss 0.603484
[epoch6, step1756]: loss 0.487419
[epoch6, step1757]: loss 0.421118
[epoch6, step1758]: loss 0.667142
[epoch6, step1759]: loss 0.447325
[epoch6, step1760]: loss 0.679179
[epoch6, step1761]: loss 0.509359
[epoch6, step1762]: loss 0.723591
[epoch6, step1763]: loss 0.685376
[epoch6, step1764]: loss 0.651940
[epoch6, step1765]: loss 0.404145
[epoch6, step1766]: loss 0.553436
[epoch6, step1767]: loss 0.638698
[epoch6, step1768]: loss 0.605169
[epoch6, step1769]: loss 0.604420
[epoch6, step1770]: loss 0.649117
[epoch6, step1771]: loss 0.757415
[epoch6, step1772]: loss 0.645853
[epoch6, step1773]: loss 0.437559
[epoch6, step1774]: loss 0.474533
[epoch6, step1775]: loss 0.551897
[epoch6, step1776]: loss 0.300502
[epoch6, step1777]: loss 0.666170
[epoch6, step1778]: loss 0.551193
[epoch6, step1779]: loss 0.664858
[epoch6, step1780]: loss 0.480349
[epoch6, step1781]: loss 0.559332
[epoch6, step1782]: loss 0.281501
[epoch6, step1783]: loss 0.577460
[epoch6, step1784]: loss 0.527744
[epoch6, step1785]: loss 0.477962
[epoch6, step1786]: loss 0.248930
[epoch6, step1787]: loss 0.423157
[epoch6, step1788]: loss 0.643020
[epoch6, step1789]: loss 0.576465
[epoch6, step1790]: loss 0.453507
[epoch6, step1791]: loss 0.398916
[epoch6, step1792]: loss 0.573258
[epoch6, step1793]: loss 0.624386
[epoch6, step1794]: loss 0.458296
[epoch6, step1795]: loss 0.689919
[epoch6, step1796]: loss 0.326843
[epoch6, step1797]: loss 0.638431
[epoch6, step1798]: loss 0.603034
[epoch6, step1799]: loss 0.631378
[epoch6, step1800]: loss 0.588031
[epoch6, step1801]: loss 0.331990
[epoch6, step1802]: loss 0.447144
[epoch6, step1803]: loss 0.458486
[epoch6, step1804]: loss 0.389543
[epoch6, step1805]: loss 0.567333
[epoch6, step1806]: loss 0.437831
[epoch6, step1807]: loss 0.521927
[epoch6, step1808]: loss 0.460798
[epoch6, step1809]: loss 0.575321
[epoch6, step1810]: loss 0.591752
[epoch6, step1811]: loss 0.453639
[epoch6, step1812]: loss 0.613009
[epoch6, step1813]: loss 0.669849
[epoch6, step1814]: loss 0.304225
[epoch6, step1815]: loss 0.576172
[epoch6, step1816]: loss 0.852782
[epoch6, step1817]: loss 0.556149
[epoch6, step1818]: loss 0.514449
[epoch6, step1819]: loss 0.377652
[epoch6, step1820]: loss 0.594607
[epoch6, step1821]: loss 0.474340
[epoch6, step1822]: loss 0.566973
[epoch6, step1823]: loss 0.639388
[epoch6, step1824]: loss 0.592873
[epoch6, step1825]: loss 0.329358
[epoch6, step1826]: loss 0.659501
[epoch6, step1827]: loss 0.649930
[epoch6, step1828]: loss 0.628710
[epoch6, step1829]: loss 0.482688
[epoch6, step1830]: loss 0.284905
[epoch6, step1831]: loss 0.587225
[epoch6, step1832]: loss 0.506326
[epoch6, step1833]: loss 0.484601
[epoch6, step1834]: loss 0.465343
[epoch6, step1835]: loss 0.437404
[epoch6, step1836]: loss 0.192901
[epoch6, step1837]: loss 0.518385
[epoch6, step1838]: loss 0.525456
[epoch6, step1839]: loss 0.322345
[epoch6, step1840]: loss 0.558336
[epoch6, step1841]: loss 0.571816
[epoch6, step1842]: loss 0.560907
[epoch6, step1843]: loss 0.763390
[epoch6, step1844]: loss 0.623561
[epoch6, step1845]: loss 0.709262
[epoch6, step1846]: loss 0.561821
[epoch6, step1847]: loss 0.553679
[epoch6, step1848]: loss 0.876173
[epoch6, step1849]: loss 0.738247
[epoch6, step1850]: loss 0.707683
[epoch6, step1851]: loss 0.670882
[epoch6, step1852]: loss 0.782454
[epoch6, step1853]: loss 0.356513
[epoch6, step1854]: loss 0.800714
[epoch6, step1855]: loss 0.661581
[epoch6, step1856]: loss 0.409980
[epoch6, step1857]: loss 0.500405
[epoch6, step1858]: loss 0.493444
[epoch6, step1859]: loss 0.669767
[epoch6, step1860]: loss 0.450938
[epoch6, step1861]: loss 0.350276
[epoch6, step1862]: loss 0.649239
[epoch6, step1863]: loss 0.741642
[epoch6, step1864]: loss 0.425322
[epoch6, step1865]: loss 0.771397
[epoch6, step1866]: loss 0.518839
[epoch6, step1867]: loss 0.335390
[epoch6, step1868]: loss 0.638777
[epoch6, step1869]: loss 0.720418
[epoch6, step1870]: loss 0.766760
[epoch6, step1871]: loss 0.372921
[epoch6, step1872]: loss 0.552651
[epoch6, step1873]: loss 0.229820
[epoch6, step1874]: loss 0.619997
[epoch6, step1875]: loss 0.384296
[epoch6, step1876]: loss 0.516292
[epoch6, step1877]: loss 0.526135
[epoch6, step1878]: loss 0.646124
[epoch6, step1879]: loss 0.549230
[epoch6, step1880]: loss 0.448673
[epoch6, step1881]: loss 0.718476
[epoch6, step1882]: loss 0.790865
[epoch6, step1883]: loss 0.625996
[epoch6, step1884]: loss 0.550875
[epoch6, step1885]: loss 0.328215
[epoch6, step1886]: loss 0.572095
[epoch6, step1887]: loss 0.584728
[epoch6, step1888]: loss 0.584091
[epoch6, step1889]: loss 0.459146
[epoch6, step1890]: loss 0.454387
[epoch6, step1891]: loss 0.514497
[epoch6, step1892]: loss 0.418144
[epoch6, step1893]: loss 0.712503
[epoch6, step1894]: loss 0.292467
[epoch6, step1895]: loss 0.628067
[epoch6, step1896]: loss 0.387736
[epoch6, step1897]: loss 0.548528
[epoch6, step1898]: loss 0.686013
[epoch6, step1899]: loss 0.462797
[epoch6, step1900]: loss 0.119121
[epoch6, step1901]: loss 0.451151
[epoch6, step1902]: loss 0.396774
[epoch6, step1903]: loss 0.505108
[epoch6, step1904]: loss 0.460792
[epoch6, step1905]: loss 0.484427
[epoch6, step1906]: loss 0.612468
[epoch6, step1907]: loss 0.158397
[epoch6, step1908]: loss 0.775858
[epoch6, step1909]: loss 0.397466
[epoch6, step1910]: loss 0.365014
[epoch6, step1911]: loss 0.490721
[epoch6, step1912]: loss 0.485759
[epoch6, step1913]: loss 0.504647
[epoch6, step1914]: loss 0.709384
[epoch6, step1915]: loss 0.283376
[epoch6, step1916]: loss 0.283151
[epoch6, step1917]: loss 0.555042
[epoch6, step1918]: loss 0.525097
[epoch6, step1919]: loss 0.674898
[epoch6, step1920]: loss 0.701021
[epoch6, step1921]: loss 0.725109
[epoch6, step1922]: loss 0.620315
[epoch6, step1923]: loss 0.560246
[epoch6, step1924]: loss 0.745711
[epoch6, step1925]: loss 0.608164
[epoch6, step1926]: loss 0.701765
[epoch6, step1927]: loss 0.406134
[epoch6, step1928]: loss 0.518933
[epoch6, step1929]: loss 0.448171
[epoch6, step1930]: loss 0.712188
[epoch6, step1931]: loss 0.670473
[epoch6, step1932]: loss 0.553075
[epoch6, step1933]: loss 0.724765
[epoch6, step1934]: loss 0.548004
[epoch6, step1935]: loss 0.820750
[epoch6, step1936]: loss 0.506820
[epoch6, step1937]: loss 0.419418
[epoch6, step1938]: loss 0.288794
[epoch6, step1939]: loss 0.478502
[epoch6, step1940]: loss 0.345017
[epoch6, step1941]: loss 0.681646
[epoch6, step1942]: loss 0.841337
[epoch6, step1943]: loss 0.472501
[epoch6, step1944]: loss 0.539856
[epoch6, step1945]: loss 0.718045
[epoch6, step1946]: loss 0.571055
[epoch6, step1947]: loss 0.626468
[epoch6, step1948]: loss 0.846235
[epoch6, step1949]: loss 0.609621
[epoch6, step1950]: loss 0.610437
[epoch6, step1951]: loss 0.448911
[epoch6, step1952]: loss 0.572839
[epoch6, step1953]: loss 0.442140
[epoch6, step1954]: loss 0.764460
[epoch6, step1955]: loss 0.405854
[epoch6, step1956]: loss 0.578679
[epoch6, step1957]: loss 0.713634
[epoch6, step1958]: loss 0.624043
[epoch6, step1959]: loss 0.749309
[epoch6, step1960]: loss 0.139192
[epoch6, step1961]: loss 0.328389
[epoch6, step1962]: loss 0.647207
[epoch6, step1963]: loss 0.774120
[epoch6, step1964]: loss 0.593924
[epoch6, step1965]: loss 0.538175
[epoch6, step1966]: loss 0.520536
[epoch6, step1967]: loss 0.324644
[epoch6, step1968]: loss 0.452327
[epoch6, step1969]: loss 0.596437
[epoch6, step1970]: loss 0.577237
[epoch6, step1971]: loss 0.622591
[epoch6, step1972]: loss 0.437751
[epoch6, step1973]: loss 0.312785
[epoch6, step1974]: loss 0.656983
[epoch6, step1975]: loss 0.298321
[epoch6, step1976]: loss 0.457910
[epoch6, step1977]: loss 0.647352
[epoch6, step1978]: loss 0.613227
[epoch6, step1979]: loss 0.643798
[epoch6, step1980]: loss 0.740756
[epoch6, step1981]: loss 0.719505
[epoch6, step1982]: loss 0.606410
[epoch6, step1983]: loss 0.644096
[epoch6, step1984]: loss 0.284026
[epoch6, step1985]: loss 0.494572
[epoch6, step1986]: loss 0.680586
[epoch6, step1987]: loss 0.529868
[epoch6, step1988]: loss 0.211954
[epoch6, step1989]: loss 0.512817
[epoch6, step1990]: loss 0.397353
[epoch6, step1991]: loss 0.754427
[epoch6, step1992]: loss 0.510957
[epoch6, step1993]: loss 0.629094
[epoch6, step1994]: loss 0.644234
[epoch6, step1995]: loss 0.570085
[epoch6, step1996]: loss 0.253298
[epoch6, step1997]: loss 0.687460
[epoch6, step1998]: loss 0.358172
[epoch6, step1999]: loss 0.723903
[epoch6, step2000]: loss 0.456098
[epoch6, step2001]: loss 0.458924
[epoch6, step2002]: loss 0.487527
[epoch6, step2003]: loss 0.440274
[epoch6, step2004]: loss 0.507390
[epoch6, step2005]: loss 0.600841
[epoch6, step2006]: loss 0.735550
[epoch6, step2007]: loss 0.752783
[epoch6, step2008]: loss 0.568794
[epoch6, step2009]: loss 0.808147
[epoch6, step2010]: loss 0.291177
[epoch6, step2011]: loss 0.505236
[epoch6, step2012]: loss 0.476645
[epoch6, step2013]: loss 0.509928
[epoch6, step2014]: loss 0.479856
[epoch6, step2015]: loss 0.492363
[epoch6, step2016]: loss 0.559864
[epoch6, step2017]: loss 0.493123
[epoch6, step2018]: loss 0.731736
[epoch6, step2019]: loss 0.442853
[epoch6, step2020]: loss 0.452965
[epoch6, step2021]: loss 0.603216
[epoch6, step2022]: loss 0.726626
[epoch6, step2023]: loss 0.577029
[epoch6, step2024]: loss 0.489089
[epoch6, step2025]: loss 0.351817
[epoch6, step2026]: loss 0.598034
[epoch6, step2027]: loss 0.608371
[epoch6, step2028]: loss 0.678166
[epoch6, step2029]: loss 0.538681
[epoch6, step2030]: loss 0.618878
[epoch6, step2031]: loss 0.734196
[epoch6, step2032]: loss 0.662496
[epoch6, step2033]: loss 0.326411
[epoch6, step2034]: loss 0.663117
[epoch6, step2035]: loss 0.492641
[epoch6, step2036]: loss 0.442913
[epoch6, step2037]: loss 0.562632
[epoch6, step2038]: loss 0.510022
[epoch6, step2039]: loss 0.526682
[epoch6, step2040]: loss 0.300051
[epoch6, step2041]: loss 0.627907
[epoch6, step2042]: loss 0.276035
[epoch6, step2043]: loss 0.634929
[epoch6, step2044]: loss 0.601895
[epoch6, step2045]: loss 0.657439
[epoch6, step2046]: loss 0.685832
[epoch6, step2047]: loss 0.629798
[epoch6, step2048]: loss 0.662906
[epoch6, step2049]: loss 0.449557
[epoch6, step2050]: loss 0.434062
[epoch6, step2051]: loss 0.514400
[epoch6, step2052]: loss 0.327085
[epoch6, step2053]: loss 0.550829
[epoch6, step2054]: loss 0.631835
[epoch6, step2055]: loss 0.683550
[epoch6, step2056]: loss 0.437388
[epoch6, step2057]: loss 0.546058
[epoch6, step2058]: loss 0.531927
[epoch6, step2059]: loss 0.682061
[epoch6, step2060]: loss 0.500852
[epoch6, step2061]: loss 0.631318
[epoch6, step2062]: loss 0.554429
[epoch6, step2063]: loss 0.390421
[epoch6, step2064]: loss 0.840482
[epoch6, step2065]: loss 0.712822
[epoch6, step2066]: loss 0.394290
[epoch6, step2067]: loss 0.600467
[epoch6, step2068]: loss 0.709574
[epoch6, step2069]: loss 0.664365
[epoch6, step2070]: loss 0.490468
[epoch6, step2071]: loss 0.540399
[epoch6, step2072]: loss 0.550277
[epoch6, step2073]: loss 0.440428
[epoch6, step2074]: loss 0.739871
[epoch6, step2075]: loss 0.684848
[epoch6, step2076]: loss 0.399965
[epoch6, step2077]: loss 0.469201
[epoch6, step2078]: loss 0.689056
[epoch6, step2079]: loss 0.171604
[epoch6, step2080]: loss 0.609779
[epoch6, step2081]: loss 0.615162
[epoch6, step2082]: loss 0.548094
[epoch6, step2083]: loss 0.554499
[epoch6, step2084]: loss 0.707396
[epoch6, step2085]: loss 0.610207
[epoch6, step2086]: loss 0.404173
[epoch6, step2087]: loss 0.520805
[epoch6, step2088]: loss 0.652695
[epoch6, step2089]: loss 0.614835
[epoch6, step2090]: loss 0.427415
[epoch6, step2091]: loss 0.416722
[epoch6, step2092]: loss 0.698608
[epoch6, step2093]: loss 0.528614
[epoch6, step2094]: loss 0.549296
[epoch6, step2095]: loss 0.606574
[epoch6, step2096]: loss 0.741995
[epoch6, step2097]: loss 0.636837
[epoch6, step2098]: loss 0.689848
[epoch6, step2099]: loss 0.499381
[epoch6, step2100]: loss 0.654358
[epoch6, step2101]: loss 0.167399
[epoch6, step2102]: loss 0.490823
[epoch6, step2103]: loss 0.481720
[epoch6, step2104]: loss 0.563653
[epoch6, step2105]: loss 0.408617
[epoch6, step2106]: loss 0.692161
[epoch6, step2107]: loss 0.570283
[epoch6, step2108]: loss 0.360531
[epoch6, step2109]: loss 0.522496
[epoch6, step2110]: loss 0.699589
[epoch6, step2111]: loss 0.590168
[epoch6, step2112]: loss 0.848699
[epoch6, step2113]: loss 0.668231
[epoch6, step2114]: loss 0.488873
[epoch6, step2115]: loss 0.659996
[epoch6, step2116]: loss 0.455195
[epoch6, step2117]: loss 0.496854
[epoch6, step2118]: loss 0.531680
[epoch6, step2119]: loss 0.744120
[epoch6, step2120]: loss 0.466166
[epoch6, step2121]: loss 0.505822
[epoch6, step2122]: loss 0.345657
[epoch6, step2123]: loss 0.660237
[epoch6, step2124]: loss 0.496457
[epoch6, step2125]: loss 0.298239
[epoch6, step2126]: loss 0.780527
[epoch6, step2127]: loss 0.274152
[epoch6, step2128]: loss 0.510208
[epoch6, step2129]: loss 0.452728
[epoch6, step2130]: loss 0.384955
[epoch6, step2131]: loss 0.329854
[epoch6, step2132]: loss 0.565165
[epoch6, step2133]: loss 0.482246
[epoch6, step2134]: loss 0.484948
[epoch6, step2135]: loss 0.328520
[epoch6, step2136]: loss 0.558090
[epoch6, step2137]: loss 0.542942
[epoch6, step2138]: loss 0.481963
[epoch6, step2139]: loss 0.416117
[epoch6, step2140]: loss 0.559074
[epoch6, step2141]: loss 0.466152
[epoch6, step2142]: loss 0.670529
[epoch6, step2143]: loss 0.513249
[epoch6, step2144]: loss 0.311702
[epoch6, step2145]: loss 0.601258
[epoch6, step2146]: loss 0.705462
[epoch6, step2147]: loss 0.484919
[epoch6, step2148]: loss 0.628643
[epoch6, step2149]: loss 0.776707
[epoch6, step2150]: loss 0.607272
[epoch6, step2151]: loss 0.168744
[epoch6, step2152]: loss 0.606210
[epoch6, step2153]: loss 0.561281
[epoch6, step2154]: loss 0.721817
[epoch6, step2155]: loss 0.410023
[epoch6, step2156]: loss 0.692356
[epoch6, step2157]: loss 0.545939
[epoch6, step2158]: loss 0.618441
[epoch6, step2159]: loss 0.581822
[epoch6, step2160]: loss 0.447664
[epoch6, step2161]: loss 0.557850
[epoch6, step2162]: loss 0.546142
[epoch6, step2163]: loss 0.488692
[epoch6, step2164]: loss 0.628663
[epoch6, step2165]: loss 0.597394
[epoch6, step2166]: loss 0.563366
[epoch6, step2167]: loss 0.636341
[epoch6, step2168]: loss 0.710802
[epoch6, step2169]: loss 0.518710
[epoch6, step2170]: loss 0.474224
[epoch6, step2171]: loss 0.383304
[epoch6, step2172]: loss 0.463865
[epoch6, step2173]: loss 0.292635
[epoch6, step2174]: loss 0.640482
[epoch6, step2175]: loss 0.621655
[epoch6, step2176]: loss 0.662417
[epoch6, step2177]: loss 0.584137
[epoch6, step2178]: loss 0.476413
[epoch6, step2179]: loss 0.529730
[epoch6, step2180]: loss 0.809143
[epoch6, step2181]: loss 0.309797
[epoch6, step2182]: loss 0.540882
[epoch6, step2183]: loss 0.358837
[epoch6, step2184]: loss 0.453133
[epoch6, step2185]: loss 0.385451
[epoch6, step2186]: loss 0.487409
[epoch6, step2187]: loss 0.457399
[epoch6, step2188]: loss 0.589409
[epoch6, step2189]: loss 0.601197
[epoch6, step2190]: loss 0.453052
[epoch6, step2191]: loss 0.423230
[epoch6, step2192]: loss 0.451434
[epoch6, step2193]: loss 0.628151
[epoch6, step2194]: loss 0.422352
[epoch6, step2195]: loss 0.591893
[epoch6, step2196]: loss 0.754175
[epoch6, step2197]: loss 0.650745
[epoch6, step2198]: loss 0.519743
[epoch6, step2199]: loss 0.520286
[epoch6, step2200]: loss 0.535478
[epoch6, step2201]: loss 0.579283
[epoch6, step2202]: loss 0.162907
[epoch6, step2203]: loss 0.469299
[epoch6, step2204]: loss 0.614059
[epoch6, step2205]: loss 0.633057
[epoch6, step2206]: loss 0.716177
[epoch6, step2207]: loss 0.440541
[epoch6, step2208]: loss 0.853543
[epoch6, step2209]: loss 0.560787
[epoch6, step2210]: loss 0.636848
[epoch6, step2211]: loss 0.563048
[epoch6, step2212]: loss 0.548960
[epoch6, step2213]: loss 0.727835
[epoch6, step2214]: loss 0.643848
[epoch6, step2215]: loss 0.659646
[epoch6, step2216]: loss 0.432631
[epoch6, step2217]: loss 0.357109
[epoch6, step2218]: loss 0.323335
[epoch6, step2219]: loss 0.738021
[epoch6, step2220]: loss 0.499727
[epoch6, step2221]: loss 0.620116
[epoch6, step2222]: loss 0.563318
[epoch6, step2223]: loss 0.625597
[epoch6, step2224]: loss 0.380435
[epoch6, step2225]: loss 0.638587
[epoch6, step2226]: loss 0.502284
[epoch6, step2227]: loss 0.506476
[epoch6, step2228]: loss 0.604641
[epoch6, step2229]: loss 0.672273
[epoch6, step2230]: loss 0.520571
[epoch6, step2231]: loss 0.496070
[epoch6, step2232]: loss 0.584701
[epoch6, step2233]: loss 0.465528
[epoch6, step2234]: loss 0.473620
[epoch6, step2235]: loss 0.378421
[epoch6, step2236]: loss 0.680757
[epoch6, step2237]: loss 0.483569
[epoch6, step2238]: loss 0.316851
[epoch6, step2239]: loss 0.553154
[epoch6, step2240]: loss 0.346131
[epoch6, step2241]: loss 0.587677
[epoch6, step2242]: loss 0.631283
[epoch6, step2243]: loss 0.548824
[epoch6, step2244]: loss 0.738141
[epoch6, step2245]: loss 0.357708
[epoch6, step2246]: loss 0.538831
[epoch6, step2247]: loss 0.513131
[epoch6, step2248]: loss 0.459301
[epoch6, step2249]: loss 0.487994
[epoch6, step2250]: loss 0.364544
[epoch6, step2251]: loss 0.241347
[epoch6, step2252]: loss 0.798920
[epoch6, step2253]: loss 0.335532
[epoch6, step2254]: loss 0.603251
[epoch6, step2255]: loss 0.707675
[epoch6, step2256]: loss 0.669155
[epoch6, step2257]: loss 0.649367
[epoch6, step2258]: loss 0.341586
[epoch6, step2259]: loss 0.548611
[epoch6, step2260]: loss 0.298316
[epoch6, step2261]: loss 0.524301
[epoch6, step2262]: loss 0.742223
[epoch6, step2263]: loss 0.584391
[epoch6, step2264]: loss 0.351336
[epoch6, step2265]: loss 0.511380
[epoch6, step2266]: loss 0.424492
[epoch6, step2267]: loss 0.727198
[epoch6, step2268]: loss 0.366326
[epoch6, step2269]: loss 0.744884
[epoch6, step2270]: loss 0.339564
[epoch6, step2271]: loss 0.268607
[epoch6, step2272]: loss 0.619201
[epoch6, step2273]: loss 0.547728
[epoch6, step2274]: loss 0.514451
[epoch6, step2275]: loss 0.496600
[epoch6, step2276]: loss 0.429131
[epoch6, step2277]: loss 0.462841
[epoch6, step2278]: loss 0.316329
[epoch6, step2279]: loss 0.455942
[epoch6, step2280]: loss 0.548720
[epoch6, step2281]: loss 0.614964
[epoch6, step2282]: loss 0.720189
[epoch6, step2283]: loss 0.491920
[epoch6, step2284]: loss 0.576567
[epoch6, step2285]: loss 0.714794
[epoch6, step2286]: loss 0.566801
[epoch6, step2287]: loss 0.574847
[epoch6, step2288]: loss 0.642479
[epoch6, step2289]: loss 0.377031
[epoch6, step2290]: loss 0.415352
[epoch6, step2291]: loss 0.677599
[epoch6, step2292]: loss 0.513439
[epoch6, step2293]: loss 0.703498
[epoch6, step2294]: loss 0.309079
[epoch6, step2295]: loss 0.603505
[epoch6, step2296]: loss 0.530104
[epoch6, step2297]: loss 0.362440
[epoch6, step2298]: loss 0.528082
[epoch6, step2299]: loss 0.629578
[epoch6, step2300]: loss 0.582544
[epoch6, step2301]: loss 0.229792
[epoch6, step2302]: loss 0.667676
[epoch6, step2303]: loss 0.527026
[epoch6, step2304]: loss 0.482667
[epoch6, step2305]: loss 0.491253
[epoch6, step2306]: loss 0.430769
[epoch6, step2307]: loss 0.437738
[epoch6, step2308]: loss 0.543222
[epoch6, step2309]: loss 0.600642
[epoch6, step2310]: loss 0.618769
[epoch6, step2311]: loss 0.592169
[epoch6, step2312]: loss 0.880962
[epoch6, step2313]: loss 0.547876
[epoch6, step2314]: loss 0.329865
[epoch6, step2315]: loss 0.633244
[epoch6, step2316]: loss 0.355812
[epoch6, step2317]: loss 0.575371
[epoch6, step2318]: loss 0.362234
[epoch6, step2319]: loss 0.643798
[epoch6, step2320]: loss 0.608989
[epoch6, step2321]: loss 0.726103
[epoch6, step2322]: loss 0.684194
[epoch6, step2323]: loss 0.562375
[epoch6, step2324]: loss 0.671477
[epoch6, step2325]: loss 0.466545
[epoch6, step2326]: loss 0.688133
[epoch6, step2327]: loss 0.445245
[epoch6, step2328]: loss 0.533620
[epoch6, step2329]: loss 0.409832
[epoch6, step2330]: loss 0.416129
[epoch6, step2331]: loss 0.542955
[epoch6, step2332]: loss 0.600098
[epoch6, step2333]: loss 0.350682
[epoch6, step2334]: loss 0.463439
[epoch6, step2335]: loss 0.604050
[epoch6, step2336]: loss 0.327050
[epoch6, step2337]: loss 0.357369
[epoch6, step2338]: loss 0.632518
[epoch6, step2339]: loss 0.584877
[epoch6, step2340]: loss 0.726281
[epoch6, step2341]: loss 0.554456
[epoch6, step2342]: loss 0.592810
[epoch6, step2343]: loss 0.499838
[epoch6, step2344]: loss 0.372509
[epoch6, step2345]: loss 0.473450
[epoch6, step2346]: loss 0.272652
[epoch6, step2347]: loss 0.306144
[epoch6, step2348]: loss 0.600454
[epoch6, step2349]: loss 0.762801
[epoch6, step2350]: loss 0.560542
[epoch6, step2351]: loss 0.708206
[epoch6, step2352]: loss 0.870784
[epoch6, step2353]: loss 0.413670
[epoch6, step2354]: loss 0.529035
[epoch6, step2355]: loss 0.443940
[epoch6, step2356]: loss 0.487448
[epoch6, step2357]: loss 0.519466
[epoch6, step2358]: loss 0.539844
[epoch6, step2359]: loss 0.584853
[epoch6, step2360]: loss 0.412126
[epoch6, step2361]: loss 0.729734
[epoch6, step2362]: loss 0.396279
[epoch6, step2363]: loss 0.472373
[epoch6, step2364]: loss 0.543790
[epoch6, step2365]: loss 0.481001
[epoch6, step2366]: loss 0.484389
[epoch6, step2367]: loss 0.529908
[epoch6, step2368]: loss 0.660295
[epoch6, step2369]: loss 0.662225
[epoch6, step2370]: loss 0.454650
[epoch6, step2371]: loss 0.453165
[epoch6, step2372]: loss 0.479018
[epoch6, step2373]: loss 0.519826
[epoch6, step2374]: loss 0.335936
[epoch6, step2375]: loss 0.528282
[epoch6, step2376]: loss 0.473314
[epoch6, step2377]: loss 0.416462
[epoch6, step2378]: loss 0.602549
[epoch6, step2379]: loss 0.538534
[epoch6, step2380]: loss 0.387300
[epoch6, step2381]: loss 0.541891
[epoch6, step2382]: loss 0.722566
[epoch6, step2383]: loss 0.564024
[epoch6, step2384]: loss 0.320603
[epoch6, step2385]: loss 0.683305
[epoch6, step2386]: loss 0.689973
[epoch6, step2387]: loss 0.176459
[epoch6, step2388]: loss 0.424428
[epoch6, step2389]: loss 0.739814
[epoch6, step2390]: loss 0.399900
[epoch6, step2391]: loss 0.640321
[epoch6, step2392]: loss 0.599684
[epoch6, step2393]: loss 0.553998
[epoch6, step2394]: loss 0.523402
[epoch6, step2395]: loss 0.516487
[epoch6, step2396]: loss 0.511210
[epoch6, step2397]: loss 0.498648
[epoch6, step2398]: loss 0.795502
[epoch6, step2399]: loss 0.527696
[epoch6, step2400]: loss 0.784767
[epoch6, step2401]: loss 0.283775
[epoch6, step2402]: loss 0.471526
[epoch6, step2403]: loss 0.230123
[epoch6, step2404]: loss 0.764477
[epoch6, step2405]: loss 0.679447
[epoch6, step2406]: loss 0.350753
[epoch6, step2407]: loss 0.516476
[epoch6, step2408]: loss 0.351380
[epoch6, step2409]: loss 0.370098
[epoch6, step2410]: loss 0.541032
[epoch6, step2411]: loss 0.258733
[epoch6, step2412]: loss 0.352036
[epoch6, step2413]: loss 0.423886
[epoch6, step2414]: loss 0.450258
[epoch6, step2415]: loss 0.617501
[epoch6, step2416]: loss 0.397710
[epoch6, step2417]: loss 0.570166
[epoch6, step2418]: loss 0.503983
[epoch6, step2419]: loss 0.562276
[epoch6, step2420]: loss 0.364935
[epoch6, step2421]: loss 0.471839
[epoch6, step2422]: loss 0.650684
[epoch6, step2423]: loss 0.412598
[epoch6, step2424]: loss 0.523719
[epoch6, step2425]: loss 0.307578
[epoch6, step2426]: loss 0.578519
[epoch6, step2427]: loss 0.795356
[epoch6, step2428]: loss 0.429157
[epoch6, step2429]: loss 0.447463
[epoch6, step2430]: loss 0.773290
[epoch6, step2431]: loss 0.423181
[epoch6, step2432]: loss 0.627066
[epoch6, step2433]: loss 0.405741
[epoch6, step2434]: loss 0.519251
[epoch6, step2435]: loss 0.718460
[epoch6, step2436]: loss 0.565711
[epoch6, step2437]: loss 0.441645
[epoch6, step2438]: loss 0.455366
[epoch6, step2439]: loss 0.425096
[epoch6, step2440]: loss 0.566983
[epoch6, step2441]: loss 0.500635
[epoch6, step2442]: loss 0.626441
[epoch6, step2443]: loss 0.558560
[epoch6, step2444]: loss 0.549553
[epoch6, step2445]: loss 0.504938
[epoch6, step2446]: loss 0.344077
[epoch6, step2447]: loss 0.350016
[epoch6, step2448]: loss 0.263467
[epoch6, step2449]: loss 0.549319
[epoch6, step2450]: loss 0.570016
[epoch6, step2451]: loss 0.440777
[epoch6, step2452]: loss 0.364748
[epoch6, step2453]: loss 0.836092
[epoch6, step2454]: loss 0.524524
[epoch6, step2455]: loss 0.507354
[epoch6, step2456]: loss 0.495762
[epoch6, step2457]: loss 0.548991
[epoch6, step2458]: loss 0.529235
[epoch6, step2459]: loss 0.521563
[epoch6, step2460]: loss 0.626947
[epoch6, step2461]: loss 0.734512
[epoch6, step2462]: loss 0.446552
[epoch6, step2463]: loss 0.678645
[epoch6, step2464]: loss 0.571172
[epoch6, step2465]: loss 0.623878
[epoch6, step2466]: loss 0.475684
[epoch6, step2467]: loss 0.558267
[epoch6, step2468]: loss 0.433320
[epoch6, step2469]: loss 0.677243
[epoch6, step2470]: loss 0.666977
[epoch6, step2471]: loss 0.397719
[epoch6, step2472]: loss 0.437304
[epoch6, step2473]: loss 0.403186
[epoch6, step2474]: loss 0.434467
[epoch6, step2475]: loss 0.526601
[epoch6, step2476]: loss 0.535334
[epoch6, step2477]: loss 0.488619
[epoch6, step2478]: loss 0.561638
[epoch6, step2479]: loss 0.544336
[epoch6, step2480]: loss 0.384807
[epoch6, step2481]: loss 0.535814
[epoch6, step2482]: loss 0.519103
[epoch6, step2483]: loss 0.620749
[epoch6, step2484]: loss 0.545665
[epoch6, step2485]: loss 0.418199
[epoch6, step2486]: loss 0.403608
[epoch6, step2487]: loss 0.897752
[epoch6, step2488]: loss 0.607281
[epoch6, step2489]: loss 0.407783
[epoch6, step2490]: loss 0.618982
[epoch6, step2491]: loss 0.429014
[epoch6, step2492]: loss 0.473616
[epoch6, step2493]: loss 0.495887
[epoch6, step2494]: loss 0.607156
[epoch6, step2495]: loss 0.466287
[epoch6, step2496]: loss 0.551983
[epoch6, step2497]: loss 0.710507
[epoch6, step2498]: loss 0.676417
[epoch6, step2499]: loss 0.376034
[epoch6, step2500]: loss 0.449806
[epoch6, step2501]: loss 0.586370
[epoch6, step2502]: loss 0.664465
[epoch6, step2503]: loss 0.664388
[epoch6, step2504]: loss 0.614116
[epoch6, step2505]: loss 0.493758
[epoch6, step2506]: loss 0.478158
[epoch6, step2507]: loss 0.826560
[epoch6, step2508]: loss 0.551650
[epoch6, step2509]: loss 0.557003
[epoch6, step2510]: loss 0.426264
[epoch6, step2511]: loss 0.287544
[epoch6, step2512]: loss 0.290831
[epoch6, step2513]: loss 0.654921
[epoch6, step2514]: loss 0.557943
[epoch6, step2515]: loss 0.426404
[epoch6, step2516]: loss 0.632704
[epoch6, step2517]: loss 0.710211
[epoch6, step2518]: loss 0.728234
[epoch6, step2519]: loss 0.638966
[epoch6, step2520]: loss 0.487392
[epoch6, step2521]: loss 0.436168
[epoch6, step2522]: loss 0.558673
[epoch6, step2523]: loss 0.574077
[epoch6, step2524]: loss 0.710293
[epoch6, step2525]: loss 0.495378
[epoch6, step2526]: loss 0.570859
[epoch6, step2527]: loss 0.567919
[epoch6, step2528]: loss 0.449183
[epoch6, step2529]: loss 0.673838
[epoch6, step2530]: loss 0.350228
[epoch6, step2531]: loss 0.462843
[epoch6, step2532]: loss 0.589393
[epoch6, step2533]: loss 0.599421
[epoch6, step2534]: loss 0.346244
[epoch6, step2535]: loss 0.785189
[epoch6, step2536]: loss 0.744133
[epoch6, step2537]: loss 0.663685
[epoch6, step2538]: loss 0.537886
[epoch6, step2539]: loss 0.559784
[epoch6, step2540]: loss 0.166786
[epoch6, step2541]: loss 0.713515
[epoch6, step2542]: loss 0.487262
[epoch6, step2543]: loss 0.552696
[epoch6, step2544]: loss 0.727834
[epoch6, step2545]: loss 0.606794
[epoch6, step2546]: loss 0.408127
[epoch6, step2547]: loss 0.330959
[epoch6, step2548]: loss 0.677936
[epoch6, step2549]: loss 0.483488
[epoch6, step2550]: loss 0.416097
[epoch6, step2551]: loss 0.606697
[epoch6, step2552]: loss 0.772940
[epoch6, step2553]: loss 0.717961
[epoch6, step2554]: loss 0.564501
[epoch6, step2555]: loss 0.362152
[epoch6, step2556]: loss 0.490162
[epoch6, step2557]: loss 0.677254
[epoch6, step2558]: loss 0.442132
[epoch6, step2559]: loss 0.508441
[epoch6, step2560]: loss 0.480424
[epoch6, step2561]: loss 0.794831
[epoch6, step2562]: loss 0.527538
[epoch6, step2563]: loss 0.617067
[epoch6, step2564]: loss 0.416290
[epoch6, step2565]: loss 0.570578
[epoch6, step2566]: loss 0.574711
[epoch6, step2567]: loss 0.535187
[epoch6, step2568]: loss 0.612202
[epoch6, step2569]: loss 0.674191
[epoch6, step2570]: loss 0.539621
[epoch6, step2571]: loss 0.387086
[epoch6, step2572]: loss 0.541381
[epoch6, step2573]: loss 0.353994
[epoch6, step2574]: loss 0.713914
[epoch6, step2575]: loss 0.458912
[epoch6, step2576]: loss 0.522447
[epoch6, step2577]: loss 0.632374
[epoch6, step2578]: loss 0.419967
[epoch6, step2579]: loss 0.443887
[epoch6, step2580]: loss 0.679336
[epoch6, step2581]: loss 0.288544
[epoch6, step2582]: loss 0.500310
[epoch6, step2583]: loss 0.446064
[epoch6, step2584]: loss 0.778911
[epoch6, step2585]: loss 0.491644
[epoch6, step2586]: loss 0.666615
[epoch6, step2587]: loss 0.747013
[epoch6, step2588]: loss 0.416359
[epoch6, step2589]: loss 0.529186
[epoch6, step2590]: loss 0.623170
[epoch6, step2591]: loss 0.831923
[epoch6, step2592]: loss 0.502874
[epoch6, step2593]: loss 0.561339
[epoch6, step2594]: loss 0.546512
[epoch6, step2595]: loss 0.650846
[epoch6, step2596]: loss 0.599653
[epoch6, step2597]: loss 0.467579
[epoch6, step2598]: loss 0.466535
[epoch6, step2599]: loss 0.690725
[epoch6, step2600]: loss 0.585138
[epoch6, step2601]: loss 0.638032
[epoch6, step2602]: loss 0.464937
[epoch6, step2603]: loss 0.578799
[epoch6, step2604]: loss 0.404121
[epoch6, step2605]: loss 0.587609
[epoch6, step2606]: loss 0.532479
[epoch6, step2607]: loss 0.477196
[epoch6, step2608]: loss 0.662124
[epoch6, step2609]: loss 0.138339
[epoch6, step2610]: loss 0.447485
[epoch6, step2611]: loss 0.488090
[epoch6, step2612]: loss 0.549726
[epoch6, step2613]: loss 0.615749
[epoch6, step2614]: loss 0.521210
[epoch6, step2615]: loss 0.602100
[epoch6, step2616]: loss 0.388608
[epoch6, step2617]: loss 0.375311
[epoch6, step2618]: loss 0.651864
[epoch6, step2619]: loss 0.518446
[epoch6, step2620]: loss 0.443076
[epoch6, step2621]: loss 0.397143
[epoch6, step2622]: loss 0.473141
[epoch6, step2623]: loss 0.634440
[epoch6, step2624]: loss 0.405687
[epoch6, step2625]: loss 0.717723
[epoch6, step2626]: loss 0.768654
[epoch6, step2627]: loss 0.740254
[epoch6, step2628]: loss 0.611570
[epoch6, step2629]: loss 0.517845
[epoch6, step2630]: loss 0.637995
[epoch6, step2631]: loss 0.193731
[epoch6, step2632]: loss 0.543389
[epoch6, step2633]: loss 0.564655
[epoch6, step2634]: loss 0.525023
[epoch6, step2635]: loss 0.503594
[epoch6, step2636]: loss 0.841376
[epoch6, step2637]: loss 0.445550
[epoch6, step2638]: loss 0.692007
[epoch6, step2639]: loss 0.546802
[epoch6, step2640]: loss 0.583762
[epoch6, step2641]: loss 0.537689
[epoch6, step2642]: loss 0.448099
[epoch6, step2643]: loss 0.583912
[epoch6, step2644]: loss 0.553048
[epoch6, step2645]: loss 0.122878
[epoch6, step2646]: loss 0.589856
[epoch6, step2647]: loss 0.438335
[epoch6, step2648]: loss 0.644317
[epoch6, step2649]: loss 0.669586
[epoch6, step2650]: loss 0.687465
[epoch6, step2651]: loss 0.552212
[epoch6, step2652]: loss 0.425428
[epoch6, step2653]: loss 0.544419
[epoch6, step2654]: loss 0.361850
[epoch6, step2655]: loss 0.508656
[epoch6, step2656]: loss 0.328959
[epoch6, step2657]: loss 0.749646
[epoch6, step2658]: loss 0.691939
[epoch6, step2659]: loss 0.403331
[epoch6, step2660]: loss 0.477218
[epoch6, step2661]: loss 0.398308
[epoch6, step2662]: loss 0.654059
[epoch6, step2663]: loss 0.478327
[epoch6, step2664]: loss 0.720013
[epoch6, step2665]: loss 0.428288
[epoch6, step2666]: loss 0.422680
[epoch6, step2667]: loss 0.493374
[epoch6, step2668]: loss 0.685539
[epoch6, step2669]: loss 0.491623
[epoch6, step2670]: loss 0.448252
[epoch6, step2671]: loss 0.562630
[epoch6, step2672]: loss 0.476170
[epoch6, step2673]: loss 0.512963
[epoch6, step2674]: loss 0.760218
[epoch6, step2675]: loss 0.346797
[epoch6, step2676]: loss 0.594201
[epoch6, step2677]: loss 0.385746
[epoch6, step2678]: loss 0.423365
[epoch6, step2679]: loss 0.546991
[epoch6, step2680]: loss 0.353845
[epoch6, step2681]: loss 0.656208
[epoch6, step2682]: loss 0.616745
[epoch6, step2683]: loss 0.164222
[epoch6, step2684]: loss 0.529031
[epoch6, step2685]: loss 0.582555
[epoch6, step2686]: loss 0.253850
[epoch6, step2687]: loss 0.495001
[epoch6, step2688]: loss 0.367977
[epoch6, step2689]: loss 0.581445
[epoch6, step2690]: loss 0.670912
[epoch6, step2691]: loss 0.370832
[epoch6, step2692]: loss 0.725986
[epoch6, step2693]: loss 0.807930
[epoch6, step2694]: loss 0.583909
[epoch6, step2695]: loss 0.460094
[epoch6, step2696]: loss 0.508646
[epoch6, step2697]: loss 0.275339
[epoch6, step2698]: loss 0.707110
[epoch6, step2699]: loss 0.375132
[epoch6, step2700]: loss 0.601702
[epoch6, step2701]: loss 0.385149
[epoch6, step2702]: loss 0.649165
[epoch6, step2703]: loss 0.440653
[epoch6, step2704]: loss 0.522642
[epoch6, step2705]: loss 0.720750
[epoch6, step2706]: loss 0.783089
[epoch6, step2707]: loss 0.704806
[epoch6, step2708]: loss 0.301442
[epoch6, step2709]: loss 0.384088
[epoch6, step2710]: loss 0.486814
[epoch6, step2711]: loss 0.549749
[epoch6, step2712]: loss 0.283042
[epoch6, step2713]: loss 0.527067
[epoch6, step2714]: loss 0.705626
[epoch6, step2715]: loss 0.508358
[epoch6, step2716]: loss 0.528727
[epoch6, step2717]: loss 0.517274
[epoch6, step2718]: loss 0.720505
[epoch6, step2719]: loss 0.458162
[epoch6, step2720]: loss 0.346964
[epoch6, step2721]: loss 0.654369
[epoch6, step2722]: loss 0.446895
[epoch6, step2723]: loss 0.598071
[epoch6, step2724]: loss 0.690024
[epoch6, step2725]: loss 0.318322
[epoch6, step2726]: loss 0.440230
[epoch6, step2727]: loss 0.310730
[epoch6, step2728]: loss 0.698008
[epoch6, step2729]: loss 0.590451
[epoch6, step2730]: loss 0.859256
[epoch6, step2731]: loss 0.726986
[epoch6, step2732]: loss 0.727682
[epoch6, step2733]: loss 0.562926
[epoch6, step2734]: loss 0.506259
[epoch6, step2735]: loss 0.685316
[epoch6, step2736]: loss 0.704019
[epoch6, step2737]: loss 0.724927
[epoch6, step2738]: loss 0.460901
[epoch6, step2739]: loss 0.398843
[epoch6, step2740]: loss 0.574250
[epoch6, step2741]: loss 0.605498
[epoch6, step2742]: loss 0.357598
[epoch6, step2743]: loss 0.728613
[epoch6, step2744]: loss 0.618700
[epoch6, step2745]: loss 0.375624
[epoch6, step2746]: loss 0.484621
[epoch6, step2747]: loss 0.658065
[epoch6, step2748]: loss 0.763887
[epoch6, step2749]: loss 0.528230
[epoch6, step2750]: loss 0.323913
[epoch6, step2751]: loss 0.589662
[epoch6, step2752]: loss 0.192509
[epoch6, step2753]: loss 0.559648
[epoch6, step2754]: loss 0.463375
[epoch6, step2755]: loss 0.658304
[epoch6, step2756]: loss 0.519856
[epoch6, step2757]: loss 0.373245
[epoch6, step2758]: loss 0.572696
[epoch6, step2759]: loss 0.476449
[epoch6, step2760]: loss 0.499175
[epoch6, step2761]: loss 0.593496
[epoch6, step2762]: loss 0.624397
[epoch6, step2763]: loss 0.445626
[epoch6, step2764]: loss 0.592356
[epoch6, step2765]: loss 0.707089
[epoch6, step2766]: loss 0.592156
[epoch6, step2767]: loss 0.467022
[epoch6, step2768]: loss 0.682176
[epoch6, step2769]: loss 0.602888
[epoch6, step2770]: loss 0.430342
[epoch6, step2771]: loss 0.271914
[epoch6, step2772]: loss 0.289775
[epoch6, step2773]: loss 0.647727
[epoch6, step2774]: loss 0.701181
[epoch6, step2775]: loss 0.450547
[epoch6, step2776]: loss 0.639624
[epoch6, step2777]: loss 0.618645
[epoch6, step2778]: loss 0.535498
[epoch6, step2779]: loss 0.686960
[epoch6, step2780]: loss 0.273798
[epoch6, step2781]: loss 0.546016
[epoch6, step2782]: loss 0.458422
[epoch6, step2783]: loss 0.409236
[epoch6, step2784]: loss 0.663152
[epoch6, step2785]: loss 0.485809
[epoch6, step2786]: loss 0.631991
[epoch6, step2787]: loss 0.500423
[epoch6, step2788]: loss 0.324858
[epoch6, step2789]: loss 0.575791
[epoch6, step2790]: loss 0.173824
[epoch6, step2791]: loss 0.366058
[epoch6, step2792]: loss 0.353493
[epoch6, step2793]: loss 0.340361
[epoch6, step2794]: loss 0.553057
[epoch6, step2795]: loss 0.738349
[epoch6, step2796]: loss 0.622824
[epoch6, step2797]: loss 0.481008
[epoch6, step2798]: loss 0.667746
[epoch6, step2799]: loss 0.592486
[epoch6, step2800]: loss 0.417735
[epoch6, step2801]: loss 0.626311
[epoch6, step2802]: loss 0.642474
[epoch6, step2803]: loss 0.548765
[epoch6, step2804]: loss 0.641553
[epoch6, step2805]: loss 0.461893
[epoch6, step2806]: loss 0.147439
[epoch6, step2807]: loss 0.556377
[epoch6, step2808]: loss 0.503777
[epoch6, step2809]: loss 0.605209
[epoch6, step2810]: loss 0.729734
[epoch6, step2811]: loss 0.519874
[epoch6, step2812]: loss 0.561147
[epoch6, step2813]: loss 0.332141
[epoch6, step2814]: loss 0.634686
[epoch6, step2815]: loss 0.485737
[epoch6, step2816]: loss 0.573494
[epoch6, step2817]: loss 0.281570
[epoch6, step2818]: loss 0.330465
[epoch6, step2819]: loss 0.532629
[epoch6, step2820]: loss 0.386217
[epoch6, step2821]: loss 0.489618
[epoch6, step2822]: loss 0.774299
[epoch6, step2823]: loss 0.353939
[epoch6, step2824]: loss 0.668239
[epoch6, step2825]: loss 0.612910
[epoch6, step2826]: loss 0.560006
[epoch6, step2827]: loss 0.579169
[epoch6, step2828]: loss 0.689471
[epoch6, step2829]: loss 0.386557
[epoch6, step2830]: loss 0.577564
[epoch6, step2831]: loss 0.614074
[epoch6, step2832]: loss 0.380462
[epoch6, step2833]: loss 0.784140
[epoch6, step2834]: loss 0.173772
[epoch6, step2835]: loss 0.558534
[epoch6, step2836]: loss 0.498847
[epoch6, step2837]: loss 0.143166
[epoch6, step2838]: loss 0.443868
[epoch6, step2839]: loss 0.389223
[epoch6, step2840]: loss 0.788639
[epoch6, step2841]: loss 0.726619
[epoch6, step2842]: loss 0.568810
[epoch6, step2843]: loss 0.582956
[epoch6, step2844]: loss 0.249883
[epoch6, step2845]: loss 0.629485
[epoch6, step2846]: loss 0.764405
[epoch6, step2847]: loss 0.395121
[epoch6, step2848]: loss 0.568646
[epoch6, step2849]: loss 0.712416
[epoch6, step2850]: loss 0.500656
[epoch6, step2851]: loss 0.419293
[epoch6, step2852]: loss 0.323440
[epoch6, step2853]: loss 0.647094
[epoch6, step2854]: loss 0.553083
[epoch6, step2855]: loss 0.620970
[epoch6, step2856]: loss 0.506122
[epoch6, step2857]: loss 0.620595
[epoch6, step2858]: loss 0.465724
[epoch6, step2859]: loss 0.257237
[epoch6, step2860]: loss 0.647177
[epoch6, step2861]: loss 0.585438
[epoch6, step2862]: loss 0.752680
[epoch6, step2863]: loss 0.590656
[epoch6, step2864]: loss 0.688600
[epoch6, step2865]: loss 0.312033
[epoch6, step2866]: loss 0.602593
[epoch6, step2867]: loss 0.272151
[epoch6, step2868]: loss 0.555762
[epoch6, step2869]: loss 0.645485
[epoch6, step2870]: loss 0.620394
[epoch6, step2871]: loss 0.509614
[epoch6, step2872]: loss 0.685916
[epoch6, step2873]: loss 0.529596
[epoch6, step2874]: loss 0.539732
[epoch6, step2875]: loss 0.660072
[epoch6, step2876]: loss 0.350343
[epoch6, step2877]: loss 0.546658
[epoch6, step2878]: loss 0.525779
[epoch6, step2879]: loss 0.564267
[epoch6, step2880]: loss 0.612147
[epoch6, step2881]: loss 0.415296
[epoch6, step2882]: loss 0.628872
[epoch6, step2883]: loss 0.578548
[epoch6, step2884]: loss 0.637270
[epoch6, step2885]: loss 0.399660
[epoch6, step2886]: loss 0.511998
[epoch6, step2887]: loss 0.399389
[epoch6, step2888]: loss 0.300840
[epoch6, step2889]: loss 0.176033
[epoch6, step2890]: loss 0.645084
[epoch6, step2891]: loss 0.520853
[epoch6, step2892]: loss 0.694097
[epoch6, step2893]: loss 0.525820
[epoch6, step2894]: loss 0.492855
[epoch6, step2895]: loss 0.294322
[epoch6, step2896]: loss 0.562839
[epoch6, step2897]: loss 0.344754
[epoch6, step2898]: loss 0.630535
[epoch6, step2899]: loss 0.521393
[epoch6, step2900]: loss 0.594149
[epoch6, step2901]: loss 0.707551
[epoch6, step2902]: loss 0.705167
[epoch6, step2903]: loss 0.458559
[epoch6, step2904]: loss 0.602767
[epoch6, step2905]: loss 0.511216
[epoch6, step2906]: loss 0.391684
[epoch6, step2907]: loss 0.424678
[epoch6, step2908]: loss 0.539494
[epoch6, step2909]: loss 0.607708
[epoch6, step2910]: loss 0.671622
[epoch6, step2911]: loss 0.366152
[epoch6, step2912]: loss 0.716207
[epoch6, step2913]: loss 0.586783
[epoch6, step2914]: loss 0.457047
[epoch6, step2915]: loss 0.541645
[epoch6, step2916]: loss 0.596754
[epoch6, step2917]: loss 0.670083
[epoch6, step2918]: loss 0.735826
[epoch6, step2919]: loss 0.663156
[epoch6, step2920]: loss 0.631646
[epoch6, step2921]: loss 0.471996
[epoch6, step2922]: loss 0.727477
[epoch6, step2923]: loss 0.395928
[epoch6, step2924]: loss 0.488297
[epoch6, step2925]: loss 0.532784
[epoch6, step2926]: loss 0.611558
[epoch6, step2927]: loss 0.539422
[epoch6, step2928]: loss 0.827901
[epoch6, step2929]: loss 0.573448
[epoch6, step2930]: loss 0.847665
[epoch6, step2931]: loss 0.748806
[epoch6, step2932]: loss 0.501924
[epoch6, step2933]: loss 0.498465
[epoch6, step2934]: loss 0.489428
[epoch6, step2935]: loss 0.579379
[epoch6, step2936]: loss 0.440496
[epoch6, step2937]: loss 0.838343
[epoch6, step2938]: loss 0.484440
[epoch6, step2939]: loss 0.518398
[epoch6, step2940]: loss 0.639200
[epoch6, step2941]: loss 0.625016
[epoch6, step2942]: loss 0.742724
[epoch6, step2943]: loss 0.489551
[epoch6, step2944]: loss 0.421324
[epoch6, step2945]: loss 0.398576
[epoch6, step2946]: loss 0.373733
[epoch6, step2947]: loss 0.577282
[epoch6, step2948]: loss 0.390130
[epoch6, step2949]: loss 0.386312
[epoch6, step2950]: loss 0.630907
[epoch6, step2951]: loss 0.621022
[epoch6, step2952]: loss 0.384478
[epoch6, step2953]: loss 0.669873
[epoch6, step2954]: loss 0.770288
[epoch6, step2955]: loss 0.425168
[epoch6, step2956]: loss 0.305218
[epoch6, step2957]: loss 0.313678
[epoch6, step2958]: loss 0.313148
[epoch6, step2959]: loss 0.545725
[epoch6, step2960]: loss 0.590299
[epoch6, step2961]: loss 0.625583
[epoch6, step2962]: loss 0.405690
[epoch6, step2963]: loss 0.597524
[epoch6, step2964]: loss 0.749266
[epoch6, step2965]: loss 0.776392
[epoch6, step2966]: loss 0.555405
[epoch6, step2967]: loss 0.590671
[epoch6, step2968]: loss 0.610067
[epoch6, step2969]: loss 0.317471
[epoch6, step2970]: loss 0.570975
[epoch6, step2971]: loss 0.756171
[epoch6, step2972]: loss 0.612850
[epoch6, step2973]: loss 0.311392
[epoch6, step2974]: loss 0.512162
[epoch6, step2975]: loss 0.617617
[epoch6, step2976]: loss 0.379119
[epoch6, step2977]: loss 0.536656
[epoch6, step2978]: loss 0.594538
[epoch6, step2979]: loss 0.700216
[epoch6, step2980]: loss 0.150150
[epoch6, step2981]: loss 0.716042
[epoch6, step2982]: loss 0.516657
[epoch6, step2983]: loss 0.540761
[epoch6, step2984]: loss 0.648270
[epoch6, step2985]: loss 0.545817
[epoch6, step2986]: loss 0.560096
[epoch6, step2987]: loss 0.763898
[epoch6, step2988]: loss 0.480869
[epoch6, step2989]: loss 0.421079
[epoch6, step2990]: loss 0.593737
[epoch6, step2991]: loss 0.582708
[epoch6, step2992]: loss 0.573185
[epoch6, step2993]: loss 0.628736
[epoch6, step2994]: loss 0.621787
[epoch6, step2995]: loss 0.459053
[epoch6, step2996]: loss 0.737853
[epoch6, step2997]: loss 0.433319
[epoch6, step2998]: loss 0.590398
[epoch6, step2999]: loss 0.178820
[epoch6, step3000]: loss 0.624560
[epoch6, step3001]: loss 0.588382
[epoch6, step3002]: loss 0.552821
[epoch6, step3003]: loss 0.410110
[epoch6, step3004]: loss 0.524580
[epoch6, step3005]: loss 0.811976
[epoch6, step3006]: loss 0.648694
[epoch6, step3007]: loss 0.237665
[epoch6, step3008]: loss 0.398030
[epoch6, step3009]: loss 0.668688
[epoch6, step3010]: loss 0.507757
[epoch6, step3011]: loss 0.491779
[epoch6, step3012]: loss 0.706707
[epoch6, step3013]: loss 0.483319
[epoch6, step3014]: loss 0.272806
[epoch6, step3015]: loss 0.477086
[epoch6, step3016]: loss 0.464980
[epoch6, step3017]: loss 0.764783
[epoch6, step3018]: loss 0.695398
[epoch6, step3019]: loss 0.610585
[epoch6, step3020]: loss 0.636074
[epoch6, step3021]: loss 0.820763
[epoch6, step3022]: loss 0.625272
[epoch6, step3023]: loss 0.775615
[epoch6, step3024]: loss 0.579593
[epoch6, step3025]: loss 0.519754
[epoch6, step3026]: loss 0.366012
[epoch6, step3027]: loss 0.281786
[epoch6, step3028]: loss 0.609933
[epoch6, step3029]: loss 0.201706
[epoch6, step3030]: loss 0.469379
[epoch6, step3031]: loss 0.319323
[epoch6, step3032]: loss 0.474575
[epoch6, step3033]: loss 0.611384
[epoch6, step3034]: loss 0.296632
[epoch6, step3035]: loss 0.471501
[epoch6, step3036]: loss 0.825801
[epoch6, step3037]: loss 0.506807
[epoch6, step3038]: loss 0.529974
[epoch6, step3039]: loss 0.446777
[epoch6, step3040]: loss 0.485548
[epoch6, step3041]: loss 0.523908
[epoch6, step3042]: loss 0.575092
[epoch6, step3043]: loss 0.545937
[epoch6, step3044]: loss 0.481502
[epoch6, step3045]: loss 0.456572
[epoch6, step3046]: loss 0.701501
[epoch6, step3047]: loss 0.480387
[epoch6, step3048]: loss 0.364310
[epoch6, step3049]: loss 0.565372
[epoch6, step3050]: loss 0.594423
[epoch6, step3051]: loss 0.540632
[epoch6, step3052]: loss 0.646864
[epoch6, step3053]: loss 0.607240
[epoch6, step3054]: loss 0.492979
[epoch6, step3055]: loss 0.687982
[epoch6, step3056]: loss 0.190281
[epoch6, step3057]: loss 0.519191
[epoch6, step3058]: loss 0.611192
[epoch6, step3059]: loss 0.722915
[epoch6, step3060]: loss 0.677930
[epoch6, step3061]: loss 0.216163
[epoch6, step3062]: loss 0.543560
[epoch6, step3063]: loss 0.628561
[epoch6, step3064]: loss 0.674421
[epoch6, step3065]: loss 0.577448
[epoch6, step3066]: loss 0.553020
[epoch6, step3067]: loss 0.634281
[epoch6, step3068]: loss 0.553797
[epoch6, step3069]: loss 0.668831
[epoch6, step3070]: loss 0.712122
[epoch6, step3071]: loss 0.538589
[epoch6, step3072]: loss 0.189417
[epoch6, step3073]: loss 0.618703
[epoch6, step3074]: loss 0.587071
[epoch6, step3075]: loss 0.713251
[epoch6, step3076]: loss 0.531936

[epoch6]: avg loss 0.531936

[epoch7, step1]: loss 0.513806
[epoch7, step2]: loss 0.505446
[epoch7, step3]: loss 0.798622
[epoch7, step4]: loss 0.517604
[epoch7, step5]: loss 0.468129
[epoch7, step6]: loss 0.390421
[epoch7, step7]: loss 0.383605
[epoch7, step8]: loss 0.553998
[epoch7, step9]: loss 0.584059
[epoch7, step10]: loss 0.441685
[epoch7, step11]: loss 0.527124
[epoch7, step12]: loss 0.584626
[epoch7, step13]: loss 0.529711
[epoch7, step14]: loss 0.348090
[epoch7, step15]: loss 0.532551
[epoch7, step16]: loss 0.546724
[epoch7, step17]: loss 0.704120
[epoch7, step18]: loss 0.438064
[epoch7, step19]: loss 0.603615
[epoch7, step20]: loss 0.556198
[epoch7, step21]: loss 0.705476
[epoch7, step22]: loss 0.540846
[epoch7, step23]: loss 0.457812
[epoch7, step24]: loss 0.605212
[epoch7, step25]: loss 0.567481
[epoch7, step26]: loss 0.586734
[epoch7, step27]: loss 0.527957
[epoch7, step28]: loss 0.688186
[epoch7, step29]: loss 0.665640
[epoch7, step30]: loss 0.675847
[epoch7, step31]: loss 0.435826
[epoch7, step32]: loss 0.267903
[epoch7, step33]: loss 0.462715
[epoch7, step34]: loss 0.468564
[epoch7, step35]: loss 0.390727
[epoch7, step36]: loss 0.530147
[epoch7, step37]: loss 0.464867
[epoch7, step38]: loss 0.613437
[epoch7, step39]: loss 0.547150
[epoch7, step40]: loss 0.620551
[epoch7, step41]: loss 0.495788
[epoch7, step42]: loss 0.555709
[epoch7, step43]: loss 0.637740
[epoch7, step44]: loss 0.635458
[epoch7, step45]: loss 0.478048
[epoch7, step46]: loss 0.553402
[epoch7, step47]: loss 0.515530
[epoch7, step48]: loss 0.333610
[epoch7, step49]: loss 0.588627
[epoch7, step50]: loss 0.295628
[epoch7, step51]: loss 0.425669
[epoch7, step52]: loss 0.382410
[epoch7, step53]: loss 0.698236
[epoch7, step54]: loss 0.624872
[epoch7, step55]: loss 0.632469
[epoch7, step56]: loss 0.634786
[epoch7, step57]: loss 0.571559
[epoch7, step58]: loss 0.505661
[epoch7, step59]: loss 0.600477
[epoch7, step60]: loss 0.322828
[epoch7, step61]: loss 0.241039
[epoch7, step62]: loss 0.459308
[epoch7, step63]: loss 0.483452
[epoch7, step64]: loss 0.623920
[epoch7, step65]: loss 0.595519
[epoch7, step66]: loss 0.605875
[epoch7, step67]: loss 0.606267
[epoch7, step68]: loss 0.258419
[epoch7, step69]: loss 0.609069
[epoch7, step70]: loss 0.518938
[epoch7, step71]: loss 0.523162
[epoch7, step72]: loss 0.784162
[epoch7, step73]: loss 0.477502
[epoch7, step74]: loss 0.203926
[epoch7, step75]: loss 0.546305
[epoch7, step76]: loss 0.641917
[epoch7, step77]: loss 0.617009
[epoch7, step78]: loss 0.688543
[epoch7, step79]: loss 0.529041
[epoch7, step80]: loss 0.496651
[epoch7, step81]: loss 0.743542
[epoch7, step82]: loss 0.465268
[epoch7, step83]: loss 0.368839
[epoch7, step84]: loss 0.629402
[epoch7, step85]: loss 0.420323
[epoch7, step86]: loss 0.749249
[epoch7, step87]: loss 0.491044
[epoch7, step88]: loss 0.576707
[epoch7, step89]: loss 0.494626
[epoch7, step90]: loss 0.761933
[epoch7, step91]: loss 0.497589
[epoch7, step92]: loss 0.490446
[epoch7, step93]: loss 0.528949
[epoch7, step94]: loss 0.366404
[epoch7, step95]: loss 0.620642
[epoch7, step96]: loss 0.639406
[epoch7, step97]: loss 0.498911
[epoch7, step98]: loss 0.547536
[epoch7, step99]: loss 0.442812
[epoch7, step100]: loss 0.662648
[epoch7, step101]: loss 0.608437
[epoch7, step102]: loss 0.362922
[epoch7, step103]: loss 0.434633
[epoch7, step104]: loss 0.701171
[epoch7, step105]: loss 0.632148
[epoch7, step106]: loss 0.293867
[epoch7, step107]: loss 0.626272
[epoch7, step108]: loss 0.304461
[epoch7, step109]: loss 0.496118
[epoch7, step110]: loss 0.883879
[epoch7, step111]: loss 0.512767
[epoch7, step112]: loss 0.342369
[epoch7, step113]: loss 0.188015
[epoch7, step114]: loss 0.646095
[epoch7, step115]: loss 0.391721
[epoch7, step116]: loss 0.630927
[epoch7, step117]: loss 0.488022
[epoch7, step118]: loss 0.536302
[epoch7, step119]: loss 0.650950
[epoch7, step120]: loss 0.397145
[epoch7, step121]: loss 0.441397
[epoch7, step122]: loss 0.557298
[epoch7, step123]: loss 0.517095
[epoch7, step124]: loss 0.607210
[epoch7, step125]: loss 0.580225
[epoch7, step126]: loss 0.629829
[epoch7, step127]: loss 0.585588
[epoch7, step128]: loss 0.694834
[epoch7, step129]: loss 0.328268
[epoch7, step130]: loss 0.548208
[epoch7, step131]: loss 0.768420
[epoch7, step132]: loss 0.701681
[epoch7, step133]: loss 0.685471
[epoch7, step134]: loss 0.390382
[epoch7, step135]: loss 0.506747
[epoch7, step136]: loss 0.496420
[epoch7, step137]: loss 0.866056
[epoch7, step138]: loss 0.436389
[epoch7, step139]: loss 0.391034
[epoch7, step140]: loss 0.618997
[epoch7, step141]: loss 0.477204
[epoch7, step142]: loss 0.693891
[epoch7, step143]: loss 0.399367
[epoch7, step144]: loss 0.519282
[epoch7, step145]: loss 0.526688
[epoch7, step146]: loss 0.433315
[epoch7, step147]: loss 0.695675
[epoch7, step148]: loss 0.437254
[epoch7, step149]: loss 0.426262
[epoch7, step150]: loss 0.783221
[epoch7, step151]: loss 0.474069
[epoch7, step152]: loss 0.443172
[epoch7, step153]: loss 0.635016
[epoch7, step154]: loss 0.621588
[epoch7, step155]: loss 0.662790
[epoch7, step156]: loss 0.547731
[epoch7, step157]: loss 0.524448
[epoch7, step158]: loss 0.609736
[epoch7, step159]: loss 0.480882
[epoch7, step160]: loss 0.530014
[epoch7, step161]: loss 0.579684
[epoch7, step162]: loss 0.432261
[epoch7, step163]: loss 0.771110
[epoch7, step164]: loss 0.731638
[epoch7, step165]: loss 0.572928
[epoch7, step166]: loss 0.501897
[epoch7, step167]: loss 0.515513
[epoch7, step168]: loss 0.367238
[epoch7, step169]: loss 0.640433
[epoch7, step170]: loss 0.306282
[epoch7, step171]: loss 0.697023
[epoch7, step172]: loss 0.432225
[epoch7, step173]: loss 0.424947
[epoch7, step174]: loss 0.713220
[epoch7, step175]: loss 0.431192
[epoch7, step176]: loss 0.453373
[epoch7, step177]: loss 0.571718
[epoch7, step178]: loss 0.585233
[epoch7, step179]: loss 0.475535
[epoch7, step180]: loss 0.646542
[epoch7, step181]: loss 0.268557
[epoch7, step182]: loss 0.648856
[epoch7, step183]: loss 0.598202
[epoch7, step184]: loss 0.470170
[epoch7, step185]: loss 0.381771
[epoch7, step186]: loss 0.432917
[epoch7, step187]: loss 0.433829
[epoch7, step188]: loss 0.568910
[epoch7, step189]: loss 0.631188
[epoch7, step190]: loss 0.398975
[epoch7, step191]: loss 0.251952
[epoch7, step192]: loss 0.630573
[epoch7, step193]: loss 0.343825
[epoch7, step194]: loss 0.583306
[epoch7, step195]: loss 0.510983
[epoch7, step196]: loss 0.623432
[epoch7, step197]: loss 0.524426
[epoch7, step198]: loss 0.394722
[epoch7, step199]: loss 0.596098
[epoch7, step200]: loss 0.361724
[epoch7, step201]: loss 0.357185
[epoch7, step202]: loss 0.503932
[epoch7, step203]: loss 0.581717
[epoch7, step204]: loss 0.302843
[epoch7, step205]: loss 0.509158
[epoch7, step206]: loss 0.523641
[epoch7, step207]: loss 0.733426
[epoch7, step208]: loss 0.334153
[epoch7, step209]: loss 0.427600
[epoch7, step210]: loss 0.597845
[epoch7, step211]: loss 0.702082
[epoch7, step212]: loss 0.618709
[epoch7, step213]: loss 0.729526
[epoch7, step214]: loss 0.656622
[epoch7, step215]: loss 0.746974
[epoch7, step216]: loss 0.501717
[epoch7, step217]: loss 0.412627
[epoch7, step218]: loss 0.288089
[epoch7, step219]: loss 0.552067
[epoch7, step220]: loss 0.527323
[epoch7, step221]: loss 0.428977
[epoch7, step222]: loss 0.650243
[epoch7, step223]: loss 0.495634
[epoch7, step224]: loss 0.568400
[epoch7, step225]: loss 0.527338
[epoch7, step226]: loss 0.587033
[epoch7, step227]: loss 0.380759
[epoch7, step228]: loss 0.578239
[epoch7, step229]: loss 0.762673
[epoch7, step230]: loss 0.834007
[epoch7, step231]: loss 0.577134
[epoch7, step232]: loss 0.443193
[epoch7, step233]: loss 0.703507
[epoch7, step234]: loss 0.806888
[epoch7, step235]: loss 0.607832
[epoch7, step236]: loss 0.441388
[epoch7, step237]: loss 0.589173
[epoch7, step238]: loss 0.687855
[epoch7, step239]: loss 0.282792
[epoch7, step240]: loss 0.538725
[epoch7, step241]: loss 0.387910
[epoch7, step242]: loss 0.677676
[epoch7, step243]: loss 0.426952
[epoch7, step244]: loss 0.442612
[epoch7, step245]: loss 0.385773
[epoch7, step246]: loss 0.465305
[epoch7, step247]: loss 0.780596
[epoch7, step248]: loss 0.484356
[epoch7, step249]: loss 0.723965
[epoch7, step250]: loss 0.599832
[epoch7, step251]: loss 0.592870
[epoch7, step252]: loss 0.449993
[epoch7, step253]: loss 0.440344
[epoch7, step254]: loss 0.583158
[epoch7, step255]: loss 0.361767
[epoch7, step256]: loss 0.649753
[epoch7, step257]: loss 0.481175
[epoch7, step258]: loss 0.435601
[epoch7, step259]: loss 0.478322
[epoch7, step260]: loss 0.671264
[epoch7, step261]: loss 0.599359
[epoch7, step262]: loss 0.658631
[epoch7, step263]: loss 0.471221
[epoch7, step264]: loss 0.615543
[epoch7, step265]: loss 0.574777
[epoch7, step266]: loss 0.466845
[epoch7, step267]: loss 0.323943
[epoch7, step268]: loss 0.477181
[epoch7, step269]: loss 0.430041
[epoch7, step270]: loss 0.755718
[epoch7, step271]: loss 0.720231
[epoch7, step272]: loss 0.667471
[epoch7, step273]: loss 0.517059
[epoch7, step274]: loss 0.473096
[epoch7, step275]: loss 0.550089
[epoch7, step276]: loss 0.253009
[epoch7, step277]: loss 0.272978
[epoch7, step278]: loss 0.295956
[epoch7, step279]: loss 0.438715
[epoch7, step280]: loss 0.687390
[epoch7, step281]: loss 0.663241
[epoch7, step282]: loss 0.621210
[epoch7, step283]: loss 0.411994
[epoch7, step284]: loss 0.378598
[epoch7, step285]: loss 0.482240
[epoch7, step286]: loss 0.551902
[epoch7, step287]: loss 0.673253
[epoch7, step288]: loss 0.469354
[epoch7, step289]: loss 0.761270
[epoch7, step290]: loss 0.712624
[epoch7, step291]: loss 0.353842
[epoch7, step292]: loss 0.732409
[epoch7, step293]: loss 0.631513
[epoch7, step294]: loss 0.323736
[epoch7, step295]: loss 0.534681
[epoch7, step296]: loss 0.543796
[epoch7, step297]: loss 0.501399
[epoch7, step298]: loss 0.411216
[epoch7, step299]: loss 0.540176
[epoch7, step300]: loss 0.575072
[epoch7, step301]: loss 0.658424
[epoch7, step302]: loss 0.535370
[epoch7, step303]: loss 0.465832
[epoch7, step304]: loss 0.686238
[epoch7, step305]: loss 0.527817
[epoch7, step306]: loss 0.673055
[epoch7, step307]: loss 0.538751
[epoch7, step308]: loss 0.463981
[epoch7, step309]: loss 0.698094
[epoch7, step310]: loss 0.632381
[epoch7, step311]: loss 0.570817
[epoch7, step312]: loss 0.574667
[epoch7, step313]: loss 0.611136
[epoch7, step314]: loss 0.610029
[epoch7, step315]: loss 0.525365
[epoch7, step316]: loss 0.644123
[epoch7, step317]: loss 0.653136
[epoch7, step318]: loss 0.639455
[epoch7, step319]: loss 0.486905
[epoch7, step320]: loss 0.622728
[epoch7, step321]: loss 0.379934
[epoch7, step322]: loss 0.520032
[epoch7, step323]: loss 0.585937
[epoch7, step324]: loss 0.486347
[epoch7, step325]: loss 0.343275
[epoch7, step326]: loss 0.493036
[epoch7, step327]: loss 0.503346
[epoch7, step328]: loss 0.906989
[epoch7, step329]: loss 0.661356
[epoch7, step330]: loss 0.648831
[epoch7, step331]: loss 0.714536
[epoch7, step332]: loss 0.671726
[epoch7, step333]: loss 0.732795
[epoch7, step334]: loss 0.304649
[epoch7, step335]: loss 0.607414
[epoch7, step336]: loss 0.483002
[epoch7, step337]: loss 0.494173
[epoch7, step338]: loss 0.731768
[epoch7, step339]: loss 0.627542
[epoch7, step340]: loss 0.283139
[epoch7, step341]: loss 0.460087
[epoch7, step342]: loss 0.248105
[epoch7, step343]: loss 0.655502
[epoch7, step344]: loss 0.496153
[epoch7, step345]: loss 0.887301
[epoch7, step346]: loss 0.624281
[epoch7, step347]: loss 0.280267
[epoch7, step348]: loss 0.348150
[epoch7, step349]: loss 0.439671
[epoch7, step350]: loss 0.494975
[epoch7, step351]: loss 0.482855
[epoch7, step352]: loss 0.656415
[epoch7, step353]: loss 0.652717
[epoch7, step354]: loss 0.410178
[epoch7, step355]: loss 0.395083
[epoch7, step356]: loss 0.668357
[epoch7, step357]: loss 0.566978
[epoch7, step358]: loss 0.758058
[epoch7, step359]: loss 0.588603
[epoch7, step360]: loss 0.731561
[epoch7, step361]: loss 0.693959
[epoch7, step362]: loss 0.481084
[epoch7, step363]: loss 0.303056
[epoch7, step364]: loss 0.686654
[epoch7, step365]: loss 0.566583
[epoch7, step366]: loss 0.760631
[epoch7, step367]: loss 0.322746
[epoch7, step368]: loss 0.372272
[epoch7, step369]: loss 0.722017
[epoch7, step370]: loss 0.776024
[epoch7, step371]: loss 0.643743
[epoch7, step372]: loss 0.396737
[epoch7, step373]: loss 0.562580
[epoch7, step374]: loss 0.512242
[epoch7, step375]: loss 0.707085
[epoch7, step376]: loss 0.389681
[epoch7, step377]: loss 0.622015
[epoch7, step378]: loss 0.549141
[epoch7, step379]: loss 0.585100
[epoch7, step380]: loss 0.502873
[epoch7, step381]: loss 0.590313
[epoch7, step382]: loss 0.197741
[epoch7, step383]: loss 0.441625
[epoch7, step384]: loss 0.538264
[epoch7, step385]: loss 0.583227
[epoch7, step386]: loss 0.472941
[epoch7, step387]: loss 0.543461
[epoch7, step388]: loss 0.553013
[epoch7, step389]: loss 0.682715
[epoch7, step390]: loss 0.431503
[epoch7, step391]: loss 0.502921
[epoch7, step392]: loss 0.430751
[epoch7, step393]: loss 0.637425
[epoch7, step394]: loss 0.578421
[epoch7, step395]: loss 0.543337
[epoch7, step396]: loss 0.650722
[epoch7, step397]: loss 0.326920
[epoch7, step398]: loss 0.610680
[epoch7, step399]: loss 0.680255
[epoch7, step400]: loss 0.534566
[epoch7, step401]: loss 0.365517
[epoch7, step402]: loss 0.511262
[epoch7, step403]: loss 0.414161
[epoch7, step404]: loss 0.562958
[epoch7, step405]: loss 0.403922
[epoch7, step406]: loss 0.540864
[epoch7, step407]: loss 0.459668
[epoch7, step408]: loss 0.591053
[epoch7, step409]: loss 0.722127
[epoch7, step410]: loss 0.288855
[epoch7, step411]: loss 0.305209
[epoch7, step412]: loss 0.291209
[epoch7, step413]: loss 0.515571
[epoch7, step414]: loss 0.281405
[epoch7, step415]: loss 0.597767
[epoch7, step416]: loss 0.595609
[epoch7, step417]: loss 0.405107
[epoch7, step418]: loss 0.759333
[epoch7, step419]: loss 0.860703
[epoch7, step420]: loss 0.629532
[epoch7, step421]: loss 0.579012
[epoch7, step422]: loss 0.458124
[epoch7, step423]: loss 0.335469
[epoch7, step424]: loss 0.500164
[epoch7, step425]: loss 0.315343
[epoch7, step426]: loss 0.701801
[epoch7, step427]: loss 0.352145
[epoch7, step428]: loss 0.350472
[epoch7, step429]: loss 0.539135
[epoch7, step430]: loss 0.584409
[epoch7, step431]: loss 0.715754
[epoch7, step432]: loss 0.475789
[epoch7, step433]: loss 0.716455
[epoch7, step434]: loss 0.524002
[epoch7, step435]: loss 0.377041
[epoch7, step436]: loss 0.643081
[epoch7, step437]: loss 0.384811
[epoch7, step438]: loss 0.539232
[epoch7, step439]: loss 0.573149
[epoch7, step440]: loss 0.530317
[epoch7, step441]: loss 0.601323
[epoch7, step442]: loss 0.441814
[epoch7, step443]: loss 0.562009
[epoch7, step444]: loss 0.575335
[epoch7, step445]: loss 0.435638
[epoch7, step446]: loss 0.202162
[epoch7, step447]: loss 0.537412
[epoch7, step448]: loss 0.621532
[epoch7, step449]: loss 0.642356
[epoch7, step450]: loss 0.681491
[epoch7, step451]: loss 0.615426
[epoch7, step452]: loss 0.610831
[epoch7, step453]: loss 0.605256
[epoch7, step454]: loss 0.538783
[epoch7, step455]: loss 0.634193
[epoch7, step456]: loss 0.461551
[epoch7, step457]: loss 0.738445
[epoch7, step458]: loss 0.548749
[epoch7, step459]: loss 0.322710
[epoch7, step460]: loss 0.376124
[epoch7, step461]: loss 0.667529
[epoch7, step462]: loss 0.715816
[epoch7, step463]: loss 0.710115
[epoch7, step464]: loss 0.736900
[epoch7, step465]: loss 0.511614
[epoch7, step466]: loss 0.546885
[epoch7, step467]: loss 0.450919
[epoch7, step468]: loss 0.389856
[epoch7, step469]: loss 0.448221
[epoch7, step470]: loss 0.485835
[epoch7, step471]: loss 0.680127
[epoch7, step472]: loss 0.499015
[epoch7, step473]: loss 0.409472
[epoch7, step474]: loss 0.642483
[epoch7, step475]: loss 0.616984
[epoch7, step476]: loss 0.665946
[epoch7, step477]: loss 0.596216
[epoch7, step478]: loss 0.736493
[epoch7, step479]: loss 0.556112
[epoch7, step480]: loss 0.701302
[epoch7, step481]: loss 0.730732
[epoch7, step482]: loss 0.421556
[epoch7, step483]: loss 0.549806
[epoch7, step484]: loss 0.439138
[epoch7, step485]: loss 0.354081
[epoch7, step486]: loss 0.350139
[epoch7, step487]: loss 0.585720
[epoch7, step488]: loss 0.749192
[epoch7, step489]: loss 0.616140
[epoch7, step490]: loss 0.388516
[epoch7, step491]: loss 0.437418
[epoch7, step492]: loss 0.510044
[epoch7, step493]: loss 0.620967
[epoch7, step494]: loss 0.725358
[epoch7, step495]: loss 0.489678
[epoch7, step496]: loss 0.521645
[epoch7, step497]: loss 0.522017
[epoch7, step498]: loss 0.390787
[epoch7, step499]: loss 0.556838
[epoch7, step500]: loss 0.708944
[epoch7, step501]: loss 0.466467
[epoch7, step502]: loss 0.364249
[epoch7, step503]: loss 0.704298
[epoch7, step504]: loss 0.568777
[epoch7, step505]: loss 0.409123
[epoch7, step506]: loss 0.609975
[epoch7, step507]: loss 0.749524
[epoch7, step508]: loss 0.795960
[epoch7, step509]: loss 0.724367
[epoch7, step510]: loss 0.808449
[epoch7, step511]: loss 0.574582
[epoch7, step512]: loss 0.589602
[epoch7, step513]: loss 0.487005
[epoch7, step514]: loss 0.286599
[epoch7, step515]: loss 0.624134
[epoch7, step516]: loss 0.744855
[epoch7, step517]: loss 0.375037
[epoch7, step518]: loss 0.571242
[epoch7, step519]: loss 0.554168
[epoch7, step520]: loss 0.369220
[epoch7, step521]: loss 0.739170
[epoch7, step522]: loss 0.370651
[epoch7, step523]: loss 0.480329
[epoch7, step524]: loss 0.496327
[epoch7, step525]: loss 0.568436
[epoch7, step526]: loss 0.665304
[epoch7, step527]: loss 0.523705
[epoch7, step528]: loss 0.353003
[epoch7, step529]: loss 0.734990
[epoch7, step530]: loss 0.765714
[epoch7, step531]: loss 0.384170
[epoch7, step532]: loss 0.508731
[epoch7, step533]: loss 0.417365
[epoch7, step534]: loss 0.659747
[epoch7, step535]: loss 0.473917
[epoch7, step536]: loss 0.480447
[epoch7, step537]: loss 0.720563
[epoch7, step538]: loss 0.682432
[epoch7, step539]: loss 0.688988
[epoch7, step540]: loss 0.285881
[epoch7, step541]: loss 0.742575
[epoch7, step542]: loss 0.595317
[epoch7, step543]: loss 0.573644
[epoch7, step544]: loss 0.502066
[epoch7, step545]: loss 0.480526
[epoch7, step546]: loss 0.452654
[epoch7, step547]: loss 0.113578
[epoch7, step548]: loss 0.387055
[epoch7, step549]: loss 0.587319
[epoch7, step550]: loss 0.719145
[epoch7, step551]: loss 0.604469
[epoch7, step552]: loss 0.505394
[epoch7, step553]: loss 0.536355
[epoch7, step554]: loss 0.455957
[epoch7, step555]: loss 0.609883
[epoch7, step556]: loss 0.547614
[epoch7, step557]: loss 0.675220
[epoch7, step558]: loss 0.580046
[epoch7, step559]: loss 0.381856
[epoch7, step560]: loss 0.749325
[epoch7, step561]: loss 0.531086
[epoch7, step562]: loss 0.589135
[epoch7, step563]: loss 0.429717
[epoch7, step564]: loss 0.523273
[epoch7, step565]: loss 0.551963
[epoch7, step566]: loss 0.489161
[epoch7, step567]: loss 0.693556
[epoch7, step568]: loss 0.609796
[epoch7, step569]: loss 0.167691
[epoch7, step570]: loss 0.398135
[epoch7, step571]: loss 0.697227
[epoch7, step572]: loss 0.550540
[epoch7, step573]: loss 0.578967
[epoch7, step574]: loss 0.675109
[epoch7, step575]: loss 0.574777
[epoch7, step576]: loss 0.480716
[epoch7, step577]: loss 0.401644
[epoch7, step578]: loss 0.553584
[epoch7, step579]: loss 0.401687
[epoch7, step580]: loss 0.673746
[epoch7, step581]: loss 0.428008
[epoch7, step582]: loss 0.385325
[epoch7, step583]: loss 0.470659
[epoch7, step584]: loss 0.730741
[epoch7, step585]: loss 0.593958
[epoch7, step586]: loss 0.523758
[epoch7, step587]: loss 0.174228
[epoch7, step588]: loss 0.707384
[epoch7, step589]: loss 0.672505
[epoch7, step590]: loss 0.342653
[epoch7, step591]: loss 0.405361
[epoch7, step592]: loss 0.426147
[epoch7, step593]: loss 0.673068
[epoch7, step594]: loss 0.457734
[epoch7, step595]: loss 0.319409
[epoch7, step596]: loss 0.511942
[epoch7, step597]: loss 0.413617
[epoch7, step598]: loss 0.484346
[epoch7, step599]: loss 0.691480
[epoch7, step600]: loss 0.865387
[epoch7, step601]: loss 0.821258
[epoch7, step602]: loss 0.498299
[epoch7, step603]: loss 0.300565
[epoch7, step604]: loss 0.647948
[epoch7, step605]: loss 0.641205
[epoch7, step606]: loss 0.540336
[epoch7, step607]: loss 0.375088
[epoch7, step608]: loss 0.408528
[epoch7, step609]: loss 0.422935
[epoch7, step610]: loss 0.582631
[epoch7, step611]: loss 0.726239
[epoch7, step612]: loss 0.522910
[epoch7, step613]: loss 0.486565
[epoch7, step614]: loss 0.763892
[epoch7, step615]: loss 0.648128
[epoch7, step616]: loss 0.722887
[epoch7, step617]: loss 0.685644
[epoch7, step618]: loss 0.437378
[epoch7, step619]: loss 0.527306
[epoch7, step620]: loss 0.407814
[epoch7, step621]: loss 0.358455
[epoch7, step622]: loss 0.624015
[epoch7, step623]: loss 0.567087
[epoch7, step624]: loss 0.525512
[epoch7, step625]: loss 0.688608
[epoch7, step626]: loss 0.754205
[epoch7, step627]: loss 0.503277
[epoch7, step628]: loss 0.665525
[epoch7, step629]: loss 0.627583
[epoch7, step630]: loss 0.527112
[epoch7, step631]: loss 0.528666
[epoch7, step632]: loss 0.501097
[epoch7, step633]: loss 0.427268
[epoch7, step634]: loss 0.490382
[epoch7, step635]: loss 0.668264
[epoch7, step636]: loss 0.700181
[epoch7, step637]: loss 0.136186
[epoch7, step638]: loss 0.340895
[epoch7, step639]: loss 0.378910
[epoch7, step640]: loss 0.409954
[epoch7, step641]: loss 0.762787
[epoch7, step642]: loss 0.453718
[epoch7, step643]: loss 0.371853
[epoch7, step644]: loss 0.501439
[epoch7, step645]: loss 0.397308
[epoch7, step646]: loss 0.525828
[epoch7, step647]: loss 0.548180
[epoch7, step648]: loss 0.547968
[epoch7, step649]: loss 0.507991
[epoch7, step650]: loss 0.515043
[epoch7, step651]: loss 0.559307
[epoch7, step652]: loss 0.702493
[epoch7, step653]: loss 0.658179
[epoch7, step654]: loss 0.369579
[epoch7, step655]: loss 0.412292
[epoch7, step656]: loss 0.426777
[epoch7, step657]: loss 0.333992
[epoch7, step658]: loss 0.500696
[epoch7, step659]: loss 0.181180
[epoch7, step660]: loss 0.465048
[epoch7, step661]: loss 0.577448
[epoch7, step662]: loss 0.498584
[epoch7, step663]: loss 0.527561
[epoch7, step664]: loss 0.578381
[epoch7, step665]: loss 0.469444
[epoch7, step666]: loss 0.443377
[epoch7, step667]: loss 0.632268
[epoch7, step668]: loss 0.519226
[epoch7, step669]: loss 0.419584
[epoch7, step670]: loss 0.470266
[epoch7, step671]: loss 0.609602
[epoch7, step672]: loss 0.272504
[epoch7, step673]: loss 0.517268
[epoch7, step674]: loss 0.570172
[epoch7, step675]: loss 0.291164
[epoch7, step676]: loss 0.654240
[epoch7, step677]: loss 0.515143
[epoch7, step678]: loss 0.645950
[epoch7, step679]: loss 0.624082
[epoch7, step680]: loss 0.721728
[epoch7, step681]: loss 0.587087
[epoch7, step682]: loss 0.584337
[epoch7, step683]: loss 0.452418
[epoch7, step684]: loss 0.727286
[epoch7, step685]: loss 0.440103
[epoch7, step686]: loss 0.257005
[epoch7, step687]: loss 0.401861
[epoch7, step688]: loss 0.681135
[epoch7, step689]: loss 0.507772
[epoch7, step690]: loss 0.365957
[epoch7, step691]: loss 0.712294
[epoch7, step692]: loss 0.518993
[epoch7, step693]: loss 0.520880
[epoch7, step694]: loss 0.461282
[epoch7, step695]: loss 0.427164
[epoch7, step696]: loss 0.719142
[epoch7, step697]: loss 0.441836
[epoch7, step698]: loss 0.517594
[epoch7, step699]: loss 0.738422
[epoch7, step700]: loss 0.689212
[epoch7, step701]: loss 0.657574
[epoch7, step702]: loss 0.468737
[epoch7, step703]: loss 0.449605
[epoch7, step704]: loss 0.654798
[epoch7, step705]: loss 0.467792
[epoch7, step706]: loss 0.751713
[epoch7, step707]: loss 0.526292
[epoch7, step708]: loss 0.514228
[epoch7, step709]: loss 0.502723
[epoch7, step710]: loss 0.798809
[epoch7, step711]: loss 0.423845
[epoch7, step712]: loss 0.692259
[epoch7, step713]: loss 0.483425
[epoch7, step714]: loss 0.676571
[epoch7, step715]: loss 0.330017
[epoch7, step716]: loss 0.340541
[epoch7, step717]: loss 0.604747
[epoch7, step718]: loss 0.557235
[epoch7, step719]: loss 0.626184
[epoch7, step720]: loss 0.343109
[epoch7, step721]: loss 0.668728
[epoch7, step722]: loss 0.741399
[epoch7, step723]: loss 0.625613
[epoch7, step724]: loss 0.647615
[epoch7, step725]: loss 0.332660
[epoch7, step726]: loss 0.726438
[epoch7, step727]: loss 0.338724
[epoch7, step728]: loss 0.506959
[epoch7, step729]: loss 0.338908
[epoch7, step730]: loss 0.653939
[epoch7, step731]: loss 0.517660
[epoch7, step732]: loss 0.382744
[epoch7, step733]: loss 0.596152
[epoch7, step734]: loss 0.623019
[epoch7, step735]: loss 0.463836
[epoch7, step736]: loss 0.674595
[epoch7, step737]: loss 0.512664
[epoch7, step738]: loss 0.439524
[epoch7, step739]: loss 0.505310
[epoch7, step740]: loss 0.241828
[epoch7, step741]: loss 0.293823
[epoch7, step742]: loss 0.463500
[epoch7, step743]: loss 0.522976
[epoch7, step744]: loss 0.393439
[epoch7, step745]: loss 0.774529
[epoch7, step746]: loss 0.691968
[epoch7, step747]: loss 0.462282
[epoch7, step748]: loss 0.406197
[epoch7, step749]: loss 0.583205
[epoch7, step750]: loss 0.502287
[epoch7, step751]: loss 0.420260
[epoch7, step752]: loss 0.583120
[epoch7, step753]: loss 0.718304
[epoch7, step754]: loss 0.531452
[epoch7, step755]: loss 0.543588
[epoch7, step756]: loss 0.445385
[epoch7, step757]: loss 0.459754
[epoch7, step758]: loss 0.587765
[epoch7, step759]: loss 0.537379
[epoch7, step760]: loss 0.361793
[epoch7, step761]: loss 0.426517
[epoch7, step762]: loss 0.386418
[epoch7, step763]: loss 0.573334
[epoch7, step764]: loss 0.688384
[epoch7, step765]: loss 0.567702
[epoch7, step766]: loss 0.424256
[epoch7, step767]: loss 0.647059
[epoch7, step768]: loss 0.584964
[epoch7, step769]: loss 0.578951
[epoch7, step770]: loss 0.705724
[epoch7, step771]: loss 0.696865
[epoch7, step772]: loss 0.437631
[epoch7, step773]: loss 0.596927
[epoch7, step774]: loss 0.457893
[epoch7, step775]: loss 0.797169
[epoch7, step776]: loss 0.479423
[epoch7, step777]: loss 0.795729
[epoch7, step778]: loss 0.488637
[epoch7, step779]: loss 0.322155
[epoch7, step780]: loss 0.588007
[epoch7, step781]: loss 0.454960
[epoch7, step782]: loss 0.614705
[epoch7, step783]: loss 0.534379
[epoch7, step784]: loss 0.628632
[epoch7, step785]: loss 0.569346
[epoch7, step786]: loss 0.406054
[epoch7, step787]: loss 0.547863
[epoch7, step788]: loss 0.496047
[epoch7, step789]: loss 0.549962
[epoch7, step790]: loss 0.460099
[epoch7, step791]: loss 0.737248
[epoch7, step792]: loss 0.669522
[epoch7, step793]: loss 0.667139
[epoch7, step794]: loss 0.522535
[epoch7, step795]: loss 0.650788
[epoch7, step796]: loss 0.598438
[epoch7, step797]: loss 0.404849
[epoch7, step798]: loss 0.390294
[epoch7, step799]: loss 0.318810
[epoch7, step800]: loss 0.584543
[epoch7, step801]: loss 0.500496
[epoch7, step802]: loss 0.541669
[epoch7, step803]: loss 0.598041
[epoch7, step804]: loss 0.547309
[epoch7, step805]: loss 0.556984
[epoch7, step806]: loss 0.706172
[epoch7, step807]: loss 0.279678
[epoch7, step808]: loss 0.397196
[epoch7, step809]: loss 0.527427
[epoch7, step810]: loss 0.508745
[epoch7, step811]: loss 0.737560
[epoch7, step812]: loss 0.645434
[epoch7, step813]: loss 0.574728
[epoch7, step814]: loss 0.364025
[epoch7, step815]: loss 0.511594
[epoch7, step816]: loss 0.538594
[epoch7, step817]: loss 0.727864
[epoch7, step818]: loss 0.519290
[epoch7, step819]: loss 0.407859
[epoch7, step820]: loss 0.747360
[epoch7, step821]: loss 0.532631
[epoch7, step822]: loss 0.199339
[epoch7, step823]: loss 0.528702
[epoch7, step824]: loss 0.249208
[epoch7, step825]: loss 0.443632
[epoch7, step826]: loss 0.435948
[epoch7, step827]: loss 0.633187
[epoch7, step828]: loss 0.587524
[epoch7, step829]: loss 0.334991
[epoch7, step830]: loss 0.632851
[epoch7, step831]: loss 0.667029
[epoch7, step832]: loss 0.596297
[epoch7, step833]: loss 0.171618
[epoch7, step834]: loss 0.372865
[epoch7, step835]: loss 0.703236
[epoch7, step836]: loss 0.603732
[epoch7, step837]: loss 0.538464
[epoch7, step838]: loss 0.513127
[epoch7, step839]: loss 0.633947
[epoch7, step840]: loss 0.425953
[epoch7, step841]: loss 0.649793
[epoch7, step842]: loss 0.708291
[epoch7, step843]: loss 0.497606
[epoch7, step844]: loss 0.539579
[epoch7, step845]: loss 0.662815
[epoch7, step846]: loss 0.480933
[epoch7, step847]: loss 0.510422
[epoch7, step848]: loss 0.520622
[epoch7, step849]: loss 0.557751
[epoch7, step850]: loss 0.467613
[epoch7, step851]: loss 0.264287
[epoch7, step852]: loss 0.441900
[epoch7, step853]: loss 0.602381
[epoch7, step854]: loss 0.542823
[epoch7, step855]: loss 0.334758
[epoch7, step856]: loss 0.605229
[epoch7, step857]: loss 0.258290
[epoch7, step858]: loss 0.518185
[epoch7, step859]: loss 0.572007
[epoch7, step860]: loss 0.311775
[epoch7, step861]: loss 0.306810
[epoch7, step862]: loss 0.515615
[epoch7, step863]: loss 0.592346
[epoch7, step864]: loss 0.649395
[epoch7, step865]: loss 0.512801
[epoch7, step866]: loss 0.592041
[epoch7, step867]: loss 0.418137
[epoch7, step868]: loss 0.659359
[epoch7, step869]: loss 0.687907
[epoch7, step870]: loss 0.316511
[epoch7, step871]: loss 0.588104
[epoch7, step872]: loss 0.675217
[epoch7, step873]: loss 0.366097
[epoch7, step874]: loss 0.928074
[epoch7, step875]: loss 0.552387
[epoch7, step876]: loss 0.653142
[epoch7, step877]: loss 0.492767
[epoch7, step878]: loss 0.503215
[epoch7, step879]: loss 0.454952
[epoch7, step880]: loss 0.269782
[epoch7, step881]: loss 0.647647
[epoch7, step882]: loss 0.489747
[epoch7, step883]: loss 0.539537
[epoch7, step884]: loss 0.389001
[epoch7, step885]: loss 0.586269
[epoch7, step886]: loss 0.469295
[epoch7, step887]: loss 0.420440
[epoch7, step888]: loss 0.278265
[epoch7, step889]: loss 0.632589
[epoch7, step890]: loss 0.589596
[epoch7, step891]: loss 0.426701
[epoch7, step892]: loss 0.764799
[epoch7, step893]: loss 0.496373
[epoch7, step894]: loss 0.634227
[epoch7, step895]: loss 0.466137
[epoch7, step896]: loss 0.441385
[epoch7, step897]: loss 0.569241
[epoch7, step898]: loss 0.660819
[epoch7, step899]: loss 0.719775
[epoch7, step900]: loss 0.461790
[epoch7, step901]: loss 0.469820
[epoch7, step902]: loss 0.687841
[epoch7, step903]: loss 0.577064
[epoch7, step904]: loss 0.487663
[epoch7, step905]: loss 0.501645
[epoch7, step906]: loss 0.473619
[epoch7, step907]: loss 0.682333
[epoch7, step908]: loss 0.549129
[epoch7, step909]: loss 0.643712
[epoch7, step910]: loss 0.674974
[epoch7, step911]: loss 0.459003
[epoch7, step912]: loss 0.673515
[epoch7, step913]: loss 0.648047
[epoch7, step914]: loss 0.688314
[epoch7, step915]: loss 0.457121
[epoch7, step916]: loss 0.540371
[epoch7, step917]: loss 0.863214
[epoch7, step918]: loss 0.352712
[epoch7, step919]: loss 0.439262
[epoch7, step920]: loss 0.411966
[epoch7, step921]: loss 0.388551
[epoch7, step922]: loss 0.566467
[epoch7, step923]: loss 0.276008
[epoch7, step924]: loss 0.140659
[epoch7, step925]: loss 0.457007
[epoch7, step926]: loss 0.792237
[epoch7, step927]: loss 0.543719
[epoch7, step928]: loss 0.558588
[epoch7, step929]: loss 0.552150
[epoch7, step930]: loss 0.615282
[epoch7, step931]: loss 0.780586
[epoch7, step932]: loss 0.525758
[epoch7, step933]: loss 0.568349
[epoch7, step934]: loss 0.727382
[epoch7, step935]: loss 0.300404
[epoch7, step936]: loss 0.599585
[epoch7, step937]: loss 0.550090
[epoch7, step938]: loss 0.598476
[epoch7, step939]: loss 0.524105
[epoch7, step940]: loss 0.571661
[epoch7, step941]: loss 0.403231
[epoch7, step942]: loss 0.711778
[epoch7, step943]: loss 0.491702
[epoch7, step944]: loss 0.555674
[epoch7, step945]: loss 0.543110
[epoch7, step946]: loss 0.502714
[epoch7, step947]: loss 0.667636
[epoch7, step948]: loss 0.320969
[epoch7, step949]: loss 0.290606
[epoch7, step950]: loss 0.352977
[epoch7, step951]: loss 0.293314
[epoch7, step952]: loss 0.641570
[epoch7, step953]: loss 0.144863
[epoch7, step954]: loss 0.535629
[epoch7, step955]: loss 0.498007
[epoch7, step956]: loss 0.594289
[epoch7, step957]: loss 0.619160
[epoch7, step958]: loss 0.481714
[epoch7, step959]: loss 0.583364
[epoch7, step960]: loss 0.705191
[epoch7, step961]: loss 0.552904
[epoch7, step962]: loss 0.464575
[epoch7, step963]: loss 0.420277
[epoch7, step964]: loss 0.431537
[epoch7, step965]: loss 0.715315
[epoch7, step966]: loss 0.590582
[epoch7, step967]: loss 0.544515
[epoch7, step968]: loss 0.534518
[epoch7, step969]: loss 0.307197
[epoch7, step970]: loss 0.573462
[epoch7, step971]: loss 0.798616
[epoch7, step972]: loss 0.706465
[epoch7, step973]: loss 0.577966
[epoch7, step974]: loss 0.288666
[epoch7, step975]: loss 0.499951
[epoch7, step976]: loss 0.757106
[epoch7, step977]: loss 0.555362
[epoch7, step978]: loss 0.354762
[epoch7, step979]: loss 0.612351
[epoch7, step980]: loss 0.475614
[epoch7, step981]: loss 0.655839
[epoch7, step982]: loss 0.735713
[epoch7, step983]: loss 0.502774
[epoch7, step984]: loss 0.640224
[epoch7, step985]: loss 0.475892
[epoch7, step986]: loss 0.767543
[epoch7, step987]: loss 0.522097
[epoch7, step988]: loss 0.583057
[epoch7, step989]: loss 0.560028
[epoch7, step990]: loss 0.820505
[epoch7, step991]: loss 0.607537
[epoch7, step992]: loss 0.807260
[epoch7, step993]: loss 0.394303
[epoch7, step994]: loss 0.589060
[epoch7, step995]: loss 0.135386
[epoch7, step996]: loss 0.546159
[epoch7, step997]: loss 0.544258
[epoch7, step998]: loss 0.545114
[epoch7, step999]: loss 0.529932
[epoch7, step1000]: loss 0.581288
[epoch7, step1001]: loss 0.719393
[epoch7, step1002]: loss 0.797268
[epoch7, step1003]: loss 0.556786
[epoch7, step1004]: loss 0.435076
[epoch7, step1005]: loss 0.630127
[epoch7, step1006]: loss 0.327395
[epoch7, step1007]: loss 0.454526
[epoch7, step1008]: loss 0.543955
[epoch7, step1009]: loss 0.784327
[epoch7, step1010]: loss 0.526104
[epoch7, step1011]: loss 0.540512
[epoch7, step1012]: loss 0.504927
[epoch7, step1013]: loss 0.438378
[epoch7, step1014]: loss 0.628442
[epoch7, step1015]: loss 0.622312
[epoch7, step1016]: loss 0.765567
[epoch7, step1017]: loss 0.677609
[epoch7, step1018]: loss 0.736632
[epoch7, step1019]: loss 0.489290
[epoch7, step1020]: loss 0.655707
[epoch7, step1021]: loss 0.577803
[epoch7, step1022]: loss 0.781896
[epoch7, step1023]: loss 0.658485
[epoch7, step1024]: loss 0.569553
[epoch7, step1025]: loss 0.672866
[epoch7, step1026]: loss 0.686044
[epoch7, step1027]: loss 0.521423
[epoch7, step1028]: loss 0.494477
[epoch7, step1029]: loss 0.469867
[epoch7, step1030]: loss 0.429071
[epoch7, step1031]: loss 0.395270
[epoch7, step1032]: loss 0.510050
[epoch7, step1033]: loss 0.467540
[epoch7, step1034]: loss 0.617041
[epoch7, step1035]: loss 0.503116
[epoch7, step1036]: loss 0.727370
[epoch7, step1037]: loss 0.327744
[epoch7, step1038]: loss 0.526022
[epoch7, step1039]: loss 0.442882
[epoch7, step1040]: loss 0.406649
[epoch7, step1041]: loss 0.389307
[epoch7, step1042]: loss 0.263345
[epoch7, step1043]: loss 0.628850
[epoch7, step1044]: loss 0.244006
[epoch7, step1045]: loss 0.614583
[epoch7, step1046]: loss 0.777477
[epoch7, step1047]: loss 0.493841
[epoch7, step1048]: loss 0.178073
[epoch7, step1049]: loss 0.462759
[epoch7, step1050]: loss 0.342740
[epoch7, step1051]: loss 0.332723
[epoch7, step1052]: loss 0.490776
[epoch7, step1053]: loss 0.712975
[epoch7, step1054]: loss 0.431921
[epoch7, step1055]: loss 0.550458
[epoch7, step1056]: loss 0.684561
[epoch7, step1057]: loss 0.596071
[epoch7, step1058]: loss 0.438333
[epoch7, step1059]: loss 0.360472
[epoch7, step1060]: loss 0.369349
[epoch7, step1061]: loss 0.561113
[epoch7, step1062]: loss 0.565626
[epoch7, step1063]: loss 0.526909
[epoch7, step1064]: loss 0.464513
[epoch7, step1065]: loss 0.852645
[epoch7, step1066]: loss 0.815161
[epoch7, step1067]: loss 0.613322
[epoch7, step1068]: loss 0.542048
[epoch7, step1069]: loss 0.697263
[epoch7, step1070]: loss 0.533707
[epoch7, step1071]: loss 0.324640
[epoch7, step1072]: loss 0.698427
[epoch7, step1073]: loss 0.277351
[epoch7, step1074]: loss 0.423420
[epoch7, step1075]: loss 0.511789
[epoch7, step1076]: loss 0.537383
[epoch7, step1077]: loss 0.667745
[epoch7, step1078]: loss 0.649173
[epoch7, step1079]: loss 0.581389
[epoch7, step1080]: loss 0.536093
[epoch7, step1081]: loss 0.446848
[epoch7, step1082]: loss 0.329495
[epoch7, step1083]: loss 0.564404
[epoch7, step1084]: loss 0.435098
[epoch7, step1085]: loss 0.555348
[epoch7, step1086]: loss 0.416580
[epoch7, step1087]: loss 0.431528
[epoch7, step1088]: loss 0.495829
[epoch7, step1089]: loss 0.584105
[epoch7, step1090]: loss 0.397779
[epoch7, step1091]: loss 0.180646
[epoch7, step1092]: loss 0.214823
[epoch7, step1093]: loss 0.301327
[epoch7, step1094]: loss 0.557071
[epoch7, step1095]: loss 0.560879
[epoch7, step1096]: loss 0.584515
[epoch7, step1097]: loss 0.468250
[epoch7, step1098]: loss 0.489224
[epoch7, step1099]: loss 0.286631
[epoch7, step1100]: loss 0.614966
[epoch7, step1101]: loss 0.557173
[epoch7, step1102]: loss 0.545894
[epoch7, step1103]: loss 0.575641
[epoch7, step1104]: loss 0.689334
[epoch7, step1105]: loss 0.540431
[epoch7, step1106]: loss 0.355158
[epoch7, step1107]: loss 0.610459
[epoch7, step1108]: loss 0.530791
[epoch7, step1109]: loss 0.541270
[epoch7, step1110]: loss 0.477713
[epoch7, step1111]: loss 0.514486
[epoch7, step1112]: loss 0.726997
[epoch7, step1113]: loss 0.466602
[epoch7, step1114]: loss 0.428808
[epoch7, step1115]: loss 0.572575
[epoch7, step1116]: loss 0.714975
[epoch7, step1117]: loss 0.553392
[epoch7, step1118]: loss 0.515392
[epoch7, step1119]: loss 0.531053
[epoch7, step1120]: loss 0.499710
[epoch7, step1121]: loss 0.677154
[epoch7, step1122]: loss 0.643864
[epoch7, step1123]: loss 0.394028
[epoch7, step1124]: loss 0.379217
[epoch7, step1125]: loss 0.537047
[epoch7, step1126]: loss 0.719721
[epoch7, step1127]: loss 0.650335
[epoch7, step1128]: loss 0.650855
[epoch7, step1129]: loss 0.681531
[epoch7, step1130]: loss 0.441882
[epoch7, step1131]: loss 0.729454
[epoch7, step1132]: loss 0.466030
[epoch7, step1133]: loss 0.759073
[epoch7, step1134]: loss 0.634994
[epoch7, step1135]: loss 0.580177
[epoch7, step1136]: loss 0.520524
[epoch7, step1137]: loss 0.524650
[epoch7, step1138]: loss 0.380878
[epoch7, step1139]: loss 0.495206
[epoch7, step1140]: loss 0.242321
[epoch7, step1141]: loss 0.691875
[epoch7, step1142]: loss 0.550271
[epoch7, step1143]: loss 0.444670
[epoch7, step1144]: loss 0.689877
[epoch7, step1145]: loss 0.794512
[epoch7, step1146]: loss 0.566216
[epoch7, step1147]: loss 0.551628
[epoch7, step1148]: loss 0.598214
[epoch7, step1149]: loss 0.354833
[epoch7, step1150]: loss 0.799999
[epoch7, step1151]: loss 0.607103
[epoch7, step1152]: loss 0.679384
[epoch7, step1153]: loss 0.485822
[epoch7, step1154]: loss 0.667186
[epoch7, step1155]: loss 0.715276
[epoch7, step1156]: loss 0.257935
[epoch7, step1157]: loss 0.621877
[epoch7, step1158]: loss 0.469034
[epoch7, step1159]: loss 0.421761
[epoch7, step1160]: loss 0.537860
[epoch7, step1161]: loss 0.543041
[epoch7, step1162]: loss 0.378181
[epoch7, step1163]: loss 0.566928
[epoch7, step1164]: loss 0.506127
[epoch7, step1165]: loss 0.508570
[epoch7, step1166]: loss 0.486878
[epoch7, step1167]: loss 0.707092
[epoch7, step1168]: loss 0.448717
[epoch7, step1169]: loss 0.451181
[epoch7, step1170]: loss 0.411876
[epoch7, step1171]: loss 0.834559
[epoch7, step1172]: loss 0.599141
[epoch7, step1173]: loss 0.426838
[epoch7, step1174]: loss 0.608821
[epoch7, step1175]: loss 0.652412
[epoch7, step1176]: loss 0.532024
[epoch7, step1177]: loss 0.492426
[epoch7, step1178]: loss 0.286038
[epoch7, step1179]: loss 0.614138
[epoch7, step1180]: loss 0.479542
[epoch7, step1181]: loss 0.524725
[epoch7, step1182]: loss 0.507246
[epoch7, step1183]: loss 0.477741
[epoch7, step1184]: loss 0.759182
[epoch7, step1185]: loss 0.521520
[epoch7, step1186]: loss 0.388052
[epoch7, step1187]: loss 0.469376
[epoch7, step1188]: loss 0.358743
[epoch7, step1189]: loss 0.751103
[epoch7, step1190]: loss 0.461105
[epoch7, step1191]: loss 0.501850
[epoch7, step1192]: loss 0.576312
[epoch7, step1193]: loss 0.667056
[epoch7, step1194]: loss 0.476710
[epoch7, step1195]: loss 0.571001
[epoch7, step1196]: loss 0.660031
[epoch7, step1197]: loss 0.682839
[epoch7, step1198]: loss 0.492903
[epoch7, step1199]: loss 0.558299
[epoch7, step1200]: loss 0.706467
[epoch7, step1201]: loss 0.318577
[epoch7, step1202]: loss 0.661594
[epoch7, step1203]: loss 0.692901
[epoch7, step1204]: loss 0.543087
[epoch7, step1205]: loss 0.399132
[epoch7, step1206]: loss 0.525815
[epoch7, step1207]: loss 0.577214
[epoch7, step1208]: loss 0.413215
[epoch7, step1209]: loss 0.147282
[epoch7, step1210]: loss 0.750338
[epoch7, step1211]: loss 0.401429
[epoch7, step1212]: loss 0.508517
[epoch7, step1213]: loss 0.659539
[epoch7, step1214]: loss 0.565868
[epoch7, step1215]: loss 0.697157
[epoch7, step1216]: loss 0.402314
[epoch7, step1217]: loss 0.569332
[epoch7, step1218]: loss 0.453514
[epoch7, step1219]: loss 0.458161
[epoch7, step1220]: loss 0.684216
[epoch7, step1221]: loss 0.879096
[epoch7, step1222]: loss 0.485375
[epoch7, step1223]: loss 0.555752
[epoch7, step1224]: loss 0.504869
[epoch7, step1225]: loss 0.270190
[epoch7, step1226]: loss 0.713873
[epoch7, step1227]: loss 0.648890
[epoch7, step1228]: loss 0.641223
[epoch7, step1229]: loss 0.226167
[epoch7, step1230]: loss 0.480763
[epoch7, step1231]: loss 0.610156
[epoch7, step1232]: loss 0.536999
[epoch7, step1233]: loss 0.606482
[epoch7, step1234]: loss 0.385956
[epoch7, step1235]: loss 0.689645
[epoch7, step1236]: loss 0.542027
[epoch7, step1237]: loss 0.303821
[epoch7, step1238]: loss 0.617810
[epoch7, step1239]: loss 0.525284
[epoch7, step1240]: loss 0.420047
[epoch7, step1241]: loss 0.342482
[epoch7, step1242]: loss 0.639073
[epoch7, step1243]: loss 0.676504
[epoch7, step1244]: loss 0.471499
[epoch7, step1245]: loss 0.515644
[epoch7, step1246]: loss 0.686145
[epoch7, step1247]: loss 0.453925
[epoch7, step1248]: loss 0.563238
[epoch7, step1249]: loss 0.349038
[epoch7, step1250]: loss 0.424961
[epoch7, step1251]: loss 0.475682
[epoch7, step1252]: loss 0.602737
[epoch7, step1253]: loss 0.592688
[epoch7, step1254]: loss 0.475301
[epoch7, step1255]: loss 0.503185
[epoch7, step1256]: loss 0.378029
[epoch7, step1257]: loss 0.359381
[epoch7, step1258]: loss 0.703447
[epoch7, step1259]: loss 0.718380
[epoch7, step1260]: loss 0.355375
[epoch7, step1261]: loss 0.596883
[epoch7, step1262]: loss 0.364300
[epoch7, step1263]: loss 0.683927
[epoch7, step1264]: loss 0.589292
[epoch7, step1265]: loss 0.651149
[epoch7, step1266]: loss 0.665981
[epoch7, step1267]: loss 0.517119
[epoch7, step1268]: loss 0.409243
[epoch7, step1269]: loss 0.473565
[epoch7, step1270]: loss 0.480807
[epoch7, step1271]: loss 0.331575
[epoch7, step1272]: loss 0.470803
[epoch7, step1273]: loss 0.416709
[epoch7, step1274]: loss 0.679051
[epoch7, step1275]: loss 0.489095
[epoch7, step1276]: loss 0.635331
[epoch7, step1277]: loss 0.635136
[epoch7, step1278]: loss 0.639140
[epoch7, step1279]: loss 0.385715
[epoch7, step1280]: loss 0.533905
[epoch7, step1281]: loss 0.535363
[epoch7, step1282]: loss 0.534402
[epoch7, step1283]: loss 0.462751
[epoch7, step1284]: loss 0.674109
[epoch7, step1285]: loss 0.398613
[epoch7, step1286]: loss 0.560683
[epoch7, step1287]: loss 0.619566
[epoch7, step1288]: loss 0.389970
[epoch7, step1289]: loss 0.642945
[epoch7, step1290]: loss 0.636690
[epoch7, step1291]: loss 0.699216
[epoch7, step1292]: loss 0.429046
[epoch7, step1293]: loss 0.647276
[epoch7, step1294]: loss 0.468121
[epoch7, step1295]: loss 0.528249
[epoch7, step1296]: loss 0.540044
[epoch7, step1297]: loss 0.395099
[epoch7, step1298]: loss 0.634653
[epoch7, step1299]: loss 0.632766
[epoch7, step1300]: loss 0.571776
[epoch7, step1301]: loss 0.306927
[epoch7, step1302]: loss 0.598541
[epoch7, step1303]: loss 0.114161
[epoch7, step1304]: loss 0.640778
[epoch7, step1305]: loss 0.664023
[epoch7, step1306]: loss 0.544752
[epoch7, step1307]: loss 0.564466
[epoch7, step1308]: loss 0.512212
[epoch7, step1309]: loss 0.515794
[epoch7, step1310]: loss 0.405304
[epoch7, step1311]: loss 0.512217
[epoch7, step1312]: loss 0.376143
[epoch7, step1313]: loss 0.590544
[epoch7, step1314]: loss 0.413826
[epoch7, step1315]: loss 0.495386
[epoch7, step1316]: loss 0.483891
[epoch7, step1317]: loss 0.677521
[epoch7, step1318]: loss 0.517344
[epoch7, step1319]: loss 0.493455
[epoch7, step1320]: loss 0.408765
[epoch7, step1321]: loss 0.538427
[epoch7, step1322]: loss 0.833315
[epoch7, step1323]: loss 0.686520
[epoch7, step1324]: loss 0.538905
[epoch7, step1325]: loss 0.742863
[epoch7, step1326]: loss 0.588938
[epoch7, step1327]: loss 0.614882
[epoch7, step1328]: loss 0.404378
[epoch7, step1329]: loss 0.202111
[epoch7, step1330]: loss 0.712560
[epoch7, step1331]: loss 0.314999
[epoch7, step1332]: loss 0.258983
[epoch7, step1333]: loss 0.590282
[epoch7, step1334]: loss 0.500107
[epoch7, step1335]: loss 0.730696
[epoch7, step1336]: loss 0.601621
[epoch7, step1337]: loss 0.473159
[epoch7, step1338]: loss 0.562027
[epoch7, step1339]: loss 0.747589
[epoch7, step1340]: loss 0.532855
[epoch7, step1341]: loss 0.734358
[epoch7, step1342]: loss 0.482973
[epoch7, step1343]: loss 0.622467
[epoch7, step1344]: loss 0.617660
[epoch7, step1345]: loss 0.447248
[epoch7, step1346]: loss 0.589074
[epoch7, step1347]: loss 0.422293
[epoch7, step1348]: loss 0.512275
[epoch7, step1349]: loss 0.513315
[epoch7, step1350]: loss 0.476155
[epoch7, step1351]: loss 0.701776
[epoch7, step1352]: loss 0.752978
[epoch7, step1353]: loss 0.630291
[epoch7, step1354]: loss 0.735782
[epoch7, step1355]: loss 0.758236
[epoch7, step1356]: loss 0.398483
[epoch7, step1357]: loss 0.687949
[epoch7, step1358]: loss 0.374214
[epoch7, step1359]: loss 0.415938
[epoch7, step1360]: loss 0.596130
[epoch7, step1361]: loss 0.414339
[epoch7, step1362]: loss 0.636903
[epoch7, step1363]: loss 0.475533
[epoch7, step1364]: loss 0.501189
[epoch7, step1365]: loss 0.763869
[epoch7, step1366]: loss 0.719437
[epoch7, step1367]: loss 0.492045
[epoch7, step1368]: loss 0.681172
[epoch7, step1369]: loss 0.478164
[epoch7, step1370]: loss 0.479087
[epoch7, step1371]: loss 0.663488
[epoch7, step1372]: loss 0.492657
[epoch7, step1373]: loss 0.726395
[epoch7, step1374]: loss 0.694927
[epoch7, step1375]: loss 0.513483
[epoch7, step1376]: loss 0.322828
[epoch7, step1377]: loss 0.475574
[epoch7, step1378]: loss 0.482333
[epoch7, step1379]: loss 0.731316
[epoch7, step1380]: loss 0.693304
[epoch7, step1381]: loss 0.365898
[epoch7, step1382]: loss 0.704427
[epoch7, step1383]: loss 0.796326
[epoch7, step1384]: loss 0.337088
[epoch7, step1385]: loss 0.655945
[epoch7, step1386]: loss 0.589509
[epoch7, step1387]: loss 0.672895
[epoch7, step1388]: loss 0.257489
[epoch7, step1389]: loss 0.395284
[epoch7, step1390]: loss 0.592812
[epoch7, step1391]: loss 0.468731
[epoch7, step1392]: loss 0.738989
[epoch7, step1393]: loss 0.545115
[epoch7, step1394]: loss 0.338952
[epoch7, step1395]: loss 0.539314
[epoch7, step1396]: loss 0.652998
[epoch7, step1397]: loss 0.382194
[epoch7, step1398]: loss 0.523467
[epoch7, step1399]: loss 0.652477
[epoch7, step1400]: loss 0.525908
[epoch7, step1401]: loss 0.436544
[epoch7, step1402]: loss 0.294582
[epoch7, step1403]: loss 0.496265
[epoch7, step1404]: loss 0.651511
[epoch7, step1405]: loss 0.556885
[epoch7, step1406]: loss 0.396673
[epoch7, step1407]: loss 0.453227
[epoch7, step1408]: loss 0.644519
[epoch7, step1409]: loss 0.442057
[epoch7, step1410]: loss 0.360509
[epoch7, step1411]: loss 0.531059
[epoch7, step1412]: loss 0.622774
[epoch7, step1413]: loss 0.492672
[epoch7, step1414]: loss 0.515386
[epoch7, step1415]: loss 0.302774
[epoch7, step1416]: loss 0.582749
[epoch7, step1417]: loss 0.640064
[epoch7, step1418]: loss 0.623663
[epoch7, step1419]: loss 0.643281
[epoch7, step1420]: loss 0.575641
[epoch7, step1421]: loss 0.430403
[epoch7, step1422]: loss 0.481916
[epoch7, step1423]: loss 0.394288
[epoch7, step1424]: loss 0.723947
[epoch7, step1425]: loss 0.614687
[epoch7, step1426]: loss 0.519284
[epoch7, step1427]: loss 0.502415
[epoch7, step1428]: loss 0.513231
[epoch7, step1429]: loss 0.480223
[epoch7, step1430]: loss 0.492716
[epoch7, step1431]: loss 0.436857
[epoch7, step1432]: loss 0.624290
[epoch7, step1433]: loss 0.487392
[epoch7, step1434]: loss 0.562098
[epoch7, step1435]: loss 0.376293
[epoch7, step1436]: loss 0.571287
[epoch7, step1437]: loss 0.676184
[epoch7, step1438]: loss 0.476444
[epoch7, step1439]: loss 0.308060
[epoch7, step1440]: loss 0.604877
[epoch7, step1441]: loss 0.646301
[epoch7, step1442]: loss 0.629755
[epoch7, step1443]: loss 0.544039
[epoch7, step1444]: loss 0.686817
[epoch7, step1445]: loss 0.511856
[epoch7, step1446]: loss 0.474367
[epoch7, step1447]: loss 0.661862
[epoch7, step1448]: loss 0.583228
[epoch7, step1449]: loss 0.746395
[epoch7, step1450]: loss 0.736199
[epoch7, step1451]: loss 0.472076
[epoch7, step1452]: loss 0.446907
[epoch7, step1453]: loss 0.511368
[epoch7, step1454]: loss 0.458250
[epoch7, step1455]: loss 0.557388
[epoch7, step1456]: loss 0.526241
[epoch7, step1457]: loss 0.394405
[epoch7, step1458]: loss 0.593104
[epoch7, step1459]: loss 0.268905
[epoch7, step1460]: loss 0.340646
[epoch7, step1461]: loss 0.457098
[epoch7, step1462]: loss 0.611185
[epoch7, step1463]: loss 0.425425
[epoch7, step1464]: loss 0.670307
[epoch7, step1465]: loss 0.586763
[epoch7, step1466]: loss 0.606235
[epoch7, step1467]: loss 0.582637
[epoch7, step1468]: loss 0.718246
[epoch7, step1469]: loss 0.621489
[epoch7, step1470]: loss 0.524816
[epoch7, step1471]: loss 0.416387
[epoch7, step1472]: loss 0.500119
[epoch7, step1473]: loss 0.428455
[epoch7, step1474]: loss 0.576078
[epoch7, step1475]: loss 0.480405
[epoch7, step1476]: loss 0.500066
[epoch7, step1477]: loss 0.636401
[epoch7, step1478]: loss 0.555353
[epoch7, step1479]: loss 0.451230
[epoch7, step1480]: loss 0.651859
[epoch7, step1481]: loss 0.640337
[epoch7, step1482]: loss 0.225976
[epoch7, step1483]: loss 0.611924
[epoch7, step1484]: loss 0.569169
[epoch7, step1485]: loss 0.578340
[epoch7, step1486]: loss 0.636356
[epoch7, step1487]: loss 0.584532
[epoch7, step1488]: loss 0.704772
[epoch7, step1489]: loss 0.399155
[epoch7, step1490]: loss 0.712329
[epoch7, step1491]: loss 0.677414
[epoch7, step1492]: loss 0.483682
[epoch7, step1493]: loss 0.461753
[epoch7, step1494]: loss 0.524959
[epoch7, step1495]: loss 0.723536
[epoch7, step1496]: loss 0.480327
[epoch7, step1497]: loss 0.628033
[epoch7, step1498]: loss 0.764948
[epoch7, step1499]: loss 0.453167
[epoch7, step1500]: loss 0.700610
[epoch7, step1501]: loss 0.775334
[epoch7, step1502]: loss 0.405780
[epoch7, step1503]: loss 0.526540
[epoch7, step1504]: loss 0.631212
[epoch7, step1505]: loss 0.298742
[epoch7, step1506]: loss 0.530080
[epoch7, step1507]: loss 0.584011
[epoch7, step1508]: loss 0.711408
[epoch7, step1509]: loss 0.330626
[epoch7, step1510]: loss 0.607625
[epoch7, step1511]: loss 0.696900
[epoch7, step1512]: loss 0.385218
[epoch7, step1513]: loss 0.767693
[epoch7, step1514]: loss 0.244782
[epoch7, step1515]: loss 0.592098
[epoch7, step1516]: loss 0.427850
[epoch7, step1517]: loss 0.592491
[epoch7, step1518]: loss 0.675282
[epoch7, step1519]: loss 0.409000
[epoch7, step1520]: loss 0.651420
[epoch7, step1521]: loss 0.465884
[epoch7, step1522]: loss 0.447831
[epoch7, step1523]: loss 0.571332
[epoch7, step1524]: loss 0.432162
[epoch7, step1525]: loss 0.402859
[epoch7, step1526]: loss 0.488963
[epoch7, step1527]: loss 0.469043
[epoch7, step1528]: loss 0.843709
[epoch7, step1529]: loss 0.609377
[epoch7, step1530]: loss 0.508282
[epoch7, step1531]: loss 0.421665
[epoch7, step1532]: loss 0.474058
[epoch7, step1533]: loss 0.493756
[epoch7, step1534]: loss 0.621158
[epoch7, step1535]: loss 0.604356
[epoch7, step1536]: loss 0.612359
[epoch7, step1537]: loss 0.488019
[epoch7, step1538]: loss 0.507671
[epoch7, step1539]: loss 0.461495
[epoch7, step1540]: loss 0.346791
[epoch7, step1541]: loss 0.413672
[epoch7, step1542]: loss 0.522746
[epoch7, step1543]: loss 0.446604
[epoch7, step1544]: loss 0.504344
[epoch7, step1545]: loss 0.385909
[epoch7, step1546]: loss 0.714687
[epoch7, step1547]: loss 0.537837
[epoch7, step1548]: loss 0.613798
[epoch7, step1549]: loss 0.231577
[epoch7, step1550]: loss 0.367846
[epoch7, step1551]: loss 0.623551
[epoch7, step1552]: loss 0.622493
[epoch7, step1553]: loss 0.616959
[epoch7, step1554]: loss 0.370088
[epoch7, step1555]: loss 0.326240
[epoch7, step1556]: loss 0.696907
[epoch7, step1557]: loss 0.565848
[epoch7, step1558]: loss 0.604419
[epoch7, step1559]: loss 0.595825
[epoch7, step1560]: loss 0.601732
[epoch7, step1561]: loss 0.453543
[epoch7, step1562]: loss 0.649573
[epoch7, step1563]: loss 0.623183
[epoch7, step1564]: loss 0.381154
[epoch7, step1565]: loss 0.572536
[epoch7, step1566]: loss 0.486080
[epoch7, step1567]: loss 0.662635
[epoch7, step1568]: loss 0.599189
[epoch7, step1569]: loss 0.535257
[epoch7, step1570]: loss 0.296985
[epoch7, step1571]: loss 0.528610
[epoch7, step1572]: loss 0.515061
[epoch7, step1573]: loss 0.735060
[epoch7, step1574]: loss 0.643937
[epoch7, step1575]: loss 0.502082
[epoch7, step1576]: loss 0.389209
[epoch7, step1577]: loss 0.645052
[epoch7, step1578]: loss 0.110817
[epoch7, step1579]: loss 0.253142
[epoch7, step1580]: loss 0.720951
[epoch7, step1581]: loss 0.494049
[epoch7, step1582]: loss 0.674211
[epoch7, step1583]: loss 0.529780
[epoch7, step1584]: loss 0.717648
[epoch7, step1585]: loss 0.381191
[epoch7, step1586]: loss 0.646792
[epoch7, step1587]: loss 0.716813
[epoch7, step1588]: loss 0.367310
[epoch7, step1589]: loss 0.605985
[epoch7, step1590]: loss 0.627438
[epoch7, step1591]: loss 0.655671
[epoch7, step1592]: loss 0.463866
[epoch7, step1593]: loss 0.659589
[epoch7, step1594]: loss 0.613490
[epoch7, step1595]: loss 0.295038
[epoch7, step1596]: loss 0.561025
[epoch7, step1597]: loss 0.597177
[epoch7, step1598]: loss 0.350212
[epoch7, step1599]: loss 0.478326
[epoch7, step1600]: loss 0.367845
[epoch7, step1601]: loss 0.528634
[epoch7, step1602]: loss 0.573593
[epoch7, step1603]: loss 0.162833
[epoch7, step1604]: loss 0.769547
[epoch7, step1605]: loss 0.184880
[epoch7, step1606]: loss 0.759206
[epoch7, step1607]: loss 0.468242
[epoch7, step1608]: loss 0.469812
[epoch7, step1609]: loss 0.500864
[epoch7, step1610]: loss 0.498344
[epoch7, step1611]: loss 0.546436
[epoch7, step1612]: loss 0.679587
[epoch7, step1613]: loss 0.490663
[epoch7, step1614]: loss 0.551695
[epoch7, step1615]: loss 0.551345
[epoch7, step1616]: loss 0.504960
[epoch7, step1617]: loss 0.597076
[epoch7, step1618]: loss 0.388926
[epoch7, step1619]: loss 0.603632
[epoch7, step1620]: loss 0.457014
[epoch7, step1621]: loss 0.298843
[epoch7, step1622]: loss 0.454315
[epoch7, step1623]: loss 0.643700
[epoch7, step1624]: loss 0.325002
[epoch7, step1625]: loss 0.485083
[epoch7, step1626]: loss 0.420815
[epoch7, step1627]: loss 0.379750
[epoch7, step1628]: loss 0.389856
[epoch7, step1629]: loss 0.486139
[epoch7, step1630]: loss 0.495554
[epoch7, step1631]: loss 0.550849
[epoch7, step1632]: loss 0.562385
[epoch7, step1633]: loss 0.382709
[epoch7, step1634]: loss 0.520365
[epoch7, step1635]: loss 0.437467
[epoch7, step1636]: loss 0.536756
[epoch7, step1637]: loss 0.539444
[epoch7, step1638]: loss 0.808930
[epoch7, step1639]: loss 0.593800
[epoch7, step1640]: loss 0.344211
[epoch7, step1641]: loss 0.467349
[epoch7, step1642]: loss 0.824303
[epoch7, step1643]: loss 0.700512
[epoch7, step1644]: loss 0.607823
[epoch7, step1645]: loss 0.546647
[epoch7, step1646]: loss 0.469341
[epoch7, step1647]: loss 0.581807
[epoch7, step1648]: loss 0.675444
[epoch7, step1649]: loss 0.441784
[epoch7, step1650]: loss 0.433804
[epoch7, step1651]: loss 0.446194
[epoch7, step1652]: loss 0.478473
[epoch7, step1653]: loss 0.598605
[epoch7, step1654]: loss 0.664903
[epoch7, step1655]: loss 0.701120
[epoch7, step1656]: loss 0.541973
[epoch7, step1657]: loss 0.492419
[epoch7, step1658]: loss 0.446029
[epoch7, step1659]: loss 0.432941
[epoch7, step1660]: loss 0.290839
[epoch7, step1661]: loss 0.524105
[epoch7, step1662]: loss 0.611384
[epoch7, step1663]: loss 0.634228
[epoch7, step1664]: loss 0.356114
[epoch7, step1665]: loss 0.526736
[epoch7, step1666]: loss 0.666971
[epoch7, step1667]: loss 0.444421
[epoch7, step1668]: loss 0.653402
[epoch7, step1669]: loss 0.464449
[epoch7, step1670]: loss 0.539391
[epoch7, step1671]: loss 0.456720
[epoch7, step1672]: loss 0.369677
[epoch7, step1673]: loss 0.522798
[epoch7, step1674]: loss 0.563370
[epoch7, step1675]: loss 0.307822
[epoch7, step1676]: loss 0.234656
[epoch7, step1677]: loss 0.415511
[epoch7, step1678]: loss 0.536424
[epoch7, step1679]: loss 0.758344
[epoch7, step1680]: loss 0.104295
[epoch7, step1681]: loss 0.592891
[epoch7, step1682]: loss 0.504316
[epoch7, step1683]: loss 0.453283
[epoch7, step1684]: loss 0.360777
[epoch7, step1685]: loss 0.523791
[epoch7, step1686]: loss 0.617255
[epoch7, step1687]: loss 0.283130
[epoch7, step1688]: loss 0.633018
[epoch7, step1689]: loss 0.520256
[epoch7, step1690]: loss 0.641874
[epoch7, step1691]: loss 0.644181
[epoch7, step1692]: loss 0.479128
[epoch7, step1693]: loss 0.656495
[epoch7, step1694]: loss 0.594841
[epoch7, step1695]: loss 0.508157
[epoch7, step1696]: loss 0.740800
[epoch7, step1697]: loss 0.641699
[epoch7, step1698]: loss 0.295200
[epoch7, step1699]: loss 0.502186
[epoch7, step1700]: loss 0.537857
[epoch7, step1701]: loss 0.612158
[epoch7, step1702]: loss 0.598179
[epoch7, step1703]: loss 0.402512
[epoch7, step1704]: loss 0.468396
[epoch7, step1705]: loss 0.705751
[epoch7, step1706]: loss 0.687356
[epoch7, step1707]: loss 0.447442
[epoch7, step1708]: loss 0.558611
[epoch7, step1709]: loss 0.541669
[epoch7, step1710]: loss 0.335192
[epoch7, step1711]: loss 0.575570
[epoch7, step1712]: loss 0.540837
[epoch7, step1713]: loss 0.822159
[epoch7, step1714]: loss 0.667939
[epoch7, step1715]: loss 0.420588
[epoch7, step1716]: loss 0.467865
[epoch7, step1717]: loss 0.442629
[epoch7, step1718]: loss 0.415439
[epoch7, step1719]: loss 0.480443
[epoch7, step1720]: loss 0.428773
[epoch7, step1721]: loss 0.607578
[epoch7, step1722]: loss 0.701102
[epoch7, step1723]: loss 0.273625
[epoch7, step1724]: loss 0.770194
[epoch7, step1725]: loss 0.410909
[epoch7, step1726]: loss 0.546381
[epoch7, step1727]: loss 0.623563
[epoch7, step1728]: loss 0.627281
[epoch7, step1729]: loss 0.455007
[epoch7, step1730]: loss 0.357278
[epoch7, step1731]: loss 0.414166
[epoch7, step1732]: loss 0.572865
[epoch7, step1733]: loss 0.559328
[epoch7, step1734]: loss 0.354098
[epoch7, step1735]: loss 0.369569
[epoch7, step1736]: loss 0.442832
[epoch7, step1737]: loss 0.492196
[epoch7, step1738]: loss 0.423150
[epoch7, step1739]: loss 0.275961
[epoch7, step1740]: loss 0.314780
[epoch7, step1741]: loss 0.328456
[epoch7, step1742]: loss 0.581289
[epoch7, step1743]: loss 0.586649
[epoch7, step1744]: loss 0.245282
[epoch7, step1745]: loss 0.646580
[epoch7, step1746]: loss 0.485781
[epoch7, step1747]: loss 0.642578
[epoch7, step1748]: loss 0.336905
[epoch7, step1749]: loss 0.644389
[epoch7, step1750]: loss 0.705557
[epoch7, step1751]: loss 0.475013
[epoch7, step1752]: loss 0.451469
[epoch7, step1753]: loss 0.522034
[epoch7, step1754]: loss 0.681175
[epoch7, step1755]: loss 0.404480
[epoch7, step1756]: loss 0.778604
[epoch7, step1757]: loss 0.882506
[epoch7, step1758]: loss 0.292925
[epoch7, step1759]: loss 0.408093
[epoch7, step1760]: loss 0.579041
[epoch7, step1761]: loss 0.637648
[epoch7, step1762]: loss 0.564293
[epoch7, step1763]: loss 0.585251
[epoch7, step1764]: loss 0.656568
[epoch7, step1765]: loss 0.431683
[epoch7, step1766]: loss 0.270651
[epoch7, step1767]: loss 0.607818
[epoch7, step1768]: loss 0.529306
[epoch7, step1769]: loss 0.272947
[epoch7, step1770]: loss 0.538226
[epoch7, step1771]: loss 0.665680
[epoch7, step1772]: loss 0.609287
[epoch7, step1773]: loss 0.608866
[epoch7, step1774]: loss 0.547244
[epoch7, step1775]: loss 0.533802
[epoch7, step1776]: loss 0.460865
[epoch7, step1777]: loss 0.678061
[epoch7, step1778]: loss 0.456875
[epoch7, step1779]: loss 0.499080
[epoch7, step1780]: loss 0.712511
[epoch7, step1781]: loss 0.697270
[epoch7, step1782]: loss 0.360040
[epoch7, step1783]: loss 0.535047
[epoch7, step1784]: loss 0.532904
[epoch7, step1785]: loss 0.324990
[epoch7, step1786]: loss 0.717393
[epoch7, step1787]: loss 0.566815
[epoch7, step1788]: loss 0.492963
[epoch7, step1789]: loss 0.576711
[epoch7, step1790]: loss 0.686659
[epoch7, step1791]: loss 0.642482
[epoch7, step1792]: loss 0.538643
[epoch7, step1793]: loss 0.315432
[epoch7, step1794]: loss 0.659113
[epoch7, step1795]: loss 0.207567
[epoch7, step1796]: loss 0.667170
[epoch7, step1797]: loss 0.702307
[epoch7, step1798]: loss 0.688886
[epoch7, step1799]: loss 0.403193
[epoch7, step1800]: loss 0.563780
[epoch7, step1801]: loss 0.555850
[epoch7, step1802]: loss 0.650306
[epoch7, step1803]: loss 0.663601
[epoch7, step1804]: loss 0.539219
[epoch7, step1805]: loss 0.741981
[epoch7, step1806]: loss 0.473909
[epoch7, step1807]: loss 0.478522
[epoch7, step1808]: loss 0.612217
[epoch7, step1809]: loss 0.585091
[epoch7, step1810]: loss 0.650938
[epoch7, step1811]: loss 0.632395
[epoch7, step1812]: loss 0.478825
[epoch7, step1813]: loss 0.514118
[epoch7, step1814]: loss 0.651447
[epoch7, step1815]: loss 0.479355
[epoch7, step1816]: loss 0.494357
[epoch7, step1817]: loss 0.329811
[epoch7, step1818]: loss 0.578483
[epoch7, step1819]: loss 0.364879
[epoch7, step1820]: loss 0.384345
[epoch7, step1821]: loss 0.649253
[epoch7, step1822]: loss 0.728076
[epoch7, step1823]: loss 0.651681
[epoch7, step1824]: loss 0.486318
[epoch7, step1825]: loss 0.357907
[epoch7, step1826]: loss 0.617691
[epoch7, step1827]: loss 0.194197
[epoch7, step1828]: loss 0.561361
[epoch7, step1829]: loss 0.724601
[epoch7, step1830]: loss 0.664960
[epoch7, step1831]: loss 0.397618
[epoch7, step1832]: loss 0.465809
[epoch7, step1833]: loss 0.477658
[epoch7, step1834]: loss 0.695769
[epoch7, step1835]: loss 0.466442
[epoch7, step1836]: loss 0.445694
[epoch7, step1837]: loss 0.416870
[epoch7, step1838]: loss 0.646416
[epoch7, step1839]: loss 0.542171
[epoch7, step1840]: loss 0.580575
[epoch7, step1841]: loss 0.647449
[epoch7, step1842]: loss 0.788811
[epoch7, step1843]: loss 0.360857
[epoch7, step1844]: loss 0.358574
[epoch7, step1845]: loss 0.479807
[epoch7, step1846]: loss 0.492558
[epoch7, step1847]: loss 0.353203
[epoch7, step1848]: loss 0.694656
[epoch7, step1849]: loss 0.291329
[epoch7, step1850]: loss 0.522546
[epoch7, step1851]: loss 0.538274
[epoch7, step1852]: loss 0.549140
[epoch7, step1853]: loss 0.373665
[epoch7, step1854]: loss 0.814654
[epoch7, step1855]: loss 0.650827
[epoch7, step1856]: loss 0.479130
[epoch7, step1857]: loss 0.439858
[epoch7, step1858]: loss 0.383010
[epoch7, step1859]: loss 0.728874
[epoch7, step1860]: loss 0.606968
[epoch7, step1861]: loss 0.337595
[epoch7, step1862]: loss 0.495948
[epoch7, step1863]: loss 0.251484
[epoch7, step1864]: loss 0.605458
[epoch7, step1865]: loss 0.612443
[epoch7, step1866]: loss 0.647216
[epoch7, step1867]: loss 0.553059
[epoch7, step1868]: loss 0.518420
[epoch7, step1869]: loss 0.496282
[epoch7, step1870]: loss 0.229657
[epoch7, step1871]: loss 0.658488
[epoch7, step1872]: loss 0.431889
[epoch7, step1873]: loss 0.376459
[epoch7, step1874]: loss 0.427763
[epoch7, step1875]: loss 0.181204
[epoch7, step1876]: loss 0.433307
[epoch7, step1877]: loss 0.710225
[epoch7, step1878]: loss 0.633520
[epoch7, step1879]: loss 0.314126
[epoch7, step1880]: loss 0.518528
[epoch7, step1881]: loss 0.741093
[epoch7, step1882]: loss 0.763672
[epoch7, step1883]: loss 0.314484
[epoch7, step1884]: loss 0.633702
[epoch7, step1885]: loss 0.643320
[epoch7, step1886]: loss 0.156471
[epoch7, step1887]: loss 0.611679
[epoch7, step1888]: loss 0.515071
[epoch7, step1889]: loss 0.297763
[epoch7, step1890]: loss 0.715604
[epoch7, step1891]: loss 0.476630
[epoch7, step1892]: loss 0.348429
[epoch7, step1893]: loss 0.549203
[epoch7, step1894]: loss 0.667309
[epoch7, step1895]: loss 0.720772
[epoch7, step1896]: loss 0.761091
[epoch7, step1897]: loss 0.717429
[epoch7, step1898]: loss 0.574352
[epoch7, step1899]: loss 0.380306
[epoch7, step1900]: loss 0.584324
[epoch7, step1901]: loss 0.455691
[epoch7, step1902]: loss 0.575417
[epoch7, step1903]: loss 0.423508
[epoch7, step1904]: loss 0.599951
[epoch7, step1905]: loss 0.431084
[epoch7, step1906]: loss 0.716552
[epoch7, step1907]: loss 0.680617
[epoch7, step1908]: loss 0.413367
[epoch7, step1909]: loss 0.415362
[epoch7, step1910]: loss 0.475008
[epoch7, step1911]: loss 0.562769
[epoch7, step1912]: loss 0.613976
[epoch7, step1913]: loss 0.372962
[epoch7, step1914]: loss 0.462387
[epoch7, step1915]: loss 0.431973
[epoch7, step1916]: loss 0.563443
[epoch7, step1917]: loss 0.431256
[epoch7, step1918]: loss 0.396208
[epoch7, step1919]: loss 0.589997
[epoch7, step1920]: loss 0.730625
[epoch7, step1921]: loss 0.525358
[epoch7, step1922]: loss 0.403733
[epoch7, step1923]: loss 0.667012
[epoch7, step1924]: loss 0.524708
[epoch7, step1925]: loss 0.473085
[epoch7, step1926]: loss 0.590234
[epoch7, step1927]: loss 0.219248
[epoch7, step1928]: loss 0.493480
[epoch7, step1929]: loss 0.617699
[epoch7, step1930]: loss 0.615247
[epoch7, step1931]: loss 0.405024
[epoch7, step1932]: loss 0.444229
[epoch7, step1933]: loss 0.430645
[epoch7, step1934]: loss 0.733071
[epoch7, step1935]: loss 0.767389
[epoch7, step1936]: loss 0.392086
[epoch7, step1937]: loss 0.321167
[epoch7, step1938]: loss 0.356491
[epoch7, step1939]: loss 0.645263
[epoch7, step1940]: loss 0.500170
[epoch7, step1941]: loss 0.332380
[epoch7, step1942]: loss 0.411506
[epoch7, step1943]: loss 0.592363
[epoch7, step1944]: loss 0.185767
[epoch7, step1945]: loss 0.298023
[epoch7, step1946]: loss 0.410028
[epoch7, step1947]: loss 0.668976
[epoch7, step1948]: loss 0.579631
[epoch7, step1949]: loss 0.268624
[epoch7, step1950]: loss 0.674662
[epoch7, step1951]: loss 0.516039
[epoch7, step1952]: loss 0.588008
[epoch7, step1953]: loss 0.282873
[epoch7, step1954]: loss 0.607594
[epoch7, step1955]: loss 0.588743
[epoch7, step1956]: loss 0.612980
[epoch7, step1957]: loss 0.491037
[epoch7, step1958]: loss 0.603558
[epoch7, step1959]: loss 0.816402
[epoch7, step1960]: loss 0.564377
[epoch7, step1961]: loss 0.497879
[epoch7, step1962]: loss 0.670882
[epoch7, step1963]: loss 0.552828
[epoch7, step1964]: loss 0.637095
[epoch7, step1965]: loss 0.641470
[epoch7, step1966]: loss 0.675403
[epoch7, step1967]: loss 0.580895
[epoch7, step1968]: loss 0.650566
[epoch7, step1969]: loss 0.405100
[epoch7, step1970]: loss 0.375118
[epoch7, step1971]: loss 0.310043
[epoch7, step1972]: loss 0.540166
[epoch7, step1973]: loss 0.309422
[epoch7, step1974]: loss 0.390936
[epoch7, step1975]: loss 0.312366
[epoch7, step1976]: loss 0.545130
[epoch7, step1977]: loss 0.593857
[epoch7, step1978]: loss 0.566138
[epoch7, step1979]: loss 0.803125
[epoch7, step1980]: loss 0.498073
[epoch7, step1981]: loss 0.639945
[epoch7, step1982]: loss 0.572208
[epoch7, step1983]: loss 0.754362
[epoch7, step1984]: loss 0.344446
[epoch7, step1985]: loss 0.537225
[epoch7, step1986]: loss 0.523830
[epoch7, step1987]: loss 0.309329
[epoch7, step1988]: loss 0.412274
[epoch7, step1989]: loss 0.516158
[epoch7, step1990]: loss 0.473147
[epoch7, step1991]: loss 0.548110
[epoch7, step1992]: loss 0.511988
[epoch7, step1993]: loss 0.640745
[epoch7, step1994]: loss 0.541974
[epoch7, step1995]: loss 0.581915
[epoch7, step1996]: loss 0.410450
[epoch7, step1997]: loss 0.425361
[epoch7, step1998]: loss 0.369910
[epoch7, step1999]: loss 0.825825
[epoch7, step2000]: loss 0.531707
[epoch7, step2001]: loss 0.361596
[epoch7, step2002]: loss 0.370111
[epoch7, step2003]: loss 0.264832
[epoch7, step2004]: loss 0.569337
[epoch7, step2005]: loss 0.370709
[epoch7, step2006]: loss 0.642186
[epoch7, step2007]: loss 0.432625
[epoch7, step2008]: loss 0.381651
[epoch7, step2009]: loss 0.728841
[epoch7, step2010]: loss 0.383261
[epoch7, step2011]: loss 0.532768
[epoch7, step2012]: loss 0.456982
[epoch7, step2013]: loss 0.533967
[epoch7, step2014]: loss 0.287297
[epoch7, step2015]: loss 0.483348
[epoch7, step2016]: loss 0.626276
[epoch7, step2017]: loss 0.619253
[epoch7, step2018]: loss 0.243100
[epoch7, step2019]: loss 0.547849
[epoch7, step2020]: loss 0.195091
[epoch7, step2021]: loss 0.677734
[epoch7, step2022]: loss 0.454554
[epoch7, step2023]: loss 0.448186
[epoch7, step2024]: loss 0.551186
[epoch7, step2025]: loss 0.601908
[epoch7, step2026]: loss 0.475878
[epoch7, step2027]: loss 0.527528
[epoch7, step2028]: loss 0.358284
[epoch7, step2029]: loss 0.760642
[epoch7, step2030]: loss 0.742746
[epoch7, step2031]: loss 0.592278
[epoch7, step2032]: loss 0.622637
[epoch7, step2033]: loss 0.540944
[epoch7, step2034]: loss 0.245861
[epoch7, step2035]: loss 0.723527
[epoch7, step2036]: loss 0.710904
[epoch7, step2037]: loss 0.573719
[epoch7, step2038]: loss 0.397568
[epoch7, step2039]: loss 0.447440
[epoch7, step2040]: loss 0.550783
[epoch7, step2041]: loss 0.400733
[epoch7, step2042]: loss 0.363660
[epoch7, step2043]: loss 0.239792
[epoch7, step2044]: loss 0.518658
[epoch7, step2045]: loss 0.609859
[epoch7, step2046]: loss 0.381782
[epoch7, step2047]: loss 0.776011
[epoch7, step2048]: loss 0.484487
[epoch7, step2049]: loss 0.248664
[epoch7, step2050]: loss 0.645505
[epoch7, step2051]: loss 0.443384
[epoch7, step2052]: loss 0.426717
[epoch7, step2053]: loss 0.443872
[epoch7, step2054]: loss 0.663799
[epoch7, step2055]: loss 0.535838
[epoch7, step2056]: loss 0.464863
[epoch7, step2057]: loss 0.527639
[epoch7, step2058]: loss 0.479048
[epoch7, step2059]: loss 0.687128
[epoch7, step2060]: loss 0.662604
[epoch7, step2061]: loss 0.496850
[epoch7, step2062]: loss 0.516433
[epoch7, step2063]: loss 0.273972
[epoch7, step2064]: loss 0.628771
[epoch7, step2065]: loss 0.580131
[epoch7, step2066]: loss 0.531117
[epoch7, step2067]: loss 0.479248
[epoch7, step2068]: loss 0.269901
[epoch7, step2069]: loss 0.494142
[epoch7, step2070]: loss 0.579164
[epoch7, step2071]: loss 0.421391
[epoch7, step2072]: loss 0.275738
[epoch7, step2073]: loss 0.472708
[epoch7, step2074]: loss 0.534311
[epoch7, step2075]: loss 0.622228
[epoch7, step2076]: loss 0.282491
[epoch7, step2077]: loss 0.548319
[epoch7, step2078]: loss 0.309870
[epoch7, step2079]: loss 0.588347
[epoch7, step2080]: loss 0.718935
[epoch7, step2081]: loss 0.598995
[epoch7, step2082]: loss 0.600261
[epoch7, step2083]: loss 0.590568
[epoch7, step2084]: loss 0.687712
[epoch7, step2085]: loss 0.435676
[epoch7, step2086]: loss 0.642061
[epoch7, step2087]: loss 0.497893
[epoch7, step2088]: loss 0.351894
[epoch7, step2089]: loss 0.373005
[epoch7, step2090]: loss 0.540251
[epoch7, step2091]: loss 0.655239
[epoch7, step2092]: loss 0.426452
[epoch7, step2093]: loss 0.310894
[epoch7, step2094]: loss 0.528650
[epoch7, step2095]: loss 0.525968
[epoch7, step2096]: loss 0.472503
[epoch7, step2097]: loss 0.546035
[epoch7, step2098]: loss 0.460349
[epoch7, step2099]: loss 0.387915
[epoch7, step2100]: loss 0.444369
[epoch7, step2101]: loss 0.303612
[epoch7, step2102]: loss 0.663925
[epoch7, step2103]: loss 0.367638
[epoch7, step2104]: loss 0.592723
[epoch7, step2105]: loss 0.118724
[epoch7, step2106]: loss 0.496661
[epoch7, step2107]: loss 0.662702
[epoch7, step2108]: loss 0.609209
[epoch7, step2109]: loss 0.558267
[epoch7, step2110]: loss 0.411284
[epoch7, step2111]: loss 0.245397
[epoch7, step2112]: loss 0.739298
[epoch7, step2113]: loss 0.516298
[epoch7, step2114]: loss 0.212168
[epoch7, step2115]: loss 0.773996
[epoch7, step2116]: loss 0.514471
[epoch7, step2117]: loss 0.234098
[epoch7, step2118]: loss 0.572748
[epoch7, step2119]: loss 0.551990
[epoch7, step2120]: loss 0.649238
[epoch7, step2121]: loss 0.566971
[epoch7, step2122]: loss 0.396768
[epoch7, step2123]: loss 0.713991
[epoch7, step2124]: loss 0.502472
[epoch7, step2125]: loss 0.630065
[epoch7, step2126]: loss 0.485993
[epoch7, step2127]: loss 0.652793
[epoch7, step2128]: loss 0.589365
[epoch7, step2129]: loss 0.639419
[epoch7, step2130]: loss 0.331380
[epoch7, step2131]: loss 0.471502
[epoch7, step2132]: loss 0.586042
[epoch7, step2133]: loss 0.512142
[epoch7, step2134]: loss 0.596847
[epoch7, step2135]: loss 0.377532
[epoch7, step2136]: loss 0.551227
[epoch7, step2137]: loss 0.450730
[epoch7, step2138]: loss 0.168619
[epoch7, step2139]: loss 0.535770
[epoch7, step2140]: loss 0.540854
[epoch7, step2141]: loss 0.280093
[epoch7, step2142]: loss 0.610309
[epoch7, step2143]: loss 0.431806
[epoch7, step2144]: loss 0.456798
[epoch7, step2145]: loss 0.446537
[epoch7, step2146]: loss 0.663562
[epoch7, step2147]: loss 0.508332
[epoch7, step2148]: loss 0.376266
[epoch7, step2149]: loss 0.753776
[epoch7, step2150]: loss 0.274756
[epoch7, step2151]: loss 0.706021
[epoch7, step2152]: loss 0.302931
[epoch7, step2153]: loss 0.542882
[epoch7, step2154]: loss 0.443609
[epoch7, step2155]: loss 0.501696
[epoch7, step2156]: loss 0.428489
[epoch7, step2157]: loss 0.660841
[epoch7, step2158]: loss 0.553554
[epoch7, step2159]: loss 0.394572
[epoch7, step2160]: loss 0.763387
[epoch7, step2161]: loss 0.630152
[epoch7, step2162]: loss 0.448318
[epoch7, step2163]: loss 0.429684
[epoch7, step2164]: loss 0.662599
[epoch7, step2165]: loss 0.521911
[epoch7, step2166]: loss 0.357359
[epoch7, step2167]: loss 0.602999
[epoch7, step2168]: loss 0.726543
[epoch7, step2169]: loss 0.288768
[epoch7, step2170]: loss 0.440360
[epoch7, step2171]: loss 0.526519
[epoch7, step2172]: loss 0.476477
[epoch7, step2173]: loss 0.348023
[epoch7, step2174]: loss 0.628454
[epoch7, step2175]: loss 0.511781
[epoch7, step2176]: loss 0.550438
[epoch7, step2177]: loss 0.637841
[epoch7, step2178]: loss 0.368499
[epoch7, step2179]: loss 0.356125
[epoch7, step2180]: loss 0.569573
[epoch7, step2181]: loss 0.531668
[epoch7, step2182]: loss 0.624240
[epoch7, step2183]: loss 0.492320
[epoch7, step2184]: loss 0.798726
[epoch7, step2185]: loss 0.558692
[epoch7, step2186]: loss 0.595861
[epoch7, step2187]: loss 0.640398
[epoch7, step2188]: loss 0.730835
[epoch7, step2189]: loss 0.423009
[epoch7, step2190]: loss 0.858688
[epoch7, step2191]: loss 0.518282
[epoch7, step2192]: loss 0.359741
[epoch7, step2193]: loss 0.556964
[epoch7, step2194]: loss 0.479498
[epoch7, step2195]: loss 0.532014
[epoch7, step2196]: loss 0.535417
[epoch7, step2197]: loss 0.666348
[epoch7, step2198]: loss 0.372947
[epoch7, step2199]: loss 0.393931
[epoch7, step2200]: loss 0.386371
[epoch7, step2201]: loss 0.568112
[epoch7, step2202]: loss 0.657259
[epoch7, step2203]: loss 0.608856
[epoch7, step2204]: loss 0.441592
[epoch7, step2205]: loss 0.527862
[epoch7, step2206]: loss 0.679757
[epoch7, step2207]: loss 0.334364
[epoch7, step2208]: loss 0.503254
[epoch7, step2209]: loss 0.413769
[epoch7, step2210]: loss 0.687485
[epoch7, step2211]: loss 0.503092
[epoch7, step2212]: loss 0.562623
[epoch7, step2213]: loss 0.608707
[epoch7, step2214]: loss 0.338343
[epoch7, step2215]: loss 0.545080
[epoch7, step2216]: loss 0.457676
[epoch7, step2217]: loss 0.572119
[epoch7, step2218]: loss 0.744932
[epoch7, step2219]: loss 0.605346
[epoch7, step2220]: loss 0.569726
[epoch7, step2221]: loss 0.529433
[epoch7, step2222]: loss 0.702007
[epoch7, step2223]: loss 0.545920
[epoch7, step2224]: loss 0.713874
[epoch7, step2225]: loss 0.426610
[epoch7, step2226]: loss 0.630625
[epoch7, step2227]: loss 0.528225
[epoch7, step2228]: loss 0.548701
[epoch7, step2229]: loss 0.676090
[epoch7, step2230]: loss 0.605166
[epoch7, step2231]: loss 0.296824
[epoch7, step2232]: loss 0.562239
[epoch7, step2233]: loss 0.436005
[epoch7, step2234]: loss 0.664443
[epoch7, step2235]: loss 0.704925
[epoch7, step2236]: loss 0.454410
[epoch7, step2237]: loss 0.647389
[epoch7, step2238]: loss 0.428529
[epoch7, step2239]: loss 0.637615
[epoch7, step2240]: loss 0.388104
[epoch7, step2241]: loss 0.729910
[epoch7, step2242]: loss 0.504904
[epoch7, step2243]: loss 0.566453
[epoch7, step2244]: loss 0.826208
[epoch7, step2245]: loss 0.790989
[epoch7, step2246]: loss 0.609034
[epoch7, step2247]: loss 0.655477
[epoch7, step2248]: loss 0.514777
[epoch7, step2249]: loss 0.297992
[epoch7, step2250]: loss 0.536116
[epoch7, step2251]: loss 0.491091
[epoch7, step2252]: loss 0.451350
[epoch7, step2253]: loss 0.580671
[epoch7, step2254]: loss 0.416795
[epoch7, step2255]: loss 0.459329
[epoch7, step2256]: loss 0.364279
[epoch7, step2257]: loss 0.471351
[epoch7, step2258]: loss 0.464150
[epoch7, step2259]: loss 0.454628
[epoch7, step2260]: loss 0.558245
[epoch7, step2261]: loss 0.643481
[epoch7, step2262]: loss 0.584505
[epoch7, step2263]: loss 0.598309
[epoch7, step2264]: loss 0.375404
[epoch7, step2265]: loss 0.454936
[epoch7, step2266]: loss 0.430698
[epoch7, step2267]: loss 0.684404
[epoch7, step2268]: loss 0.308284
[epoch7, step2269]: loss 0.687889
[epoch7, step2270]: loss 0.526046
[epoch7, step2271]: loss 0.631721
[epoch7, step2272]: loss 0.487384
[epoch7, step2273]: loss 0.532695
[epoch7, step2274]: loss 0.601122
[epoch7, step2275]: loss 0.375428
[epoch7, step2276]: loss 0.740966
[epoch7, step2277]: loss 0.381755
[epoch7, step2278]: loss 0.468932
[epoch7, step2279]: loss 0.539801
[epoch7, step2280]: loss 0.275140
[epoch7, step2281]: loss 0.552446
[epoch7, step2282]: loss 0.571127
[epoch7, step2283]: loss 0.655235
[epoch7, step2284]: loss 0.678616
[epoch7, step2285]: loss 0.653901
[epoch7, step2286]: loss 0.411676
[epoch7, step2287]: loss 0.406377
[epoch7, step2288]: loss 0.674812
[epoch7, step2289]: loss 0.525376
[epoch7, step2290]: loss 0.392925
[epoch7, step2291]: loss 0.566503
[epoch7, step2292]: loss 0.628102
[epoch7, step2293]: loss 0.437426
[epoch7, step2294]: loss 0.714784
[epoch7, step2295]: loss 0.478658
[epoch7, step2296]: loss 0.657868
[epoch7, step2297]: loss 0.684349
[epoch7, step2298]: loss 0.564151
[epoch7, step2299]: loss 0.635891
[epoch7, step2300]: loss 0.792139
[epoch7, step2301]: loss 0.425809
[epoch7, step2302]: loss 0.473401
[epoch7, step2303]: loss 0.583612
[epoch7, step2304]: loss 0.512717
[epoch7, step2305]: loss 0.515834
[epoch7, step2306]: loss 0.723504
[epoch7, step2307]: loss 0.538866
[epoch7, step2308]: loss 0.577022
[epoch7, step2309]: loss 0.754573
[epoch7, step2310]: loss 0.536964
[epoch7, step2311]: loss 0.221645
[epoch7, step2312]: loss 0.705762
[epoch7, step2313]: loss 0.642364
[epoch7, step2314]: loss 0.617252
[epoch7, step2315]: loss 0.578347
[epoch7, step2316]: loss 0.598653
[epoch7, step2317]: loss 0.583179
[epoch7, step2318]: loss 0.459392
[epoch7, step2319]: loss 0.492281
[epoch7, step2320]: loss 0.666467
[epoch7, step2321]: loss 0.666927
[epoch7, step2322]: loss 0.699497
[epoch7, step2323]: loss 0.772655
[epoch7, step2324]: loss 0.464183
[epoch7, step2325]: loss 0.310452
[epoch7, step2326]: loss 0.568185
[epoch7, step2327]: loss 0.382727
[epoch7, step2328]: loss 0.505150
[epoch7, step2329]: loss 0.573690
[epoch7, step2330]: loss 0.388618
[epoch7, step2331]: loss 0.522440
[epoch7, step2332]: loss 0.657463
[epoch7, step2333]: loss 0.597285
[epoch7, step2334]: loss 0.379969
[epoch7, step2335]: loss 0.426217
[epoch7, step2336]: loss 0.616237
[epoch7, step2337]: loss 0.546223
[epoch7, step2338]: loss 0.387373
[epoch7, step2339]: loss 0.389107
[epoch7, step2340]: loss 0.720332
[epoch7, step2341]: loss 0.429793
[epoch7, step2342]: loss 0.533284
[epoch7, step2343]: loss 0.506231
[epoch7, step2344]: loss 0.546220
[epoch7, step2345]: loss 0.704959
[epoch7, step2346]: loss 0.373782
[epoch7, step2347]: loss 0.645952
[epoch7, step2348]: loss 0.287586
[epoch7, step2349]: loss 0.467847
[epoch7, step2350]: loss 0.386695
[epoch7, step2351]: loss 0.518961
[epoch7, step2352]: loss 0.591315
[epoch7, step2353]: loss 0.670304
[epoch7, step2354]: loss 0.609364
[epoch7, step2355]: loss 0.439075
[epoch7, step2356]: loss 0.466411
[epoch7, step2357]: loss 0.477151
[epoch7, step2358]: loss 0.726289
[epoch7, step2359]: loss 0.517036
[epoch7, step2360]: loss 0.352885
[epoch7, step2361]: loss 0.706862
[epoch7, step2362]: loss 0.414544
[epoch7, step2363]: loss 0.603767
[epoch7, step2364]: loss 0.620775
[epoch7, step2365]: loss 0.662635
[epoch7, step2366]: loss 0.446725
[epoch7, step2367]: loss 0.475889
[epoch7, step2368]: loss 0.596796
[epoch7, step2369]: loss 0.602943
[epoch7, step2370]: loss 0.542314
[epoch7, step2371]: loss 0.473378
[epoch7, step2372]: loss 0.387348
[epoch7, step2373]: loss 0.582277
[epoch7, step2374]: loss 0.619221
[epoch7, step2375]: loss 0.537053
[epoch7, step2376]: loss 0.644797
[epoch7, step2377]: loss 0.568926
[epoch7, step2378]: loss 0.557836
[epoch7, step2379]: loss 0.548421
[epoch7, step2380]: loss 0.391107
[epoch7, step2381]: loss 0.554354
[epoch7, step2382]: loss 0.366904
[epoch7, step2383]: loss 0.457923
[epoch7, step2384]: loss 0.589467
[epoch7, step2385]: loss 0.358020
[epoch7, step2386]: loss 0.516025
[epoch7, step2387]: loss 0.319215
[epoch7, step2388]: loss 0.218546
[epoch7, step2389]: loss 0.522140
[epoch7, step2390]: loss 0.593484
[epoch7, step2391]: loss 0.709413
[epoch7, step2392]: loss 0.602532
[epoch7, step2393]: loss 0.512034
[epoch7, step2394]: loss 0.671283
[epoch7, step2395]: loss 0.535748
[epoch7, step2396]: loss 0.532871
[epoch7, step2397]: loss 0.585890
[epoch7, step2398]: loss 0.428209
[epoch7, step2399]: loss 0.464951
[epoch7, step2400]: loss 0.107315
[epoch7, step2401]: loss 0.434193
[epoch7, step2402]: loss 0.275916
[epoch7, step2403]: loss 0.486194
[epoch7, step2404]: loss 0.457842
[epoch7, step2405]: loss 0.582360
[epoch7, step2406]: loss 0.569612
[epoch7, step2407]: loss 0.593152
[epoch7, step2408]: loss 0.662784
[epoch7, step2409]: loss 0.641573
[epoch7, step2410]: loss 0.709498
[epoch7, step2411]: loss 0.464449
[epoch7, step2412]: loss 0.581092
[epoch7, step2413]: loss 0.457734
[epoch7, step2414]: loss 0.530969
[epoch7, step2415]: loss 0.435783
[epoch7, step2416]: loss 0.624893
[epoch7, step2417]: loss 0.396663
[epoch7, step2418]: loss 0.367244
[epoch7, step2419]: loss 0.503327
[epoch7, step2420]: loss 0.580227
[epoch7, step2421]: loss 0.698812
[epoch7, step2422]: loss 0.584777
[epoch7, step2423]: loss 0.282278
[epoch7, step2424]: loss 0.520381
[epoch7, step2425]: loss 0.272513
[epoch7, step2426]: loss 0.622134
[epoch7, step2427]: loss 0.634404
[epoch7, step2428]: loss 0.142765
[epoch7, step2429]: loss 0.456346
[epoch7, step2430]: loss 0.707819
[epoch7, step2431]: loss 0.401790
[epoch7, step2432]: loss 0.743972
[epoch7, step2433]: loss 0.456044
[epoch7, step2434]: loss 0.452539
[epoch7, step2435]: loss 0.389244
[epoch7, step2436]: loss 0.498804
[epoch7, step2437]: loss 0.779157
[epoch7, step2438]: loss 0.515276
[epoch7, step2439]: loss 0.503207
[epoch7, step2440]: loss 0.624142
[epoch7, step2441]: loss 0.360359
[epoch7, step2442]: loss 0.569004
[epoch7, step2443]: loss 0.585192
[epoch7, step2444]: loss 0.539696
[epoch7, step2445]: loss 0.599914
[epoch7, step2446]: loss 0.421037
[epoch7, step2447]: loss 0.544195
[epoch7, step2448]: loss 0.317915
[epoch7, step2449]: loss 0.555490
[epoch7, step2450]: loss 0.467113
[epoch7, step2451]: loss 0.579071
[epoch7, step2452]: loss 0.685096
[epoch7, step2453]: loss 0.497183
[epoch7, step2454]: loss 0.379320
[epoch7, step2455]: loss 0.402132
[epoch7, step2456]: loss 0.553933
[epoch7, step2457]: loss 0.415167
[epoch7, step2458]: loss 0.299749
[epoch7, step2459]: loss 0.639809
[epoch7, step2460]: loss 0.625421
[epoch7, step2461]: loss 0.630181
[epoch7, step2462]: loss 0.517802
[epoch7, step2463]: loss 0.495043
[epoch7, step2464]: loss 0.466352
[epoch7, step2465]: loss 0.299116
[epoch7, step2466]: loss 0.312574
[epoch7, step2467]: loss 0.269089
[epoch7, step2468]: loss 0.514549
[epoch7, step2469]: loss 0.685161
[epoch7, step2470]: loss 0.499864
[epoch7, step2471]: loss 0.376361
[epoch7, step2472]: loss 0.367437
[epoch7, step2473]: loss 0.615872
[epoch7, step2474]: loss 0.463679
[epoch7, step2475]: loss 0.474054
[epoch7, step2476]: loss 0.409534
[epoch7, step2477]: loss 0.455057
[epoch7, step2478]: loss 0.654620
[epoch7, step2479]: loss 0.264614
[epoch7, step2480]: loss 0.442693
[epoch7, step2481]: loss 0.520934
[epoch7, step2482]: loss 0.578205
[epoch7, step2483]: loss 0.478528
[epoch7, step2484]: loss 0.540742
[epoch7, step2485]: loss 0.515054
[epoch7, step2486]: loss 0.554465
[epoch7, step2487]: loss 0.650644
[epoch7, step2488]: loss 0.814381
[epoch7, step2489]: loss 0.348279
[epoch7, step2490]: loss 0.294925
[epoch7, step2491]: loss 0.457009
[epoch7, step2492]: loss 0.535636
[epoch7, step2493]: loss 0.396332
[epoch7, step2494]: loss 0.122469
[epoch7, step2495]: loss 0.421257
[epoch7, step2496]: loss 0.470312
[epoch7, step2497]: loss 0.444295
[epoch7, step2498]: loss 0.662199
[epoch7, step2499]: loss 0.638507
[epoch7, step2500]: loss 0.597776
[epoch7, step2501]: loss 0.721501
[epoch7, step2502]: loss 0.783623
[epoch7, step2503]: loss 0.697233
[epoch7, step2504]: loss 0.597631
[epoch7, step2505]: loss 0.542083
[epoch7, step2506]: loss 0.528570
[epoch7, step2507]: loss 0.612398
[epoch7, step2508]: loss 0.651442
[epoch7, step2509]: loss 0.283333
[epoch7, step2510]: loss 0.730117
[epoch7, step2511]: loss 0.551768
[epoch7, step2512]: loss 0.384322
[epoch7, step2513]: loss 0.417023
[epoch7, step2514]: loss 0.400574
[epoch7, step2515]: loss 0.801072
[epoch7, step2516]: loss 0.294468
[epoch7, step2517]: loss 0.407192
[epoch7, step2518]: loss 0.101013
[epoch7, step2519]: loss 0.601487
[epoch7, step2520]: loss 0.349108
[epoch7, step2521]: loss 0.714039
[epoch7, step2522]: loss 0.817384
[epoch7, step2523]: loss 0.697545
[epoch7, step2524]: loss 0.542529
[epoch7, step2525]: loss 0.598260
[epoch7, step2526]: loss 0.653159
[epoch7, step2527]: loss 0.633228
[epoch7, step2528]: loss 0.475903
[epoch7, step2529]: loss 0.480003
[epoch7, step2530]: loss 0.625716
[epoch7, step2531]: loss 0.385900
[epoch7, step2532]: loss 0.486963
[epoch7, step2533]: loss 0.470103
[epoch7, step2534]: loss 0.415895
[epoch7, step2535]: loss 0.549378
[epoch7, step2536]: loss 0.738598
[epoch7, step2537]: loss 0.492212
[epoch7, step2538]: loss 0.522370
[epoch7, step2539]: loss 0.394137
[epoch7, step2540]: loss 0.529262
[epoch7, step2541]: loss 0.528973
[epoch7, step2542]: loss 0.447932
[epoch7, step2543]: loss 0.380930
[epoch7, step2544]: loss 0.714935
[epoch7, step2545]: loss 0.813422
[epoch7, step2546]: loss 0.516002
[epoch7, step2547]: loss 0.690770
[epoch7, step2548]: loss 0.618339
[epoch7, step2549]: loss 0.611782
[epoch7, step2550]: loss 0.543844
[epoch7, step2551]: loss 0.645447
[epoch7, step2552]: loss 0.378823
[epoch7, step2553]: loss 0.374841
[epoch7, step2554]: loss 0.619275
[epoch7, step2555]: loss 0.613549
[epoch7, step2556]: loss 0.777487
[epoch7, step2557]: loss 0.479089
[epoch7, step2558]: loss 0.375876
[epoch7, step2559]: loss 0.556598
[epoch7, step2560]: loss 0.605944
[epoch7, step2561]: loss 0.625523
[epoch7, step2562]: loss 0.327122
[epoch7, step2563]: loss 0.451297
[epoch7, step2564]: loss 0.526774
[epoch7, step2565]: loss 0.479524
[epoch7, step2566]: loss 0.448528
[epoch7, step2567]: loss 0.305427
[epoch7, step2568]: loss 0.661391
[epoch7, step2569]: loss 0.791633
[epoch7, step2570]: loss 0.355822
[epoch7, step2571]: loss 0.778980
[epoch7, step2572]: loss 0.732256
[epoch7, step2573]: loss 0.525265
[epoch7, step2574]: loss 0.420499
[epoch7, step2575]: loss 0.527298
[epoch7, step2576]: loss 0.283068
[epoch7, step2577]: loss 0.724173
[epoch7, step2578]: loss 0.557986
[epoch7, step2579]: loss 0.450943
[epoch7, step2580]: loss 0.451978
[epoch7, step2581]: loss 0.391546
[epoch7, step2582]: loss 0.592544
[epoch7, step2583]: loss 0.533358
[epoch7, step2584]: loss 0.604422
[epoch7, step2585]: loss 0.436915
[epoch7, step2586]: loss 0.315702
[epoch7, step2587]: loss 0.300756
[epoch7, step2588]: loss 0.376832
[epoch7, step2589]: loss 0.495855
[epoch7, step2590]: loss 0.595412
[epoch7, step2591]: loss 0.555400
[epoch7, step2592]: loss 0.464836
[epoch7, step2593]: loss 0.255482
[epoch7, step2594]: loss 0.600229
[epoch7, step2595]: loss 0.576564
[epoch7, step2596]: loss 0.638603
[epoch7, step2597]: loss 0.696437
[epoch7, step2598]: loss 0.513437
[epoch7, step2599]: loss 0.619811
[epoch7, step2600]: loss 0.546254
[epoch7, step2601]: loss 0.455865
[epoch7, step2602]: loss 0.741808
[epoch7, step2603]: loss 0.359795
[epoch7, step2604]: loss 0.234483
[epoch7, step2605]: loss 0.528329
[epoch7, step2606]: loss 0.392691
[epoch7, step2607]: loss 0.428649
[epoch7, step2608]: loss 0.735028
[epoch7, step2609]: loss 0.384962
[epoch7, step2610]: loss 0.435334
[epoch7, step2611]: loss 0.463628
[epoch7, step2612]: loss 0.440335
[epoch7, step2613]: loss 0.434121
[epoch7, step2614]: loss 0.606751
[epoch7, step2615]: loss 0.566618
[epoch7, step2616]: loss 0.457489
[epoch7, step2617]: loss 0.593088
[epoch7, step2618]: loss 0.584267
[epoch7, step2619]: loss 0.673822
[epoch7, step2620]: loss 0.548279
[epoch7, step2621]: loss 0.852664
[epoch7, step2622]: loss 0.546586
[epoch7, step2623]: loss 0.559199
[epoch7, step2624]: loss 0.505702
[epoch7, step2625]: loss 0.549415
[epoch7, step2626]: loss 0.642513
[epoch7, step2627]: loss 0.513386
[epoch7, step2628]: loss 0.692640
[epoch7, step2629]: loss 0.627386
[epoch7, step2630]: loss 0.509726
[epoch7, step2631]: loss 0.532360
[epoch7, step2632]: loss 0.438284
[epoch7, step2633]: loss 0.565537
[epoch7, step2634]: loss 0.637097
[epoch7, step2635]: loss 0.369960
[epoch7, step2636]: loss 0.653567
[epoch7, step2637]: loss 0.608230
[epoch7, step2638]: loss 0.473424
[epoch7, step2639]: loss 0.707242
[epoch7, step2640]: loss 0.663834
[epoch7, step2641]: loss 0.615273
[epoch7, step2642]: loss 0.710648
[epoch7, step2643]: loss 0.469011
[epoch7, step2644]: loss 0.564550
[epoch7, step2645]: loss 0.400596
[epoch7, step2646]: loss 0.630076
[epoch7, step2647]: loss 0.360710
[epoch7, step2648]: loss 0.714599
[epoch7, step2649]: loss 0.740234
[epoch7, step2650]: loss 0.734188
[epoch7, step2651]: loss 0.511743
[epoch7, step2652]: loss 0.250025
[epoch7, step2653]: loss 0.574250
[epoch7, step2654]: loss 0.527664
[epoch7, step2655]: loss 0.386788
[epoch7, step2656]: loss 0.375325
[epoch7, step2657]: loss 0.685763
[epoch7, step2658]: loss 0.628032
[epoch7, step2659]: loss 0.296726
[epoch7, step2660]: loss 0.576001
[epoch7, step2661]: loss 0.486034
[epoch7, step2662]: loss 0.584072
[epoch7, step2663]: loss 0.561896
[epoch7, step2664]: loss 0.689083
[epoch7, step2665]: loss 0.408478
[epoch7, step2666]: loss 0.547544
[epoch7, step2667]: loss 0.445477
[epoch7, step2668]: loss 0.472077
[epoch7, step2669]: loss 0.628789
[epoch7, step2670]: loss 0.740131
[epoch7, step2671]: loss 0.325780
[epoch7, step2672]: loss 0.329400
[epoch7, step2673]: loss 0.607452
[epoch7, step2674]: loss 0.567766
[epoch7, step2675]: loss 0.628260
[epoch7, step2676]: loss 0.449209
[epoch7, step2677]: loss 0.538394
[epoch7, step2678]: loss 0.507893
[epoch7, step2679]: loss 0.350675
[epoch7, step2680]: loss 0.297075
[epoch7, step2681]: loss 0.617829
[epoch7, step2682]: loss 0.480785
[epoch7, step2683]: loss 0.287125
[epoch7, step2684]: loss 0.678657
[epoch7, step2685]: loss 0.505058
[epoch7, step2686]: loss 0.515284
[epoch7, step2687]: loss 0.526967
[epoch7, step2688]: loss 0.634359
[epoch7, step2689]: loss 0.495717
[epoch7, step2690]: loss 0.383447
[epoch7, step2691]: loss 0.415444
[epoch7, step2692]: loss 0.570297
[epoch7, step2693]: loss 0.424519
[epoch7, step2694]: loss 0.406771
[epoch7, step2695]: loss 0.604039
[epoch7, step2696]: loss 0.585184
[epoch7, step2697]: loss 0.317557
[epoch7, step2698]: loss 0.463919
[epoch7, step2699]: loss 0.484372
[epoch7, step2700]: loss 0.493552
[epoch7, step2701]: loss 0.483236
[epoch7, step2702]: loss 0.481518
[epoch7, step2703]: loss 0.461720
[epoch7, step2704]: loss 0.571468
[epoch7, step2705]: loss 0.397541
[epoch7, step2706]: loss 0.648125
[epoch7, step2707]: loss 0.257028
[epoch7, step2708]: loss 0.570821
[epoch7, step2709]: loss 0.721057
[epoch7, step2710]: loss 0.542675
[epoch7, step2711]: loss 0.746835
[epoch7, step2712]: loss 0.738776
[epoch7, step2713]: loss 0.715991
[epoch7, step2714]: loss 0.627072
[epoch7, step2715]: loss 0.549852
[epoch7, step2716]: loss 0.488040
[epoch7, step2717]: loss 0.584498
[epoch7, step2718]: loss 0.472188
[epoch7, step2719]: loss 0.628927
[epoch7, step2720]: loss 0.586117
[epoch7, step2721]: loss 0.604147
[epoch7, step2722]: loss 0.635997
[epoch7, step2723]: loss 0.336643
[epoch7, step2724]: loss 0.565223
[epoch7, step2725]: loss 0.548314
[epoch7, step2726]: loss 0.868253
[epoch7, step2727]: loss 0.638233
[epoch7, step2728]: loss 0.464128
[epoch7, step2729]: loss 0.507662
[epoch7, step2730]: loss 0.567307
[epoch7, step2731]: loss 0.415991
[epoch7, step2732]: loss 0.452267
[epoch7, step2733]: loss 0.385023
[epoch7, step2734]: loss 0.617980
[epoch7, step2735]: loss 0.498160
[epoch7, step2736]: loss 0.563636
[epoch7, step2737]: loss 0.416539
[epoch7, step2738]: loss 0.496354
[epoch7, step2739]: loss 0.497390
[epoch7, step2740]: loss 0.394796
[epoch7, step2741]: loss 0.613752
[epoch7, step2742]: loss 0.500756
[epoch7, step2743]: loss 0.773245
[epoch7, step2744]: loss 0.488306
[epoch7, step2745]: loss 0.647913
[epoch7, step2746]: loss 0.664358
[epoch7, step2747]: loss 0.774415
[epoch7, step2748]: loss 0.658867
[epoch7, step2749]: loss 0.527047
[epoch7, step2750]: loss 0.358674
[epoch7, step2751]: loss 0.551397
[epoch7, step2752]: loss 0.508988
[epoch7, step2753]: loss 0.437914
[epoch7, step2754]: loss 0.498689
[epoch7, step2755]: loss 0.260727
[epoch7, step2756]: loss 0.513191
[epoch7, step2757]: loss 0.686164
[epoch7, step2758]: loss 0.503528
[epoch7, step2759]: loss 0.604386
[epoch7, step2760]: loss 0.325004
[epoch7, step2761]: loss 0.569327
[epoch7, step2762]: loss 0.541497
[epoch7, step2763]: loss 0.671946
[epoch7, step2764]: loss 0.681382
[epoch7, step2765]: loss 0.485597
[epoch7, step2766]: loss 0.474246
[epoch7, step2767]: loss 0.388955
[epoch7, step2768]: loss 0.534932
[epoch7, step2769]: loss 0.468328
[epoch7, step2770]: loss 0.461726
[epoch7, step2771]: loss 0.606420
[epoch7, step2772]: loss 0.554277
[epoch7, step2773]: loss 0.296427
[epoch7, step2774]: loss 0.564251
[epoch7, step2775]: loss 0.550610
[epoch7, step2776]: loss 0.681758
[epoch7, step2777]: loss 0.437854
[epoch7, step2778]: loss 0.477679
[epoch7, step2779]: loss 0.578244
[epoch7, step2780]: loss 0.692850
[epoch7, step2781]: loss 0.606588
[epoch7, step2782]: loss 0.522939
[epoch7, step2783]: loss 0.608097
[epoch7, step2784]: loss 0.460611
[epoch7, step2785]: loss 0.571808
[epoch7, step2786]: loss 0.539048
[epoch7, step2787]: loss 0.500590
[epoch7, step2788]: loss 0.560055
[epoch7, step2789]: loss 0.673120
[epoch7, step2790]: loss 0.572880
[epoch7, step2791]: loss 0.272473
[epoch7, step2792]: loss 0.432474
[epoch7, step2793]: loss 0.523819
[epoch7, step2794]: loss 0.494751
[epoch7, step2795]: loss 0.516573
[epoch7, step2796]: loss 0.275361
[epoch7, step2797]: loss 0.608147
[epoch7, step2798]: loss 0.470786
[epoch7, step2799]: loss 0.538752
[epoch7, step2800]: loss 0.343366
[epoch7, step2801]: loss 0.669082
[epoch7, step2802]: loss 0.450322
[epoch7, step2803]: loss 0.444874
[epoch7, step2804]: loss 0.520353
[epoch7, step2805]: loss 0.144713
[epoch7, step2806]: loss 0.615293
[epoch7, step2807]: loss 0.400403
[epoch7, step2808]: loss 0.293844
[epoch7, step2809]: loss 0.518286
[epoch7, step2810]: loss 0.487657
[epoch7, step2811]: loss 0.480247
[epoch7, step2812]: loss 0.451096
[epoch7, step2813]: loss 0.534353
[epoch7, step2814]: loss 0.867640
[epoch7, step2815]: loss 0.434069
[epoch7, step2816]: loss 0.325314
[epoch7, step2817]: loss 0.279082
[epoch7, step2818]: loss 0.667466
[epoch7, step2819]: loss 0.463258
[epoch7, step2820]: loss 0.558998
[epoch7, step2821]: loss 0.481137
[epoch7, step2822]: loss 0.565951
[epoch7, step2823]: loss 0.571430
[epoch7, step2824]: loss 0.379256
[epoch7, step2825]: loss 0.465773
[epoch7, step2826]: loss 0.722171
[epoch7, step2827]: loss 0.663564
[epoch7, step2828]: loss 0.402148
[epoch7, step2829]: loss 0.484329
[epoch7, step2830]: loss 0.741809
[epoch7, step2831]: loss 0.506289
[epoch7, step2832]: loss 0.504884
[epoch7, step2833]: loss 0.410778
[epoch7, step2834]: loss 0.466667
[epoch7, step2835]: loss 0.740409
[epoch7, step2836]: loss 0.542880
[epoch7, step2837]: loss 0.678141
[epoch7, step2838]: loss 0.608625
[epoch7, step2839]: loss 0.560485
[epoch7, step2840]: loss 0.537530
[epoch7, step2841]: loss 0.419397
[epoch7, step2842]: loss 0.389502
[epoch7, step2843]: loss 0.263471
[epoch7, step2844]: loss 0.898377
[epoch7, step2845]: loss 0.365631
[epoch7, step2846]: loss 0.563638
[epoch7, step2847]: loss 0.663956
[epoch7, step2848]: loss 0.687170
[epoch7, step2849]: loss 0.464049
[epoch7, step2850]: loss 0.143618
[epoch7, step2851]: loss 0.382920
[epoch7, step2852]: loss 0.337852
[epoch7, step2853]: loss 0.406315
[epoch7, step2854]: loss 0.608059
[epoch7, step2855]: loss 0.641200
[epoch7, step2856]: loss 0.411318
[epoch7, step2857]: loss 0.483387
[epoch7, step2858]: loss 0.449155
[epoch7, step2859]: loss 0.561206
[epoch7, step2860]: loss 0.681786
[epoch7, step2861]: loss 0.559832
[epoch7, step2862]: loss 0.639524
[epoch7, step2863]: loss 0.667075
[epoch7, step2864]: loss 0.563546
[epoch7, step2865]: loss 0.299619
[epoch7, step2866]: loss 0.535626
[epoch7, step2867]: loss 0.614220
[epoch7, step2868]: loss 0.362017
[epoch7, step2869]: loss 0.665461
[epoch7, step2870]: loss 0.530374
[epoch7, step2871]: loss 0.548142
[epoch7, step2872]: loss 0.412348
[epoch7, step2873]: loss 0.484061
[epoch7, step2874]: loss 0.554585
[epoch7, step2875]: loss 0.680171
[epoch7, step2876]: loss 0.539423
[epoch7, step2877]: loss 0.497121
[epoch7, step2878]: loss 0.567305
[epoch7, step2879]: loss 0.579153
[epoch7, step2880]: loss 0.521137
[epoch7, step2881]: loss 0.472111
[epoch7, step2882]: loss 0.598686
[epoch7, step2883]: loss 0.590936
[epoch7, step2884]: loss 0.573828
[epoch7, step2885]: loss 0.565590
[epoch7, step2886]: loss 0.565136
[epoch7, step2887]: loss 0.531500
[epoch7, step2888]: loss 0.713123
[epoch7, step2889]: loss 0.277448
[epoch7, step2890]: loss 0.571088
[epoch7, step2891]: loss 0.568646
[epoch7, step2892]: loss 0.382521
[epoch7, step2893]: loss 0.753397
[epoch7, step2894]: loss 0.653266
[epoch7, step2895]: loss 0.481755
[epoch7, step2896]: loss 0.176249
[epoch7, step2897]: loss 0.637726
[epoch7, step2898]: loss 0.612735
[epoch7, step2899]: loss 0.447871
[epoch7, step2900]: loss 0.338065
[epoch7, step2901]: loss 0.516402
[epoch7, step2902]: loss 0.654409
[epoch7, step2903]: loss 0.457600
[epoch7, step2904]: loss 0.433500
[epoch7, step2905]: loss 0.431219
[epoch7, step2906]: loss 0.571287
[epoch7, step2907]: loss 0.736958
[epoch7, step2908]: loss 0.385311
[epoch7, step2909]: loss 0.600882
[epoch7, step2910]: loss 0.601729
[epoch7, step2911]: loss 0.438380
[epoch7, step2912]: loss 0.612278
[epoch7, step2913]: loss 0.713554
[epoch7, step2914]: loss 0.748776
[epoch7, step2915]: loss 0.771863
[epoch7, step2916]: loss 0.403534
[epoch7, step2917]: loss 0.504252
[epoch7, step2918]: loss 0.530795
[epoch7, step2919]: loss 0.600891
[epoch7, step2920]: loss 0.273999
[epoch7, step2921]: loss 0.562325
[epoch7, step2922]: loss 0.335808
[epoch7, step2923]: loss 0.692503
[epoch7, step2924]: loss 0.717643
[epoch7, step2925]: loss 0.537079
[epoch7, step2926]: loss 0.650351
[epoch7, step2927]: loss 0.593553
[epoch7, step2928]: loss 0.718929
[epoch7, step2929]: loss 0.480017
[epoch7, step2930]: loss 0.612331
[epoch7, step2931]: loss 0.621834
[epoch7, step2932]: loss 0.399036
[epoch7, step2933]: loss 0.420370
[epoch7, step2934]: loss 0.670655
[epoch7, step2935]: loss 0.653249
[epoch7, step2936]: loss 0.701364
[epoch7, step2937]: loss 0.627127
[epoch7, step2938]: loss 0.417464
[epoch7, step2939]: loss 0.490551
[epoch7, step2940]: loss 0.619318
[epoch7, step2941]: loss 0.614083
[epoch7, step2942]: loss 0.510711
[epoch7, step2943]: loss 0.601425
[epoch7, step2944]: loss 0.529685
[epoch7, step2945]: loss 0.720735
[epoch7, step2946]: loss 0.456992
[epoch7, step2947]: loss 0.412430
[epoch7, step2948]: loss 0.474447
[epoch7, step2949]: loss 0.304624
[epoch7, step2950]: loss 0.439759
[epoch7, step2951]: loss 0.539733
[epoch7, step2952]: loss 0.516849
[epoch7, step2953]: loss 0.755161
[epoch7, step2954]: loss 0.376243
[epoch7, step2955]: loss 0.552133
[epoch7, step2956]: loss 0.722058
[epoch7, step2957]: loss 0.644067
[epoch7, step2958]: loss 0.477461
[epoch7, step2959]: loss 0.401602
[epoch7, step2960]: loss 0.374068
[epoch7, step2961]: loss 0.660066
[epoch7, step2962]: loss 0.496421
[epoch7, step2963]: loss 0.539971
[epoch7, step2964]: loss 0.557697
[epoch7, step2965]: loss 0.553123
[epoch7, step2966]: loss 0.406868
[epoch7, step2967]: loss 0.266413
[epoch7, step2968]: loss 0.514606
[epoch7, step2969]: loss 0.670598
[epoch7, step2970]: loss 0.415926
[epoch7, step2971]: loss 0.483966
[epoch7, step2972]: loss 0.479073
[epoch7, step2973]: loss 0.552111
[epoch7, step2974]: loss 0.449825
[epoch7, step2975]: loss 0.766789
[epoch7, step2976]: loss 0.463624
[epoch7, step2977]: loss 0.422678
[epoch7, step2978]: loss 0.427394
[epoch7, step2979]: loss 0.549847
[epoch7, step2980]: loss 0.670197
[epoch7, step2981]: loss 0.392526
[epoch7, step2982]: loss 0.557720
[epoch7, step2983]: loss 0.645351
[epoch7, step2984]: loss 0.617857
[epoch7, step2985]: loss 0.442045
[epoch7, step2986]: loss 0.323295
[epoch7, step2987]: loss 0.361559
[epoch7, step2988]: loss 0.332885
[epoch7, step2989]: loss 0.503225
[epoch7, step2990]: loss 0.560988
[epoch7, step2991]: loss 0.591231
[epoch7, step2992]: loss 0.526973
[epoch7, step2993]: loss 0.538023
[epoch7, step2994]: loss 0.695645
[epoch7, step2995]: loss 0.577767
[epoch7, step2996]: loss 0.147866
[epoch7, step2997]: loss 0.547203
[epoch7, step2998]: loss 0.629131
[epoch7, step2999]: loss 0.616988
[epoch7, step3000]: loss 0.497762
[epoch7, step3001]: loss 0.775071
[epoch7, step3002]: loss 0.434279
[epoch7, step3003]: loss 0.269313
[epoch7, step3004]: loss 0.292537
[epoch7, step3005]: loss 0.560572
[epoch7, step3006]: loss 0.407908
[epoch7, step3007]: loss 0.405871
[epoch7, step3008]: loss 0.516177
[epoch7, step3009]: loss 0.535437
[epoch7, step3010]: loss 0.493875
[epoch7, step3011]: loss 0.365282
[epoch7, step3012]: loss 0.412305
[epoch7, step3013]: loss 0.546918
[epoch7, step3014]: loss 0.609796
[epoch7, step3015]: loss 0.785537
[epoch7, step3016]: loss 0.508571
[epoch7, step3017]: loss 0.713950
[epoch7, step3018]: loss 0.569596
[epoch7, step3019]: loss 0.608196
[epoch7, step3020]: loss 0.495858
[epoch7, step3021]: loss 0.616607
[epoch7, step3022]: loss 0.452252
[epoch7, step3023]: loss 0.418619
[epoch7, step3024]: loss 0.569561
[epoch7, step3025]: loss 0.557743
[epoch7, step3026]: loss 0.372797
[epoch7, step3027]: loss 0.516779
[epoch7, step3028]: loss 0.674295
[epoch7, step3029]: loss 0.590293
[epoch7, step3030]: loss 0.589384
[epoch7, step3031]: loss 0.598643
[epoch7, step3032]: loss 0.756996
[epoch7, step3033]: loss 0.342338
[epoch7, step3034]: loss 0.570939
[epoch7, step3035]: loss 0.500652
[epoch7, step3036]: loss 0.742943
[epoch7, step3037]: loss 0.643143
[epoch7, step3038]: loss 0.726747
[epoch7, step3039]: loss 0.373310
[epoch7, step3040]: loss 0.561163
[epoch7, step3041]: loss 0.607748
[epoch7, step3042]: loss 0.492329
[epoch7, step3043]: loss 0.621701
[epoch7, step3044]: loss 0.546129
[epoch7, step3045]: loss 0.547582
[epoch7, step3046]: loss 0.706899
[epoch7, step3047]: loss 0.638600
[epoch7, step3048]: loss 0.753612
[epoch7, step3049]: loss 0.531406
[epoch7, step3050]: loss 0.479883
[epoch7, step3051]: loss 0.415951
[epoch7, step3052]: loss 0.693789
[epoch7, step3053]: loss 0.715156
[epoch7, step3054]: loss 0.492477
[epoch7, step3055]: loss 0.681425
[epoch7, step3056]: loss 0.551793
[epoch7, step3057]: loss 0.494376
[epoch7, step3058]: loss 0.515827
[epoch7, step3059]: loss 0.408572
[epoch7, step3060]: loss 0.624959
[epoch7, step3061]: loss 0.748263
[epoch7, step3062]: loss 0.732910
[epoch7, step3063]: loss 0.470768
[epoch7, step3064]: loss 0.568148
[epoch7, step3065]: loss 0.439637
[epoch7, step3066]: loss 0.689148
[epoch7, step3067]: loss 0.300427
[epoch7, step3068]: loss 0.405017
[epoch7, step3069]: loss 0.743405
[epoch7, step3070]: loss 0.334417
[epoch7, step3071]: loss 0.640478
[epoch7, step3072]: loss 0.697595
[epoch7, step3073]: loss 0.625763
[epoch7, step3074]: loss 0.844953
[epoch7, step3075]: loss 0.632924
[epoch7, step3076]: loss 0.464487

[epoch7]: avg loss 0.464487

[epoch8, step1]: loss 0.381883
[epoch8, step2]: loss 0.567119
[epoch8, step3]: loss 0.597564
[epoch8, step4]: loss 0.331306
[epoch8, step5]: loss 0.275349
[epoch8, step6]: loss 0.556627
[epoch8, step7]: loss 0.496131
[epoch8, step8]: loss 0.617407
[epoch8, step9]: loss 0.667125
[epoch8, step10]: loss 0.470000
[epoch8, step11]: loss 0.430484
[epoch8, step12]: loss 0.473951
[epoch8, step13]: loss 0.823272
[epoch8, step14]: loss 0.460513
[epoch8, step15]: loss 0.354147
[epoch8, step16]: loss 0.395571
[epoch8, step17]: loss 0.309606
[epoch8, step18]: loss 0.619973
[epoch8, step19]: loss 0.626120
[epoch8, step20]: loss 0.469166
[epoch8, step21]: loss 0.687924
[epoch8, step22]: loss 0.468075
[epoch8, step23]: loss 0.653145
[epoch8, step24]: loss 0.512460
[epoch8, step25]: loss 0.554755
[epoch8, step26]: loss 0.412236
[epoch8, step27]: loss 0.319784
[epoch8, step28]: loss 0.711269
[epoch8, step29]: loss 0.491295
[epoch8, step30]: loss 0.648389
[epoch8, step31]: loss 0.480370
[epoch8, step32]: loss 0.711332
[epoch8, step33]: loss 0.561568
[epoch8, step34]: loss 0.724378
[epoch8, step35]: loss 0.560139
[epoch8, step36]: loss 0.311547
[epoch8, step37]: loss 0.503151
[epoch8, step38]: loss 0.498706
[epoch8, step39]: loss 0.532625
[epoch8, step40]: loss 0.430408
[epoch8, step41]: loss 0.602762
[epoch8, step42]: loss 0.173816
[epoch8, step43]: loss 0.427006
[epoch8, step44]: loss 0.587108
[epoch8, step45]: loss 0.548331
[epoch8, step46]: loss 0.553374
[epoch8, step47]: loss 0.423409
[epoch8, step48]: loss 0.480004
[epoch8, step49]: loss 0.399452
[epoch8, step50]: loss 0.503513
[epoch8, step51]: loss 0.515247
[epoch8, step52]: loss 0.596332
[epoch8, step53]: loss 0.565820
[epoch8, step54]: loss 0.339209
[epoch8, step55]: loss 0.513290
[epoch8, step56]: loss 0.445507
[epoch8, step57]: loss 0.508064
[epoch8, step58]: loss 0.708837
[epoch8, step59]: loss 0.790178
[epoch8, step60]: loss 0.715145
[epoch8, step61]: loss 0.377748
[epoch8, step62]: loss 0.502335
[epoch8, step63]: loss 0.554129
[epoch8, step64]: loss 0.518884
[epoch8, step65]: loss 0.714824
[epoch8, step66]: loss 0.522750
[epoch8, step67]: loss 0.478096
[epoch8, step68]: loss 0.535345
[epoch8, step69]: loss 0.413473
[epoch8, step70]: loss 0.489127
[epoch8, step71]: loss 0.443724
[epoch8, step72]: loss 0.626024
[epoch8, step73]: loss 0.664694
[epoch8, step74]: loss 0.686830
[epoch8, step75]: loss 0.706903
[epoch8, step76]: loss 0.413519
[epoch8, step77]: loss 0.633703
[epoch8, step78]: loss 0.480659
[epoch8, step79]: loss 0.402305
[epoch8, step80]: loss 0.566758
[epoch8, step81]: loss 0.288126
[epoch8, step82]: loss 0.539703
[epoch8, step83]: loss 0.295380
[epoch8, step84]: loss 0.565714
[epoch8, step85]: loss 0.426129
[epoch8, step86]: loss 0.620747
[epoch8, step87]: loss 0.507755
[epoch8, step88]: loss 0.594114
[epoch8, step89]: loss 0.732271
[epoch8, step90]: loss 0.376832
[epoch8, step91]: loss 0.388420
[epoch8, step92]: loss 0.551019
[epoch8, step93]: loss 0.682356
[epoch8, step94]: loss 0.450830
[epoch8, step95]: loss 0.538523
[epoch8, step96]: loss 0.587917
[epoch8, step97]: loss 0.723955
[epoch8, step98]: loss 0.266126
[epoch8, step99]: loss 0.559182
[epoch8, step100]: loss 0.543184
[epoch8, step101]: loss 0.321496
[epoch8, step102]: loss 0.738284
[epoch8, step103]: loss 0.510591
[epoch8, step104]: loss 0.477729
[epoch8, step105]: loss 0.580354
[epoch8, step106]: loss 0.370459
[epoch8, step107]: loss 0.686014
[epoch8, step108]: loss 0.441285
[epoch8, step109]: loss 0.439194
[epoch8, step110]: loss 0.229111
[epoch8, step111]: loss 0.306817
[epoch8, step112]: loss 0.379276
[epoch8, step113]: loss 0.384740
[epoch8, step114]: loss 0.787685
[epoch8, step115]: loss 0.633590
[epoch8, step116]: loss 0.479833
[epoch8, step117]: loss 0.551295
[epoch8, step118]: loss 0.768446
[epoch8, step119]: loss 0.612541
[epoch8, step120]: loss 0.503486
[epoch8, step121]: loss 0.495164
[epoch8, step122]: loss 0.558588
[epoch8, step123]: loss 0.602517
[epoch8, step124]: loss 0.716939
[epoch8, step125]: loss 0.377766
[epoch8, step126]: loss 0.352303
[epoch8, step127]: loss 0.472395
[epoch8, step128]: loss 0.672134
[epoch8, step129]: loss 0.436371
[epoch8, step130]: loss 0.859620
[epoch8, step131]: loss 0.634516
[epoch8, step132]: loss 0.483692
[epoch8, step133]: loss 0.632114
[epoch8, step134]: loss 0.545278
[epoch8, step135]: loss 0.559760
[epoch8, step136]: loss 0.438684
[epoch8, step137]: loss 0.505416
[epoch8, step138]: loss 0.644699
[epoch8, step139]: loss 0.608726
[epoch8, step140]: loss 0.718275
[epoch8, step141]: loss 0.649599
[epoch8, step142]: loss 0.562388
[epoch8, step143]: loss 0.548993
[epoch8, step144]: loss 0.479562
[epoch8, step145]: loss 0.416336
[epoch8, step146]: loss 0.649464
[epoch8, step147]: loss 0.574792
[epoch8, step148]: loss 0.526910
[epoch8, step149]: loss 0.494002
[epoch8, step150]: loss 0.571600
[epoch8, step151]: loss 0.582560
[epoch8, step152]: loss 0.487818
[epoch8, step153]: loss 0.654458
[epoch8, step154]: loss 0.557178
[epoch8, step155]: loss 0.642436
[epoch8, step156]: loss 0.719978
[epoch8, step157]: loss 0.514768
[epoch8, step158]: loss 0.509430
[epoch8, step159]: loss 0.653753
[epoch8, step160]: loss 0.446680
[epoch8, step161]: loss 0.740635
[epoch8, step162]: loss 0.363817
[epoch8, step163]: loss 0.435792
[epoch8, step164]: loss 0.554942
[epoch8, step165]: loss 0.377089
[epoch8, step166]: loss 0.758904
[epoch8, step167]: loss 0.502562
[epoch8, step168]: loss 0.609949
[epoch8, step169]: loss 0.631899
[epoch8, step170]: loss 0.623313
[epoch8, step171]: loss 0.620229
[epoch8, step172]: loss 0.303901
[epoch8, step173]: loss 0.366424
[epoch8, step174]: loss 0.388126
[epoch8, step175]: loss 0.427523
[epoch8, step176]: loss 0.721750
[epoch8, step177]: loss 0.503291
[epoch8, step178]: loss 0.554177
[epoch8, step179]: loss 0.511765
[epoch8, step180]: loss 0.464481
[epoch8, step181]: loss 0.496052
[epoch8, step182]: loss 0.445251
[epoch8, step183]: loss 0.323193
[epoch8, step184]: loss 0.677146
[epoch8, step185]: loss 0.418556
[epoch8, step186]: loss 0.473173
[epoch8, step187]: loss 0.573116
[epoch8, step188]: loss 0.574533
[epoch8, step189]: loss 0.516159
[epoch8, step190]: loss 0.583986
[epoch8, step191]: loss 0.710311
[epoch8, step192]: loss 0.650738
[epoch8, step193]: loss 0.707661
[epoch8, step194]: loss 0.502190
[epoch8, step195]: loss 0.372019
[epoch8, step196]: loss 0.501620
[epoch8, step197]: loss 0.596707
[epoch8, step198]: loss 0.651808
[epoch8, step199]: loss 0.517618
[epoch8, step200]: loss 0.493786
[epoch8, step201]: loss 0.376768
[epoch8, step202]: loss 0.667870
[epoch8, step203]: loss 0.551714
[epoch8, step204]: loss 0.288509
[epoch8, step205]: loss 0.641623
[epoch8, step206]: loss 0.635745
[epoch8, step207]: loss 0.556510
[epoch8, step208]: loss 0.656900
[epoch8, step209]: loss 0.400481
[epoch8, step210]: loss 0.492876
[epoch8, step211]: loss 0.498587
[epoch8, step212]: loss 0.420265
[epoch8, step213]: loss 0.509965
[epoch8, step214]: loss 0.399759
[epoch8, step215]: loss 0.732330
[epoch8, step216]: loss 0.572251
[epoch8, step217]: loss 0.429239
[epoch8, step218]: loss 0.667456
[epoch8, step219]: loss 0.639609
[epoch8, step220]: loss 0.354859
[epoch8, step221]: loss 0.307993
[epoch8, step222]: loss 0.622760
[epoch8, step223]: loss 0.474942
[epoch8, step224]: loss 0.674359
[epoch8, step225]: loss 0.405824
[epoch8, step226]: loss 0.430277
[epoch8, step227]: loss 0.518980
[epoch8, step228]: loss 0.581478
[epoch8, step229]: loss 0.567660
[epoch8, step230]: loss 0.522289
[epoch8, step231]: loss 0.472174
[epoch8, step232]: loss 0.437915
[epoch8, step233]: loss 0.558746
[epoch8, step234]: loss 0.442602
[epoch8, step235]: loss 0.682828
[epoch8, step236]: loss 0.370191
[epoch8, step237]: loss 0.420289
[epoch8, step238]: loss 0.493576
[epoch8, step239]: loss 0.394545
[epoch8, step240]: loss 0.770399
[epoch8, step241]: loss 0.508177
[epoch8, step242]: loss 0.552146
[epoch8, step243]: loss 0.795036
[epoch8, step244]: loss 0.416481
[epoch8, step245]: loss 0.510398
[epoch8, step246]: loss 0.568686
[epoch8, step247]: loss 0.381583
[epoch8, step248]: loss 0.470448
[epoch8, step249]: loss 0.605439
[epoch8, step250]: loss 0.586304
[epoch8, step251]: loss 0.204367
[epoch8, step252]: loss 0.597840
[epoch8, step253]: loss 0.548756
[epoch8, step254]: loss 0.619991
[epoch8, step255]: loss 0.445925
[epoch8, step256]: loss 0.591563
[epoch8, step257]: loss 0.646557
[epoch8, step258]: loss 0.672532
[epoch8, step259]: loss 0.751931
[epoch8, step260]: loss 0.543023
[epoch8, step261]: loss 0.324133
[epoch8, step262]: loss 0.454952
[epoch8, step263]: loss 0.216975
[epoch8, step264]: loss 0.328620
[epoch8, step265]: loss 0.445344
[epoch8, step266]: loss 0.469818
[epoch8, step267]: loss 0.585868
[epoch8, step268]: loss 0.567671
[epoch8, step269]: loss 0.357859
[epoch8, step270]: loss 0.497858
[epoch8, step271]: loss 0.444867
[epoch8, step272]: loss 0.561544
[epoch8, step273]: loss 0.455018
[epoch8, step274]: loss 0.385816
[epoch8, step275]: loss 0.402516
[epoch8, step276]: loss 0.497714
[epoch8, step277]: loss 0.445264
[epoch8, step278]: loss 0.241490
[epoch8, step279]: loss 0.518049
[epoch8, step280]: loss 0.419635
[epoch8, step281]: loss 0.223677
[epoch8, step282]: loss 0.507282
[epoch8, step283]: loss 0.480266
[epoch8, step284]: loss 0.695538
[epoch8, step285]: loss 0.150196
[epoch8, step286]: loss 0.600483
[epoch8, step287]: loss 0.413567
[epoch8, step288]: loss 0.625706
[epoch8, step289]: loss 0.305668
[epoch8, step290]: loss 0.616046
[epoch8, step291]: loss 0.408364
[epoch8, step292]: loss 0.559773
[epoch8, step293]: loss 0.646833
[epoch8, step294]: loss 0.596763
[epoch8, step295]: loss 0.128201
[epoch8, step296]: loss 0.494404
[epoch8, step297]: loss 0.491279
[epoch8, step298]: loss 0.647783
[epoch8, step299]: loss 0.678459
[epoch8, step300]: loss 0.579476
[epoch8, step301]: loss 0.590457
[epoch8, step302]: loss 0.310206
[epoch8, step303]: loss 0.711311
[epoch8, step304]: loss 0.538508
[epoch8, step305]: loss 0.709926
[epoch8, step306]: loss 0.666314
[epoch8, step307]: loss 0.506652
[epoch8, step308]: loss 0.557894
[epoch8, step309]: loss 0.414288
[epoch8, step310]: loss 0.734365
[epoch8, step311]: loss 0.490651
[epoch8, step312]: loss 0.698202
[epoch8, step313]: loss 0.596762
[epoch8, step314]: loss 0.598256
[epoch8, step315]: loss 0.331442
[epoch8, step316]: loss 0.689543
[epoch8, step317]: loss 0.529759
[epoch8, step318]: loss 0.443598
[epoch8, step319]: loss 0.318980
[epoch8, step320]: loss 0.611092
[epoch8, step321]: loss 0.498616
[epoch8, step322]: loss 0.514991
[epoch8, step323]: loss 0.536546
[epoch8, step324]: loss 0.495166
[epoch8, step325]: loss 0.441204
[epoch8, step326]: loss 0.723774
[epoch8, step327]: loss 0.463272
[epoch8, step328]: loss 0.354551
[epoch8, step329]: loss 0.263735
[epoch8, step330]: loss 0.297967
[epoch8, step331]: loss 0.527223
[epoch8, step332]: loss 0.731246
[epoch8, step333]: loss 0.516742
[epoch8, step334]: loss 0.678367
[epoch8, step335]: loss 0.649027
[epoch8, step336]: loss 0.518035
[epoch8, step337]: loss 0.263888
[epoch8, step338]: loss 0.517881
[epoch8, step339]: loss 0.497716
[epoch8, step340]: loss 0.716103
[epoch8, step341]: loss 0.597027
[epoch8, step342]: loss 0.309907
[epoch8, step343]: loss 0.534797
[epoch8, step344]: loss 0.420638
[epoch8, step345]: loss 0.647725
[epoch8, step346]: loss 0.322353
[epoch8, step347]: loss 0.361738
[epoch8, step348]: loss 0.541609
[epoch8, step349]: loss 0.397236
[epoch8, step350]: loss 0.598576
[epoch8, step351]: loss 0.521506
[epoch8, step352]: loss 0.510844
[epoch8, step353]: loss 0.553003
[epoch8, step354]: loss 0.465521
[epoch8, step355]: loss 0.522708
[epoch8, step356]: loss 0.441132
[epoch8, step357]: loss 0.228037
[epoch8, step358]: loss 0.604346
[epoch8, step359]: loss 0.675014
[epoch8, step360]: loss 0.810451
[epoch8, step361]: loss 0.273011
[epoch8, step362]: loss 0.610785
[epoch8, step363]: loss 0.441558
[epoch8, step364]: loss 0.527989
[epoch8, step365]: loss 0.330896
[epoch8, step366]: loss 0.518897
[epoch8, step367]: loss 0.695461
[epoch8, step368]: loss 0.589304
[epoch8, step369]: loss 0.245941
[epoch8, step370]: loss 0.392954
[epoch8, step371]: loss 0.507065
[epoch8, step372]: loss 0.499660
[epoch8, step373]: loss 0.690199
[epoch8, step374]: loss 0.511366
[epoch8, step375]: loss 0.524544
[epoch8, step376]: loss 0.521700
[epoch8, step377]: loss 0.566223
[epoch8, step378]: loss 0.282420
[epoch8, step379]: loss 0.394335
[epoch8, step380]: loss 0.441640
[epoch8, step381]: loss 0.502718
[epoch8, step382]: loss 0.599375
[epoch8, step383]: loss 0.876724
[epoch8, step384]: loss 0.534195
[epoch8, step385]: loss 0.797330
[epoch8, step386]: loss 0.612453
[epoch8, step387]: loss 0.735049
[epoch8, step388]: loss 0.612589
[epoch8, step389]: loss 0.326979
[epoch8, step390]: loss 0.464491
[epoch8, step391]: loss 0.400957
[epoch8, step392]: loss 0.712749
[epoch8, step393]: loss 0.625930
[epoch8, step394]: loss 0.472824
[epoch8, step395]: loss 0.700652
[epoch8, step396]: loss 0.732765
[epoch8, step397]: loss 0.582395
[epoch8, step398]: loss 0.451920
[epoch8, step399]: loss 0.412203
[epoch8, step400]: loss 0.138696
[epoch8, step401]: loss 0.195735
[epoch8, step402]: loss 0.586942
[epoch8, step403]: loss 0.715362
[epoch8, step404]: loss 0.532149
[epoch8, step405]: loss 0.503626
[epoch8, step406]: loss 0.600705
[epoch8, step407]: loss 0.425635
[epoch8, step408]: loss 0.483402
[epoch8, step409]: loss 0.556450
[epoch8, step410]: loss 0.468305
[epoch8, step411]: loss 0.308498
[epoch8, step412]: loss 0.789608
[epoch8, step413]: loss 0.398573
[epoch8, step414]: loss 0.491088
[epoch8, step415]: loss 0.530199
[epoch8, step416]: loss 0.606872
[epoch8, step417]: loss 0.677403
[epoch8, step418]: loss 0.309031
[epoch8, step419]: loss 0.450186
[epoch8, step420]: loss 0.497722
[epoch8, step421]: loss 0.661557
[epoch8, step422]: loss 0.576026
[epoch8, step423]: loss 0.617935
[epoch8, step424]: loss 0.513544
[epoch8, step425]: loss 0.290887
[epoch8, step426]: loss 0.573410
[epoch8, step427]: loss 0.536649
[epoch8, step428]: loss 0.444106
[epoch8, step429]: loss 0.516595
[epoch8, step430]: loss 0.580763
[epoch8, step431]: loss 0.597565
[epoch8, step432]: loss 0.415889
[epoch8, step433]: loss 0.601094
[epoch8, step434]: loss 0.686719
[epoch8, step435]: loss 0.540395
[epoch8, step436]: loss 0.456428
[epoch8, step437]: loss 0.610077
[epoch8, step438]: loss 0.375917
[epoch8, step439]: loss 0.313629
[epoch8, step440]: loss 0.672382
[epoch8, step441]: loss 0.651359
[epoch8, step442]: loss 0.260190
[epoch8, step443]: loss 0.431905
[epoch8, step444]: loss 0.568548
[epoch8, step445]: loss 0.581355
[epoch8, step446]: loss 0.513750
[epoch8, step447]: loss 0.619668
[epoch8, step448]: loss 0.419514
[epoch8, step449]: loss 0.549093
[epoch8, step450]: loss 0.511385
[epoch8, step451]: loss 0.741430
[epoch8, step452]: loss 0.535700
[epoch8, step453]: loss 0.590605
[epoch8, step454]: loss 0.634452
[epoch8, step455]: loss 0.659633
[epoch8, step456]: loss 0.653962
[epoch8, step457]: loss 0.537525
[epoch8, step458]: loss 0.476050
[epoch8, step459]: loss 0.595180
[epoch8, step460]: loss 0.534910
[epoch8, step461]: loss 0.743751
[epoch8, step462]: loss 0.858420
[epoch8, step463]: loss 0.572110
[epoch8, step464]: loss 0.703101
[epoch8, step465]: loss 0.442382
[epoch8, step466]: loss 0.341367
[epoch8, step467]: loss 0.496474
[epoch8, step468]: loss 0.363693
[epoch8, step469]: loss 0.582733
[epoch8, step470]: loss 0.378092
[epoch8, step471]: loss 0.525157
[epoch8, step472]: loss 0.538197
[epoch8, step473]: loss 0.613053
[epoch8, step474]: loss 0.375780
[epoch8, step475]: loss 0.544419
[epoch8, step476]: loss 0.763325
[epoch8, step477]: loss 0.602739
[epoch8, step478]: loss 0.572102
[epoch8, step479]: loss 0.409947
[epoch8, step480]: loss 0.457655
[epoch8, step481]: loss 0.318244
[epoch8, step482]: loss 0.587569
[epoch8, step483]: loss 0.334919
[epoch8, step484]: loss 0.474259
[epoch8, step485]: loss 0.520578
[epoch8, step486]: loss 0.530233
[epoch8, step487]: loss 0.353803
[epoch8, step488]: loss 0.513338
[epoch8, step489]: loss 0.475192
[epoch8, step490]: loss 0.274755
[epoch8, step491]: loss 0.712156
[epoch8, step492]: loss 0.475455
[epoch8, step493]: loss 0.619177
[epoch8, step494]: loss 0.640314
[epoch8, step495]: loss 0.648118
[epoch8, step496]: loss 0.378050
[epoch8, step497]: loss 0.482785
[epoch8, step498]: loss 0.463499
[epoch8, step499]: loss 0.499791
[epoch8, step500]: loss 0.384354
[epoch8, step501]: loss 0.509150
[epoch8, step502]: loss 0.421855
[epoch8, step503]: loss 0.606721
[epoch8, step504]: loss 0.497879
[epoch8, step505]: loss 0.581366
[epoch8, step506]: loss 0.652191
[epoch8, step507]: loss 0.602112
[epoch8, step508]: loss 0.388433
[epoch8, step509]: loss 0.603623
[epoch8, step510]: loss 0.768310
[epoch8, step511]: loss 0.357893
[epoch8, step512]: loss 0.357233
[epoch8, step513]: loss 0.738096
[epoch8, step514]: loss 0.518171
[epoch8, step515]: loss 0.532728
[epoch8, step516]: loss 0.360566
[epoch8, step517]: loss 0.344604
[epoch8, step518]: loss 0.222359
[epoch8, step519]: loss 0.739682
[epoch8, step520]: loss 0.699875
[epoch8, step521]: loss 0.662641
[epoch8, step522]: loss 0.510163
[epoch8, step523]: loss 0.525189
[epoch8, step524]: loss 0.532834
[epoch8, step525]: loss 0.690843
[epoch8, step526]: loss 0.714494
[epoch8, step527]: loss 0.745975
[epoch8, step528]: loss 0.626899
[epoch8, step529]: loss 0.592957
[epoch8, step530]: loss 0.672941
[epoch8, step531]: loss 0.468956
[epoch8, step532]: loss 0.613978
[epoch8, step533]: loss 0.789641
[epoch8, step534]: loss 0.409373
[epoch8, step535]: loss 0.480045
[epoch8, step536]: loss 0.464172
[epoch8, step537]: loss 0.466598
[epoch8, step538]: loss 0.340835
[epoch8, step539]: loss 0.499822
[epoch8, step540]: loss 0.263165
[epoch8, step541]: loss 0.281138
[epoch8, step542]: loss 0.553960
[epoch8, step543]: loss 0.512308
[epoch8, step544]: loss 0.458727
[epoch8, step545]: loss 0.373701
[epoch8, step546]: loss 0.436763
[epoch8, step547]: loss 0.590405
[epoch8, step548]: loss 0.696559
[epoch8, step549]: loss 0.416837
[epoch8, step550]: loss 0.611267
[epoch8, step551]: loss 0.467420
[epoch8, step552]: loss 0.312711
[epoch8, step553]: loss 0.660048
[epoch8, step554]: loss 0.468280
[epoch8, step555]: loss 0.572697
[epoch8, step556]: loss 0.753371
[epoch8, step557]: loss 0.376396
[epoch8, step558]: loss 0.556550
[epoch8, step559]: loss 0.460777
[epoch8, step560]: loss 0.498028
[epoch8, step561]: loss 0.270769
[epoch8, step562]: loss 0.539081
[epoch8, step563]: loss 0.692729
[epoch8, step564]: loss 0.337775
[epoch8, step565]: loss 0.477508
[epoch8, step566]: loss 0.230836
[epoch8, step567]: loss 0.389662
[epoch8, step568]: loss 0.364098
[epoch8, step569]: loss 0.463088
[epoch8, step570]: loss 0.448851
[epoch8, step571]: loss 0.411493
[epoch8, step572]: loss 0.606969
[epoch8, step573]: loss 0.575405
[epoch8, step574]: loss 0.581479
[epoch8, step575]: loss 0.658603
[epoch8, step576]: loss 0.547026
[epoch8, step577]: loss 0.364252
[epoch8, step578]: loss 0.250201
[epoch8, step579]: loss 0.598338
[epoch8, step580]: loss 0.730829
[epoch8, step581]: loss 0.606094
[epoch8, step582]: loss 0.370622
[epoch8, step583]: loss 0.766261
[epoch8, step584]: loss 0.607062
[epoch8, step585]: loss 0.429596
[epoch8, step586]: loss 0.409077
[epoch8, step587]: loss 0.671878
[epoch8, step588]: loss 0.701853
[epoch8, step589]: loss 0.449451
[epoch8, step590]: loss 0.571114
[epoch8, step591]: loss 0.594078
[epoch8, step592]: loss 0.509978
[epoch8, step593]: loss 0.305535
[epoch8, step594]: loss 0.421830
[epoch8, step595]: loss 0.558754
[epoch8, step596]: loss 0.283750
[epoch8, step597]: loss 0.628537
[epoch8, step598]: loss 0.565240
[epoch8, step599]: loss 0.692325
[epoch8, step600]: loss 0.746169
[epoch8, step601]: loss 0.558373
[epoch8, step602]: loss 0.289672
[epoch8, step603]: loss 0.365508
[epoch8, step604]: loss 0.437789
[epoch8, step605]: loss 0.445909
[epoch8, step606]: loss 0.491291
[epoch8, step607]: loss 0.470153
[epoch8, step608]: loss 0.571818
[epoch8, step609]: loss 0.683134
[epoch8, step610]: loss 0.452907
[epoch8, step611]: loss 0.132501
[epoch8, step612]: loss 0.670389
[epoch8, step613]: loss 0.108667
[epoch8, step614]: loss 0.750233
[epoch8, step615]: loss 0.354103
[epoch8, step616]: loss 0.348799
[epoch8, step617]: loss 0.546185
[epoch8, step618]: loss 0.432393
[epoch8, step619]: loss 0.511957
[epoch8, step620]: loss 0.452401
[epoch8, step621]: loss 0.312155
[epoch8, step622]: loss 0.602083
[epoch8, step623]: loss 0.436014
[epoch8, step624]: loss 0.534094
[epoch8, step625]: loss 0.529669
[epoch8, step626]: loss 0.393280
[epoch8, step627]: loss 0.453482
[epoch8, step628]: loss 0.509688
[epoch8, step629]: loss 0.296925
[epoch8, step630]: loss 0.516795
[epoch8, step631]: loss 0.792428
[epoch8, step632]: loss 0.523276
[epoch8, step633]: loss 0.510562
[epoch8, step634]: loss 0.325145
[epoch8, step635]: loss 0.664414
[epoch8, step636]: loss 0.247180
[epoch8, step637]: loss 0.445893
[epoch8, step638]: loss 0.664838
[epoch8, step639]: loss 0.727991
[epoch8, step640]: loss 0.462432
[epoch8, step641]: loss 0.393791
[epoch8, step642]: loss 0.569479
[epoch8, step643]: loss 0.554521
[epoch8, step644]: loss 0.445751
[epoch8, step645]: loss 0.548374
[epoch8, step646]: loss 0.708075
[epoch8, step647]: loss 0.624942
[epoch8, step648]: loss 0.390006
[epoch8, step649]: loss 0.515819
[epoch8, step650]: loss 0.503540
[epoch8, step651]: loss 0.631858
[epoch8, step652]: loss 0.554706
[epoch8, step653]: loss 0.414869
[epoch8, step654]: loss 0.622896
[epoch8, step655]: loss 0.308161
[epoch8, step656]: loss 0.306496
[epoch8, step657]: loss 0.551425
[epoch8, step658]: loss 0.678356
[epoch8, step659]: loss 0.542925
[epoch8, step660]: loss 0.693606
[epoch8, step661]: loss 0.509897
[epoch8, step662]: loss 0.438167
[epoch8, step663]: loss 0.484949
[epoch8, step664]: loss 0.385153
[epoch8, step665]: loss 0.680911
[epoch8, step666]: loss 0.112416
[epoch8, step667]: loss 0.376422
[epoch8, step668]: loss 0.543479
[epoch8, step669]: loss 0.474519
[epoch8, step670]: loss 0.587254
[epoch8, step671]: loss 0.421818
[epoch8, step672]: loss 0.368244
[epoch8, step673]: loss 0.448736
[epoch8, step674]: loss 0.403101
[epoch8, step675]: loss 0.766628
[epoch8, step676]: loss 0.192161
[epoch8, step677]: loss 0.447181
[epoch8, step678]: loss 0.645380
[epoch8, step679]: loss 0.271084
[epoch8, step680]: loss 0.322630
[epoch8, step681]: loss 0.509073
[epoch8, step682]: loss 0.422997
[epoch8, step683]: loss 0.244812
[epoch8, step684]: loss 0.288659
[epoch8, step685]: loss 0.563461
[epoch8, step686]: loss 0.624062
[epoch8, step687]: loss 0.631755
[epoch8, step688]: loss 0.260485
[epoch8, step689]: loss 0.542450
[epoch8, step690]: loss 0.438669
[epoch8, step691]: loss 0.384271
[epoch8, step692]: loss 0.389990
[epoch8, step693]: loss 0.538955
[epoch8, step694]: loss 0.446158
[epoch8, step695]: loss 0.355669
[epoch8, step696]: loss 0.532154
[epoch8, step697]: loss 0.549257
[epoch8, step698]: loss 0.507319
[epoch8, step699]: loss 0.602453
[epoch8, step700]: loss 0.684138
[epoch8, step701]: loss 0.706046
[epoch8, step702]: loss 0.403075
[epoch8, step703]: loss 0.500916
[epoch8, step704]: loss 0.332427
[epoch8, step705]: loss 0.459200
[epoch8, step706]: loss 0.646016
[epoch8, step707]: loss 0.611880
[epoch8, step708]: loss 0.469200
[epoch8, step709]: loss 0.426319
[epoch8, step710]: loss 0.490291
[epoch8, step711]: loss 0.629135
[epoch8, step712]: loss 0.521991
[epoch8, step713]: loss 0.601170
[epoch8, step714]: loss 0.393053
[epoch8, step715]: loss 0.167586
[epoch8, step716]: loss 0.532522
[epoch8, step717]: loss 0.423419
[epoch8, step718]: loss 0.800547
[epoch8, step719]: loss 0.506799
[epoch8, step720]: loss 0.482777
[epoch8, step721]: loss 0.777614
[epoch8, step722]: loss 0.530441
[epoch8, step723]: loss 0.585199
[epoch8, step724]: loss 0.334248
[epoch8, step725]: loss 0.148229
[epoch8, step726]: loss 0.270425
[epoch8, step727]: loss 0.416846
[epoch8, step728]: loss 0.534537
[epoch8, step729]: loss 0.683785
[epoch8, step730]: loss 0.385609
[epoch8, step731]: loss 0.697231
[epoch8, step732]: loss 0.288629
[epoch8, step733]: loss 0.301391
[epoch8, step734]: loss 0.340702
[epoch8, step735]: loss 0.635551
[epoch8, step736]: loss 0.281503
[epoch8, step737]: loss 0.594053
[epoch8, step738]: loss 0.573168
[epoch8, step739]: loss 0.519990
[epoch8, step740]: loss 0.207202
[epoch8, step741]: loss 0.618293
[epoch8, step742]: loss 0.341091
[epoch8, step743]: loss 0.649111
[epoch8, step744]: loss 0.735803
[epoch8, step745]: loss 0.490237
[epoch8, step746]: loss 0.578761
[epoch8, step747]: loss 0.474013
[epoch8, step748]: loss 0.546574
[epoch8, step749]: loss 0.562003
[epoch8, step750]: loss 0.435951
[epoch8, step751]: loss 0.668202
[epoch8, step752]: loss 0.585523
[epoch8, step753]: loss 0.701914
[epoch8, step754]: loss 0.518889
[epoch8, step755]: loss 0.295593
[epoch8, step756]: loss 0.525385
[epoch8, step757]: loss 0.373486
[epoch8, step758]: loss 0.616877
[epoch8, step759]: loss 0.601615
[epoch8, step760]: loss 0.656553
[epoch8, step761]: loss 0.404994
[epoch8, step762]: loss 0.494272
[epoch8, step763]: loss 0.469700
[epoch8, step764]: loss 0.502916
[epoch8, step765]: loss 0.562779
[epoch8, step766]: loss 0.457661
[epoch8, step767]: loss 0.540699
[epoch8, step768]: loss 0.231287
[epoch8, step769]: loss 0.514958
[epoch8, step770]: loss 0.610626
[epoch8, step771]: loss 0.748272
[epoch8, step772]: loss 0.508231
[epoch8, step773]: loss 0.768864
[epoch8, step774]: loss 0.621949
[epoch8, step775]: loss 0.454287
[epoch8, step776]: loss 0.341177
[epoch8, step777]: loss 0.482963
[epoch8, step778]: loss 0.720660
[epoch8, step779]: loss 0.543979
[epoch8, step780]: loss 0.615911
[epoch8, step781]: loss 0.132645
[epoch8, step782]: loss 0.491489
[epoch8, step783]: loss 0.576990
[epoch8, step784]: loss 0.578503
[epoch8, step785]: loss 0.280947
[epoch8, step786]: loss 0.539716
[epoch8, step787]: loss 0.530399
[epoch8, step788]: loss 0.656906
[epoch8, step789]: loss 0.395384
[epoch8, step790]: loss 0.702019
[epoch8, step791]: loss 0.144724
[epoch8, step792]: loss 0.744496
[epoch8, step793]: loss 0.683899
[epoch8, step794]: loss 0.592303
[epoch8, step795]: loss 0.414883
[epoch8, step796]: loss 0.663592
[epoch8, step797]: loss 0.476050
[epoch8, step798]: loss 0.322116
[epoch8, step799]: loss 0.733468
[epoch8, step800]: loss 0.748720
[epoch8, step801]: loss 0.563871
[epoch8, step802]: loss 0.462976
[epoch8, step803]: loss 0.518803
[epoch8, step804]: loss 0.535984
[epoch8, step805]: loss 0.499198
[epoch8, step806]: loss 0.607371
[epoch8, step807]: loss 0.624837
[epoch8, step808]: loss 0.194015
[epoch8, step809]: loss 0.696197
[epoch8, step810]: loss 0.415538
[epoch8, step811]: loss 0.721527
[epoch8, step812]: loss 0.580393
[epoch8, step813]: loss 0.397515
[epoch8, step814]: loss 0.457171
[epoch8, step815]: loss 0.561843
[epoch8, step816]: loss 0.535892
[epoch8, step817]: loss 0.386548
[epoch8, step818]: loss 0.666162
[epoch8, step819]: loss 0.130982
[epoch8, step820]: loss 0.672242
[epoch8, step821]: loss 0.420947
[epoch8, step822]: loss 0.403478
[epoch8, step823]: loss 0.581968
[epoch8, step824]: loss 0.538790
[epoch8, step825]: loss 0.443356
[epoch8, step826]: loss 0.623139
[epoch8, step827]: loss 0.417748
[epoch8, step828]: loss 0.417040
[epoch8, step829]: loss 0.639188
[epoch8, step830]: loss 0.588924
[epoch8, step831]: loss 0.305546
[epoch8, step832]: loss 0.663452
[epoch8, step833]: loss 0.655472
[epoch8, step834]: loss 0.678759
[epoch8, step835]: loss 0.526561
[epoch8, step836]: loss 0.467378
[epoch8, step837]: loss 0.576586
[epoch8, step838]: loss 0.774259
[epoch8, step839]: loss 0.524352
[epoch8, step840]: loss 0.525484
[epoch8, step841]: loss 0.530523
[epoch8, step842]: loss 0.268416
[epoch8, step843]: loss 0.460104
[epoch8, step844]: loss 0.621460
[epoch8, step845]: loss 0.574417
[epoch8, step846]: loss 0.303861
[epoch8, step847]: loss 0.246086
[epoch8, step848]: loss 0.472110
[epoch8, step849]: loss 0.390707
[epoch8, step850]: loss 0.390015
[epoch8, step851]: loss 0.585340
[epoch8, step852]: loss 0.307521
[epoch8, step853]: loss 0.520062
[epoch8, step854]: loss 0.545495
[epoch8, step855]: loss 0.671790
[epoch8, step856]: loss 0.539197
[epoch8, step857]: loss 0.418946
[epoch8, step858]: loss 0.431938
[epoch8, step859]: loss 0.481498
[epoch8, step860]: loss 0.506017
[epoch8, step861]: loss 0.519395
[epoch8, step862]: loss 0.447148
[epoch8, step863]: loss 0.657409
[epoch8, step864]: loss 0.510471
[epoch8, step865]: loss 0.667543
[epoch8, step866]: loss 0.571391
[epoch8, step867]: loss 0.435452
[epoch8, step868]: loss 0.672559
[epoch8, step869]: loss 0.589621
[epoch8, step870]: loss 0.270815
[epoch8, step871]: loss 0.510447
[epoch8, step872]: loss 0.621982
[epoch8, step873]: loss 0.623648
[epoch8, step874]: loss 0.350187
[epoch8, step875]: loss 0.597954
[epoch8, step876]: loss 0.623418
[epoch8, step877]: loss 0.566473
[epoch8, step878]: loss 0.467194
[epoch8, step879]: loss 0.508919
[epoch8, step880]: loss 0.555690
[epoch8, step881]: loss 0.590028
[epoch8, step882]: loss 0.390902
[epoch8, step883]: loss 0.635445
[epoch8, step884]: loss 0.736357
[epoch8, step885]: loss 0.689954
[epoch8, step886]: loss 0.368146
[epoch8, step887]: loss 0.631814
[epoch8, step888]: loss 0.673820
[epoch8, step889]: loss 0.273203
[epoch8, step890]: loss 0.208113
[epoch8, step891]: loss 0.501897
[epoch8, step892]: loss 0.505190
[epoch8, step893]: loss 0.480605
[epoch8, step894]: loss 0.690605
[epoch8, step895]: loss 0.374912
[epoch8, step896]: loss 0.427653
[epoch8, step897]: loss 0.455050
[epoch8, step898]: loss 0.365047
[epoch8, step899]: loss 0.601776
[epoch8, step900]: loss 0.529945
[epoch8, step901]: loss 0.297512
[epoch8, step902]: loss 0.621435
[epoch8, step903]: loss 0.626843
[epoch8, step904]: loss 0.236561
[epoch8, step905]: loss 0.486858
[epoch8, step906]: loss 0.610881
[epoch8, step907]: loss 0.567926
[epoch8, step908]: loss 0.537669
[epoch8, step909]: loss 0.217740
[epoch8, step910]: loss 0.701059
[epoch8, step911]: loss 0.502719
[epoch8, step912]: loss 0.596649
[epoch8, step913]: loss 0.524498
[epoch8, step914]: loss 0.508644
[epoch8, step915]: loss 0.626990
[epoch8, step916]: loss 0.283373
[epoch8, step917]: loss 0.638427
[epoch8, step918]: loss 0.311444
[epoch8, step919]: loss 0.723813
[epoch8, step920]: loss 0.390007
[epoch8, step921]: loss 0.468157
[epoch8, step922]: loss 0.531422
[epoch8, step923]: loss 0.322494
[epoch8, step924]: loss 0.510494
[epoch8, step925]: loss 0.664292
[epoch8, step926]: loss 0.459642
[epoch8, step927]: loss 0.525495
[epoch8, step928]: loss 0.521363
[epoch8, step929]: loss 0.692616
[epoch8, step930]: loss 0.460307
[epoch8, step931]: loss 0.655726
[epoch8, step932]: loss 0.396266
[epoch8, step933]: loss 0.423467
[epoch8, step934]: loss 0.768001
[epoch8, step935]: loss 0.759585
[epoch8, step936]: loss 0.166350
[epoch8, step937]: loss 0.636108
[epoch8, step938]: loss 0.583558
[epoch8, step939]: loss 0.501740
[epoch8, step940]: loss 0.585758
[epoch8, step941]: loss 0.630865
[epoch8, step942]: loss 0.449988
[epoch8, step943]: loss 0.554448
[epoch8, step944]: loss 0.447108
[epoch8, step945]: loss 0.578727
[epoch8, step946]: loss 0.359700
[epoch8, step947]: loss 0.501229
[epoch8, step948]: loss 0.540425
[epoch8, step949]: loss 0.500202
[epoch8, step950]: loss 0.753289
[epoch8, step951]: loss 0.432844
[epoch8, step952]: loss 0.409726
[epoch8, step953]: loss 0.621210
[epoch8, step954]: loss 0.582231
[epoch8, step955]: loss 0.530306
[epoch8, step956]: loss 0.726122
[epoch8, step957]: loss 0.670057
[epoch8, step958]: loss 0.530690
[epoch8, step959]: loss 0.474280
[epoch8, step960]: loss 0.609498
[epoch8, step961]: loss 0.660708
[epoch8, step962]: loss 0.537799
[epoch8, step963]: loss 0.378800
[epoch8, step964]: loss 0.583463
[epoch8, step965]: loss 0.688285
[epoch8, step966]: loss 0.519289
[epoch8, step967]: loss 0.272685
[epoch8, step968]: loss 0.649010
[epoch8, step969]: loss 0.551024
[epoch8, step970]: loss 0.430495
[epoch8, step971]: loss 0.189764
[epoch8, step972]: loss 0.471192
[epoch8, step973]: loss 0.403884
[epoch8, step974]: loss 0.386947
[epoch8, step975]: loss 0.386764
[epoch8, step976]: loss 0.673337
[epoch8, step977]: loss 0.580707
[epoch8, step978]: loss 0.431588
[epoch8, step979]: loss 0.561460
[epoch8, step980]: loss 0.656671
[epoch8, step981]: loss 0.580209
[epoch8, step982]: loss 0.572216
[epoch8, step983]: loss 0.401395
[epoch8, step984]: loss 0.370571
[epoch8, step985]: loss 0.201728
[epoch8, step986]: loss 0.611338
[epoch8, step987]: loss 0.525651
[epoch8, step988]: loss 0.549355
[epoch8, step989]: loss 0.450244
[epoch8, step990]: loss 0.474384
[epoch8, step991]: loss 0.759250
[epoch8, step992]: loss 0.309952
[epoch8, step993]: loss 0.397972
[epoch8, step994]: loss 0.445398
[epoch8, step995]: loss 0.497868
[epoch8, step996]: loss 0.653950
[epoch8, step997]: loss 0.509818
[epoch8, step998]: loss 0.592603
[epoch8, step999]: loss 0.396459
[epoch8, step1000]: loss 0.672209
[epoch8, step1001]: loss 0.342171
[epoch8, step1002]: loss 0.435866
[epoch8, step1003]: loss 0.529274
[epoch8, step1004]: loss 0.466400
[epoch8, step1005]: loss 0.591270
[epoch8, step1006]: loss 0.546168
[epoch8, step1007]: loss 0.604708
[epoch8, step1008]: loss 0.583767
[epoch8, step1009]: loss 0.554797
[epoch8, step1010]: loss 0.542994
[epoch8, step1011]: loss 0.527847
[epoch8, step1012]: loss 0.214690
[epoch8, step1013]: loss 0.380544
[epoch8, step1014]: loss 0.437231
[epoch8, step1015]: loss 0.670208
[epoch8, step1016]: loss 0.408326
[epoch8, step1017]: loss 0.671636
[epoch8, step1018]: loss 0.664129
[epoch8, step1019]: loss 0.333002
[epoch8, step1020]: loss 0.644473
[epoch8, step1021]: loss 0.605769
[epoch8, step1022]: loss 0.738642
[epoch8, step1023]: loss 0.544993
[epoch8, step1024]: loss 0.619662
[epoch8, step1025]: loss 0.470547
[epoch8, step1026]: loss 0.502952
[epoch8, step1027]: loss 0.557405
[epoch8, step1028]: loss 0.268921
[epoch8, step1029]: loss 0.645076
[epoch8, step1030]: loss 0.453963
[epoch8, step1031]: loss 0.649824
[epoch8, step1032]: loss 0.411782
[epoch8, step1033]: loss 0.813620
[epoch8, step1034]: loss 0.122355
[epoch8, step1035]: loss 0.284550
[epoch8, step1036]: loss 0.625466
[epoch8, step1037]: loss 0.542169
[epoch8, step1038]: loss 0.337394
[epoch8, step1039]: loss 0.675671
[epoch8, step1040]: loss 0.446054
[epoch8, step1041]: loss 0.763042
[epoch8, step1042]: loss 0.293378
[epoch8, step1043]: loss 0.565665
[epoch8, step1044]: loss 0.548702
[epoch8, step1045]: loss 0.464948
[epoch8, step1046]: loss 0.496124
[epoch8, step1047]: loss 0.757920
[epoch8, step1048]: loss 0.621563
[epoch8, step1049]: loss 0.646051
[epoch8, step1050]: loss 0.506104
[epoch8, step1051]: loss 0.729746
[epoch8, step1052]: loss 0.470488
[epoch8, step1053]: loss 0.550361
[epoch8, step1054]: loss 0.333768
[epoch8, step1055]: loss 0.620572
[epoch8, step1056]: loss 0.713256
[epoch8, step1057]: loss 0.644287
[epoch8, step1058]: loss 0.523189
[epoch8, step1059]: loss 0.534030
[epoch8, step1060]: loss 0.539644
[epoch8, step1061]: loss 0.539476
[epoch8, step1062]: loss 0.531081
[epoch8, step1063]: loss 0.292104
[epoch8, step1064]: loss 0.634052
[epoch8, step1065]: loss 0.492513
[epoch8, step1066]: loss 0.456357
[epoch8, step1067]: loss 0.691227
[epoch8, step1068]: loss 0.593495
[epoch8, step1069]: loss 0.609515
[epoch8, step1070]: loss 0.392320
[epoch8, step1071]: loss 0.147035
[epoch8, step1072]: loss 0.447986
[epoch8, step1073]: loss 0.274508
[epoch8, step1074]: loss 0.267674
[epoch8, step1075]: loss 0.181684
[epoch8, step1076]: loss 0.580812
[epoch8, step1077]: loss 0.683590
[epoch8, step1078]: loss 0.675383
[epoch8, step1079]: loss 0.610659
[epoch8, step1080]: loss 0.385275
[epoch8, step1081]: loss 0.622430
[epoch8, step1082]: loss 0.618593
[epoch8, step1083]: loss 0.545954
[epoch8, step1084]: loss 0.662175
[epoch8, step1085]: loss 0.418281
[epoch8, step1086]: loss 0.423652
[epoch8, step1087]: loss 0.245531
[epoch8, step1088]: loss 0.638358
[epoch8, step1089]: loss 0.769585
[epoch8, step1090]: loss 0.647451
[epoch8, step1091]: loss 0.810584
[epoch8, step1092]: loss 0.698504
[epoch8, step1093]: loss 0.762421
[epoch8, step1094]: loss 0.400142
[epoch8, step1095]: loss 0.850480
[epoch8, step1096]: loss 0.430743
[epoch8, step1097]: loss 0.429793
[epoch8, step1098]: loss 0.182140
[epoch8, step1099]: loss 0.399020
[epoch8, step1100]: loss 0.354559
[epoch8, step1101]: loss 0.658841
[epoch8, step1102]: loss 0.613908
[epoch8, step1103]: loss 0.689625
[epoch8, step1104]: loss 0.453927
[epoch8, step1105]: loss 0.634170
[epoch8, step1106]: loss 0.211881
[epoch8, step1107]: loss 0.551783
[epoch8, step1108]: loss 0.418973
[epoch8, step1109]: loss 0.442292
[epoch8, step1110]: loss 0.433514
[epoch8, step1111]: loss 0.387631
[epoch8, step1112]: loss 0.294522
[epoch8, step1113]: loss 0.320154
[epoch8, step1114]: loss 0.137278
[epoch8, step1115]: loss 0.338260
[epoch8, step1116]: loss 0.802034
[epoch8, step1117]: loss 0.507094
[epoch8, step1118]: loss 0.647868
[epoch8, step1119]: loss 0.409050
[epoch8, step1120]: loss 0.556291
[epoch8, step1121]: loss 0.604682
[epoch8, step1122]: loss 0.498390
[epoch8, step1123]: loss 0.481284
[epoch8, step1124]: loss 0.497064
[epoch8, step1125]: loss 0.508737
[epoch8, step1126]: loss 0.758072
[epoch8, step1127]: loss 0.572292
[epoch8, step1128]: loss 0.667343
[epoch8, step1129]: loss 0.529848
[epoch8, step1130]: loss 0.307354
[epoch8, step1131]: loss 0.604971
[epoch8, step1132]: loss 0.337258
[epoch8, step1133]: loss 0.482995
[epoch8, step1134]: loss 0.364114
[epoch8, step1135]: loss 0.662078
[epoch8, step1136]: loss 0.560250
[epoch8, step1137]: loss 0.308034
[epoch8, step1138]: loss 0.486634
[epoch8, step1139]: loss 0.334953
[epoch8, step1140]: loss 0.695942
[epoch8, step1141]: loss 0.472976
[epoch8, step1142]: loss 0.643524
[epoch8, step1143]: loss 0.391793
[epoch8, step1144]: loss 0.582746
[epoch8, step1145]: loss 0.501390
[epoch8, step1146]: loss 0.557136
[epoch8, step1147]: loss 0.247421
[epoch8, step1148]: loss 0.408271
[epoch8, step1149]: loss 0.628189
[epoch8, step1150]: loss 0.532375
[epoch8, step1151]: loss 0.538401
[epoch8, step1152]: loss 0.232579
[epoch8, step1153]: loss 0.752062
[epoch8, step1154]: loss 0.600442
[epoch8, step1155]: loss 0.648687
[epoch8, step1156]: loss 0.447557
[epoch8, step1157]: loss 0.522913
[epoch8, step1158]: loss 0.448127
[epoch8, step1159]: loss 0.588283
[epoch8, step1160]: loss 0.447000
[epoch8, step1161]: loss 0.446550
[epoch8, step1162]: loss 0.537262
[epoch8, step1163]: loss 0.742813
[epoch8, step1164]: loss 0.611797
[epoch8, step1165]: loss 0.525075
[epoch8, step1166]: loss 0.274484
[epoch8, step1167]: loss 0.568974
[epoch8, step1168]: loss 0.586700
[epoch8, step1169]: loss 0.689624
[epoch8, step1170]: loss 0.628914
[epoch8, step1171]: loss 0.402418
[epoch8, step1172]: loss 0.533980
[epoch8, step1173]: loss 0.389232
[epoch8, step1174]: loss 0.402290
[epoch8, step1175]: loss 0.572614
[epoch8, step1176]: loss 0.636666
[epoch8, step1177]: loss 0.662945
[epoch8, step1178]: loss 0.473069
[epoch8, step1179]: loss 0.562397
[epoch8, step1180]: loss 0.526637
[epoch8, step1181]: loss 0.392602
[epoch8, step1182]: loss 0.507310
[epoch8, step1183]: loss 0.530616
[epoch8, step1184]: loss 0.745911
[epoch8, step1185]: loss 0.486181
[epoch8, step1186]: loss 0.591661
[epoch8, step1187]: loss 0.691601
[epoch8, step1188]: loss 0.401764
[epoch8, step1189]: loss 0.324062
[epoch8, step1190]: loss 0.250323
[epoch8, step1191]: loss 0.317954
[epoch8, step1192]: loss 0.643039
[epoch8, step1193]: loss 0.581642
[epoch8, step1194]: loss 0.341052
[epoch8, step1195]: loss 0.465390
[epoch8, step1196]: loss 0.460715
[epoch8, step1197]: loss 0.295830
[epoch8, step1198]: loss 0.543049
[epoch8, step1199]: loss 0.577856
[epoch8, step1200]: loss 0.368742
[epoch8, step1201]: loss 0.512469
[epoch8, step1202]: loss 0.661630
[epoch8, step1203]: loss 0.415045
[epoch8, step1204]: loss 0.351126
[epoch8, step1205]: loss 0.456568
[epoch8, step1206]: loss 0.475563
[epoch8, step1207]: loss 0.538700
[epoch8, step1208]: loss 0.613017
[epoch8, step1209]: loss 0.401667
[epoch8, step1210]: loss 0.428262
[epoch8, step1211]: loss 0.727286
[epoch8, step1212]: loss 0.677918
[epoch8, step1213]: loss 0.474800
[epoch8, step1214]: loss 0.560913
[epoch8, step1215]: loss 0.418623
[epoch8, step1216]: loss 0.534647
[epoch8, step1217]: loss 0.726806
[epoch8, step1218]: loss 0.511925
[epoch8, step1219]: loss 0.561534
[epoch8, step1220]: loss 0.425953
[epoch8, step1221]: loss 0.449228
[epoch8, step1222]: loss 0.374601
[epoch8, step1223]: loss 0.621150
[epoch8, step1224]: loss 0.532549
[epoch8, step1225]: loss 0.469283
[epoch8, step1226]: loss 0.586754
[epoch8, step1227]: loss 0.745567
[epoch8, step1228]: loss 0.140730
[epoch8, step1229]: loss 0.467510
[epoch8, step1230]: loss 0.612481
[epoch8, step1231]: loss 0.450692
[epoch8, step1232]: loss 0.756701
[epoch8, step1233]: loss 0.579184
[epoch8, step1234]: loss 0.539038
[epoch8, step1235]: loss 0.532720
[epoch8, step1236]: loss 0.609551
[epoch8, step1237]: loss 0.634104
[epoch8, step1238]: loss 0.502144
[epoch8, step1239]: loss 0.442477
[epoch8, step1240]: loss 0.491555
[epoch8, step1241]: loss 0.588052
[epoch8, step1242]: loss 0.633220
[epoch8, step1243]: loss 0.677767
[epoch8, step1244]: loss 0.469418
[epoch8, step1245]: loss 0.603672
[epoch8, step1246]: loss 0.356251
[epoch8, step1247]: loss 0.511419
[epoch8, step1248]: loss 0.570625
[epoch8, step1249]: loss 0.440620
[epoch8, step1250]: loss 0.703688
[epoch8, step1251]: loss 0.534847
[epoch8, step1252]: loss 0.525079
[epoch8, step1253]: loss 0.595049
[epoch8, step1254]: loss 0.485484
[epoch8, step1255]: loss 0.493867
[epoch8, step1256]: loss 0.458489
[epoch8, step1257]: loss 0.474732
[epoch8, step1258]: loss 0.599858
[epoch8, step1259]: loss 0.627687
[epoch8, step1260]: loss 0.589119
[epoch8, step1261]: loss 0.422948
[epoch8, step1262]: loss 0.477391
[epoch8, step1263]: loss 0.390418
[epoch8, step1264]: loss 0.494288
[epoch8, step1265]: loss 0.507231
[epoch8, step1266]: loss 0.539162
[epoch8, step1267]: loss 0.448173
[epoch8, step1268]: loss 0.431475
[epoch8, step1269]: loss 0.410924
[epoch8, step1270]: loss 0.646327
[epoch8, step1271]: loss 0.725712
[epoch8, step1272]: loss 0.555538
[epoch8, step1273]: loss 0.587233
[epoch8, step1274]: loss 0.539198
[epoch8, step1275]: loss 0.514466
[epoch8, step1276]: loss 0.612564
[epoch8, step1277]: loss 0.610250
[epoch8, step1278]: loss 0.523823
[epoch8, step1279]: loss 0.516655
[epoch8, step1280]: loss 0.352721
[epoch8, step1281]: loss 0.373111
[epoch8, step1282]: loss 0.644917
[epoch8, step1283]: loss 0.468315
[epoch8, step1284]: loss 0.539157
[epoch8, step1285]: loss 0.533088
[epoch8, step1286]: loss 0.446361
[epoch8, step1287]: loss 0.717017
[epoch8, step1288]: loss 0.664982
[epoch8, step1289]: loss 0.786584
[epoch8, step1290]: loss 0.362702
[epoch8, step1291]: loss 0.510506
[epoch8, step1292]: loss 0.533776
[epoch8, step1293]: loss 0.625425
[epoch8, step1294]: loss 0.574707
[epoch8, step1295]: loss 0.494334
[epoch8, step1296]: loss 0.565933
[epoch8, step1297]: loss 0.380931
[epoch8, step1298]: loss 0.520109
[epoch8, step1299]: loss 0.595339
[epoch8, step1300]: loss 0.382969
[epoch8, step1301]: loss 0.688069
[epoch8, step1302]: loss 0.526913
[epoch8, step1303]: loss 0.252266
[epoch8, step1304]: loss 0.637159
[epoch8, step1305]: loss 0.482171
[epoch8, step1306]: loss 0.473239
[epoch8, step1307]: loss 0.568410
[epoch8, step1308]: loss 0.470065
[epoch8, step1309]: loss 0.297020
[epoch8, step1310]: loss 0.498531
[epoch8, step1311]: loss 0.360410
[epoch8, step1312]: loss 0.469622
[epoch8, step1313]: loss 0.565166
[epoch8, step1314]: loss 0.647989
[epoch8, step1315]: loss 0.418466
[epoch8, step1316]: loss 0.424111
[epoch8, step1317]: loss 0.379361
[epoch8, step1318]: loss 0.481852
[epoch8, step1319]: loss 0.742734
[epoch8, step1320]: loss 0.465338
[epoch8, step1321]: loss 0.595519
[epoch8, step1322]: loss 0.457437
[epoch8, step1323]: loss 0.461775
[epoch8, step1324]: loss 0.500423
[epoch8, step1325]: loss 0.565407
[epoch8, step1326]: loss 0.573846
[epoch8, step1327]: loss 0.538713
[epoch8, step1328]: loss 0.313388
[epoch8, step1329]: loss 0.474730
[epoch8, step1330]: loss 0.103389
[epoch8, step1331]: loss 0.510867
[epoch8, step1332]: loss 0.451895
[epoch8, step1333]: loss 0.604266
[epoch8, step1334]: loss 0.595670
[epoch8, step1335]: loss 0.402495
[epoch8, step1336]: loss 0.583736
[epoch8, step1337]: loss 0.442178
[epoch8, step1338]: loss 0.424177
[epoch8, step1339]: loss 0.322671
[epoch8, step1340]: loss 0.670032
[epoch8, step1341]: loss 0.616113
[epoch8, step1342]: loss 0.564615
[epoch8, step1343]: loss 0.672386
[epoch8, step1344]: loss 0.601432
[epoch8, step1345]: loss 0.651762
[epoch8, step1346]: loss 0.662551
[epoch8, step1347]: loss 0.723941
[epoch8, step1348]: loss 0.510485
[epoch8, step1349]: loss 0.562386
[epoch8, step1350]: loss 0.453892
[epoch8, step1351]: loss 0.356049
[epoch8, step1352]: loss 0.751856
[epoch8, step1353]: loss 0.290278
[epoch8, step1354]: loss 0.580000
[epoch8, step1355]: loss 0.567937
[epoch8, step1356]: loss 0.541337
[epoch8, step1357]: loss 0.325543
[epoch8, step1358]: loss 0.479227
[epoch8, step1359]: loss 0.466472
[epoch8, step1360]: loss 0.379586
[epoch8, step1361]: loss 0.535894
[epoch8, step1362]: loss 0.606882
[epoch8, step1363]: loss 0.335099
[epoch8, step1364]: loss 0.655951
[epoch8, step1365]: loss 0.447935
[epoch8, step1366]: loss 0.746984
[epoch8, step1367]: loss 0.688886
[epoch8, step1368]: loss 0.758440
[epoch8, step1369]: loss 0.552582
[epoch8, step1370]: loss 0.341758
[epoch8, step1371]: loss 0.593330
[epoch8, step1372]: loss 0.526496
[epoch8, step1373]: loss 0.448188
[epoch8, step1374]: loss 0.407082
[epoch8, step1375]: loss 0.738299
[epoch8, step1376]: loss 0.388022
[epoch8, step1377]: loss 0.671036
[epoch8, step1378]: loss 0.389536
[epoch8, step1379]: loss 0.745306
[epoch8, step1380]: loss 0.690140
[epoch8, step1381]: loss 0.602521
[epoch8, step1382]: loss 0.437173
[epoch8, step1383]: loss 0.586039
[epoch8, step1384]: loss 0.552237
[epoch8, step1385]: loss 0.477252
[epoch8, step1386]: loss 0.539046
[epoch8, step1387]: loss 0.561887
[epoch8, step1388]: loss 0.577151
[epoch8, step1389]: loss 0.659349
[epoch8, step1390]: loss 0.568762
[epoch8, step1391]: loss 0.554537
[epoch8, step1392]: loss 0.506237
[epoch8, step1393]: loss 0.615768
[epoch8, step1394]: loss 0.515250
[epoch8, step1395]: loss 0.460876
[epoch8, step1396]: loss 0.443412
[epoch8, step1397]: loss 0.539214
[epoch8, step1398]: loss 0.743837
[epoch8, step1399]: loss 0.506151
[epoch8, step1400]: loss 0.480129
[epoch8, step1401]: loss 0.700831
[epoch8, step1402]: loss 0.408627
[epoch8, step1403]: loss 0.638549
[epoch8, step1404]: loss 0.609299
[epoch8, step1405]: loss 0.514163
[epoch8, step1406]: loss 0.573840
[epoch8, step1407]: loss 0.408123
[epoch8, step1408]: loss 0.249031
[epoch8, step1409]: loss 0.302111
[epoch8, step1410]: loss 0.433852
[epoch8, step1411]: loss 0.592584
[epoch8, step1412]: loss 0.643026
[epoch8, step1413]: loss 0.394501
[epoch8, step1414]: loss 0.543563
[epoch8, step1415]: loss 0.507877
[epoch8, step1416]: loss 0.532409
[epoch8, step1417]: loss 0.470422
[epoch8, step1418]: loss 0.388743
[epoch8, step1419]: loss 0.538436
[epoch8, step1420]: loss 0.255271
[epoch8, step1421]: loss 0.547880
[epoch8, step1422]: loss 0.478480
[epoch8, step1423]: loss 0.436155
[epoch8, step1424]: loss 0.261018
[epoch8, step1425]: loss 0.677873
[epoch8, step1426]: loss 0.510287
[epoch8, step1427]: loss 0.596231
[epoch8, step1428]: loss 0.303370
[epoch8, step1429]: loss 0.574467
[epoch8, step1430]: loss 0.587790
[epoch8, step1431]: loss 0.683069
[epoch8, step1432]: loss 0.480891
[epoch8, step1433]: loss 0.573932
[epoch8, step1434]: loss 0.641614
[epoch8, step1435]: loss 0.168101
[epoch8, step1436]: loss 0.573789
[epoch8, step1437]: loss 0.440676
[epoch8, step1438]: loss 0.620331
[epoch8, step1439]: loss 0.575852
[epoch8, step1440]: loss 0.516740
[epoch8, step1441]: loss 0.630092
[epoch8, step1442]: loss 0.318497
[epoch8, step1443]: loss 0.512759
[epoch8, step1444]: loss 0.374743
[epoch8, step1445]: loss 0.620884
[epoch8, step1446]: loss 0.433535
[epoch8, step1447]: loss 0.561014
[epoch8, step1448]: loss 0.529460
[epoch8, step1449]: loss 0.534760
[epoch8, step1450]: loss 0.456034
[epoch8, step1451]: loss 0.428953
[epoch8, step1452]: loss 0.606760
[epoch8, step1453]: loss 0.640220
[epoch8, step1454]: loss 0.621406
[epoch8, step1455]: loss 0.657885
[epoch8, step1456]: loss 0.599310
[epoch8, step1457]: loss 0.798551
[epoch8, step1458]: loss 0.387528
[epoch8, step1459]: loss 0.325200
[epoch8, step1460]: loss 0.468288
[epoch8, step1461]: loss 0.532952
[epoch8, step1462]: loss 0.257520
[epoch8, step1463]: loss 0.576489
[epoch8, step1464]: loss 0.643946
[epoch8, step1465]: loss 0.179646
[epoch8, step1466]: loss 0.372776
[epoch8, step1467]: loss 0.489409
[epoch8, step1468]: loss 0.746344
[epoch8, step1469]: loss 0.438856
[epoch8, step1470]: loss 0.205737
[epoch8, step1471]: loss 0.391671
[epoch8, step1472]: loss 0.424895
[epoch8, step1473]: loss 0.833050
[epoch8, step1474]: loss 0.426160
[epoch8, step1475]: loss 0.518085
[epoch8, step1476]: loss 0.578192
[epoch8, step1477]: loss 0.741341
[epoch8, step1478]: loss 0.680962
[epoch8, step1479]: loss 0.175170
[epoch8, step1480]: loss 0.637500
[epoch8, step1481]: loss 0.327392
[epoch8, step1482]: loss 0.620900
[epoch8, step1483]: loss 0.654223
[epoch8, step1484]: loss 0.651388
[epoch8, step1485]: loss 0.333008
[epoch8, step1486]: loss 0.465125
[epoch8, step1487]: loss 0.243389
[epoch8, step1488]: loss 0.287666
[epoch8, step1489]: loss 0.458167
[epoch8, step1490]: loss 0.583112
[epoch8, step1491]: loss 0.542691
[epoch8, step1492]: loss 0.603926
[epoch8, step1493]: loss 0.570660
[epoch8, step1494]: loss 0.392113
[epoch8, step1495]: loss 0.664459
[epoch8, step1496]: loss 0.678621
[epoch8, step1497]: loss 0.788541
[epoch8, step1498]: loss 0.446477
[epoch8, step1499]: loss 0.552656
[epoch8, step1500]: loss 0.659550
[epoch8, step1501]: loss 0.410014
[epoch8, step1502]: loss 0.696549
[epoch8, step1503]: loss 0.568530
[epoch8, step1504]: loss 0.797044
[epoch8, step1505]: loss 0.625174
[epoch8, step1506]: loss 0.696604
[epoch8, step1507]: loss 0.556733
[epoch8, step1508]: loss 0.562230
[epoch8, step1509]: loss 0.692541
[epoch8, step1510]: loss 0.596694
[epoch8, step1511]: loss 0.503024
[epoch8, step1512]: loss 0.540501
[epoch8, step1513]: loss 0.516807
[epoch8, step1514]: loss 0.437928
[epoch8, step1515]: loss 0.473297
[epoch8, step1516]: loss 0.564039
[epoch8, step1517]: loss 0.687487
[epoch8, step1518]: loss 0.546579
[epoch8, step1519]: loss 0.568061
[epoch8, step1520]: loss 0.356260
[epoch8, step1521]: loss 0.611998
[epoch8, step1522]: loss 0.696343
[epoch8, step1523]: loss 0.473912
[epoch8, step1524]: loss 0.608132
[epoch8, step1525]: loss 0.556414
[epoch8, step1526]: loss 0.711059
[epoch8, step1527]: loss 0.491297
[epoch8, step1528]: loss 0.194013
[epoch8, step1529]: loss 0.524578
[epoch8, step1530]: loss 0.703772
[epoch8, step1531]: loss 0.455720
[epoch8, step1532]: loss 0.449560
[epoch8, step1533]: loss 0.611994
[epoch8, step1534]: loss 0.646517
[epoch8, step1535]: loss 0.801503
[epoch8, step1536]: loss 0.555898
[epoch8, step1537]: loss 0.353755
[epoch8, step1538]: loss 0.467247
[epoch8, step1539]: loss 0.612884
[epoch8, step1540]: loss 0.429544
[epoch8, step1541]: loss 0.612392
[epoch8, step1542]: loss 0.556569
[epoch8, step1543]: loss 0.747368
[epoch8, step1544]: loss 0.536412
[epoch8, step1545]: loss 0.429777
[epoch8, step1546]: loss 0.222933
[epoch8, step1547]: loss 0.753031
[epoch8, step1548]: loss 0.727762
[epoch8, step1549]: loss 0.508299
[epoch8, step1550]: loss 0.473944
[epoch8, step1551]: loss 0.586121
[epoch8, step1552]: loss 0.488858
[epoch8, step1553]: loss 0.525909
[epoch8, step1554]: loss 0.282539
[epoch8, step1555]: loss 0.466298
[epoch8, step1556]: loss 0.576339
[epoch8, step1557]: loss 0.322615
[epoch8, step1558]: loss 0.714267
[epoch8, step1559]: loss 0.476062
[epoch8, step1560]: loss 0.520886
[epoch8, step1561]: loss 0.533341
[epoch8, step1562]: loss 0.537366
[epoch8, step1563]: loss 0.786836
[epoch8, step1564]: loss 0.507102
[epoch8, step1565]: loss 0.552703
[epoch8, step1566]: loss 0.349483
[epoch8, step1567]: loss 0.467806
[epoch8, step1568]: loss 0.753872
[epoch8, step1569]: loss 0.548973
[epoch8, step1570]: loss 0.426309
[epoch8, step1571]: loss 0.656720
[epoch8, step1572]: loss 0.525455
[epoch8, step1573]: loss 0.577697
[epoch8, step1574]: loss 0.720834
[epoch8, step1575]: loss 0.624160
[epoch8, step1576]: loss 0.660320
[epoch8, step1577]: loss 0.365001
[epoch8, step1578]: loss 0.373110
[epoch8, step1579]: loss 0.582813
[epoch8, step1580]: loss 0.470611
[epoch8, step1581]: loss 0.733387
[epoch8, step1582]: loss 0.419990
[epoch8, step1583]: loss 0.671845
[epoch8, step1584]: loss 0.435313
[epoch8, step1585]: loss 0.534212
[epoch8, step1586]: loss 0.664434
[epoch8, step1587]: loss 0.686814
[epoch8, step1588]: loss 0.501167
[epoch8, step1589]: loss 0.572723
[epoch8, step1590]: loss 0.532105
[epoch8, step1591]: loss 0.641848
[epoch8, step1592]: loss 0.411082
[epoch8, step1593]: loss 0.443501
[epoch8, step1594]: loss 0.707969
[epoch8, step1595]: loss 0.374281
[epoch8, step1596]: loss 0.589149
[epoch8, step1597]: loss 0.590990
[epoch8, step1598]: loss 0.572419
[epoch8, step1599]: loss 0.394927
[epoch8, step1600]: loss 0.616021
[epoch8, step1601]: loss 0.586212
[epoch8, step1602]: loss 0.557770
[epoch8, step1603]: loss 0.563687
[epoch8, step1604]: loss 0.180153
[epoch8, step1605]: loss 0.606385
[epoch8, step1606]: loss 0.727584
[epoch8, step1607]: loss 0.424685
[epoch8, step1608]: loss 0.317168
[epoch8, step1609]: loss 0.539231
[epoch8, step1610]: loss 0.541521
[epoch8, step1611]: loss 0.686863
[epoch8, step1612]: loss 0.412640
[epoch8, step1613]: loss 0.526385
[epoch8, step1614]: loss 0.188275
[epoch8, step1615]: loss 0.662495
[epoch8, step1616]: loss 0.499720
[epoch8, step1617]: loss 0.585479
[epoch8, step1618]: loss 0.547497
[epoch8, step1619]: loss 0.371512
[epoch8, step1620]: loss 0.650609
[epoch8, step1621]: loss 0.303874
[epoch8, step1622]: loss 0.370143
[epoch8, step1623]: loss 0.584046
[epoch8, step1624]: loss 0.379257
[epoch8, step1625]: loss 0.402638
[epoch8, step1626]: loss 0.590037
[epoch8, step1627]: loss 0.792763
[epoch8, step1628]: loss 0.285856
[epoch8, step1629]: loss 0.180249
[epoch8, step1630]: loss 0.662411
[epoch8, step1631]: loss 0.527543
[epoch8, step1632]: loss 0.467346
[epoch8, step1633]: loss 0.202281
[epoch8, step1634]: loss 0.360046
[epoch8, step1635]: loss 0.406147
[epoch8, step1636]: loss 0.348820
[epoch8, step1637]: loss 0.428510
[epoch8, step1638]: loss 0.682738
[epoch8, step1639]: loss 0.683491
[epoch8, step1640]: loss 0.586141
[epoch8, step1641]: loss 0.643180
[epoch8, step1642]: loss 0.473900
[epoch8, step1643]: loss 0.634268
[epoch8, step1644]: loss 0.585854
[epoch8, step1645]: loss 0.482440
[epoch8, step1646]: loss 0.639159
[epoch8, step1647]: loss 0.551090
[epoch8, step1648]: loss 0.443620
[epoch8, step1649]: loss 0.553668
[epoch8, step1650]: loss 0.272143
[epoch8, step1651]: loss 0.548333
[epoch8, step1652]: loss 0.310535
[epoch8, step1653]: loss 0.477556
[epoch8, step1654]: loss 0.397284
[epoch8, step1655]: loss 0.647472
[epoch8, step1656]: loss 0.548183
[epoch8, step1657]: loss 0.713946
[epoch8, step1658]: loss 0.375464
[epoch8, step1659]: loss 0.290373
[epoch8, step1660]: loss 0.624562
[epoch8, step1661]: loss 0.696343
[epoch8, step1662]: loss 0.575544
[epoch8, step1663]: loss 0.517067
[epoch8, step1664]: loss 0.461188
[epoch8, step1665]: loss 0.405971
[epoch8, step1666]: loss 0.267224
[epoch8, step1667]: loss 0.315355
[epoch8, step1668]: loss 0.471325
[epoch8, step1669]: loss 0.373550
[epoch8, step1670]: loss 0.524735
[epoch8, step1671]: loss 0.496419
[epoch8, step1672]: loss 0.323772
[epoch8, step1673]: loss 0.762776
[epoch8, step1674]: loss 0.442006
[epoch8, step1675]: loss 0.576730
[epoch8, step1676]: loss 0.671144
[epoch8, step1677]: loss 0.330191
[epoch8, step1678]: loss 0.388423
[epoch8, step1679]: loss 0.701936
[epoch8, step1680]: loss 0.515911
[epoch8, step1681]: loss 0.539786
[epoch8, step1682]: loss 0.566526
[epoch8, step1683]: loss 0.413786
[epoch8, step1684]: loss 0.504915
[epoch8, step1685]: loss 0.540608
[epoch8, step1686]: loss 0.612507
[epoch8, step1687]: loss 0.618483
[epoch8, step1688]: loss 0.544513
[epoch8, step1689]: loss 0.428185
[epoch8, step1690]: loss 0.357244
[epoch8, step1691]: loss 0.399075
[epoch8, step1692]: loss 0.366186
[epoch8, step1693]: loss 0.394846
[epoch8, step1694]: loss 0.445857
[epoch8, step1695]: loss 0.458089
[epoch8, step1696]: loss 0.332075
[epoch8, step1697]: loss 0.449505
[epoch8, step1698]: loss 0.497677
[epoch8, step1699]: loss 0.367738
[epoch8, step1700]: loss 0.272491
[epoch8, step1701]: loss 0.235922
[epoch8, step1702]: loss 0.456447
[epoch8, step1703]: loss 0.595642
[epoch8, step1704]: loss 0.241883
[epoch8, step1705]: loss 0.221135
[epoch8, step1706]: loss 0.488592
[epoch8, step1707]: loss 0.710702
[epoch8, step1708]: loss 0.423618
[epoch8, step1709]: loss 0.421382
[epoch8, step1710]: loss 0.563436
[epoch8, step1711]: loss 0.359551
[epoch8, step1712]: loss 0.420785
[epoch8, step1713]: loss 0.276669
[epoch8, step1714]: loss 0.376120
[epoch8, step1715]: loss 0.447207
[epoch8, step1716]: loss 0.516819
[epoch8, step1717]: loss 0.602587
[epoch8, step1718]: loss 0.616690
[epoch8, step1719]: loss 0.514168
[epoch8, step1720]: loss 0.694629
[epoch8, step1721]: loss 0.585082
[epoch8, step1722]: loss 0.623836
[epoch8, step1723]: loss 0.596421
[epoch8, step1724]: loss 0.689281
[epoch8, step1725]: loss 0.365304
[epoch8, step1726]: loss 0.584169
[epoch8, step1727]: loss 0.654906
[epoch8, step1728]: loss 0.367569
[epoch8, step1729]: loss 0.522826
[epoch8, step1730]: loss 0.399330
[epoch8, step1731]: loss 0.464340
[epoch8, step1732]: loss 0.375723
[epoch8, step1733]: loss 0.362860
[epoch8, step1734]: loss 0.384845
[epoch8, step1735]: loss 0.670237
[epoch8, step1736]: loss 0.640235
[epoch8, step1737]: loss 0.724223
[epoch8, step1738]: loss 0.260402
[epoch8, step1739]: loss 0.617327
[epoch8, step1740]: loss 0.641662
[epoch8, step1741]: loss 0.418660
[epoch8, step1742]: loss 0.436555
[epoch8, step1743]: loss 0.797229
[epoch8, step1744]: loss 0.764991
[epoch8, step1745]: loss 0.785859
[epoch8, step1746]: loss 0.680898
[epoch8, step1747]: loss 0.637329
[epoch8, step1748]: loss 0.571670
[epoch8, step1749]: loss 0.208833
[epoch8, step1750]: loss 0.542284
[epoch8, step1751]: loss 0.560877
[epoch8, step1752]: loss 0.626926
[epoch8, step1753]: loss 0.430952
[epoch8, step1754]: loss 0.607919
[epoch8, step1755]: loss 0.652478
[epoch8, step1756]: loss 0.433455
[epoch8, step1757]: loss 0.462574
[epoch8, step1758]: loss 0.361329
[epoch8, step1759]: loss 0.622531
[epoch8, step1760]: loss 0.581789
[epoch8, step1761]: loss 0.652175
[epoch8, step1762]: loss 0.430482
[epoch8, step1763]: loss 0.390693
[epoch8, step1764]: loss 0.491026
[epoch8, step1765]: loss 0.724312
[epoch8, step1766]: loss 0.295184
[epoch8, step1767]: loss 0.609424
[epoch8, step1768]: loss 0.728806
[epoch8, step1769]: loss 0.427632
[epoch8, step1770]: loss 0.527895
[epoch8, step1771]: loss 0.561809
[epoch8, step1772]: loss 0.508483
[epoch8, step1773]: loss 0.301514
[epoch8, step1774]: loss 0.458736
[epoch8, step1775]: loss 0.270677
[epoch8, step1776]: loss 0.481584
[epoch8, step1777]: loss 0.568826
[epoch8, step1778]: loss 0.480376
[epoch8, step1779]: loss 0.623602
[epoch8, step1780]: loss 0.442410
[epoch8, step1781]: loss 0.160886
[epoch8, step1782]: loss 0.347732
[epoch8, step1783]: loss 0.824156
[epoch8, step1784]: loss 0.538364
[epoch8, step1785]: loss 0.499217
[epoch8, step1786]: loss 0.631256
[epoch8, step1787]: loss 0.582394
[epoch8, step1788]: loss 0.367670
[epoch8, step1789]: loss 0.572468
[epoch8, step1790]: loss 0.429331
[epoch8, step1791]: loss 0.490148
[epoch8, step1792]: loss 0.589496
[epoch8, step1793]: loss 0.330836
[epoch8, step1794]: loss 0.745736
[epoch8, step1795]: loss 0.626771
[epoch8, step1796]: loss 0.556501
[epoch8, step1797]: loss 0.650472
[epoch8, step1798]: loss 0.657119
[epoch8, step1799]: loss 0.717291
[epoch8, step1800]: loss 0.652238
[epoch8, step1801]: loss 0.556452
[epoch8, step1802]: loss 0.640212
[epoch8, step1803]: loss 0.665569
[epoch8, step1804]: loss 0.372246
[epoch8, step1805]: loss 0.533391
[epoch8, step1806]: loss 0.707975
[epoch8, step1807]: loss 0.540769
[epoch8, step1808]: loss 0.676569
[epoch8, step1809]: loss 0.510606
[epoch8, step1810]: loss 0.663202
[epoch8, step1811]: loss 0.542500
[epoch8, step1812]: loss 0.296084
[epoch8, step1813]: loss 0.550449
[epoch8, step1814]: loss 0.397725
[epoch8, step1815]: loss 0.721979
[epoch8, step1816]: loss 0.510002
[epoch8, step1817]: loss 0.554402
[epoch8, step1818]: loss 0.619245
[epoch8, step1819]: loss 0.625223
[epoch8, step1820]: loss 0.525277
[epoch8, step1821]: loss 0.438256
[epoch8, step1822]: loss 0.253241
[epoch8, step1823]: loss 0.762211
[epoch8, step1824]: loss 0.399333
[epoch8, step1825]: loss 0.582723
[epoch8, step1826]: loss 0.548726
[epoch8, step1827]: loss 0.391735
[epoch8, step1828]: loss 0.713789
[epoch8, step1829]: loss 0.369952
[epoch8, step1830]: loss 0.765802
[epoch8, step1831]: loss 0.542691
[epoch8, step1832]: loss 0.694600
[epoch8, step1833]: loss 0.449310
[epoch8, step1834]: loss 0.427404
[epoch8, step1835]: loss 0.372510
[epoch8, step1836]: loss 0.415479
[epoch8, step1837]: loss 0.582893
[epoch8, step1838]: loss 0.671881
[epoch8, step1839]: loss 0.742077
[epoch8, step1840]: loss 0.626741
[epoch8, step1841]: loss 0.443029
[epoch8, step1842]: loss 0.511063
[epoch8, step1843]: loss 0.484854
[epoch8, step1844]: loss 0.665493
[epoch8, step1845]: loss 0.545157
[epoch8, step1846]: loss 0.588109
[epoch8, step1847]: loss 0.359779
[epoch8, step1848]: loss 0.669727
[epoch8, step1849]: loss 0.279593
[epoch8, step1850]: loss 0.324489
[epoch8, step1851]: loss 0.508476
[epoch8, step1852]: loss 0.679144
[epoch8, step1853]: loss 0.413931
[epoch8, step1854]: loss 0.514041
[epoch8, step1855]: loss 0.472541
[epoch8, step1856]: loss 0.325035
[epoch8, step1857]: loss 0.365772
[epoch8, step1858]: loss 0.302311
[epoch8, step1859]: loss 0.530059
[epoch8, step1860]: loss 0.587961
[epoch8, step1861]: loss 0.279340
[epoch8, step1862]: loss 0.426585
[epoch8, step1863]: loss 0.498325
[epoch8, step1864]: loss 0.538657
[epoch8, step1865]: loss 0.641305
[epoch8, step1866]: loss 0.366994
[epoch8, step1867]: loss 0.627002
[epoch8, step1868]: loss 0.559203
[epoch8, step1869]: loss 0.289448
[epoch8, step1870]: loss 0.749800
[epoch8, step1871]: loss 0.531529
[epoch8, step1872]: loss 0.446656
[epoch8, step1873]: loss 0.623925
[epoch8, step1874]: loss 0.471011
[epoch8, step1875]: loss 0.599845
[epoch8, step1876]: loss 0.542364
[epoch8, step1877]: loss 0.414143
[epoch8, step1878]: loss 0.558884
[epoch8, step1879]: loss 0.615880
[epoch8, step1880]: loss 0.403204
[epoch8, step1881]: loss 0.684819
[epoch8, step1882]: loss 0.558896
[epoch8, step1883]: loss 0.378054
[epoch8, step1884]: loss 0.708109
[epoch8, step1885]: loss 0.418067
[epoch8, step1886]: loss 0.631885
[epoch8, step1887]: loss 0.483953
[epoch8, step1888]: loss 0.466192
[epoch8, step1889]: loss 0.822314
[epoch8, step1890]: loss 0.482660
[epoch8, step1891]: loss 0.405134
[epoch8, step1892]: loss 0.569326
[epoch8, step1893]: loss 0.496852
[epoch8, step1894]: loss 0.561796
[epoch8, step1895]: loss 0.346275
[epoch8, step1896]: loss 0.612086
[epoch8, step1897]: loss 0.475717
[epoch8, step1898]: loss 0.468929
[epoch8, step1899]: loss 0.691767
[epoch8, step1900]: loss 0.705469
[epoch8, step1901]: loss 0.576880
[epoch8, step1902]: loss 0.398114
[epoch8, step1903]: loss 0.702134
[epoch8, step1904]: loss 0.502430
[epoch8, step1905]: loss 0.270560
[epoch8, step1906]: loss 0.378967
[epoch8, step1907]: loss 0.499277
[epoch8, step1908]: loss 0.193694
[epoch8, step1909]: loss 0.435454
[epoch8, step1910]: loss 0.430046
[epoch8, step1911]: loss 0.553789
[epoch8, step1912]: loss 0.290352
[epoch8, step1913]: loss 0.640601
[epoch8, step1914]: loss 0.735721
[epoch8, step1915]: loss 0.579265
[epoch8, step1916]: loss 0.507772
[epoch8, step1917]: loss 0.620065
[epoch8, step1918]: loss 0.577082
[epoch8, step1919]: loss 0.390715
[epoch8, step1920]: loss 0.497369
[epoch8, step1921]: loss 0.463206
[epoch8, step1922]: loss 0.575814
[epoch8, step1923]: loss 0.632378
[epoch8, step1924]: loss 0.314108
[epoch8, step1925]: loss 0.681124
[epoch8, step1926]: loss 0.659067
[epoch8, step1927]: loss 0.631157
[epoch8, step1928]: loss 0.381905
[epoch8, step1929]: loss 0.703573
[epoch8, step1930]: loss 0.499249
[epoch8, step1931]: loss 0.563502
[epoch8, step1932]: loss 0.549271
[epoch8, step1933]: loss 0.439353
[epoch8, step1934]: loss 0.626362
[epoch8, step1935]: loss 0.464510
[epoch8, step1936]: loss 0.620465
[epoch8, step1937]: loss 0.627305
[epoch8, step1938]: loss 0.407001
[epoch8, step1939]: loss 0.599555
[epoch8, step1940]: loss 0.564644
[epoch8, step1941]: loss 0.507788
[epoch8, step1942]: loss 0.457516
[epoch8, step1943]: loss 0.380029
[epoch8, step1944]: loss 0.521992
[epoch8, step1945]: loss 0.306543
[epoch8, step1946]: loss 0.567536
[epoch8, step1947]: loss 0.568719
[epoch8, step1948]: loss 0.427389
[epoch8, step1949]: loss 0.687675
[epoch8, step1950]: loss 0.543046
[epoch8, step1951]: loss 0.364778
[epoch8, step1952]: loss 0.315899
[epoch8, step1953]: loss 0.564199
[epoch8, step1954]: loss 0.703411
[epoch8, step1955]: loss 0.578693
[epoch8, step1956]: loss 0.478857
[epoch8, step1957]: loss 0.621909
[epoch8, step1958]: loss 0.693790
[epoch8, step1959]: loss 0.546562
[epoch8, step1960]: loss 0.559971
[epoch8, step1961]: loss 0.595474
[epoch8, step1962]: loss 0.686961
[epoch8, step1963]: loss 0.597187
[epoch8, step1964]: loss 0.584103
[epoch8, step1965]: loss 0.579988
[epoch8, step1966]: loss 0.577672
[epoch8, step1967]: loss 0.505258
[epoch8, step1968]: loss 0.550036
[epoch8, step1969]: loss 0.603671
[epoch8, step1970]: loss 0.624257
[epoch8, step1971]: loss 0.530851
[epoch8, step1972]: loss 0.547513
[epoch8, step1973]: loss 0.561348
[epoch8, step1974]: loss 0.506375
[epoch8, step1975]: loss 0.691269
[epoch8, step1976]: loss 0.453002
[epoch8, step1977]: loss 0.364942
[epoch8, step1978]: loss 0.657608
[epoch8, step1979]: loss 0.565061
[epoch8, step1980]: loss 0.478995
[epoch8, step1981]: loss 0.397195
[epoch8, step1982]: loss 0.789988
[epoch8, step1983]: loss 0.533042
[epoch8, step1984]: loss 0.607993
[epoch8, step1985]: loss 0.573236
[epoch8, step1986]: loss 0.658368
[epoch8, step1987]: loss 0.412223
[epoch8, step1988]: loss 0.481596
[epoch8, step1989]: loss 0.376059
[epoch8, step1990]: loss 0.194069
[epoch8, step1991]: loss 0.502212
[epoch8, step1992]: loss 0.476530
[epoch8, step1993]: loss 0.617945
[epoch8, step1994]: loss 0.527722
[epoch8, step1995]: loss 0.609360
[epoch8, step1996]: loss 0.598284
[epoch8, step1997]: loss 0.234249
[epoch8, step1998]: loss 0.528331
[epoch8, step1999]: loss 0.774655
[epoch8, step2000]: loss 0.356200
[epoch8, step2001]: loss 0.510891
[epoch8, step2002]: loss 0.418212
[epoch8, step2003]: loss 0.456936
[epoch8, step2004]: loss 0.616749
[epoch8, step2005]: loss 0.275673
[epoch8, step2006]: loss 0.679292
[epoch8, step2007]: loss 0.462038
[epoch8, step2008]: loss 0.662414
[epoch8, step2009]: loss 0.643105
[epoch8, step2010]: loss 0.654229
[epoch8, step2011]: loss 0.591396
[epoch8, step2012]: loss 0.583603
[epoch8, step2013]: loss 0.530126
[epoch8, step2014]: loss 0.640015
[epoch8, step2015]: loss 0.515041
[epoch8, step2016]: loss 0.711347
[epoch8, step2017]: loss 0.560926
[epoch8, step2018]: loss 0.329168
[epoch8, step2019]: loss 0.340252
[epoch8, step2020]: loss 0.550215
[epoch8, step2021]: loss 0.614224
[epoch8, step2022]: loss 0.453937
[epoch8, step2023]: loss 0.517621
[epoch8, step2024]: loss 0.562359
[epoch8, step2025]: loss 0.528140
[epoch8, step2026]: loss 0.608380
[epoch8, step2027]: loss 0.501917
[epoch8, step2028]: loss 0.486267
[epoch8, step2029]: loss 0.580707
[epoch8, step2030]: loss 0.581201
[epoch8, step2031]: loss 0.611489
[epoch8, step2032]: loss 0.465475
[epoch8, step2033]: loss 0.371194
[epoch8, step2034]: loss 0.570863
[epoch8, step2035]: loss 0.387411
[epoch8, step2036]: loss 0.437890
[epoch8, step2037]: loss 0.514968
[epoch8, step2038]: loss 0.539412
[epoch8, step2039]: loss 0.463631
[epoch8, step2040]: loss 0.549937
[epoch8, step2041]: loss 0.611805
[epoch8, step2042]: loss 0.716128
[epoch8, step2043]: loss 0.652141
[epoch8, step2044]: loss 0.583307
[epoch8, step2045]: loss 0.478359
[epoch8, step2046]: loss 0.496121
[epoch8, step2047]: loss 0.367475
[epoch8, step2048]: loss 0.372804
[epoch8, step2049]: loss 0.426360
[epoch8, step2050]: loss 0.232730
[epoch8, step2051]: loss 0.379907
[epoch8, step2052]: loss 0.420924
[epoch8, step2053]: loss 0.596833
[epoch8, step2054]: loss 0.637922
[epoch8, step2055]: loss 0.388626
[epoch8, step2056]: loss 0.344110
[epoch8, step2057]: loss 0.650452
[epoch8, step2058]: loss 0.543294
[epoch8, step2059]: loss 0.513905
[epoch8, step2060]: loss 0.568146
[epoch8, step2061]: loss 0.660457
[epoch8, step2062]: loss 0.287136
[epoch8, step2063]: loss 0.450804
[epoch8, step2064]: loss 0.403279
[epoch8, step2065]: loss 0.793039
[epoch8, step2066]: loss 0.475931
[epoch8, step2067]: loss 0.558966
[epoch8, step2068]: loss 0.576122
[epoch8, step2069]: loss 0.692596
[epoch8, step2070]: loss 0.470272
[epoch8, step2071]: loss 0.640127
[epoch8, step2072]: loss 0.756176
[epoch8, step2073]: loss 0.530744
[epoch8, step2074]: loss 0.532446
[epoch8, step2075]: loss 0.465037
[epoch8, step2076]: loss 0.468337
[epoch8, step2077]: loss 0.604003
[epoch8, step2078]: loss 0.398303
[epoch8, step2079]: loss 0.376765
[epoch8, step2080]: loss 0.460680
[epoch8, step2081]: loss 0.502659
[epoch8, step2082]: loss 0.623733
[epoch8, step2083]: loss 0.507021
[epoch8, step2084]: loss 0.565631
[epoch8, step2085]: loss 0.523056
[epoch8, step2086]: loss 0.561605
[epoch8, step2087]: loss 0.318594
[epoch8, step2088]: loss 0.599987
[epoch8, step2089]: loss 0.596302
[epoch8, step2090]: loss 0.550102
[epoch8, step2091]: loss 0.357958
[epoch8, step2092]: loss 0.607532
[epoch8, step2093]: loss 0.455376
[epoch8, step2094]: loss 0.575859
[epoch8, step2095]: loss 0.617085
[epoch8, step2096]: loss 0.740789
[epoch8, step2097]: loss 0.670895
[epoch8, step2098]: loss 0.464729
[epoch8, step2099]: loss 0.373846
[epoch8, step2100]: loss 0.752418
[epoch8, step2101]: loss 0.451071
[epoch8, step2102]: loss 0.667843
[epoch8, step2103]: loss 0.562892
[epoch8, step2104]: loss 0.480455
[epoch8, step2105]: loss 0.513843
[epoch8, step2106]: loss 0.659019
[epoch8, step2107]: loss 0.836283
[epoch8, step2108]: loss 0.593079
[epoch8, step2109]: loss 0.555529
[epoch8, step2110]: loss 0.389110
[epoch8, step2111]: loss 0.596239
[epoch8, step2112]: loss 0.665147
[epoch8, step2113]: loss 0.161887
[epoch8, step2114]: loss 0.713849
[epoch8, step2115]: loss 0.622879
[epoch8, step2116]: loss 0.502250
[epoch8, step2117]: loss 0.783624
[epoch8, step2118]: loss 0.683949
[epoch8, step2119]: loss 0.666877
[epoch8, step2120]: loss 0.435539
[epoch8, step2121]: loss 0.662211
[epoch8, step2122]: loss 0.494369
[epoch8, step2123]: loss 0.668264
[epoch8, step2124]: loss 0.527232
[epoch8, step2125]: loss 0.507169
[epoch8, step2126]: loss 0.471007
[epoch8, step2127]: loss 0.459270
[epoch8, step2128]: loss 0.638772
[epoch8, step2129]: loss 0.147692
[epoch8, step2130]: loss 0.424347
[epoch8, step2131]: loss 0.465647
[epoch8, step2132]: loss 0.550390
[epoch8, step2133]: loss 0.699267
[epoch8, step2134]: loss 0.580837
[epoch8, step2135]: loss 0.399911
[epoch8, step2136]: loss 0.264160
[epoch8, step2137]: loss 0.569364
[epoch8, step2138]: loss 0.544123
[epoch8, step2139]: loss 0.632809
[epoch8, step2140]: loss 0.702028
[epoch8, step2141]: loss 0.399559
[epoch8, step2142]: loss 0.570785
[epoch8, step2143]: loss 0.334220
[epoch8, step2144]: loss 0.649045
[epoch8, step2145]: loss 0.541838
[epoch8, step2146]: loss 0.586509
[epoch8, step2147]: loss 0.338581
[epoch8, step2148]: loss 0.400255
[epoch8, step2149]: loss 0.543563
[epoch8, step2150]: loss 0.494881
[epoch8, step2151]: loss 0.372926
[epoch8, step2152]: loss 0.383347
[epoch8, step2153]: loss 0.672385
[epoch8, step2154]: loss 0.654970
[epoch8, step2155]: loss 0.240889
[epoch8, step2156]: loss 0.689261
[epoch8, step2157]: loss 0.565147
[epoch8, step2158]: loss 0.659564
[epoch8, step2159]: loss 0.349825
[epoch8, step2160]: loss 0.274858
[epoch8, step2161]: loss 0.598660
[epoch8, step2162]: loss 0.500164
[epoch8, step2163]: loss 0.583410
[epoch8, step2164]: loss 0.526763
[epoch8, step2165]: loss 0.440924
[epoch8, step2166]: loss 0.640259
[epoch8, step2167]: loss 0.617006
[epoch8, step2168]: loss 0.533531
[epoch8, step2169]: loss 0.533591
[epoch8, step2170]: loss 0.525001
[epoch8, step2171]: loss 0.576111
[epoch8, step2172]: loss 0.592808
[epoch8, step2173]: loss 0.610645
[epoch8, step2174]: loss 0.401307
[epoch8, step2175]: loss 0.520266
[epoch8, step2176]: loss 0.683505
[epoch8, step2177]: loss 0.237080
[epoch8, step2178]: loss 0.351160
[epoch8, step2179]: loss 0.464303
[epoch8, step2180]: loss 0.478622
[epoch8, step2181]: loss 0.680453
[epoch8, step2182]: loss 0.477147
[epoch8, step2183]: loss 0.499527
[epoch8, step2184]: loss 0.668604
[epoch8, step2185]: loss 0.497822
[epoch8, step2186]: loss 0.458801
[epoch8, step2187]: loss 0.796404
[epoch8, step2188]: loss 0.609497
[epoch8, step2189]: loss 0.603945
[epoch8, step2190]: loss 0.469787
[epoch8, step2191]: loss 0.573619
[epoch8, step2192]: loss 0.406395
[epoch8, step2193]: loss 0.600509
[epoch8, step2194]: loss 0.686921
[epoch8, step2195]: loss 0.475183
[epoch8, step2196]: loss 0.541837
[epoch8, step2197]: loss 0.409579
[epoch8, step2198]: loss 0.393999
[epoch8, step2199]: loss 0.820553
[epoch8, step2200]: loss 0.607410
[epoch8, step2201]: loss 0.502292
[epoch8, step2202]: loss 0.692391
[epoch8, step2203]: loss 0.440933
[epoch8, step2204]: loss 0.547023
[epoch8, step2205]: loss 0.408285
[epoch8, step2206]: loss 0.619500
[epoch8, step2207]: loss 0.556178
[epoch8, step2208]: loss 0.513589
[epoch8, step2209]: loss 0.295871
[epoch8, step2210]: loss 0.132103
[epoch8, step2211]: loss 0.496907
[epoch8, step2212]: loss 0.259366
[epoch8, step2213]: loss 0.558099
[epoch8, step2214]: loss 0.719767
[epoch8, step2215]: loss 0.420341
[epoch8, step2216]: loss 0.627573
[epoch8, step2217]: loss 0.669059
[epoch8, step2218]: loss 0.700848
[epoch8, step2219]: loss 0.395402
[epoch8, step2220]: loss 0.539089
[epoch8, step2221]: loss 0.506271
[epoch8, step2222]: loss 0.474839
[epoch8, step2223]: loss 0.696473
[epoch8, step2224]: loss 0.550379
[epoch8, step2225]: loss 0.502458
[epoch8, step2226]: loss 0.647206
[epoch8, step2227]: loss 0.752896
[epoch8, step2228]: loss 0.615672
[epoch8, step2229]: loss 0.328320
[epoch8, step2230]: loss 0.465593
[epoch8, step2231]: loss 0.512863
[epoch8, step2232]: loss 0.444832
[epoch8, step2233]: loss 0.614688
[epoch8, step2234]: loss 0.628710
[epoch8, step2235]: loss 0.475982
[epoch8, step2236]: loss 0.606933
[epoch8, step2237]: loss 0.611163
[epoch8, step2238]: loss 0.502861
[epoch8, step2239]: loss 0.282557
[epoch8, step2240]: loss 0.625956
[epoch8, step2241]: loss 0.461492
[epoch8, step2242]: loss 0.693098
[epoch8, step2243]: loss 0.615857
[epoch8, step2244]: loss 0.605259
[epoch8, step2245]: loss 0.479731
[epoch8, step2246]: loss 0.680727
[epoch8, step2247]: loss 0.511803
[epoch8, step2248]: loss 0.402021
[epoch8, step2249]: loss 0.545323
[epoch8, step2250]: loss 0.493594
[epoch8, step2251]: loss 0.328370
[epoch8, step2252]: loss 0.615388
[epoch8, step2253]: loss 0.397962
[epoch8, step2254]: loss 0.778514
[epoch8, step2255]: loss 0.411678
[epoch8, step2256]: loss 0.431373
[epoch8, step2257]: loss 0.487264
[epoch8, step2258]: loss 0.563131
[epoch8, step2259]: loss 0.428078
[epoch8, step2260]: loss 0.519426
[epoch8, step2261]: loss 0.602559
[epoch8, step2262]: loss 0.557388
[epoch8, step2263]: loss 0.378943
[epoch8, step2264]: loss 0.575576
[epoch8, step2265]: loss 0.279760
[epoch8, step2266]: loss 0.302894
[epoch8, step2267]: loss 0.315055
[epoch8, step2268]: loss 0.722806
[epoch8, step2269]: loss 0.504924
[epoch8, step2270]: loss 0.425606
[epoch8, step2271]: loss 0.456534
[epoch8, step2272]: loss 0.547051
[epoch8, step2273]: loss 0.743900
[epoch8, step2274]: loss 0.369109
[epoch8, step2275]: loss 0.645742
[epoch8, step2276]: loss 0.580512
[epoch8, step2277]: loss 0.482249
[epoch8, step2278]: loss 0.418790
[epoch8, step2279]: loss 0.609896
[epoch8, step2280]: loss 0.701617
[epoch8, step2281]: loss 0.620219
[epoch8, step2282]: loss 0.562763
[epoch8, step2283]: loss 0.715651
[epoch8, step2284]: loss 0.572517
[epoch8, step2285]: loss 0.592229
[epoch8, step2286]: loss 0.431152
[epoch8, step2287]: loss 0.731089
[epoch8, step2288]: loss 0.151255
[epoch8, step2289]: loss 0.593975
[epoch8, step2290]: loss 0.684903
[epoch8, step2291]: loss 0.274443
[epoch8, step2292]: loss 0.542213
[epoch8, step2293]: loss 0.452840
[epoch8, step2294]: loss 0.634912
[epoch8, step2295]: loss 0.289782
[epoch8, step2296]: loss 0.568879
[epoch8, step2297]: loss 0.774830
[epoch8, step2298]: loss 0.515236
[epoch8, step2299]: loss 0.404395
[epoch8, step2300]: loss 0.380660
[epoch8, step2301]: loss 0.638568
[epoch8, step2302]: loss 0.649958
[epoch8, step2303]: loss 0.187828
[epoch8, step2304]: loss 0.582391
[epoch8, step2305]: loss 0.494799
[epoch8, step2306]: loss 0.418000
[epoch8, step2307]: loss 0.376904
[epoch8, step2308]: loss 0.473246
[epoch8, step2309]: loss 0.672862
[epoch8, step2310]: loss 0.470554
[epoch8, step2311]: loss 0.495836
[epoch8, step2312]: loss 0.428658
[epoch8, step2313]: loss 0.514163
[epoch8, step2314]: loss 0.549162
[epoch8, step2315]: loss 0.631444
[epoch8, step2316]: loss 0.479488
[epoch8, step2317]: loss 0.453726
[epoch8, step2318]: loss 0.133545
[epoch8, step2319]: loss 0.528750
[epoch8, step2320]: loss 0.646062
[epoch8, step2321]: loss 0.522337
[epoch8, step2322]: loss 0.414221
[epoch8, step2323]: loss 0.537743
[epoch8, step2324]: loss 0.772146
[epoch8, step2325]: loss 0.442473
[epoch8, step2326]: loss 0.546282
[epoch8, step2327]: loss 0.505074
[epoch8, step2328]: loss 0.599245
[epoch8, step2329]: loss 0.364853
[epoch8, step2330]: loss 0.265225
[epoch8, step2331]: loss 0.462694
[epoch8, step2332]: loss 0.658152
[epoch8, step2333]: loss 0.403206
[epoch8, step2334]: loss 0.497806
[epoch8, step2335]: loss 0.431303
[epoch8, step2336]: loss 0.512370
[epoch8, step2337]: loss 0.547754
[epoch8, step2338]: loss 0.522283
[epoch8, step2339]: loss 0.428462
[epoch8, step2340]: loss 0.744655
[epoch8, step2341]: loss 0.671760
[epoch8, step2342]: loss 0.487093
[epoch8, step2343]: loss 0.459439
[epoch8, step2344]: loss 0.569789
[epoch8, step2345]: loss 0.493565
[epoch8, step2346]: loss 0.476893
[epoch8, step2347]: loss 0.567257
[epoch8, step2348]: loss 0.490888
[epoch8, step2349]: loss 0.127657
[epoch8, step2350]: loss 0.581118
[epoch8, step2351]: loss 0.152111
[epoch8, step2352]: loss 0.279162
[epoch8, step2353]: loss 0.605352
[epoch8, step2354]: loss 0.287245
[epoch8, step2355]: loss 0.478718
[epoch8, step2356]: loss 0.452072
[epoch8, step2357]: loss 0.488510
[epoch8, step2358]: loss 0.183135
[epoch8, step2359]: loss 0.143204
[epoch8, step2360]: loss 0.500096
[epoch8, step2361]: loss 0.485093
[epoch8, step2362]: loss 0.495711
[epoch8, step2363]: loss 0.744968
[epoch8, step2364]: loss 0.259789
[epoch8, step2365]: loss 0.535618
[epoch8, step2366]: loss 0.558716
[epoch8, step2367]: loss 0.630223
[epoch8, step2368]: loss 0.288064
[epoch8, step2369]: loss 0.590854
[epoch8, step2370]: loss 0.487742
[epoch8, step2371]: loss 0.441450
[epoch8, step2372]: loss 0.540256
[epoch8, step2373]: loss 0.487922
[epoch8, step2374]: loss 0.617898
[epoch8, step2375]: loss 0.373246
[epoch8, step2376]: loss 0.577917
[epoch8, step2377]: loss 0.571529
[epoch8, step2378]: loss 0.578203
[epoch8, step2379]: loss 0.700336
[epoch8, step2380]: loss 0.407394
[epoch8, step2381]: loss 0.564877
[epoch8, step2382]: loss 0.549413
[epoch8, step2383]: loss 0.476659
[epoch8, step2384]: loss 0.609222
[epoch8, step2385]: loss 0.517613
[epoch8, step2386]: loss 0.365975
[epoch8, step2387]: loss 0.698590
[epoch8, step2388]: loss 0.563941
[epoch8, step2389]: loss 0.340834
[epoch8, step2390]: loss 0.491935
[epoch8, step2391]: loss 0.409854
[epoch8, step2392]: loss 0.603854
[epoch8, step2393]: loss 0.468801
[epoch8, step2394]: loss 0.562426
[epoch8, step2395]: loss 0.544799
[epoch8, step2396]: loss 0.473321
[epoch8, step2397]: loss 0.481738
[epoch8, step2398]: loss 0.643734
[epoch8, step2399]: loss 0.416187
[epoch8, step2400]: loss 0.454946
[epoch8, step2401]: loss 0.537363
[epoch8, step2402]: loss 0.571782
[epoch8, step2403]: loss 0.661960
[epoch8, step2404]: loss 0.479128
[epoch8, step2405]: loss 0.395717
[epoch8, step2406]: loss 0.488798
[epoch8, step2407]: loss 0.480767
[epoch8, step2408]: loss 0.515564
[epoch8, step2409]: loss 0.729492
[epoch8, step2410]: loss 0.286360
[epoch8, step2411]: loss 0.461302
[epoch8, step2412]: loss 0.535684
[epoch8, step2413]: loss 0.384594
[epoch8, step2414]: loss 0.812507
[epoch8, step2415]: loss 0.640918
[epoch8, step2416]: loss 0.607085
[epoch8, step2417]: loss 0.630637
[epoch8, step2418]: loss 0.325091
[epoch8, step2419]: loss 0.395957
[epoch8, step2420]: loss 0.526395
[epoch8, step2421]: loss 0.725171
[epoch8, step2422]: loss 0.457579
[epoch8, step2423]: loss 0.488510
[epoch8, step2424]: loss 0.647406
[epoch8, step2425]: loss 0.387616
[epoch8, step2426]: loss 0.614332
[epoch8, step2427]: loss 0.457687
[epoch8, step2428]: loss 0.536721
[epoch8, step2429]: loss 0.626591
[epoch8, step2430]: loss 0.594656
[epoch8, step2431]: loss 0.381629
[epoch8, step2432]: loss 0.383177
[epoch8, step2433]: loss 0.391422
[epoch8, step2434]: loss 0.600335
[epoch8, step2435]: loss 0.409504
[epoch8, step2436]: loss 0.400235
[epoch8, step2437]: loss 0.554545
[epoch8, step2438]: loss 0.505823
[epoch8, step2439]: loss 0.303249
[epoch8, step2440]: loss 0.617304
[epoch8, step2441]: loss 0.321548
[epoch8, step2442]: loss 0.703154
[epoch8, step2443]: loss 0.469621
[epoch8, step2444]: loss 0.620472
[epoch8, step2445]: loss 0.395538
[epoch8, step2446]: loss 0.403586
[epoch8, step2447]: loss 0.518825
[epoch8, step2448]: loss 0.137553
[epoch8, step2449]: loss 0.518329
[epoch8, step2450]: loss 0.605210
[epoch8, step2451]: loss 0.691012
[epoch8, step2452]: loss 0.298611
[epoch8, step2453]: loss 0.450199
[epoch8, step2454]: loss 0.573121
[epoch8, step2455]: loss 0.485329
[epoch8, step2456]: loss 0.496973
[epoch8, step2457]: loss 0.792559
[epoch8, step2458]: loss 0.692584
[epoch8, step2459]: loss 0.678334
[epoch8, step2460]: loss 0.405055
[epoch8, step2461]: loss 0.680293
[epoch8, step2462]: loss 0.512565
[epoch8, step2463]: loss 0.241302
[epoch8, step2464]: loss 0.415455
[epoch8, step2465]: loss 0.665104
[epoch8, step2466]: loss 0.358256
[epoch8, step2467]: loss 0.374290
[epoch8, step2468]: loss 0.589471
[epoch8, step2469]: loss 0.525334
[epoch8, step2470]: loss 0.406601
[epoch8, step2471]: loss 0.567712
[epoch8, step2472]: loss 0.385525
[epoch8, step2473]: loss 0.528896
[epoch8, step2474]: loss 0.480643
[epoch8, step2475]: loss 0.571369
[epoch8, step2476]: loss 0.589919
[epoch8, step2477]: loss 0.795008
[epoch8, step2478]: loss 0.381199
[epoch8, step2479]: loss 0.523527
[epoch8, step2480]: loss 0.596886
[epoch8, step2481]: loss 0.300648
[epoch8, step2482]: loss 0.572506
[epoch8, step2483]: loss 0.715738
[epoch8, step2484]: loss 0.572885
[epoch8, step2485]: loss 0.203655
[epoch8, step2486]: loss 0.557393
[epoch8, step2487]: loss 0.621602
[epoch8, step2488]: loss 0.675227
[epoch8, step2489]: loss 0.507994
[epoch8, step2490]: loss 0.326811
[epoch8, step2491]: loss 0.651832
[epoch8, step2492]: loss 0.387737
[epoch8, step2493]: loss 0.619646
[epoch8, step2494]: loss 0.718274
[epoch8, step2495]: loss 0.611146
[epoch8, step2496]: loss 0.388094
[epoch8, step2497]: loss 0.116293
[epoch8, step2498]: loss 0.487081
[epoch8, step2499]: loss 0.601602
[epoch8, step2500]: loss 0.449001
[epoch8, step2501]: loss 0.504334
[epoch8, step2502]: loss 0.530938
[epoch8, step2503]: loss 0.546982
[epoch8, step2504]: loss 0.292607
[epoch8, step2505]: loss 0.629584
[epoch8, step2506]: loss 0.617782
[epoch8, step2507]: loss 0.440972
[epoch8, step2508]: loss 0.350400
[epoch8, step2509]: loss 0.492034
[epoch8, step2510]: loss 0.595824
[epoch8, step2511]: loss 0.534436
[epoch8, step2512]: loss 0.454142
[epoch8, step2513]: loss 0.479544
[epoch8, step2514]: loss 0.650581
[epoch8, step2515]: loss 0.577484
[epoch8, step2516]: loss 0.651102
[epoch8, step2517]: loss 0.491554
[epoch8, step2518]: loss 0.480600
[epoch8, step2519]: loss 0.683878
[epoch8, step2520]: loss 0.515692
[epoch8, step2521]: loss 0.491896
[epoch8, step2522]: loss 0.338955
[epoch8, step2523]: loss 0.644547
[epoch8, step2524]: loss 0.657146
[epoch8, step2525]: loss 0.399263
[epoch8, step2526]: loss 0.714553
[epoch8, step2527]: loss 0.467744
[epoch8, step2528]: loss 0.610065
[epoch8, step2529]: loss 0.657491
[epoch8, step2530]: loss 0.661829
[epoch8, step2531]: loss 0.655754
[epoch8, step2532]: loss 0.361766
[epoch8, step2533]: loss 0.685884
[epoch8, step2534]: loss 0.694062
[epoch8, step2535]: loss 0.673151
[epoch8, step2536]: loss 0.495509
[epoch8, step2537]: loss 0.528235
[epoch8, step2538]: loss 0.447910
[epoch8, step2539]: loss 0.364425
[epoch8, step2540]: loss 0.701335
[epoch8, step2541]: loss 0.285826
[epoch8, step2542]: loss 0.651328
[epoch8, step2543]: loss 0.630084
[epoch8, step2544]: loss 0.603078
[epoch8, step2545]: loss 0.538645
[epoch8, step2546]: loss 0.526948
[epoch8, step2547]: loss 0.630330
[epoch8, step2548]: loss 0.647897
[epoch8, step2549]: loss 0.658753
[epoch8, step2550]: loss 0.563356
[epoch8, step2551]: loss 0.545793
[epoch8, step2552]: loss 0.372265
[epoch8, step2553]: loss 0.579517
[epoch8, step2554]: loss 0.680436
[epoch8, step2555]: loss 0.483254
[epoch8, step2556]: loss 0.650172
[epoch8, step2557]: loss 0.605907
[epoch8, step2558]: loss 0.602197
[epoch8, step2559]: loss 0.482752
[epoch8, step2560]: loss 0.706012
[epoch8, step2561]: loss 0.552658
[epoch8, step2562]: loss 0.263016
[epoch8, step2563]: loss 0.533201
[epoch8, step2564]: loss 0.678075
[epoch8, step2565]: loss 0.289826
[epoch8, step2566]: loss 0.617288
[epoch8, step2567]: loss 0.434039
[epoch8, step2568]: loss 0.326725
[epoch8, step2569]: loss 0.668994
[epoch8, step2570]: loss 0.517374
[epoch8, step2571]: loss 0.543381
[epoch8, step2572]: loss 0.534421
[epoch8, step2573]: loss 0.503870
[epoch8, step2574]: loss 0.735635
[epoch8, step2575]: loss 0.417832
[epoch8, step2576]: loss 0.440664
[epoch8, step2577]: loss 0.590644
[epoch8, step2578]: loss 0.782072
[epoch8, step2579]: loss 0.521605
[epoch8, step2580]: loss 0.516022
[epoch8, step2581]: loss 0.783415
[epoch8, step2582]: loss 0.715841
[epoch8, step2583]: loss 0.452440
[epoch8, step2584]: loss 0.581998
[epoch8, step2585]: loss 0.338744
[epoch8, step2586]: loss 0.616006
[epoch8, step2587]: loss 0.646421
[epoch8, step2588]: loss 0.458079
[epoch8, step2589]: loss 0.730807
[epoch8, step2590]: loss 0.409280
[epoch8, step2591]: loss 0.614680
[epoch8, step2592]: loss 0.502775
[epoch8, step2593]: loss 0.697841
[epoch8, step2594]: loss 0.464240
[epoch8, step2595]: loss 0.365858
[epoch8, step2596]: loss 0.656567
[epoch8, step2597]: loss 0.534650
[epoch8, step2598]: loss 0.550306
[epoch8, step2599]: loss 0.566875
[epoch8, step2600]: loss 0.319690
[epoch8, step2601]: loss 0.738995
[epoch8, step2602]: loss 0.649754
[epoch8, step2603]: loss 0.525845
[epoch8, step2604]: loss 0.694649
[epoch8, step2605]: loss 0.530893
[epoch8, step2606]: loss 0.499593
[epoch8, step2607]: loss 0.278452
[epoch8, step2608]: loss 0.463882
[epoch8, step2609]: loss 0.566524
[epoch8, step2610]: loss 0.658317
[epoch8, step2611]: loss 0.457960
[epoch8, step2612]: loss 0.361553
[epoch8, step2613]: loss 0.554594
[epoch8, step2614]: loss 0.280221
[epoch8, step2615]: loss 0.519991
[epoch8, step2616]: loss 0.542189
[epoch8, step2617]: loss 0.436152
[epoch8, step2618]: loss 0.411739
[epoch8, step2619]: loss 0.506199
[epoch8, step2620]: loss 0.624770
[epoch8, step2621]: loss 0.476202
[epoch8, step2622]: loss 0.311208
[epoch8, step2623]: loss 0.639458
[epoch8, step2624]: loss 0.525229
[epoch8, step2625]: loss 0.433689
[epoch8, step2626]: loss 0.328069
[epoch8, step2627]: loss 0.513310
[epoch8, step2628]: loss 0.675647
[epoch8, step2629]: loss 0.498217
[epoch8, step2630]: loss 0.659502
[epoch8, step2631]: loss 0.596797
[epoch8, step2632]: loss 0.652078
[epoch8, step2633]: loss 0.583810
[epoch8, step2634]: loss 0.621654
[epoch8, step2635]: loss 0.547291
[epoch8, step2636]: loss 0.376890
[epoch8, step2637]: loss 0.397873
[epoch8, step2638]: loss 0.822388
[epoch8, step2639]: loss 0.498147
[epoch8, step2640]: loss 0.807085
[epoch8, step2641]: loss 0.533922
[epoch8, step2642]: loss 0.498652
[epoch8, step2643]: loss 0.533699
[epoch8, step2644]: loss 0.432513
[epoch8, step2645]: loss 0.579170
[epoch8, step2646]: loss 0.735148
[epoch8, step2647]: loss 0.452796
[epoch8, step2648]: loss 0.538048
[epoch8, step2649]: loss 0.686488
[epoch8, step2650]: loss 0.581288
[epoch8, step2651]: loss 0.430080
[epoch8, step2652]: loss 0.166565
[epoch8, step2653]: loss 0.366409
[epoch8, step2654]: loss 0.518525
[epoch8, step2655]: loss 0.360886
[epoch8, step2656]: loss 0.635277
[epoch8, step2657]: loss 0.612018
[epoch8, step2658]: loss 0.445577
[epoch8, step2659]: loss 0.451450
[epoch8, step2660]: loss 0.510703
[epoch8, step2661]: loss 0.515835
[epoch8, step2662]: loss 0.617418
[epoch8, step2663]: loss 0.505857
[epoch8, step2664]: loss 0.827319
[epoch8, step2665]: loss 0.575504
[epoch8, step2666]: loss 0.493133
[epoch8, step2667]: loss 0.623895
[epoch8, step2668]: loss 0.372728
[epoch8, step2669]: loss 0.435452
[epoch8, step2670]: loss 0.389799
[epoch8, step2671]: loss 0.410586
[epoch8, step2672]: loss 0.554677
[epoch8, step2673]: loss 0.496167
[epoch8, step2674]: loss 0.556942
[epoch8, step2675]: loss 0.546800
[epoch8, step2676]: loss 0.405430
[epoch8, step2677]: loss 0.628153
[epoch8, step2678]: loss 0.599685
[epoch8, step2679]: loss 0.352405
[epoch8, step2680]: loss 0.457579
[epoch8, step2681]: loss 0.211075
[epoch8, step2682]: loss 0.407755
[epoch8, step2683]: loss 0.514160
[epoch8, step2684]: loss 0.466079
[epoch8, step2685]: loss 0.308665
[epoch8, step2686]: loss 0.263690
[epoch8, step2687]: loss 0.306453
[epoch8, step2688]: loss 0.752665
[epoch8, step2689]: loss 0.597893
[epoch8, step2690]: loss 0.587229
[epoch8, step2691]: loss 0.533257
[epoch8, step2692]: loss 0.621170
[epoch8, step2693]: loss 0.500856
[epoch8, step2694]: loss 0.659204
[epoch8, step2695]: loss 0.374998
[epoch8, step2696]: loss 0.453254
[epoch8, step2697]: loss 0.625154
[epoch8, step2698]: loss 0.593593
[epoch8, step2699]: loss 0.354002
[epoch8, step2700]: loss 0.591318
[epoch8, step2701]: loss 0.511837
[epoch8, step2702]: loss 0.314660
[epoch8, step2703]: loss 0.492385
[epoch8, step2704]: loss 0.436281
[epoch8, step2705]: loss 0.702460
[epoch8, step2706]: loss 0.598096
[epoch8, step2707]: loss 0.514124
[epoch8, step2708]: loss 0.629278
[epoch8, step2709]: loss 0.688354
[epoch8, step2710]: loss 0.368795
[epoch8, step2711]: loss 0.468718
[epoch8, step2712]: loss 0.618346
[epoch8, step2713]: loss 0.576834
[epoch8, step2714]: loss 0.492275
[epoch8, step2715]: loss 0.506113
[epoch8, step2716]: loss 0.589121
[epoch8, step2717]: loss 0.526181
[epoch8, step2718]: loss 0.565825
[epoch8, step2719]: loss 0.479191
[epoch8, step2720]: loss 0.633649
[epoch8, step2721]: loss 0.482135
[epoch8, step2722]: loss 0.618950
[epoch8, step2723]: loss 0.551628
[epoch8, step2724]: loss 0.461980
[epoch8, step2725]: loss 0.533759
[epoch8, step2726]: loss 0.483337
[epoch8, step2727]: loss 0.596234
[epoch8, step2728]: loss 0.453273
[epoch8, step2729]: loss 0.607811
[epoch8, step2730]: loss 0.756554
[epoch8, step2731]: loss 0.344883
[epoch8, step2732]: loss 0.489243
[epoch8, step2733]: loss 0.642759
[epoch8, step2734]: loss 0.721202
[epoch8, step2735]: loss 0.484917
[epoch8, step2736]: loss 0.484425
[epoch8, step2737]: loss 0.664671
[epoch8, step2738]: loss 0.684717
[epoch8, step2739]: loss 0.476137
[epoch8, step2740]: loss 0.581042
[epoch8, step2741]: loss 0.649687
[epoch8, step2742]: loss 0.480044
[epoch8, step2743]: loss 0.642275
[epoch8, step2744]: loss 0.397478
[epoch8, step2745]: loss 0.507274
[epoch8, step2746]: loss 0.485502
[epoch8, step2747]: loss 0.464499
[epoch8, step2748]: loss 0.471354
[epoch8, step2749]: loss 0.387208
[epoch8, step2750]: loss 0.632961
[epoch8, step2751]: loss 0.380004
[epoch8, step2752]: loss 0.686617
[epoch8, step2753]: loss 0.466016
[epoch8, step2754]: loss 0.442647
[epoch8, step2755]: loss 0.712419
[epoch8, step2756]: loss 0.642357
[epoch8, step2757]: loss 0.526291
[epoch8, step2758]: loss 0.522021
[epoch8, step2759]: loss 0.592760
[epoch8, step2760]: loss 0.567312
[epoch8, step2761]: loss 0.648572
[epoch8, step2762]: loss 0.539104
[epoch8, step2763]: loss 0.474026
[epoch8, step2764]: loss 0.613554
[epoch8, step2765]: loss 0.471849
[epoch8, step2766]: loss 0.433033
[epoch8, step2767]: loss 0.391276
[epoch8, step2768]: loss 0.508397
[epoch8, step2769]: loss 0.711090
[epoch8, step2770]: loss 0.564649
[epoch8, step2771]: loss 0.522191
[epoch8, step2772]: loss 0.300477
[epoch8, step2773]: loss 0.538782
[epoch8, step2774]: loss 0.649437
[epoch8, step2775]: loss 0.513956
[epoch8, step2776]: loss 0.669248
[epoch8, step2777]: loss 0.687791
[epoch8, step2778]: loss 0.605496
[epoch8, step2779]: loss 0.468505
[epoch8, step2780]: loss 0.593880
[epoch8, step2781]: loss 0.446241
[epoch8, step2782]: loss 0.576372
[epoch8, step2783]: loss 0.595741
[epoch8, step2784]: loss 0.372363
[epoch8, step2785]: loss 0.367314
[epoch8, step2786]: loss 0.394784
[epoch8, step2787]: loss 0.524371
[epoch8, step2788]: loss 0.464310
[epoch8, step2789]: loss 0.487407
[epoch8, step2790]: loss 0.366690
[epoch8, step2791]: loss 0.585168
[epoch8, step2792]: loss 0.553457
[epoch8, step2793]: loss 0.565617
[epoch8, step2794]: loss 0.349692
[epoch8, step2795]: loss 0.475929
[epoch8, step2796]: loss 0.398906
[epoch8, step2797]: loss 0.391315
[epoch8, step2798]: loss 0.532912
[epoch8, step2799]: loss 0.698043
[epoch8, step2800]: loss 0.763090
[epoch8, step2801]: loss 0.600131
[epoch8, step2802]: loss 0.646711
[epoch8, step2803]: loss 0.579831
[epoch8, step2804]: loss 0.419974
[epoch8, step2805]: loss 0.502045
[epoch8, step2806]: loss 0.641432
[epoch8, step2807]: loss 0.541807
[epoch8, step2808]: loss 0.652757
[epoch8, step2809]: loss 0.579719
[epoch8, step2810]: loss 0.708948
[epoch8, step2811]: loss 0.266402
[epoch8, step2812]: loss 0.539657
[epoch8, step2813]: loss 0.538070
[epoch8, step2814]: loss 0.464558
[epoch8, step2815]: loss 0.715859
[epoch8, step2816]: loss 0.276443
[epoch8, step2817]: loss 0.596399
[epoch8, step2818]: loss 0.368819
[epoch8, step2819]: loss 0.357291
[epoch8, step2820]: loss 0.385835
[epoch8, step2821]: loss 0.499799
[epoch8, step2822]: loss 0.475293
[epoch8, step2823]: loss 0.615387
[epoch8, step2824]: loss 0.137304
[epoch8, step2825]: loss 0.620794
[epoch8, step2826]: loss 0.711396
[epoch8, step2827]: loss 0.550933
[epoch8, step2828]: loss 0.573959
[epoch8, step2829]: loss 0.359846
[epoch8, step2830]: loss 0.481771
[epoch8, step2831]: loss 0.292338
[epoch8, step2832]: loss 0.526390
[epoch8, step2833]: loss 0.699893
[epoch8, step2834]: loss 0.701146
[epoch8, step2835]: loss 0.457083
[epoch8, step2836]: loss 0.425399
[epoch8, step2837]: loss 0.403054
[epoch8, step2838]: loss 0.505657
[epoch8, step2839]: loss 0.551447
[epoch8, step2840]: loss 0.493313
[epoch8, step2841]: loss 0.616656
[epoch8, step2842]: loss 0.538014
[epoch8, step2843]: loss 0.307059
[epoch8, step2844]: loss 0.587113
[epoch8, step2845]: loss 0.608338
[epoch8, step2846]: loss 0.621643
[epoch8, step2847]: loss 0.145488
[epoch8, step2848]: loss 0.356439
[epoch8, step2849]: loss 0.728772
[epoch8, step2850]: loss 0.683495
[epoch8, step2851]: loss 0.502223
[epoch8, step2852]: loss 0.302664
[epoch8, step2853]: loss 0.365047
[epoch8, step2854]: loss 0.568813
[epoch8, step2855]: loss 0.402660
[epoch8, step2856]: loss 0.668727
[epoch8, step2857]: loss 0.512293
[epoch8, step2858]: loss 0.604411
[epoch8, step2859]: loss 0.374088
[epoch8, step2860]: loss 0.256170
[epoch8, step2861]: loss 0.675283
[epoch8, step2862]: loss 0.470535
[epoch8, step2863]: loss 0.685048
[epoch8, step2864]: loss 0.532347
[epoch8, step2865]: loss 0.523103
[epoch8, step2866]: loss 0.486284
[epoch8, step2867]: loss 0.423940
[epoch8, step2868]: loss 0.323275
[epoch8, step2869]: loss 0.560947
[epoch8, step2870]: loss 0.691640
[epoch8, step2871]: loss 0.291772
[epoch8, step2872]: loss 0.603274
[epoch8, step2873]: loss 0.700479
[epoch8, step2874]: loss 0.426344
[epoch8, step2875]: loss 0.556765
[epoch8, step2876]: loss 0.523811
[epoch8, step2877]: loss 0.568779
[epoch8, step2878]: loss 0.678637
[epoch8, step2879]: loss 0.414410
[epoch8, step2880]: loss 0.622293
[epoch8, step2881]: loss 0.734917
[epoch8, step2882]: loss 0.508188
[epoch8, step2883]: loss 0.557730
[epoch8, step2884]: loss 0.496017
[epoch8, step2885]: loss 0.347961
[epoch8, step2886]: loss 0.426594
[epoch8, step2887]: loss 0.677046
[epoch8, step2888]: loss 0.604924
[epoch8, step2889]: loss 0.554831
[epoch8, step2890]: loss 0.646034
[epoch8, step2891]: loss 0.405509
[epoch8, step2892]: loss 0.724375
[epoch8, step2893]: loss 0.487926
[epoch8, step2894]: loss 0.370137
[epoch8, step2895]: loss 0.604235
[epoch8, step2896]: loss 0.436071
[epoch8, step2897]: loss 0.300255
[epoch8, step2898]: loss 0.548939
[epoch8, step2899]: loss 0.545794
[epoch8, step2900]: loss 0.313542
[epoch8, step2901]: loss 0.304750
[epoch8, step2902]: loss 0.487016
[epoch8, step2903]: loss 0.524520
[epoch8, step2904]: loss 0.310983
[epoch8, step2905]: loss 0.296202
[epoch8, step2906]: loss 0.316070
[epoch8, step2907]: loss 0.613645
[epoch8, step2908]: loss 0.513454
[epoch8, step2909]: loss 0.530949
[epoch8, step2910]: loss 0.452548
[epoch8, step2911]: loss 0.588179
[epoch8, step2912]: loss 0.484673
[epoch8, step2913]: loss 0.529096
[epoch8, step2914]: loss 0.554562
[epoch8, step2915]: loss 0.729124
[epoch8, step2916]: loss 0.541466
[epoch8, step2917]: loss 0.713368
[epoch8, step2918]: loss 0.431037
[epoch8, step2919]: loss 0.563055
[epoch8, step2920]: loss 0.536604
[epoch8, step2921]: loss 0.644783
[epoch8, step2922]: loss 0.586272
[epoch8, step2923]: loss 0.589644
[epoch8, step2924]: loss 0.544435
[epoch8, step2925]: loss 0.458077
[epoch8, step2926]: loss 0.636665
[epoch8, step2927]: loss 0.790014
[epoch8, step2928]: loss 0.453502
[epoch8, step2929]: loss 0.566648
[epoch8, step2930]: loss 0.688472
[epoch8, step2931]: loss 0.628398
[epoch8, step2932]: loss 0.544693
[epoch8, step2933]: loss 0.531120
[epoch8, step2934]: loss 0.703138
[epoch8, step2935]: loss 0.643438
[epoch8, step2936]: loss 0.347671
[epoch8, step2937]: loss 0.469361
[epoch8, step2938]: loss 0.544449
[epoch8, step2939]: loss 0.558463
[epoch8, step2940]: loss 0.571531
[epoch8, step2941]: loss 0.600849
[epoch8, step2942]: loss 0.142701
[epoch8, step2943]: loss 0.486046
[epoch8, step2944]: loss 0.625608
[epoch8, step2945]: loss 0.586037
[epoch8, step2946]: loss 0.552550
[epoch8, step2947]: loss 0.224111
[epoch8, step2948]: loss 0.439375
[epoch8, step2949]: loss 0.438638
[epoch8, step2950]: loss 0.545833
[epoch8, step2951]: loss 0.391202
[epoch8, step2952]: loss 0.620151
[epoch8, step2953]: loss 0.748113
[epoch8, step2954]: loss 0.642647
[epoch8, step2955]: loss 0.293506
[epoch8, step2956]: loss 0.441872
[epoch8, step2957]: loss 0.465351
[epoch8, step2958]: loss 0.481789
[epoch8, step2959]: loss 0.355068
[epoch8, step2960]: loss 0.472316
[epoch8, step2961]: loss 0.649751
[epoch8, step2962]: loss 0.737038
[epoch8, step2963]: loss 0.595280
[epoch8, step2964]: loss 0.545721
[epoch8, step2965]: loss 0.527977
[epoch8, step2966]: loss 0.264523
[epoch8, step2967]: loss 0.513370
[epoch8, step2968]: loss 0.603746
[epoch8, step2969]: loss 0.429711
[epoch8, step2970]: loss 0.546573
[epoch8, step2971]: loss 0.679943
[epoch8, step2972]: loss 0.625639
[epoch8, step2973]: loss 0.365494
[epoch8, step2974]: loss 0.525654
[epoch8, step2975]: loss 0.485607
[epoch8, step2976]: loss 0.566820
[epoch8, step2977]: loss 0.536381
[epoch8, step2978]: loss 0.599728
[epoch8, step2979]: loss 0.424712
[epoch8, step2980]: loss 0.607716
[epoch8, step2981]: loss 0.466644
[epoch8, step2982]: loss 0.318736
[epoch8, step2983]: loss 0.622083
[epoch8, step2984]: loss 0.260729
[epoch8, step2985]: loss 0.625270
[epoch8, step2986]: loss 0.472607
[epoch8, step2987]: loss 0.510975
[epoch8, step2988]: loss 0.756777
[epoch8, step2989]: loss 0.540092
[epoch8, step2990]: loss 0.640515
[epoch8, step2991]: loss 0.442703
[epoch8, step2992]: loss 0.670240
[epoch8, step2993]: loss 0.468882
[epoch8, step2994]: loss 0.504325
[epoch8, step2995]: loss 0.477963
[epoch8, step2996]: loss 0.605942
[epoch8, step2997]: loss 0.568641
[epoch8, step2998]: loss 0.692944
[epoch8, step2999]: loss 0.509065
[epoch8, step3000]: loss 0.553506
[epoch8, step3001]: loss 0.289971
[epoch8, step3002]: loss 0.522490
[epoch8, step3003]: loss 0.388598
[epoch8, step3004]: loss 0.482202
[epoch8, step3005]: loss 0.556377
[epoch8, step3006]: loss 0.491413
[epoch8, step3007]: loss 0.565345
[epoch8, step3008]: loss 0.497209
[epoch8, step3009]: loss 0.434048
[epoch8, step3010]: loss 0.337865
[epoch8, step3011]: loss 0.430763
[epoch8, step3012]: loss 0.533375
[epoch8, step3013]: loss 0.693131
[epoch8, step3014]: loss 0.521400
[epoch8, step3015]: loss 0.573842
[epoch8, step3016]: loss 0.284578
[epoch8, step3017]: loss 0.668894
[epoch8, step3018]: loss 0.479313
[epoch8, step3019]: loss 0.477905
[epoch8, step3020]: loss 0.469964
[epoch8, step3021]: loss 0.321225
[epoch8, step3022]: loss 0.533143
[epoch8, step3023]: loss 0.482422
[epoch8, step3024]: loss 0.400634
[epoch8, step3025]: loss 0.463764
[epoch8, step3026]: loss 0.501057
[epoch8, step3027]: loss 0.570230
[epoch8, step3028]: loss 0.622502
[epoch8, step3029]: loss 0.436432
[epoch8, step3030]: loss 0.497358
[epoch8, step3031]: loss 0.533571
[epoch8, step3032]: loss 0.531351
[epoch8, step3033]: loss 0.551941
[epoch8, step3034]: loss 0.566377
[epoch8, step3035]: loss 0.523011
[epoch8, step3036]: loss 0.375725
[epoch8, step3037]: loss 0.695239
[epoch8, step3038]: loss 0.408394
[epoch8, step3039]: loss 0.657874
[epoch8, step3040]: loss 0.604771
[epoch8, step3041]: loss 0.584908
[epoch8, step3042]: loss 0.435270
[epoch8, step3043]: loss 0.797453
[epoch8, step3044]: loss 0.478861
[epoch8, step3045]: loss 0.587676
[epoch8, step3046]: loss 0.490752
[epoch8, step3047]: loss 0.611552
[epoch8, step3048]: loss 0.410187
[epoch8, step3049]: loss 0.346640
[epoch8, step3050]: loss 0.674438
[epoch8, step3051]: loss 0.416526
[epoch8, step3052]: loss 0.696118
[epoch8, step3053]: loss 0.587040
[epoch8, step3054]: loss 0.420619
[epoch8, step3055]: loss 0.377141
[epoch8, step3056]: loss 0.402658
[epoch8, step3057]: loss 0.400288
[epoch8, step3058]: loss 0.632600
[epoch8, step3059]: loss 0.635735
[epoch8, step3060]: loss 0.510675
[epoch8, step3061]: loss 0.567150
[epoch8, step3062]: loss 0.488337
[epoch8, step3063]: loss 0.859109
[epoch8, step3064]: loss 0.505893
[epoch8, step3065]: loss 0.364920
[epoch8, step3066]: loss 0.408859
[epoch8, step3067]: loss 0.469314
[epoch8, step3068]: loss 0.398800
[epoch8, step3069]: loss 0.338276
[epoch8, step3070]: loss 0.482817
[epoch8, step3071]: loss 0.539353
[epoch8, step3072]: loss 0.515741
[epoch8, step3073]: loss 0.534592
[epoch8, step3074]: loss 0.509833
[epoch8, step3075]: loss 0.384455
[epoch8, step3076]: loss 0.296047

[epoch8]: avg loss 0.296047

[epoch9, step1]: loss 0.566542
[epoch9, step2]: loss 0.463012
[epoch9, step3]: loss 0.440600
[epoch9, step4]: loss 0.436422
[epoch9, step5]: loss 0.464848
[epoch9, step6]: loss 0.572195
[epoch9, step7]: loss 0.665603
[epoch9, step8]: loss 0.371296
[epoch9, step9]: loss 0.448078
[epoch9, step10]: loss 0.393973
[epoch9, step11]: loss 0.584901
[epoch9, step12]: loss 0.586509
[epoch9, step13]: loss 0.575912
[epoch9, step14]: loss 0.334242
[epoch9, step15]: loss 0.546013
[epoch9, step16]: loss 0.317997
[epoch9, step17]: loss 0.623452
[epoch9, step18]: loss 0.592819
[epoch9, step19]: loss 0.593555
[epoch9, step20]: loss 0.705381
[epoch9, step21]: loss 0.546317
[epoch9, step22]: loss 0.619501
[epoch9, step23]: loss 0.523449
[epoch9, step24]: loss 0.505208
[epoch9, step25]: loss 0.699248
[epoch9, step26]: loss 0.222925
[epoch9, step27]: loss 0.703106
[epoch9, step28]: loss 0.370060
[epoch9, step29]: loss 0.451465
[epoch9, step30]: loss 0.567007
[epoch9, step31]: loss 0.551906
[epoch9, step32]: loss 0.448641
[epoch9, step33]: loss 0.658465
[epoch9, step34]: loss 0.418463
[epoch9, step35]: loss 0.521942
[epoch9, step36]: loss 0.489932
[epoch9, step37]: loss 0.460537
[epoch9, step38]: loss 0.394915
[epoch9, step39]: loss 0.522372
[epoch9, step40]: loss 0.571561
[epoch9, step41]: loss 0.501229
[epoch9, step42]: loss 0.369119
[epoch9, step43]: loss 0.585500
[epoch9, step44]: loss 0.663443
[epoch9, step45]: loss 0.573184
[epoch9, step46]: loss 0.584787
[epoch9, step47]: loss 0.679570
[epoch9, step48]: loss 0.787683
[epoch9, step49]: loss 0.545137
[epoch9, step50]: loss 0.522698
[epoch9, step51]: loss 0.492787
[epoch9, step52]: loss 0.502128
[epoch9, step53]: loss 0.409729
[epoch9, step54]: loss 0.537518
[epoch9, step55]: loss 0.558478
[epoch9, step56]: loss 0.614385
[epoch9, step57]: loss 0.562163
[epoch9, step58]: loss 0.185415
[epoch9, step59]: loss 0.596315
[epoch9, step60]: loss 0.668524
[epoch9, step61]: loss 0.722203
[epoch9, step62]: loss 0.492609
[epoch9, step63]: loss 0.531257
[epoch9, step64]: loss 0.527522
[epoch9, step65]: loss 0.460548
[epoch9, step66]: loss 0.378553
[epoch9, step67]: loss 0.524304
[epoch9, step68]: loss 0.612939
[epoch9, step69]: loss 0.562762
[epoch9, step70]: loss 0.598089
[epoch9, step71]: loss 0.491795
[epoch9, step72]: loss 0.537564
[epoch9, step73]: loss 0.370128
[epoch9, step74]: loss 0.492900
[epoch9, step75]: loss 0.378218
[epoch9, step76]: loss 0.518317
[epoch9, step77]: loss 0.486953
[epoch9, step78]: loss 0.404663
[epoch9, step79]: loss 0.777903
[epoch9, step80]: loss 0.509082
[epoch9, step81]: loss 0.517376
[epoch9, step82]: loss 0.623317
[epoch9, step83]: loss 0.591513
[epoch9, step84]: loss 0.702893
[epoch9, step85]: loss 0.609692
[epoch9, step86]: loss 0.689466
[epoch9, step87]: loss 0.642915
[epoch9, step88]: loss 0.417411
[epoch9, step89]: loss 0.392631
[epoch9, step90]: loss 0.377590
[epoch9, step91]: loss 0.505378
[epoch9, step92]: loss 0.464180
[epoch9, step93]: loss 0.394418
[epoch9, step94]: loss 0.486972
[epoch9, step95]: loss 0.155357
[epoch9, step96]: loss 0.623927
[epoch9, step97]: loss 0.329157
[epoch9, step98]: loss 0.535006
[epoch9, step99]: loss 0.239417
[epoch9, step100]: loss 0.724424
[epoch9, step101]: loss 0.559496
[epoch9, step102]: loss 0.446473
[epoch9, step103]: loss 0.597154
[epoch9, step104]: loss 0.356781
[epoch9, step105]: loss 0.599382
[epoch9, step106]: loss 0.596920
[epoch9, step107]: loss 0.596053
[epoch9, step108]: loss 0.473779
[epoch9, step109]: loss 0.716928
[epoch9, step110]: loss 0.684179
[epoch9, step111]: loss 0.534807
[epoch9, step112]: loss 0.267902
[epoch9, step113]: loss 0.606564
[epoch9, step114]: loss 0.446310
[epoch9, step115]: loss 0.474608
[epoch9, step116]: loss 0.548023
[epoch9, step117]: loss 0.529042
[epoch9, step118]: loss 0.580187
[epoch9, step119]: loss 0.701973
[epoch9, step120]: loss 0.686343
[epoch9, step121]: loss 0.427302
[epoch9, step122]: loss 0.613402
[epoch9, step123]: loss 0.773163
[epoch9, step124]: loss 0.633646
[epoch9, step125]: loss 0.362036
[epoch9, step126]: loss 0.580235
[epoch9, step127]: loss 0.510058
[epoch9, step128]: loss 0.514654
[epoch9, step129]: loss 0.609952
[epoch9, step130]: loss 0.721760
[epoch9, step131]: loss 0.437319
[epoch9, step132]: loss 0.545084
[epoch9, step133]: loss 0.272443
[epoch9, step134]: loss 0.482084
[epoch9, step135]: loss 0.382146
[epoch9, step136]: loss 0.609684
[epoch9, step137]: loss 0.530564
[epoch9, step138]: loss 0.347978
[epoch9, step139]: loss 0.479912
[epoch9, step140]: loss 0.736034
[epoch9, step141]: loss 0.537441
[epoch9, step142]: loss 0.493513
[epoch9, step143]: loss 0.506619
[epoch9, step144]: loss 0.393467
[epoch9, step145]: loss 0.226752
[epoch9, step146]: loss 0.621190
[epoch9, step147]: loss 0.443606
[epoch9, step148]: loss 0.532007
[epoch9, step149]: loss 0.592313
[epoch9, step150]: loss 0.342366
[epoch9, step151]: loss 0.499152
[epoch9, step152]: loss 0.666018
[epoch9, step153]: loss 0.535491
[epoch9, step154]: loss 0.521744
[epoch9, step155]: loss 0.589797
[epoch9, step156]: loss 0.446051
[epoch9, step157]: loss 0.518757
[epoch9, step158]: loss 0.748350
[epoch9, step159]: loss 0.651119
[epoch9, step160]: loss 0.373686
[epoch9, step161]: loss 0.276666
[epoch9, step162]: loss 0.754763
[epoch9, step163]: loss 0.685607
[epoch9, step164]: loss 0.618598
[epoch9, step165]: loss 0.449902
[epoch9, step166]: loss 0.550333
[epoch9, step167]: loss 0.528234
[epoch9, step168]: loss 0.517189
[epoch9, step169]: loss 0.440967
[epoch9, step170]: loss 0.442953
[epoch9, step171]: loss 0.293259
[epoch9, step172]: loss 0.730766
[epoch9, step173]: loss 0.299012
[epoch9, step174]: loss 0.698948
[epoch9, step175]: loss 0.537419
[epoch9, step176]: loss 0.529761
[epoch9, step177]: loss 0.665154
[epoch9, step178]: loss 0.470712
[epoch9, step179]: loss 0.593915
[epoch9, step180]: loss 0.434072
[epoch9, step181]: loss 0.483014
[epoch9, step182]: loss 0.599191
[epoch9, step183]: loss 0.608217
[epoch9, step184]: loss 0.491439
[epoch9, step185]: loss 0.685729
[epoch9, step186]: loss 0.593500
[epoch9, step187]: loss 0.746520
[epoch9, step188]: loss 0.586684
[epoch9, step189]: loss 0.495754
[epoch9, step190]: loss 0.583357
[epoch9, step191]: loss 0.386322
[epoch9, step192]: loss 0.667195
[epoch9, step193]: loss 0.447242
[epoch9, step194]: loss 0.621215
[epoch9, step195]: loss 0.500588
[epoch9, step196]: loss 0.595693
[epoch9, step197]: loss 0.463615
[epoch9, step198]: loss 0.582864
[epoch9, step199]: loss 0.548667
[epoch9, step200]: loss 0.559923
[epoch9, step201]: loss 0.605785
[epoch9, step202]: loss 0.702928
[epoch9, step203]: loss 0.344599
[epoch9, step204]: loss 0.520258
[epoch9, step205]: loss 0.406380
[epoch9, step206]: loss 0.466162
[epoch9, step207]: loss 0.681792
[epoch9, step208]: loss 0.730649
[epoch9, step209]: loss 0.522590
[epoch9, step210]: loss 0.563031
[epoch9, step211]: loss 0.521526
[epoch9, step212]: loss 0.455226
[epoch9, step213]: loss 0.605819
[epoch9, step214]: loss 0.442769
[epoch9, step215]: loss 0.375617
[epoch9, step216]: loss 0.445839
[epoch9, step217]: loss 0.465280
[epoch9, step218]: loss 0.338600
[epoch9, step219]: loss 0.363990
[epoch9, step220]: loss 0.622154
[epoch9, step221]: loss 0.285968
[epoch9, step222]: loss 0.449885
[epoch9, step223]: loss 0.472956
[epoch9, step224]: loss 0.638737
[epoch9, step225]: loss 0.476259
[epoch9, step226]: loss 0.434109
[epoch9, step227]: loss 0.340831
[epoch9, step228]: loss 0.697173
[epoch9, step229]: loss 0.371019
[epoch9, step230]: loss 0.335449
[epoch9, step231]: loss 0.642350
[epoch9, step232]: loss 0.538395
[epoch9, step233]: loss 0.437366
[epoch9, step234]: loss 0.478389
[epoch9, step235]: loss 0.525625
[epoch9, step236]: loss 0.594594
[epoch9, step237]: loss 0.469542
[epoch9, step238]: loss 0.618706
[epoch9, step239]: loss 0.511893
[epoch9, step240]: loss 0.678426
[epoch9, step241]: loss 0.516534
[epoch9, step242]: loss 0.427784
[epoch9, step243]: loss 0.350046
[epoch9, step244]: loss 0.386893
[epoch9, step245]: loss 0.564804
[epoch9, step246]: loss 0.635616
[epoch9, step247]: loss 0.655434
[epoch9, step248]: loss 0.633214
[epoch9, step249]: loss 0.496582
[epoch9, step250]: loss 0.542598
[epoch9, step251]: loss 0.457607
[epoch9, step252]: loss 0.371613
[epoch9, step253]: loss 0.589027
[epoch9, step254]: loss 0.608595
[epoch9, step255]: loss 0.459367
[epoch9, step256]: loss 0.663589
[epoch9, step257]: loss 0.573358
[epoch9, step258]: loss 0.623420
[epoch9, step259]: loss 0.536965
[epoch9, step260]: loss 0.507376
[epoch9, step261]: loss 0.553870
[epoch9, step262]: loss 0.616274
[epoch9, step263]: loss 0.806895
[epoch9, step264]: loss 0.461816
[epoch9, step265]: loss 0.636440
[epoch9, step266]: loss 0.491155
[epoch9, step267]: loss 0.490273
[epoch9, step268]: loss 0.377532
[epoch9, step269]: loss 0.583350
[epoch9, step270]: loss 0.629290
[epoch9, step271]: loss 0.588441
[epoch9, step272]: loss 0.481673
[epoch9, step273]: loss 0.592178
[epoch9, step274]: loss 0.550726
[epoch9, step275]: loss 0.493087
[epoch9, step276]: loss 0.571181
[epoch9, step277]: loss 0.439652
[epoch9, step278]: loss 0.822507
[epoch9, step279]: loss 0.485398
[epoch9, step280]: loss 0.575819
[epoch9, step281]: loss 0.495034
[epoch9, step282]: loss 0.337360
[epoch9, step283]: loss 0.459730
[epoch9, step284]: loss 0.410329
[epoch9, step285]: loss 0.473930
[epoch9, step286]: loss 0.511810
[epoch9, step287]: loss 0.315974
[epoch9, step288]: loss 0.674755
[epoch9, step289]: loss 0.359627
[epoch9, step290]: loss 0.478385
[epoch9, step291]: loss 0.437818
[epoch9, step292]: loss 0.276691
[epoch9, step293]: loss 0.772276
[epoch9, step294]: loss 0.756593
[epoch9, step295]: loss 0.505366
[epoch9, step296]: loss 0.320639
[epoch9, step297]: loss 0.507652
[epoch9, step298]: loss 0.767191
[epoch9, step299]: loss 0.451112
[epoch9, step300]: loss 0.475515
[epoch9, step301]: loss 0.410422
[epoch9, step302]: loss 0.571971
[epoch9, step303]: loss 0.336550
[epoch9, step304]: loss 0.667654
[epoch9, step305]: loss 0.494008
[epoch9, step306]: loss 0.666271
[epoch9, step307]: loss 0.494108
[epoch9, step308]: loss 0.485528
[epoch9, step309]: loss 0.389011
[epoch9, step310]: loss 0.464520
[epoch9, step311]: loss 0.563851
[epoch9, step312]: loss 0.602477
[epoch9, step313]: loss 0.458959
[epoch9, step314]: loss 0.565812
[epoch9, step315]: loss 0.506716
[epoch9, step316]: loss 0.683477
[epoch9, step317]: loss 0.401706
[epoch9, step318]: loss 0.422632
[epoch9, step319]: loss 0.709613
[epoch9, step320]: loss 0.148867
[epoch9, step321]: loss 0.401049
[epoch9, step322]: loss 0.473344
[epoch9, step323]: loss 0.529407
[epoch9, step324]: loss 0.451322
[epoch9, step325]: loss 0.431217
[epoch9, step326]: loss 0.591990
[epoch9, step327]: loss 0.712676
[epoch9, step328]: loss 0.533591
[epoch9, step329]: loss 0.452109
[epoch9, step330]: loss 0.526144
[epoch9, step331]: loss 0.694086
[epoch9, step332]: loss 0.295645
[epoch9, step333]: loss 0.421358
[epoch9, step334]: loss 0.488315
[epoch9, step335]: loss 0.320203
[epoch9, step336]: loss 0.542174
[epoch9, step337]: loss 0.354028
[epoch9, step338]: loss 0.299909
[epoch9, step339]: loss 0.371169
[epoch9, step340]: loss 0.584563
[epoch9, step341]: loss 0.518598
[epoch9, step342]: loss 0.407723
[epoch9, step343]: loss 0.417584
[epoch9, step344]: loss 0.607642
[epoch9, step345]: loss 0.469442
[epoch9, step346]: loss 0.310029
[epoch9, step347]: loss 0.386622
[epoch9, step348]: loss 0.497015
[epoch9, step349]: loss 0.730535
[epoch9, step350]: loss 0.465140
[epoch9, step351]: loss 0.683767
[epoch9, step352]: loss 0.435579
[epoch9, step353]: loss 0.481776
[epoch9, step354]: loss 0.718450
[epoch9, step355]: loss 0.545717
[epoch9, step356]: loss 0.351112
[epoch9, step357]: loss 0.491874
[epoch9, step358]: loss 0.485479
[epoch9, step359]: loss 0.402747
[epoch9, step360]: loss 0.571080
[epoch9, step361]: loss 0.337076
[epoch9, step362]: loss 0.315197
[epoch9, step363]: loss 0.590579
[epoch9, step364]: loss 0.509694
[epoch9, step365]: loss 0.448207
[epoch9, step366]: loss 0.639835
[epoch9, step367]: loss 0.537495
[epoch9, step368]: loss 0.528849
[epoch9, step369]: loss 0.401319
[epoch9, step370]: loss 0.777447
[epoch9, step371]: loss 0.408197
[epoch9, step372]: loss 0.533935
[epoch9, step373]: loss 0.741708
[epoch9, step374]: loss 0.701663
[epoch9, step375]: loss 0.380178
[epoch9, step376]: loss 0.619251
[epoch9, step377]: loss 0.617514
[epoch9, step378]: loss 0.451143
[epoch9, step379]: loss 0.346377
[epoch9, step380]: loss 0.340380
[epoch9, step381]: loss 0.655338
[epoch9, step382]: loss 0.728578
[epoch9, step383]: loss 0.363856
[epoch9, step384]: loss 0.374052
[epoch9, step385]: loss 0.536034
[epoch9, step386]: loss 0.547704
[epoch9, step387]: loss 0.661539
[epoch9, step388]: loss 0.301878
[epoch9, step389]: loss 0.772098
[epoch9, step390]: loss 0.467806
[epoch9, step391]: loss 0.694665
[epoch9, step392]: loss 0.491713
[epoch9, step393]: loss 0.543125
[epoch9, step394]: loss 0.394473
[epoch9, step395]: loss 0.400094
[epoch9, step396]: loss 0.451105
[epoch9, step397]: loss 0.552467
[epoch9, step398]: loss 0.657093
[epoch9, step399]: loss 0.238918
[epoch9, step400]: loss 0.287347
[epoch9, step401]: loss 0.733811
[epoch9, step402]: loss 0.737798
[epoch9, step403]: loss 0.463916
[epoch9, step404]: loss 0.756778
[epoch9, step405]: loss 0.839097
[epoch9, step406]: loss 0.395483
[epoch9, step407]: loss 0.584804
[epoch9, step408]: loss 0.458953
[epoch9, step409]: loss 0.502191
[epoch9, step410]: loss 0.525261
[epoch9, step411]: loss 0.458853
[epoch9, step412]: loss 0.265037
[epoch9, step413]: loss 0.661812
[epoch9, step414]: loss 0.440578
[epoch9, step415]: loss 0.263762
[epoch9, step416]: loss 0.352253
[epoch9, step417]: loss 0.445828
[epoch9, step418]: loss 0.646401
[epoch9, step419]: loss 0.229745
[epoch9, step420]: loss 0.378380
[epoch9, step421]: loss 0.471659
[epoch9, step422]: loss 0.787714
[epoch9, step423]: loss 0.464863
[epoch9, step424]: loss 0.662703
[epoch9, step425]: loss 0.712250
[epoch9, step426]: loss 0.500639
[epoch9, step427]: loss 0.497096
[epoch9, step428]: loss 0.493601
[epoch9, step429]: loss 0.768011
[epoch9, step430]: loss 0.585954
[epoch9, step431]: loss 0.525429
[epoch9, step432]: loss 0.525321
[epoch9, step433]: loss 0.330668
[epoch9, step434]: loss 0.471537
[epoch9, step435]: loss 0.507697
[epoch9, step436]: loss 0.496097
[epoch9, step437]: loss 0.612751
[epoch9, step438]: loss 0.579925
[epoch9, step439]: loss 0.624306
[epoch9, step440]: loss 0.577929
[epoch9, step441]: loss 0.541135
[epoch9, step442]: loss 0.521166
[epoch9, step443]: loss 0.571098
[epoch9, step444]: loss 0.248235
[epoch9, step445]: loss 0.630406
[epoch9, step446]: loss 0.474043
[epoch9, step447]: loss 0.416964
[epoch9, step448]: loss 0.245619
[epoch9, step449]: loss 0.596254
[epoch9, step450]: loss 0.583783
[epoch9, step451]: loss 0.520084
[epoch9, step452]: loss 0.426574
[epoch9, step453]: loss 0.237722
[epoch9, step454]: loss 0.261528
[epoch9, step455]: loss 0.404203
[epoch9, step456]: loss 0.534752
[epoch9, step457]: loss 0.617900
[epoch9, step458]: loss 0.509102
[epoch9, step459]: loss 0.510415
[epoch9, step460]: loss 0.366566
[epoch9, step461]: loss 0.659713
[epoch9, step462]: loss 0.490112
[epoch9, step463]: loss 0.569408
[epoch9, step464]: loss 0.464256
[epoch9, step465]: loss 0.157790
[epoch9, step466]: loss 0.722298
[epoch9, step467]: loss 0.578278
[epoch9, step468]: loss 0.598768
[epoch9, step469]: loss 0.481192
[epoch9, step470]: loss 0.389043
[epoch9, step471]: loss 0.472199
[epoch9, step472]: loss 0.612280
[epoch9, step473]: loss 0.617984
[epoch9, step474]: loss 0.527190
[epoch9, step475]: loss 0.644290
[epoch9, step476]: loss 0.410046
[epoch9, step477]: loss 0.609138
[epoch9, step478]: loss 0.403024
[epoch9, step479]: loss 0.575525
[epoch9, step480]: loss 0.247856
[epoch9, step481]: loss 0.631825
[epoch9, step482]: loss 0.416665
[epoch9, step483]: loss 0.675863
[epoch9, step484]: loss 0.647441
[epoch9, step485]: loss 0.595353
[epoch9, step486]: loss 0.496193
[epoch9, step487]: loss 0.314641
[epoch9, step488]: loss 0.474052
[epoch9, step489]: loss 0.452810
[epoch9, step490]: loss 0.478573
[epoch9, step491]: loss 0.369900
[epoch9, step492]: loss 0.614634
[epoch9, step493]: loss 0.350446
[epoch9, step494]: loss 0.586612
[epoch9, step495]: loss 0.649379
[epoch9, step496]: loss 0.355517
[epoch9, step497]: loss 0.580283
[epoch9, step498]: loss 0.297067
[epoch9, step499]: loss 0.536276
[epoch9, step500]: loss 0.570277
[epoch9, step501]: loss 0.434731
[epoch9, step502]: loss 0.523342
[epoch9, step503]: loss 0.344385
[epoch9, step504]: loss 0.622195
[epoch9, step505]: loss 0.622062
[epoch9, step506]: loss 0.681038
[epoch9, step507]: loss 0.636252
[epoch9, step508]: loss 0.569643
[epoch9, step509]: loss 0.667297
[epoch9, step510]: loss 0.484954
[epoch9, step511]: loss 0.572810
[epoch9, step512]: loss 0.631158
[epoch9, step513]: loss 0.499796
[epoch9, step514]: loss 0.319276
[epoch9, step515]: loss 0.574568
[epoch9, step516]: loss 0.545798
[epoch9, step517]: loss 0.617924
[epoch9, step518]: loss 0.306749
[epoch9, step519]: loss 0.765651
[epoch9, step520]: loss 0.588845
[epoch9, step521]: loss 0.234567
[epoch9, step522]: loss 0.718775
[epoch9, step523]: loss 0.620468
[epoch9, step524]: loss 0.610449
[epoch9, step525]: loss 0.356627
[epoch9, step526]: loss 0.241309
[epoch9, step527]: loss 0.424291
[epoch9, step528]: loss 0.527239
[epoch9, step529]: loss 0.517418
[epoch9, step530]: loss 0.595281
[epoch9, step531]: loss 0.543880
[epoch9, step532]: loss 0.572246
[epoch9, step533]: loss 0.659877
[epoch9, step534]: loss 0.528711
[epoch9, step535]: loss 0.546262
[epoch9, step536]: loss 0.505675
[epoch9, step537]: loss 0.621237
[epoch9, step538]: loss 0.510146
[epoch9, step539]: loss 0.451973
[epoch9, step540]: loss 0.392463
[epoch9, step541]: loss 0.662900
[epoch9, step542]: loss 0.352791
[epoch9, step543]: loss 0.640911
[epoch9, step544]: loss 0.528878
[epoch9, step545]: loss 0.218927
[epoch9, step546]: loss 0.554432
[epoch9, step547]: loss 0.645329
[epoch9, step548]: loss 0.498371
[epoch9, step549]: loss 0.481657
[epoch9, step550]: loss 0.369734
[epoch9, step551]: loss 0.450574
[epoch9, step552]: loss 0.564264
[epoch9, step553]: loss 0.670138
[epoch9, step554]: loss 0.280715
[epoch9, step555]: loss 0.588916
[epoch9, step556]: loss 0.354944
[epoch9, step557]: loss 0.724028
[epoch9, step558]: loss 0.345121
[epoch9, step559]: loss 0.195385
[epoch9, step560]: loss 0.612135
[epoch9, step561]: loss 0.678721
[epoch9, step562]: loss 0.146034
[epoch9, step563]: loss 0.585150
[epoch9, step564]: loss 0.685444
[epoch9, step565]: loss 0.655095
[epoch9, step566]: loss 0.651352
[epoch9, step567]: loss 0.707248
[epoch9, step568]: loss 0.276855
[epoch9, step569]: loss 0.585385
[epoch9, step570]: loss 0.606002
[epoch9, step571]: loss 0.640801
[epoch9, step572]: loss 0.355397
[epoch9, step573]: loss 0.585042
[epoch9, step574]: loss 0.619968
[epoch9, step575]: loss 0.558990
[epoch9, step576]: loss 0.710675
[epoch9, step577]: loss 0.444045
[epoch9, step578]: loss 0.502476
[epoch9, step579]: loss 0.295037
[epoch9, step580]: loss 0.552391
[epoch9, step581]: loss 0.595620
[epoch9, step582]: loss 0.332474
[epoch9, step583]: loss 0.371281
[epoch9, step584]: loss 0.604344
[epoch9, step585]: loss 0.484029
[epoch9, step586]: loss 0.655925
[epoch9, step587]: loss 0.340776
[epoch9, step588]: loss 0.570572
[epoch9, step589]: loss 0.557323
[epoch9, step590]: loss 0.292476
[epoch9, step591]: loss 0.726474
[epoch9, step592]: loss 0.471862
[epoch9, step593]: loss 0.649598
[epoch9, step594]: loss 0.450190
[epoch9, step595]: loss 0.473939
[epoch9, step596]: loss 0.544780
[epoch9, step597]: loss 0.496136
[epoch9, step598]: loss 0.463509
[epoch9, step599]: loss 0.424381
[epoch9, step600]: loss 0.582659
[epoch9, step601]: loss 0.435494
[epoch9, step602]: loss 0.382991
[epoch9, step603]: loss 0.496149
[epoch9, step604]: loss 0.544754
[epoch9, step605]: loss 0.494505
[epoch9, step606]: loss 0.569234
[epoch9, step607]: loss 0.212067
[epoch9, step608]: loss 0.322883
[epoch9, step609]: loss 0.464626
[epoch9, step610]: loss 0.539907
[epoch9, step611]: loss 0.701373
[epoch9, step612]: loss 0.455228
[epoch9, step613]: loss 0.613532
[epoch9, step614]: loss 0.556959
[epoch9, step615]: loss 0.482499
[epoch9, step616]: loss 0.579692
[epoch9, step617]: loss 0.644818
[epoch9, step618]: loss 0.667301
[epoch9, step619]: loss 0.362612
[epoch9, step620]: loss 0.523399
[epoch9, step621]: loss 0.516248
[epoch9, step622]: loss 0.475879
[epoch9, step623]: loss 0.590388
[epoch9, step624]: loss 0.676794
[epoch9, step625]: loss 0.514577
[epoch9, step626]: loss 0.510260
[epoch9, step627]: loss 0.520141
[epoch9, step628]: loss 0.405659
[epoch9, step629]: loss 0.702403
[epoch9, step630]: loss 0.551367
[epoch9, step631]: loss 0.154106
[epoch9, step632]: loss 0.660499
[epoch9, step633]: loss 0.263779
[epoch9, step634]: loss 0.450404
[epoch9, step635]: loss 0.527427
[epoch9, step636]: loss 0.395615
[epoch9, step637]: loss 0.528686
[epoch9, step638]: loss 0.637135
[epoch9, step639]: loss 0.430472
[epoch9, step640]: loss 0.459196
[epoch9, step641]: loss 0.602072
[epoch9, step642]: loss 0.711649
[epoch9, step643]: loss 0.384247
[epoch9, step644]: loss 0.506076
[epoch9, step645]: loss 0.650973
[epoch9, step646]: loss 0.681656
[epoch9, step647]: loss 0.546091
[epoch9, step648]: loss 0.665827
[epoch9, step649]: loss 0.626100
[epoch9, step650]: loss 0.318586
[epoch9, step651]: loss 0.689873
[epoch9, step652]: loss 0.531864
[epoch9, step653]: loss 0.490938
[epoch9, step654]: loss 0.713264
[epoch9, step655]: loss 0.352564
[epoch9, step656]: loss 0.535438
[epoch9, step657]: loss 0.557899
[epoch9, step658]: loss 0.548848
[epoch9, step659]: loss 0.463713
[epoch9, step660]: loss 0.574702
[epoch9, step661]: loss 0.390366
[epoch9, step662]: loss 0.463895
[epoch9, step663]: loss 0.713942
[epoch9, step664]: loss 0.622197
[epoch9, step665]: loss 0.572978
[epoch9, step666]: loss 0.610415
[epoch9, step667]: loss 0.463590
[epoch9, step668]: loss 0.301275
[epoch9, step669]: loss 0.397792
[epoch9, step670]: loss 0.623187
[epoch9, step671]: loss 0.527070
[epoch9, step672]: loss 0.343026
[epoch9, step673]: loss 0.449169
[epoch9, step674]: loss 0.552040
[epoch9, step675]: loss 0.699447
[epoch9, step676]: loss 0.459459
[epoch9, step677]: loss 0.550005
[epoch9, step678]: loss 0.335771
[epoch9, step679]: loss 0.474772
[epoch9, step680]: loss 0.558329
[epoch9, step681]: loss 0.631283
[epoch9, step682]: loss 0.558803
[epoch9, step683]: loss 0.640049
[epoch9, step684]: loss 0.397101
[epoch9, step685]: loss 0.525159
[epoch9, step686]: loss 0.447511
[epoch9, step687]: loss 0.621561
[epoch9, step688]: loss 0.657119
[epoch9, step689]: loss 0.654837
[epoch9, step690]: loss 0.476208
[epoch9, step691]: loss 0.589544
[epoch9, step692]: loss 0.474150
[epoch9, step693]: loss 0.709096
[epoch9, step694]: loss 0.196612
[epoch9, step695]: loss 0.588048
[epoch9, step696]: loss 0.544750
[epoch9, step697]: loss 0.304991
[epoch9, step698]: loss 0.356124
[epoch9, step699]: loss 0.616920
[epoch9, step700]: loss 0.708937
[epoch9, step701]: loss 0.559553
[epoch9, step702]: loss 0.442593
[epoch9, step703]: loss 0.503233
[epoch9, step704]: loss 0.605102
[epoch9, step705]: loss 0.399002
[epoch9, step706]: loss 0.568613
[epoch9, step707]: loss 0.584065
[epoch9, step708]: loss 0.564391
[epoch9, step709]: loss 0.376444
[epoch9, step710]: loss 0.566808
[epoch9, step711]: loss 0.603668
[epoch9, step712]: loss 0.756057
[epoch9, step713]: loss 0.558206
[epoch9, step714]: loss 0.513642
[epoch9, step715]: loss 0.566591
[epoch9, step716]: loss 0.615260
[epoch9, step717]: loss 0.398498
[epoch9, step718]: loss 0.740085
[epoch9, step719]: loss 0.615899
[epoch9, step720]: loss 0.547004
[epoch9, step721]: loss 0.687151
[epoch9, step722]: loss 0.547532
[epoch9, step723]: loss 0.760699
[epoch9, step724]: loss 0.465025
[epoch9, step725]: loss 0.375808
[epoch9, step726]: loss 0.558263
[epoch9, step727]: loss 0.514798
[epoch9, step728]: loss 0.533944
[epoch9, step729]: loss 0.380772
[epoch9, step730]: loss 0.645870
[epoch9, step731]: loss 0.554963
[epoch9, step732]: loss 0.730242
[epoch9, step733]: loss 0.598160
[epoch9, step734]: loss 0.423829
[epoch9, step735]: loss 0.664620
[epoch9, step736]: loss 0.409922
[epoch9, step737]: loss 0.667065
[epoch9, step738]: loss 0.435086
[epoch9, step739]: loss 0.607324
[epoch9, step740]: loss 0.611556
[epoch9, step741]: loss 0.482625
[epoch9, step742]: loss 0.350139
[epoch9, step743]: loss 0.347580
[epoch9, step744]: loss 0.266122
[epoch9, step745]: loss 0.700380
[epoch9, step746]: loss 0.551094
[epoch9, step747]: loss 0.485096
[epoch9, step748]: loss 0.673630
[epoch9, step749]: loss 0.478089
[epoch9, step750]: loss 0.530088
[epoch9, step751]: loss 0.577519
[epoch9, step752]: loss 0.528517
[epoch9, step753]: loss 0.562314
[epoch9, step754]: loss 0.124961
[epoch9, step755]: loss 0.622367
[epoch9, step756]: loss 0.430297
[epoch9, step757]: loss 0.243439
[epoch9, step758]: loss 0.509231
[epoch9, step759]: loss 0.511886
[epoch9, step760]: loss 0.286563
[epoch9, step761]: loss 0.515510
[epoch9, step762]: loss 0.665604
[epoch9, step763]: loss 0.600680
[epoch9, step764]: loss 0.679638
[epoch9, step765]: loss 0.469178
[epoch9, step766]: loss 0.386037
[epoch9, step767]: loss 0.444862
[epoch9, step768]: loss 0.408279
[epoch9, step769]: loss 0.702011
[epoch9, step770]: loss 0.399423
[epoch9, step771]: loss 0.402275
[epoch9, step772]: loss 0.505725
[epoch9, step773]: loss 0.669803
[epoch9, step774]: loss 0.427801
[epoch9, step775]: loss 0.503957
[epoch9, step776]: loss 0.566226
[epoch9, step777]: loss 0.316343
[epoch9, step778]: loss 0.518882
[epoch9, step779]: loss 0.622243
[epoch9, step780]: loss 0.583907
[epoch9, step781]: loss 0.561782
[epoch9, step782]: loss 0.526519
[epoch9, step783]: loss 0.649163
[epoch9, step784]: loss 0.563010
[epoch9, step785]: loss 0.699288
[epoch9, step786]: loss 0.472697
[epoch9, step787]: loss 0.566450
[epoch9, step788]: loss 0.566418
[epoch9, step789]: loss 0.386353
[epoch9, step790]: loss 0.445904
[epoch9, step791]: loss 0.482116
[epoch9, step792]: loss 0.427284
[epoch9, step793]: loss 0.260654
[epoch9, step794]: loss 0.544838
[epoch9, step795]: loss 0.291743
[epoch9, step796]: loss 0.385885
[epoch9, step797]: loss 0.698979
[epoch9, step798]: loss 0.489495
[epoch9, step799]: loss 0.489304
[epoch9, step800]: loss 0.544564
[epoch9, step801]: loss 0.487400
[epoch9, step802]: loss 0.398331
[epoch9, step803]: loss 0.280245
[epoch9, step804]: loss 0.430910
[epoch9, step805]: loss 0.631007
[epoch9, step806]: loss 0.485597
[epoch9, step807]: loss 0.402369
[epoch9, step808]: loss 0.434960
[epoch9, step809]: loss 0.358455
[epoch9, step810]: loss 0.498118
[epoch9, step811]: loss 0.556064
[epoch9, step812]: loss 0.606762
[epoch9, step813]: loss 0.494127
[epoch9, step814]: loss 0.464586
[epoch9, step815]: loss 0.669849
[epoch9, step816]: loss 0.404545
[epoch9, step817]: loss 0.547947
[epoch9, step818]: loss 0.302033
[epoch9, step819]: loss 0.533687
[epoch9, step820]: loss 0.483854
[epoch9, step821]: loss 0.523927
[epoch9, step822]: loss 0.754612
[epoch9, step823]: loss 0.861240
[epoch9, step824]: loss 0.370135
[epoch9, step825]: loss 0.321471
[epoch9, step826]: loss 0.634609
[epoch9, step827]: loss 0.518210
[epoch9, step828]: loss 0.424627
[epoch9, step829]: loss 0.572251
[epoch9, step830]: loss 0.442300
[epoch9, step831]: loss 0.550362
[epoch9, step832]: loss 0.674968
[epoch9, step833]: loss 0.251496
[epoch9, step834]: loss 0.496603
[epoch9, step835]: loss 0.477540
[epoch9, step836]: loss 0.683770
[epoch9, step837]: loss 0.278867
[epoch9, step838]: loss 0.433410
[epoch9, step839]: loss 0.325846
[epoch9, step840]: loss 0.360996
[epoch9, step841]: loss 0.582706
[epoch9, step842]: loss 0.375060
[epoch9, step843]: loss 0.445129
[epoch9, step844]: loss 0.517938
[epoch9, step845]: loss 0.620548
[epoch9, step846]: loss 0.317502
[epoch9, step847]: loss 0.559190
[epoch9, step848]: loss 0.452304
[epoch9, step849]: loss 0.627007
[epoch9, step850]: loss 0.571566
[epoch9, step851]: loss 0.593080
[epoch9, step852]: loss 0.553774
[epoch9, step853]: loss 0.662521
[epoch9, step854]: loss 0.362557
[epoch9, step855]: loss 0.595588
[epoch9, step856]: loss 0.463290
[epoch9, step857]: loss 0.617336
[epoch9, step858]: loss 0.572837
[epoch9, step859]: loss 0.411374
[epoch9, step860]: loss 0.392275
[epoch9, step861]: loss 0.590502
[epoch9, step862]: loss 0.623743
[epoch9, step863]: loss 0.658350
[epoch9, step864]: loss 0.746472
[epoch9, step865]: loss 0.747790
[epoch9, step866]: loss 0.666020
[epoch9, step867]: loss 0.663867
[epoch9, step868]: loss 0.487153
[epoch9, step869]: loss 0.387119
[epoch9, step870]: loss 0.523653
[epoch9, step871]: loss 0.690531
[epoch9, step872]: loss 0.534450
[epoch9, step873]: loss 0.216345
[epoch9, step874]: loss 0.518720
[epoch9, step875]: loss 0.683936
[epoch9, step876]: loss 0.648270
[epoch9, step877]: loss 0.719393
[epoch9, step878]: loss 0.597842
[epoch9, step879]: loss 0.351764
[epoch9, step880]: loss 0.545271
[epoch9, step881]: loss 0.672393
[epoch9, step882]: loss 0.486092
[epoch9, step883]: loss 0.569763
[epoch9, step884]: loss 0.446272
[epoch9, step885]: loss 0.630981
[epoch9, step886]: loss 0.645555
[epoch9, step887]: loss 0.583732
[epoch9, step888]: loss 0.520017
[epoch9, step889]: loss 0.521445
[epoch9, step890]: loss 0.432688
[epoch9, step891]: loss 0.449133
[epoch9, step892]: loss 0.324452
[epoch9, step893]: loss 0.699051
[epoch9, step894]: loss 0.601688
[epoch9, step895]: loss 0.634349
[epoch9, step896]: loss 0.625999
[epoch9, step897]: loss 0.690982
[epoch9, step898]: loss 0.549315
[epoch9, step899]: loss 0.670400
[epoch9, step900]: loss 0.382294
[epoch9, step901]: loss 0.561284
[epoch9, step902]: loss 0.331114
[epoch9, step903]: loss 0.398073
[epoch9, step904]: loss 0.674390
[epoch9, step905]: loss 0.557586
[epoch9, step906]: loss 0.344263
[epoch9, step907]: loss 0.372500
[epoch9, step908]: loss 0.428505
[epoch9, step909]: loss 0.492732
[epoch9, step910]: loss 0.645635
[epoch9, step911]: loss 0.274185
[epoch9, step912]: loss 0.640486
[epoch9, step913]: loss 0.457363
[epoch9, step914]: loss 0.614398
[epoch9, step915]: loss 0.525684
[epoch9, step916]: loss 0.467143
[epoch9, step917]: loss 0.650086
[epoch9, step918]: loss 0.398144
[epoch9, step919]: loss 0.293638
[epoch9, step920]: loss 0.633117
[epoch9, step921]: loss 0.525552
[epoch9, step922]: loss 0.443306
[epoch9, step923]: loss 0.550283
[epoch9, step924]: loss 0.571340
[epoch9, step925]: loss 0.768914
[epoch9, step926]: loss 0.528575
[epoch9, step927]: loss 0.471042
[epoch9, step928]: loss 0.517553
[epoch9, step929]: loss 0.545049
[epoch9, step930]: loss 0.346895
[epoch9, step931]: loss 0.588695
[epoch9, step932]: loss 0.607328
[epoch9, step933]: loss 0.615310
[epoch9, step934]: loss 0.341609
[epoch9, step935]: loss 0.341352
[epoch9, step936]: loss 0.723301
[epoch9, step937]: loss 0.418473
[epoch9, step938]: loss 0.753082
[epoch9, step939]: loss 0.454110
[epoch9, step940]: loss 0.499529
[epoch9, step941]: loss 0.533507
[epoch9, step942]: loss 0.713359
[epoch9, step943]: loss 0.365851
[epoch9, step944]: loss 0.341511
[epoch9, step945]: loss 0.408838
[epoch9, step946]: loss 0.293707
[epoch9, step947]: loss 0.516501
[epoch9, step948]: loss 0.670932
[epoch9, step949]: loss 0.529616
[epoch9, step950]: loss 0.414537
[epoch9, step951]: loss 0.516794
[epoch9, step952]: loss 0.399057
[epoch9, step953]: loss 0.637542
[epoch9, step954]: loss 0.464566
[epoch9, step955]: loss 0.607376
[epoch9, step956]: loss 0.462590
[epoch9, step957]: loss 0.403902
[epoch9, step958]: loss 0.495956
[epoch9, step959]: loss 0.401424
[epoch9, step960]: loss 0.432291
[epoch9, step961]: loss 0.555953
[epoch9, step962]: loss 0.453497
[epoch9, step963]: loss 0.682622
[epoch9, step964]: loss 0.360556
[epoch9, step965]: loss 0.498236
[epoch9, step966]: loss 0.487569
[epoch9, step967]: loss 0.434176
[epoch9, step968]: loss 0.322189
[epoch9, step969]: loss 0.757949
[epoch9, step970]: loss 0.455647
[epoch9, step971]: loss 0.250904
[epoch9, step972]: loss 0.532225
[epoch9, step973]: loss 0.576055
[epoch9, step974]: loss 0.477548
[epoch9, step975]: loss 0.581438
[epoch9, step976]: loss 0.422224
[epoch9, step977]: loss 0.561927
[epoch9, step978]: loss 0.582882
[epoch9, step979]: loss 0.526101
[epoch9, step980]: loss 0.366422
[epoch9, step981]: loss 0.260963
[epoch9, step982]: loss 0.356085
[epoch9, step983]: loss 0.570522
[epoch9, step984]: loss 0.434579
[epoch9, step985]: loss 0.670474
[epoch9, step986]: loss 0.541126
[epoch9, step987]: loss 0.475003
[epoch9, step988]: loss 0.423417
[epoch9, step989]: loss 0.261166
[epoch9, step990]: loss 0.510338
[epoch9, step991]: loss 0.415457
[epoch9, step992]: loss 0.459320
[epoch9, step993]: loss 0.622552
[epoch9, step994]: loss 0.456097
[epoch9, step995]: loss 0.475133
[epoch9, step996]: loss 0.606073
[epoch9, step997]: loss 0.537183
[epoch9, step998]: loss 0.503969
[epoch9, step999]: loss 0.554369
[epoch9, step1000]: loss 0.641634
[epoch9, step1001]: loss 0.394496
[epoch9, step1002]: loss 0.783882
[epoch9, step1003]: loss 0.478394
[epoch9, step1004]: loss 0.314846
[epoch9, step1005]: loss 0.502535
[epoch9, step1006]: loss 0.396176
[epoch9, step1007]: loss 0.649916
[epoch9, step1008]: loss 0.458519
[epoch9, step1009]: loss 0.458440
[epoch9, step1010]: loss 0.549612
[epoch9, step1011]: loss 0.528519
[epoch9, step1012]: loss 0.485783
[epoch9, step1013]: loss 0.508321
[epoch9, step1014]: loss 0.602134
[epoch9, step1015]: loss 0.594191
[epoch9, step1016]: loss 0.532895
[epoch9, step1017]: loss 0.446747
[epoch9, step1018]: loss 0.506940
[epoch9, step1019]: loss 0.411173
[epoch9, step1020]: loss 0.600803
[epoch9, step1021]: loss 0.597952
[epoch9, step1022]: loss 0.573863
[epoch9, step1023]: loss 0.639403
[epoch9, step1024]: loss 0.513317
[epoch9, step1025]: loss 0.299832
[epoch9, step1026]: loss 0.738237
[epoch9, step1027]: loss 0.640879
[epoch9, step1028]: loss 0.522235
[epoch9, step1029]: loss 0.433487
[epoch9, step1030]: loss 0.549648
[epoch9, step1031]: loss 0.776963
[epoch9, step1032]: loss 0.474660
[epoch9, step1033]: loss 0.566821
[epoch9, step1034]: loss 0.458003
[epoch9, step1035]: loss 0.420348
[epoch9, step1036]: loss 0.578166
[epoch9, step1037]: loss 0.438158
[epoch9, step1038]: loss 0.689608
[epoch9, step1039]: loss 0.578057
[epoch9, step1040]: loss 0.719113
[epoch9, step1041]: loss 0.266064
[epoch9, step1042]: loss 0.280895
[epoch9, step1043]: loss 0.561616
[epoch9, step1044]: loss 0.501168
[epoch9, step1045]: loss 0.263540
[epoch9, step1046]: loss 0.355389
[epoch9, step1047]: loss 0.550869
[epoch9, step1048]: loss 0.452920
[epoch9, step1049]: loss 0.378543
[epoch9, step1050]: loss 0.501512
[epoch9, step1051]: loss 0.696988
[epoch9, step1052]: loss 0.451940
[epoch9, step1053]: loss 0.611909
[epoch9, step1054]: loss 0.654290
[epoch9, step1055]: loss 0.671355
[epoch9, step1056]: loss 0.331892
[epoch9, step1057]: loss 0.630496
[epoch9, step1058]: loss 0.741079
[epoch9, step1059]: loss 0.478439
[epoch9, step1060]: loss 0.513135
[epoch9, step1061]: loss 0.549523
[epoch9, step1062]: loss 0.545768
[epoch9, step1063]: loss 0.551738
[epoch9, step1064]: loss 0.393322
[epoch9, step1065]: loss 0.568386
[epoch9, step1066]: loss 0.484731
[epoch9, step1067]: loss 0.484844
[epoch9, step1068]: loss 0.398531
[epoch9, step1069]: loss 0.436878
[epoch9, step1070]: loss 0.265053
[epoch9, step1071]: loss 0.457837
[epoch9, step1072]: loss 0.386327
[epoch9, step1073]: loss 0.490089
[epoch9, step1074]: loss 0.652776
[epoch9, step1075]: loss 0.287556
[epoch9, step1076]: loss 0.511035
[epoch9, step1077]: loss 0.473364
[epoch9, step1078]: loss 0.535076
[epoch9, step1079]: loss 0.566346
[epoch9, step1080]: loss 0.566755
[epoch9, step1081]: loss 0.731712
[epoch9, step1082]: loss 0.432223
[epoch9, step1083]: loss 0.606708
[epoch9, step1084]: loss 0.326439
[epoch9, step1085]: loss 0.561258
[epoch9, step1086]: loss 0.163779
[epoch9, step1087]: loss 0.400639
[epoch9, step1088]: loss 0.420516
[epoch9, step1089]: loss 0.406763
[epoch9, step1090]: loss 0.677724
[epoch9, step1091]: loss 0.511552
[epoch9, step1092]: loss 0.580595
[epoch9, step1093]: loss 0.758495
[epoch9, step1094]: loss 0.740153
[epoch9, step1095]: loss 0.517980
[epoch9, step1096]: loss 0.541767
[epoch9, step1097]: loss 0.523318
[epoch9, step1098]: loss 0.585246
[epoch9, step1099]: loss 0.489286
[epoch9, step1100]: loss 0.485872
[epoch9, step1101]: loss 0.638370
[epoch9, step1102]: loss 0.601404
[epoch9, step1103]: loss 0.674533
[epoch9, step1104]: loss 0.387929
[epoch9, step1105]: loss 0.608612
[epoch9, step1106]: loss 0.555088
[epoch9, step1107]: loss 0.428180
[epoch9, step1108]: loss 0.483215
[epoch9, step1109]: loss 0.434010
[epoch9, step1110]: loss 0.317125
[epoch9, step1111]: loss 0.596782
[epoch9, step1112]: loss 0.690171
[epoch9, step1113]: loss 0.466819
[epoch9, step1114]: loss 0.638461
[epoch9, step1115]: loss 0.141636
[epoch9, step1116]: loss 0.409428
[epoch9, step1117]: loss 0.383651
[epoch9, step1118]: loss 0.649893
[epoch9, step1119]: loss 0.467964
[epoch9, step1120]: loss 0.705136
[epoch9, step1121]: loss 0.588862
[epoch9, step1122]: loss 0.354072
[epoch9, step1123]: loss 0.464523
[epoch9, step1124]: loss 0.545646
[epoch9, step1125]: loss 0.506411
[epoch9, step1126]: loss 0.520792
[epoch9, step1127]: loss 0.428434
[epoch9, step1128]: loss 0.619322
[epoch9, step1129]: loss 0.671784
[epoch9, step1130]: loss 0.584626
[epoch9, step1131]: loss 0.680541
[epoch9, step1132]: loss 0.405994
[epoch9, step1133]: loss 0.573159
[epoch9, step1134]: loss 0.444255
[epoch9, step1135]: loss 0.658039
[epoch9, step1136]: loss 0.563861
[epoch9, step1137]: loss 0.420696
[epoch9, step1138]: loss 0.383538
[epoch9, step1139]: loss 0.295914
[epoch9, step1140]: loss 0.582509
[epoch9, step1141]: loss 0.454202
[epoch9, step1142]: loss 0.470940
[epoch9, step1143]: loss 0.586733
[epoch9, step1144]: loss 0.445040
[epoch9, step1145]: loss 0.439612
[epoch9, step1146]: loss 0.382613
[epoch9, step1147]: loss 0.422475
[epoch9, step1148]: loss 0.507452
[epoch9, step1149]: loss 0.512533
[epoch9, step1150]: loss 0.711338
[epoch9, step1151]: loss 0.379767
[epoch9, step1152]: loss 0.441669
[epoch9, step1153]: loss 0.661358
[epoch9, step1154]: loss 0.500107
[epoch9, step1155]: loss 0.541890
[epoch9, step1156]: loss 0.445521
[epoch9, step1157]: loss 0.557952
[epoch9, step1158]: loss 0.378013
[epoch9, step1159]: loss 0.636502
[epoch9, step1160]: loss 0.565133
[epoch9, step1161]: loss 0.274192
[epoch9, step1162]: loss 0.632781
[epoch9, step1163]: loss 0.554918
[epoch9, step1164]: loss 0.398523
[epoch9, step1165]: loss 0.382862
[epoch9, step1166]: loss 0.649109
[epoch9, step1167]: loss 0.587549
[epoch9, step1168]: loss 0.472825
[epoch9, step1169]: loss 0.696689
[epoch9, step1170]: loss 0.599178
[epoch9, step1171]: loss 0.621309
[epoch9, step1172]: loss 0.536858
[epoch9, step1173]: loss 0.671213
[epoch9, step1174]: loss 0.484543
[epoch9, step1175]: loss 0.492753
[epoch9, step1176]: loss 0.420617
[epoch9, step1177]: loss 0.515929
[epoch9, step1178]: loss 0.182827
[epoch9, step1179]: loss 0.544912
[epoch9, step1180]: loss 0.466971
[epoch9, step1181]: loss 0.467794
[epoch9, step1182]: loss 0.582722
[epoch9, step1183]: loss 0.542199
[epoch9, step1184]: loss 0.514619
[epoch9, step1185]: loss 0.361769
[epoch9, step1186]: loss 0.572201
[epoch9, step1187]: loss 0.661718
[epoch9, step1188]: loss 0.413715
[epoch9, step1189]: loss 0.514231
[epoch9, step1190]: loss 0.430767
[epoch9, step1191]: loss 0.528751
[epoch9, step1192]: loss 0.284828
[epoch9, step1193]: loss 0.547857
[epoch9, step1194]: loss 0.587539
[epoch9, step1195]: loss 0.345259
[epoch9, step1196]: loss 0.457126
[epoch9, step1197]: loss 0.593857
[epoch9, step1198]: loss 0.420100
[epoch9, step1199]: loss 0.412810
[epoch9, step1200]: loss 0.569152
[epoch9, step1201]: loss 0.641432
[epoch9, step1202]: loss 0.576885
[epoch9, step1203]: loss 0.480052
[epoch9, step1204]: loss 0.328300
[epoch9, step1205]: loss 0.473125
[epoch9, step1206]: loss 0.470500
[epoch9, step1207]: loss 0.455952
[epoch9, step1208]: loss 0.531109
[epoch9, step1209]: loss 0.479372
[epoch9, step1210]: loss 0.402238
[epoch9, step1211]: loss 0.521430
[epoch9, step1212]: loss 0.441099
[epoch9, step1213]: loss 0.489322
[epoch9, step1214]: loss 0.547303
[epoch9, step1215]: loss 0.450215
[epoch9, step1216]: loss 0.189394
[epoch9, step1217]: loss 0.641610
[epoch9, step1218]: loss 0.590590
[epoch9, step1219]: loss 0.826505
[epoch9, step1220]: loss 0.520474
[epoch9, step1221]: loss 0.467028
[epoch9, step1222]: loss 0.463167
[epoch9, step1223]: loss 0.579294
[epoch9, step1224]: loss 0.430776
[epoch9, step1225]: loss 0.556126
[epoch9, step1226]: loss 0.561942
[epoch9, step1227]: loss 0.243407
[epoch9, step1228]: loss 0.233975
[epoch9, step1229]: loss 0.577498
[epoch9, step1230]: loss 0.653625
[epoch9, step1231]: loss 0.618337
[epoch9, step1232]: loss 0.340296
[epoch9, step1233]: loss 0.583372
[epoch9, step1234]: loss 0.459886
[epoch9, step1235]: loss 0.539022
[epoch9, step1236]: loss 0.392605
[epoch9, step1237]: loss 0.541848
[epoch9, step1238]: loss 0.652686
[epoch9, step1239]: loss 0.609914
[epoch9, step1240]: loss 0.239351
[epoch9, step1241]: loss 0.720419
[epoch9, step1242]: loss 0.323333
[epoch9, step1243]: loss 0.625931
[epoch9, step1244]: loss 0.632349
[epoch9, step1245]: loss 0.369897
[epoch9, step1246]: loss 0.670342
[epoch9, step1247]: loss 0.531970
[epoch9, step1248]: loss 0.776726
[epoch9, step1249]: loss 0.552430
[epoch9, step1250]: loss 0.385772
[epoch9, step1251]: loss 0.278487
[epoch9, step1252]: loss 0.492929
[epoch9, step1253]: loss 0.602226
[epoch9, step1254]: loss 0.491573
[epoch9, step1255]: loss 0.500089
[epoch9, step1256]: loss 0.439065
[epoch9, step1257]: loss 0.487491
[epoch9, step1258]: loss 0.699628
[epoch9, step1259]: loss 0.403247
[epoch9, step1260]: loss 0.435689
[epoch9, step1261]: loss 0.396253
[epoch9, step1262]: loss 0.510879
[epoch9, step1263]: loss 0.533610
[epoch9, step1264]: loss 0.266848
[epoch9, step1265]: loss 0.527785
[epoch9, step1266]: loss 0.487049
[epoch9, step1267]: loss 0.443631
[epoch9, step1268]: loss 0.547310
[epoch9, step1269]: loss 0.545086
[epoch9, step1270]: loss 0.505198
[epoch9, step1271]: loss 0.505453
[epoch9, step1272]: loss 0.540131
[epoch9, step1273]: loss 0.345899
[epoch9, step1274]: loss 0.540570
[epoch9, step1275]: loss 0.409383
[epoch9, step1276]: loss 0.365939
[epoch9, step1277]: loss 0.362137
[epoch9, step1278]: loss 0.612258
[epoch9, step1279]: loss 0.372031
[epoch9, step1280]: loss 0.264006
[epoch9, step1281]: loss 0.489239
[epoch9, step1282]: loss 0.596175
[epoch9, step1283]: loss 0.585675
[epoch9, step1284]: loss 0.247242
[epoch9, step1285]: loss 0.623838
[epoch9, step1286]: loss 0.533771
[epoch9, step1287]: loss 0.410273
[epoch9, step1288]: loss 0.352505
[epoch9, step1289]: loss 0.447366
[epoch9, step1290]: loss 0.549426
[epoch9, step1291]: loss 0.531566
[epoch9, step1292]: loss 0.567995
[epoch9, step1293]: loss 0.500877
[epoch9, step1294]: loss 0.475377
[epoch9, step1295]: loss 0.605958
[epoch9, step1296]: loss 0.785815
[epoch9, step1297]: loss 0.631008
[epoch9, step1298]: loss 0.239198
[epoch9, step1299]: loss 0.558711
[epoch9, step1300]: loss 0.469010
[epoch9, step1301]: loss 0.753424
[epoch9, step1302]: loss 0.600246
[epoch9, step1303]: loss 0.413341
[epoch9, step1304]: loss 0.273588
[epoch9, step1305]: loss 0.220044
[epoch9, step1306]: loss 0.523285
[epoch9, step1307]: loss 0.485642
[epoch9, step1308]: loss 0.156132
[epoch9, step1309]: loss 0.447559
[epoch9, step1310]: loss 0.481755
[epoch9, step1311]: loss 0.349812
[epoch9, step1312]: loss 0.561802
[epoch9, step1313]: loss 0.620275
[epoch9, step1314]: loss 0.536299
[epoch9, step1315]: loss 0.319250
[epoch9, step1316]: loss 0.451144
[epoch9, step1317]: loss 0.369522
[epoch9, step1318]: loss 0.440747
[epoch9, step1319]: loss 0.639967
[epoch9, step1320]: loss 0.398467
[epoch9, step1321]: loss 0.551578
[epoch9, step1322]: loss 0.494250
[epoch9, step1323]: loss 0.474070
[epoch9, step1324]: loss 0.560831
[epoch9, step1325]: loss 0.367930
[epoch9, step1326]: loss 0.488594
[epoch9, step1327]: loss 0.534991
[epoch9, step1328]: loss 0.668953
[epoch9, step1329]: loss 0.430830
[epoch9, step1330]: loss 0.691876
[epoch9, step1331]: loss 0.721801
[epoch9, step1332]: loss 0.460162
[epoch9, step1333]: loss 0.411189
[epoch9, step1334]: loss 0.633807
[epoch9, step1335]: loss 0.582554
[epoch9, step1336]: loss 0.178026
[epoch9, step1337]: loss 0.506628
[epoch9, step1338]: loss 0.540943
[epoch9, step1339]: loss 0.531110
[epoch9, step1340]: loss 0.221276
[epoch9, step1341]: loss 0.552563
[epoch9, step1342]: loss 0.324602
[epoch9, step1343]: loss 0.696804
[epoch9, step1344]: loss 0.294575
[epoch9, step1345]: loss 0.784106
[epoch9, step1346]: loss 0.584330
[epoch9, step1347]: loss 0.570566
[epoch9, step1348]: loss 0.551506
[epoch9, step1349]: loss 0.608888
[epoch9, step1350]: loss 0.553835
[epoch9, step1351]: loss 0.453016
[epoch9, step1352]: loss 0.422196
[epoch9, step1353]: loss 0.434227
[epoch9, step1354]: loss 0.339689
[epoch9, step1355]: loss 0.432120
[epoch9, step1356]: loss 0.492289
[epoch9, step1357]: loss 0.332972
[epoch9, step1358]: loss 0.505575
[epoch9, step1359]: loss 0.641068
[epoch9, step1360]: loss 0.626337
[epoch9, step1361]: loss 0.564753
[epoch9, step1362]: loss 0.435635
[epoch9, step1363]: loss 0.392436
[epoch9, step1364]: loss 0.455731
[epoch9, step1365]: loss 0.580969
[epoch9, step1366]: loss 0.355037
[epoch9, step1367]: loss 0.663297
[epoch9, step1368]: loss 0.654508
[epoch9, step1369]: loss 0.387333
[epoch9, step1370]: loss 0.592195
[epoch9, step1371]: loss 0.374940
[epoch9, step1372]: loss 0.464126
[epoch9, step1373]: loss 0.367661
[epoch9, step1374]: loss 0.452391
[epoch9, step1375]: loss 0.323855
[epoch9, step1376]: loss 0.602204
[epoch9, step1377]: loss 0.418263
[epoch9, step1378]: loss 0.410103
[epoch9, step1379]: loss 0.737716
[epoch9, step1380]: loss 0.466702
[epoch9, step1381]: loss 0.713380
[epoch9, step1382]: loss 0.631188
[epoch9, step1383]: loss 0.703074
[epoch9, step1384]: loss 0.777962
[epoch9, step1385]: loss 0.412582
[epoch9, step1386]: loss 0.668351
[epoch9, step1387]: loss 0.508621
[epoch9, step1388]: loss 0.702022
[epoch9, step1389]: loss 0.614429
[epoch9, step1390]: loss 0.619020
[epoch9, step1391]: loss 0.297673
[epoch9, step1392]: loss 0.234223
[epoch9, step1393]: loss 0.375092
[epoch9, step1394]: loss 0.526472
[epoch9, step1395]: loss 0.500882
[epoch9, step1396]: loss 0.152853
[epoch9, step1397]: loss 0.531794
[epoch9, step1398]: loss 0.374606
[epoch9, step1399]: loss 0.448605
[epoch9, step1400]: loss 0.421550
[epoch9, step1401]: loss 0.683251
[epoch9, step1402]: loss 0.268124
[epoch9, step1403]: loss 0.373801
[epoch9, step1404]: loss 0.732346
[epoch9, step1405]: loss 0.562608
[epoch9, step1406]: loss 0.422092
[epoch9, step1407]: loss 0.486936
[epoch9, step1408]: loss 0.590249
[epoch9, step1409]: loss 0.411050
[epoch9, step1410]: loss 0.693653
[epoch9, step1411]: loss 0.554894
[epoch9, step1412]: loss 0.265366
[epoch9, step1413]: loss 0.347262
[epoch9, step1414]: loss 0.533611
[epoch9, step1415]: loss 0.404464
[epoch9, step1416]: loss 0.427987
[epoch9, step1417]: loss 0.355939
[epoch9, step1418]: loss 0.498560
[epoch9, step1419]: loss 0.384186
[epoch9, step1420]: loss 0.703081
[epoch9, step1421]: loss 0.664017
[epoch9, step1422]: loss 0.351260
[epoch9, step1423]: loss 0.244929
[epoch9, step1424]: loss 0.113807
[epoch9, step1425]: loss 0.428803
[epoch9, step1426]: loss 0.408222
[epoch9, step1427]: loss 0.467059
[epoch9, step1428]: loss 0.381923
[epoch9, step1429]: loss 0.604054
[epoch9, step1430]: loss 0.487200
[epoch9, step1431]: loss 0.246731
[epoch9, step1432]: loss 0.463840
[epoch9, step1433]: loss 0.603967
[epoch9, step1434]: loss 0.446548
[epoch9, step1435]: loss 0.500151
[epoch9, step1436]: loss 0.613377
[epoch9, step1437]: loss 0.335654
[epoch9, step1438]: loss 0.507676
[epoch9, step1439]: loss 0.632670
[epoch9, step1440]: loss 0.655597
[epoch9, step1441]: loss 0.486692
[epoch9, step1442]: loss 0.521469
[epoch9, step1443]: loss 0.510363
[epoch9, step1444]: loss 0.578107
[epoch9, step1445]: loss 0.761060
[epoch9, step1446]: loss 0.464521
[epoch9, step1447]: loss 0.737842
[epoch9, step1448]: loss 0.540304
[epoch9, step1449]: loss 0.613334
[epoch9, step1450]: loss 0.311568
[epoch9, step1451]: loss 0.695113
[epoch9, step1452]: loss 0.324479
[epoch9, step1453]: loss 0.505285
[epoch9, step1454]: loss 0.530742
[epoch9, step1455]: loss 0.444449
[epoch9, step1456]: loss 0.343473
[epoch9, step1457]: loss 0.587922
[epoch9, step1458]: loss 0.554231
[epoch9, step1459]: loss 0.573645
[epoch9, step1460]: loss 0.374667
[epoch9, step1461]: loss 0.432885
[epoch9, step1462]: loss 0.218129
[epoch9, step1463]: loss 0.270655
[epoch9, step1464]: loss 0.525971
[epoch9, step1465]: loss 0.677133
[epoch9, step1466]: loss 0.461514
[epoch9, step1467]: loss 0.512150
[epoch9, step1468]: loss 0.213737
[epoch9, step1469]: loss 0.736482
[epoch9, step1470]: loss 0.640109
[epoch9, step1471]: loss 0.444488
[epoch9, step1472]: loss 0.584830
[epoch9, step1473]: loss 0.638267
[epoch9, step1474]: loss 0.542536
[epoch9, step1475]: loss 0.613239
[epoch9, step1476]: loss 0.431931
[epoch9, step1477]: loss 0.678652
[epoch9, step1478]: loss 0.207424
[epoch9, step1479]: loss 0.477356
[epoch9, step1480]: loss 0.510708
[epoch9, step1481]: loss 0.571427
[epoch9, step1482]: loss 0.676952
[epoch9, step1483]: loss 0.611380
[epoch9, step1484]: loss 0.744199
[epoch9, step1485]: loss 0.545774
[epoch9, step1486]: loss 0.445900
[epoch9, step1487]: loss 0.547214
[epoch9, step1488]: loss 0.531522
[epoch9, step1489]: loss 0.207403
[epoch9, step1490]: loss 0.526013
[epoch9, step1491]: loss 0.445404
[epoch9, step1492]: loss 0.554758
[epoch9, step1493]: loss 0.633989
[epoch9, step1494]: loss 0.160404
[epoch9, step1495]: loss 0.660308
[epoch9, step1496]: loss 0.421938
[epoch9, step1497]: loss 0.450265
[epoch9, step1498]: loss 0.592206
[epoch9, step1499]: loss 0.510133
[epoch9, step1500]: loss 0.512383
[epoch9, step1501]: loss 0.515983
[epoch9, step1502]: loss 0.640907
[epoch9, step1503]: loss 0.695659
[epoch9, step1504]: loss 0.588772
[epoch9, step1505]: loss 0.530548
[epoch9, step1506]: loss 0.469536
[epoch9, step1507]: loss 0.567712
[epoch9, step1508]: loss 0.460279
[epoch9, step1509]: loss 0.365591
[epoch9, step1510]: loss 0.408317
[epoch9, step1511]: loss 0.656976
[epoch9, step1512]: loss 0.631206
[epoch9, step1513]: loss 0.428911
[epoch9, step1514]: loss 0.433997
[epoch9, step1515]: loss 0.511101
[epoch9, step1516]: loss 0.359258
[epoch9, step1517]: loss 0.625902
[epoch9, step1518]: loss 0.552790
[epoch9, step1519]: loss 0.508072
[epoch9, step1520]: loss 0.663566
[epoch9, step1521]: loss 0.471406
[epoch9, step1522]: loss 0.323481
[epoch9, step1523]: loss 0.475047
[epoch9, step1524]: loss 0.235252
[epoch9, step1525]: loss 0.408502
[epoch9, step1526]: loss 0.558165
[epoch9, step1527]: loss 0.451915
[epoch9, step1528]: loss 0.706707
[epoch9, step1529]: loss 0.514448
[epoch9, step1530]: loss 0.620545
[epoch9, step1531]: loss 0.503717
[epoch9, step1532]: loss 0.491542
[epoch9, step1533]: loss 0.526483
[epoch9, step1534]: loss 0.488343
[epoch9, step1535]: loss 0.403371
[epoch9, step1536]: loss 0.451431
[epoch9, step1537]: loss 0.411342
[epoch9, step1538]: loss 0.324059
[epoch9, step1539]: loss 0.296978
[epoch9, step1540]: loss 0.364857
[epoch9, step1541]: loss 0.544787
[epoch9, step1542]: loss 0.650317
[epoch9, step1543]: loss 0.244800
[epoch9, step1544]: loss 0.571660
[epoch9, step1545]: loss 0.544929
[epoch9, step1546]: loss 0.514118
[epoch9, step1547]: loss 0.598605
[epoch9, step1548]: loss 0.582988
[epoch9, step1549]: loss 0.482645
[epoch9, step1550]: loss 0.769452
[epoch9, step1551]: loss 0.561493
[epoch9, step1552]: loss 0.708901
[epoch9, step1553]: loss 0.576397
[epoch9, step1554]: loss 0.711011
[epoch9, step1555]: loss 0.465926
[epoch9, step1556]: loss 0.579964
[epoch9, step1557]: loss 0.506171
[epoch9, step1558]: loss 0.659506
[epoch9, step1559]: loss 0.602739
[epoch9, step1560]: loss 0.509153
[epoch9, step1561]: loss 0.544298
[epoch9, step1562]: loss 0.566360
[epoch9, step1563]: loss 0.580044
[epoch9, step1564]: loss 0.261206
[epoch9, step1565]: loss 0.374076
[epoch9, step1566]: loss 0.420903
[epoch9, step1567]: loss 0.305486
[epoch9, step1568]: loss 0.558897
[epoch9, step1569]: loss 0.472394
[epoch9, step1570]: loss 0.304371
[epoch9, step1571]: loss 0.450587
[epoch9, step1572]: loss 0.739209
[epoch9, step1573]: loss 0.650457
[epoch9, step1574]: loss 0.570291
[epoch9, step1575]: loss 0.546264
[epoch9, step1576]: loss 0.260162
[epoch9, step1577]: loss 0.329977
[epoch9, step1578]: loss 0.441268
[epoch9, step1579]: loss 0.153188
[epoch9, step1580]: loss 0.452176
[epoch9, step1581]: loss 0.462065
[epoch9, step1582]: loss 0.390687
[epoch9, step1583]: loss 0.655479
[epoch9, step1584]: loss 0.609207
[epoch9, step1585]: loss 0.633888
[epoch9, step1586]: loss 0.472095
[epoch9, step1587]: loss 0.517518
[epoch9, step1588]: loss 0.462436
[epoch9, step1589]: loss 0.455045
[epoch9, step1590]: loss 0.616267
[epoch9, step1591]: loss 0.317080
[epoch9, step1592]: loss 0.434841
[epoch9, step1593]: loss 0.373375
[epoch9, step1594]: loss 0.481898
[epoch9, step1595]: loss 0.716003
[epoch9, step1596]: loss 0.404663
[epoch9, step1597]: loss 0.704246
[epoch9, step1598]: loss 0.558357
[epoch9, step1599]: loss 0.819286
[epoch9, step1600]: loss 0.623108
[epoch9, step1601]: loss 0.517988
[epoch9, step1602]: loss 0.730913
[epoch9, step1603]: loss 0.525203
[epoch9, step1604]: loss 0.392361
[epoch9, step1605]: loss 0.505221
[epoch9, step1606]: loss 0.531786
[epoch9, step1607]: loss 0.563629
[epoch9, step1608]: loss 0.260764
[epoch9, step1609]: loss 0.571845
[epoch9, step1610]: loss 0.433293
[epoch9, step1611]: loss 0.310453
[epoch9, step1612]: loss 0.555251
[epoch9, step1613]: loss 0.404349
[epoch9, step1614]: loss 0.581730
[epoch9, step1615]: loss 0.428083
[epoch9, step1616]: loss 0.454192
[epoch9, step1617]: loss 0.510465
[epoch9, step1618]: loss 0.417576
[epoch9, step1619]: loss 0.729292
[epoch9, step1620]: loss 0.706095
[epoch9, step1621]: loss 0.502785
[epoch9, step1622]: loss 0.585718
[epoch9, step1623]: loss 0.618798
[epoch9, step1624]: loss 0.651355
[epoch9, step1625]: loss 0.386618
[epoch9, step1626]: loss 0.629134
[epoch9, step1627]: loss 0.493050
[epoch9, step1628]: loss 0.432860
[epoch9, step1629]: loss 0.650652
[epoch9, step1630]: loss 0.760461
[epoch9, step1631]: loss 0.284103
[epoch9, step1632]: loss 0.427257
[epoch9, step1633]: loss 0.506278
[epoch9, step1634]: loss 0.525394
[epoch9, step1635]: loss 0.448897
[epoch9, step1636]: loss 0.503339
[epoch9, step1637]: loss 0.531292
[epoch9, step1638]: loss 0.574563
[epoch9, step1639]: loss 0.506031
[epoch9, step1640]: loss 0.527999
[epoch9, step1641]: loss 0.516142
[epoch9, step1642]: loss 0.347777
[epoch9, step1643]: loss 0.661712
[epoch9, step1644]: loss 0.467295
[epoch9, step1645]: loss 0.724642
[epoch9, step1646]: loss 0.525576
[epoch9, step1647]: loss 0.552571
[epoch9, step1648]: loss 0.684955
[epoch9, step1649]: loss 0.377201
[epoch9, step1650]: loss 0.541513
[epoch9, step1651]: loss 0.635805
[epoch9, step1652]: loss 0.114652
[epoch9, step1653]: loss 0.746881
[epoch9, step1654]: loss 0.677899
[epoch9, step1655]: loss 0.669204
[epoch9, step1656]: loss 0.315027
[epoch9, step1657]: loss 0.248311
[epoch9, step1658]: loss 0.430538
[epoch9, step1659]: loss 0.655277
[epoch9, step1660]: loss 0.509011
[epoch9, step1661]: loss 0.425914
[epoch9, step1662]: loss 0.473884
[epoch9, step1663]: loss 0.672554
[epoch9, step1664]: loss 0.344014
[epoch9, step1665]: loss 0.706703
[epoch9, step1666]: loss 0.567328
[epoch9, step1667]: loss 0.587148
[epoch9, step1668]: loss 0.485912
[epoch9, step1669]: loss 0.428709
[epoch9, step1670]: loss 0.560796
[epoch9, step1671]: loss 0.488063
[epoch9, step1672]: loss 0.650923
[epoch9, step1673]: loss 0.666165
[epoch9, step1674]: loss 0.616065
[epoch9, step1675]: loss 0.443407
[epoch9, step1676]: loss 0.539198
[epoch9, step1677]: loss 0.683326
[epoch9, step1678]: loss 0.568853
[epoch9, step1679]: loss 0.424454
[epoch9, step1680]: loss 0.655194
[epoch9, step1681]: loss 0.687491
[epoch9, step1682]: loss 0.502403
[epoch9, step1683]: loss 0.682059
[epoch9, step1684]: loss 0.565346
[epoch9, step1685]: loss 0.500274
[epoch9, step1686]: loss 0.586930
[epoch9, step1687]: loss 0.705316
[epoch9, step1688]: loss 0.626339
[epoch9, step1689]: loss 0.479671
[epoch9, step1690]: loss 0.612334
[epoch9, step1691]: loss 0.630092
[epoch9, step1692]: loss 0.389109
[epoch9, step1693]: loss 0.690233
[epoch9, step1694]: loss 0.380377
[epoch9, step1695]: loss 0.363890
[epoch9, step1696]: loss 0.480736
[epoch9, step1697]: loss 0.810496
[epoch9, step1698]: loss 0.585954
[epoch9, step1699]: loss 0.428105
[epoch9, step1700]: loss 0.390387
[epoch9, step1701]: loss 0.588636
[epoch9, step1702]: loss 0.517370
[epoch9, step1703]: loss 0.570523
[epoch9, step1704]: loss 0.516966
[epoch9, step1705]: loss 0.521399
[epoch9, step1706]: loss 0.601889
[epoch9, step1707]: loss 0.460372
[epoch9, step1708]: loss 0.429656
[epoch9, step1709]: loss 0.259024
[epoch9, step1710]: loss 0.607286
[epoch9, step1711]: loss 0.283625
[epoch9, step1712]: loss 0.405711
[epoch9, step1713]: loss 0.373054
[epoch9, step1714]: loss 0.516558
[epoch9, step1715]: loss 0.522633
[epoch9, step1716]: loss 0.262532
[epoch9, step1717]: loss 0.688902
[epoch9, step1718]: loss 0.734294
[epoch9, step1719]: loss 0.351823
[epoch9, step1720]: loss 0.638847
[epoch9, step1721]: loss 0.624114
[epoch9, step1722]: loss 0.160318
[epoch9, step1723]: loss 0.642829
[epoch9, step1724]: loss 0.549167
[epoch9, step1725]: loss 0.604310
[epoch9, step1726]: loss 0.436381
[epoch9, step1727]: loss 0.620338
[epoch9, step1728]: loss 0.544762
[epoch9, step1729]: loss 0.447466
[epoch9, step1730]: loss 0.390197
[epoch9, step1731]: loss 0.555153
[epoch9, step1732]: loss 0.621845
[epoch9, step1733]: loss 0.669632
[epoch9, step1734]: loss 0.454698
[epoch9, step1735]: loss 0.268222
[epoch9, step1736]: loss 0.645154
[epoch9, step1737]: loss 0.464373
[epoch9, step1738]: loss 0.524458
[epoch9, step1739]: loss 0.533355
[epoch9, step1740]: loss 0.619037
[epoch9, step1741]: loss 0.505092
[epoch9, step1742]: loss 0.331288
[epoch9, step1743]: loss 0.367674
[epoch9, step1744]: loss 0.506889
[epoch9, step1745]: loss 0.643917
[epoch9, step1746]: loss 0.412197
[epoch9, step1747]: loss 0.368959
[epoch9, step1748]: loss 0.561789
[epoch9, step1749]: loss 0.620872
[epoch9, step1750]: loss 0.457305
[epoch9, step1751]: loss 0.591712
[epoch9, step1752]: loss 0.801725
[epoch9, step1753]: loss 0.740854
[epoch9, step1754]: loss 0.716517
[epoch9, step1755]: loss 0.204411
[epoch9, step1756]: loss 0.649188
[epoch9, step1757]: loss 0.477065
[epoch9, step1758]: loss 0.453337
[epoch9, step1759]: loss 0.476890
[epoch9, step1760]: loss 0.488239
[epoch9, step1761]: loss 0.568214
[epoch9, step1762]: loss 0.653864
[epoch9, step1763]: loss 0.373632
[epoch9, step1764]: loss 0.469123
[epoch9, step1765]: loss 0.538532
[epoch9, step1766]: loss 0.513645
[epoch9, step1767]: loss 0.707237
[epoch9, step1768]: loss 0.536349
[epoch9, step1769]: loss 0.522402
[epoch9, step1770]: loss 0.570175
[epoch9, step1771]: loss 0.272242
[epoch9, step1772]: loss 0.500003
[epoch9, step1773]: loss 0.480072
[epoch9, step1774]: loss 0.453264
[epoch9, step1775]: loss 0.637261
[epoch9, step1776]: loss 0.691950
[epoch9, step1777]: loss 0.637236
[epoch9, step1778]: loss 0.539821
[epoch9, step1779]: loss 0.633616
[epoch9, step1780]: loss 0.629464
[epoch9, step1781]: loss 0.372609
[epoch9, step1782]: loss 0.501138
[epoch9, step1783]: loss 0.528116
[epoch9, step1784]: loss 0.393522
[epoch9, step1785]: loss 0.429116
[epoch9, step1786]: loss 0.709651
[epoch9, step1787]: loss 0.639625
[epoch9, step1788]: loss 0.383675
[epoch9, step1789]: loss 0.536646
[epoch9, step1790]: loss 0.576165
[epoch9, step1791]: loss 0.485847
[epoch9, step1792]: loss 0.467612
[epoch9, step1793]: loss 0.665677
[epoch9, step1794]: loss 0.501701
[epoch9, step1795]: loss 0.403826
[epoch9, step1796]: loss 0.499258
[epoch9, step1797]: loss 0.534713
[epoch9, step1798]: loss 0.504165
[epoch9, step1799]: loss 0.470886
[epoch9, step1800]: loss 0.456794
[epoch9, step1801]: loss 0.549215
[epoch9, step1802]: loss 0.469376
[epoch9, step1803]: loss 0.489697
[epoch9, step1804]: loss 0.308296
[epoch9, step1805]: loss 0.438224
[epoch9, step1806]: loss 0.774400
[epoch9, step1807]: loss 0.315643
[epoch9, step1808]: loss 0.631875
[epoch9, step1809]: loss 0.378502
[epoch9, step1810]: loss 0.680853
[epoch9, step1811]: loss 0.457848
[epoch9, step1812]: loss 0.693953
[epoch9, step1813]: loss 0.697020
[epoch9, step1814]: loss 0.400457
[epoch9, step1815]: loss 0.642495
[epoch9, step1816]: loss 0.512288
[epoch9, step1817]: loss 0.677625
[epoch9, step1818]: loss 0.543901
[epoch9, step1819]: loss 0.519734
[epoch9, step1820]: loss 0.462810
[epoch9, step1821]: loss 0.658041
[epoch9, step1822]: loss 0.663153
[epoch9, step1823]: loss 0.497942
[epoch9, step1824]: loss 0.267535
[epoch9, step1825]: loss 0.648352
[epoch9, step1826]: loss 0.472439
[epoch9, step1827]: loss 0.407837
[epoch9, step1828]: loss 0.551476
[epoch9, step1829]: loss 0.340556
[epoch9, step1830]: loss 0.435058
[epoch9, step1831]: loss 0.487785
[epoch9, step1832]: loss 0.094468
[epoch9, step1833]: loss 0.517386
[epoch9, step1834]: loss 0.597409
[epoch9, step1835]: loss 0.643661
[epoch9, step1836]: loss 0.549264
[epoch9, step1837]: loss 0.547382
[epoch9, step1838]: loss 0.617915
[epoch9, step1839]: loss 0.534329
[epoch9, step1840]: loss 0.637918
[epoch9, step1841]: loss 0.679109
[epoch9, step1842]: loss 0.427126
[epoch9, step1843]: loss 0.560049
[epoch9, step1844]: loss 0.552186
[epoch9, step1845]: loss 0.478313
[epoch9, step1846]: loss 0.144767
[epoch9, step1847]: loss 0.346299
[epoch9, step1848]: loss 0.665550
[epoch9, step1849]: loss 0.606730
[epoch9, step1850]: loss 0.292424
[epoch9, step1851]: loss 0.424605
[epoch9, step1852]: loss 0.540097
[epoch9, step1853]: loss 0.618346
[epoch9, step1854]: loss 0.448903
[epoch9, step1855]: loss 0.689738
[epoch9, step1856]: loss 0.465678
[epoch9, step1857]: loss 0.567273
[epoch9, step1858]: loss 0.588250
[epoch9, step1859]: loss 0.627656
[epoch9, step1860]: loss 0.607812
[epoch9, step1861]: loss 0.556325
[epoch9, step1862]: loss 0.485157
[epoch9, step1863]: loss 0.514397
[epoch9, step1864]: loss 0.576480
[epoch9, step1865]: loss 0.478771
[epoch9, step1866]: loss 0.597486
[epoch9, step1867]: loss 0.344410
[epoch9, step1868]: loss 0.565887
[epoch9, step1869]: loss 0.621152
[epoch9, step1870]: loss 0.589291
[epoch9, step1871]: loss 0.518467
[epoch9, step1872]: loss 0.548027
[epoch9, step1873]: loss 0.542896
[epoch9, step1874]: loss 0.525353
[epoch9, step1875]: loss 0.482063
[epoch9, step1876]: loss 0.566894
[epoch9, step1877]: loss 0.644956
[epoch9, step1878]: loss 0.308002
[epoch9, step1879]: loss 0.234513
[epoch9, step1880]: loss 0.573527
[epoch9, step1881]: loss 0.598024
[epoch9, step1882]: loss 0.157240
[epoch9, step1883]: loss 0.611168
[epoch9, step1884]: loss 0.727665
[epoch9, step1885]: loss 0.441049
[epoch9, step1886]: loss 0.447811
[epoch9, step1887]: loss 0.541052
[epoch9, step1888]: loss 0.742606
[epoch9, step1889]: loss 0.366217
[epoch9, step1890]: loss 0.358009
[epoch9, step1891]: loss 0.368247
[epoch9, step1892]: loss 0.315816
[epoch9, step1893]: loss 0.490200
[epoch9, step1894]: loss 0.668766
[epoch9, step1895]: loss 0.469714
[epoch9, step1896]: loss 0.615081
[epoch9, step1897]: loss 0.547575
[epoch9, step1898]: loss 0.526058
[epoch9, step1899]: loss 0.468004
[epoch9, step1900]: loss 0.473169
[epoch9, step1901]: loss 0.503788
[epoch9, step1902]: loss 0.475395
[epoch9, step1903]: loss 0.430614
[epoch9, step1904]: loss 0.314913
[epoch9, step1905]: loss 0.590754
[epoch9, step1906]: loss 0.478245
[epoch9, step1907]: loss 0.550498
[epoch9, step1908]: loss 0.570150
[epoch9, step1909]: loss 0.638797
[epoch9, step1910]: loss 0.683369
[epoch9, step1911]: loss 0.567872
[epoch9, step1912]: loss 0.564108
[epoch9, step1913]: loss 0.425371
[epoch9, step1914]: loss 0.669856
[epoch9, step1915]: loss 0.704040
[epoch9, step1916]: loss 0.688247
[epoch9, step1917]: loss 0.459179
[epoch9, step1918]: loss 0.442285
[epoch9, step1919]: loss 0.498839
[epoch9, step1920]: loss 0.442362
[epoch9, step1921]: loss 0.525192
[epoch9, step1922]: loss 0.496733
[epoch9, step1923]: loss 0.659684
[epoch9, step1924]: loss 0.390515
[epoch9, step1925]: loss 0.441918
[epoch9, step1926]: loss 0.614709
[epoch9, step1927]: loss 0.462706
[epoch9, step1928]: loss 0.580468
[epoch9, step1929]: loss 0.486486
[epoch9, step1930]: loss 0.359457
[epoch9, step1931]: loss 0.552550
[epoch9, step1932]: loss 0.273138
[epoch9, step1933]: loss 0.359329
[epoch9, step1934]: loss 0.614327
[epoch9, step1935]: loss 0.249612
[epoch9, step1936]: loss 0.692852
[epoch9, step1937]: loss 0.596999
[epoch9, step1938]: loss 0.729128
[epoch9, step1939]: loss 0.680659
[epoch9, step1940]: loss 0.570092
[epoch9, step1941]: loss 0.632616
[epoch9, step1942]: loss 0.381971
[epoch9, step1943]: loss 0.466971
[epoch9, step1944]: loss 0.593037
[epoch9, step1945]: loss 0.604715
[epoch9, step1946]: loss 0.782597
[epoch9, step1947]: loss 0.533499
[epoch9, step1948]: loss 0.360980
[epoch9, step1949]: loss 0.550897
[epoch9, step1950]: loss 0.358689
[epoch9, step1951]: loss 0.443150
[epoch9, step1952]: loss 0.385932
[epoch9, step1953]: loss 0.421101
[epoch9, step1954]: loss 0.452510
[epoch9, step1955]: loss 0.110662
[epoch9, step1956]: loss 0.413583
[epoch9, step1957]: loss 0.405809
[epoch9, step1958]: loss 0.658148
[epoch9, step1959]: loss 0.458031
[epoch9, step1960]: loss 0.377945
[epoch9, step1961]: loss 0.477679
[epoch9, step1962]: loss 0.312270
[epoch9, step1963]: loss 0.534193
[epoch9, step1964]: loss 0.519529
[epoch9, step1965]: loss 0.641983
[epoch9, step1966]: loss 0.715157
[epoch9, step1967]: loss 0.478175
[epoch9, step1968]: loss 0.600841
[epoch9, step1969]: loss 0.381170
[epoch9, step1970]: loss 0.417514
[epoch9, step1971]: loss 0.506929
[epoch9, step1972]: loss 0.414826
[epoch9, step1973]: loss 0.578762
[epoch9, step1974]: loss 0.442333
[epoch9, step1975]: loss 0.513998
[epoch9, step1976]: loss 0.413371
[epoch9, step1977]: loss 0.423467
[epoch9, step1978]: loss 0.344675
[epoch9, step1979]: loss 0.557180
[epoch9, step1980]: loss 0.698496
[epoch9, step1981]: loss 0.742252
[epoch9, step1982]: loss 0.345772
[epoch9, step1983]: loss 0.487631
[epoch9, step1984]: loss 0.616768
[epoch9, step1985]: loss 0.546850
[epoch9, step1986]: loss 0.469782
[epoch9, step1987]: loss 0.555958
[epoch9, step1988]: loss 0.668784
[epoch9, step1989]: loss 0.495135
[epoch9, step1990]: loss 0.646766
[epoch9, step1991]: loss 0.457694
[epoch9, step1992]: loss 0.530721
[epoch9, step1993]: loss 0.840758
[epoch9, step1994]: loss 0.453136
[epoch9, step1995]: loss 0.633018
[epoch9, step1996]: loss 0.414212
[epoch9, step1997]: loss 0.483731
[epoch9, step1998]: loss 0.518085
[epoch9, step1999]: loss 0.632324
[epoch9, step2000]: loss 0.326664
[epoch9, step2001]: loss 0.401244
[epoch9, step2002]: loss 0.694993
[epoch9, step2003]: loss 0.685993
[epoch9, step2004]: loss 0.480366
[epoch9, step2005]: loss 0.693520
[epoch9, step2006]: loss 0.559346
[epoch9, step2007]: loss 0.445825
[epoch9, step2008]: loss 0.454375
[epoch9, step2009]: loss 0.482997
[epoch9, step2010]: loss 0.329050
[epoch9, step2011]: loss 0.366716
[epoch9, step2012]: loss 0.386727
[epoch9, step2013]: loss 0.178893
[epoch9, step2014]: loss 0.345799
[epoch9, step2015]: loss 0.790218
[epoch9, step2016]: loss 0.364472
[epoch9, step2017]: loss 0.470630
[epoch9, step2018]: loss 0.541836
[epoch9, step2019]: loss 0.485296
[epoch9, step2020]: loss 0.500743
[epoch9, step2021]: loss 0.546318
[epoch9, step2022]: loss 0.344289
[epoch9, step2023]: loss 0.634524
[epoch9, step2024]: loss 0.434547
[epoch9, step2025]: loss 0.485771
[epoch9, step2026]: loss 0.458415
[epoch9, step2027]: loss 0.137302
[epoch9, step2028]: loss 0.456423
[epoch9, step2029]: loss 0.568233
[epoch9, step2030]: loss 0.446743
[epoch9, step2031]: loss 0.668199
[epoch9, step2032]: loss 0.234139
[epoch9, step2033]: loss 0.481973
[epoch9, step2034]: loss 0.497584
[epoch9, step2035]: loss 0.502064
[epoch9, step2036]: loss 0.443373
[epoch9, step2037]: loss 0.378458
[epoch9, step2038]: loss 0.516672
[epoch9, step2039]: loss 0.596614
[epoch9, step2040]: loss 0.204853
[epoch9, step2041]: loss 0.460057
[epoch9, step2042]: loss 0.516954
[epoch9, step2043]: loss 0.334684
[epoch9, step2044]: loss 0.604131
[epoch9, step2045]: loss 0.524754
[epoch9, step2046]: loss 0.690001
[epoch9, step2047]: loss 0.620806
[epoch9, step2048]: loss 0.599548
[epoch9, step2049]: loss 0.264814
[epoch9, step2050]: loss 0.460825
[epoch9, step2051]: loss 0.540855
[epoch9, step2052]: loss 0.367704
[epoch9, step2053]: loss 0.472991
[epoch9, step2054]: loss 0.786690
[epoch9, step2055]: loss 0.422824
[epoch9, step2056]: loss 0.550270
[epoch9, step2057]: loss 0.474252
[epoch9, step2058]: loss 0.511344
[epoch9, step2059]: loss 0.527553
[epoch9, step2060]: loss 0.431808
[epoch9, step2061]: loss 0.758247
[epoch9, step2062]: loss 0.659943
[epoch9, step2063]: loss 0.626343
[epoch9, step2064]: loss 0.530906
[epoch9, step2065]: loss 0.803681
[epoch9, step2066]: loss 0.647239
[epoch9, step2067]: loss 0.480369
[epoch9, step2068]: loss 0.585185
[epoch9, step2069]: loss 0.512339
[epoch9, step2070]: loss 0.437141
[epoch9, step2071]: loss 0.350044
[epoch9, step2072]: loss 0.416702
[epoch9, step2073]: loss 0.613173
[epoch9, step2074]: loss 0.323046
[epoch9, step2075]: loss 0.762705
[epoch9, step2076]: loss 0.484352
[epoch9, step2077]: loss 0.833500
[epoch9, step2078]: loss 0.653144
[epoch9, step2079]: loss 0.559951
[epoch9, step2080]: loss 0.176015
[epoch9, step2081]: loss 0.647207
[epoch9, step2082]: loss 0.456910
[epoch9, step2083]: loss 0.430083
[epoch9, step2084]: loss 0.418958
[epoch9, step2085]: loss 0.410098
[epoch9, step2086]: loss 0.489574
[epoch9, step2087]: loss 0.436692
[epoch9, step2088]: loss 0.528274
[epoch9, step2089]: loss 0.142424
[epoch9, step2090]: loss 0.477026
[epoch9, step2091]: loss 0.527075
[epoch9, step2092]: loss 0.462786
[epoch9, step2093]: loss 0.639011
[epoch9, step2094]: loss 0.488562
[epoch9, step2095]: loss 0.517513
[epoch9, step2096]: loss 0.424841
[epoch9, step2097]: loss 0.374934
[epoch9, step2098]: loss 0.715828
[epoch9, step2099]: loss 0.450979
[epoch9, step2100]: loss 0.290639
[epoch9, step2101]: loss 0.598696
[epoch9, step2102]: loss 0.262517
[epoch9, step2103]: loss 0.548084
[epoch9, step2104]: loss 0.350503
[epoch9, step2105]: loss 0.494931
[epoch9, step2106]: loss 0.339762
[epoch9, step2107]: loss 0.498724
[epoch9, step2108]: loss 0.289451
[epoch9, step2109]: loss 0.280338
[epoch9, step2110]: loss 0.543949
[epoch9, step2111]: loss 0.497073
[epoch9, step2112]: loss 0.455911
[epoch9, step2113]: loss 0.596301
[epoch9, step2114]: loss 0.463197
[epoch9, step2115]: loss 0.408465
[epoch9, step2116]: loss 0.574268
[epoch9, step2117]: loss 0.565186
[epoch9, step2118]: loss 0.614931
[epoch9, step2119]: loss 0.400826
[epoch9, step2120]: loss 0.368691
[epoch9, step2121]: loss 0.698525
[epoch9, step2122]: loss 0.595252
[epoch9, step2123]: loss 0.593769
[epoch9, step2124]: loss 0.633355
[epoch9, step2125]: loss 0.640428
[epoch9, step2126]: loss 0.473972
[epoch9, step2127]: loss 0.561962
[epoch9, step2128]: loss 0.534294
[epoch9, step2129]: loss 0.650846
[epoch9, step2130]: loss 0.655080
[epoch9, step2131]: loss 0.469364
[epoch9, step2132]: loss 0.428610
[epoch9, step2133]: loss 0.527371
[epoch9, step2134]: loss 0.663167
[epoch9, step2135]: loss 0.502830
[epoch9, step2136]: loss 0.523991
[epoch9, step2137]: loss 0.666159
[epoch9, step2138]: loss 0.636531
[epoch9, step2139]: loss 0.740484
[epoch9, step2140]: loss 0.154763
[epoch9, step2141]: loss 0.470493
[epoch9, step2142]: loss 0.679297
[epoch9, step2143]: loss 0.650139
[epoch9, step2144]: loss 0.649223
[epoch9, step2145]: loss 0.397695
[epoch9, step2146]: loss 0.408431
[epoch9, step2147]: loss 0.374039
[epoch9, step2148]: loss 0.537922
[epoch9, step2149]: loss 0.409371
[epoch9, step2150]: loss 0.609097
[epoch9, step2151]: loss 0.476365
[epoch9, step2152]: loss 0.557373
[epoch9, step2153]: loss 0.159693
[epoch9, step2154]: loss 0.654538
[epoch9, step2155]: loss 0.634174
[epoch9, step2156]: loss 0.727708
[epoch9, step2157]: loss 0.740917
[epoch9, step2158]: loss 0.725768
[epoch9, step2159]: loss 0.653279
[epoch9, step2160]: loss 0.591764
[epoch9, step2161]: loss 0.362638
[epoch9, step2162]: loss 0.419001
[epoch9, step2163]: loss 0.586160
[epoch9, step2164]: loss 0.386391
[epoch9, step2165]: loss 0.232950
[epoch9, step2166]: loss 0.554444
[epoch9, step2167]: loss 0.630199
[epoch9, step2168]: loss 0.440669
[epoch9, step2169]: loss 0.483911
[epoch9, step2170]: loss 0.519445
[epoch9, step2171]: loss 0.467064
[epoch9, step2172]: loss 0.513754
[epoch9, step2173]: loss 0.435683
[epoch9, step2174]: loss 0.562164
[epoch9, step2175]: loss 0.358805
[epoch9, step2176]: loss 0.603896
[epoch9, step2177]: loss 0.271248
[epoch9, step2178]: loss 0.595805
[epoch9, step2179]: loss 0.532708
[epoch9, step2180]: loss 0.613347
[epoch9, step2181]: loss 0.633365
[epoch9, step2182]: loss 0.657606
[epoch9, step2183]: loss 0.402067
[epoch9, step2184]: loss 0.573157
[epoch9, step2185]: loss 0.449383
[epoch9, step2186]: loss 0.420176
[epoch9, step2187]: loss 0.454199
[epoch9, step2188]: loss 0.640763
[epoch9, step2189]: loss 0.304976
[epoch9, step2190]: loss 0.473604
[epoch9, step2191]: loss 0.526978
[epoch9, step2192]: loss 0.524715
[epoch9, step2193]: loss 0.452494
[epoch9, step2194]: loss 0.590326
[epoch9, step2195]: loss 0.541145
[epoch9, step2196]: loss 0.590822
[epoch9, step2197]: loss 0.321322
[epoch9, step2198]: loss 0.112097
[epoch9, step2199]: loss 0.585041
[epoch9, step2200]: loss 0.358715
[epoch9, step2201]: loss 0.589290
[epoch9, step2202]: loss 0.592454
[epoch9, step2203]: loss 0.610382
[epoch9, step2204]: loss 0.554297
[epoch9, step2205]: loss 0.424974
[epoch9, step2206]: loss 0.663434
[epoch9, step2207]: loss 0.649952
[epoch9, step2208]: loss 0.134655
[epoch9, step2209]: loss 0.497282
[epoch9, step2210]: loss 0.370993
[epoch9, step2211]: loss 0.467448
[epoch9, step2212]: loss 0.398819
[epoch9, step2213]: loss 0.608004
[epoch9, step2214]: loss 0.522167
[epoch9, step2215]: loss 0.323772
[epoch9, step2216]: loss 0.440970
[epoch9, step2217]: loss 0.524296
[epoch9, step2218]: loss 0.379425
[epoch9, step2219]: loss 0.139019
[epoch9, step2220]: loss 0.722503
[epoch9, step2221]: loss 0.548316
[epoch9, step2222]: loss 0.504591
[epoch9, step2223]: loss 0.434892
[epoch9, step2224]: loss 0.643462
[epoch9, step2225]: loss 0.385952
[epoch9, step2226]: loss 0.405752
[epoch9, step2227]: loss 0.615923
[epoch9, step2228]: loss 0.620774
[epoch9, step2229]: loss 0.421722
[epoch9, step2230]: loss 0.654163
[epoch9, step2231]: loss 0.563537
[epoch9, step2232]: loss 0.264804
[epoch9, step2233]: loss 0.741672
[epoch9, step2234]: loss 0.426565
[epoch9, step2235]: loss 0.642303
[epoch9, step2236]: loss 0.349927
[epoch9, step2237]: loss 0.562383
[epoch9, step2238]: loss 0.582904
[epoch9, step2239]: loss 0.408653
[epoch9, step2240]: loss 0.235660
[epoch9, step2241]: loss 0.695035
[epoch9, step2242]: loss 0.378641
[epoch9, step2243]: loss 0.355503
[epoch9, step2244]: loss 0.386707
[epoch9, step2245]: loss 0.714370
[epoch9, step2246]: loss 0.402904
[epoch9, step2247]: loss 0.763975
[epoch9, step2248]: loss 0.598423
[epoch9, step2249]: loss 0.481949
[epoch9, step2250]: loss 0.732612
[epoch9, step2251]: loss 0.535097
[epoch9, step2252]: loss 0.532449
[epoch9, step2253]: loss 0.378361
[epoch9, step2254]: loss 0.763102
[epoch9, step2255]: loss 0.692331
[epoch9, step2256]: loss 0.159917
[epoch9, step2257]: loss 0.563548
[epoch9, step2258]: loss 0.264349
[epoch9, step2259]: loss 0.362493
[epoch9, step2260]: loss 0.539943
[epoch9, step2261]: loss 0.610769
[epoch9, step2262]: loss 0.496745
[epoch9, step2263]: loss 0.552449
[epoch9, step2264]: loss 0.556055
[epoch9, step2265]: loss 0.406117
[epoch9, step2266]: loss 0.556130
[epoch9, step2267]: loss 0.321537
[epoch9, step2268]: loss 0.435676
[epoch9, step2269]: loss 0.522815
[epoch9, step2270]: loss 0.541422
[epoch9, step2271]: loss 0.443834
[epoch9, step2272]: loss 0.626631
[epoch9, step2273]: loss 0.580490
[epoch9, step2274]: loss 0.535523
[epoch9, step2275]: loss 0.334359
[epoch9, step2276]: loss 0.310252
[epoch9, step2277]: loss 0.722763
[epoch9, step2278]: loss 0.543867
[epoch9, step2279]: loss 0.468049
[epoch9, step2280]: loss 0.388406
[epoch9, step2281]: loss 0.746376
[epoch9, step2282]: loss 0.517673
[epoch9, step2283]: loss 0.719867
[epoch9, step2284]: loss 0.272860
[epoch9, step2285]: loss 0.198308
[epoch9, step2286]: loss 0.259470
[epoch9, step2287]: loss 0.274606
[epoch9, step2288]: loss 0.586940
[epoch9, step2289]: loss 0.499271
[epoch9, step2290]: loss 0.605437
[epoch9, step2291]: loss 0.505656
[epoch9, step2292]: loss 0.434563
[epoch9, step2293]: loss 0.269368
[epoch9, step2294]: loss 0.413814
[epoch9, step2295]: loss 0.261632
[epoch9, step2296]: loss 0.301326
[epoch9, step2297]: loss 0.569121
[epoch9, step2298]: loss 0.536230
[epoch9, step2299]: loss 0.576531
[epoch9, step2300]: loss 0.602518
[epoch9, step2301]: loss 0.678532
[epoch9, step2302]: loss 0.228666
[epoch9, step2303]: loss 0.611662
[epoch9, step2304]: loss 0.535755
[epoch9, step2305]: loss 0.526667
[epoch9, step2306]: loss 0.425579
[epoch9, step2307]: loss 0.411564
[epoch9, step2308]: loss 0.709293
[epoch9, step2309]: loss 0.381787
[epoch9, step2310]: loss 0.314388
[epoch9, step2311]: loss 0.645794
[epoch9, step2312]: loss 0.627468
[epoch9, step2313]: loss 0.547200
[epoch9, step2314]: loss 0.325721
[epoch9, step2315]: loss 0.397012
[epoch9, step2316]: loss 0.431609
[epoch9, step2317]: loss 0.500897
[epoch9, step2318]: loss 0.403442
[epoch9, step2319]: loss 0.718596
[epoch9, step2320]: loss 0.607500
[epoch9, step2321]: loss 0.564446
[epoch9, step2322]: loss 0.697135
[epoch9, step2323]: loss 0.435870
[epoch9, step2324]: loss 0.570263
[epoch9, step2325]: loss 0.480150
[epoch9, step2326]: loss 0.529959
[epoch9, step2327]: loss 0.401377
[epoch9, step2328]: loss 0.504204
[epoch9, step2329]: loss 0.375389
[epoch9, step2330]: loss 0.415574
[epoch9, step2331]: loss 0.695061
[epoch9, step2332]: loss 0.568701
[epoch9, step2333]: loss 0.320903
[epoch9, step2334]: loss 0.653509
[epoch9, step2335]: loss 0.509205
[epoch9, step2336]: loss 0.475524
[epoch9, step2337]: loss 0.490810
[epoch9, step2338]: loss 0.447947
[epoch9, step2339]: loss 0.516044
[epoch9, step2340]: loss 0.356765
[epoch9, step2341]: loss 0.462086
[epoch9, step2342]: loss 0.492622
[epoch9, step2343]: loss 0.595796
[epoch9, step2344]: loss 0.461775
[epoch9, step2345]: loss 0.784826
[epoch9, step2346]: loss 0.373951
[epoch9, step2347]: loss 0.748868
[epoch9, step2348]: loss 0.754350
[epoch9, step2349]: loss 0.547328
[epoch9, step2350]: loss 0.338245
[epoch9, step2351]: loss 0.454007
[epoch9, step2352]: loss 0.438283
[epoch9, step2353]: loss 0.469875
[epoch9, step2354]: loss 0.594325
[epoch9, step2355]: loss 0.541229
[epoch9, step2356]: loss 0.600281
[epoch9, step2357]: loss 0.834558
[epoch9, step2358]: loss 0.393114
[epoch9, step2359]: loss 0.574312
[epoch9, step2360]: loss 0.217953
[epoch9, step2361]: loss 0.647206
[epoch9, step2362]: loss 0.192066
[epoch9, step2363]: loss 0.516547
[epoch9, step2364]: loss 0.622892
[epoch9, step2365]: loss 0.532389
[epoch9, step2366]: loss 0.332732
[epoch9, step2367]: loss 0.361530
[epoch9, step2368]: loss 0.397165
[epoch9, step2369]: loss 0.522177
[epoch9, step2370]: loss 0.620466
[epoch9, step2371]: loss 0.136992
[epoch9, step2372]: loss 0.565508
[epoch9, step2373]: loss 0.515659
[epoch9, step2374]: loss 0.445963
[epoch9, step2375]: loss 0.675027
[epoch9, step2376]: loss 0.558233
[epoch9, step2377]: loss 0.468255
[epoch9, step2378]: loss 0.527656
[epoch9, step2379]: loss 0.402220
[epoch9, step2380]: loss 0.582317
[epoch9, step2381]: loss 0.502372
[epoch9, step2382]: loss 0.547070
[epoch9, step2383]: loss 0.513352
[epoch9, step2384]: loss 0.268213
[epoch9, step2385]: loss 0.614920
[epoch9, step2386]: loss 0.713346
[epoch9, step2387]: loss 0.416991
[epoch9, step2388]: loss 0.370754
[epoch9, step2389]: loss 0.280454
[epoch9, step2390]: loss 0.560749
[epoch9, step2391]: loss 0.412997
[epoch9, step2392]: loss 0.634228
[epoch9, step2393]: loss 0.485799
[epoch9, step2394]: loss 0.563751
[epoch9, step2395]: loss 0.467036
[epoch9, step2396]: loss 0.326665
[epoch9, step2397]: loss 0.283825
[epoch9, step2398]: loss 0.421142
[epoch9, step2399]: loss 0.547814
[epoch9, step2400]: loss 0.582938
[epoch9, step2401]: loss 0.344237
[epoch9, step2402]: loss 0.400169
[epoch9, step2403]: loss 0.667752
[epoch9, step2404]: loss 0.422151
[epoch9, step2405]: loss 0.621871
[epoch9, step2406]: loss 0.616409
[epoch9, step2407]: loss 0.591989
[epoch9, step2408]: loss 0.333102
[epoch9, step2409]: loss 0.211377
[epoch9, step2410]: loss 0.505881
[epoch9, step2411]: loss 0.404784
[epoch9, step2412]: loss 0.676176
[epoch9, step2413]: loss 0.541669
[epoch9, step2414]: loss 0.276519
[epoch9, step2415]: loss 0.515436
[epoch9, step2416]: loss 0.457480
[epoch9, step2417]: loss 0.556407
[epoch9, step2418]: loss 0.569071
[epoch9, step2419]: loss 0.744394
[epoch9, step2420]: loss 0.442719
[epoch9, step2421]: loss 0.631306
[epoch9, step2422]: loss 0.468528
[epoch9, step2423]: loss 0.555506
[epoch9, step2424]: loss 0.524673
[epoch9, step2425]: loss 0.256576
[epoch9, step2426]: loss 0.539385
[epoch9, step2427]: loss 0.476917
[epoch9, step2428]: loss 0.504145
[epoch9, step2429]: loss 0.688898
[epoch9, step2430]: loss 0.432853
[epoch9, step2431]: loss 0.588722
[epoch9, step2432]: loss 0.374819
[epoch9, step2433]: loss 0.328476
[epoch9, step2434]: loss 0.239140
[epoch9, step2435]: loss 0.269337
[epoch9, step2436]: loss 0.531295
[epoch9, step2437]: loss 0.625203
[epoch9, step2438]: loss 0.570929
[epoch9, step2439]: loss 0.614575
[epoch9, step2440]: loss 0.604346
[epoch9, step2441]: loss 0.498467
[epoch9, step2442]: loss 0.598837
[epoch9, step2443]: loss 0.632521
[epoch9, step2444]: loss 0.420859
[epoch9, step2445]: loss 0.589186
[epoch9, step2446]: loss 0.507293
[epoch9, step2447]: loss 0.556316
[epoch9, step2448]: loss 0.667198
[epoch9, step2449]: loss 0.586507
[epoch9, step2450]: loss 0.332970
[epoch9, step2451]: loss 0.795568
[epoch9, step2452]: loss 0.689318
[epoch9, step2453]: loss 0.534159
[epoch9, step2454]: loss 0.503956
[epoch9, step2455]: loss 0.565697
[epoch9, step2456]: loss 0.344011
[epoch9, step2457]: loss 0.424689
[epoch9, step2458]: loss 0.501737
[epoch9, step2459]: loss 0.441443
[epoch9, step2460]: loss 0.503560
[epoch9, step2461]: loss 0.416206
[epoch9, step2462]: loss 0.587249
[epoch9, step2463]: loss 0.607277
[epoch9, step2464]: loss 0.669054
[epoch9, step2465]: loss 0.737541
[epoch9, step2466]: loss 0.599929
[epoch9, step2467]: loss 0.353656
[epoch9, step2468]: loss 0.399369
[epoch9, step2469]: loss 0.255806
[epoch9, step2470]: loss 0.679393
[epoch9, step2471]: loss 0.346747
[epoch9, step2472]: loss 0.344629
[epoch9, step2473]: loss 0.665776
[epoch9, step2474]: loss 0.379530
[epoch9, step2475]: loss 0.621271
[epoch9, step2476]: loss 0.596717
[epoch9, step2477]: loss 0.534183
[epoch9, step2478]: loss 0.422109
[epoch9, step2479]: loss 0.469497
[epoch9, step2480]: loss 0.812731
[epoch9, step2481]: loss 0.499300
[epoch9, step2482]: loss 0.556747
[epoch9, step2483]: loss 0.302956
[epoch9, step2484]: loss 0.604965
[epoch9, step2485]: loss 0.353383
[epoch9, step2486]: loss 0.523205
[epoch9, step2487]: loss 0.358900
[epoch9, step2488]: loss 0.549985
[epoch9, step2489]: loss 0.474302
[epoch9, step2490]: loss 0.473845
[epoch9, step2491]: loss 0.682664
[epoch9, step2492]: loss 0.351212
[epoch9, step2493]: loss 0.512238
[epoch9, step2494]: loss 0.532507
[epoch9, step2495]: loss 0.318939
[epoch9, step2496]: loss 0.657475
[epoch9, step2497]: loss 0.431656
[epoch9, step2498]: loss 0.545153
[epoch9, step2499]: loss 0.660230
[epoch9, step2500]: loss 0.495481
[epoch9, step2501]: loss 0.496517
[epoch9, step2502]: loss 0.551284
[epoch9, step2503]: loss 0.485879
[epoch9, step2504]: loss 0.450471
[epoch9, step2505]: loss 0.438860
[epoch9, step2506]: loss 0.276912
[epoch9, step2507]: loss 0.243238
[epoch9, step2508]: loss 0.436254
[epoch9, step2509]: loss 0.397725
[epoch9, step2510]: loss 0.552303
[epoch9, step2511]: loss 0.599572
[epoch9, step2512]: loss 0.505119
[epoch9, step2513]: loss 0.518919
[epoch9, step2514]: loss 0.557477
[epoch9, step2515]: loss 0.735529
[epoch9, step2516]: loss 0.630446
[epoch9, step2517]: loss 0.496865
[epoch9, step2518]: loss 0.372476
[epoch9, step2519]: loss 0.404176
[epoch9, step2520]: loss 0.606816
[epoch9, step2521]: loss 0.391499
[epoch9, step2522]: loss 0.382432
[epoch9, step2523]: loss 0.169968
[epoch9, step2524]: loss 0.336105
[epoch9, step2525]: loss 0.553898
[epoch9, step2526]: loss 0.517595
[epoch9, step2527]: loss 0.458124
[epoch9, step2528]: loss 0.168514
[epoch9, step2529]: loss 0.608697
[epoch9, step2530]: loss 0.489110
[epoch9, step2531]: loss 0.406090
[epoch9, step2532]: loss 0.423035
[epoch9, step2533]: loss 0.631241
[epoch9, step2534]: loss 0.543203
[epoch9, step2535]: loss 0.261577
[epoch9, step2536]: loss 0.453679
[epoch9, step2537]: loss 0.353522
[epoch9, step2538]: loss 0.600575
[epoch9, step2539]: loss 0.346620
[epoch9, step2540]: loss 0.601169
[epoch9, step2541]: loss 0.532805
[epoch9, step2542]: loss 0.463282
[epoch9, step2543]: loss 0.393517
[epoch9, step2544]: loss 0.638890
[epoch9, step2545]: loss 0.628235
[epoch9, step2546]: loss 0.609579
[epoch9, step2547]: loss 0.674213
[epoch9, step2548]: loss 0.465211
[epoch9, step2549]: loss 0.517295
[epoch9, step2550]: loss 0.383938
[epoch9, step2551]: loss 0.630648
[epoch9, step2552]: loss 0.588415
[epoch9, step2553]: loss 0.524455
[epoch9, step2554]: loss 0.543581
[epoch9, step2555]: loss 0.298778
[epoch9, step2556]: loss 0.467082
[epoch9, step2557]: loss 0.677242
[epoch9, step2558]: loss 0.545397
[epoch9, step2559]: loss 0.408362
[epoch9, step2560]: loss 0.526415
[epoch9, step2561]: loss 0.327211
[epoch9, step2562]: loss 0.720235
[epoch9, step2563]: loss 0.459292
[epoch9, step2564]: loss 0.436318
[epoch9, step2565]: loss 0.581822
[epoch9, step2566]: loss 0.551048
[epoch9, step2567]: loss 0.618673
[epoch9, step2568]: loss 0.576247
[epoch9, step2569]: loss 0.273449
[epoch9, step2570]: loss 0.525082
[epoch9, step2571]: loss 0.611388
[epoch9, step2572]: loss 0.665180
[epoch9, step2573]: loss 0.619371
[epoch9, step2574]: loss 0.342060
[epoch9, step2575]: loss 0.418292
[epoch9, step2576]: loss 0.736932
[epoch9, step2577]: loss 0.621606
[epoch9, step2578]: loss 0.662837
[epoch9, step2579]: loss 0.390399
[epoch9, step2580]: loss 0.262223
[epoch9, step2581]: loss 0.517031
[epoch9, step2582]: loss 0.294460
[epoch9, step2583]: loss 0.574187
[epoch9, step2584]: loss 0.397566
[epoch9, step2585]: loss 0.510687
[epoch9, step2586]: loss 0.598717
[epoch9, step2587]: loss 0.461185
[epoch9, step2588]: loss 0.656353
[epoch9, step2589]: loss 0.475877
[epoch9, step2590]: loss 0.455976
[epoch9, step2591]: loss 0.410585
[epoch9, step2592]: loss 0.517526
[epoch9, step2593]: loss 0.497462
[epoch9, step2594]: loss 0.554765
[epoch9, step2595]: loss 0.377863
[epoch9, step2596]: loss 0.427903
[epoch9, step2597]: loss 0.418884
[epoch9, step2598]: loss 0.613133
[epoch9, step2599]: loss 0.730446
[epoch9, step2600]: loss 0.511188
[epoch9, step2601]: loss 0.654541
[epoch9, step2602]: loss 0.606322
[epoch9, step2603]: loss 0.509863
[epoch9, step2604]: loss 0.532999
[epoch9, step2605]: loss 0.434131
[epoch9, step2606]: loss 0.519718
[epoch9, step2607]: loss 0.483432
[epoch9, step2608]: loss 0.312369
[epoch9, step2609]: loss 0.609703
[epoch9, step2610]: loss 0.304567
[epoch9, step2611]: loss 0.498295
[epoch9, step2612]: loss 0.622341
[epoch9, step2613]: loss 0.449662
[epoch9, step2614]: loss 0.570233
[epoch9, step2615]: loss 0.567427
[epoch9, step2616]: loss 0.431676
[epoch9, step2617]: loss 0.634737
[epoch9, step2618]: loss 0.510091
[epoch9, step2619]: loss 0.153584
[epoch9, step2620]: loss 0.484479
[epoch9, step2621]: loss 0.653888
[epoch9, step2622]: loss 0.465094
[epoch9, step2623]: loss 0.692338
[epoch9, step2624]: loss 0.739086
[epoch9, step2625]: loss 0.440498
[epoch9, step2626]: loss 0.344332
[epoch9, step2627]: loss 0.603384
[epoch9, step2628]: loss 0.501994
[epoch9, step2629]: loss 0.523967
[epoch9, step2630]: loss 0.459545
[epoch9, step2631]: loss 0.473878
[epoch9, step2632]: loss 0.369405
[epoch9, step2633]: loss 0.304963
[epoch9, step2634]: loss 0.677129
[epoch9, step2635]: loss 0.485394
[epoch9, step2636]: loss 0.455521
[epoch9, step2637]: loss 0.740048
[epoch9, step2638]: loss 0.508488
[epoch9, step2639]: loss 0.328476
[epoch9, step2640]: loss 0.423769
[epoch9, step2641]: loss 0.295153
[epoch9, step2642]: loss 0.609759
[epoch9, step2643]: loss 0.358815
[epoch9, step2644]: loss 0.644856
[epoch9, step2645]: loss 0.447040
[epoch9, step2646]: loss 0.675534
[epoch9, step2647]: loss 0.587972
[epoch9, step2648]: loss 0.577397
[epoch9, step2649]: loss 0.382238
[epoch9, step2650]: loss 0.562618
[epoch9, step2651]: loss 0.644384
[epoch9, step2652]: loss 0.628306
[epoch9, step2653]: loss 0.491795
[epoch9, step2654]: loss 0.586066
[epoch9, step2655]: loss 0.590220
[epoch9, step2656]: loss 0.473622
[epoch9, step2657]: loss 0.713531
[epoch9, step2658]: loss 0.548190
[epoch9, step2659]: loss 0.523707
[epoch9, step2660]: loss 0.552267
[epoch9, step2661]: loss 0.295682
[epoch9, step2662]: loss 0.586599
[epoch9, step2663]: loss 0.447227
[epoch9, step2664]: loss 0.599804
[epoch9, step2665]: loss 0.765029
[epoch9, step2666]: loss 0.536663
[epoch9, step2667]: loss 0.319688
[epoch9, step2668]: loss 0.568185
[epoch9, step2669]: loss 0.522675
[epoch9, step2670]: loss 0.402734
[epoch9, step2671]: loss 0.685541
[epoch9, step2672]: loss 0.286365
[epoch9, step2673]: loss 0.658357
[epoch9, step2674]: loss 0.756429
[epoch9, step2675]: loss 0.458594
[epoch9, step2676]: loss 0.439591
[epoch9, step2677]: loss 0.471160
[epoch9, step2678]: loss 0.641987
[epoch9, step2679]: loss 0.434799
[epoch9, step2680]: loss 0.741213
[epoch9, step2681]: loss 0.463867
[epoch9, step2682]: loss 0.485636
[epoch9, step2683]: loss 0.619902
[epoch9, step2684]: loss 0.254203
[epoch9, step2685]: loss 0.429925
[epoch9, step2686]: loss 0.584517
[epoch9, step2687]: loss 0.811213
[epoch9, step2688]: loss 0.550695
[epoch9, step2689]: loss 0.515082
[epoch9, step2690]: loss 0.337564
[epoch9, step2691]: loss 0.413076
[epoch9, step2692]: loss 0.419344
[epoch9, step2693]: loss 0.539761
[epoch9, step2694]: loss 0.386925
[epoch9, step2695]: loss 0.368133
[epoch9, step2696]: loss 0.509028
[epoch9, step2697]: loss 0.466846
[epoch9, step2698]: loss 0.564102
[epoch9, step2699]: loss 0.601654
[epoch9, step2700]: loss 0.475604
[epoch9, step2701]: loss 0.822184
[epoch9, step2702]: loss 0.758577
[epoch9, step2703]: loss 0.498402
[epoch9, step2704]: loss 0.520872
[epoch9, step2705]: loss 0.379185
[epoch9, step2706]: loss 0.577059
[epoch9, step2707]: loss 0.637133
[epoch9, step2708]: loss 0.362363
[epoch9, step2709]: loss 0.461171
[epoch9, step2710]: loss 0.648360
[epoch9, step2711]: loss 0.490913
[epoch9, step2712]: loss 0.283175
[epoch9, step2713]: loss 0.452936
[epoch9, step2714]: loss 0.450121
[epoch9, step2715]: loss 0.547180
[epoch9, step2716]: loss 0.294753
[epoch9, step2717]: loss 0.606872
[epoch9, step2718]: loss 0.608753
[epoch9, step2719]: loss 0.539456
[epoch9, step2720]: loss 0.575486
[epoch9, step2721]: loss 0.634517
[epoch9, step2722]: loss 0.291081
[epoch9, step2723]: loss 0.224281
[epoch9, step2724]: loss 0.631636
[epoch9, step2725]: loss 0.742191
[epoch9, step2726]: loss 0.533590
[epoch9, step2727]: loss 0.390883
[epoch9, step2728]: loss 0.416414
[epoch9, step2729]: loss 0.522474
[epoch9, step2730]: loss 0.267133
[epoch9, step2731]: loss 0.642736
[epoch9, step2732]: loss 0.569861
[epoch9, step2733]: loss 0.513397
[epoch9, step2734]: loss 0.611430
[epoch9, step2735]: loss 0.532131
[epoch9, step2736]: loss 0.603012
[epoch9, step2737]: loss 0.462190
[epoch9, step2738]: loss 0.691474
[epoch9, step2739]: loss 0.472310
[epoch9, step2740]: loss 0.584785
[epoch9, step2741]: loss 0.627969
[epoch9, step2742]: loss 0.365046
[epoch9, step2743]: loss 0.287377
[epoch9, step2744]: loss 0.608275
[epoch9, step2745]: loss 0.519895
[epoch9, step2746]: loss 0.404683
[epoch9, step2747]: loss 0.553911
[epoch9, step2748]: loss 0.590009
[epoch9, step2749]: loss 0.482857
[epoch9, step2750]: loss 0.388070
[epoch9, step2751]: loss 0.268798
[epoch9, step2752]: loss 0.262480
[epoch9, step2753]: loss 0.331867
[epoch9, step2754]: loss 0.391639
[epoch9, step2755]: loss 0.668209
[epoch9, step2756]: loss 0.468746
[epoch9, step2757]: loss 0.476516
[epoch9, step2758]: loss 0.408583
[epoch9, step2759]: loss 0.651053
[epoch9, step2760]: loss 0.347907
[epoch9, step2761]: loss 0.455716
[epoch9, step2762]: loss 0.360048
[epoch9, step2763]: loss 0.084892
[epoch9, step2764]: loss 0.482160
[epoch9, step2765]: loss 0.395547
[epoch9, step2766]: loss 0.524454
[epoch9, step2767]: loss 0.572941
[epoch9, step2768]: loss 0.386690
[epoch9, step2769]: loss 0.501456
[epoch9, step2770]: loss 0.354925
[epoch9, step2771]: loss 0.666312
[epoch9, step2772]: loss 0.304769
[epoch9, step2773]: loss 0.290057
[epoch9, step2774]: loss 0.439584
[epoch9, step2775]: loss 0.384542
[epoch9, step2776]: loss 0.442651
[epoch9, step2777]: loss 0.387182
[epoch9, step2778]: loss 0.515382
[epoch9, step2779]: loss 0.531133
[epoch9, step2780]: loss 0.308390
[epoch9, step2781]: loss 0.514977
[epoch9, step2782]: loss 0.269101
[epoch9, step2783]: loss 0.338185
[epoch9, step2784]: loss 0.529736
[epoch9, step2785]: loss 0.204037
[epoch9, step2786]: loss 0.526555
[epoch9, step2787]: loss 0.747289
[epoch9, step2788]: loss 0.565480
[epoch9, step2789]: loss 0.352836
[epoch9, step2790]: loss 0.588720
[epoch9, step2791]: loss 0.638153
[epoch9, step2792]: loss 0.235893
[epoch9, step2793]: loss 0.488739
[epoch9, step2794]: loss 0.445544
[epoch9, step2795]: loss 0.570107
[epoch9, step2796]: loss 0.462502
[epoch9, step2797]: loss 0.522868
[epoch9, step2798]: loss 0.727390
[epoch9, step2799]: loss 0.500268
[epoch9, step2800]: loss 0.468911
[epoch9, step2801]: loss 0.339174
[epoch9, step2802]: loss 0.476026
[epoch9, step2803]: loss 0.382451
[epoch9, step2804]: loss 0.757151
[epoch9, step2805]: loss 0.430256
[epoch9, step2806]: loss 0.412842
[epoch9, step2807]: loss 0.561800
[epoch9, step2808]: loss 0.614854
[epoch9, step2809]: loss 0.729484
[epoch9, step2810]: loss 0.715327
[epoch9, step2811]: loss 0.523076
[epoch9, step2812]: loss 0.572953
[epoch9, step2813]: loss 0.530500
[epoch9, step2814]: loss 0.481890
[epoch9, step2815]: loss 0.330308
[epoch9, step2816]: loss 0.418893
[epoch9, step2817]: loss 0.442709
[epoch9, step2818]: loss 0.605003
[epoch9, step2819]: loss 0.711251
[epoch9, step2820]: loss 0.614220
[epoch9, step2821]: loss 0.102524
[epoch9, step2822]: loss 0.544081
[epoch9, step2823]: loss 0.420278
[epoch9, step2824]: loss 0.572585
[epoch9, step2825]: loss 0.510282
[epoch9, step2826]: loss 0.600972
[epoch9, step2827]: loss 0.527642
[epoch9, step2828]: loss 0.719303
[epoch9, step2829]: loss 0.595071
[epoch9, step2830]: loss 0.370394
[epoch9, step2831]: loss 0.547729
[epoch9, step2832]: loss 0.420922
[epoch9, step2833]: loss 0.604515
[epoch9, step2834]: loss 0.319958
[epoch9, step2835]: loss 0.655299
[epoch9, step2836]: loss 0.496631
[epoch9, step2837]: loss 0.590970
[epoch9, step2838]: loss 0.659645
[epoch9, step2839]: loss 0.548485
[epoch9, step2840]: loss 0.632261
[epoch9, step2841]: loss 0.425317
[epoch9, step2842]: loss 0.474132
[epoch9, step2843]: loss 0.598595
[epoch9, step2844]: loss 0.639255
[epoch9, step2845]: loss 0.526059
[epoch9, step2846]: loss 0.742545
[epoch9, step2847]: loss 0.578893
[epoch9, step2848]: loss 0.535385
[epoch9, step2849]: loss 0.573887
[epoch9, step2850]: loss 0.632405
[epoch9, step2851]: loss 0.387767
[epoch9, step2852]: loss 0.580872
[epoch9, step2853]: loss 0.610095
[epoch9, step2854]: loss 0.586006
[epoch9, step2855]: loss 0.688049
[epoch9, step2856]: loss 0.676442
[epoch9, step2857]: loss 0.223001
[epoch9, step2858]: loss 0.654441
[epoch9, step2859]: loss 0.443063
[epoch9, step2860]: loss 0.366903
[epoch9, step2861]: loss 0.473714
[epoch9, step2862]: loss 0.467488
[epoch9, step2863]: loss 0.535826
[epoch9, step2864]: loss 0.504560
[epoch9, step2865]: loss 0.434984
[epoch9, step2866]: loss 0.606672
[epoch9, step2867]: loss 0.454330
[epoch9, step2868]: loss 0.297011
[epoch9, step2869]: loss 0.591342
[epoch9, step2870]: loss 0.464050
[epoch9, step2871]: loss 0.548954
[epoch9, step2872]: loss 0.511172
[epoch9, step2873]: loss 0.272071
[epoch9, step2874]: loss 0.270414
[epoch9, step2875]: loss 0.577785
[epoch9, step2876]: loss 0.663106
[epoch9, step2877]: loss 0.420249
[epoch9, step2878]: loss 0.510687
[epoch9, step2879]: loss 0.542325
[epoch9, step2880]: loss 0.600150
[epoch9, step2881]: loss 0.513827
[epoch9, step2882]: loss 0.744956
[epoch9, step2883]: loss 0.341392
[epoch9, step2884]: loss 0.688446
[epoch9, step2885]: loss 0.653636
[epoch9, step2886]: loss 0.562844
[epoch9, step2887]: loss 0.613374
[epoch9, step2888]: loss 0.495715
[epoch9, step2889]: loss 0.624251
[epoch9, step2890]: loss 0.636209
[epoch9, step2891]: loss 0.613250
[epoch9, step2892]: loss 0.443329
[epoch9, step2893]: loss 0.540115
[epoch9, step2894]: loss 0.627655
[epoch9, step2895]: loss 0.664337
[epoch9, step2896]: loss 0.587843
[epoch9, step2897]: loss 0.503064
[epoch9, step2898]: loss 0.534565
[epoch9, step2899]: loss 0.680903
[epoch9, step2900]: loss 0.452263
[epoch9, step2901]: loss 0.571025
[epoch9, step2902]: loss 0.287077
[epoch9, step2903]: loss 0.564635
[epoch9, step2904]: loss 0.742743
[epoch9, step2905]: loss 0.520952
[epoch9, step2906]: loss 0.582049
[epoch9, step2907]: loss 0.390587
[epoch9, step2908]: loss 0.495105
[epoch9, step2909]: loss 0.462657
[epoch9, step2910]: loss 0.591973
[epoch9, step2911]: loss 0.571236
[epoch9, step2912]: loss 0.296040
[epoch9, step2913]: loss 0.744851
[epoch9, step2914]: loss 0.554994
[epoch9, step2915]: loss 0.366681
[epoch9, step2916]: loss 0.370963
[epoch9, step2917]: loss 0.530732
[epoch9, step2918]: loss 0.368325
[epoch9, step2919]: loss 0.575215
[epoch9, step2920]: loss 0.299373
[epoch9, step2921]: loss 0.549646
[epoch9, step2922]: loss 0.606899
[epoch9, step2923]: loss 0.563146
[epoch9, step2924]: loss 0.595759
[epoch9, step2925]: loss 0.381148
[epoch9, step2926]: loss 0.482066
[epoch9, step2927]: loss 0.487197
[epoch9, step2928]: loss 0.462348
[epoch9, step2929]: loss 0.455360
[epoch9, step2930]: loss 0.594226
[epoch9, step2931]: loss 0.674881
[epoch9, step2932]: loss 0.540338
[epoch9, step2933]: loss 0.444651
[epoch9, step2934]: loss 0.549533
[epoch9, step2935]: loss 0.499637
[epoch9, step2936]: loss 0.594938
[epoch9, step2937]: loss 0.458233
[epoch9, step2938]: loss 0.543612
[epoch9, step2939]: loss 0.452439
[epoch9, step2940]: loss 0.380886
[epoch9, step2941]: loss 0.594472
[epoch9, step2942]: loss 0.526210
[epoch9, step2943]: loss 0.591495
[epoch9, step2944]: loss 0.543614
[epoch9, step2945]: loss 0.574590
[epoch9, step2946]: loss 0.580057
[epoch9, step2947]: loss 0.615511
[epoch9, step2948]: loss 0.277974
[epoch9, step2949]: loss 0.573503
[epoch9, step2950]: loss 0.441097
[epoch9, step2951]: loss 0.512210
[epoch9, step2952]: loss 0.444392
[epoch9, step2953]: loss 0.289791
[epoch9, step2954]: loss 0.409508
[epoch9, step2955]: loss 0.636102
[epoch9, step2956]: loss 0.468593
[epoch9, step2957]: loss 0.410133
[epoch9, step2958]: loss 0.610283
[epoch9, step2959]: loss 0.390439
[epoch9, step2960]: loss 0.617019
[epoch9, step2961]: loss 0.503397
[epoch9, step2962]: loss 0.528354
[epoch9, step2963]: loss 0.724101
[epoch9, step2964]: loss 0.435708
[epoch9, step2965]: loss 0.470042
[epoch9, step2966]: loss 0.673229
[epoch9, step2967]: loss 0.419327
[epoch9, step2968]: loss 0.411455
[epoch9, step2969]: loss 0.319859
[epoch9, step2970]: loss 0.395279
[epoch9, step2971]: loss 0.425885
[epoch9, step2972]: loss 0.454804
[epoch9, step2973]: loss 0.310990
[epoch9, step2974]: loss 0.461533
[epoch9, step2975]: loss 0.717432
[epoch9, step2976]: loss 0.342017
[epoch9, step2977]: loss 0.609859
[epoch9, step2978]: loss 0.410824
[epoch9, step2979]: loss 0.616238
[epoch9, step2980]: loss 0.357257
[epoch9, step2981]: loss 0.478902
[epoch9, step2982]: loss 0.308626
[epoch9, step2983]: loss 0.505536
[epoch9, step2984]: loss 0.706583
[epoch9, step2985]: loss 0.597527
[epoch9, step2986]: loss 0.579848
[epoch9, step2987]: loss 0.444221
[epoch9, step2988]: loss 0.473364
[epoch9, step2989]: loss 0.399929
[epoch9, step2990]: loss 0.537533
[epoch9, step2991]: loss 0.410417
[epoch9, step2992]: loss 0.583261
[epoch9, step2993]: loss 0.623439
[epoch9, step2994]: loss 0.720834
[epoch9, step2995]: loss 0.524607
[epoch9, step2996]: loss 0.494729
[epoch9, step2997]: loss 0.476873
[epoch9, step2998]: loss 0.746173
[epoch9, step2999]: loss 0.392154
[epoch9, step3000]: loss 0.473164
[epoch9, step3001]: loss 0.478331
[epoch9, step3002]: loss 0.525713
[epoch9, step3003]: loss 0.453851
[epoch9, step3004]: loss 0.675808
[epoch9, step3005]: loss 0.505740
[epoch9, step3006]: loss 0.543419
[epoch9, step3007]: loss 0.523962
[epoch9, step3008]: loss 0.591104
[epoch9, step3009]: loss 0.612943
[epoch9, step3010]: loss 0.569929
[epoch9, step3011]: loss 0.483743
[epoch9, step3012]: loss 0.637456
[epoch9, step3013]: loss 0.560168
[epoch9, step3014]: loss 0.563048
[epoch9, step3015]: loss 0.662354
[epoch9, step3016]: loss 0.105492
[epoch9, step3017]: loss 0.495591
[epoch9, step3018]: loss 0.463886
[epoch9, step3019]: loss 0.510646
[epoch9, step3020]: loss 0.521702
[epoch9, step3021]: loss 0.548086
[epoch9, step3022]: loss 0.497549
[epoch9, step3023]: loss 0.602979
[epoch9, step3024]: loss 0.487994
[epoch9, step3025]: loss 0.552609
[epoch9, step3026]: loss 0.545657
[epoch9, step3027]: loss 0.401697
[epoch9, step3028]: loss 0.655404
[epoch9, step3029]: loss 0.595165
[epoch9, step3030]: loss 0.627798
[epoch9, step3031]: loss 0.572828
[epoch9, step3032]: loss 0.631602
[epoch9, step3033]: loss 0.387349
[epoch9, step3034]: loss 0.396325
[epoch9, step3035]: loss 0.426967
[epoch9, step3036]: loss 0.576062
[epoch9, step3037]: loss 0.690374
[epoch9, step3038]: loss 0.355284
[epoch9, step3039]: loss 0.502457
[epoch9, step3040]: loss 0.536361
[epoch9, step3041]: loss 0.571546
[epoch9, step3042]: loss 0.540310
[epoch9, step3043]: loss 0.436067
[epoch9, step3044]: loss 0.680707
[epoch9, step3045]: loss 0.403242
[epoch9, step3046]: loss 0.649687
[epoch9, step3047]: loss 0.485827
[epoch9, step3048]: loss 0.523136
[epoch9, step3049]: loss 0.498707
[epoch9, step3050]: loss 0.635090
[epoch9, step3051]: loss 0.502864
[epoch9, step3052]: loss 0.429711
[epoch9, step3053]: loss 0.391381
[epoch9, step3054]: loss 0.398941
[epoch9, step3055]: loss 0.452946
[epoch9, step3056]: loss 0.535003
[epoch9, step3057]: loss 0.583931
[epoch9, step3058]: loss 0.219647
[epoch9, step3059]: loss 0.551946
[epoch9, step3060]: loss 0.475752
[epoch9, step3061]: loss 0.314127
[epoch9, step3062]: loss 0.744207
[epoch9, step3063]: loss 0.717110
[epoch9, step3064]: loss 0.332226
[epoch9, step3065]: loss 0.567701
[epoch9, step3066]: loss 0.599088
[epoch9, step3067]: loss 0.539140
[epoch9, step3068]: loss 0.628270
[epoch9, step3069]: loss 0.570763
[epoch9, step3070]: loss 0.595421
[epoch9, step3071]: loss 0.457023
[epoch9, step3072]: loss 0.252571
[epoch9, step3073]: loss 0.485996
[epoch9, step3074]: loss 0.616428
[epoch9, step3075]: loss 0.209507
[epoch9, step3076]: loss 0.483620

[epoch9]: avg loss 0.483620

[epoch10, step1]: loss 0.622724
[epoch10, step2]: loss 0.359082
[epoch10, step3]: loss 0.600879
[epoch10, step4]: loss 0.426755
[epoch10, step5]: loss 0.515201
[epoch10, step6]: loss 0.485969
[epoch10, step7]: loss 0.562740
[epoch10, step8]: loss 0.565393
[epoch10, step9]: loss 0.668967
[epoch10, step10]: loss 0.376851
[epoch10, step11]: loss 0.501690
[epoch10, step12]: loss 0.532245
[epoch10, step13]: loss 0.517204
[epoch10, step14]: loss 0.447411
[epoch10, step15]: loss 0.304812
[epoch10, step16]: loss 0.536127
[epoch10, step17]: loss 0.439928
[epoch10, step18]: loss 0.676042
[epoch10, step19]: loss 0.647434
[epoch10, step20]: loss 0.410976
[epoch10, step21]: loss 0.583574
[epoch10, step22]: loss 0.714519
[epoch10, step23]: loss 0.489833
[epoch10, step24]: loss 0.484476
[epoch10, step25]: loss 0.486675
[epoch10, step26]: loss 0.462685
[epoch10, step27]: loss 0.516332
[epoch10, step28]: loss 0.680089
[epoch10, step29]: loss 0.609436
[epoch10, step30]: loss 0.343904
[epoch10, step31]: loss 0.410213
[epoch10, step32]: loss 0.228179
[epoch10, step33]: loss 0.572741
[epoch10, step34]: loss 0.690340
[epoch10, step35]: loss 0.297880
[epoch10, step36]: loss 0.578354
[epoch10, step37]: loss 0.453237
[epoch10, step38]: loss 0.671205
[epoch10, step39]: loss 0.195068
[epoch10, step40]: loss 0.490613
[epoch10, step41]: loss 0.558727
[epoch10, step42]: loss 0.725110
[epoch10, step43]: loss 0.561779
[epoch10, step44]: loss 0.426663
[epoch10, step45]: loss 0.483055
[epoch10, step46]: loss 0.497548
[epoch10, step47]: loss 0.362289
[epoch10, step48]: loss 0.580240
[epoch10, step49]: loss 0.488022
[epoch10, step50]: loss 0.411996
[epoch10, step51]: loss 0.502168
[epoch10, step52]: loss 0.572104
[epoch10, step53]: loss 0.354899
[epoch10, step54]: loss 0.460004
[epoch10, step55]: loss 0.168763
[epoch10, step56]: loss 0.502173
[epoch10, step57]: loss 0.448129
[epoch10, step58]: loss 0.696985
[epoch10, step59]: loss 0.395474
[epoch10, step60]: loss 0.589886
[epoch10, step61]: loss 0.629428
[epoch10, step62]: loss 0.571465
[epoch10, step63]: loss 0.359927
[epoch10, step64]: loss 0.299992
[epoch10, step65]: loss 0.781052
[epoch10, step66]: loss 0.305079
[epoch10, step67]: loss 0.459174
[epoch10, step68]: loss 0.607988
[epoch10, step69]: loss 0.309865
[epoch10, step70]: loss 0.235107
[epoch10, step71]: loss 0.455363
[epoch10, step72]: loss 0.473166
[epoch10, step73]: loss 0.536188
[epoch10, step74]: loss 0.276481
[epoch10, step75]: loss 0.565611
[epoch10, step76]: loss 0.199684
[epoch10, step77]: loss 0.385833
[epoch10, step78]: loss 0.228595
[epoch10, step79]: loss 0.441117
[epoch10, step80]: loss 0.335699
[epoch10, step81]: loss 0.502535
[epoch10, step82]: loss 0.669516
[epoch10, step83]: loss 0.729666
[epoch10, step84]: loss 0.683712
[epoch10, step85]: loss 0.678718
[epoch10, step86]: loss 0.706973
[epoch10, step87]: loss 0.602363
[epoch10, step88]: loss 0.270273
[epoch10, step89]: loss 0.422883
[epoch10, step90]: loss 0.482327
[epoch10, step91]: loss 0.597458
[epoch10, step92]: loss 0.374545
[epoch10, step93]: loss 0.586875
[epoch10, step94]: loss 0.595705
[epoch10, step95]: loss 0.414627
[epoch10, step96]: loss 0.629731
[epoch10, step97]: loss 0.560678
[epoch10, step98]: loss 0.591322
[epoch10, step99]: loss 0.083209
[epoch10, step100]: loss 0.528779
[epoch10, step101]: loss 0.553714
[epoch10, step102]: loss 0.524804
[epoch10, step103]: loss 0.516159
[epoch10, step104]: loss 0.298278
[epoch10, step105]: loss 0.426445
[epoch10, step106]: loss 0.452092
[epoch10, step107]: loss 0.409126
[epoch10, step108]: loss 0.566955
[epoch10, step109]: loss 0.472328
[epoch10, step110]: loss 0.453969
[epoch10, step111]: loss 0.401349
[epoch10, step112]: loss 0.641350
[epoch10, step113]: loss 0.503926
[epoch10, step114]: loss 0.526424
[epoch10, step115]: loss 0.706408
[epoch10, step116]: loss 0.664567
[epoch10, step117]: loss 0.541542
[epoch10, step118]: loss 0.486962
[epoch10, step119]: loss 0.695891
[epoch10, step120]: loss 0.406779
[epoch10, step121]: loss 0.527181
[epoch10, step122]: loss 0.730316
[epoch10, step123]: loss 0.586201
[epoch10, step124]: loss 0.396049
[epoch10, step125]: loss 0.769521
[epoch10, step126]: loss 0.493244
[epoch10, step127]: loss 0.463709
[epoch10, step128]: loss 0.480556
[epoch10, step129]: loss 0.423166
[epoch10, step130]: loss 0.617598
[epoch10, step131]: loss 0.326921
[epoch10, step132]: loss 0.437609
[epoch10, step133]: loss 0.709616
[epoch10, step134]: loss 0.595004
[epoch10, step135]: loss 0.349056
[epoch10, step136]: loss 0.547514
[epoch10, step137]: loss 0.411032
[epoch10, step138]: loss 0.518546
[epoch10, step139]: loss 0.484494
[epoch10, step140]: loss 0.257346
[epoch10, step141]: loss 0.254705
[epoch10, step142]: loss 0.422711
[epoch10, step143]: loss 0.543542
[epoch10, step144]: loss 0.638534
[epoch10, step145]: loss 0.599701
[epoch10, step146]: loss 0.180742
[epoch10, step147]: loss 0.432219
[epoch10, step148]: loss 0.712750
[epoch10, step149]: loss 0.582431
[epoch10, step150]: loss 0.521923
[epoch10, step151]: loss 0.530618
[epoch10, step152]: loss 0.349511
[epoch10, step153]: loss 0.603499
[epoch10, step154]: loss 0.520508
[epoch10, step155]: loss 0.657697
[epoch10, step156]: loss 0.694915
[epoch10, step157]: loss 0.641323
[epoch10, step158]: loss 0.411222
[epoch10, step159]: loss 0.557451
[epoch10, step160]: loss 0.810857
[epoch10, step161]: loss 0.451016
[epoch10, step162]: loss 0.374470
[epoch10, step163]: loss 0.283440
[epoch10, step164]: loss 0.434550
[epoch10, step165]: loss 0.594666
[epoch10, step166]: loss 0.486219
[epoch10, step167]: loss 0.693775
[epoch10, step168]: loss 0.307294
[epoch10, step169]: loss 0.588307
[epoch10, step170]: loss 0.690168
[epoch10, step171]: loss 0.651517
[epoch10, step172]: loss 0.696880
[epoch10, step173]: loss 0.506067
[epoch10, step174]: loss 0.629301
[epoch10, step175]: loss 0.489198
[epoch10, step176]: loss 0.268614
[epoch10, step177]: loss 0.425870
[epoch10, step178]: loss 0.557352
[epoch10, step179]: loss 0.433014
[epoch10, step180]: loss 0.324495
[epoch10, step181]: loss 0.556793
[epoch10, step182]: loss 0.489212
[epoch10, step183]: loss 0.519176
[epoch10, step184]: loss 0.106319
[epoch10, step185]: loss 0.467520
[epoch10, step186]: loss 0.273336
[epoch10, step187]: loss 0.577040
[epoch10, step188]: loss 0.512434
[epoch10, step189]: loss 0.504272
[epoch10, step190]: loss 0.353208
[epoch10, step191]: loss 0.660070
[epoch10, step192]: loss 0.515352
[epoch10, step193]: loss 0.340221
[epoch10, step194]: loss 0.452204
[epoch10, step195]: loss 0.534256
[epoch10, step196]: loss 0.308864
[epoch10, step197]: loss 0.503809
[epoch10, step198]: loss 0.562364
[epoch10, step199]: loss 0.439594
[epoch10, step200]: loss 0.508463
[epoch10, step201]: loss 0.433432
[epoch10, step202]: loss 0.505925
[epoch10, step203]: loss 0.648323
[epoch10, step204]: loss 0.554151
[epoch10, step205]: loss 0.463663
[epoch10, step206]: loss 0.326346
[epoch10, step207]: loss 0.749861
[epoch10, step208]: loss 0.560975
[epoch10, step209]: loss 0.482419
[epoch10, step210]: loss 0.281056
[epoch10, step211]: loss 0.629323
[epoch10, step212]: loss 0.598800
[epoch10, step213]: loss 0.518363
[epoch10, step214]: loss 0.220760
[epoch10, step215]: loss 0.597229
[epoch10, step216]: loss 0.419955
[epoch10, step217]: loss 0.375786
[epoch10, step218]: loss 0.605934
[epoch10, step219]: loss 0.626414
[epoch10, step220]: loss 0.559699
[epoch10, step221]: loss 0.581374
[epoch10, step222]: loss 0.312234
[epoch10, step223]: loss 0.261631
[epoch10, step224]: loss 0.489986
[epoch10, step225]: loss 0.386323
[epoch10, step226]: loss 0.530691
[epoch10, step227]: loss 0.452474
[epoch10, step228]: loss 0.498249
[epoch10, step229]: loss 0.406615
[epoch10, step230]: loss 0.466711
[epoch10, step231]: loss 0.373155
[epoch10, step232]: loss 0.579530
[epoch10, step233]: loss 0.438761
[epoch10, step234]: loss 0.686740
[epoch10, step235]: loss 0.636878
[epoch10, step236]: loss 0.512928
[epoch10, step237]: loss 0.589686
[epoch10, step238]: loss 0.519220
[epoch10, step239]: loss 0.597444
[epoch10, step240]: loss 0.600201
[epoch10, step241]: loss 0.559717
[epoch10, step242]: loss 0.400956
[epoch10, step243]: loss 0.471343
[epoch10, step244]: loss 0.382054
[epoch10, step245]: loss 0.368784
[epoch10, step246]: loss 0.689466
[epoch10, step247]: loss 0.590887
[epoch10, step248]: loss 0.533270
[epoch10, step249]: loss 0.577198
[epoch10, step250]: loss 0.707340
[epoch10, step251]: loss 0.560579
[epoch10, step252]: loss 0.278619
[epoch10, step253]: loss 0.375526
[epoch10, step254]: loss 0.575112
[epoch10, step255]: loss 0.624300
[epoch10, step256]: loss 0.637290
[epoch10, step257]: loss 0.460605
[epoch10, step258]: loss 0.573120
[epoch10, step259]: loss 0.273854
[epoch10, step260]: loss 0.613876
[epoch10, step261]: loss 0.531763
[epoch10, step262]: loss 0.653621
[epoch10, step263]: loss 0.286520
[epoch10, step264]: loss 0.601605
[epoch10, step265]: loss 0.325098
[epoch10, step266]: loss 0.518757
[epoch10, step267]: loss 0.727435
[epoch10, step268]: loss 0.393944
[epoch10, step269]: loss 0.374233
[epoch10, step270]: loss 0.812040
[epoch10, step271]: loss 0.417688
[epoch10, step272]: loss 0.382225
[epoch10, step273]: loss 0.628586
[epoch10, step274]: loss 0.477776
[epoch10, step275]: loss 0.617412
[epoch10, step276]: loss 0.606969
[epoch10, step277]: loss 0.420207
[epoch10, step278]: loss 0.403492
[epoch10, step279]: loss 0.715459
[epoch10, step280]: loss 0.536408
[epoch10, step281]: loss 0.581436
[epoch10, step282]: loss 0.568710
[epoch10, step283]: loss 0.458537
[epoch10, step284]: loss 0.355845
[epoch10, step285]: loss 0.756506
[epoch10, step286]: loss 0.588995
[epoch10, step287]: loss 0.403286
[epoch10, step288]: loss 0.718850
[epoch10, step289]: loss 0.566243
[epoch10, step290]: loss 0.370453
[epoch10, step291]: loss 0.350796
[epoch10, step292]: loss 0.660775
[epoch10, step293]: loss 0.508550
[epoch10, step294]: loss 0.552130
[epoch10, step295]: loss 0.478860
[epoch10, step296]: loss 0.267059
[epoch10, step297]: loss 0.471162
[epoch10, step298]: loss 0.532483
[epoch10, step299]: loss 0.367082
[epoch10, step300]: loss 0.255465
[epoch10, step301]: loss 0.541856
[epoch10, step302]: loss 0.576757
[epoch10, step303]: loss 0.833980
[epoch10, step304]: loss 0.495071
[epoch10, step305]: loss 0.314120
[epoch10, step306]: loss 0.660841
[epoch10, step307]: loss 0.418919
[epoch10, step308]: loss 0.410033
[epoch10, step309]: loss 0.808317
[epoch10, step310]: loss 0.645726
[epoch10, step311]: loss 0.429868
[epoch10, step312]: loss 0.498853
[epoch10, step313]: loss 0.270125
[epoch10, step314]: loss 0.609037
[epoch10, step315]: loss 0.569684
[epoch10, step316]: loss 0.542498
[epoch10, step317]: loss 0.506913
[epoch10, step318]: loss 0.661773
[epoch10, step319]: loss 0.365310
[epoch10, step320]: loss 0.427446
[epoch10, step321]: loss 0.631727
[epoch10, step322]: loss 0.441101
[epoch10, step323]: loss 0.564603
[epoch10, step324]: loss 0.565312
[epoch10, step325]: loss 0.449197
[epoch10, step326]: loss 0.379376
[epoch10, step327]: loss 0.528910
[epoch10, step328]: loss 0.502779
[epoch10, step329]: loss 0.643637
[epoch10, step330]: loss 0.191000
[epoch10, step331]: loss 0.691859
[epoch10, step332]: loss 0.417061
[epoch10, step333]: loss 0.648720
[epoch10, step334]: loss 0.241343
[epoch10, step335]: loss 0.639395
[epoch10, step336]: loss 0.662168
[epoch10, step337]: loss 0.260821
[epoch10, step338]: loss 0.557083
[epoch10, step339]: loss 0.451361
[epoch10, step340]: loss 0.515428
[epoch10, step341]: loss 0.604825
[epoch10, step342]: loss 0.549920
[epoch10, step343]: loss 0.703144
[epoch10, step344]: loss 0.675937
[epoch10, step345]: loss 0.520843
[epoch10, step346]: loss 0.403560
[epoch10, step347]: loss 0.455376
[epoch10, step348]: loss 0.635626
[epoch10, step349]: loss 0.465202
[epoch10, step350]: loss 0.339003
[epoch10, step351]: loss 0.456693
[epoch10, step352]: loss 0.502260
[epoch10, step353]: loss 0.641827
[epoch10, step354]: loss 0.610639
[epoch10, step355]: loss 0.807561
[epoch10, step356]: loss 0.253216
[epoch10, step357]: loss 0.551078
[epoch10, step358]: loss 0.619284
[epoch10, step359]: loss 0.353909
[epoch10, step360]: loss 0.625362
[epoch10, step361]: loss 0.645943
[epoch10, step362]: loss 0.397090
[epoch10, step363]: loss 0.372432
[epoch10, step364]: loss 0.295008
[epoch10, step365]: loss 0.504837
[epoch10, step366]: loss 0.274161
[epoch10, step367]: loss 0.645527
[epoch10, step368]: loss 0.732190
[epoch10, step369]: loss 0.584106
[epoch10, step370]: loss 0.750952
[epoch10, step371]: loss 0.640558
[epoch10, step372]: loss 0.585653
[epoch10, step373]: loss 0.209692
[epoch10, step374]: loss 0.591378
[epoch10, step375]: loss 0.715443
[epoch10, step376]: loss 0.425717
[epoch10, step377]: loss 0.546660
[epoch10, step378]: loss 0.283988
[epoch10, step379]: loss 0.479469
[epoch10, step380]: loss 0.521733
[epoch10, step381]: loss 0.615702
[epoch10, step382]: loss 0.362166
[epoch10, step383]: loss 0.341082
[epoch10, step384]: loss 0.588666
[epoch10, step385]: loss 0.276197
[epoch10, step386]: loss 0.351403
[epoch10, step387]: loss 0.548895
[epoch10, step388]: loss 0.387626
[epoch10, step389]: loss 0.228550
[epoch10, step390]: loss 0.560166
[epoch10, step391]: loss 0.419783
[epoch10, step392]: loss 0.441725
[epoch10, step393]: loss 0.516787
[epoch10, step394]: loss 0.401904
[epoch10, step395]: loss 0.624788
[epoch10, step396]: loss 0.248779
[epoch10, step397]: loss 0.632890
[epoch10, step398]: loss 0.755315
[epoch10, step399]: loss 0.514137
[epoch10, step400]: loss 0.516449
[epoch10, step401]: loss 0.651998
[epoch10, step402]: loss 0.589631
[epoch10, step403]: loss 0.464766
[epoch10, step404]: loss 0.417865
[epoch10, step405]: loss 0.698909
[epoch10, step406]: loss 0.671349
[epoch10, step407]: loss 0.343641
[epoch10, step408]: loss 0.704883
[epoch10, step409]: loss 0.548734
[epoch10, step410]: loss 0.660807
[epoch10, step411]: loss 0.633095
[epoch10, step412]: loss 0.284841
[epoch10, step413]: loss 0.584974
[epoch10, step414]: loss 0.503779
[epoch10, step415]: loss 0.723147
[epoch10, step416]: loss 0.512496
[epoch10, step417]: loss 0.529774
[epoch10, step418]: loss 0.349031
[epoch10, step419]: loss 0.569422
[epoch10, step420]: loss 0.505772
[epoch10, step421]: loss 0.351931
[epoch10, step422]: loss 0.625728
[epoch10, step423]: loss 0.681572
[epoch10, step424]: loss 0.651349
[epoch10, step425]: loss 0.518166
[epoch10, step426]: loss 0.602089
[epoch10, step427]: loss 0.677217
[epoch10, step428]: loss 0.506390
[epoch10, step429]: loss 0.496498
[epoch10, step430]: loss 0.731006
[epoch10, step431]: loss 0.673716
[epoch10, step432]: loss 0.497339
[epoch10, step433]: loss 0.537002
[epoch10, step434]: loss 0.280240
[epoch10, step435]: loss 0.386682
[epoch10, step436]: loss 0.241356
[epoch10, step437]: loss 0.691401
[epoch10, step438]: loss 0.718588
[epoch10, step439]: loss 0.590366
[epoch10, step440]: loss 0.624177
[epoch10, step441]: loss 0.585755
[epoch10, step442]: loss 0.432806
[epoch10, step443]: loss 0.493863
[epoch10, step444]: loss 0.656085
[epoch10, step445]: loss 0.612600
[epoch10, step446]: loss 0.641101
[epoch10, step447]: loss 0.385669
[epoch10, step448]: loss 0.489454
[epoch10, step449]: loss 0.593683
[epoch10, step450]: loss 0.700924
[epoch10, step451]: loss 0.623437
[epoch10, step452]: loss 0.124044
[epoch10, step453]: loss 0.524550
[epoch10, step454]: loss 0.462395
[epoch10, step455]: loss 0.545560
[epoch10, step456]: loss 0.501805
[epoch10, step457]: loss 0.586246
[epoch10, step458]: loss 0.420395
[epoch10, step459]: loss 0.458198
[epoch10, step460]: loss 0.332572
[epoch10, step461]: loss 0.646631
[epoch10, step462]: loss 0.540642
[epoch10, step463]: loss 0.652665
[epoch10, step464]: loss 0.591231
[epoch10, step465]: loss 0.636791
[epoch10, step466]: loss 0.561427
[epoch10, step467]: loss 0.481101
[epoch10, step468]: loss 0.555971
[epoch10, step469]: loss 0.408911
[epoch10, step470]: loss 0.444822
[epoch10, step471]: loss 0.444193
[epoch10, step472]: loss 0.644590
[epoch10, step473]: loss 0.538534
[epoch10, step474]: loss 0.525246
[epoch10, step475]: loss 0.584409
[epoch10, step476]: loss 0.576450
[epoch10, step477]: loss 0.656728
[epoch10, step478]: loss 0.489245
[epoch10, step479]: loss 0.530457
[epoch10, step480]: loss 0.786572
[epoch10, step481]: loss 0.619724
[epoch10, step482]: loss 0.491154
[epoch10, step483]: loss 0.573383
[epoch10, step484]: loss 0.605665
[epoch10, step485]: loss 0.172145
[epoch10, step486]: loss 0.271089
[epoch10, step487]: loss 0.455041
[epoch10, step488]: loss 0.366451
[epoch10, step489]: loss 0.485492
[epoch10, step490]: loss 0.542050
[epoch10, step491]: loss 0.102614
[epoch10, step492]: loss 0.587851
[epoch10, step493]: loss 0.605685
[epoch10, step494]: loss 0.596394
[epoch10, step495]: loss 0.481620
[epoch10, step496]: loss 0.347211
[epoch10, step497]: loss 0.567162
[epoch10, step498]: loss 0.551781
[epoch10, step499]: loss 0.601538
[epoch10, step500]: loss 0.574194
[epoch10, step501]: loss 0.511273
[epoch10, step502]: loss 0.446427
[epoch10, step503]: loss 0.332002
[epoch10, step504]: loss 0.539666
[epoch10, step505]: loss 0.261026
[epoch10, step506]: loss 0.720783
[epoch10, step507]: loss 0.471292
[epoch10, step508]: loss 0.544490
[epoch10, step509]: loss 0.537931
[epoch10, step510]: loss 0.461087
[epoch10, step511]: loss 0.337489
[epoch10, step512]: loss 0.692988
[epoch10, step513]: loss 0.493691
[epoch10, step514]: loss 0.321540
[epoch10, step515]: loss 0.433032
[epoch10, step516]: loss 0.488696
[epoch10, step517]: loss 0.481210
[epoch10, step518]: loss 0.281076
[epoch10, step519]: loss 0.351351
[epoch10, step520]: loss 0.396669
[epoch10, step521]: loss 0.647013
[epoch10, step522]: loss 0.506764
[epoch10, step523]: loss 0.558397
[epoch10, step524]: loss 0.461561
[epoch10, step525]: loss 0.427405
[epoch10, step526]: loss 0.526141
[epoch10, step527]: loss 0.441586
[epoch10, step528]: loss 0.648964
[epoch10, step529]: loss 0.639133
[epoch10, step530]: loss 0.510320
[epoch10, step531]: loss 0.303132
[epoch10, step532]: loss 0.372444
[epoch10, step533]: loss 0.495846
[epoch10, step534]: loss 0.573796
[epoch10, step535]: loss 0.685699
[epoch10, step536]: loss 0.556465
[epoch10, step537]: loss 0.546906
[epoch10, step538]: loss 0.438820
[epoch10, step539]: loss 0.573079
[epoch10, step540]: loss 0.425088
[epoch10, step541]: loss 0.647358
[epoch10, step542]: loss 0.691974
[epoch10, step543]: loss 0.450048
[epoch10, step544]: loss 0.552520
[epoch10, step545]: loss 0.667146
[epoch10, step546]: loss 0.457469
[epoch10, step547]: loss 0.619629
[epoch10, step548]: loss 0.569218
[epoch10, step549]: loss 0.569633
[epoch10, step550]: loss 0.521719
[epoch10, step551]: loss 0.522851
[epoch10, step552]: loss 0.692205
[epoch10, step553]: loss 0.454009
[epoch10, step554]: loss 0.321867
[epoch10, step555]: loss 0.545542
[epoch10, step556]: loss 0.621846
[epoch10, step557]: loss 0.521508
[epoch10, step558]: loss 0.702728
[epoch10, step559]: loss 0.446690
[epoch10, step560]: loss 0.692644
[epoch10, step561]: loss 0.454231
[epoch10, step562]: loss 0.559439
[epoch10, step563]: loss 0.673205
[epoch10, step564]: loss 0.386537
[epoch10, step565]: loss 0.428303
[epoch10, step566]: loss 0.445229
[epoch10, step567]: loss 0.538599
[epoch10, step568]: loss 0.406879
[epoch10, step569]: loss 0.305071
[epoch10, step570]: loss 0.694496
[epoch10, step571]: loss 0.647519
[epoch10, step572]: loss 0.535842
[epoch10, step573]: loss 0.523505
[epoch10, step574]: loss 0.339679
[epoch10, step575]: loss 0.392764
[epoch10, step576]: loss 0.531045
[epoch10, step577]: loss 0.616806
[epoch10, step578]: loss 0.596832
[epoch10, step579]: loss 0.470062
[epoch10, step580]: loss 0.453096
[epoch10, step581]: loss 0.476810
[epoch10, step582]: loss 0.343096
[epoch10, step583]: loss 0.636929
[epoch10, step584]: loss 0.475093
[epoch10, step585]: loss 0.601466
[epoch10, step586]: loss 0.641714
[epoch10, step587]: loss 0.641366
[epoch10, step588]: loss 0.438520
[epoch10, step589]: loss 0.489448
[epoch10, step590]: loss 0.587568
[epoch10, step591]: loss 0.593440
[epoch10, step592]: loss 0.511042
[epoch10, step593]: loss 0.487128
[epoch10, step594]: loss 0.340284
[epoch10, step595]: loss 0.494391
[epoch10, step596]: loss 0.579681
[epoch10, step597]: loss 0.357517
[epoch10, step598]: loss 0.461144
[epoch10, step599]: loss 0.617439
[epoch10, step600]: loss 0.615089
[epoch10, step601]: loss 0.379668
[epoch10, step602]: loss 0.321356
[epoch10, step603]: loss 0.674458
[epoch10, step604]: loss 0.389557
[epoch10, step605]: loss 0.289411
[epoch10, step606]: loss 0.732113
[epoch10, step607]: loss 0.446140
[epoch10, step608]: loss 0.571962
[epoch10, step609]: loss 0.564713
[epoch10, step610]: loss 0.294787
[epoch10, step611]: loss 0.587688
[epoch10, step612]: loss 0.372288
[epoch10, step613]: loss 0.562913
[epoch10, step614]: loss 0.254348
[epoch10, step615]: loss 0.666221
[epoch10, step616]: loss 0.425927
[epoch10, step617]: loss 0.517552
[epoch10, step618]: loss 0.290373
[epoch10, step619]: loss 0.267533
[epoch10, step620]: loss 0.379421
[epoch10, step621]: loss 0.427588
[epoch10, step622]: loss 0.558274
[epoch10, step623]: loss 0.473229
[epoch10, step624]: loss 0.384424
[epoch10, step625]: loss 0.596950
[epoch10, step626]: loss 0.625401
[epoch10, step627]: loss 0.520549
[epoch10, step628]: loss 0.367058
[epoch10, step629]: loss 0.579950
[epoch10, step630]: loss 0.502731
[epoch10, step631]: loss 0.549743
[epoch10, step632]: loss 0.398165
[epoch10, step633]: loss 0.572093
[epoch10, step634]: loss 0.264300
[epoch10, step635]: loss 0.443970
[epoch10, step636]: loss 0.372265
[epoch10, step637]: loss 0.296696
[epoch10, step638]: loss 0.504888
[epoch10, step639]: loss 0.484200
[epoch10, step640]: loss 0.323263
[epoch10, step641]: loss 0.563734
[epoch10, step642]: loss 0.503019
[epoch10, step643]: loss 0.375826
[epoch10, step644]: loss 0.479096
[epoch10, step645]: loss 0.509232
[epoch10, step646]: loss 0.518843
[epoch10, step647]: loss 0.431269
[epoch10, step648]: loss 0.621840
[epoch10, step649]: loss 0.480368
[epoch10, step650]: loss 0.631457
[epoch10, step651]: loss 0.553794
[epoch10, step652]: loss 0.546432
[epoch10, step653]: loss 0.833719
[epoch10, step654]: loss 0.486082
[epoch10, step655]: loss 0.522258
[epoch10, step656]: loss 0.355762
[epoch10, step657]: loss 0.716557
[epoch10, step658]: loss 0.673422
[epoch10, step659]: loss 0.568283
[epoch10, step660]: loss 0.581797
[epoch10, step661]: loss 0.668181
[epoch10, step662]: loss 0.662996
[epoch10, step663]: loss 0.294501
[epoch10, step664]: loss 0.547621
[epoch10, step665]: loss 0.439285
[epoch10, step666]: loss 0.190906
[epoch10, step667]: loss 0.483148
[epoch10, step668]: loss 0.608550
[epoch10, step669]: loss 0.316162
[epoch10, step670]: loss 0.375021
[epoch10, step671]: loss 0.493433
[epoch10, step672]: loss 0.454067
[epoch10, step673]: loss 0.397932
[epoch10, step674]: loss 0.645835
[epoch10, step675]: loss 0.671669
[epoch10, step676]: loss 0.654378
[epoch10, step677]: loss 0.697774
[epoch10, step678]: loss 0.440223
[epoch10, step679]: loss 0.680976
[epoch10, step680]: loss 0.428038
[epoch10, step681]: loss 0.451319
[epoch10, step682]: loss 0.512988
[epoch10, step683]: loss 0.416606
[epoch10, step684]: loss 0.594083
[epoch10, step685]: loss 0.490900
[epoch10, step686]: loss 0.457443
[epoch10, step687]: loss 0.710345
[epoch10, step688]: loss 0.507710
[epoch10, step689]: loss 0.371291
[epoch10, step690]: loss 0.374130
[epoch10, step691]: loss 0.595255
[epoch10, step692]: loss 0.440102
[epoch10, step693]: loss 0.384086
[epoch10, step694]: loss 0.580534
[epoch10, step695]: loss 0.325566
[epoch10, step696]: loss 0.299286
[epoch10, step697]: loss 0.515238
[epoch10, step698]: loss 0.312020
[epoch10, step699]: loss 0.400390
[epoch10, step700]: loss 0.424850
[epoch10, step701]: loss 0.441429
[epoch10, step702]: loss 0.412505
[epoch10, step703]: loss 0.436969
[epoch10, step704]: loss 0.225292
[epoch10, step705]: loss 0.736016
[epoch10, step706]: loss 0.740758
[epoch10, step707]: loss 0.651929
[epoch10, step708]: loss 0.520940
[epoch10, step709]: loss 0.646956
[epoch10, step710]: loss 0.604034
[epoch10, step711]: loss 0.550811
[epoch10, step712]: loss 0.633261
[epoch10, step713]: loss 0.507525
[epoch10, step714]: loss 0.421370
[epoch10, step715]: loss 0.436104
[epoch10, step716]: loss 0.535815
[epoch10, step717]: loss 0.428457
[epoch10, step718]: loss 0.462454
[epoch10, step719]: loss 0.470181
[epoch10, step720]: loss 0.393711
[epoch10, step721]: loss 0.356506
[epoch10, step722]: loss 0.273641
[epoch10, step723]: loss 0.600084
[epoch10, step724]: loss 0.548262
[epoch10, step725]: loss 0.563917
[epoch10, step726]: loss 0.441399
[epoch10, step727]: loss 0.665763
[epoch10, step728]: loss 0.475363
[epoch10, step729]: loss 0.509474
[epoch10, step730]: loss 0.471630
[epoch10, step731]: loss 0.588549
[epoch10, step732]: loss 0.424689
[epoch10, step733]: loss 0.498885
[epoch10, step734]: loss 0.632895
[epoch10, step735]: loss 0.623367
[epoch10, step736]: loss 0.393247
[epoch10, step737]: loss 0.589859
[epoch10, step738]: loss 0.336096
[epoch10, step739]: loss 0.588181
[epoch10, step740]: loss 0.198020
[epoch10, step741]: loss 0.537010
[epoch10, step742]: loss 0.407534
[epoch10, step743]: loss 0.465148
[epoch10, step744]: loss 0.549152
[epoch10, step745]: loss 0.584490
[epoch10, step746]: loss 0.643439
[epoch10, step747]: loss 0.354439
[epoch10, step748]: loss 0.665983
[epoch10, step749]: loss 0.460842
[epoch10, step750]: loss 0.307490
[epoch10, step751]: loss 0.525716
[epoch10, step752]: loss 0.553178
[epoch10, step753]: loss 0.538468
[epoch10, step754]: loss 0.496035
[epoch10, step755]: loss 0.348239
[epoch10, step756]: loss 0.161739
[epoch10, step757]: loss 0.683977
[epoch10, step758]: loss 0.482436
[epoch10, step759]: loss 0.492581
[epoch10, step760]: loss 0.563997
[epoch10, step761]: loss 0.450685
[epoch10, step762]: loss 0.340822
[epoch10, step763]: loss 0.343248
[epoch10, step764]: loss 0.372871
[epoch10, step765]: loss 0.704654
[epoch10, step766]: loss 0.688641
[epoch10, step767]: loss 0.504006
[epoch10, step768]: loss 0.679362
[epoch10, step769]: loss 0.605738
[epoch10, step770]: loss 0.355471
[epoch10, step771]: loss 0.413947
[epoch10, step772]: loss 0.563953
[epoch10, step773]: loss 0.343190
[epoch10, step774]: loss 0.432073
[epoch10, step775]: loss 0.461964
[epoch10, step776]: loss 0.588572
[epoch10, step777]: loss 0.258453
[epoch10, step778]: loss 0.824440
[epoch10, step779]: loss 0.430161
[epoch10, step780]: loss 0.610167
[epoch10, step781]: loss 0.519569
[epoch10, step782]: loss 0.564208
[epoch10, step783]: loss 0.332708
[epoch10, step784]: loss 0.683794
[epoch10, step785]: loss 0.389165
[epoch10, step786]: loss 0.568786
[epoch10, step787]: loss 0.363065
[epoch10, step788]: loss 0.371226
[epoch10, step789]: loss 0.484120
[epoch10, step790]: loss 0.454852
[epoch10, step791]: loss 0.449206
[epoch10, step792]: loss 0.504595
[epoch10, step793]: loss 0.493769
[epoch10, step794]: loss 0.451759
[epoch10, step795]: loss 0.712708
[epoch10, step796]: loss 0.278195
[epoch10, step797]: loss 0.580412
[epoch10, step798]: loss 0.534491
[epoch10, step799]: loss 0.451521
[epoch10, step800]: loss 0.586837
[epoch10, step801]: loss 0.369845
[epoch10, step802]: loss 0.636354
[epoch10, step803]: loss 0.421037
[epoch10, step804]: loss 0.478259
[epoch10, step805]: loss 0.364715
[epoch10, step806]: loss 0.481818
[epoch10, step807]: loss 0.506265
[epoch10, step808]: loss 0.553461
[epoch10, step809]: loss 0.572342
[epoch10, step810]: loss 0.495293
[epoch10, step811]: loss 0.311589
[epoch10, step812]: loss 0.508183
[epoch10, step813]: loss 0.551503
[epoch10, step814]: loss 0.440196
[epoch10, step815]: loss 0.391010
[epoch10, step816]: loss 0.509072
[epoch10, step817]: loss 0.477473
[epoch10, step818]: loss 0.716021
[epoch10, step819]: loss 0.469989
[epoch10, step820]: loss 0.520951
[epoch10, step821]: loss 0.486936
[epoch10, step822]: loss 0.266333
[epoch10, step823]: loss 0.416513
[epoch10, step824]: loss 0.770135
[epoch10, step825]: loss 0.448667
[epoch10, step826]: loss 0.492026
[epoch10, step827]: loss 0.356010
[epoch10, step828]: loss 0.578984
[epoch10, step829]: loss 0.477430
[epoch10, step830]: loss 0.630429
[epoch10, step831]: loss 0.443217
[epoch10, step832]: loss 0.625050
[epoch10, step833]: loss 0.396628
[epoch10, step834]: loss 0.557597
[epoch10, step835]: loss 0.409079
[epoch10, step836]: loss 0.374361
[epoch10, step837]: loss 0.469229
[epoch10, step838]: loss 0.538227
[epoch10, step839]: loss 0.490435
[epoch10, step840]: loss 0.467556
[epoch10, step841]: loss 0.667506
[epoch10, step842]: loss 0.674470
[epoch10, step843]: loss 0.600721
[epoch10, step844]: loss 0.330505
[epoch10, step845]: loss 0.533060
[epoch10, step846]: loss 0.649861
[epoch10, step847]: loss 0.632824
[epoch10, step848]: loss 0.426203
[epoch10, step849]: loss 0.295337
[epoch10, step850]: loss 0.468260
[epoch10, step851]: loss 0.390720
[epoch10, step852]: loss 0.506849
[epoch10, step853]: loss 0.569076
[epoch10, step854]: loss 0.602010
[epoch10, step855]: loss 0.300620
[epoch10, step856]: loss 0.618216
[epoch10, step857]: loss 0.720862
[epoch10, step858]: loss 0.581093
[epoch10, step859]: loss 0.371538
[epoch10, step860]: loss 0.542631
[epoch10, step861]: loss 0.406997
[epoch10, step862]: loss 0.538699
[epoch10, step863]: loss 0.664339
[epoch10, step864]: loss 0.676506
[epoch10, step865]: loss 0.694261
[epoch10, step866]: loss 0.436052
[epoch10, step867]: loss 0.635895
[epoch10, step868]: loss 0.697182
[epoch10, step869]: loss 0.553344
[epoch10, step870]: loss 0.482047
[epoch10, step871]: loss 0.492470
[epoch10, step872]: loss 0.666943
[epoch10, step873]: loss 0.536133
[epoch10, step874]: loss 0.556265
[epoch10, step875]: loss 0.400555
[epoch10, step876]: loss 0.619752
[epoch10, step877]: loss 0.432126
[epoch10, step878]: loss 0.591521
[epoch10, step879]: loss 0.604337
[epoch10, step880]: loss 0.551928
[epoch10, step881]: loss 0.612350
[epoch10, step882]: loss 0.511063
[epoch10, step883]: loss 0.463600
[epoch10, step884]: loss 0.465462
[epoch10, step885]: loss 0.464507
[epoch10, step886]: loss 0.477913
[epoch10, step887]: loss 0.506560
[epoch10, step888]: loss 0.373985
[epoch10, step889]: loss 0.502259
[epoch10, step890]: loss 0.408867
[epoch10, step891]: loss 0.680148
[epoch10, step892]: loss 0.285353
[epoch10, step893]: loss 0.461426
[epoch10, step894]: loss 0.312988
[epoch10, step895]: loss 0.479527
[epoch10, step896]: loss 0.434102
[epoch10, step897]: loss 0.558057
[epoch10, step898]: loss 0.419825
[epoch10, step899]: loss 0.627545
[epoch10, step900]: loss 0.584396
[epoch10, step901]: loss 0.488072
[epoch10, step902]: loss 0.150070
[epoch10, step903]: loss 0.561928
[epoch10, step904]: loss 0.258339
[epoch10, step905]: loss 0.609617
[epoch10, step906]: loss 0.433919
[epoch10, step907]: loss 0.595273
[epoch10, step908]: loss 0.266058
[epoch10, step909]: loss 0.435036
[epoch10, step910]: loss 0.680159
[epoch10, step911]: loss 0.464776
[epoch10, step912]: loss 0.491549
[epoch10, step913]: loss 0.474717
[epoch10, step914]: loss 0.475253
[epoch10, step915]: loss 0.499770
[epoch10, step916]: loss 0.497204
[epoch10, step917]: loss 0.703269
[epoch10, step918]: loss 0.640014
[epoch10, step919]: loss 0.324569
[epoch10, step920]: loss 0.420010
[epoch10, step921]: loss 0.488773
[epoch10, step922]: loss 0.509622
[epoch10, step923]: loss 0.475435
[epoch10, step924]: loss 0.619011
[epoch10, step925]: loss 0.561042
[epoch10, step926]: loss 0.379410
[epoch10, step927]: loss 0.487837
[epoch10, step928]: loss 0.435357
[epoch10, step929]: loss 0.505713
[epoch10, step930]: loss 0.153177
[epoch10, step931]: loss 0.559748
[epoch10, step932]: loss 0.463682
[epoch10, step933]: loss 0.594977
[epoch10, step934]: loss 0.639368
[epoch10, step935]: loss 0.725529
[epoch10, step936]: loss 0.431898
[epoch10, step937]: loss 0.492955
[epoch10, step938]: loss 0.641880
[epoch10, step939]: loss 0.489354
[epoch10, step940]: loss 0.597853
[epoch10, step941]: loss 0.338137
[epoch10, step942]: loss 0.664399
[epoch10, step943]: loss 0.636170
[epoch10, step944]: loss 0.379763
[epoch10, step945]: loss 0.673727
[epoch10, step946]: loss 0.434000
[epoch10, step947]: loss 0.739451
[epoch10, step948]: loss 0.358336
[epoch10, step949]: loss 0.538944
[epoch10, step950]: loss 0.676350
[epoch10, step951]: loss 0.563320
[epoch10, step952]: loss 0.497323
[epoch10, step953]: loss 0.503416
[epoch10, step954]: loss 0.569861
[epoch10, step955]: loss 0.477249
[epoch10, step956]: loss 0.565491
[epoch10, step957]: loss 0.526006
[epoch10, step958]: loss 0.261522
[epoch10, step959]: loss 0.274512
[epoch10, step960]: loss 0.478864
[epoch10, step961]: loss 0.536516
[epoch10, step962]: loss 0.207740
[epoch10, step963]: loss 0.607842
[epoch10, step964]: loss 0.348020
[epoch10, step965]: loss 0.578128
[epoch10, step966]: loss 0.701785
[epoch10, step967]: loss 0.535711
[epoch10, step968]: loss 0.229637
[epoch10, step969]: loss 0.689455
[epoch10, step970]: loss 0.392772
[epoch10, step971]: loss 0.364919
[epoch10, step972]: loss 0.564373
[epoch10, step973]: loss 0.467804
[epoch10, step974]: loss 0.646938
[epoch10, step975]: loss 0.539958
[epoch10, step976]: loss 0.379911
[epoch10, step977]: loss 0.648348
[epoch10, step978]: loss 0.609167
[epoch10, step979]: loss 0.506005
[epoch10, step980]: loss 0.710803
[epoch10, step981]: loss 0.495066
[epoch10, step982]: loss 0.537839
[epoch10, step983]: loss 0.665136
[epoch10, step984]: loss 0.556783
[epoch10, step985]: loss 0.580898
[epoch10, step986]: loss 0.602661
[epoch10, step987]: loss 0.362850
[epoch10, step988]: loss 0.555889
[epoch10, step989]: loss 0.502614
[epoch10, step990]: loss 0.571883
[epoch10, step991]: loss 0.115434
[epoch10, step992]: loss 0.381148
[epoch10, step993]: loss 0.451539
[epoch10, step994]: loss 0.396649
[epoch10, step995]: loss 0.645444
[epoch10, step996]: loss 0.166390
[epoch10, step997]: loss 0.405592
[epoch10, step998]: loss 0.490514
[epoch10, step999]: loss 0.423180
[epoch10, step1000]: loss 0.503401
[epoch10, step1001]: loss 0.330090
[epoch10, step1002]: loss 0.443595
[epoch10, step1003]: loss 0.365883
[epoch10, step1004]: loss 0.523211
[epoch10, step1005]: loss 0.534607
[epoch10, step1006]: loss 0.590591
[epoch10, step1007]: loss 0.621150
[epoch10, step1008]: loss 0.362105
[epoch10, step1009]: loss 0.681673
[epoch10, step1010]: loss 0.364832
[epoch10, step1011]: loss 0.623937
[epoch10, step1012]: loss 0.485075
[epoch10, step1013]: loss 0.585192
[epoch10, step1014]: loss 0.261954
[epoch10, step1015]: loss 0.623556
[epoch10, step1016]: loss 0.257538
[epoch10, step1017]: loss 0.220176
[epoch10, step1018]: loss 0.283927
[epoch10, step1019]: loss 0.674470
[epoch10, step1020]: loss 0.484479
[epoch10, step1021]: loss 0.152368
[epoch10, step1022]: loss 0.555408
[epoch10, step1023]: loss 0.189724
[epoch10, step1024]: loss 0.728077
[epoch10, step1025]: loss 0.777825
[epoch10, step1026]: loss 0.852628
[epoch10, step1027]: loss 0.496515
[epoch10, step1028]: loss 0.287759
[epoch10, step1029]: loss 0.461068
[epoch10, step1030]: loss 0.523498
[epoch10, step1031]: loss 0.646762
[epoch10, step1032]: loss 0.608192
[epoch10, step1033]: loss 0.419399
[epoch10, step1034]: loss 0.475468
[epoch10, step1035]: loss 0.581703
[epoch10, step1036]: loss 0.444351
[epoch10, step1037]: loss 0.451189
[epoch10, step1038]: loss 0.635574
[epoch10, step1039]: loss 0.354916
[epoch10, step1040]: loss 0.544149
[epoch10, step1041]: loss 0.385252
[epoch10, step1042]: loss 0.504351
[epoch10, step1043]: loss 0.282202
[epoch10, step1044]: loss 0.741148
[epoch10, step1045]: loss 0.543714
[epoch10, step1046]: loss 0.822630
[epoch10, step1047]: loss 0.735312
[epoch10, step1048]: loss 0.421706
[epoch10, step1049]: loss 0.585528
[epoch10, step1050]: loss 0.574989
[epoch10, step1051]: loss 0.544075
[epoch10, step1052]: loss 0.383714
[epoch10, step1053]: loss 0.499193
[epoch10, step1054]: loss 0.439314
[epoch10, step1055]: loss 0.760155
[epoch10, step1056]: loss 0.382195
[epoch10, step1057]: loss 0.445672
[epoch10, step1058]: loss 0.566404
[epoch10, step1059]: loss 0.639512
[epoch10, step1060]: loss 0.556711
[epoch10, step1061]: loss 0.551765
[epoch10, step1062]: loss 0.375312
[epoch10, step1063]: loss 0.581204
[epoch10, step1064]: loss 0.420225
[epoch10, step1065]: loss 0.530077
[epoch10, step1066]: loss 0.616657
[epoch10, step1067]: loss 0.647096
[epoch10, step1068]: loss 0.529649
[epoch10, step1069]: loss 0.646057
[epoch10, step1070]: loss 0.396331
[epoch10, step1071]: loss 0.561617
[epoch10, step1072]: loss 0.624748
[epoch10, step1073]: loss 0.344386
[epoch10, step1074]: loss 0.359403
[epoch10, step1075]: loss 0.566907
[epoch10, step1076]: loss 0.740078
[epoch10, step1077]: loss 0.520000
[epoch10, step1078]: loss 0.324951
[epoch10, step1079]: loss 0.728922
[epoch10, step1080]: loss 0.545863
[epoch10, step1081]: loss 0.708743
[epoch10, step1082]: loss 0.647675
[epoch10, step1083]: loss 0.309285
[epoch10, step1084]: loss 0.602936
[epoch10, step1085]: loss 0.467577
[epoch10, step1086]: loss 0.619499
[epoch10, step1087]: loss 0.611584
[epoch10, step1088]: loss 0.289501
[epoch10, step1089]: loss 0.334918
[epoch10, step1090]: loss 0.383582
[epoch10, step1091]: loss 0.661902
[epoch10, step1092]: loss 0.453607
[epoch10, step1093]: loss 0.684257
[epoch10, step1094]: loss 0.268325
[epoch10, step1095]: loss 0.423683
[epoch10, step1096]: loss 0.646464
[epoch10, step1097]: loss 0.640392
[epoch10, step1098]: loss 0.561304
[epoch10, step1099]: loss 0.443114
[epoch10, step1100]: loss 0.462366
[epoch10, step1101]: loss 0.594928
[epoch10, step1102]: loss 0.722479
[epoch10, step1103]: loss 0.596053
[epoch10, step1104]: loss 0.523654
[epoch10, step1105]: loss 0.505530
[epoch10, step1106]: loss 0.482767
[epoch10, step1107]: loss 0.611894
[epoch10, step1108]: loss 0.597995
[epoch10, step1109]: loss 0.499395
[epoch10, step1110]: loss 0.530992
[epoch10, step1111]: loss 0.727493
[epoch10, step1112]: loss 0.492396
[epoch10, step1113]: loss 0.276221
[epoch10, step1114]: loss 0.632325
[epoch10, step1115]: loss 0.330419
[epoch10, step1116]: loss 0.421693
[epoch10, step1117]: loss 0.289268
[epoch10, step1118]: loss 0.671414
[epoch10, step1119]: loss 0.511246
[epoch10, step1120]: loss 0.624700
[epoch10, step1121]: loss 0.427949
[epoch10, step1122]: loss 0.313901
[epoch10, step1123]: loss 0.497530
[epoch10, step1124]: loss 0.292623
[epoch10, step1125]: loss 0.601282
[epoch10, step1126]: loss 0.497808
[epoch10, step1127]: loss 0.509577
[epoch10, step1128]: loss 0.763597
[epoch10, step1129]: loss 0.484857
[epoch10, step1130]: loss 0.668271
[epoch10, step1131]: loss 0.441380
[epoch10, step1132]: loss 0.697860
[epoch10, step1133]: loss 0.422073
[epoch10, step1134]: loss 0.714186
[epoch10, step1135]: loss 0.521647
[epoch10, step1136]: loss 0.543584
[epoch10, step1137]: loss 0.433258
[epoch10, step1138]: loss 0.510008
[epoch10, step1139]: loss 0.562749
[epoch10, step1140]: loss 0.329807
[epoch10, step1141]: loss 0.354091
[epoch10, step1142]: loss 0.553141
[epoch10, step1143]: loss 0.683425
[epoch10, step1144]: loss 0.573638
[epoch10, step1145]: loss 0.562030
[epoch10, step1146]: loss 0.592438
[epoch10, step1147]: loss 0.366265
[epoch10, step1148]: loss 0.164552
[epoch10, step1149]: loss 0.581191
[epoch10, step1150]: loss 0.616627
[epoch10, step1151]: loss 0.440033
[epoch10, step1152]: loss 0.604409
[epoch10, step1153]: loss 0.599102
[epoch10, step1154]: loss 0.312767
[epoch10, step1155]: loss 0.519980
[epoch10, step1156]: loss 0.517078
[epoch10, step1157]: loss 0.613578
[epoch10, step1158]: loss 0.585682
[epoch10, step1159]: loss 0.561047
[epoch10, step1160]: loss 0.585295
[epoch10, step1161]: loss 0.554820
[epoch10, step1162]: loss 0.414426
[epoch10, step1163]: loss 0.669226
[epoch10, step1164]: loss 0.510824
[epoch10, step1165]: loss 0.503497
[epoch10, step1166]: loss 0.454630
[epoch10, step1167]: loss 0.431887
[epoch10, step1168]: loss 0.481769
[epoch10, step1169]: loss 0.345969
[epoch10, step1170]: loss 0.495694
[epoch10, step1171]: loss 0.556229
[epoch10, step1172]: loss 0.637707
[epoch10, step1173]: loss 0.644507
[epoch10, step1174]: loss 0.392867
[epoch10, step1175]: loss 0.592405
[epoch10, step1176]: loss 0.549518
[epoch10, step1177]: loss 0.568964
[epoch10, step1178]: loss 0.522316
[epoch10, step1179]: loss 0.477627
[epoch10, step1180]: loss 0.365658
[epoch10, step1181]: loss 0.603518
[epoch10, step1182]: loss 0.550685
[epoch10, step1183]: loss 0.629351
[epoch10, step1184]: loss 0.540068
[epoch10, step1185]: loss 0.508307
[epoch10, step1186]: loss 0.226747
[epoch10, step1187]: loss 0.602118
[epoch10, step1188]: loss 0.701992
[epoch10, step1189]: loss 0.460225
[epoch10, step1190]: loss 0.514225
[epoch10, step1191]: loss 0.247949
[epoch10, step1192]: loss 0.609270
[epoch10, step1193]: loss 0.125154
[epoch10, step1194]: loss 0.407121
[epoch10, step1195]: loss 0.544746
[epoch10, step1196]: loss 0.630913
[epoch10, step1197]: loss 0.663628
[epoch10, step1198]: loss 0.540910
[epoch10, step1199]: loss 0.379654
[epoch10, step1200]: loss 0.273933
[epoch10, step1201]: loss 0.664482
[epoch10, step1202]: loss 0.503423
[epoch10, step1203]: loss 0.663015
[epoch10, step1204]: loss 0.344723
[epoch10, step1205]: loss 0.539185
[epoch10, step1206]: loss 0.714202
[epoch10, step1207]: loss 0.241405
[epoch10, step1208]: loss 0.543785
[epoch10, step1209]: loss 0.683663
[epoch10, step1210]: loss 0.516501
[epoch10, step1211]: loss 0.388956
[epoch10, step1212]: loss 0.278917
[epoch10, step1213]: loss 0.582434
[epoch10, step1214]: loss 0.597394
[epoch10, step1215]: loss 0.547952
[epoch10, step1216]: loss 0.107783
[epoch10, step1217]: loss 0.593979
[epoch10, step1218]: loss 0.674387
[epoch10, step1219]: loss 0.556633
[epoch10, step1220]: loss 0.418624
[epoch10, step1221]: loss 0.616696
[epoch10, step1222]: loss 0.389133
[epoch10, step1223]: loss 0.349639
[epoch10, step1224]: loss 0.384537
[epoch10, step1225]: loss 0.535582
[epoch10, step1226]: loss 0.287391
[epoch10, step1227]: loss 0.472358
[epoch10, step1228]: loss 0.793696
[epoch10, step1229]: loss 0.366229
[epoch10, step1230]: loss 0.477542
[epoch10, step1231]: loss 0.540748
[epoch10, step1232]: loss 0.528053
[epoch10, step1233]: loss 0.496428
[epoch10, step1234]: loss 0.286869
[epoch10, step1235]: loss 0.402399
[epoch10, step1236]: loss 0.488277
[epoch10, step1237]: loss 0.383518
[epoch10, step1238]: loss 0.284899
[epoch10, step1239]: loss 0.553291
[epoch10, step1240]: loss 0.534078
[epoch10, step1241]: loss 0.626677
[epoch10, step1242]: loss 0.437947
[epoch10, step1243]: loss 0.499284
[epoch10, step1244]: loss 0.454015
[epoch10, step1245]: loss 0.615827
[epoch10, step1246]: loss 0.400984
[epoch10, step1247]: loss 0.357436
[epoch10, step1248]: loss 0.701990
[epoch10, step1249]: loss 0.604362
[epoch10, step1250]: loss 0.358631
[epoch10, step1251]: loss 0.552966
[epoch10, step1252]: loss 0.559278
[epoch10, step1253]: loss 0.372589
[epoch10, step1254]: loss 0.751563
[epoch10, step1255]: loss 0.240550
[epoch10, step1256]: loss 0.579799
[epoch10, step1257]: loss 0.648157
[epoch10, step1258]: loss 0.630008
[epoch10, step1259]: loss 0.681025
[epoch10, step1260]: loss 0.275854
[epoch10, step1261]: loss 0.576527
[epoch10, step1262]: loss 0.378143
[epoch10, step1263]: loss 0.645799
[epoch10, step1264]: loss 0.424710
[epoch10, step1265]: loss 0.588677
[epoch10, step1266]: loss 0.329579
[epoch10, step1267]: loss 0.445032
[epoch10, step1268]: loss 0.616388
[epoch10, step1269]: loss 0.549494
[epoch10, step1270]: loss 0.706044
[epoch10, step1271]: loss 0.890617
[epoch10, step1272]: loss 0.371469
[epoch10, step1273]: loss 0.606265
[epoch10, step1274]: loss 0.533616
[epoch10, step1275]: loss 0.207520
[epoch10, step1276]: loss 0.457846
[epoch10, step1277]: loss 0.512326
[epoch10, step1278]: loss 0.477633
[epoch10, step1279]: loss 0.242065
[epoch10, step1280]: loss 0.632072
[epoch10, step1281]: loss 0.490653
[epoch10, step1282]: loss 0.431844
[epoch10, step1283]: loss 0.607608
[epoch10, step1284]: loss 0.544754
[epoch10, step1285]: loss 0.432059
[epoch10, step1286]: loss 0.188694
[epoch10, step1287]: loss 0.365970
[epoch10, step1288]: loss 0.544246
[epoch10, step1289]: loss 0.536257
[epoch10, step1290]: loss 0.466438
[epoch10, step1291]: loss 0.220865
[epoch10, step1292]: loss 0.483846
[epoch10, step1293]: loss 0.404501
[epoch10, step1294]: loss 0.550823
[epoch10, step1295]: loss 0.443049
[epoch10, step1296]: loss 0.497881
[epoch10, step1297]: loss 0.575231
[epoch10, step1298]: loss 0.611787
[epoch10, step1299]: loss 0.407081
[epoch10, step1300]: loss 0.403426
[epoch10, step1301]: loss 0.532145
[epoch10, step1302]: loss 0.586754
[epoch10, step1303]: loss 0.524051
[epoch10, step1304]: loss 0.415671
[epoch10, step1305]: loss 0.468800
[epoch10, step1306]: loss 0.660439
[epoch10, step1307]: loss 0.491767
[epoch10, step1308]: loss 0.571879
[epoch10, step1309]: loss 0.266960
[epoch10, step1310]: loss 0.303658
[epoch10, step1311]: loss 0.299643
[epoch10, step1312]: loss 0.529010
[epoch10, step1313]: loss 0.285402
[epoch10, step1314]: loss 0.366337
[epoch10, step1315]: loss 0.492210
[epoch10, step1316]: loss 0.358908
[epoch10, step1317]: loss 0.282104
[epoch10, step1318]: loss 0.381451
[epoch10, step1319]: loss 0.341796
[epoch10, step1320]: loss 0.268079
[epoch10, step1321]: loss 0.572837
[epoch10, step1322]: loss 0.612833
[epoch10, step1323]: loss 0.708166
[epoch10, step1324]: loss 0.532653
[epoch10, step1325]: loss 0.688551
[epoch10, step1326]: loss 0.640021
[epoch10, step1327]: loss 0.608064
[epoch10, step1328]: loss 0.467665
[epoch10, step1329]: loss 0.443306
[epoch10, step1330]: loss 0.524456
[epoch10, step1331]: loss 0.419806
[epoch10, step1332]: loss 0.495761
[epoch10, step1333]: loss 0.569634
[epoch10, step1334]: loss 0.315032
[epoch10, step1335]: loss 0.640701
[epoch10, step1336]: loss 0.342680
[epoch10, step1337]: loss 0.661353
[epoch10, step1338]: loss 0.608829
[epoch10, step1339]: loss 0.466355
[epoch10, step1340]: loss 0.365385
[epoch10, step1341]: loss 0.633798
[epoch10, step1342]: loss 0.616857
[epoch10, step1343]: loss 0.433039
[epoch10, step1344]: loss 0.400578
[epoch10, step1345]: loss 0.513376
[epoch10, step1346]: loss 0.490252
[epoch10, step1347]: loss 0.410125
[epoch10, step1348]: loss 0.639525
[epoch10, step1349]: loss 0.745284
[epoch10, step1350]: loss 0.532750
[epoch10, step1351]: loss 0.605745
[epoch10, step1352]: loss 0.735034
[epoch10, step1353]: loss 0.456510
[epoch10, step1354]: loss 0.557845
[epoch10, step1355]: loss 0.361701
[epoch10, step1356]: loss 0.732491
[epoch10, step1357]: loss 0.424691
[epoch10, step1358]: loss 0.747060
[epoch10, step1359]: loss 0.633495
[epoch10, step1360]: loss 0.404335
[epoch10, step1361]: loss 0.643546
[epoch10, step1362]: loss 0.569278
[epoch10, step1363]: loss 0.457156
[epoch10, step1364]: loss 0.512543
[epoch10, step1365]: loss 0.605037
[epoch10, step1366]: loss 0.524155
[epoch10, step1367]: loss 0.530147
[epoch10, step1368]: loss 0.336297
[epoch10, step1369]: loss 0.371332
[epoch10, step1370]: loss 0.571469
[epoch10, step1371]: loss 0.515574
[epoch10, step1372]: loss 0.338358
[epoch10, step1373]: loss 0.558389
[epoch10, step1374]: loss 0.682947
[epoch10, step1375]: loss 0.516356
[epoch10, step1376]: loss 0.452745
[epoch10, step1377]: loss 0.757430
[epoch10, step1378]: loss 0.505134
[epoch10, step1379]: loss 0.521327
[epoch10, step1380]: loss 0.415232
[epoch10, step1381]: loss 0.702098
[epoch10, step1382]: loss 0.591021
[epoch10, step1383]: loss 0.411745
[epoch10, step1384]: loss 0.279878
[epoch10, step1385]: loss 0.704899
[epoch10, step1386]: loss 0.610066
[epoch10, step1387]: loss 0.631828
[epoch10, step1388]: loss 0.563757
[epoch10, step1389]: loss 0.460148
[epoch10, step1390]: loss 0.677527
[epoch10, step1391]: loss 0.655404
[epoch10, step1392]: loss 0.658760
[epoch10, step1393]: loss 0.532543
[epoch10, step1394]: loss 0.634587
[epoch10, step1395]: loss 0.575011
[epoch10, step1396]: loss 0.531875
[epoch10, step1397]: loss 0.602351
[epoch10, step1398]: loss 0.571303
[epoch10, step1399]: loss 0.578320
[epoch10, step1400]: loss 0.634079
[epoch10, step1401]: loss 0.651304
[epoch10, step1402]: loss 0.412845
[epoch10, step1403]: loss 0.539898
[epoch10, step1404]: loss 0.564995
[epoch10, step1405]: loss 0.763316
[epoch10, step1406]: loss 0.281198
[epoch10, step1407]: loss 0.470233
[epoch10, step1408]: loss 0.587215
[epoch10, step1409]: loss 0.599726
[epoch10, step1410]: loss 0.269022
[epoch10, step1411]: loss 0.543710
[epoch10, step1412]: loss 0.360507
[epoch10, step1413]: loss 0.380866
[epoch10, step1414]: loss 0.504345
[epoch10, step1415]: loss 0.555603
[epoch10, step1416]: loss 0.500187
[epoch10, step1417]: loss 0.447144
[epoch10, step1418]: loss 0.566739
[epoch10, step1419]: loss 0.546513
[epoch10, step1420]: loss 0.578055
[epoch10, step1421]: loss 0.351122
[epoch10, step1422]: loss 0.535864
[epoch10, step1423]: loss 0.367268
[epoch10, step1424]: loss 0.652968
[epoch10, step1425]: loss 0.303345
[epoch10, step1426]: loss 0.392490
[epoch10, step1427]: loss 0.379767
[epoch10, step1428]: loss 0.633911
[epoch10, step1429]: loss 0.607912
[epoch10, step1430]: loss 0.542066
[epoch10, step1431]: loss 0.316337
[epoch10, step1432]: loss 0.594475
[epoch10, step1433]: loss 0.698076
[epoch10, step1434]: loss 0.707985
[epoch10, step1435]: loss 0.469955
[epoch10, step1436]: loss 0.724532
[epoch10, step1437]: loss 0.279286
[epoch10, step1438]: loss 0.496868
[epoch10, step1439]: loss 0.476057
[epoch10, step1440]: loss 0.478386
[epoch10, step1441]: loss 0.606002
[epoch10, step1442]: loss 0.557324
[epoch10, step1443]: loss 0.591102
[epoch10, step1444]: loss 0.464366
[epoch10, step1445]: loss 0.423128
[epoch10, step1446]: loss 0.457898
[epoch10, step1447]: loss 0.568250
[epoch10, step1448]: loss 0.560367
[epoch10, step1449]: loss 0.463438
[epoch10, step1450]: loss 0.386993
[epoch10, step1451]: loss 0.446188
[epoch10, step1452]: loss 0.480575
[epoch10, step1453]: loss 0.499557
[epoch10, step1454]: loss 0.501543
[epoch10, step1455]: loss 0.577321
[epoch10, step1456]: loss 0.534268
[epoch10, step1457]: loss 0.586318
[epoch10, step1458]: loss 0.582101
[epoch10, step1459]: loss 0.391468
[epoch10, step1460]: loss 0.311946
[epoch10, step1461]: loss 0.224115
[epoch10, step1462]: loss 0.580059
[epoch10, step1463]: loss 0.684538
[epoch10, step1464]: loss 0.336772
[epoch10, step1465]: loss 0.589373
[epoch10, step1466]: loss 0.635538
[epoch10, step1467]: loss 0.313502
[epoch10, step1468]: loss 0.274307
[epoch10, step1469]: loss 0.552240
[epoch10, step1470]: loss 0.682907
[epoch10, step1471]: loss 0.295369
[epoch10, step1472]: loss 0.426967
[epoch10, step1473]: loss 0.105243
[epoch10, step1474]: loss 0.602555
[epoch10, step1475]: loss 0.399581
[epoch10, step1476]: loss 0.281819
[epoch10, step1477]: loss 0.783101
[epoch10, step1478]: loss 0.386654
[epoch10, step1479]: loss 0.587387
[epoch10, step1480]: loss 0.511034
[epoch10, step1481]: loss 0.377387
[epoch10, step1482]: loss 0.397143
[epoch10, step1483]: loss 0.578176
[epoch10, step1484]: loss 0.692886
[epoch10, step1485]: loss 0.611220
[epoch10, step1486]: loss 0.441752
[epoch10, step1487]: loss 0.362430
[epoch10, step1488]: loss 0.651178
[epoch10, step1489]: loss 0.565453
[epoch10, step1490]: loss 0.371544
[epoch10, step1491]: loss 0.382122
[epoch10, step1492]: loss 0.374073
[epoch10, step1493]: loss 0.583831
[epoch10, step1494]: loss 0.469912
[epoch10, step1495]: loss 0.546148
[epoch10, step1496]: loss 0.348139
[epoch10, step1497]: loss 0.462687
[epoch10, step1498]: loss 0.357149
[epoch10, step1499]: loss 0.521687
[epoch10, step1500]: loss 0.627124
[epoch10, step1501]: loss 0.689935
[epoch10, step1502]: loss 0.463099
[epoch10, step1503]: loss 0.372611
[epoch10, step1504]: loss 0.490753
[epoch10, step1505]: loss 0.419582
[epoch10, step1506]: loss 0.484480
[epoch10, step1507]: loss 0.453140
[epoch10, step1508]: loss 0.499710
[epoch10, step1509]: loss 0.371794
[epoch10, step1510]: loss 0.378616
[epoch10, step1511]: loss 0.631034
[epoch10, step1512]: loss 0.815855
[epoch10, step1513]: loss 0.470569
[epoch10, step1514]: loss 0.673533
[epoch10, step1515]: loss 0.361923
[epoch10, step1516]: loss 0.492405
[epoch10, step1517]: loss 0.468924
[epoch10, step1518]: loss 0.749419
[epoch10, step1519]: loss 0.255268
[epoch10, step1520]: loss 0.378036
[epoch10, step1521]: loss 0.545959
[epoch10, step1522]: loss 0.433526
[epoch10, step1523]: loss 0.538926
[epoch10, step1524]: loss 0.520671
[epoch10, step1525]: loss 0.681530
[epoch10, step1526]: loss 0.652497
[epoch10, step1527]: loss 0.548395
[epoch10, step1528]: loss 0.438827
[epoch10, step1529]: loss 0.344455
[epoch10, step1530]: loss 0.273060
[epoch10, step1531]: loss 0.437813
[epoch10, step1532]: loss 0.419348
[epoch10, step1533]: loss 0.755836
[epoch10, step1534]: loss 0.460233
[epoch10, step1535]: loss 0.374839
[epoch10, step1536]: loss 0.546600
[epoch10, step1537]: loss 0.568681
[epoch10, step1538]: loss 0.627950
[epoch10, step1539]: loss 0.522783
[epoch10, step1540]: loss 0.558011
[epoch10, step1541]: loss 0.434388
[epoch10, step1542]: loss 0.565253
[epoch10, step1543]: loss 0.669355
[epoch10, step1544]: loss 0.523390
[epoch10, step1545]: loss 0.594038
[epoch10, step1546]: loss 0.614580
[epoch10, step1547]: loss 0.132685
[epoch10, step1548]: loss 0.434938
[epoch10, step1549]: loss 0.298201
[epoch10, step1550]: loss 0.441616
[epoch10, step1551]: loss 0.398672
[epoch10, step1552]: loss 0.540118
[epoch10, step1553]: loss 0.313142
[epoch10, step1554]: loss 0.446442
[epoch10, step1555]: loss 0.498479
[epoch10, step1556]: loss 0.536913
[epoch10, step1557]: loss 0.652046
[epoch10, step1558]: loss 0.535693
[epoch10, step1559]: loss 0.342106
[epoch10, step1560]: loss 0.512535
[epoch10, step1561]: loss 0.636472
[epoch10, step1562]: loss 0.531987
[epoch10, step1563]: loss 0.584443
[epoch10, step1564]: loss 0.705501
[epoch10, step1565]: loss 0.598711
[epoch10, step1566]: loss 0.430291
[epoch10, step1567]: loss 0.581226
[epoch10, step1568]: loss 0.558674
[epoch10, step1569]: loss 0.506526
[epoch10, step1570]: loss 0.384602
[epoch10, step1571]: loss 0.540525
[epoch10, step1572]: loss 0.382837
[epoch10, step1573]: loss 0.484297
[epoch10, step1574]: loss 0.615004
[epoch10, step1575]: loss 0.581079
[epoch10, step1576]: loss 0.733002
[epoch10, step1577]: loss 0.539936
[epoch10, step1578]: loss 0.418469
[epoch10, step1579]: loss 0.548408
[epoch10, step1580]: loss 0.235966
[epoch10, step1581]: loss 0.611644
[epoch10, step1582]: loss 0.501132
[epoch10, step1583]: loss 0.619566
[epoch10, step1584]: loss 0.186153
[epoch10, step1585]: loss 0.343000
[epoch10, step1586]: loss 0.268415
[epoch10, step1587]: loss 0.670991
[epoch10, step1588]: loss 0.292687
[epoch10, step1589]: loss 0.578620
[epoch10, step1590]: loss 0.574691
[epoch10, step1591]: loss 0.398966
[epoch10, step1592]: loss 0.231766
[epoch10, step1593]: loss 0.499784
[epoch10, step1594]: loss 0.457185
[epoch10, step1595]: loss 0.268197
[epoch10, step1596]: loss 0.579014
[epoch10, step1597]: loss 0.283460
[epoch10, step1598]: loss 0.538890
[epoch10, step1599]: loss 0.392789
[epoch10, step1600]: loss 0.748646
[epoch10, step1601]: loss 0.282695
[epoch10, step1602]: loss 0.517914
[epoch10, step1603]: loss 0.523873
[epoch10, step1604]: loss 0.695309
[epoch10, step1605]: loss 0.328041
[epoch10, step1606]: loss 0.564970
[epoch10, step1607]: loss 0.387183
[epoch10, step1608]: loss 0.665341
[epoch10, step1609]: loss 0.467208
[epoch10, step1610]: loss 0.609570
[epoch10, step1611]: loss 0.503091
[epoch10, step1612]: loss 0.507372
[epoch10, step1613]: loss 0.366251
[epoch10, step1614]: loss 0.509570
[epoch10, step1615]: loss 0.648290
[epoch10, step1616]: loss 0.154821
[epoch10, step1617]: loss 0.432116
[epoch10, step1618]: loss 0.434950
[epoch10, step1619]: loss 0.738966
[epoch10, step1620]: loss 0.607128
[epoch10, step1621]: loss 0.183535
[epoch10, step1622]: loss 0.449060
[epoch10, step1623]: loss 0.594476
[epoch10, step1624]: loss 0.349554
[epoch10, step1625]: loss 0.477696
[epoch10, step1626]: loss 0.482325
[epoch10, step1627]: loss 0.554581
[epoch10, step1628]: loss 0.694892
[epoch10, step1629]: loss 0.665262
[epoch10, step1630]: loss 0.437735
[epoch10, step1631]: loss 0.155375
[epoch10, step1632]: loss 0.508825
[epoch10, step1633]: loss 0.602534
[epoch10, step1634]: loss 0.475517
[epoch10, step1635]: loss 0.423538
[epoch10, step1636]: loss 0.519528
[epoch10, step1637]: loss 0.504552
[epoch10, step1638]: loss 0.263562
[epoch10, step1639]: loss 0.467493
[epoch10, step1640]: loss 0.388677
[epoch10, step1641]: loss 0.397053
[epoch10, step1642]: loss 0.516457
[epoch10, step1643]: loss 0.575855
[epoch10, step1644]: loss 0.338663
[epoch10, step1645]: loss 0.617873
[epoch10, step1646]: loss 0.491033
[epoch10, step1647]: loss 0.675595
[epoch10, step1648]: loss 0.565067
[epoch10, step1649]: loss 0.349249
[epoch10, step1650]: loss 0.660092
[epoch10, step1651]: loss 0.336199
[epoch10, step1652]: loss 0.711488
[epoch10, step1653]: loss 0.691015
[epoch10, step1654]: loss 0.329497
[epoch10, step1655]: loss 0.346556
[epoch10, step1656]: loss 0.553274
[epoch10, step1657]: loss 0.515781
[epoch10, step1658]: loss 0.260546
[epoch10, step1659]: loss 0.570745
[epoch10, step1660]: loss 0.399240
[epoch10, step1661]: loss 0.275200
[epoch10, step1662]: loss 0.372533
[epoch10, step1663]: loss 0.427175
[epoch10, step1664]: loss 0.299257
[epoch10, step1665]: loss 0.447573
[epoch10, step1666]: loss 0.624404
[epoch10, step1667]: loss 0.577440
[epoch10, step1668]: loss 0.489435
[epoch10, step1669]: loss 0.460545
[epoch10, step1670]: loss 0.695948
[epoch10, step1671]: loss 0.710388
[epoch10, step1672]: loss 0.267378
[epoch10, step1673]: loss 0.389798
[epoch10, step1674]: loss 0.257286
[epoch10, step1675]: loss 0.273591
[epoch10, step1676]: loss 0.558395
[epoch10, step1677]: loss 0.689393
[epoch10, step1678]: loss 0.425899
[epoch10, step1679]: loss 0.457835
[epoch10, step1680]: loss 0.685645
[epoch10, step1681]: loss 0.397139
[epoch10, step1682]: loss 0.484110
[epoch10, step1683]: loss 0.670001
[epoch10, step1684]: loss 0.580240
[epoch10, step1685]: loss 0.429347
[epoch10, step1686]: loss 0.645194
[epoch10, step1687]: loss 0.515557
[epoch10, step1688]: loss 0.615513
[epoch10, step1689]: loss 0.528037
[epoch10, step1690]: loss 0.469217
[epoch10, step1691]: loss 0.332744
[epoch10, step1692]: loss 0.402454
[epoch10, step1693]: loss 0.442138
[epoch10, step1694]: loss 0.507182
[epoch10, step1695]: loss 0.504763
[epoch10, step1696]: loss 0.450566
[epoch10, step1697]: loss 0.529086
[epoch10, step1698]: loss 0.536263
[epoch10, step1699]: loss 0.621889
[epoch10, step1700]: loss 0.542664
[epoch10, step1701]: loss 0.543626
[epoch10, step1702]: loss 0.675339
[epoch10, step1703]: loss 0.591591
[epoch10, step1704]: loss 0.654233
[epoch10, step1705]: loss 0.375683
[epoch10, step1706]: loss 0.587942
[epoch10, step1707]: loss 0.335546
[epoch10, step1708]: loss 0.443367
[epoch10, step1709]: loss 0.624306
[epoch10, step1710]: loss 0.227897
[epoch10, step1711]: loss 0.330920
[epoch10, step1712]: loss 0.513867
[epoch10, step1713]: loss 0.535795
[epoch10, step1714]: loss 0.645001
[epoch10, step1715]: loss 0.616346
[epoch10, step1716]: loss 0.472232
[epoch10, step1717]: loss 0.612358
[epoch10, step1718]: loss 0.455570
[epoch10, step1719]: loss 0.552608
[epoch10, step1720]: loss 0.579864
[epoch10, step1721]: loss 0.451973
[epoch10, step1722]: loss 0.496069
[epoch10, step1723]: loss 0.636599
[epoch10, step1724]: loss 0.482958
[epoch10, step1725]: loss 0.635151
[epoch10, step1726]: loss 0.555418
[epoch10, step1727]: loss 0.181826
[epoch10, step1728]: loss 0.394370
[epoch10, step1729]: loss 0.527812
[epoch10, step1730]: loss 0.361641
[epoch10, step1731]: loss 0.563969
[epoch10, step1732]: loss 0.385818
[epoch10, step1733]: loss 0.667692
[epoch10, step1734]: loss 0.457161
[epoch10, step1735]: loss 0.717030
[epoch10, step1736]: loss 0.322751
[epoch10, step1737]: loss 0.461097
[epoch10, step1738]: loss 0.288879
[epoch10, step1739]: loss 0.621591
[epoch10, step1740]: loss 0.412531
[epoch10, step1741]: loss 0.699120
[epoch10, step1742]: loss 0.445155
[epoch10, step1743]: loss 0.320434
[epoch10, step1744]: loss 0.680270
[epoch10, step1745]: loss 0.442705
[epoch10, step1746]: loss 0.361157
[epoch10, step1747]: loss 0.458313
[epoch10, step1748]: loss 0.300567
[epoch10, step1749]: loss 0.742933
[epoch10, step1750]: loss 0.426384
[epoch10, step1751]: loss 0.671906
[epoch10, step1752]: loss 0.607428
[epoch10, step1753]: loss 0.464622
[epoch10, step1754]: loss 0.368427
[epoch10, step1755]: loss 0.500901
[epoch10, step1756]: loss 0.653078
[epoch10, step1757]: loss 0.761155
[epoch10, step1758]: loss 0.595747
[epoch10, step1759]: loss 0.764165
[epoch10, step1760]: loss 0.434887
[epoch10, step1761]: loss 0.416376
[epoch10, step1762]: loss 0.502572
[epoch10, step1763]: loss 0.734874
[epoch10, step1764]: loss 0.607740
[epoch10, step1765]: loss 0.339756
[epoch10, step1766]: loss 0.241906
[epoch10, step1767]: loss 0.531760
[epoch10, step1768]: loss 0.488280
[epoch10, step1769]: loss 0.637012
[epoch10, step1770]: loss 0.604224
[epoch10, step1771]: loss 0.725344
[epoch10, step1772]: loss 0.409971
[epoch10, step1773]: loss 0.495329
[epoch10, step1774]: loss 0.444013
[epoch10, step1775]: loss 0.574438
[epoch10, step1776]: loss 0.454351
[epoch10, step1777]: loss 0.622879
[epoch10, step1778]: loss 0.468661
[epoch10, step1779]: loss 0.594935
[epoch10, step1780]: loss 0.438465
[epoch10, step1781]: loss 0.394286
[epoch10, step1782]: loss 0.422379
[epoch10, step1783]: loss 0.395586
[epoch10, step1784]: loss 0.356564
[epoch10, step1785]: loss 0.289235
[epoch10, step1786]: loss 0.555689
[epoch10, step1787]: loss 0.369622
[epoch10, step1788]: loss 0.537501
[epoch10, step1789]: loss 0.275679
[epoch10, step1790]: loss 0.627174
[epoch10, step1791]: loss 0.350228
[epoch10, step1792]: loss 0.533201
[epoch10, step1793]: loss 0.341080
[epoch10, step1794]: loss 0.552426
[epoch10, step1795]: loss 0.296997
[epoch10, step1796]: loss 0.617715
[epoch10, step1797]: loss 0.183281
[epoch10, step1798]: loss 0.452238
[epoch10, step1799]: loss 0.386756
[epoch10, step1800]: loss 0.299455
[epoch10, step1801]: loss 0.685281
[epoch10, step1802]: loss 0.406062
[epoch10, step1803]: loss 0.462368
[epoch10, step1804]: loss 0.715216
[epoch10, step1805]: loss 0.693260
[epoch10, step1806]: loss 0.586551
[epoch10, step1807]: loss 0.422273
[epoch10, step1808]: loss 0.549477
[epoch10, step1809]: loss 0.490929
[epoch10, step1810]: loss 0.685361
[epoch10, step1811]: loss 0.412667
[epoch10, step1812]: loss 0.540434
[epoch10, step1813]: loss 0.312073
[epoch10, step1814]: loss 0.503477
[epoch10, step1815]: loss 0.524228
[epoch10, step1816]: loss 0.449888
[epoch10, step1817]: loss 0.500179
[epoch10, step1818]: loss 0.703702
[epoch10, step1819]: loss 0.660151
[epoch10, step1820]: loss 0.483580
[epoch10, step1821]: loss 0.570568
[epoch10, step1822]: loss 0.609548
[epoch10, step1823]: loss 0.649089
[epoch10, step1824]: loss 0.592766
[epoch10, step1825]: loss 0.659665
[epoch10, step1826]: loss 0.397508
[epoch10, step1827]: loss 0.643413
[epoch10, step1828]: loss 0.386479
[epoch10, step1829]: loss 0.636081
[epoch10, step1830]: loss 0.373382
[epoch10, step1831]: loss 0.178909
[epoch10, step1832]: loss 0.557747
[epoch10, step1833]: loss 0.467101
[epoch10, step1834]: loss 0.389601
[epoch10, step1835]: loss 0.515799
[epoch10, step1836]: loss 0.313861
[epoch10, step1837]: loss 0.519038
[epoch10, step1838]: loss 0.576912
[epoch10, step1839]: loss 0.617608
[epoch10, step1840]: loss 0.291063
[epoch10, step1841]: loss 0.611071
[epoch10, step1842]: loss 0.556372
[epoch10, step1843]: loss 0.336192
[epoch10, step1844]: loss 0.358383
[epoch10, step1845]: loss 0.516190
[epoch10, step1846]: loss 0.327625
[epoch10, step1847]: loss 0.388274
[epoch10, step1848]: loss 0.660523
[epoch10, step1849]: loss 0.549451
[epoch10, step1850]: loss 0.468773
[epoch10, step1851]: loss 0.580694
[epoch10, step1852]: loss 0.360173
[epoch10, step1853]: loss 0.522734
[epoch10, step1854]: loss 0.366955
[epoch10, step1855]: loss 0.563274
[epoch10, step1856]: loss 0.689298
[epoch10, step1857]: loss 0.424090
[epoch10, step1858]: loss 0.443003
[epoch10, step1859]: loss 0.572831
[epoch10, step1860]: loss 0.516698
[epoch10, step1861]: loss 0.388580
[epoch10, step1862]: loss 0.504595
[epoch10, step1863]: loss 0.553789
[epoch10, step1864]: loss 0.178392
[epoch10, step1865]: loss 0.472977
[epoch10, step1866]: loss 0.789671
[epoch10, step1867]: loss 0.395785
[epoch10, step1868]: loss 0.713378
[epoch10, step1869]: loss 0.830967
[epoch10, step1870]: loss 0.667385
[epoch10, step1871]: loss 0.616728
[epoch10, step1872]: loss 0.419542
[epoch10, step1873]: loss 0.356909
[epoch10, step1874]: loss 0.553196
[epoch10, step1875]: loss 0.419587
[epoch10, step1876]: loss 0.454414
[epoch10, step1877]: loss 0.485158
[epoch10, step1878]: loss 0.356399
[epoch10, step1879]: loss 0.137924
[epoch10, step1880]: loss 0.604820
[epoch10, step1881]: loss 0.553162
[epoch10, step1882]: loss 0.623000
[epoch10, step1883]: loss 0.430468
[epoch10, step1884]: loss 0.464227
[epoch10, step1885]: loss 0.500323
[epoch10, step1886]: loss 0.620451
[epoch10, step1887]: loss 0.598578
[epoch10, step1888]: loss 0.641667
[epoch10, step1889]: loss 0.486408
[epoch10, step1890]: loss 0.549603
[epoch10, step1891]: loss 0.559021
[epoch10, step1892]: loss 0.472799
[epoch10, step1893]: loss 0.655897
[epoch10, step1894]: loss 0.364539
[epoch10, step1895]: loss 0.468977
[epoch10, step1896]: loss 0.382460
[epoch10, step1897]: loss 0.553438
[epoch10, step1898]: loss 0.621144
[epoch10, step1899]: loss 0.575853
[epoch10, step1900]: loss 0.435410
[epoch10, step1901]: loss 0.602208
[epoch10, step1902]: loss 0.489271
[epoch10, step1903]: loss 0.481405
[epoch10, step1904]: loss 0.702233
[epoch10, step1905]: loss 0.394173
[epoch10, step1906]: loss 0.281520
[epoch10, step1907]: loss 0.400005
[epoch10, step1908]: loss 0.453279
[epoch10, step1909]: loss 0.535115
[epoch10, step1910]: loss 0.462485
[epoch10, step1911]: loss 0.506328
[epoch10, step1912]: loss 0.577045
[epoch10, step1913]: loss 0.611914
[epoch10, step1914]: loss 0.513065
[epoch10, step1915]: loss 0.628327
[epoch10, step1916]: loss 0.601412
[epoch10, step1917]: loss 0.435438
[epoch10, step1918]: loss 0.668097
[epoch10, step1919]: loss 0.517380
[epoch10, step1920]: loss 0.402434
[epoch10, step1921]: loss 0.457021
[epoch10, step1922]: loss 0.485783
[epoch10, step1923]: loss 0.559459
[epoch10, step1924]: loss 0.421871
[epoch10, step1925]: loss 0.670051
[epoch10, step1926]: loss 0.612369
[epoch10, step1927]: loss 0.573298
[epoch10, step1928]: loss 0.553087
[epoch10, step1929]: loss 0.544826
[epoch10, step1930]: loss 0.493748
[epoch10, step1931]: loss 0.258652
[epoch10, step1932]: loss 0.421268
[epoch10, step1933]: loss 0.422610
[epoch10, step1934]: loss 0.674647
[epoch10, step1935]: loss 0.555986
[epoch10, step1936]: loss 0.582261
[epoch10, step1937]: loss 0.280237
[epoch10, step1938]: loss 0.478442
[epoch10, step1939]: loss 0.546143
[epoch10, step1940]: loss 0.619187
[epoch10, step1941]: loss 0.410999
[epoch10, step1942]: loss 0.257981
[epoch10, step1943]: loss 0.576796
[epoch10, step1944]: loss 0.229250
[epoch10, step1945]: loss 0.469502
[epoch10, step1946]: loss 0.579317
[epoch10, step1947]: loss 0.583085
[epoch10, step1948]: loss 0.188416
[epoch10, step1949]: loss 0.458712
[epoch10, step1950]: loss 0.560909
[epoch10, step1951]: loss 0.749490
[epoch10, step1952]: loss 0.340897
[epoch10, step1953]: loss 0.481922
[epoch10, step1954]: loss 0.482067
[epoch10, step1955]: loss 0.390514
[epoch10, step1956]: loss 0.579779
[epoch10, step1957]: loss 0.301999
[epoch10, step1958]: loss 0.391283
[epoch10, step1959]: loss 0.531642
[epoch10, step1960]: loss 0.439318
[epoch10, step1961]: loss 0.485875
[epoch10, step1962]: loss 0.608294
[epoch10, step1963]: loss 0.428346
[epoch10, step1964]: loss 0.678293
[epoch10, step1965]: loss 0.727769
[epoch10, step1966]: loss 0.343496
[epoch10, step1967]: loss 0.520896
[epoch10, step1968]: loss 0.581267
[epoch10, step1969]: loss 0.234378
[epoch10, step1970]: loss 0.492988
[epoch10, step1971]: loss 0.467119
[epoch10, step1972]: loss 0.278413
[epoch10, step1973]: loss 0.511145
[epoch10, step1974]: loss 0.407214
[epoch10, step1975]: loss 0.646559
[epoch10, step1976]: loss 0.226761
[epoch10, step1977]: loss 0.401960
[epoch10, step1978]: loss 0.706237
[epoch10, step1979]: loss 0.567301
[epoch10, step1980]: loss 0.361042
[epoch10, step1981]: loss 0.612681
[epoch10, step1982]: loss 0.696100
[epoch10, step1983]: loss 0.474852
[epoch10, step1984]: loss 0.365695
[epoch10, step1985]: loss 0.552458
[epoch10, step1986]: loss 0.599047
[epoch10, step1987]: loss 0.345648
[epoch10, step1988]: loss 0.622636
[epoch10, step1989]: loss 0.514496
[epoch10, step1990]: loss 0.491170
[epoch10, step1991]: loss 0.367998
[epoch10, step1992]: loss 0.668781
[epoch10, step1993]: loss 0.333463
[epoch10, step1994]: loss 0.429883
[epoch10, step1995]: loss 0.237440
[epoch10, step1996]: loss 0.625007
[epoch10, step1997]: loss 0.429797
[epoch10, step1998]: loss 0.425373
[epoch10, step1999]: loss 0.266373
[epoch10, step2000]: loss 0.591657
[epoch10, step2001]: loss 0.525779
[epoch10, step2002]: loss 0.507340
[epoch10, step2003]: loss 0.677507
[epoch10, step2004]: loss 0.450394
[epoch10, step2005]: loss 0.750014
[epoch10, step2006]: loss 0.559860
[epoch10, step2007]: loss 0.600113
[epoch10, step2008]: loss 0.365559
[epoch10, step2009]: loss 0.366722
[epoch10, step2010]: loss 0.571930
[epoch10, step2011]: loss 0.482085
[epoch10, step2012]: loss 0.583344
[epoch10, step2013]: loss 0.499152
[epoch10, step2014]: loss 0.445897
[epoch10, step2015]: loss 0.655767
[epoch10, step2016]: loss 0.586123
[epoch10, step2017]: loss 0.391171
[epoch10, step2018]: loss 0.350602
[epoch10, step2019]: loss 0.177590
[epoch10, step2020]: loss 0.523623
[epoch10, step2021]: loss 0.243058
[epoch10, step2022]: loss 0.636969
[epoch10, step2023]: loss 0.645454
[epoch10, step2024]: loss 0.371865
[epoch10, step2025]: loss 0.597190
[epoch10, step2026]: loss 0.715983
[epoch10, step2027]: loss 0.444877
[epoch10, step2028]: loss 0.479296
[epoch10, step2029]: loss 0.672944
[epoch10, step2030]: loss 0.531642
[epoch10, step2031]: loss 0.490078
[epoch10, step2032]: loss 0.704441
[epoch10, step2033]: loss 0.638113
[epoch10, step2034]: loss 0.548590
[epoch10, step2035]: loss 0.592631
[epoch10, step2036]: loss 0.565385
[epoch10, step2037]: loss 0.269648
[epoch10, step2038]: loss 0.448714
[epoch10, step2039]: loss 0.370213
[epoch10, step2040]: loss 0.389351
[epoch10, step2041]: loss 0.525200
[epoch10, step2042]: loss 0.605830
[epoch10, step2043]: loss 0.531200
[epoch10, step2044]: loss 0.290613
[epoch10, step2045]: loss 0.566125
[epoch10, step2046]: loss 0.612796
[epoch10, step2047]: loss 0.578511
[epoch10, step2048]: loss 0.615265
[epoch10, step2049]: loss 0.751359
[epoch10, step2050]: loss 0.660400
[epoch10, step2051]: loss 0.569154
[epoch10, step2052]: loss 0.792694
[epoch10, step2053]: loss 0.642037
[epoch10, step2054]: loss 0.418644
[epoch10, step2055]: loss 0.381356
[epoch10, step2056]: loss 0.584664
[epoch10, step2057]: loss 0.482549
[epoch10, step2058]: loss 0.523901
[epoch10, step2059]: loss 0.553447
[epoch10, step2060]: loss 0.489890
[epoch10, step2061]: loss 0.433180
[epoch10, step2062]: loss 0.259609
[epoch10, step2063]: loss 0.406130
[epoch10, step2064]: loss 0.702083
[epoch10, step2065]: loss 0.544590
[epoch10, step2066]: loss 0.582325
[epoch10, step2067]: loss 0.258382
[epoch10, step2068]: loss 0.487463
[epoch10, step2069]: loss 0.439168
[epoch10, step2070]: loss 0.390862
[epoch10, step2071]: loss 0.433739
[epoch10, step2072]: loss 0.444912
[epoch10, step2073]: loss 0.377573
[epoch10, step2074]: loss 0.469300
[epoch10, step2075]: loss 0.274981
[epoch10, step2076]: loss 0.511613
[epoch10, step2077]: loss 0.132800
[epoch10, step2078]: loss 0.394206
[epoch10, step2079]: loss 0.564975
[epoch10, step2080]: loss 0.427894
[epoch10, step2081]: loss 0.617443
[epoch10, step2082]: loss 0.375581
[epoch10, step2083]: loss 0.758757
[epoch10, step2084]: loss 0.463847
[epoch10, step2085]: loss 0.627916
[epoch10, step2086]: loss 0.388261
[epoch10, step2087]: loss 0.346378
[epoch10, step2088]: loss 0.610255
[epoch10, step2089]: loss 0.549682
[epoch10, step2090]: loss 0.445667
[epoch10, step2091]: loss 0.631542
[epoch10, step2092]: loss 0.586883
[epoch10, step2093]: loss 0.449152
[epoch10, step2094]: loss 0.817181
[epoch10, step2095]: loss 0.557152
[epoch10, step2096]: loss 0.508280
[epoch10, step2097]: loss 0.572193
[epoch10, step2098]: loss 0.467230
[epoch10, step2099]: loss 0.652120
[epoch10, step2100]: loss 0.334200
[epoch10, step2101]: loss 0.467927
[epoch10, step2102]: loss 0.528831
[epoch10, step2103]: loss 0.736873
[epoch10, step2104]: loss 0.097080
[epoch10, step2105]: loss 0.475718
[epoch10, step2106]: loss 0.488582
[epoch10, step2107]: loss 0.558853
[epoch10, step2108]: loss 0.453186
[epoch10, step2109]: loss 0.611277
[epoch10, step2110]: loss 0.334481
[epoch10, step2111]: loss 0.613655
[epoch10, step2112]: loss 0.460014
[epoch10, step2113]: loss 0.396446
[epoch10, step2114]: loss 0.511111
[epoch10, step2115]: loss 0.516532
[epoch10, step2116]: loss 0.500746
[epoch10, step2117]: loss 0.501087
[epoch10, step2118]: loss 0.120963
[epoch10, step2119]: loss 0.540655
[epoch10, step2120]: loss 0.276237
[epoch10, step2121]: loss 0.559272
[epoch10, step2122]: loss 0.636171
[epoch10, step2123]: loss 0.385491
[epoch10, step2124]: loss 0.513993
[epoch10, step2125]: loss 0.509875
[epoch10, step2126]: loss 0.573329
[epoch10, step2127]: loss 0.622728
[epoch10, step2128]: loss 0.753445
[epoch10, step2129]: loss 0.421853
[epoch10, step2130]: loss 0.645374
[epoch10, step2131]: loss 0.419400
[epoch10, step2132]: loss 0.373784
[epoch10, step2133]: loss 0.411238
[epoch10, step2134]: loss 0.380542
[epoch10, step2135]: loss 0.607845
[epoch10, step2136]: loss 0.331607
[epoch10, step2137]: loss 0.546691
[epoch10, step2138]: loss 0.673649
[epoch10, step2139]: loss 0.502956
[epoch10, step2140]: loss 0.554743
[epoch10, step2141]: loss 0.468541
[epoch10, step2142]: loss 0.580730
[epoch10, step2143]: loss 0.543220
[epoch10, step2144]: loss 0.543181
[epoch10, step2145]: loss 0.461031
[epoch10, step2146]: loss 0.576195
[epoch10, step2147]: loss 0.271284
[epoch10, step2148]: loss 0.570303
[epoch10, step2149]: loss 0.692151
[epoch10, step2150]: loss 0.504405
[epoch10, step2151]: loss 0.613039
[epoch10, step2152]: loss 0.557917
[epoch10, step2153]: loss 0.462412
[epoch10, step2154]: loss 0.363617
[epoch10, step2155]: loss 0.427589
[epoch10, step2156]: loss 0.579795
[epoch10, step2157]: loss 0.556771
[epoch10, step2158]: loss 0.639318
[epoch10, step2159]: loss 0.527603
[epoch10, step2160]: loss 0.392305
[epoch10, step2161]: loss 0.673147
[epoch10, step2162]: loss 0.347345
[epoch10, step2163]: loss 0.454038
[epoch10, step2164]: loss 0.455903
[epoch10, step2165]: loss 0.702337
[epoch10, step2166]: loss 0.635544
[epoch10, step2167]: loss 0.385689
[epoch10, step2168]: loss 0.312198
[epoch10, step2169]: loss 0.388599
[epoch10, step2170]: loss 0.486658
[epoch10, step2171]: loss 0.265842
[epoch10, step2172]: loss 0.555987
[epoch10, step2173]: loss 0.737208
[epoch10, step2174]: loss 0.704733
[epoch10, step2175]: loss 0.453106
[epoch10, step2176]: loss 0.411133
[epoch10, step2177]: loss 0.495489
[epoch10, step2178]: loss 0.326035
[epoch10, step2179]: loss 0.447847
[epoch10, step2180]: loss 0.446254
[epoch10, step2181]: loss 0.421841
[epoch10, step2182]: loss 0.529346
[epoch10, step2183]: loss 0.293975
[epoch10, step2184]: loss 0.430979
[epoch10, step2185]: loss 0.446876
[epoch10, step2186]: loss 0.414114
[epoch10, step2187]: loss 0.684164
[epoch10, step2188]: loss 0.746629
[epoch10, step2189]: loss 0.528263
[epoch10, step2190]: loss 0.346841
[epoch10, step2191]: loss 0.572917
[epoch10, step2192]: loss 0.550872
[epoch10, step2193]: loss 0.458426
[epoch10, step2194]: loss 0.454485
[epoch10, step2195]: loss 0.312229
[epoch10, step2196]: loss 0.473799
[epoch10, step2197]: loss 0.371668
[epoch10, step2198]: loss 0.416315
[epoch10, step2199]: loss 0.544664
[epoch10, step2200]: loss 0.246947
[epoch10, step2201]: loss 0.568724
[epoch10, step2202]: loss 0.454722
[epoch10, step2203]: loss 0.333857
[epoch10, step2204]: loss 0.427906
[epoch10, step2205]: loss 0.585756
[epoch10, step2206]: loss 0.498589
[epoch10, step2207]: loss 0.293987
[epoch10, step2208]: loss 0.571182
[epoch10, step2209]: loss 0.331664
[epoch10, step2210]: loss 0.417841
[epoch10, step2211]: loss 0.302066
[epoch10, step2212]: loss 0.658426
[epoch10, step2213]: loss 0.326319
[epoch10, step2214]: loss 0.267161
[epoch10, step2215]: loss 0.507827
[epoch10, step2216]: loss 0.557269
[epoch10, step2217]: loss 0.659830
[epoch10, step2218]: loss 0.393945
[epoch10, step2219]: loss 0.527954
[epoch10, step2220]: loss 0.645000
[epoch10, step2221]: loss 0.610928
[epoch10, step2222]: loss 0.370095
[epoch10, step2223]: loss 0.627236
[epoch10, step2224]: loss 0.759186
[epoch10, step2225]: loss 0.332069
[epoch10, step2226]: loss 0.343145
[epoch10, step2227]: loss 0.424491
[epoch10, step2228]: loss 0.550268
[epoch10, step2229]: loss 0.580218
[epoch10, step2230]: loss 0.556857
[epoch10, step2231]: loss 0.592534
[epoch10, step2232]: loss 0.581922
[epoch10, step2233]: loss 0.433866
[epoch10, step2234]: loss 0.718775
[epoch10, step2235]: loss 0.483581
[epoch10, step2236]: loss 0.584266
[epoch10, step2237]: loss 0.514856
[epoch10, step2238]: loss 0.586631
[epoch10, step2239]: loss 0.497070
[epoch10, step2240]: loss 0.472703
[epoch10, step2241]: loss 0.275253
[epoch10, step2242]: loss 0.818597
[epoch10, step2243]: loss 0.569286
[epoch10, step2244]: loss 0.534257
[epoch10, step2245]: loss 0.464890
[epoch10, step2246]: loss 0.545914
[epoch10, step2247]: loss 0.556051
[epoch10, step2248]: loss 0.170249
[epoch10, step2249]: loss 0.604231
[epoch10, step2250]: loss 0.444442
[epoch10, step2251]: loss 0.440971
[epoch10, step2252]: loss 0.725751
[epoch10, step2253]: loss 0.485949
[epoch10, step2254]: loss 0.532113
[epoch10, step2255]: loss 0.440413
[epoch10, step2256]: loss 0.550806
[epoch10, step2257]: loss 0.410540
[epoch10, step2258]: loss 0.185166
[epoch10, step2259]: loss 0.592053
[epoch10, step2260]: loss 0.437113
[epoch10, step2261]: loss 0.582159
[epoch10, step2262]: loss 0.669477
[epoch10, step2263]: loss 0.579166
[epoch10, step2264]: loss 0.254256
[epoch10, step2265]: loss 0.643306
[epoch10, step2266]: loss 0.441419
[epoch10, step2267]: loss 0.494539
[epoch10, step2268]: loss 0.528359
[epoch10, step2269]: loss 0.449885
[epoch10, step2270]: loss 0.565330
[epoch10, step2271]: loss 0.646576
[epoch10, step2272]: loss 0.581327
[epoch10, step2273]: loss 0.582766
[epoch10, step2274]: loss 0.598176
[epoch10, step2275]: loss 0.480611
[epoch10, step2276]: loss 0.552170
[epoch10, step2277]: loss 0.687430
[epoch10, step2278]: loss 0.575424
[epoch10, step2279]: loss 0.501254
[epoch10, step2280]: loss 0.429071
[epoch10, step2281]: loss 0.412975
[epoch10, step2282]: loss 0.599944
[epoch10, step2283]: loss 0.589252
[epoch10, step2284]: loss 0.539277
[epoch10, step2285]: loss 0.661430
[epoch10, step2286]: loss 0.549793
[epoch10, step2287]: loss 0.524498
[epoch10, step2288]: loss 0.336522
[epoch10, step2289]: loss 0.316377
[epoch10, step2290]: loss 0.505162
[epoch10, step2291]: loss 0.498237
[epoch10, step2292]: loss 0.583785
[epoch10, step2293]: loss 0.253650
[epoch10, step2294]: loss 0.630152
[epoch10, step2295]: loss 0.242159
[epoch10, step2296]: loss 0.537564
[epoch10, step2297]: loss 0.503658
[epoch10, step2298]: loss 0.579178
[epoch10, step2299]: loss 0.532004
[epoch10, step2300]: loss 0.489808
[epoch10, step2301]: loss 0.692360
[epoch10, step2302]: loss 0.601096
[epoch10, step2303]: loss 0.445200
[epoch10, step2304]: loss 0.374516
[epoch10, step2305]: loss 0.473409
[epoch10, step2306]: loss 0.601361
[epoch10, step2307]: loss 0.528907
[epoch10, step2308]: loss 0.674027
[epoch10, step2309]: loss 0.632450
[epoch10, step2310]: loss 0.249309
[epoch10, step2311]: loss 0.526565
[epoch10, step2312]: loss 0.435216
[epoch10, step2313]: loss 0.587763
[epoch10, step2314]: loss 0.524159
[epoch10, step2315]: loss 0.440376
[epoch10, step2316]: loss 0.540481
[epoch10, step2317]: loss 0.595455
[epoch10, step2318]: loss 0.436127
[epoch10, step2319]: loss 0.556049
[epoch10, step2320]: loss 0.610386
[epoch10, step2321]: loss 0.395662
[epoch10, step2322]: loss 0.502514
[epoch10, step2323]: loss 0.384114
[epoch10, step2324]: loss 0.437218
[epoch10, step2325]: loss 0.477326
[epoch10, step2326]: loss 0.473016
[epoch10, step2327]: loss 0.608233
[epoch10, step2328]: loss 0.555173
[epoch10, step2329]: loss 0.649914
[epoch10, step2330]: loss 0.449015
[epoch10, step2331]: loss 0.470096
[epoch10, step2332]: loss 0.500690
[epoch10, step2333]: loss 0.580383
[epoch10, step2334]: loss 0.615776
[epoch10, step2335]: loss 0.504082
[epoch10, step2336]: loss 0.426935
[epoch10, step2337]: loss 0.191940
[epoch10, step2338]: loss 0.648302
[epoch10, step2339]: loss 0.533164
[epoch10, step2340]: loss 0.531372
[epoch10, step2341]: loss 0.693462
[epoch10, step2342]: loss 0.274776
[epoch10, step2343]: loss 0.517440
[epoch10, step2344]: loss 0.590466
[epoch10, step2345]: loss 0.256554
[epoch10, step2346]: loss 0.637650
[epoch10, step2347]: loss 0.571366
[epoch10, step2348]: loss 0.811253
[epoch10, step2349]: loss 0.553515
[epoch10, step2350]: loss 0.738853
[epoch10, step2351]: loss 0.448527
[epoch10, step2352]: loss 0.493197
[epoch10, step2353]: loss 0.529608
[epoch10, step2354]: loss 0.323248
[epoch10, step2355]: loss 0.431418
[epoch10, step2356]: loss 0.504526
[epoch10, step2357]: loss 0.359606
[epoch10, step2358]: loss 0.698157
[epoch10, step2359]: loss 0.595981
[epoch10, step2360]: loss 0.585909
[epoch10, step2361]: loss 0.484503
[epoch10, step2362]: loss 0.452925
[epoch10, step2363]: loss 0.361130
[epoch10, step2364]: loss 0.335003
[epoch10, step2365]: loss 0.451988
[epoch10, step2366]: loss 0.620553
[epoch10, step2367]: loss 0.382197
[epoch10, step2368]: loss 0.530643
[epoch10, step2369]: loss 0.320423
[epoch10, step2370]: loss 0.343317
[epoch10, step2371]: loss 0.398973
[epoch10, step2372]: loss 0.498099
[epoch10, step2373]: loss 0.236402
[epoch10, step2374]: loss 0.370655
[epoch10, step2375]: loss 0.292581
[epoch10, step2376]: loss 0.305690
[epoch10, step2377]: loss 0.450369
[epoch10, step2378]: loss 0.468118
[epoch10, step2379]: loss 0.427024
[epoch10, step2380]: loss 0.558238
[epoch10, step2381]: loss 0.598089
[epoch10, step2382]: loss 0.342738
[epoch10, step2383]: loss 0.795285
[epoch10, step2384]: loss 0.310654
[epoch10, step2385]: loss 0.455989
[epoch10, step2386]: loss 0.139984
[epoch10, step2387]: loss 0.544729
[epoch10, step2388]: loss 0.511174
[epoch10, step2389]: loss 0.742968
[epoch10, step2390]: loss 0.700945
[epoch10, step2391]: loss 0.452451
[epoch10, step2392]: loss 0.655663
[epoch10, step2393]: loss 0.523516
[epoch10, step2394]: loss 0.414292
[epoch10, step2395]: loss 0.238588
[epoch10, step2396]: loss 0.545170
[epoch10, step2397]: loss 0.361281
[epoch10, step2398]: loss 0.721771
[epoch10, step2399]: loss 0.562768
[epoch10, step2400]: loss 0.442084
[epoch10, step2401]: loss 0.714630
[epoch10, step2402]: loss 0.414826
[epoch10, step2403]: loss 0.490081
[epoch10, step2404]: loss 0.495636
[epoch10, step2405]: loss 0.670139
[epoch10, step2406]: loss 0.521425
[epoch10, step2407]: loss 0.605092
[epoch10, step2408]: loss 0.452759
[epoch10, step2409]: loss 0.393897
[epoch10, step2410]: loss 0.487106
[epoch10, step2411]: loss 0.535556
[epoch10, step2412]: loss 0.471395
[epoch10, step2413]: loss 0.169652
[epoch10, step2414]: loss 0.570012
[epoch10, step2415]: loss 0.413040
[epoch10, step2416]: loss 0.399364
[epoch10, step2417]: loss 0.536520
[epoch10, step2418]: loss 0.190194
[epoch10, step2419]: loss 0.644895
[epoch10, step2420]: loss 0.470700
[epoch10, step2421]: loss 0.441284
[epoch10, step2422]: loss 0.464599
[epoch10, step2423]: loss 0.394617
[epoch10, step2424]: loss 0.422467
[epoch10, step2425]: loss 0.419767
[epoch10, step2426]: loss 0.497854
[epoch10, step2427]: loss 0.625156
[epoch10, step2428]: loss 0.618333
[epoch10, step2429]: loss 0.542879
[epoch10, step2430]: loss 0.532202
[epoch10, step2431]: loss 0.688515
[epoch10, step2432]: loss 0.583374
[epoch10, step2433]: loss 0.505836
[epoch10, step2434]: loss 0.615236
[epoch10, step2435]: loss 0.364908
[epoch10, step2436]: loss 0.410270
[epoch10, step2437]: loss 0.354358
[epoch10, step2438]: loss 0.728456
[epoch10, step2439]: loss 0.241050
[epoch10, step2440]: loss 0.461589
[epoch10, step2441]: loss 0.597908
[epoch10, step2442]: loss 0.378213
[epoch10, step2443]: loss 0.651233
[epoch10, step2444]: loss 0.585523
[epoch10, step2445]: loss 0.475065
[epoch10, step2446]: loss 0.256938
[epoch10, step2447]: loss 0.315831
[epoch10, step2448]: loss 0.471181
[epoch10, step2449]: loss 0.708031
[epoch10, step2450]: loss 0.528588
[epoch10, step2451]: loss 0.376870
[epoch10, step2452]: loss 0.529749
[epoch10, step2453]: loss 0.474300
[epoch10, step2454]: loss 0.595965
[epoch10, step2455]: loss 0.624039
[epoch10, step2456]: loss 0.481380
[epoch10, step2457]: loss 0.636942
[epoch10, step2458]: loss 0.473867
[epoch10, step2459]: loss 0.477120
[epoch10, step2460]: loss 0.486059
[epoch10, step2461]: loss 0.361556
[epoch10, step2462]: loss 0.497128
[epoch10, step2463]: loss 0.370238
[epoch10, step2464]: loss 0.204016
[epoch10, step2465]: loss 0.450512
[epoch10, step2466]: loss 0.687575
[epoch10, step2467]: loss 0.561727
[epoch10, step2468]: loss 0.506200
[epoch10, step2469]: loss 0.433224
[epoch10, step2470]: loss 0.485459
[epoch10, step2471]: loss 0.481247
[epoch10, step2472]: loss 0.648959
[epoch10, step2473]: loss 0.541975
[epoch10, step2474]: loss 0.475915
[epoch10, step2475]: loss 0.568995
[epoch10, step2476]: loss 0.537749
[epoch10, step2477]: loss 0.320995
[epoch10, step2478]: loss 0.313891
[epoch10, step2479]: loss 0.312743
[epoch10, step2480]: loss 0.490519
[epoch10, step2481]: loss 0.194055
[epoch10, step2482]: loss 0.636672
[epoch10, step2483]: loss 0.460214
[epoch10, step2484]: loss 0.362248
[epoch10, step2485]: loss 0.767247
[epoch10, step2486]: loss 0.495022
[epoch10, step2487]: loss 0.459555
[epoch10, step2488]: loss 0.440023
[epoch10, step2489]: loss 0.343027
[epoch10, step2490]: loss 0.440135
[epoch10, step2491]: loss 0.466951
[epoch10, step2492]: loss 0.640184
[epoch10, step2493]: loss 0.578625
[epoch10, step2494]: loss 0.509943
[epoch10, step2495]: loss 0.479606
[epoch10, step2496]: loss 0.628790
[epoch10, step2497]: loss 0.504759
[epoch10, step2498]: loss 0.473052
[epoch10, step2499]: loss 0.493995
[epoch10, step2500]: loss 0.470406
[epoch10, step2501]: loss 0.544761
[epoch10, step2502]: loss 0.458380
[epoch10, step2503]: loss 0.434079
[epoch10, step2504]: loss 0.325385
[epoch10, step2505]: loss 0.629895
[epoch10, step2506]: loss 0.338560
[epoch10, step2507]: loss 0.277122
[epoch10, step2508]: loss 0.643055
[epoch10, step2509]: loss 0.601515
[epoch10, step2510]: loss 0.597000
[epoch10, step2511]: loss 0.362941
[epoch10, step2512]: loss 0.474916
[epoch10, step2513]: loss 0.441975
[epoch10, step2514]: loss 0.373386
[epoch10, step2515]: loss 0.592404
[epoch10, step2516]: loss 0.532071
[epoch10, step2517]: loss 0.581612
[epoch10, step2518]: loss 0.713857
[epoch10, step2519]: loss 0.474555
[epoch10, step2520]: loss 0.594625
[epoch10, step2521]: loss 0.574761
[epoch10, step2522]: loss 0.451056
[epoch10, step2523]: loss 0.417996
[epoch10, step2524]: loss 0.463957
[epoch10, step2525]: loss 0.602312
[epoch10, step2526]: loss 0.560110
[epoch10, step2527]: loss 0.442831
[epoch10, step2528]: loss 0.492144
[epoch10, step2529]: loss 0.545091
[epoch10, step2530]: loss 0.164748
[epoch10, step2531]: loss 0.667013
[epoch10, step2532]: loss 0.569105
[epoch10, step2533]: loss 0.472352
[epoch10, step2534]: loss 0.393941
[epoch10, step2535]: loss 0.132924
[epoch10, step2536]: loss 0.659041
[epoch10, step2537]: loss 0.150797
[epoch10, step2538]: loss 0.470167
[epoch10, step2539]: loss 0.457866
[epoch10, step2540]: loss 0.368705
[epoch10, step2541]: loss 0.477758
[epoch10, step2542]: loss 0.588431
[epoch10, step2543]: loss 0.301006
[epoch10, step2544]: loss 0.433906
[epoch10, step2545]: loss 0.657950
[epoch10, step2546]: loss 0.585474
[epoch10, step2547]: loss 0.727840
[epoch10, step2548]: loss 0.377522
[epoch10, step2549]: loss 0.525682
[epoch10, step2550]: loss 0.609545
[epoch10, step2551]: loss 0.482249
[epoch10, step2552]: loss 0.616974
[epoch10, step2553]: loss 0.614977
[epoch10, step2554]: loss 0.401328
[epoch10, step2555]: loss 0.316857
[epoch10, step2556]: loss 0.435176
[epoch10, step2557]: loss 0.613045
[epoch10, step2558]: loss 0.690958
[epoch10, step2559]: loss 0.533635
[epoch10, step2560]: loss 0.523727
[epoch10, step2561]: loss 0.321594
[epoch10, step2562]: loss 0.301919
[epoch10, step2563]: loss 0.310016
[epoch10, step2564]: loss 0.254035
[epoch10, step2565]: loss 0.457475
[epoch10, step2566]: loss 0.407231
[epoch10, step2567]: loss 0.262350
[epoch10, step2568]: loss 0.501455
[epoch10, step2569]: loss 0.589564
[epoch10, step2570]: loss 0.621655
[epoch10, step2571]: loss 0.526065
[epoch10, step2572]: loss 0.358793
[epoch10, step2573]: loss 0.487159
[epoch10, step2574]: loss 0.262111
[epoch10, step2575]: loss 0.286489
[epoch10, step2576]: loss 0.548745
[epoch10, step2577]: loss 0.411785
[epoch10, step2578]: loss 0.502065
[epoch10, step2579]: loss 0.470442
[epoch10, step2580]: loss 0.473654
[epoch10, step2581]: loss 0.248503
[epoch10, step2582]: loss 0.322798
[epoch10, step2583]: loss 0.602026
[epoch10, step2584]: loss 0.455624
[epoch10, step2585]: loss 0.511962
[epoch10, step2586]: loss 0.489323
[epoch10, step2587]: loss 0.654026
[epoch10, step2588]: loss 0.120018
[epoch10, step2589]: loss 0.530736
[epoch10, step2590]: loss 0.411499
[epoch10, step2591]: loss 0.597849
[epoch10, step2592]: loss 0.397535
[epoch10, step2593]: loss 0.465791
[epoch10, step2594]: loss 0.627119
[epoch10, step2595]: loss 0.435522
[epoch10, step2596]: loss 0.548132
[epoch10, step2597]: loss 0.646631
[epoch10, step2598]: loss 0.323378
[epoch10, step2599]: loss 0.614103
[epoch10, step2600]: loss 0.479179
[epoch10, step2601]: loss 0.561193
[epoch10, step2602]: loss 0.411537
[epoch10, step2603]: loss 0.596513
[epoch10, step2604]: loss 0.277992
[epoch10, step2605]: loss 0.669387
[epoch10, step2606]: loss 0.589385
[epoch10, step2607]: loss 0.504443
[epoch10, step2608]: loss 0.463446
[epoch10, step2609]: loss 0.303985
[epoch10, step2610]: loss 0.269497
[epoch10, step2611]: loss 0.640867
[epoch10, step2612]: loss 0.653402
[epoch10, step2613]: loss 0.505236
[epoch10, step2614]: loss 0.384959
[epoch10, step2615]: loss 0.300025
[epoch10, step2616]: loss 0.306639
[epoch10, step2617]: loss 0.528953
[epoch10, step2618]: loss 0.545937
[epoch10, step2619]: loss 0.453740
[epoch10, step2620]: loss 0.397253
[epoch10, step2621]: loss 0.259932
[epoch10, step2622]: loss 0.573516
[epoch10, step2623]: loss 0.237426
[epoch10, step2624]: loss 0.531491
[epoch10, step2625]: loss 0.635910
[epoch10, step2626]: loss 0.351420
[epoch10, step2627]: loss 0.404869
[epoch10, step2628]: loss 0.555975
[epoch10, step2629]: loss 0.440224
[epoch10, step2630]: loss 0.656715
[epoch10, step2631]: loss 0.651985
[epoch10, step2632]: loss 0.593452
[epoch10, step2633]: loss 0.543138
[epoch10, step2634]: loss 0.555860
[epoch10, step2635]: loss 0.465339
[epoch10, step2636]: loss 0.457449
[epoch10, step2637]: loss 0.352113
[epoch10, step2638]: loss 0.588911
[epoch10, step2639]: loss 0.500530
[epoch10, step2640]: loss 0.731345
[epoch10, step2641]: loss 0.634938
[epoch10, step2642]: loss 0.390266
[epoch10, step2643]: loss 0.689975
[epoch10, step2644]: loss 0.581034
[epoch10, step2645]: loss 0.272167
[epoch10, step2646]: loss 0.470867
[epoch10, step2647]: loss 0.249679
[epoch10, step2648]: loss 0.338415
[epoch10, step2649]: loss 0.527759
[epoch10, step2650]: loss 0.504262
[epoch10, step2651]: loss 0.370965
[epoch10, step2652]: loss 0.387678
[epoch10, step2653]: loss 0.556218
[epoch10, step2654]: loss 0.303722
[epoch10, step2655]: loss 0.312037
[epoch10, step2656]: loss 0.476707
[epoch10, step2657]: loss 0.285990
[epoch10, step2658]: loss 0.469856
[epoch10, step2659]: loss 0.554087
[epoch10, step2660]: loss 0.458233
[epoch10, step2661]: loss 0.561566
[epoch10, step2662]: loss 0.146831
[epoch10, step2663]: loss 0.743343
[epoch10, step2664]: loss 0.851721
[epoch10, step2665]: loss 0.483436
[epoch10, step2666]: loss 0.586830
[epoch10, step2667]: loss 0.514183
[epoch10, step2668]: loss 0.606784
[epoch10, step2669]: loss 0.653705
[epoch10, step2670]: loss 0.513625
[epoch10, step2671]: loss 0.529331
[epoch10, step2672]: loss 0.446305
[epoch10, step2673]: loss 0.263116
[epoch10, step2674]: loss 0.622853
[epoch10, step2675]: loss 0.361424
[epoch10, step2676]: loss 0.622321
[epoch10, step2677]: loss 0.483775
[epoch10, step2678]: loss 0.387650
[epoch10, step2679]: loss 0.513284
[epoch10, step2680]: loss 0.634964
[epoch10, step2681]: loss 0.529815
[epoch10, step2682]: loss 0.726195
[epoch10, step2683]: loss 0.514426
[epoch10, step2684]: loss 0.618419
[epoch10, step2685]: loss 0.544629
[epoch10, step2686]: loss 0.574547
[epoch10, step2687]: loss 0.538589
[epoch10, step2688]: loss 0.500312
[epoch10, step2689]: loss 0.557481
[epoch10, step2690]: loss 0.582792
[epoch10, step2691]: loss 0.496711
[epoch10, step2692]: loss 0.501006
[epoch10, step2693]: loss 0.532093
[epoch10, step2694]: loss 0.548509
[epoch10, step2695]: loss 0.557492
[epoch10, step2696]: loss 0.404521
[epoch10, step2697]: loss 0.432398
[epoch10, step2698]: loss 0.227041
[epoch10, step2699]: loss 0.473464
[epoch10, step2700]: loss 0.614905
[epoch10, step2701]: loss 0.612713
[epoch10, step2702]: loss 0.615683
[epoch10, step2703]: loss 0.371599
[epoch10, step2704]: loss 0.705752
[epoch10, step2705]: loss 0.465171
[epoch10, step2706]: loss 0.409696
[epoch10, step2707]: loss 0.288867
[epoch10, step2708]: loss 0.643757
[epoch10, step2709]: loss 0.471774
[epoch10, step2710]: loss 0.546624
[epoch10, step2711]: loss 0.579545
[epoch10, step2712]: loss 0.488641
[epoch10, step2713]: loss 0.485052
[epoch10, step2714]: loss 0.567615
[epoch10, step2715]: loss 0.467266
[epoch10, step2716]: loss 0.724715
[epoch10, step2717]: loss 0.535706
[epoch10, step2718]: loss 0.525432
[epoch10, step2719]: loss 0.451601
[epoch10, step2720]: loss 0.763581
[epoch10, step2721]: loss 0.666747
[epoch10, step2722]: loss 0.467746
[epoch10, step2723]: loss 0.534140
[epoch10, step2724]: loss 0.282674
[epoch10, step2725]: loss 0.431168
[epoch10, step2726]: loss 0.619815
[epoch10, step2727]: loss 0.394794
[epoch10, step2728]: loss 0.649220
[epoch10, step2729]: loss 0.479101
[epoch10, step2730]: loss 0.587527
[epoch10, step2731]: loss 0.479136
[epoch10, step2732]: loss 0.544954
[epoch10, step2733]: loss 0.256430
[epoch10, step2734]: loss 0.485361
[epoch10, step2735]: loss 0.706184
[epoch10, step2736]: loss 0.439294
[epoch10, step2737]: loss 0.495651
[epoch10, step2738]: loss 0.369054
[epoch10, step2739]: loss 0.407695
[epoch10, step2740]: loss 0.523037
[epoch10, step2741]: loss 0.349130
[epoch10, step2742]: loss 0.474203
[epoch10, step2743]: loss 0.412741
[epoch10, step2744]: loss 0.383403
[epoch10, step2745]: loss 0.455202
[epoch10, step2746]: loss 0.653120
[epoch10, step2747]: loss 0.431999
[epoch10, step2748]: loss 0.714281
[epoch10, step2749]: loss 0.212346
[epoch10, step2750]: loss 0.388956
[epoch10, step2751]: loss 0.387910
[epoch10, step2752]: loss 0.738084
[epoch10, step2753]: loss 0.525135
[epoch10, step2754]: loss 0.575154
[epoch10, step2755]: loss 0.457797
[epoch10, step2756]: loss 0.400483
[epoch10, step2757]: loss 0.336586
[epoch10, step2758]: loss 0.365227
[epoch10, step2759]: loss 0.742810
[epoch10, step2760]: loss 0.466629
[epoch10, step2761]: loss 0.535785
[epoch10, step2762]: loss 0.646326
[epoch10, step2763]: loss 0.526737
[epoch10, step2764]: loss 0.571733
[epoch10, step2765]: loss 0.471208
[epoch10, step2766]: loss 0.339421
[epoch10, step2767]: loss 0.096803
[epoch10, step2768]: loss 0.518295
[epoch10, step2769]: loss 0.761226
[epoch10, step2770]: loss 0.520295
[epoch10, step2771]: loss 0.844988
[epoch10, step2772]: loss 0.451276
[epoch10, step2773]: loss 0.401393
[epoch10, step2774]: loss 0.583762
[epoch10, step2775]: loss 0.625173
[epoch10, step2776]: loss 0.329527
[epoch10, step2777]: loss 0.406223
[epoch10, step2778]: loss 0.555819
[epoch10, step2779]: loss 0.538222
[epoch10, step2780]: loss 0.586966
[epoch10, step2781]: loss 0.678810
[epoch10, step2782]: loss 0.642782
[epoch10, step2783]: loss 0.525203
[epoch10, step2784]: loss 0.464927
[epoch10, step2785]: loss 0.702011
[epoch10, step2786]: loss 0.346336
[epoch10, step2787]: loss 0.665377
[epoch10, step2788]: loss 0.684310
[epoch10, step2789]: loss 0.656770
[epoch10, step2790]: loss 0.634327
[epoch10, step2791]: loss 0.423266
[epoch10, step2792]: loss 0.447636
[epoch10, step2793]: loss 0.453990
[epoch10, step2794]: loss 0.597486
[epoch10, step2795]: loss 0.652173
[epoch10, step2796]: loss 0.572508
[epoch10, step2797]: loss 0.524908
[epoch10, step2798]: loss 0.430741
[epoch10, step2799]: loss 0.345127
[epoch10, step2800]: loss 0.260572
[epoch10, step2801]: loss 0.634979
[epoch10, step2802]: loss 0.451643
[epoch10, step2803]: loss 0.385846
[epoch10, step2804]: loss 0.399799
[epoch10, step2805]: loss 0.283951
[epoch10, step2806]: loss 0.644999
[epoch10, step2807]: loss 0.511663
[epoch10, step2808]: loss 0.526696
[epoch10, step2809]: loss 0.612936
[epoch10, step2810]: loss 0.489492
[epoch10, step2811]: loss 0.538593
[epoch10, step2812]: loss 0.382860
[epoch10, step2813]: loss 0.633049
[epoch10, step2814]: loss 0.359657
[epoch10, step2815]: loss 0.439153
[epoch10, step2816]: loss 0.509169
[epoch10, step2817]: loss 0.438854
[epoch10, step2818]: loss 0.397756
[epoch10, step2819]: loss 0.668422
[epoch10, step2820]: loss 0.460817
[epoch10, step2821]: loss 0.518244
[epoch10, step2822]: loss 0.330444
[epoch10, step2823]: loss 0.587390
[epoch10, step2824]: loss 0.213008
[epoch10, step2825]: loss 0.599560
[epoch10, step2826]: loss 0.647611
[epoch10, step2827]: loss 0.442919
[epoch10, step2828]: loss 0.522408
[epoch10, step2829]: loss 0.557000
[epoch10, step2830]: loss 0.548829
[epoch10, step2831]: loss 0.446540
[epoch10, step2832]: loss 0.619634
[epoch10, step2833]: loss 0.408381
[epoch10, step2834]: loss 0.419292
[epoch10, step2835]: loss 0.679257
[epoch10, step2836]: loss 0.363903
[epoch10, step2837]: loss 0.721465
[epoch10, step2838]: loss 0.471262
[epoch10, step2839]: loss 0.348314
[epoch10, step2840]: loss 0.487254
[epoch10, step2841]: loss 0.354050
[epoch10, step2842]: loss 0.264685
[epoch10, step2843]: loss 0.471261
[epoch10, step2844]: loss 0.690456
[epoch10, step2845]: loss 0.669076
[epoch10, step2846]: loss 0.624697
[epoch10, step2847]: loss 0.485302
[epoch10, step2848]: loss 0.534929
[epoch10, step2849]: loss 0.433323
[epoch10, step2850]: loss 0.664681
[epoch10, step2851]: loss 0.466176
[epoch10, step2852]: loss 0.484423
[epoch10, step2853]: loss 0.527908
[epoch10, step2854]: loss 0.626824
[epoch10, step2855]: loss 0.414167
[epoch10, step2856]: loss 0.681971
[epoch10, step2857]: loss 0.620201
[epoch10, step2858]: loss 0.358347
[epoch10, step2859]: loss 0.551068
[epoch10, step2860]: loss 0.415075
[epoch10, step2861]: loss 0.441562
[epoch10, step2862]: loss 0.638012
[epoch10, step2863]: loss 0.715017
[epoch10, step2864]: loss 0.434586
[epoch10, step2865]: loss 0.372019
[epoch10, step2866]: loss 0.304174
[epoch10, step2867]: loss 0.466590
[epoch10, step2868]: loss 0.494642
[epoch10, step2869]: loss 0.445274
[epoch10, step2870]: loss 0.501379
[epoch10, step2871]: loss 0.593743
[epoch10, step2872]: loss 0.438246
[epoch10, step2873]: loss 0.537351
[epoch10, step2874]: loss 0.551320
[epoch10, step2875]: loss 0.588858
[epoch10, step2876]: loss 0.670676
[epoch10, step2877]: loss 0.453874
[epoch10, step2878]: loss 0.411999
[epoch10, step2879]: loss 0.473734
[epoch10, step2880]: loss 0.617711
[epoch10, step2881]: loss 0.581303
[epoch10, step2882]: loss 0.465747
[epoch10, step2883]: loss 0.555953
[epoch10, step2884]: loss 0.461218
[epoch10, step2885]: loss 0.399611
[epoch10, step2886]: loss 0.512417
[epoch10, step2887]: loss 0.528160
[epoch10, step2888]: loss 0.518017
[epoch10, step2889]: loss 0.464523
[epoch10, step2890]: loss 0.486541
[epoch10, step2891]: loss 0.352456
[epoch10, step2892]: loss 0.559091
[epoch10, step2893]: loss 0.278516
[epoch10, step2894]: loss 0.546037
[epoch10, step2895]: loss 0.598777
[epoch10, step2896]: loss 0.415339
[epoch10, step2897]: loss 0.643832
[epoch10, step2898]: loss 0.385224
[epoch10, step2899]: loss 0.499825
[epoch10, step2900]: loss 0.612609
[epoch10, step2901]: loss 0.392942
[epoch10, step2902]: loss 0.492013
[epoch10, step2903]: loss 0.417048
[epoch10, step2904]: loss 0.428615
[epoch10, step2905]: loss 0.294633
[epoch10, step2906]: loss 0.609607
[epoch10, step2907]: loss 0.728387
[epoch10, step2908]: loss 0.622832
[epoch10, step2909]: loss 0.619809
[epoch10, step2910]: loss 0.691615
[epoch10, step2911]: loss 0.417312
[epoch10, step2912]: loss 0.429258
[epoch10, step2913]: loss 0.620280
[epoch10, step2914]: loss 0.511646
[epoch10, step2915]: loss 0.255219
[epoch10, step2916]: loss 0.392112
[epoch10, step2917]: loss 0.454470
[epoch10, step2918]: loss 0.621002
[epoch10, step2919]: loss 0.428526
[epoch10, step2920]: loss 0.550017
[epoch10, step2921]: loss 0.395482
[epoch10, step2922]: loss 0.626595
[epoch10, step2923]: loss 0.422761
[epoch10, step2924]: loss 0.542158
[epoch10, step2925]: loss 0.548732
[epoch10, step2926]: loss 0.455103
[epoch10, step2927]: loss 0.220986
[epoch10, step2928]: loss 0.426677
[epoch10, step2929]: loss 0.671928
[epoch10, step2930]: loss 0.323040
[epoch10, step2931]: loss 0.487259
[epoch10, step2932]: loss 0.479826
[epoch10, step2933]: loss 0.559043
[epoch10, step2934]: loss 0.702677
[epoch10, step2935]: loss 0.466936
[epoch10, step2936]: loss 0.293075
[epoch10, step2937]: loss 0.532262
[epoch10, step2938]: loss 0.353578
[epoch10, step2939]: loss 0.681065
[epoch10, step2940]: loss 0.588848
[epoch10, step2941]: loss 0.236509
[epoch10, step2942]: loss 0.627348
[epoch10, step2943]: loss 0.585563
[epoch10, step2944]: loss 0.645539
[epoch10, step2945]: loss 0.445041
[epoch10, step2946]: loss 0.431750
[epoch10, step2947]: loss 0.237056
[epoch10, step2948]: loss 0.593423
[epoch10, step2949]: loss 0.463890
[epoch10, step2950]: loss 0.431942
[epoch10, step2951]: loss 0.535815
[epoch10, step2952]: loss 0.574994
[epoch10, step2953]: loss 0.537208
[epoch10, step2954]: loss 0.431591
[epoch10, step2955]: loss 0.356058
[epoch10, step2956]: loss 0.305771
[epoch10, step2957]: loss 0.543393
[epoch10, step2958]: loss 0.484222
[epoch10, step2959]: loss 0.525194
[epoch10, step2960]: loss 0.527745
[epoch10, step2961]: loss 0.506647
[epoch10, step2962]: loss 0.642438
[epoch10, step2963]: loss 0.609137
[epoch10, step2964]: loss 0.723004
[epoch10, step2965]: loss 0.502054
[epoch10, step2966]: loss 0.480654
[epoch10, step2967]: loss 0.417179
[epoch10, step2968]: loss 0.715404
[epoch10, step2969]: loss 0.551216
[epoch10, step2970]: loss 0.399834
[epoch10, step2971]: loss 0.548599
[epoch10, step2972]: loss 0.474595
[epoch10, step2973]: loss 0.647294
[epoch10, step2974]: loss 0.464412
[epoch10, step2975]: loss 0.516115
[epoch10, step2976]: loss 0.353482
[epoch10, step2977]: loss 0.643877
[epoch10, step2978]: loss 0.679310
[epoch10, step2979]: loss 0.677186
[epoch10, step2980]: loss 0.395278
[epoch10, step2981]: loss 0.554982
[epoch10, step2982]: loss 0.479927
[epoch10, step2983]: loss 0.510598
[epoch10, step2984]: loss 0.434031
[epoch10, step2985]: loss 0.486543
[epoch10, step2986]: loss 0.409556
[epoch10, step2987]: loss 0.634910
[epoch10, step2988]: loss 0.493652
[epoch10, step2989]: loss 0.743946
[epoch10, step2990]: loss 0.589496
[epoch10, step2991]: loss 0.527898
[epoch10, step2992]: loss 0.654306
[epoch10, step2993]: loss 0.400248
[epoch10, step2994]: loss 0.557710
[epoch10, step2995]: loss 0.550596
[epoch10, step2996]: loss 0.566355
[epoch10, step2997]: loss 0.588067
[epoch10, step2998]: loss 0.573253
[epoch10, step2999]: loss 0.497562
[epoch10, step3000]: loss 0.588753
[epoch10, step3001]: loss 0.451907
[epoch10, step3002]: loss 0.519862
[epoch10, step3003]: loss 0.437638
[epoch10, step3004]: loss 0.450968
[epoch10, step3005]: loss 0.656663
[epoch10, step3006]: loss 0.739502
[epoch10, step3007]: loss 0.575290
[epoch10, step3008]: loss 0.528764
[epoch10, step3009]: loss 0.670109
[epoch10, step3010]: loss 0.490980
[epoch10, step3011]: loss 0.369707
[epoch10, step3012]: loss 0.400485
[epoch10, step3013]: loss 0.683338
[epoch10, step3014]: loss 0.598733
[epoch10, step3015]: loss 0.701058
[epoch10, step3016]: loss 0.475921
[epoch10, step3017]: loss 0.511378
[epoch10, step3018]: loss 0.266993
[epoch10, step3019]: loss 0.617103
[epoch10, step3020]: loss 0.105444
[epoch10, step3021]: loss 0.365761
[epoch10, step3022]: loss 0.401870
[epoch10, step3023]: loss 0.661626
[epoch10, step3024]: loss 0.503704
[epoch10, step3025]: loss 0.666978
[epoch10, step3026]: loss 0.431000
[epoch10, step3027]: loss 0.693422
[epoch10, step3028]: loss 0.490958
[epoch10, step3029]: loss 0.690135
[epoch10, step3030]: loss 0.400831
[epoch10, step3031]: loss 0.635084
[epoch10, step3032]: loss 0.595050
[epoch10, step3033]: loss 0.600089
[epoch10, step3034]: loss 0.667747
[epoch10, step3035]: loss 0.440553
[epoch10, step3036]: loss 0.280064
[epoch10, step3037]: loss 0.580521
[epoch10, step3038]: loss 0.624612
[epoch10, step3039]: loss 0.591313
[epoch10, step3040]: loss 0.478953
[epoch10, step3041]: loss 0.466562
[epoch10, step3042]: loss 0.593908
[epoch10, step3043]: loss 0.300979
[epoch10, step3044]: loss 0.695085
[epoch10, step3045]: loss 0.443776
[epoch10, step3046]: loss 0.432754
[epoch10, step3047]: loss 0.579735
[epoch10, step3048]: loss 0.537653
[epoch10, step3049]: loss 0.546993
[epoch10, step3050]: loss 0.574185
[epoch10, step3051]: loss 0.520359
[epoch10, step3052]: loss 0.135542
[epoch10, step3053]: loss 0.359324
[epoch10, step3054]: loss 0.342433
[epoch10, step3055]: loss 0.573537
[epoch10, step3056]: loss 0.503584
[epoch10, step3057]: loss 0.582179
[epoch10, step3058]: loss 0.603018
[epoch10, step3059]: loss 0.560413
[epoch10, step3060]: loss 0.600452
[epoch10, step3061]: loss 0.324535
[epoch10, step3062]: loss 0.571277
[epoch10, step3063]: loss 0.292538
[epoch10, step3064]: loss 0.323233
[epoch10, step3065]: loss 0.547840
[epoch10, step3066]: loss 0.344887
[epoch10, step3067]: loss 0.538290
[epoch10, step3068]: loss 0.728238
[epoch10, step3069]: loss 0.624619
[epoch10, step3070]: loss 0.705197
[epoch10, step3071]: loss 0.436331
[epoch10, step3072]: loss 0.173369
[epoch10, step3073]: loss 0.401518
[epoch10, step3074]: loss 0.513067
[epoch10, step3075]: loss 0.557444
[epoch10, step3076]: loss 0.605344

[epoch10]: avg loss 0.605344

[epoch11, step1]: loss 0.685911
[epoch11, step2]: loss 0.657734
[epoch11, step3]: loss 0.470842
[epoch11, step4]: loss 0.333981
[epoch11, step5]: loss 0.617203
[epoch11, step6]: loss 0.286668
[epoch11, step7]: loss 0.610210
[epoch11, step8]: loss 0.654695
[epoch11, step9]: loss 0.526493
[epoch11, step10]: loss 0.294065
[epoch11, step11]: loss 0.491385
[epoch11, step12]: loss 0.572338
[epoch11, step13]: loss 0.422863
[epoch11, step14]: loss 0.368496
[epoch11, step15]: loss 0.256821
[epoch11, step16]: loss 0.455850
[epoch11, step17]: loss 0.553248
[epoch11, step18]: loss 0.364598
[epoch11, step19]: loss 0.482421
[epoch11, step20]: loss 0.481983
[epoch11, step21]: loss 0.587006
[epoch11, step22]: loss 0.343149
[epoch11, step23]: loss 0.510836
[epoch11, step24]: loss 0.726121
[epoch11, step25]: loss 0.619840
[epoch11, step26]: loss 0.443340
[epoch11, step27]: loss 0.466594
[epoch11, step28]: loss 0.442847
[epoch11, step29]: loss 0.560721
[epoch11, step30]: loss 0.704999
[epoch11, step31]: loss 0.436067
[epoch11, step32]: loss 0.546320
[epoch11, step33]: loss 0.375440
[epoch11, step34]: loss 0.668289
[epoch11, step35]: loss 0.741855
[epoch11, step36]: loss 0.649062
[epoch11, step37]: loss 0.584700
[epoch11, step38]: loss 0.441826
[epoch11, step39]: loss 0.232213
[epoch11, step40]: loss 0.434221
[epoch11, step41]: loss 0.647808
[epoch11, step42]: loss 0.582552
[epoch11, step43]: loss 0.387918
[epoch11, step44]: loss 0.628903
[epoch11, step45]: loss 0.686116
[epoch11, step46]: loss 0.433597
[epoch11, step47]: loss 0.652183
[epoch11, step48]: loss 0.397689
[epoch11, step49]: loss 0.361524
[epoch11, step50]: loss 0.385317
[epoch11, step51]: loss 0.588989
[epoch11, step52]: loss 0.602376
[epoch11, step53]: loss 0.340048
[epoch11, step54]: loss 0.636820
[epoch11, step55]: loss 0.685782
[epoch11, step56]: loss 0.699733
[epoch11, step57]: loss 0.629836
[epoch11, step58]: loss 0.364497
[epoch11, step59]: loss 0.605569
[epoch11, step60]: loss 0.559515
[epoch11, step61]: loss 0.368162
[epoch11, step62]: loss 0.351057
[epoch11, step63]: loss 0.459438
[epoch11, step64]: loss 0.465450
[epoch11, step65]: loss 0.335877
[epoch11, step66]: loss 0.475764
[epoch11, step67]: loss 0.541525
[epoch11, step68]: loss 0.512860
[epoch11, step69]: loss 0.342735
[epoch11, step70]: loss 0.469170
[epoch11, step71]: loss 0.580098
[epoch11, step72]: loss 0.486181
[epoch11, step73]: loss 0.621138
[epoch11, step74]: loss 0.232765
[epoch11, step75]: loss 0.534714
[epoch11, step76]: loss 0.549589
[epoch11, step77]: loss 0.533392
[epoch11, step78]: loss 0.491440
[epoch11, step79]: loss 0.661189
[epoch11, step80]: loss 0.520879
[epoch11, step81]: loss 0.478583
[epoch11, step82]: loss 0.402826
[epoch11, step83]: loss 0.439732
[epoch11, step84]: loss 0.667497
[epoch11, step85]: loss 0.525387
[epoch11, step86]: loss 0.477791
[epoch11, step87]: loss 0.122806
[epoch11, step88]: loss 0.641540
[epoch11, step89]: loss 0.598061
[epoch11, step90]: loss 0.673798
[epoch11, step91]: loss 0.555413
[epoch11, step92]: loss 0.746533
[epoch11, step93]: loss 0.640283
[epoch11, step94]: loss 0.509442
[epoch11, step95]: loss 0.625911
[epoch11, step96]: loss 0.485301
[epoch11, step97]: loss 0.549676
[epoch11, step98]: loss 0.409521
[epoch11, step99]: loss 0.582785
[epoch11, step100]: loss 0.389846
[epoch11, step101]: loss 0.450191
[epoch11, step102]: loss 0.451700
[epoch11, step103]: loss 0.394580
[epoch11, step104]: loss 0.434287
[epoch11, step105]: loss 0.515537
[epoch11, step106]: loss 0.459980
[epoch11, step107]: loss 0.559764
[epoch11, step108]: loss 0.681625
[epoch11, step109]: loss 0.333810
[epoch11, step110]: loss 0.698471
[epoch11, step111]: loss 0.538569
[epoch11, step112]: loss 0.628261
[epoch11, step113]: loss 0.613044
[epoch11, step114]: loss 0.421039
[epoch11, step115]: loss 0.421169
[epoch11, step116]: loss 0.547794
[epoch11, step117]: loss 0.561634
[epoch11, step118]: loss 0.335620
[epoch11, step119]: loss 0.606809
[epoch11, step120]: loss 0.256303
[epoch11, step121]: loss 0.337256
[epoch11, step122]: loss 0.357779
[epoch11, step123]: loss 0.280482
[epoch11, step124]: loss 0.256269
[epoch11, step125]: loss 0.383662
[epoch11, step126]: loss 0.479688
[epoch11, step127]: loss 0.452312
[epoch11, step128]: loss 0.523613
[epoch11, step129]: loss 0.685068
[epoch11, step130]: loss 0.369583
[epoch11, step131]: loss 0.681401
[epoch11, step132]: loss 0.718694
[epoch11, step133]: loss 0.426654
[epoch11, step134]: loss 0.499842
[epoch11, step135]: loss 0.529688
[epoch11, step136]: loss 0.328405
[epoch11, step137]: loss 0.527503
[epoch11, step138]: loss 0.617913
[epoch11, step139]: loss 0.399993
[epoch11, step140]: loss 0.447812
[epoch11, step141]: loss 0.425201
[epoch11, step142]: loss 0.715330
[epoch11, step143]: loss 0.349606
[epoch11, step144]: loss 0.435161
[epoch11, step145]: loss 0.604541
[epoch11, step146]: loss 0.492647
[epoch11, step147]: loss 0.599009
[epoch11, step148]: loss 0.590565
[epoch11, step149]: loss 0.318002
[epoch11, step150]: loss 0.406367
[epoch11, step151]: loss 0.393589
[epoch11, step152]: loss 0.552021
[epoch11, step153]: loss 0.439310
[epoch11, step154]: loss 0.581721
[epoch11, step155]: loss 0.609011
[epoch11, step156]: loss 0.498491
[epoch11, step157]: loss 0.387143
[epoch11, step158]: loss 0.480282
[epoch11, step159]: loss 0.430064
[epoch11, step160]: loss 0.603902
[epoch11, step161]: loss 0.459806
[epoch11, step162]: loss 0.623214
[epoch11, step163]: loss 0.357015
[epoch11, step164]: loss 0.641566
[epoch11, step165]: loss 0.563432
[epoch11, step166]: loss 0.631040
[epoch11, step167]: loss 0.444467
[epoch11, step168]: loss 0.305500
[epoch11, step169]: loss 0.457878
[epoch11, step170]: loss 0.555410
[epoch11, step171]: loss 0.443166
[epoch11, step172]: loss 0.339216
[epoch11, step173]: loss 0.528680
[epoch11, step174]: loss 0.677392
[epoch11, step175]: loss 0.407287
[epoch11, step176]: loss 0.524460
[epoch11, step177]: loss 0.429177
[epoch11, step178]: loss 0.481920
[epoch11, step179]: loss 0.409689
[epoch11, step180]: loss 0.332620
[epoch11, step181]: loss 0.392756
[epoch11, step182]: loss 0.509215
[epoch11, step183]: loss 0.616369
[epoch11, step184]: loss 0.438656
[epoch11, step185]: loss 0.475906
[epoch11, step186]: loss 0.553295
[epoch11, step187]: loss 0.559319
[epoch11, step188]: loss 0.365600
[epoch11, step189]: loss 0.475928
[epoch11, step190]: loss 0.375735
[epoch11, step191]: loss 0.662765
[epoch11, step192]: loss 0.594436
[epoch11, step193]: loss 0.422479
[epoch11, step194]: loss 0.389812
[epoch11, step195]: loss 0.693563
[epoch11, step196]: loss 0.650168
[epoch11, step197]: loss 0.663741
[epoch11, step198]: loss 0.647051
[epoch11, step199]: loss 0.609833
[epoch11, step200]: loss 0.389967
[epoch11, step201]: loss 0.603648
[epoch11, step202]: loss 0.561865
[epoch11, step203]: loss 0.170367
[epoch11, step204]: loss 0.693879
[epoch11, step205]: loss 0.546495
[epoch11, step206]: loss 0.538857
[epoch11, step207]: loss 0.572694
[epoch11, step208]: loss 0.658848
[epoch11, step209]: loss 0.605232
[epoch11, step210]: loss 0.590460
[epoch11, step211]: loss 0.352227
[epoch11, step212]: loss 0.407358
[epoch11, step213]: loss 0.361142
[epoch11, step214]: loss 0.535193
[epoch11, step215]: loss 0.644254
[epoch11, step216]: loss 0.600429
[epoch11, step217]: loss 0.426333
[epoch11, step218]: loss 0.489617
[epoch11, step219]: loss 0.700122
[epoch11, step220]: loss 0.413093
[epoch11, step221]: loss 0.282370
[epoch11, step222]: loss 0.516015
[epoch11, step223]: loss 0.520855
[epoch11, step224]: loss 0.546431
[epoch11, step225]: loss 0.521951
[epoch11, step226]: loss 0.681017
[epoch11, step227]: loss 0.499108
[epoch11, step228]: loss 0.551438
[epoch11, step229]: loss 0.403703
[epoch11, step230]: loss 0.397809
[epoch11, step231]: loss 0.597553
[epoch11, step232]: loss 0.598045
[epoch11, step233]: loss 0.311395
[epoch11, step234]: loss 0.608078
[epoch11, step235]: loss 0.485091
[epoch11, step236]: loss 0.395822
[epoch11, step237]: loss 0.374423
[epoch11, step238]: loss 0.301083
[epoch11, step239]: loss 0.349885
[epoch11, step240]: loss 0.385789
[epoch11, step241]: loss 0.358395
[epoch11, step242]: loss 0.466967
[epoch11, step243]: loss 0.387349
[epoch11, step244]: loss 0.420412
[epoch11, step245]: loss 0.480539
[epoch11, step246]: loss 0.448290
[epoch11, step247]: loss 0.615188
[epoch11, step248]: loss 0.643439
[epoch11, step249]: loss 0.345621
[epoch11, step250]: loss 0.615358
[epoch11, step251]: loss 0.460763
[epoch11, step252]: loss 0.524203
[epoch11, step253]: loss 0.527549
[epoch11, step254]: loss 0.341818
[epoch11, step255]: loss 0.408062
[epoch11, step256]: loss 0.462002
[epoch11, step257]: loss 0.632496
[epoch11, step258]: loss 0.566966
[epoch11, step259]: loss 0.399074
[epoch11, step260]: loss 0.638985
[epoch11, step261]: loss 0.564342
[epoch11, step262]: loss 0.269328
[epoch11, step263]: loss 0.615633
[epoch11, step264]: loss 0.713271
[epoch11, step265]: loss 0.296193
[epoch11, step266]: loss 0.449451
[epoch11, step267]: loss 0.730138
[epoch11, step268]: loss 0.481893
[epoch11, step269]: loss 0.625560
[epoch11, step270]: loss 0.595569
[epoch11, step271]: loss 0.512018
[epoch11, step272]: loss 0.499996
[epoch11, step273]: loss 0.359904
[epoch11, step274]: loss 0.541495
[epoch11, step275]: loss 0.128094
[epoch11, step276]: loss 0.511158
[epoch11, step277]: loss 0.612409
[epoch11, step278]: loss 0.512657
[epoch11, step279]: loss 0.562429
[epoch11, step280]: loss 0.428348
[epoch11, step281]: loss 0.617246
[epoch11, step282]: loss 0.656356
[epoch11, step283]: loss 0.587120
[epoch11, step284]: loss 0.465791
[epoch11, step285]: loss 0.445042
[epoch11, step286]: loss 0.491382
[epoch11, step287]: loss 0.529938
[epoch11, step288]: loss 0.778503
[epoch11, step289]: loss 0.708957
[epoch11, step290]: loss 0.661876
[epoch11, step291]: loss 0.310737
[epoch11, step292]: loss 0.720725
[epoch11, step293]: loss 0.557944
[epoch11, step294]: loss 0.514394
[epoch11, step295]: loss 0.541679
[epoch11, step296]: loss 0.354852
[epoch11, step297]: loss 0.367998
[epoch11, step298]: loss 0.545043
[epoch11, step299]: loss 0.648886
[epoch11, step300]: loss 0.628489
[epoch11, step301]: loss 0.281099
[epoch11, step302]: loss 0.544425
[epoch11, step303]: loss 0.792064
[epoch11, step304]: loss 0.509159
[epoch11, step305]: loss 0.430311
[epoch11, step306]: loss 0.528547
[epoch11, step307]: loss 0.375230
[epoch11, step308]: loss 0.522198
[epoch11, step309]: loss 0.461049
[epoch11, step310]: loss 0.534508
[epoch11, step311]: loss 0.118753
[epoch11, step312]: loss 0.282145
[epoch11, step313]: loss 0.828637
[epoch11, step314]: loss 0.513166
[epoch11, step315]: loss 0.528386
[epoch11, step316]: loss 0.752580
[epoch11, step317]: loss 0.484968
[epoch11, step318]: loss 0.550648
[epoch11, step319]: loss 0.398805
[epoch11, step320]: loss 0.664748
[epoch11, step321]: loss 0.649216
[epoch11, step322]: loss 0.644874
[epoch11, step323]: loss 0.550981
[epoch11, step324]: loss 0.394502
[epoch11, step325]: loss 0.576210
[epoch11, step326]: loss 0.287055
[epoch11, step327]: loss 0.485018
[epoch11, step328]: loss 0.356113
[epoch11, step329]: loss 0.547310
[epoch11, step330]: loss 0.578288
[epoch11, step331]: loss 0.395234
[epoch11, step332]: loss 0.642501
[epoch11, step333]: loss 0.522283
[epoch11, step334]: loss 0.437573
[epoch11, step335]: loss 0.317234
[epoch11, step336]: loss 0.384362
[epoch11, step337]: loss 0.433508
[epoch11, step338]: loss 0.378086
[epoch11, step339]: loss 0.431518
[epoch11, step340]: loss 0.484482
[epoch11, step341]: loss 0.689841
[epoch11, step342]: loss 0.487343
[epoch11, step343]: loss 0.579954
[epoch11, step344]: loss 0.598469
[epoch11, step345]: loss 0.563881
[epoch11, step346]: loss 0.523087
[epoch11, step347]: loss 0.365403
[epoch11, step348]: loss 0.251235
[epoch11, step349]: loss 0.430399
[epoch11, step350]: loss 0.121817
[epoch11, step351]: loss 0.471762
[epoch11, step352]: loss 0.478547
[epoch11, step353]: loss 0.597646
[epoch11, step354]: loss 0.468531
[epoch11, step355]: loss 0.506691
[epoch11, step356]: loss 0.442757
[epoch11, step357]: loss 0.540383
[epoch11, step358]: loss 0.577158
[epoch11, step359]: loss 0.515169
[epoch11, step360]: loss 0.648669
[epoch11, step361]: loss 0.574045
[epoch11, step362]: loss 0.441424
[epoch11, step363]: loss 0.456670
[epoch11, step364]: loss 0.618544
[epoch11, step365]: loss 0.710846
[epoch11, step366]: loss 0.542605
[epoch11, step367]: loss 0.256489
[epoch11, step368]: loss 0.458574
[epoch11, step369]: loss 0.135282
[epoch11, step370]: loss 0.557860
[epoch11, step371]: loss 0.723401
[epoch11, step372]: loss 0.430252
[epoch11, step373]: loss 0.552886
[epoch11, step374]: loss 0.383669
[epoch11, step375]: loss 0.491750
[epoch11, step376]: loss 0.416464
[epoch11, step377]: loss 0.462983
[epoch11, step378]: loss 0.570705
[epoch11, step379]: loss 0.550468
[epoch11, step380]: loss 0.336873
[epoch11, step381]: loss 0.428098
[epoch11, step382]: loss 0.501726
[epoch11, step383]: loss 0.389525
[epoch11, step384]: loss 0.557438
[epoch11, step385]: loss 0.604980
[epoch11, step386]: loss 0.687896
[epoch11, step387]: loss 0.358690
[epoch11, step388]: loss 0.418217
[epoch11, step389]: loss 0.387388
[epoch11, step390]: loss 0.450565
[epoch11, step391]: loss 0.777747
[epoch11, step392]: loss 0.497019
[epoch11, step393]: loss 0.423269
[epoch11, step394]: loss 0.471770
[epoch11, step395]: loss 0.487343
[epoch11, step396]: loss 0.409502
[epoch11, step397]: loss 0.554750
[epoch11, step398]: loss 0.574367
[epoch11, step399]: loss 0.477329
[epoch11, step400]: loss 0.524722
[epoch11, step401]: loss 0.493604
[epoch11, step402]: loss 0.448083
[epoch11, step403]: loss 0.407246
[epoch11, step404]: loss 0.658403
[epoch11, step405]: loss 0.472964
[epoch11, step406]: loss 0.373582
[epoch11, step407]: loss 0.302406
[epoch11, step408]: loss 0.633834
[epoch11, step409]: loss 0.683412
[epoch11, step410]: loss 0.405408
[epoch11, step411]: loss 0.546960
[epoch11, step412]: loss 0.636198
[epoch11, step413]: loss 0.569434
[epoch11, step414]: loss 0.465858
[epoch11, step415]: loss 0.462764
[epoch11, step416]: loss 0.610815
[epoch11, step417]: loss 0.511265
[epoch11, step418]: loss 0.514565
[epoch11, step419]: loss 0.449153
[epoch11, step420]: loss 0.339303
[epoch11, step421]: loss 0.434710
[epoch11, step422]: loss 0.503275
[epoch11, step423]: loss 0.612789
[epoch11, step424]: loss 0.304229
[epoch11, step425]: loss 0.446401
[epoch11, step426]: loss 0.660648
[epoch11, step427]: loss 0.563079
[epoch11, step428]: loss 0.346001
[epoch11, step429]: loss 0.541336
[epoch11, step430]: loss 0.382963
[epoch11, step431]: loss 0.648139
[epoch11, step432]: loss 0.367710
[epoch11, step433]: loss 0.580374
[epoch11, step434]: loss 0.322490
[epoch11, step435]: loss 0.686065
[epoch11, step436]: loss 0.572726
[epoch11, step437]: loss 0.522511
[epoch11, step438]: loss 0.277443
[epoch11, step439]: loss 0.607777
[epoch11, step440]: loss 0.587672
[epoch11, step441]: loss 0.637504
[epoch11, step442]: loss 0.581467
[epoch11, step443]: loss 0.571011
[epoch11, step444]: loss 0.421870
[epoch11, step445]: loss 0.352173
[epoch11, step446]: loss 0.274790
[epoch11, step447]: loss 0.549837
[epoch11, step448]: loss 0.425947
[epoch11, step449]: loss 0.334744
[epoch11, step450]: loss 0.560217
[epoch11, step451]: loss 0.481878
[epoch11, step452]: loss 0.698019
[epoch11, step453]: loss 0.432756
[epoch11, step454]: loss 0.339472
[epoch11, step455]: loss 0.443731
[epoch11, step456]: loss 0.405420
[epoch11, step457]: loss 0.378162
[epoch11, step458]: loss 0.312152
[epoch11, step459]: loss 0.457540
[epoch11, step460]: loss 0.455588
[epoch11, step461]: loss 0.623031
[epoch11, step462]: loss 0.550679
[epoch11, step463]: loss 0.767722
[epoch11, step464]: loss 0.483004
[epoch11, step465]: loss 0.561470
[epoch11, step466]: loss 0.271963
[epoch11, step467]: loss 0.635324
[epoch11, step468]: loss 0.688186
[epoch11, step469]: loss 0.704751
[epoch11, step470]: loss 0.659997
[epoch11, step471]: loss 0.250226
[epoch11, step472]: loss 0.486198
[epoch11, step473]: loss 0.327947
[epoch11, step474]: loss 0.529853
[epoch11, step475]: loss 0.418051
[epoch11, step476]: loss 0.494941
[epoch11, step477]: loss 0.642354
[epoch11, step478]: loss 0.592181
[epoch11, step479]: loss 0.382201
[epoch11, step480]: loss 0.624765
[epoch11, step481]: loss 0.759508
[epoch11, step482]: loss 0.498307
[epoch11, step483]: loss 0.469241
[epoch11, step484]: loss 0.692581
[epoch11, step485]: loss 0.412480
[epoch11, step486]: loss 0.602867
[epoch11, step487]: loss 0.585129
[epoch11, step488]: loss 0.629878
[epoch11, step489]: loss 0.333137
[epoch11, step490]: loss 0.635604
[epoch11, step491]: loss 0.599702
[epoch11, step492]: loss 0.484772
[epoch11, step493]: loss 0.220342
[epoch11, step494]: loss 0.648348
[epoch11, step495]: loss 0.257067
[epoch11, step496]: loss 0.555115
[epoch11, step497]: loss 0.244142
[epoch11, step498]: loss 0.699586
[epoch11, step499]: loss 0.720130
[epoch11, step500]: loss 0.510399
[epoch11, step501]: loss 0.728714
[epoch11, step502]: loss 0.619151
[epoch11, step503]: loss 0.559403
[epoch11, step504]: loss 0.552176
[epoch11, step505]: loss 0.325920
[epoch11, step506]: loss 0.355382
[epoch11, step507]: loss 0.522681
[epoch11, step508]: loss 0.567031
[epoch11, step509]: loss 0.396708
[epoch11, step510]: loss 0.589185
[epoch11, step511]: loss 0.475571
[epoch11, step512]: loss 0.425453
[epoch11, step513]: loss 0.232220
[epoch11, step514]: loss 0.670818
[epoch11, step515]: loss 0.324811
[epoch11, step516]: loss 0.353021
[epoch11, step517]: loss 0.637014
[epoch11, step518]: loss 0.513754
[epoch11, step519]: loss 0.755580
[epoch11, step520]: loss 0.573934
[epoch11, step521]: loss 0.708654
[epoch11, step522]: loss 0.462376
[epoch11, step523]: loss 0.690969
[epoch11, step524]: loss 0.524957
[epoch11, step525]: loss 0.345181
[epoch11, step526]: loss 0.536256
[epoch11, step527]: loss 0.404823
[epoch11, step528]: loss 0.381603
[epoch11, step529]: loss 0.480651
[epoch11, step530]: loss 0.407284
[epoch11, step531]: loss 0.579998
[epoch11, step532]: loss 0.220561
[epoch11, step533]: loss 0.474675
[epoch11, step534]: loss 0.567216
[epoch11, step535]: loss 0.468643
[epoch11, step536]: loss 0.407701
[epoch11, step537]: loss 0.482088
[epoch11, step538]: loss 0.381789
[epoch11, step539]: loss 0.518797
[epoch11, step540]: loss 0.508704
[epoch11, step541]: loss 0.582279
[epoch11, step542]: loss 0.604666
[epoch11, step543]: loss 0.557265
[epoch11, step544]: loss 0.477728
[epoch11, step545]: loss 0.672350
[epoch11, step546]: loss 0.446190
[epoch11, step547]: loss 0.541013
[epoch11, step548]: loss 0.575851
[epoch11, step549]: loss 0.601172
[epoch11, step550]: loss 0.501192
[epoch11, step551]: loss 0.458204
[epoch11, step552]: loss 0.514905
[epoch11, step553]: loss 0.287999
[epoch11, step554]: loss 0.534699
[epoch11, step555]: loss 0.443281
[epoch11, step556]: loss 0.498927
[epoch11, step557]: loss 0.349982
[epoch11, step558]: loss 0.546225
[epoch11, step559]: loss 0.494423
[epoch11, step560]: loss 0.472907
[epoch11, step561]: loss 0.626005
[epoch11, step562]: loss 0.358235
[epoch11, step563]: loss 0.365409
[epoch11, step564]: loss 0.580411
[epoch11, step565]: loss 0.382352
[epoch11, step566]: loss 0.211989
[epoch11, step567]: loss 0.395260
[epoch11, step568]: loss 0.476196
[epoch11, step569]: loss 0.532669
[epoch11, step570]: loss 0.620822
[epoch11, step571]: loss 0.430578
[epoch11, step572]: loss 0.648256
[epoch11, step573]: loss 0.388570
[epoch11, step574]: loss 0.654206
[epoch11, step575]: loss 0.490985
[epoch11, step576]: loss 0.331049
[epoch11, step577]: loss 0.579211
[epoch11, step578]: loss 0.299586
[epoch11, step579]: loss 0.468913
[epoch11, step580]: loss 0.581417
[epoch11, step581]: loss 0.710527
[epoch11, step582]: loss 0.541068
[epoch11, step583]: loss 0.592740
[epoch11, step584]: loss 0.239062
[epoch11, step585]: loss 0.503276
[epoch11, step586]: loss 0.703740
[epoch11, step587]: loss 0.329715
[epoch11, step588]: loss 0.238482
[epoch11, step589]: loss 0.605209
[epoch11, step590]: loss 0.586755
[epoch11, step591]: loss 0.371828
[epoch11, step592]: loss 0.416655
[epoch11, step593]: loss 0.638413
[epoch11, step594]: loss 0.345191
[epoch11, step595]: loss 0.483281
[epoch11, step596]: loss 0.710947
[epoch11, step597]: loss 0.639001
[epoch11, step598]: loss 0.509301
[epoch11, step599]: loss 0.505726
[epoch11, step600]: loss 0.404707
[epoch11, step601]: loss 0.450232
[epoch11, step602]: loss 0.498603
[epoch11, step603]: loss 0.398489
[epoch11, step604]: loss 0.587657
[epoch11, step605]: loss 0.508994
[epoch11, step606]: loss 0.428314
[epoch11, step607]: loss 0.483339
[epoch11, step608]: loss 0.402485
[epoch11, step609]: loss 0.591550
[epoch11, step610]: loss 0.569151
[epoch11, step611]: loss 0.499034
[epoch11, step612]: loss 0.626209
[epoch11, step613]: loss 0.473445
[epoch11, step614]: loss 0.541704
[epoch11, step615]: loss 0.585270
[epoch11, step616]: loss 0.276788
[epoch11, step617]: loss 0.530926
[epoch11, step618]: loss 0.510813
[epoch11, step619]: loss 0.556480
[epoch11, step620]: loss 0.368279
[epoch11, step621]: loss 0.629280
[epoch11, step622]: loss 0.494494
[epoch11, step623]: loss 0.468853
[epoch11, step624]: loss 0.474393
[epoch11, step625]: loss 0.577397
[epoch11, step626]: loss 0.762121
[epoch11, step627]: loss 0.726808
[epoch11, step628]: loss 0.574205
[epoch11, step629]: loss 0.356295
[epoch11, step630]: loss 0.391255
[epoch11, step631]: loss 0.505401
[epoch11, step632]: loss 0.523280
[epoch11, step633]: loss 0.356458
[epoch11, step634]: loss 0.661295
[epoch11, step635]: loss 0.530699
[epoch11, step636]: loss 0.292014
[epoch11, step637]: loss 0.594182
[epoch11, step638]: loss 0.492996
[epoch11, step639]: loss 0.445663
[epoch11, step640]: loss 0.733287
[epoch11, step641]: loss 0.466011
[epoch11, step642]: loss 0.259151
[epoch11, step643]: loss 0.337381
[epoch11, step644]: loss 0.574173
[epoch11, step645]: loss 0.685119
[epoch11, step646]: loss 0.515017
[epoch11, step647]: loss 0.550896
[epoch11, step648]: loss 0.644586
[epoch11, step649]: loss 0.499887
[epoch11, step650]: loss 0.389645
[epoch11, step651]: loss 0.282973
[epoch11, step652]: loss 0.556075
[epoch11, step653]: loss 0.771479
[epoch11, step654]: loss 0.404183
[epoch11, step655]: loss 0.616567
[epoch11, step656]: loss 0.337211
[epoch11, step657]: loss 0.394766
[epoch11, step658]: loss 0.587041
[epoch11, step659]: loss 0.442695
[epoch11, step660]: loss 0.724538
[epoch11, step661]: loss 0.582621
[epoch11, step662]: loss 0.436772
[epoch11, step663]: loss 0.616873
[epoch11, step664]: loss 0.510394
[epoch11, step665]: loss 0.568506
[epoch11, step666]: loss 0.632049
[epoch11, step667]: loss 0.384526
[epoch11, step668]: loss 0.377455
[epoch11, step669]: loss 0.755660
[epoch11, step670]: loss 0.551408
[epoch11, step671]: loss 0.606833
[epoch11, step672]: loss 0.364049
[epoch11, step673]: loss 0.211492
[epoch11, step674]: loss 0.232214
[epoch11, step675]: loss 0.585162
[epoch11, step676]: loss 0.498669
[epoch11, step677]: loss 0.470423
[epoch11, step678]: loss 0.442140
[epoch11, step679]: loss 0.669842
[epoch11, step680]: loss 0.512328
[epoch11, step681]: loss 0.642460
[epoch11, step682]: loss 0.464861
[epoch11, step683]: loss 0.552972
[epoch11, step684]: loss 0.518891
[epoch11, step685]: loss 0.497856
[epoch11, step686]: loss 0.484413
[epoch11, step687]: loss 0.364979
[epoch11, step688]: loss 0.453439
[epoch11, step689]: loss 0.481212
[epoch11, step690]: loss 0.579643
[epoch11, step691]: loss 0.389856
[epoch11, step692]: loss 0.577743
[epoch11, step693]: loss 0.398633
[epoch11, step694]: loss 0.736637
[epoch11, step695]: loss 0.685902
[epoch11, step696]: loss 0.600998
[epoch11, step697]: loss 0.556465
[epoch11, step698]: loss 0.454813
[epoch11, step699]: loss 0.424503
[epoch11, step700]: loss 0.403374
[epoch11, step701]: loss 0.498719
[epoch11, step702]: loss 0.484184
[epoch11, step703]: loss 0.436530
[epoch11, step704]: loss 0.555557
[epoch11, step705]: loss 0.584896
[epoch11, step706]: loss 0.228157
[epoch11, step707]: loss 0.525193
[epoch11, step708]: loss 0.498395
[epoch11, step709]: loss 0.489605
[epoch11, step710]: loss 0.421306
[epoch11, step711]: loss 0.624514
[epoch11, step712]: loss 0.468301
[epoch11, step713]: loss 0.685326
[epoch11, step714]: loss 0.523828
[epoch11, step715]: loss 0.550832
[epoch11, step716]: loss 0.264895
[epoch11, step717]: loss 0.517523
[epoch11, step718]: loss 0.141702
[epoch11, step719]: loss 0.151775
[epoch11, step720]: loss 0.452548
[epoch11, step721]: loss 0.334320
[epoch11, step722]: loss 0.680128
[epoch11, step723]: loss 0.362680
[epoch11, step724]: loss 0.538273
[epoch11, step725]: loss 0.485970
[epoch11, step726]: loss 0.705739
[epoch11, step727]: loss 0.405608
[epoch11, step728]: loss 0.453327
[epoch11, step729]: loss 0.340518
[epoch11, step730]: loss 0.635009
[epoch11, step731]: loss 0.256933
[epoch11, step732]: loss 0.150083
[epoch11, step733]: loss 0.482408
[epoch11, step734]: loss 0.579248
[epoch11, step735]: loss 0.627172
[epoch11, step736]: loss 0.498261
[epoch11, step737]: loss 0.599072
[epoch11, step738]: loss 0.453406
[epoch11, step739]: loss 0.564352
[epoch11, step740]: loss 0.439200
[epoch11, step741]: loss 0.569449
[epoch11, step742]: loss 0.598337
[epoch11, step743]: loss 0.415501
[epoch11, step744]: loss 0.734730
[epoch11, step745]: loss 0.340592
[epoch11, step746]: loss 0.503529
[epoch11, step747]: loss 0.520397
[epoch11, step748]: loss 0.382885
[epoch11, step749]: loss 0.548167
[epoch11, step750]: loss 0.461532
[epoch11, step751]: loss 0.622513
[epoch11, step752]: loss 0.541304
[epoch11, step753]: loss 0.446289
[epoch11, step754]: loss 0.255250
[epoch11, step755]: loss 0.413351
[epoch11, step756]: loss 0.449825
[epoch11, step757]: loss 0.583712
[epoch11, step758]: loss 0.345368
[epoch11, step759]: loss 0.429123
[epoch11, step760]: loss 0.695341
[epoch11, step761]: loss 0.615845
[epoch11, step762]: loss 0.567840
[epoch11, step763]: loss 0.444913
[epoch11, step764]: loss 0.579020
[epoch11, step765]: loss 0.448607
[epoch11, step766]: loss 0.521550
[epoch11, step767]: loss 0.244885
[epoch11, step768]: loss 0.499651
[epoch11, step769]: loss 0.572214
[epoch11, step770]: loss 0.619255
[epoch11, step771]: loss 0.504024
[epoch11, step772]: loss 0.598510
[epoch11, step773]: loss 0.341942
[epoch11, step774]: loss 0.367306
[epoch11, step775]: loss 0.452762
[epoch11, step776]: loss 0.367250
[epoch11, step777]: loss 0.669081
[epoch11, step778]: loss 0.430770
[epoch11, step779]: loss 0.512621
[epoch11, step780]: loss 0.626982
[epoch11, step781]: loss 0.504895
[epoch11, step782]: loss 0.443754
[epoch11, step783]: loss 0.573344
[epoch11, step784]: loss 0.411380
[epoch11, step785]: loss 0.562460
[epoch11, step786]: loss 0.466950
[epoch11, step787]: loss 0.449461
[epoch11, step788]: loss 0.565886
[epoch11, step789]: loss 0.533705
[epoch11, step790]: loss 0.600909
[epoch11, step791]: loss 0.366158
[epoch11, step792]: loss 0.351074
[epoch11, step793]: loss 0.316688
[epoch11, step794]: loss 0.476508
[epoch11, step795]: loss 0.431784
[epoch11, step796]: loss 0.488541
[epoch11, step797]: loss 0.539289
[epoch11, step798]: loss 0.361739
[epoch11, step799]: loss 0.396205
[epoch11, step800]: loss 0.519748
[epoch11, step801]: loss 0.368222
[epoch11, step802]: loss 0.455838
[epoch11, step803]: loss 0.573661
[epoch11, step804]: loss 0.593905
[epoch11, step805]: loss 0.520661
[epoch11, step806]: loss 0.389537
[epoch11, step807]: loss 0.563864
[epoch11, step808]: loss 0.449073
[epoch11, step809]: loss 0.358776
[epoch11, step810]: loss 0.434575
[epoch11, step811]: loss 0.801639
[epoch11, step812]: loss 0.465308
[epoch11, step813]: loss 0.671376
[epoch11, step814]: loss 0.615897
[epoch11, step815]: loss 0.528082
[epoch11, step816]: loss 0.505097
[epoch11, step817]: loss 0.581175
[epoch11, step818]: loss 0.490308
[epoch11, step819]: loss 0.727927
[epoch11, step820]: loss 0.607940
[epoch11, step821]: loss 0.495995
[epoch11, step822]: loss 0.622954
[epoch11, step823]: loss 0.355689
[epoch11, step824]: loss 0.641650
[epoch11, step825]: loss 0.725800
[epoch11, step826]: loss 0.270045
[epoch11, step827]: loss 0.448491
[epoch11, step828]: loss 0.451160
[epoch11, step829]: loss 0.681873
[epoch11, step830]: loss 0.676844
[epoch11, step831]: loss 0.641648
[epoch11, step832]: loss 0.401108
[epoch11, step833]: loss 0.534505
[epoch11, step834]: loss 0.529168
[epoch11, step835]: loss 0.350634
[epoch11, step836]: loss 0.441030
[epoch11, step837]: loss 0.675331
[epoch11, step838]: loss 0.428754
[epoch11, step839]: loss 0.257546
[epoch11, step840]: loss 0.493818
[epoch11, step841]: loss 0.589953
[epoch11, step842]: loss 0.321395
[epoch11, step843]: loss 0.516148
[epoch11, step844]: loss 0.491618
[epoch11, step845]: loss 0.322486
[epoch11, step846]: loss 0.401562
[epoch11, step847]: loss 0.512651
[epoch11, step848]: loss 0.455979
[epoch11, step849]: loss 0.215699
[epoch11, step850]: loss 0.683595
[epoch11, step851]: loss 0.430890
[epoch11, step852]: loss 0.315670
[epoch11, step853]: loss 0.402971
[epoch11, step854]: loss 0.541070
[epoch11, step855]: loss 0.483663
[epoch11, step856]: loss 0.493424
[epoch11, step857]: loss 0.634826
[epoch11, step858]: loss 0.610028
[epoch11, step859]: loss 0.442236
[epoch11, step860]: loss 0.576746
[epoch11, step861]: loss 0.519614
[epoch11, step862]: loss 0.523832
[epoch11, step863]: loss 0.519997
[epoch11, step864]: loss 0.605621
[epoch11, step865]: loss 0.297335
[epoch11, step866]: loss 0.641544
[epoch11, step867]: loss 0.349057
[epoch11, step868]: loss 0.699410
[epoch11, step869]: loss 0.280210
[epoch11, step870]: loss 0.563307
[epoch11, step871]: loss 0.601774
[epoch11, step872]: loss 0.610787
[epoch11, step873]: loss 0.433411
[epoch11, step874]: loss 0.510305
[epoch11, step875]: loss 0.591106
[epoch11, step876]: loss 0.470482
[epoch11, step877]: loss 0.494018
[epoch11, step878]: loss 0.385347
[epoch11, step879]: loss 0.411840
[epoch11, step880]: loss 0.617351
[epoch11, step881]: loss 0.298464
[epoch11, step882]: loss 0.595440
[epoch11, step883]: loss 0.553001
[epoch11, step884]: loss 0.584430
[epoch11, step885]: loss 0.348245
[epoch11, step886]: loss 0.516978
[epoch11, step887]: loss 0.502136
[epoch11, step888]: loss 0.502393
[epoch11, step889]: loss 0.537131
[epoch11, step890]: loss 0.347119
[epoch11, step891]: loss 0.520494
[epoch11, step892]: loss 0.530483
[epoch11, step893]: loss 0.589731
[epoch11, step894]: loss 0.364107
[epoch11, step895]: loss 0.486409
[epoch11, step896]: loss 0.625330
[epoch11, step897]: loss 0.663822
[epoch11, step898]: loss 0.658345
[epoch11, step899]: loss 0.554758
[epoch11, step900]: loss 0.594539
[epoch11, step901]: loss 0.658626
[epoch11, step902]: loss 0.338621
[epoch11, step903]: loss 0.523621
[epoch11, step904]: loss 0.279743
[epoch11, step905]: loss 0.444911
[epoch11, step906]: loss 0.654099
[epoch11, step907]: loss 0.377144
[epoch11, step908]: loss 0.479662
[epoch11, step909]: loss 0.376554
[epoch11, step910]: loss 0.509331
[epoch11, step911]: loss 0.599249
[epoch11, step912]: loss 0.483644
[epoch11, step913]: loss 0.464110
[epoch11, step914]: loss 0.548810
[epoch11, step915]: loss 0.411983
[epoch11, step916]: loss 0.527370
[epoch11, step917]: loss 0.412732
[epoch11, step918]: loss 0.630295
[epoch11, step919]: loss 0.655018
[epoch11, step920]: loss 0.472204
[epoch11, step921]: loss 0.670906
[epoch11, step922]: loss 0.502778
[epoch11, step923]: loss 0.537981
[epoch11, step924]: loss 0.473780
[epoch11, step925]: loss 0.424642
[epoch11, step926]: loss 0.569951
[epoch11, step927]: loss 0.369118
[epoch11, step928]: loss 0.449863
[epoch11, step929]: loss 0.724024
[epoch11, step930]: loss 0.512337
[epoch11, step931]: loss 0.652456
[epoch11, step932]: loss 0.671859
[epoch11, step933]: loss 0.616674
[epoch11, step934]: loss 0.519057
[epoch11, step935]: loss 0.535359
[epoch11, step936]: loss 0.511136
[epoch11, step937]: loss 0.594800
[epoch11, step938]: loss 0.499629
[epoch11, step939]: loss 0.563307
[epoch11, step940]: loss 0.432703
[epoch11, step941]: loss 0.641676
[epoch11, step942]: loss 0.389508
[epoch11, step943]: loss 0.315898
[epoch11, step944]: loss 0.527791
[epoch11, step945]: loss 0.587825
[epoch11, step946]: loss 0.534895
[epoch11, step947]: loss 0.505767
[epoch11, step948]: loss 0.481728
[epoch11, step949]: loss 0.722345
[epoch11, step950]: loss 0.587037
[epoch11, step951]: loss 0.597627
[epoch11, step952]: loss 0.652422
[epoch11, step953]: loss 0.497818
[epoch11, step954]: loss 0.434570
[epoch11, step955]: loss 0.777455
[epoch11, step956]: loss 0.576450
[epoch11, step957]: loss 0.301564
[epoch11, step958]: loss 0.577188
[epoch11, step959]: loss 0.353401
[epoch11, step960]: loss 0.521536
[epoch11, step961]: loss 0.584350
[epoch11, step962]: loss 0.343775
[epoch11, step963]: loss 0.760666
[epoch11, step964]: loss 0.355735
[epoch11, step965]: loss 0.583142
[epoch11, step966]: loss 0.572083
[epoch11, step967]: loss 0.332648
[epoch11, step968]: loss 0.517889
[epoch11, step969]: loss 0.636422
[epoch11, step970]: loss 0.346288
[epoch11, step971]: loss 0.590010
[epoch11, step972]: loss 0.580344
[epoch11, step973]: loss 0.414624
[epoch11, step974]: loss 0.529897
[epoch11, step975]: loss 0.652905
[epoch11, step976]: loss 0.619783
[epoch11, step977]: loss 0.518175
[epoch11, step978]: loss 0.539130
[epoch11, step979]: loss 0.556895
[epoch11, step980]: loss 0.402052
[epoch11, step981]: loss 0.687184
[epoch11, step982]: loss 0.528350
[epoch11, step983]: loss 0.219700
[epoch11, step984]: loss 0.562169
[epoch11, step985]: loss 0.166826
[epoch11, step986]: loss 0.433160
[epoch11, step987]: loss 0.502470
[epoch11, step988]: loss 0.627136
[epoch11, step989]: loss 0.305423
[epoch11, step990]: loss 0.515365
[epoch11, step991]: loss 0.456052
[epoch11, step992]: loss 0.471102
[epoch11, step993]: loss 0.763326
[epoch11, step994]: loss 0.416603
[epoch11, step995]: loss 0.504893
[epoch11, step996]: loss 0.545157
[epoch11, step997]: loss 0.267857
[epoch11, step998]: loss 0.655459
[epoch11, step999]: loss 0.683268
[epoch11, step1000]: loss 0.394361
[epoch11, step1001]: loss 0.520701
[epoch11, step1002]: loss 0.730464
[epoch11, step1003]: loss 0.525643
[epoch11, step1004]: loss 0.507926
[epoch11, step1005]: loss 0.408318
[epoch11, step1006]: loss 0.353857
[epoch11, step1007]: loss 0.429310
[epoch11, step1008]: loss 0.448450
[epoch11, step1009]: loss 0.609908
[epoch11, step1010]: loss 0.584032
[epoch11, step1011]: loss 0.426973
[epoch11, step1012]: loss 0.371604
[epoch11, step1013]: loss 0.569085
[epoch11, step1014]: loss 0.366271
[epoch11, step1015]: loss 0.284712
[epoch11, step1016]: loss 0.141279
[epoch11, step1017]: loss 0.539654
[epoch11, step1018]: loss 0.488944
[epoch11, step1019]: loss 0.393817
[epoch11, step1020]: loss 0.328142
[epoch11, step1021]: loss 0.707174
[epoch11, step1022]: loss 0.394565
[epoch11, step1023]: loss 0.508824
[epoch11, step1024]: loss 0.454419
[epoch11, step1025]: loss 0.325263
[epoch11, step1026]: loss 0.589343
[epoch11, step1027]: loss 0.396247
[epoch11, step1028]: loss 0.539812
[epoch11, step1029]: loss 0.608832
[epoch11, step1030]: loss 0.335346
[epoch11, step1031]: loss 0.460685
[epoch11, step1032]: loss 0.425480
[epoch11, step1033]: loss 0.416784
[epoch11, step1034]: loss 0.570190
[epoch11, step1035]: loss 0.658655
[epoch11, step1036]: loss 0.554218
[epoch11, step1037]: loss 0.515291
[epoch11, step1038]: loss 0.343814
[epoch11, step1039]: loss 0.570832
[epoch11, step1040]: loss 0.603894
[epoch11, step1041]: loss 0.130507
[epoch11, step1042]: loss 0.369623
[epoch11, step1043]: loss 0.371077
[epoch11, step1044]: loss 0.587975
[epoch11, step1045]: loss 0.344768
[epoch11, step1046]: loss 0.512176
[epoch11, step1047]: loss 0.303855
[epoch11, step1048]: loss 0.440003
[epoch11, step1049]: loss 0.525050
[epoch11, step1050]: loss 0.555220
[epoch11, step1051]: loss 0.475290
[epoch11, step1052]: loss 0.355075
[epoch11, step1053]: loss 0.272882
[epoch11, step1054]: loss 0.477848
[epoch11, step1055]: loss 0.614455
[epoch11, step1056]: loss 0.749596
[epoch11, step1057]: loss 0.118064
[epoch11, step1058]: loss 0.148880
[epoch11, step1059]: loss 0.486391
[epoch11, step1060]: loss 0.418329
[epoch11, step1061]: loss 0.590539
[epoch11, step1062]: loss 0.261673
[epoch11, step1063]: loss 0.575007
[epoch11, step1064]: loss 0.482371
[epoch11, step1065]: loss 0.552924
[epoch11, step1066]: loss 0.422686
[epoch11, step1067]: loss 0.460449
[epoch11, step1068]: loss 0.421844
[epoch11, step1069]: loss 0.413443
[epoch11, step1070]: loss 0.632260
[epoch11, step1071]: loss 0.535065
[epoch11, step1072]: loss 0.489899
[epoch11, step1073]: loss 0.575368
[epoch11, step1074]: loss 0.404851
[epoch11, step1075]: loss 0.446384
[epoch11, step1076]: loss 0.462672
[epoch11, step1077]: loss 0.632231
[epoch11, step1078]: loss 0.484252
[epoch11, step1079]: loss 0.471458
[epoch11, step1080]: loss 0.305572
[epoch11, step1081]: loss 0.526173
[epoch11, step1082]: loss 0.591462
[epoch11, step1083]: loss 0.384526
[epoch11, step1084]: loss 0.284286
[epoch11, step1085]: loss 0.558554
[epoch11, step1086]: loss 0.545452
[epoch11, step1087]: loss 0.606386
[epoch11, step1088]: loss 0.462957
[epoch11, step1089]: loss 0.487681
[epoch11, step1090]: loss 0.504845
[epoch11, step1091]: loss 0.723606
[epoch11, step1092]: loss 0.450152
[epoch11, step1093]: loss 0.441487
[epoch11, step1094]: loss 0.575817
[epoch11, step1095]: loss 0.302467
[epoch11, step1096]: loss 0.437620
[epoch11, step1097]: loss 0.590497
[epoch11, step1098]: loss 0.661300
[epoch11, step1099]: loss 0.520491
[epoch11, step1100]: loss 0.421244
[epoch11, step1101]: loss 0.577160
[epoch11, step1102]: loss 0.683743
[epoch11, step1103]: loss 0.342208
[epoch11, step1104]: loss 0.335177
[epoch11, step1105]: loss 0.643687
[epoch11, step1106]: loss 0.317009
[epoch11, step1107]: loss 0.723862
[epoch11, step1108]: loss 0.408086
[epoch11, step1109]: loss 0.499445
[epoch11, step1110]: loss 0.270530
[epoch11, step1111]: loss 0.600856
[epoch11, step1112]: loss 0.420212
[epoch11, step1113]: loss 0.629171
[epoch11, step1114]: loss 0.570996
[epoch11, step1115]: loss 0.298739
[epoch11, step1116]: loss 0.519378
[epoch11, step1117]: loss 0.724917
[epoch11, step1118]: loss 0.430550
[epoch11, step1119]: loss 0.532197
[epoch11, step1120]: loss 0.635569
[epoch11, step1121]: loss 0.470730
[epoch11, step1122]: loss 0.539601
[epoch11, step1123]: loss 0.467580
[epoch11, step1124]: loss 0.311968
[epoch11, step1125]: loss 0.503573
[epoch11, step1126]: loss 0.600690
[epoch11, step1127]: loss 0.501424
[epoch11, step1128]: loss 0.471272
[epoch11, step1129]: loss 0.614629
[epoch11, step1130]: loss 0.594066
[epoch11, step1131]: loss 0.330465
[epoch11, step1132]: loss 0.613000
[epoch11, step1133]: loss 0.544310
[epoch11, step1134]: loss 0.519563
[epoch11, step1135]: loss 0.597932
[epoch11, step1136]: loss 0.654677
[epoch11, step1137]: loss 0.607996
[epoch11, step1138]: loss 0.556455
[epoch11, step1139]: loss 0.447538
[epoch11, step1140]: loss 0.559443
[epoch11, step1141]: loss 0.386687
[epoch11, step1142]: loss 0.497334
[epoch11, step1143]: loss 0.381618
[epoch11, step1144]: loss 0.689481
[epoch11, step1145]: loss 0.562747
[epoch11, step1146]: loss 0.508241
[epoch11, step1147]: loss 0.303118
[epoch11, step1148]: loss 0.664328
[epoch11, step1149]: loss 0.392094
[epoch11, step1150]: loss 0.523605
[epoch11, step1151]: loss 0.513879
[epoch11, step1152]: loss 0.197077
[epoch11, step1153]: loss 0.751476
[epoch11, step1154]: loss 0.690637
[epoch11, step1155]: loss 0.325805
[epoch11, step1156]: loss 0.524391
[epoch11, step1157]: loss 0.614738
[epoch11, step1158]: loss 0.516812
[epoch11, step1159]: loss 0.581117
[epoch11, step1160]: loss 0.313062
[epoch11, step1161]: loss 0.603448
[epoch11, step1162]: loss 0.120496
[epoch11, step1163]: loss 0.555031
[epoch11, step1164]: loss 0.674576
[epoch11, step1165]: loss 0.569619
[epoch11, step1166]: loss 0.425977
[epoch11, step1167]: loss 0.636779
[epoch11, step1168]: loss 0.569443
[epoch11, step1169]: loss 0.358173
[epoch11, step1170]: loss 0.614558
[epoch11, step1171]: loss 0.619643
[epoch11, step1172]: loss 0.349037
[epoch11, step1173]: loss 0.565541
[epoch11, step1174]: loss 0.395038
[epoch11, step1175]: loss 0.357976
[epoch11, step1176]: loss 0.444636
[epoch11, step1177]: loss 0.173503
[epoch11, step1178]: loss 0.525944
[epoch11, step1179]: loss 0.499071
[epoch11, step1180]: loss 0.656171
[epoch11, step1181]: loss 0.522561
[epoch11, step1182]: loss 0.628261
[epoch11, step1183]: loss 0.509099
[epoch11, step1184]: loss 0.528887
[epoch11, step1185]: loss 0.482415
[epoch11, step1186]: loss 0.416020
[epoch11, step1187]: loss 0.480416
[epoch11, step1188]: loss 0.599342
[epoch11, step1189]: loss 0.470373
[epoch11, step1190]: loss 0.568617
[epoch11, step1191]: loss 0.354781
[epoch11, step1192]: loss 0.466675
[epoch11, step1193]: loss 0.485139
[epoch11, step1194]: loss 0.725317
[epoch11, step1195]: loss 0.421648
[epoch11, step1196]: loss 0.400290
[epoch11, step1197]: loss 0.308498
[epoch11, step1198]: loss 0.584491
[epoch11, step1199]: loss 0.513667
[epoch11, step1200]: loss 0.434365
[epoch11, step1201]: loss 0.351769
[epoch11, step1202]: loss 0.506259
[epoch11, step1203]: loss 0.484852
[epoch11, step1204]: loss 0.548158
[epoch11, step1205]: loss 0.692853
[epoch11, step1206]: loss 0.573528
[epoch11, step1207]: loss 0.140669
[epoch11, step1208]: loss 0.584059
[epoch11, step1209]: loss 0.323743
[epoch11, step1210]: loss 0.266242
[epoch11, step1211]: loss 0.568609
[epoch11, step1212]: loss 0.493230
[epoch11, step1213]: loss 0.536335
[epoch11, step1214]: loss 0.235829
[epoch11, step1215]: loss 0.387365
[epoch11, step1216]: loss 0.441376
[epoch11, step1217]: loss 0.380661
[epoch11, step1218]: loss 0.447616
[epoch11, step1219]: loss 0.700084
[epoch11, step1220]: loss 0.614341
[epoch11, step1221]: loss 0.245885
[epoch11, step1222]: loss 0.584435
[epoch11, step1223]: loss 0.569202
[epoch11, step1224]: loss 0.197801
[epoch11, step1225]: loss 0.382958
[epoch11, step1226]: loss 0.604227
[epoch11, step1227]: loss 0.520321
[epoch11, step1228]: loss 0.321661
[epoch11, step1229]: loss 0.278579
[epoch11, step1230]: loss 0.528105
[epoch11, step1231]: loss 0.522477
[epoch11, step1232]: loss 0.527638
[epoch11, step1233]: loss 0.312466
[epoch11, step1234]: loss 0.552919
[epoch11, step1235]: loss 0.624050
[epoch11, step1236]: loss 0.361519
[epoch11, step1237]: loss 0.393794
[epoch11, step1238]: loss 0.467710
[epoch11, step1239]: loss 0.553772
[epoch11, step1240]: loss 0.549116
[epoch11, step1241]: loss 0.323092
[epoch11, step1242]: loss 0.573915
[epoch11, step1243]: loss 0.359856
[epoch11, step1244]: loss 0.530137
[epoch11, step1245]: loss 0.436891
[epoch11, step1246]: loss 0.437465
[epoch11, step1247]: loss 0.549013
[epoch11, step1248]: loss 0.563475
[epoch11, step1249]: loss 0.441235
[epoch11, step1250]: loss 0.479091
[epoch11, step1251]: loss 0.319466
[epoch11, step1252]: loss 0.248515
[epoch11, step1253]: loss 0.459811
[epoch11, step1254]: loss 0.662594
[epoch11, step1255]: loss 0.534521
[epoch11, step1256]: loss 0.654546
[epoch11, step1257]: loss 0.455965
[epoch11, step1258]: loss 0.687917
[epoch11, step1259]: loss 0.457989
[epoch11, step1260]: loss 0.445042
[epoch11, step1261]: loss 0.619858
[epoch11, step1262]: loss 0.489697
[epoch11, step1263]: loss 0.575220
[epoch11, step1264]: loss 0.618282
[epoch11, step1265]: loss 0.574575
[epoch11, step1266]: loss 0.448004
[epoch11, step1267]: loss 0.562587
[epoch11, step1268]: loss 0.500879
[epoch11, step1269]: loss 0.790409
[epoch11, step1270]: loss 0.391162
[epoch11, step1271]: loss 0.483041
[epoch11, step1272]: loss 0.421448
[epoch11, step1273]: loss 0.506776
[epoch11, step1274]: loss 0.381679
[epoch11, step1275]: loss 0.478776
[epoch11, step1276]: loss 0.355164
[epoch11, step1277]: loss 0.549706
[epoch11, step1278]: loss 0.449894
[epoch11, step1279]: loss 0.657831
[epoch11, step1280]: loss 0.284918
[epoch11, step1281]: loss 0.315090
[epoch11, step1282]: loss 0.409278
[epoch11, step1283]: loss 0.463441
[epoch11, step1284]: loss 0.274165
[epoch11, step1285]: loss 0.329342
[epoch11, step1286]: loss 0.473810
[epoch11, step1287]: loss 0.435565
[epoch11, step1288]: loss 0.648070
[epoch11, step1289]: loss 0.323694
[epoch11, step1290]: loss 0.553216
[epoch11, step1291]: loss 0.385070
[epoch11, step1292]: loss 0.611890
[epoch11, step1293]: loss 0.161028
[epoch11, step1294]: loss 0.507714
[epoch11, step1295]: loss 0.386354
[epoch11, step1296]: loss 0.396830
[epoch11, step1297]: loss 0.493643
[epoch11, step1298]: loss 0.479478
[epoch11, step1299]: loss 0.424627
[epoch11, step1300]: loss 0.376508
[epoch11, step1301]: loss 0.543472
[epoch11, step1302]: loss 0.502929
[epoch11, step1303]: loss 0.410459
[epoch11, step1304]: loss 0.296770
[epoch11, step1305]: loss 0.459070
[epoch11, step1306]: loss 0.575564
[epoch11, step1307]: loss 0.630452
[epoch11, step1308]: loss 0.222822
[epoch11, step1309]: loss 0.302050
[epoch11, step1310]: loss 0.155781
[epoch11, step1311]: loss 0.610122
[epoch11, step1312]: loss 0.343644
[epoch11, step1313]: loss 0.727623
[epoch11, step1314]: loss 0.518934
[epoch11, step1315]: loss 0.547295
[epoch11, step1316]: loss 0.424670
[epoch11, step1317]: loss 0.220034
[epoch11, step1318]: loss 0.444162
[epoch11, step1319]: loss 0.562984
[epoch11, step1320]: loss 0.566885
[epoch11, step1321]: loss 0.403266
[epoch11, step1322]: loss 0.427048
[epoch11, step1323]: loss 0.349962
[epoch11, step1324]: loss 0.387272
[epoch11, step1325]: loss 0.605925
[epoch11, step1326]: loss 0.429549
[epoch11, step1327]: loss 0.543776
[epoch11, step1328]: loss 0.528603
[epoch11, step1329]: loss 0.579541
[epoch11, step1330]: loss 0.585922
[epoch11, step1331]: loss 0.432634
[epoch11, step1332]: loss 0.669790
[epoch11, step1333]: loss 0.739024
[epoch11, step1334]: loss 0.472927
[epoch11, step1335]: loss 0.602662
[epoch11, step1336]: loss 0.501431
[epoch11, step1337]: loss 0.589007
[epoch11, step1338]: loss 0.524503
[epoch11, step1339]: loss 0.482842
[epoch11, step1340]: loss 0.497405
[epoch11, step1341]: loss 0.543621
[epoch11, step1342]: loss 0.252825
[epoch11, step1343]: loss 0.424271
[epoch11, step1344]: loss 0.534468
[epoch11, step1345]: loss 0.410436
[epoch11, step1346]: loss 0.521248
[epoch11, step1347]: loss 0.556533
[epoch11, step1348]: loss 0.416221
[epoch11, step1349]: loss 0.279548
[epoch11, step1350]: loss 0.399266
[epoch11, step1351]: loss 0.637015
[epoch11, step1352]: loss 0.547931
[epoch11, step1353]: loss 0.759443
[epoch11, step1354]: loss 0.256218
[epoch11, step1355]: loss 0.682915
[epoch11, step1356]: loss 0.519018
[epoch11, step1357]: loss 0.454502
[epoch11, step1358]: loss 0.322856
[epoch11, step1359]: loss 0.462495
[epoch11, step1360]: loss 0.615134
[epoch11, step1361]: loss 0.606164
[epoch11, step1362]: loss 0.475789
[epoch11, step1363]: loss 0.623891
[epoch11, step1364]: loss 0.562284
[epoch11, step1365]: loss 0.606880
[epoch11, step1366]: loss 0.426088
[epoch11, step1367]: loss 0.604670
[epoch11, step1368]: loss 0.339768
[epoch11, step1369]: loss 0.591464
[epoch11, step1370]: loss 0.461823
[epoch11, step1371]: loss 0.475498
[epoch11, step1372]: loss 0.600921
[epoch11, step1373]: loss 0.306375
[epoch11, step1374]: loss 0.751549
[epoch11, step1375]: loss 0.503958
[epoch11, step1376]: loss 0.664192
[epoch11, step1377]: loss 0.510803
[epoch11, step1378]: loss 0.226177
[epoch11, step1379]: loss 0.574409
[epoch11, step1380]: loss 0.617010
[epoch11, step1381]: loss 0.281129
[epoch11, step1382]: loss 0.519477
[epoch11, step1383]: loss 0.406771
[epoch11, step1384]: loss 0.432898
[epoch11, step1385]: loss 0.373986
[epoch11, step1386]: loss 0.630705
[epoch11, step1387]: loss 0.452373
[epoch11, step1388]: loss 0.618885
[epoch11, step1389]: loss 0.430471
[epoch11, step1390]: loss 0.538259
[epoch11, step1391]: loss 0.764186
[epoch11, step1392]: loss 0.471022
[epoch11, step1393]: loss 0.614125
[epoch11, step1394]: loss 0.407841
[epoch11, step1395]: loss 0.438919
[epoch11, step1396]: loss 0.535067
[epoch11, step1397]: loss 0.321837
[epoch11, step1398]: loss 0.572113
[epoch11, step1399]: loss 0.503734
[epoch11, step1400]: loss 0.462376
[epoch11, step1401]: loss 0.380021
[epoch11, step1402]: loss 0.354272
[epoch11, step1403]: loss 0.635435
[epoch11, step1404]: loss 0.584005
[epoch11, step1405]: loss 0.365095
[epoch11, step1406]: loss 0.797747
[epoch11, step1407]: loss 0.502235
[epoch11, step1408]: loss 0.458553
[epoch11, step1409]: loss 0.648757
[epoch11, step1410]: loss 0.532505
[epoch11, step1411]: loss 0.376047
[epoch11, step1412]: loss 0.537973
[epoch11, step1413]: loss 0.389852
[epoch11, step1414]: loss 0.487496
[epoch11, step1415]: loss 0.616627
[epoch11, step1416]: loss 0.340289
[epoch11, step1417]: loss 0.531864
[epoch11, step1418]: loss 0.550354
[epoch11, step1419]: loss 0.683985
[epoch11, step1420]: loss 0.371596
[epoch11, step1421]: loss 0.485602
[epoch11, step1422]: loss 0.633655
[epoch11, step1423]: loss 0.493946
[epoch11, step1424]: loss 0.664577
[epoch11, step1425]: loss 0.524248
[epoch11, step1426]: loss 0.432493
[epoch11, step1427]: loss 0.389636
[epoch11, step1428]: loss 0.455521
[epoch11, step1429]: loss 0.347472
[epoch11, step1430]: loss 0.703579
[epoch11, step1431]: loss 0.549017
[epoch11, step1432]: loss 0.572781
[epoch11, step1433]: loss 0.474680
[epoch11, step1434]: loss 0.395822
[epoch11, step1435]: loss 0.292578
[epoch11, step1436]: loss 0.583641
[epoch11, step1437]: loss 0.258047
[epoch11, step1438]: loss 0.495567
[epoch11, step1439]: loss 0.578842
[epoch11, step1440]: loss 0.382440
[epoch11, step1441]: loss 0.598329
[epoch11, step1442]: loss 0.546251
[epoch11, step1443]: loss 0.721463
[epoch11, step1444]: loss 0.467027
[epoch11, step1445]: loss 0.655371
[epoch11, step1446]: loss 0.371867
[epoch11, step1447]: loss 0.513769
[epoch11, step1448]: loss 0.402622
[epoch11, step1449]: loss 0.703278
[epoch11, step1450]: loss 0.534736
[epoch11, step1451]: loss 0.472429
[epoch11, step1452]: loss 0.763394
[epoch11, step1453]: loss 0.294581
[epoch11, step1454]: loss 0.460926
[epoch11, step1455]: loss 0.359081
[epoch11, step1456]: loss 0.490771
[epoch11, step1457]: loss 0.628619
[epoch11, step1458]: loss 0.410853
[epoch11, step1459]: loss 0.454586
[epoch11, step1460]: loss 0.574282
[epoch11, step1461]: loss 0.673225
[epoch11, step1462]: loss 0.244499
[epoch11, step1463]: loss 0.628552
[epoch11, step1464]: loss 0.291357
[epoch11, step1465]: loss 0.566265
[epoch11, step1466]: loss 0.373838
[epoch11, step1467]: loss 0.244340
[epoch11, step1468]: loss 0.740686
[epoch11, step1469]: loss 0.429403
[epoch11, step1470]: loss 0.497553
[epoch11, step1471]: loss 0.350516
[epoch11, step1472]: loss 0.459309
[epoch11, step1473]: loss 0.433926
[epoch11, step1474]: loss 0.419525
[epoch11, step1475]: loss 0.595343
[epoch11, step1476]: loss 0.522233
[epoch11, step1477]: loss 0.215569
[epoch11, step1478]: loss 0.639554
[epoch11, step1479]: loss 0.560464
[epoch11, step1480]: loss 0.268388
[epoch11, step1481]: loss 0.513875
[epoch11, step1482]: loss 0.434874
[epoch11, step1483]: loss 0.631805
[epoch11, step1484]: loss 0.461894
[epoch11, step1485]: loss 0.590843
[epoch11, step1486]: loss 0.382567
[epoch11, step1487]: loss 0.524174
[epoch11, step1488]: loss 0.524835
[epoch11, step1489]: loss 0.543369
[epoch11, step1490]: loss 0.615381
[epoch11, step1491]: loss 0.210521
[epoch11, step1492]: loss 0.388276
[epoch11, step1493]: loss 0.372288
[epoch11, step1494]: loss 0.477994
[epoch11, step1495]: loss 0.696890
[epoch11, step1496]: loss 0.464221
[epoch11, step1497]: loss 0.304712
[epoch11, step1498]: loss 0.345436
[epoch11, step1499]: loss 0.574167
[epoch11, step1500]: loss 0.428650
[epoch11, step1501]: loss 0.525749
[epoch11, step1502]: loss 0.626270
[epoch11, step1503]: loss 0.441757
[epoch11, step1504]: loss 0.619882
[epoch11, step1505]: loss 0.536225
[epoch11, step1506]: loss 0.739628
[epoch11, step1507]: loss 0.516719
[epoch11, step1508]: loss 0.468329
[epoch11, step1509]: loss 0.455612
[epoch11, step1510]: loss 0.325005
[epoch11, step1511]: loss 0.593769
[epoch11, step1512]: loss 0.453970
[epoch11, step1513]: loss 0.273900
[epoch11, step1514]: loss 0.753423
[epoch11, step1515]: loss 0.272397
[epoch11, step1516]: loss 0.560232
[epoch11, step1517]: loss 0.479570
[epoch11, step1518]: loss 0.276174
[epoch11, step1519]: loss 0.426886
[epoch11, step1520]: loss 0.630976
[epoch11, step1521]: loss 0.491815
[epoch11, step1522]: loss 0.542986
[epoch11, step1523]: loss 0.555122
[epoch11, step1524]: loss 0.460613
[epoch11, step1525]: loss 0.668317
[epoch11, step1526]: loss 0.513254
[epoch11, step1527]: loss 0.510366
[epoch11, step1528]: loss 0.413022
[epoch11, step1529]: loss 0.608527
[epoch11, step1530]: loss 0.631063
[epoch11, step1531]: loss 0.361869
[epoch11, step1532]: loss 0.527670
[epoch11, step1533]: loss 0.470398
[epoch11, step1534]: loss 0.425512
[epoch11, step1535]: loss 0.371793
[epoch11, step1536]: loss 0.344185
[epoch11, step1537]: loss 0.352235
[epoch11, step1538]: loss 0.269371
[epoch11, step1539]: loss 0.625610
[epoch11, step1540]: loss 0.226673
[epoch11, step1541]: loss 0.638492
[epoch11, step1542]: loss 0.377568
[epoch11, step1543]: loss 0.314711
[epoch11, step1544]: loss 0.625933
[epoch11, step1545]: loss 0.696146
[epoch11, step1546]: loss 0.292854
[epoch11, step1547]: loss 0.626517
[epoch11, step1548]: loss 0.466064
[epoch11, step1549]: loss 0.588582
[epoch11, step1550]: loss 0.508662
[epoch11, step1551]: loss 0.619316
[epoch11, step1552]: loss 0.490307
[epoch11, step1553]: loss 0.412732
[epoch11, step1554]: loss 0.495126
[epoch11, step1555]: loss 0.415308
[epoch11, step1556]: loss 0.513106
[epoch11, step1557]: loss 0.346397
[epoch11, step1558]: loss 0.419469
[epoch11, step1559]: loss 0.528613
[epoch11, step1560]: loss 0.569022
[epoch11, step1561]: loss 0.489397
[epoch11, step1562]: loss 0.454324
[epoch11, step1563]: loss 0.312606
[epoch11, step1564]: loss 0.442495
[epoch11, step1565]: loss 0.557572
[epoch11, step1566]: loss 0.507585
[epoch11, step1567]: loss 0.590680
[epoch11, step1568]: loss 0.320674
[epoch11, step1569]: loss 0.309049
[epoch11, step1570]: loss 0.562625
[epoch11, step1571]: loss 0.466128
[epoch11, step1572]: loss 0.434755
[epoch11, step1573]: loss 0.593755
[epoch11, step1574]: loss 0.465615
[epoch11, step1575]: loss 0.467272
[epoch11, step1576]: loss 0.402351
[epoch11, step1577]: loss 0.706846
[epoch11, step1578]: loss 0.591915
[epoch11, step1579]: loss 0.508020
[epoch11, step1580]: loss 0.584381
[epoch11, step1581]: loss 0.332824
[epoch11, step1582]: loss 0.424981
[epoch11, step1583]: loss 0.481888
[epoch11, step1584]: loss 0.472437
[epoch11, step1585]: loss 0.401948
[epoch11, step1586]: loss 0.526811
[epoch11, step1587]: loss 0.497355
[epoch11, step1588]: loss 0.611243
[epoch11, step1589]: loss 0.453129
[epoch11, step1590]: loss 0.529922
[epoch11, step1591]: loss 0.795888
[epoch11, step1592]: loss 0.474485
[epoch11, step1593]: loss 0.747798
[epoch11, step1594]: loss 0.324542
[epoch11, step1595]: loss 0.589373
[epoch11, step1596]: loss 0.345843
[epoch11, step1597]: loss 0.624574
[epoch11, step1598]: loss 0.357298
[epoch11, step1599]: loss 0.457928
[epoch11, step1600]: loss 0.582501
[epoch11, step1601]: loss 0.502473
[epoch11, step1602]: loss 0.599678
[epoch11, step1603]: loss 0.370987
[epoch11, step1604]: loss 0.417450
[epoch11, step1605]: loss 0.493658
[epoch11, step1606]: loss 0.150535
[epoch11, step1607]: loss 0.488033
[epoch11, step1608]: loss 0.390402
[epoch11, step1609]: loss 0.691686
[epoch11, step1610]: loss 0.282025
[epoch11, step1611]: loss 0.543199
[epoch11, step1612]: loss 0.447387
[epoch11, step1613]: loss 0.386307
[epoch11, step1614]: loss 0.541201
[epoch11, step1615]: loss 0.438958
[epoch11, step1616]: loss 0.376653
[epoch11, step1617]: loss 0.422766
[epoch11, step1618]: loss 0.677667
[epoch11, step1619]: loss 0.360877
[epoch11, step1620]: loss 0.576438
[epoch11, step1621]: loss 0.352442
[epoch11, step1622]: loss 0.476456
[epoch11, step1623]: loss 0.411651
[epoch11, step1624]: loss 0.506020
[epoch11, step1625]: loss 0.596365
[epoch11, step1626]: loss 0.416965
[epoch11, step1627]: loss 0.590654
[epoch11, step1628]: loss 0.483574
[epoch11, step1629]: loss 0.615566
[epoch11, step1630]: loss 0.315225
[epoch11, step1631]: loss 0.611449
[epoch11, step1632]: loss 0.257202
[epoch11, step1633]: loss 0.100314
[epoch11, step1634]: loss 0.603674
[epoch11, step1635]: loss 0.561513
[epoch11, step1636]: loss 0.221366
[epoch11, step1637]: loss 0.632676
[epoch11, step1638]: loss 0.637443
[epoch11, step1639]: loss 0.355201
[epoch11, step1640]: loss 0.464272
[epoch11, step1641]: loss 0.483795
[epoch11, step1642]: loss 0.280860
[epoch11, step1643]: loss 0.471406
[epoch11, step1644]: loss 0.557497
[epoch11, step1645]: loss 0.660652
[epoch11, step1646]: loss 0.478252
[epoch11, step1647]: loss 0.359402
[epoch11, step1648]: loss 0.690133
[epoch11, step1649]: loss 0.349288
[epoch11, step1650]: loss 0.754231
[epoch11, step1651]: loss 0.529423
[epoch11, step1652]: loss 0.621027
[epoch11, step1653]: loss 0.703820
[epoch11, step1654]: loss 0.490960
[epoch11, step1655]: loss 0.529287
[epoch11, step1656]: loss 0.494660
[epoch11, step1657]: loss 0.461660
[epoch11, step1658]: loss 0.479488
[epoch11, step1659]: loss 0.723590
[epoch11, step1660]: loss 0.579747
[epoch11, step1661]: loss 0.714978
[epoch11, step1662]: loss 0.505154
[epoch11, step1663]: loss 0.429537
[epoch11, step1664]: loss 0.245061
[epoch11, step1665]: loss 0.451995
[epoch11, step1666]: loss 0.553055
[epoch11, step1667]: loss 0.273316
[epoch11, step1668]: loss 0.591044
[epoch11, step1669]: loss 0.544357
[epoch11, step1670]: loss 0.432897
[epoch11, step1671]: loss 0.440437
[epoch11, step1672]: loss 0.703227
[epoch11, step1673]: loss 0.697044
[epoch11, step1674]: loss 0.556646
[epoch11, step1675]: loss 0.357912
[epoch11, step1676]: loss 0.471355
[epoch11, step1677]: loss 0.563281
[epoch11, step1678]: loss 0.434160
[epoch11, step1679]: loss 0.411442
[epoch11, step1680]: loss 0.588461
[epoch11, step1681]: loss 0.542259
[epoch11, step1682]: loss 0.370281
[epoch11, step1683]: loss 0.372479
[epoch11, step1684]: loss 0.445350
[epoch11, step1685]: loss 0.126983
[epoch11, step1686]: loss 0.323206
[epoch11, step1687]: loss 0.541212
[epoch11, step1688]: loss 0.508107
[epoch11, step1689]: loss 0.473851
[epoch11, step1690]: loss 0.463329
[epoch11, step1691]: loss 0.614451
[epoch11, step1692]: loss 0.485679
[epoch11, step1693]: loss 0.659011
[epoch11, step1694]: loss 0.592109
[epoch11, step1695]: loss 0.498298
[epoch11, step1696]: loss 0.657517
[epoch11, step1697]: loss 0.624903
[epoch11, step1698]: loss 0.161890
[epoch11, step1699]: loss 0.414615
[epoch11, step1700]: loss 0.602567
[epoch11, step1701]: loss 0.743972
[epoch11, step1702]: loss 0.413112
[epoch11, step1703]: loss 0.418951
[epoch11, step1704]: loss 0.397785
[epoch11, step1705]: loss 0.501366
[epoch11, step1706]: loss 0.531836
[epoch11, step1707]: loss 0.407492
[epoch11, step1708]: loss 0.721413
[epoch11, step1709]: loss 0.539550
[epoch11, step1710]: loss 0.572520
[epoch11, step1711]: loss 0.497602
[epoch11, step1712]: loss 0.541804
[epoch11, step1713]: loss 0.641786
[epoch11, step1714]: loss 0.482664
[epoch11, step1715]: loss 0.239998
[epoch11, step1716]: loss 0.560974
[epoch11, step1717]: loss 0.551290
[epoch11, step1718]: loss 0.567028
[epoch11, step1719]: loss 0.523581
[epoch11, step1720]: loss 0.510826
[epoch11, step1721]: loss 0.299569
[epoch11, step1722]: loss 0.287729
[epoch11, step1723]: loss 0.368949
[epoch11, step1724]: loss 0.661377
[epoch11, step1725]: loss 0.486770
[epoch11, step1726]: loss 0.556389
[epoch11, step1727]: loss 0.385756
[epoch11, step1728]: loss 0.527660
[epoch11, step1729]: loss 0.630945
[epoch11, step1730]: loss 0.440055
[epoch11, step1731]: loss 0.540268
[epoch11, step1732]: loss 0.534918
[epoch11, step1733]: loss 0.511687
[epoch11, step1734]: loss 0.584427
[epoch11, step1735]: loss 0.346083
[epoch11, step1736]: loss 0.559253
[epoch11, step1737]: loss 0.612755
[epoch11, step1738]: loss 0.558815
[epoch11, step1739]: loss 0.743232
[epoch11, step1740]: loss 0.435530
[epoch11, step1741]: loss 0.819817
[epoch11, step1742]: loss 0.415899
[epoch11, step1743]: loss 0.577396
[epoch11, step1744]: loss 0.541723
[epoch11, step1745]: loss 0.499377
[epoch11, step1746]: loss 0.235046
[epoch11, step1747]: loss 0.551655
[epoch11, step1748]: loss 0.533774
[epoch11, step1749]: loss 0.521003
[epoch11, step1750]: loss 0.663909
[epoch11, step1751]: loss 0.435956
[epoch11, step1752]: loss 0.326675
[epoch11, step1753]: loss 0.551052
[epoch11, step1754]: loss 0.763125
[epoch11, step1755]: loss 0.312977
[epoch11, step1756]: loss 0.536679
[epoch11, step1757]: loss 0.546170
[epoch11, step1758]: loss 0.439906
[epoch11, step1759]: loss 0.480345
[epoch11, step1760]: loss 0.500188
[epoch11, step1761]: loss 0.426460
[epoch11, step1762]: loss 0.370396
[epoch11, step1763]: loss 0.522605
[epoch11, step1764]: loss 0.299623
[epoch11, step1765]: loss 0.516715
[epoch11, step1766]: loss 0.437324
[epoch11, step1767]: loss 0.513753
[epoch11, step1768]: loss 0.696435
[epoch11, step1769]: loss 0.690478
[epoch11, step1770]: loss 0.572243
[epoch11, step1771]: loss 0.503493
[epoch11, step1772]: loss 0.235497
[epoch11, step1773]: loss 0.142125
[epoch11, step1774]: loss 0.449537
[epoch11, step1775]: loss 0.524050
[epoch11, step1776]: loss 0.417829
[epoch11, step1777]: loss 0.556618
[epoch11, step1778]: loss 0.407122
[epoch11, step1779]: loss 0.337072
[epoch11, step1780]: loss 0.424301
[epoch11, step1781]: loss 0.471514
[epoch11, step1782]: loss 0.538904
[epoch11, step1783]: loss 0.477238
[epoch11, step1784]: loss 0.687425
[epoch11, step1785]: loss 0.439193
[epoch11, step1786]: loss 0.603345
[epoch11, step1787]: loss 0.290931
[epoch11, step1788]: loss 0.512495
[epoch11, step1789]: loss 0.252586
[epoch11, step1790]: loss 0.674871
[epoch11, step1791]: loss 0.644960
[epoch11, step1792]: loss 0.687021
[epoch11, step1793]: loss 0.612782
[epoch11, step1794]: loss 0.330920
[epoch11, step1795]: loss 0.202143
[epoch11, step1796]: loss 0.356647
[epoch11, step1797]: loss 0.370377
[epoch11, step1798]: loss 0.563759
[epoch11, step1799]: loss 0.590673
[epoch11, step1800]: loss 0.464497
[epoch11, step1801]: loss 0.543821
[epoch11, step1802]: loss 0.556949
[epoch11, step1803]: loss 0.582254
[epoch11, step1804]: loss 0.546921
[epoch11, step1805]: loss 0.506991
[epoch11, step1806]: loss 0.556568
[epoch11, step1807]: loss 0.349454
[epoch11, step1808]: loss 0.243943
[epoch11, step1809]: loss 0.552915
[epoch11, step1810]: loss 0.599540
[epoch11, step1811]: loss 0.499669
[epoch11, step1812]: loss 0.485601
[epoch11, step1813]: loss 0.595443
[epoch11, step1814]: loss 0.402110
[epoch11, step1815]: loss 0.543220
[epoch11, step1816]: loss 0.540697
[epoch11, step1817]: loss 0.163713
[epoch11, step1818]: loss 0.569028
[epoch11, step1819]: loss 0.158616
[epoch11, step1820]: loss 0.487266
[epoch11, step1821]: loss 0.740317
[epoch11, step1822]: loss 0.600830
[epoch11, step1823]: loss 0.572204
[epoch11, step1824]: loss 0.360033
[epoch11, step1825]: loss 0.653352
[epoch11, step1826]: loss 0.687755
[epoch11, step1827]: loss 0.376338
[epoch11, step1828]: loss 0.294093
[epoch11, step1829]: loss 0.554802
[epoch11, step1830]: loss 0.323608
[epoch11, step1831]: loss 0.416267
[epoch11, step1832]: loss 0.649478
[epoch11, step1833]: loss 0.367266
[epoch11, step1834]: loss 0.410389
[epoch11, step1835]: loss 0.335105
[epoch11, step1836]: loss 0.258518
[epoch11, step1837]: loss 0.396434
[epoch11, step1838]: loss 0.261094
[epoch11, step1839]: loss 0.418431
[epoch11, step1840]: loss 0.478793
[epoch11, step1841]: loss 0.605963
[epoch11, step1842]: loss 0.293845
[epoch11, step1843]: loss 0.574859
[epoch11, step1844]: loss 0.471163
[epoch11, step1845]: loss 0.369053
[epoch11, step1846]: loss 0.573837
[epoch11, step1847]: loss 0.487835
[epoch11, step1848]: loss 0.520177
[epoch11, step1849]: loss 0.324518
[epoch11, step1850]: loss 0.345881
[epoch11, step1851]: loss 0.673123
[epoch11, step1852]: loss 0.639840
[epoch11, step1853]: loss 0.364548
[epoch11, step1854]: loss 0.433553
[epoch11, step1855]: loss 0.453019
[epoch11, step1856]: loss 0.557442
[epoch11, step1857]: loss 0.439406
[epoch11, step1858]: loss 0.515883
[epoch11, step1859]: loss 0.473622
[epoch11, step1860]: loss 0.565376
[epoch11, step1861]: loss 0.558230
[epoch11, step1862]: loss 0.499416
[epoch11, step1863]: loss 0.512121
[epoch11, step1864]: loss 0.551994
[epoch11, step1865]: loss 0.484182
[epoch11, step1866]: loss 0.548655
[epoch11, step1867]: loss 0.487257
[epoch11, step1868]: loss 0.673509
[epoch11, step1869]: loss 0.477943
[epoch11, step1870]: loss 0.363536
[epoch11, step1871]: loss 0.463059
[epoch11, step1872]: loss 0.645272
[epoch11, step1873]: loss 0.178233
[epoch11, step1874]: loss 0.609979
[epoch11, step1875]: loss 0.588521
[epoch11, step1876]: loss 0.594446
[epoch11, step1877]: loss 0.172577
[epoch11, step1878]: loss 0.580981
[epoch11, step1879]: loss 0.423629
[epoch11, step1880]: loss 0.415071
[epoch11, step1881]: loss 0.356664
[epoch11, step1882]: loss 0.501695
[epoch11, step1883]: loss 0.306847
[epoch11, step1884]: loss 0.142484
[epoch11, step1885]: loss 0.538540
[epoch11, step1886]: loss 0.671008
[epoch11, step1887]: loss 0.531966
[epoch11, step1888]: loss 0.333668
[epoch11, step1889]: loss 0.410575
[epoch11, step1890]: loss 0.480375
[epoch11, step1891]: loss 0.380777
[epoch11, step1892]: loss 0.531742
[epoch11, step1893]: loss 0.535977
[epoch11, step1894]: loss 0.302148
[epoch11, step1895]: loss 0.605449
[epoch11, step1896]: loss 0.675338
[epoch11, step1897]: loss 0.417365
[epoch11, step1898]: loss 0.646314
[epoch11, step1899]: loss 0.368147
[epoch11, step1900]: loss 0.765530
[epoch11, step1901]: loss 0.451115
[epoch11, step1902]: loss 0.740375
[epoch11, step1903]: loss 0.435430
[epoch11, step1904]: loss 0.494199
[epoch11, step1905]: loss 0.503777
[epoch11, step1906]: loss 0.580972
[epoch11, step1907]: loss 0.437973
[epoch11, step1908]: loss 0.481775
[epoch11, step1909]: loss 0.491622
[epoch11, step1910]: loss 0.418446
[epoch11, step1911]: loss 0.501641
[epoch11, step1912]: loss 0.359128
[epoch11, step1913]: loss 0.387850
[epoch11, step1914]: loss 0.525017
[epoch11, step1915]: loss 0.227792
[epoch11, step1916]: loss 0.556926
[epoch11, step1917]: loss 0.456997
[epoch11, step1918]: loss 0.340404
[epoch11, step1919]: loss 0.484160
[epoch11, step1920]: loss 0.496565
[epoch11, step1921]: loss 0.634592
[epoch11, step1922]: loss 0.444602
[epoch11, step1923]: loss 0.530864
[epoch11, step1924]: loss 0.520371
[epoch11, step1925]: loss 0.347302
[epoch11, step1926]: loss 0.510845
[epoch11, step1927]: loss 0.431944
[epoch11, step1928]: loss 0.470474
[epoch11, step1929]: loss 0.504024
[epoch11, step1930]: loss 0.553244
[epoch11, step1931]: loss 0.274493
[epoch11, step1932]: loss 0.641232
[epoch11, step1933]: loss 0.309209
[epoch11, step1934]: loss 0.540165
[epoch11, step1935]: loss 0.571105
[epoch11, step1936]: loss 0.342320
[epoch11, step1937]: loss 0.638167
[epoch11, step1938]: loss 0.491418
[epoch11, step1939]: loss 0.380387
[epoch11, step1940]: loss 0.441217
[epoch11, step1941]: loss 0.692828
[epoch11, step1942]: loss 0.352301
[epoch11, step1943]: loss 0.558029
[epoch11, step1944]: loss 0.517974
[epoch11, step1945]: loss 0.516356
[epoch11, step1946]: loss 0.554471
[epoch11, step1947]: loss 0.447594
[epoch11, step1948]: loss 0.176741
[epoch11, step1949]: loss 0.419066
[epoch11, step1950]: loss 0.741441
[epoch11, step1951]: loss 0.520052
[epoch11, step1952]: loss 0.670451
[epoch11, step1953]: loss 0.606900
[epoch11, step1954]: loss 0.601458
[epoch11, step1955]: loss 0.528303
[epoch11, step1956]: loss 0.655430
[epoch11, step1957]: loss 0.452415
[epoch11, step1958]: loss 0.259284
[epoch11, step1959]: loss 0.550215
[epoch11, step1960]: loss 0.344935
[epoch11, step1961]: loss 0.650436
[epoch11, step1962]: loss 0.252688
[epoch11, step1963]: loss 0.146436
[epoch11, step1964]: loss 0.647493
[epoch11, step1965]: loss 0.434763
[epoch11, step1966]: loss 0.593607
[epoch11, step1967]: loss 0.532496
[epoch11, step1968]: loss 0.628418
[epoch11, step1969]: loss 0.322820
[epoch11, step1970]: loss 0.411388
[epoch11, step1971]: loss 0.455551
[epoch11, step1972]: loss 0.452040
[epoch11, step1973]: loss 0.634253
[epoch11, step1974]: loss 0.439193
[epoch11, step1975]: loss 0.480192
[epoch11, step1976]: loss 0.405016
[epoch11, step1977]: loss 0.561449
[epoch11, step1978]: loss 0.431825
[epoch11, step1979]: loss 0.287724
[epoch11, step1980]: loss 0.487632
[epoch11, step1981]: loss 0.376813
[epoch11, step1982]: loss 0.498605
[epoch11, step1983]: loss 0.594896
[epoch11, step1984]: loss 0.624439
[epoch11, step1985]: loss 0.478856
[epoch11, step1986]: loss 0.480605
[epoch11, step1987]: loss 0.382211
[epoch11, step1988]: loss 0.570520
[epoch11, step1989]: loss 0.489605
[epoch11, step1990]: loss 0.314671
[epoch11, step1991]: loss 0.643887
[epoch11, step1992]: loss 0.579177
[epoch11, step1993]: loss 0.582185
[epoch11, step1994]: loss 0.616532
[epoch11, step1995]: loss 0.657693
[epoch11, step1996]: loss 0.491867
[epoch11, step1997]: loss 0.605888
[epoch11, step1998]: loss 0.538731
[epoch11, step1999]: loss 0.774164
[epoch11, step2000]: loss 0.394518
[epoch11, step2001]: loss 0.522066
[epoch11, step2002]: loss 0.409335
[epoch11, step2003]: loss 0.506680
[epoch11, step2004]: loss 0.581283
[epoch11, step2005]: loss 0.592552
[epoch11, step2006]: loss 0.415265
[epoch11, step2007]: loss 0.548730
[epoch11, step2008]: loss 0.339365
[epoch11, step2009]: loss 0.281901
[epoch11, step2010]: loss 0.379380
[epoch11, step2011]: loss 0.378438
[epoch11, step2012]: loss 0.715106
[epoch11, step2013]: loss 0.536194
[epoch11, step2014]: loss 0.364620
[epoch11, step2015]: loss 0.469857
[epoch11, step2016]: loss 0.461906
[epoch11, step2017]: loss 0.414065
[epoch11, step2018]: loss 0.580723
[epoch11, step2019]: loss 0.586498
[epoch11, step2020]: loss 0.642268
[epoch11, step2021]: loss 0.434391
[epoch11, step2022]: loss 0.526481
[epoch11, step2023]: loss 0.662231
[epoch11, step2024]: loss 0.494076
[epoch11, step2025]: loss 0.262592
[epoch11, step2026]: loss 0.366495
[epoch11, step2027]: loss 0.579864
[epoch11, step2028]: loss 0.780279
[epoch11, step2029]: loss 0.348153
[epoch11, step2030]: loss 0.411164
[epoch11, step2031]: loss 0.405292
[epoch11, step2032]: loss 0.526431
[epoch11, step2033]: loss 0.277974
[epoch11, step2034]: loss 0.231977
[epoch11, step2035]: loss 0.456994
[epoch11, step2036]: loss 0.393966
[epoch11, step2037]: loss 0.229730
[epoch11, step2038]: loss 0.442681
[epoch11, step2039]: loss 0.618241
[epoch11, step2040]: loss 0.664330
[epoch11, step2041]: loss 0.492301
[epoch11, step2042]: loss 0.580523
[epoch11, step2043]: loss 0.624985
[epoch11, step2044]: loss 0.570631
[epoch11, step2045]: loss 0.600679
[epoch11, step2046]: loss 0.470698
[epoch11, step2047]: loss 0.518467
[epoch11, step2048]: loss 0.546268
[epoch11, step2049]: loss 0.543068
[epoch11, step2050]: loss 0.487528
[epoch11, step2051]: loss 0.346013
[epoch11, step2052]: loss 0.281120
[epoch11, step2053]: loss 0.649033
[epoch11, step2054]: loss 0.605350
[epoch11, step2055]: loss 0.410801
[epoch11, step2056]: loss 0.528669
[epoch11, step2057]: loss 0.604500
[epoch11, step2058]: loss 0.525398
[epoch11, step2059]: loss 0.584379
[epoch11, step2060]: loss 0.639368
[epoch11, step2061]: loss 0.606655
[epoch11, step2062]: loss 0.716408
[epoch11, step2063]: loss 0.355679
[epoch11, step2064]: loss 0.542915
[epoch11, step2065]: loss 0.706185
[epoch11, step2066]: loss 0.574834
[epoch11, step2067]: loss 0.383283
[epoch11, step2068]: loss 0.474113
[epoch11, step2069]: loss 0.370529
[epoch11, step2070]: loss 0.439867
[epoch11, step2071]: loss 0.491839
[epoch11, step2072]: loss 0.584710
[epoch11, step2073]: loss 0.447586
[epoch11, step2074]: loss 0.370685
[epoch11, step2075]: loss 0.564238
[epoch11, step2076]: loss 0.427361
[epoch11, step2077]: loss 0.198049
[epoch11, step2078]: loss 0.434104
[epoch11, step2079]: loss 0.629246
[epoch11, step2080]: loss 0.605762
[epoch11, step2081]: loss 0.510548
[epoch11, step2082]: loss 0.690287
[epoch11, step2083]: loss 0.509746
[epoch11, step2084]: loss 0.377804
[epoch11, step2085]: loss 0.497161
[epoch11, step2086]: loss 0.429812
[epoch11, step2087]: loss 0.445302
[epoch11, step2088]: loss 0.200662
[epoch11, step2089]: loss 0.615620
[epoch11, step2090]: loss 0.478923
[epoch11, step2091]: loss 0.684604
[epoch11, step2092]: loss 0.245305
[epoch11, step2093]: loss 0.534047
[epoch11, step2094]: loss 0.732728
[epoch11, step2095]: loss 0.352854
[epoch11, step2096]: loss 0.403457
[epoch11, step2097]: loss 0.252960
[epoch11, step2098]: loss 0.391465
[epoch11, step2099]: loss 0.489648
[epoch11, step2100]: loss 0.654796
[epoch11, step2101]: loss 0.272654
[epoch11, step2102]: loss 0.379525
[epoch11, step2103]: loss 0.451734
[epoch11, step2104]: loss 0.256609
[epoch11, step2105]: loss 0.266574
[epoch11, step2106]: loss 0.439734
[epoch11, step2107]: loss 0.480509
[epoch11, step2108]: loss 0.579571
[epoch11, step2109]: loss 0.359573
[epoch11, step2110]: loss 0.505090
[epoch11, step2111]: loss 0.538622
[epoch11, step2112]: loss 0.453584
[epoch11, step2113]: loss 0.661486
[epoch11, step2114]: loss 0.356064
[epoch11, step2115]: loss 0.444572
[epoch11, step2116]: loss 0.459510
[epoch11, step2117]: loss 0.361698
[epoch11, step2118]: loss 0.283923
[epoch11, step2119]: loss 0.570033
[epoch11, step2120]: loss 0.565548
[epoch11, step2121]: loss 0.615348
[epoch11, step2122]: loss 0.466704
[epoch11, step2123]: loss 0.662151
[epoch11, step2124]: loss 0.409306
[epoch11, step2125]: loss 0.288261
[epoch11, step2126]: loss 0.457485
[epoch11, step2127]: loss 0.424903
[epoch11, step2128]: loss 0.349874
[epoch11, step2129]: loss 0.564871
[epoch11, step2130]: loss 0.398882
[epoch11, step2131]: loss 0.579059
[epoch11, step2132]: loss 0.401305
[epoch11, step2133]: loss 0.635721
[epoch11, step2134]: loss 0.334608
[epoch11, step2135]: loss 0.415465
[epoch11, step2136]: loss 0.534961
[epoch11, step2137]: loss 0.397227
[epoch11, step2138]: loss 0.356861
[epoch11, step2139]: loss 0.500484
[epoch11, step2140]: loss 0.249275
[epoch11, step2141]: loss 0.443906
[epoch11, step2142]: loss 0.737468
[epoch11, step2143]: loss 0.472466
[epoch11, step2144]: loss 0.543189
[epoch11, step2145]: loss 0.471612
[epoch11, step2146]: loss 0.593583
[epoch11, step2147]: loss 0.711770
[epoch11, step2148]: loss 0.486833
[epoch11, step2149]: loss 0.593899
[epoch11, step2150]: loss 0.382906
[epoch11, step2151]: loss 0.636080
[epoch11, step2152]: loss 0.760100
[epoch11, step2153]: loss 0.509085
[epoch11, step2154]: loss 0.644796
[epoch11, step2155]: loss 0.390883
[epoch11, step2156]: loss 0.400392
[epoch11, step2157]: loss 0.384462
[epoch11, step2158]: loss 0.392166
[epoch11, step2159]: loss 0.523051
[epoch11, step2160]: loss 0.467063
[epoch11, step2161]: loss 0.747569
[epoch11, step2162]: loss 0.356190
[epoch11, step2163]: loss 0.236845
[epoch11, step2164]: loss 0.578008
[epoch11, step2165]: loss 0.692728
[epoch11, step2166]: loss 0.185155
[epoch11, step2167]: loss 0.479457
[epoch11, step2168]: loss 0.666306
[epoch11, step2169]: loss 0.313956
[epoch11, step2170]: loss 0.459577
[epoch11, step2171]: loss 0.703251
[epoch11, step2172]: loss 0.623836
[epoch11, step2173]: loss 0.529679
[epoch11, step2174]: loss 0.368887
[epoch11, step2175]: loss 0.328252
[epoch11, step2176]: loss 0.347303
[epoch11, step2177]: loss 0.418952
[epoch11, step2178]: loss 0.655978
[epoch11, step2179]: loss 0.541329
[epoch11, step2180]: loss 0.535499
[epoch11, step2181]: loss 0.434843
[epoch11, step2182]: loss 0.309784
[epoch11, step2183]: loss 0.541228
[epoch11, step2184]: loss 0.561156
[epoch11, step2185]: loss 0.351086
[epoch11, step2186]: loss 0.506755
[epoch11, step2187]: loss 0.457372
[epoch11, step2188]: loss 0.438044
[epoch11, step2189]: loss 0.445068
[epoch11, step2190]: loss 0.525717
[epoch11, step2191]: loss 0.459273
[epoch11, step2192]: loss 0.646172
[epoch11, step2193]: loss 0.564361
[epoch11, step2194]: loss 0.529339
[epoch11, step2195]: loss 0.520337
[epoch11, step2196]: loss 0.635122
[epoch11, step2197]: loss 0.535815
[epoch11, step2198]: loss 0.455009
[epoch11, step2199]: loss 0.622495
[epoch11, step2200]: loss 0.415276
[epoch11, step2201]: loss 0.313785
[epoch11, step2202]: loss 0.518357
[epoch11, step2203]: loss 0.714829
[epoch11, step2204]: loss 0.757816
[epoch11, step2205]: loss 0.547348
[epoch11, step2206]: loss 0.454207
[epoch11, step2207]: loss 0.662529
[epoch11, step2208]: loss 0.517616
[epoch11, step2209]: loss 0.571242
[epoch11, step2210]: loss 0.350639
[epoch11, step2211]: loss 0.720389
[epoch11, step2212]: loss 0.636649
[epoch11, step2213]: loss 0.350064
[epoch11, step2214]: loss 0.555094
[epoch11, step2215]: loss 0.592331
[epoch11, step2216]: loss 0.372252
[epoch11, step2217]: loss 0.460056
[epoch11, step2218]: loss 0.531601
[epoch11, step2219]: loss 0.443321
[epoch11, step2220]: loss 0.553036
[epoch11, step2221]: loss 0.245723
[epoch11, step2222]: loss 0.406339
[epoch11, step2223]: loss 0.366114
[epoch11, step2224]: loss 0.326762
[epoch11, step2225]: loss 0.456143
[epoch11, step2226]: loss 0.527709
[epoch11, step2227]: loss 0.463198
[epoch11, step2228]: loss 0.546558
[epoch11, step2229]: loss 0.524294
[epoch11, step2230]: loss 0.360903
[epoch11, step2231]: loss 0.656239
[epoch11, step2232]: loss 0.564099
[epoch11, step2233]: loss 0.719554
[epoch11, step2234]: loss 0.352509
[epoch11, step2235]: loss 0.397954
[epoch11, step2236]: loss 0.432875
[epoch11, step2237]: loss 0.409977
[epoch11, step2238]: loss 0.616154
[epoch11, step2239]: loss 0.541243
[epoch11, step2240]: loss 0.328064
[epoch11, step2241]: loss 0.729044
[epoch11, step2242]: loss 0.650865
[epoch11, step2243]: loss 0.692644
[epoch11, step2244]: loss 0.497699
[epoch11, step2245]: loss 0.406311
[epoch11, step2246]: loss 0.549599
[epoch11, step2247]: loss 0.390803
[epoch11, step2248]: loss 0.745048
[epoch11, step2249]: loss 0.455859
[epoch11, step2250]: loss 0.577947
[epoch11, step2251]: loss 0.442062
[epoch11, step2252]: loss 0.404469
[epoch11, step2253]: loss 0.601000
[epoch11, step2254]: loss 0.492589
[epoch11, step2255]: loss 0.306364
[epoch11, step2256]: loss 0.713169
[epoch11, step2257]: loss 0.726601
[epoch11, step2258]: loss 0.511664
[epoch11, step2259]: loss 0.630958
[epoch11, step2260]: loss 0.568292
[epoch11, step2261]: loss 0.459144
[epoch11, step2262]: loss 0.338916
[epoch11, step2263]: loss 0.402264
[epoch11, step2264]: loss 0.408629
[epoch11, step2265]: loss 0.526230
[epoch11, step2266]: loss 0.321308
[epoch11, step2267]: loss 0.573356
[epoch11, step2268]: loss 0.372986
[epoch11, step2269]: loss 0.661916
[epoch11, step2270]: loss 0.449646
[epoch11, step2271]: loss 0.602195
[epoch11, step2272]: loss 0.581828
[epoch11, step2273]: loss 0.606785
[epoch11, step2274]: loss 0.438687
[epoch11, step2275]: loss 0.640920
[epoch11, step2276]: loss 0.534904
[epoch11, step2277]: loss 0.607493
[epoch11, step2278]: loss 0.611734
[epoch11, step2279]: loss 0.519184
[epoch11, step2280]: loss 0.540302
[epoch11, step2281]: loss 0.590124
[epoch11, step2282]: loss 0.385917
[epoch11, step2283]: loss 0.568692
[epoch11, step2284]: loss 0.644595
[epoch11, step2285]: loss 0.628142
[epoch11, step2286]: loss 0.276059
[epoch11, step2287]: loss 0.378361
[epoch11, step2288]: loss 0.617570
[epoch11, step2289]: loss 0.636257
[epoch11, step2290]: loss 0.474164
[epoch11, step2291]: loss 0.258261
[epoch11, step2292]: loss 0.520644
[epoch11, step2293]: loss 0.455433
[epoch11, step2294]: loss 0.440413
[epoch11, step2295]: loss 0.526478
[epoch11, step2296]: loss 0.613185
[epoch11, step2297]: loss 0.315185
[epoch11, step2298]: loss 0.344055
[epoch11, step2299]: loss 0.587345
[epoch11, step2300]: loss 0.456420
[epoch11, step2301]: loss 0.610714
[epoch11, step2302]: loss 0.433251
[epoch11, step2303]: loss 0.361045
[epoch11, step2304]: loss 0.584070
[epoch11, step2305]: loss 0.631260
[epoch11, step2306]: loss 0.375662
[epoch11, step2307]: loss 0.450690
[epoch11, step2308]: loss 0.360837
[epoch11, step2309]: loss 0.529407
[epoch11, step2310]: loss 0.538072
[epoch11, step2311]: loss 0.370394
[epoch11, step2312]: loss 0.476051
[epoch11, step2313]: loss 0.599251
[epoch11, step2314]: loss 0.563194
[epoch11, step2315]: loss 0.462216
[epoch11, step2316]: loss 0.512165
[epoch11, step2317]: loss 0.477278
[epoch11, step2318]: loss 0.362947
[epoch11, step2319]: loss 0.294870
[epoch11, step2320]: loss 0.561602
[epoch11, step2321]: loss 0.506794
[epoch11, step2322]: loss 0.423053
[epoch11, step2323]: loss 0.544107
[epoch11, step2324]: loss 0.457855
[epoch11, step2325]: loss 0.598780
[epoch11, step2326]: loss 0.477731
[epoch11, step2327]: loss 0.469676
[epoch11, step2328]: loss 0.493210
[epoch11, step2329]: loss 0.432646
[epoch11, step2330]: loss 0.528452
[epoch11, step2331]: loss 0.521540
[epoch11, step2332]: loss 0.259146
[epoch11, step2333]: loss 0.400363
[epoch11, step2334]: loss 0.217301
[epoch11, step2335]: loss 0.262986
[epoch11, step2336]: loss 0.440264
[epoch11, step2337]: loss 0.598141
[epoch11, step2338]: loss 0.671944
[epoch11, step2339]: loss 0.586059
[epoch11, step2340]: loss 0.281200
[epoch11, step2341]: loss 0.535226
[epoch11, step2342]: loss 0.574366
[epoch11, step2343]: loss 0.444848
[epoch11, step2344]: loss 0.622532
[epoch11, step2345]: loss 0.350722
[epoch11, step2346]: loss 0.287065
[epoch11, step2347]: loss 0.314582
[epoch11, step2348]: loss 0.405549
[epoch11, step2349]: loss 0.430294
[epoch11, step2350]: loss 0.749512
[epoch11, step2351]: loss 0.469756
[epoch11, step2352]: loss 0.555987
[epoch11, step2353]: loss 0.464447
[epoch11, step2354]: loss 0.281204
[epoch11, step2355]: loss 0.504982
[epoch11, step2356]: loss 0.370018
[epoch11, step2357]: loss 0.285337
[epoch11, step2358]: loss 0.354625
[epoch11, step2359]: loss 0.681334
[epoch11, step2360]: loss 0.290637
[epoch11, step2361]: loss 0.520250
[epoch11, step2362]: loss 0.339638
[epoch11, step2363]: loss 0.366022
[epoch11, step2364]: loss 0.518688
[epoch11, step2365]: loss 0.522556
[epoch11, step2366]: loss 0.437896
[epoch11, step2367]: loss 0.657443
[epoch11, step2368]: loss 0.514285
[epoch11, step2369]: loss 0.479720
[epoch11, step2370]: loss 0.566957
[epoch11, step2371]: loss 0.508608
[epoch11, step2372]: loss 0.439169
[epoch11, step2373]: loss 0.285336
[epoch11, step2374]: loss 0.466320
[epoch11, step2375]: loss 0.642291
[epoch11, step2376]: loss 0.494592
[epoch11, step2377]: loss 0.573467
[epoch11, step2378]: loss 0.478028
[epoch11, step2379]: loss 0.360130
[epoch11, step2380]: loss 0.557271
[epoch11, step2381]: loss 0.562758
[epoch11, step2382]: loss 0.560937
[epoch11, step2383]: loss 0.443367
[epoch11, step2384]: loss 0.446329
[epoch11, step2385]: loss 0.454513
[epoch11, step2386]: loss 0.507114
[epoch11, step2387]: loss 0.376661
[epoch11, step2388]: loss 0.561902
[epoch11, step2389]: loss 0.559123
[epoch11, step2390]: loss 0.494010
[epoch11, step2391]: loss 0.355927
[epoch11, step2392]: loss 0.309224
[epoch11, step2393]: loss 0.602951
[epoch11, step2394]: loss 0.500901
[epoch11, step2395]: loss 0.480608
[epoch11, step2396]: loss 0.422394
[epoch11, step2397]: loss 0.527163
[epoch11, step2398]: loss 0.594022
[epoch11, step2399]: loss 0.509470
[epoch11, step2400]: loss 0.498801
[epoch11, step2401]: loss 0.510196
[epoch11, step2402]: loss 0.574782
[epoch11, step2403]: loss 0.523244
[epoch11, step2404]: loss 0.606883
[epoch11, step2405]: loss 0.514389
[epoch11, step2406]: loss 0.343037
[epoch11, step2407]: loss 0.387389
[epoch11, step2408]: loss 0.461402
[epoch11, step2409]: loss 0.413654
[epoch11, step2410]: loss 0.402606
[epoch11, step2411]: loss 0.794045
[epoch11, step2412]: loss 0.549166
[epoch11, step2413]: loss 0.430380
[epoch11, step2414]: loss 0.278809
[epoch11, step2415]: loss 0.460813
[epoch11, step2416]: loss 0.517937
[epoch11, step2417]: loss 0.354795
[epoch11, step2418]: loss 0.425392
[epoch11, step2419]: loss 0.379616
[epoch11, step2420]: loss 0.719324
[epoch11, step2421]: loss 0.361352
[epoch11, step2422]: loss 0.397402
[epoch11, step2423]: loss 0.592978
[epoch11, step2424]: loss 0.446599
[epoch11, step2425]: loss 0.578513
[epoch11, step2426]: loss 0.499766
[epoch11, step2427]: loss 0.591518
[epoch11, step2428]: loss 0.486480
[epoch11, step2429]: loss 0.568212
[epoch11, step2430]: loss 0.621356
[epoch11, step2431]: loss 0.559287
[epoch11, step2432]: loss 0.582374
[epoch11, step2433]: loss 0.387248
[epoch11, step2434]: loss 0.431088
[epoch11, step2435]: loss 0.715864
[epoch11, step2436]: loss 0.608143
[epoch11, step2437]: loss 0.683294
[epoch11, step2438]: loss 0.514066
[epoch11, step2439]: loss 0.590348
[epoch11, step2440]: loss 0.250915
[epoch11, step2441]: loss 0.802804
[epoch11, step2442]: loss 0.399995
[epoch11, step2443]: loss 0.432880
[epoch11, step2444]: loss 0.524543
[epoch11, step2445]: loss 0.528707
[epoch11, step2446]: loss 0.338289
[epoch11, step2447]: loss 0.594159
[epoch11, step2448]: loss 0.354271
[epoch11, step2449]: loss 0.644719
[epoch11, step2450]: loss 0.457199
[epoch11, step2451]: loss 0.542137
[epoch11, step2452]: loss 0.623607
[epoch11, step2453]: loss 0.595733
[epoch11, step2454]: loss 0.570700
[epoch11, step2455]: loss 0.494035
[epoch11, step2456]: loss 0.579368
[epoch11, step2457]: loss 0.680652
[epoch11, step2458]: loss 0.532905
[epoch11, step2459]: loss 0.558741
[epoch11, step2460]: loss 0.555351
[epoch11, step2461]: loss 0.449199
[epoch11, step2462]: loss 0.467268
[epoch11, step2463]: loss 0.616573
[epoch11, step2464]: loss 0.418896
[epoch11, step2465]: loss 0.311502
[epoch11, step2466]: loss 0.553160
[epoch11, step2467]: loss 0.389779
[epoch11, step2468]: loss 0.548795
[epoch11, step2469]: loss 0.618372
[epoch11, step2470]: loss 0.671823
[epoch11, step2471]: loss 0.437099
[epoch11, step2472]: loss 0.447557
[epoch11, step2473]: loss 0.550249
[epoch11, step2474]: loss 0.377997
[epoch11, step2475]: loss 0.436638
[epoch11, step2476]: loss 0.585435
[epoch11, step2477]: loss 0.482048
[epoch11, step2478]: loss 0.621269
[epoch11, step2479]: loss 0.474761
[epoch11, step2480]: loss 0.597700
[epoch11, step2481]: loss 0.388439
[epoch11, step2482]: loss 0.464556
[epoch11, step2483]: loss 0.290980
[epoch11, step2484]: loss 0.603240
[epoch11, step2485]: loss 0.479038
[epoch11, step2486]: loss 0.297546
[epoch11, step2487]: loss 0.582207
[epoch11, step2488]: loss 0.510012
[epoch11, step2489]: loss 0.577764
[epoch11, step2490]: loss 0.626647
[epoch11, step2491]: loss 0.373467
[epoch11, step2492]: loss 0.276009
[epoch11, step2493]: loss 0.704569
[epoch11, step2494]: loss 0.830372
[epoch11, step2495]: loss 0.235987
[epoch11, step2496]: loss 0.464318
[epoch11, step2497]: loss 0.698647
[epoch11, step2498]: loss 0.630608
[epoch11, step2499]: loss 0.651933
[epoch11, step2500]: loss 0.278338
[epoch11, step2501]: loss 0.630583
[epoch11, step2502]: loss 0.487002
[epoch11, step2503]: loss 0.470413
[epoch11, step2504]: loss 0.581383
[epoch11, step2505]: loss 0.500466
[epoch11, step2506]: loss 0.703749
[epoch11, step2507]: loss 0.628561
[epoch11, step2508]: loss 0.280454
[epoch11, step2509]: loss 0.454911
[epoch11, step2510]: loss 0.360615
[epoch11, step2511]: loss 0.436762
[epoch11, step2512]: loss 0.435106
[epoch11, step2513]: loss 0.636783
[epoch11, step2514]: loss 0.396229
[epoch11, step2515]: loss 0.354080
[epoch11, step2516]: loss 0.433003
[epoch11, step2517]: loss 0.458934
[epoch11, step2518]: loss 0.418799
[epoch11, step2519]: loss 0.543321
[epoch11, step2520]: loss 0.308754
[epoch11, step2521]: loss 0.686041
[epoch11, step2522]: loss 0.321828
[epoch11, step2523]: loss 0.556669
[epoch11, step2524]: loss 0.644826
[epoch11, step2525]: loss 0.549992
[epoch11, step2526]: loss 0.755437
[epoch11, step2527]: loss 0.680774
[epoch11, step2528]: loss 0.508165
[epoch11, step2529]: loss 0.405746
[epoch11, step2530]: loss 0.642935
[epoch11, step2531]: loss 0.509194
[epoch11, step2532]: loss 0.466978
[epoch11, step2533]: loss 0.461438
[epoch11, step2534]: loss 0.453198
[epoch11, step2535]: loss 0.755304
[epoch11, step2536]: loss 0.491900
[epoch11, step2537]: loss 0.449501
[epoch11, step2538]: loss 0.596189
[epoch11, step2539]: loss 0.404099
[epoch11, step2540]: loss 0.472976
[epoch11, step2541]: loss 0.427247
[epoch11, step2542]: loss 0.385155
[epoch11, step2543]: loss 0.649298
[epoch11, step2544]: loss 0.482291
[epoch11, step2545]: loss 0.312800
[epoch11, step2546]: loss 0.448647
[epoch11, step2547]: loss 0.704111
[epoch11, step2548]: loss 0.391493
[epoch11, step2549]: loss 0.542791
[epoch11, step2550]: loss 0.558088
[epoch11, step2551]: loss 0.369806
[epoch11, step2552]: loss 0.549058
[epoch11, step2553]: loss 0.574819
[epoch11, step2554]: loss 0.111207
[epoch11, step2555]: loss 0.300645
[epoch11, step2556]: loss 0.676570
[epoch11, step2557]: loss 0.400530
[epoch11, step2558]: loss 0.464208
[epoch11, step2559]: loss 0.500790
[epoch11, step2560]: loss 0.458976
[epoch11, step2561]: loss 0.586646
[epoch11, step2562]: loss 0.574427
[epoch11, step2563]: loss 0.362517
[epoch11, step2564]: loss 0.714039
[epoch11, step2565]: loss 0.579982
[epoch11, step2566]: loss 0.689254
[epoch11, step2567]: loss 0.407338
[epoch11, step2568]: loss 0.205221
[epoch11, step2569]: loss 0.607717
[epoch11, step2570]: loss 0.441822
[epoch11, step2571]: loss 0.498978
[epoch11, step2572]: loss 0.581554
[epoch11, step2573]: loss 0.527378
[epoch11, step2574]: loss 0.170576
[epoch11, step2575]: loss 0.724563
[epoch11, step2576]: loss 0.284077
[epoch11, step2577]: loss 0.303076
[epoch11, step2578]: loss 0.386778
[epoch11, step2579]: loss 0.324309
[epoch11, step2580]: loss 0.570952
[epoch11, step2581]: loss 0.508817
[epoch11, step2582]: loss 0.718674
[epoch11, step2583]: loss 0.488674
[epoch11, step2584]: loss 0.629529
[epoch11, step2585]: loss 0.759493
[epoch11, step2586]: loss 0.443251
[epoch11, step2587]: loss 0.473158
[epoch11, step2588]: loss 0.376687
[epoch11, step2589]: loss 0.494867
[epoch11, step2590]: loss 0.436234
[epoch11, step2591]: loss 0.687013
[epoch11, step2592]: loss 0.584553
[epoch11, step2593]: loss 0.454290
[epoch11, step2594]: loss 0.746932
[epoch11, step2595]: loss 0.541256
[epoch11, step2596]: loss 0.291603
[epoch11, step2597]: loss 0.588309
[epoch11, step2598]: loss 0.496491
[epoch11, step2599]: loss 0.521532
[epoch11, step2600]: loss 0.458744
[epoch11, step2601]: loss 0.323364
[epoch11, step2602]: loss 0.292881
[epoch11, step2603]: loss 0.554685
[epoch11, step2604]: loss 0.432725
[epoch11, step2605]: loss 0.498580
[epoch11, step2606]: loss 0.590867
[epoch11, step2607]: loss 0.471199
[epoch11, step2608]: loss 0.413039
[epoch11, step2609]: loss 0.251971
[epoch11, step2610]: loss 0.425352
[epoch11, step2611]: loss 0.377640
[epoch11, step2612]: loss 0.322805
[epoch11, step2613]: loss 0.642418
[epoch11, step2614]: loss 0.576546
[epoch11, step2615]: loss 0.452892
[epoch11, step2616]: loss 0.399939
[epoch11, step2617]: loss 0.657207
[epoch11, step2618]: loss 0.548848
[epoch11, step2619]: loss 0.485513
[epoch11, step2620]: loss 0.539834
[epoch11, step2621]: loss 0.481872
[epoch11, step2622]: loss 0.327488
[epoch11, step2623]: loss 0.380186
[epoch11, step2624]: loss 0.406662
[epoch11, step2625]: loss 0.579631
[epoch11, step2626]: loss 0.426536
[epoch11, step2627]: loss 0.421314
[epoch11, step2628]: loss 0.370604
[epoch11, step2629]: loss 0.684612
[epoch11, step2630]: loss 0.348870
[epoch11, step2631]: loss 0.550449
[epoch11, step2632]: loss 0.449072
[epoch11, step2633]: loss 0.442669
[epoch11, step2634]: loss 0.445370
[epoch11, step2635]: loss 0.523354
[epoch11, step2636]: loss 0.681839
[epoch11, step2637]: loss 0.631392
[epoch11, step2638]: loss 0.622066
[epoch11, step2639]: loss 0.476756
[epoch11, step2640]: loss 0.262265
[epoch11, step2641]: loss 0.750428
[epoch11, step2642]: loss 0.697472
[epoch11, step2643]: loss 0.599598
[epoch11, step2644]: loss 0.638781
[epoch11, step2645]: loss 0.611629
[epoch11, step2646]: loss 0.616605
[epoch11, step2647]: loss 0.343813
[epoch11, step2648]: loss 0.454651
[epoch11, step2649]: loss 0.581598
[epoch11, step2650]: loss 0.506385
[epoch11, step2651]: loss 0.482205
[epoch11, step2652]: loss 0.267038
[epoch11, step2653]: loss 0.250100
[epoch11, step2654]: loss 0.365125
[epoch11, step2655]: loss 0.420332
[epoch11, step2656]: loss 0.109849
[epoch11, step2657]: loss 0.576428
[epoch11, step2658]: loss 0.454797
[epoch11, step2659]: loss 0.480545
[epoch11, step2660]: loss 0.599096
[epoch11, step2661]: loss 0.762073
[epoch11, step2662]: loss 0.410582
[epoch11, step2663]: loss 0.368922
[epoch11, step2664]: loss 0.563935
[epoch11, step2665]: loss 0.635690
[epoch11, step2666]: loss 0.602764
[epoch11, step2667]: loss 0.259390
[epoch11, step2668]: loss 0.597537
[epoch11, step2669]: loss 0.490171
[epoch11, step2670]: loss 0.459350
[epoch11, step2671]: loss 0.561811
[epoch11, step2672]: loss 0.450229
[epoch11, step2673]: loss 0.430687
[epoch11, step2674]: loss 0.512353
[epoch11, step2675]: loss 0.530509
[epoch11, step2676]: loss 0.603312
[epoch11, step2677]: loss 0.283322
[epoch11, step2678]: loss 0.580402
[epoch11, step2679]: loss 0.404726
[epoch11, step2680]: loss 0.341269
[epoch11, step2681]: loss 0.557800
[epoch11, step2682]: loss 0.397577
[epoch11, step2683]: loss 0.664890
[epoch11, step2684]: loss 0.330126
[epoch11, step2685]: loss 0.529076
[epoch11, step2686]: loss 0.557958
[epoch11, step2687]: loss 0.513139
[epoch11, step2688]: loss 0.493359
[epoch11, step2689]: loss 0.633780
[epoch11, step2690]: loss 0.421857
[epoch11, step2691]: loss 0.520450
[epoch11, step2692]: loss 0.441473
[epoch11, step2693]: loss 0.445259
[epoch11, step2694]: loss 0.589224
[epoch11, step2695]: loss 0.431684
[epoch11, step2696]: loss 0.628168
[epoch11, step2697]: loss 0.711836
[epoch11, step2698]: loss 0.496738
[epoch11, step2699]: loss 0.626051
[epoch11, step2700]: loss 0.621062
[epoch11, step2701]: loss 0.556794
[epoch11, step2702]: loss 0.467858
[epoch11, step2703]: loss 0.310964
[epoch11, step2704]: loss 0.420353
[epoch11, step2705]: loss 0.472869
[epoch11, step2706]: loss 0.512891
[epoch11, step2707]: loss 0.504679
[epoch11, step2708]: loss 0.463124
[epoch11, step2709]: loss 0.411636
[epoch11, step2710]: loss 0.480327
[epoch11, step2711]: loss 0.516047
[epoch11, step2712]: loss 0.694277
[epoch11, step2713]: loss 0.705717
[epoch11, step2714]: loss 0.522735
[epoch11, step2715]: loss 0.253404
[epoch11, step2716]: loss 0.567883
[epoch11, step2717]: loss 0.309257
[epoch11, step2718]: loss 0.365040
[epoch11, step2719]: loss 0.632957
[epoch11, step2720]: loss 0.573090
[epoch11, step2721]: loss 0.463472
[epoch11, step2722]: loss 0.714755
[epoch11, step2723]: loss 0.639489
[epoch11, step2724]: loss 0.642671
[epoch11, step2725]: loss 0.526018
[epoch11, step2726]: loss 0.283118
[epoch11, step2727]: loss 0.117350
[epoch11, step2728]: loss 0.600074
[epoch11, step2729]: loss 0.388695
[epoch11, step2730]: loss 0.775816
[epoch11, step2731]: loss 0.623449
[epoch11, step2732]: loss 0.641340
[epoch11, step2733]: loss 0.760117
[epoch11, step2734]: loss 0.445656
[epoch11, step2735]: loss 0.270559
[epoch11, step2736]: loss 0.595189
[epoch11, step2737]: loss 0.462391
[epoch11, step2738]: loss 0.600859
[epoch11, step2739]: loss 0.413546
[epoch11, step2740]: loss 0.567921
[epoch11, step2741]: loss 0.593792
[epoch11, step2742]: loss 0.680458
[epoch11, step2743]: loss 0.358611
[epoch11, step2744]: loss 0.465516
[epoch11, step2745]: loss 0.216637
[epoch11, step2746]: loss 0.662527
[epoch11, step2747]: loss 0.632759
[epoch11, step2748]: loss 0.438753
[epoch11, step2749]: loss 0.644029
[epoch11, step2750]: loss 0.330103
[epoch11, step2751]: loss 0.525771
[epoch11, step2752]: loss 0.513220
[epoch11, step2753]: loss 0.598976
[epoch11, step2754]: loss 0.573476
[epoch11, step2755]: loss 0.153887
[epoch11, step2756]: loss 0.328178
[epoch11, step2757]: loss 0.250793
[epoch11, step2758]: loss 0.311885
[epoch11, step2759]: loss 0.702368
[epoch11, step2760]: loss 0.550148
[epoch11, step2761]: loss 0.350329
[epoch11, step2762]: loss 0.498967
[epoch11, step2763]: loss 0.384357
[epoch11, step2764]: loss 0.378017
[epoch11, step2765]: loss 0.498713
[epoch11, step2766]: loss 0.595785
[epoch11, step2767]: loss 0.474325
[epoch11, step2768]: loss 0.379244
[epoch11, step2769]: loss 0.541667
[epoch11, step2770]: loss 0.678380
[epoch11, step2771]: loss 0.308208
[epoch11, step2772]: loss 0.557113
[epoch11, step2773]: loss 0.628941
[epoch11, step2774]: loss 0.431888
[epoch11, step2775]: loss 0.651383
[epoch11, step2776]: loss 0.565502
[epoch11, step2777]: loss 0.396863
[epoch11, step2778]: loss 0.698236
[epoch11, step2779]: loss 0.577154
[epoch11, step2780]: loss 0.655024
[epoch11, step2781]: loss 0.732130
[epoch11, step2782]: loss 0.524786
[epoch11, step2783]: loss 0.719487
[epoch11, step2784]: loss 0.268744
[epoch11, step2785]: loss 0.517421
[epoch11, step2786]: loss 0.280293
[epoch11, step2787]: loss 0.594145
[epoch11, step2788]: loss 0.460387
[epoch11, step2789]: loss 0.539828
[epoch11, step2790]: loss 0.570764
[epoch11, step2791]: loss 0.314490
[epoch11, step2792]: loss 0.356807
[epoch11, step2793]: loss 0.627304
[epoch11, step2794]: loss 0.569861
[epoch11, step2795]: loss 0.560897
[epoch11, step2796]: loss 0.614313
[epoch11, step2797]: loss 0.543622
[epoch11, step2798]: loss 0.395038
[epoch11, step2799]: loss 0.638732
[epoch11, step2800]: loss 0.603335
[epoch11, step2801]: loss 0.366192
[epoch11, step2802]: loss 0.525105
[epoch11, step2803]: loss 0.508938
[epoch11, step2804]: loss 0.622370
[epoch11, step2805]: loss 0.401468
[epoch11, step2806]: loss 0.372395
[epoch11, step2807]: loss 0.612859
[epoch11, step2808]: loss 0.505743
[epoch11, step2809]: loss 0.581301
[epoch11, step2810]: loss 0.227259
[epoch11, step2811]: loss 0.194895
[epoch11, step2812]: loss 0.625827
[epoch11, step2813]: loss 0.639149
[epoch11, step2814]: loss 0.680892
[epoch11, step2815]: loss 0.380406
[epoch11, step2816]: loss 0.407121
[epoch11, step2817]: loss 0.545890
[epoch11, step2818]: loss 0.334539
[epoch11, step2819]: loss 0.383217
[epoch11, step2820]: loss 0.321361
[epoch11, step2821]: loss 0.394567
[epoch11, step2822]: loss 0.717087
[epoch11, step2823]: loss 0.498580
[epoch11, step2824]: loss 0.447015
[epoch11, step2825]: loss 0.445584
[epoch11, step2826]: loss 0.352361
[epoch11, step2827]: loss 0.536232
[epoch11, step2828]: loss 0.570634
[epoch11, step2829]: loss 0.529016
[epoch11, step2830]: loss 0.527404
[epoch11, step2831]: loss 0.708382
[epoch11, step2832]: loss 0.487557
[epoch11, step2833]: loss 0.419687
[epoch11, step2834]: loss 0.281128
[epoch11, step2835]: loss 0.619057
[epoch11, step2836]: loss 0.471308
[epoch11, step2837]: loss 0.409274
[epoch11, step2838]: loss 0.572361
[epoch11, step2839]: loss 0.151063
[epoch11, step2840]: loss 0.446641
[epoch11, step2841]: loss 0.506510
[epoch11, step2842]: loss 0.696388
[epoch11, step2843]: loss 0.471946
[epoch11, step2844]: loss 0.593734
[epoch11, step2845]: loss 0.372092
[epoch11, step2846]: loss 0.500425
[epoch11, step2847]: loss 0.292125
[epoch11, step2848]: loss 0.535717
[epoch11, step2849]: loss 0.266978
[epoch11, step2850]: loss 0.577651
[epoch11, step2851]: loss 0.558459
[epoch11, step2852]: loss 0.640409
[epoch11, step2853]: loss 0.461526
[epoch11, step2854]: loss 0.341447
[epoch11, step2855]: loss 0.641066
[epoch11, step2856]: loss 0.430296
[epoch11, step2857]: loss 0.382911
[epoch11, step2858]: loss 0.451768
[epoch11, step2859]: loss 0.469411
[epoch11, step2860]: loss 0.597706
[epoch11, step2861]: loss 0.382135
[epoch11, step2862]: loss 0.675641
[epoch11, step2863]: loss 0.581847
[epoch11, step2864]: loss 0.605658
[epoch11, step2865]: loss 0.492471
[epoch11, step2866]: loss 0.424912
[epoch11, step2867]: loss 0.462881
[epoch11, step2868]: loss 0.491623
[epoch11, step2869]: loss 0.677068
[epoch11, step2870]: loss 0.550401
[epoch11, step2871]: loss 0.594662
[epoch11, step2872]: loss 0.479142
[epoch11, step2873]: loss 0.151762
[epoch11, step2874]: loss 0.648300
[epoch11, step2875]: loss 0.543908
[epoch11, step2876]: loss 0.462819
[epoch11, step2877]: loss 0.245183
[epoch11, step2878]: loss 0.498760
[epoch11, step2879]: loss 0.491930
[epoch11, step2880]: loss 0.472331
[epoch11, step2881]: loss 0.416864
[epoch11, step2882]: loss 0.657418
[epoch11, step2883]: loss 0.560493
[epoch11, step2884]: loss 0.551024
[epoch11, step2885]: loss 0.522048
[epoch11, step2886]: loss 0.487599
[epoch11, step2887]: loss 0.438783
[epoch11, step2888]: loss 0.670850
[epoch11, step2889]: loss 0.416121
[epoch11, step2890]: loss 0.399367
[epoch11, step2891]: loss 0.660216
[epoch11, step2892]: loss 0.544542
[epoch11, step2893]: loss 0.461394
[epoch11, step2894]: loss 0.578975
[epoch11, step2895]: loss 0.556263
[epoch11, step2896]: loss 0.576508
[epoch11, step2897]: loss 0.706642
[epoch11, step2898]: loss 0.585824
[epoch11, step2899]: loss 0.631465
[epoch11, step2900]: loss 0.645855
[epoch11, step2901]: loss 0.499800
[epoch11, step2902]: loss 0.307936
[epoch11, step2903]: loss 0.531792
[epoch11, step2904]: loss 0.670423
[epoch11, step2905]: loss 0.597356
[epoch11, step2906]: loss 0.326885
[epoch11, step2907]: loss 0.319684
[epoch11, step2908]: loss 0.711953
[epoch11, step2909]: loss 0.642842
[epoch11, step2910]: loss 0.273184
[epoch11, step2911]: loss 0.752685
[epoch11, step2912]: loss 0.580206
[epoch11, step2913]: loss 0.437670
[epoch11, step2914]: loss 0.559836
[epoch11, step2915]: loss 0.517897
[epoch11, step2916]: loss 0.511427
[epoch11, step2917]: loss 0.545773
[epoch11, step2918]: loss 0.375505
[epoch11, step2919]: loss 0.556677
[epoch11, step2920]: loss 0.330839
[epoch11, step2921]: loss 0.461929
[epoch11, step2922]: loss 0.433192
[epoch11, step2923]: loss 0.622234
[epoch11, step2924]: loss 0.673173
[epoch11, step2925]: loss 0.623866
[epoch11, step2926]: loss 0.220898
[epoch11, step2927]: loss 0.518867
[epoch11, step2928]: loss 0.491499
[epoch11, step2929]: loss 0.591696
[epoch11, step2930]: loss 0.280296
[epoch11, step2931]: loss 0.601056
[epoch11, step2932]: loss 0.723653
[epoch11, step2933]: loss 0.575845
[epoch11, step2934]: loss 0.545296
[epoch11, step2935]: loss 0.522433
[epoch11, step2936]: loss 0.439188
[epoch11, step2937]: loss 0.686160
[epoch11, step2938]: loss 0.589742
[epoch11, step2939]: loss 0.616332
[epoch11, step2940]: loss 0.507056
[epoch11, step2941]: loss 0.609845
[epoch11, step2942]: loss 0.495373
[epoch11, step2943]: loss 0.729172
[epoch11, step2944]: loss 0.136338
[epoch11, step2945]: loss 0.408329
[epoch11, step2946]: loss 0.405127
[epoch11, step2947]: loss 0.419289
[epoch11, step2948]: loss 0.513196
[epoch11, step2949]: loss 0.415730
[epoch11, step2950]: loss 0.579074
[epoch11, step2951]: loss 0.689730
[epoch11, step2952]: loss 0.563595
[epoch11, step2953]: loss 0.683923
[epoch11, step2954]: loss 0.402437
[epoch11, step2955]: loss 0.635920
[epoch11, step2956]: loss 0.657971
[epoch11, step2957]: loss 0.333599
[epoch11, step2958]: loss 0.547593
[epoch11, step2959]: loss 0.526514
[epoch11, step2960]: loss 0.595725
[epoch11, step2961]: loss 0.649894
[epoch11, step2962]: loss 0.377421
[epoch11, step2963]: loss 0.654017
[epoch11, step2964]: loss 0.328646
[epoch11, step2965]: loss 0.457099
[epoch11, step2966]: loss 0.122520
[epoch11, step2967]: loss 0.491886
[epoch11, step2968]: loss 0.616333
[epoch11, step2969]: loss 0.407951
[epoch11, step2970]: loss 0.196358
[epoch11, step2971]: loss 0.546009
[epoch11, step2972]: loss 0.604344
[epoch11, step2973]: loss 0.423959
[epoch11, step2974]: loss 0.516539
[epoch11, step2975]: loss 0.395912
[epoch11, step2976]: loss 0.541822
[epoch11, step2977]: loss 0.423879
[epoch11, step2978]: loss 0.365442
[epoch11, step2979]: loss 0.542320
[epoch11, step2980]: loss 0.433833
[epoch11, step2981]: loss 0.451234
[epoch11, step2982]: loss 0.570905
[epoch11, step2983]: loss 0.552752
[epoch11, step2984]: loss 0.435755
[epoch11, step2985]: loss 0.413555
[epoch11, step2986]: loss 0.290988
[epoch11, step2987]: loss 0.754216
[epoch11, step2988]: loss 0.384339
[epoch11, step2989]: loss 0.519929
[epoch11, step2990]: loss 0.079516
[epoch11, step2991]: loss 0.502715
[epoch11, step2992]: loss 0.588525
[epoch11, step2993]: loss 0.506322
[epoch11, step2994]: loss 0.365947
[epoch11, step2995]: loss 0.530769
[epoch11, step2996]: loss 0.447225
[epoch11, step2997]: loss 0.267087
[epoch11, step2998]: loss 0.621027
[epoch11, step2999]: loss 0.536367
[epoch11, step3000]: loss 0.653193
[epoch11, step3001]: loss 0.439373
[epoch11, step3002]: loss 0.501037
[epoch11, step3003]: loss 0.525084
[epoch11, step3004]: loss 0.538329
[epoch11, step3005]: loss 0.392180
[epoch11, step3006]: loss 0.550633
[epoch11, step3007]: loss 0.516591
[epoch11, step3008]: loss 0.421901
[epoch11, step3009]: loss 0.576830
[epoch11, step3010]: loss 0.367364
[epoch11, step3011]: loss 0.430958
[epoch11, step3012]: loss 0.667413
[epoch11, step3013]: loss 0.448567
[epoch11, step3014]: loss 0.698846
[epoch11, step3015]: loss 0.373952
[epoch11, step3016]: loss 0.310842
[epoch11, step3017]: loss 0.489465
[epoch11, step3018]: loss 0.595758
[epoch11, step3019]: loss 0.505246
[epoch11, step3020]: loss 0.683822
[epoch11, step3021]: loss 0.543132
[epoch11, step3022]: loss 0.676561
[epoch11, step3023]: loss 0.560244
[epoch11, step3024]: loss 0.609746
[epoch11, step3025]: loss 0.510271
[epoch11, step3026]: loss 0.615650
[epoch11, step3027]: loss 0.588727
[epoch11, step3028]: loss 0.573790
[epoch11, step3029]: loss 0.320131
[epoch11, step3030]: loss 0.518140
[epoch11, step3031]: loss 0.334634
[epoch11, step3032]: loss 0.637707
[epoch11, step3033]: loss 0.379814
[epoch11, step3034]: loss 0.516807
[epoch11, step3035]: loss 0.584711
[epoch11, step3036]: loss 0.488801
[epoch11, step3037]: loss 0.336550
[epoch11, step3038]: loss 0.664662
[epoch11, step3039]: loss 0.617102
[epoch11, step3040]: loss 0.513633
[epoch11, step3041]: loss 0.459890
[epoch11, step3042]: loss 0.445639
[epoch11, step3043]: loss 0.272698
[epoch11, step3044]: loss 0.488957
[epoch11, step3045]: loss 0.484593
[epoch11, step3046]: loss 0.352346
[epoch11, step3047]: loss 0.675465
[epoch11, step3048]: loss 0.513771
[epoch11, step3049]: loss 0.433799
[epoch11, step3050]: loss 0.400162
[epoch11, step3051]: loss 0.623474
[epoch11, step3052]: loss 0.580667
[epoch11, step3053]: loss 0.484790
[epoch11, step3054]: loss 0.476939
[epoch11, step3055]: loss 0.655835
[epoch11, step3056]: loss 0.603870
[epoch11, step3057]: loss 0.586403
[epoch11, step3058]: loss 0.680796
[epoch11, step3059]: loss 0.363615
[epoch11, step3060]: loss 0.610120
[epoch11, step3061]: loss 0.530274
[epoch11, step3062]: loss 0.397590
[epoch11, step3063]: loss 0.512943
[epoch11, step3064]: loss 0.525883
[epoch11, step3065]: loss 0.416040
[epoch11, step3066]: loss 0.490383
[epoch11, step3067]: loss 0.521722
[epoch11, step3068]: loss 0.296045
[epoch11, step3069]: loss 0.555913
[epoch11, step3070]: loss 0.634315
[epoch11, step3071]: loss 0.133368
[epoch11, step3072]: loss 0.534158
[epoch11, step3073]: loss 0.404947
[epoch11, step3074]: loss 0.651086
[epoch11, step3075]: loss 0.378701
[epoch11, step3076]: loss 0.582177

[epoch11]: avg loss 0.582177

[epoch12, step1]: loss 0.557017
[epoch12, step2]: loss 0.437266
[epoch12, step3]: loss 0.546590
[epoch12, step4]: loss 0.647293
[epoch12, step5]: loss 0.606815
[epoch12, step6]: loss 0.344127
[epoch12, step7]: loss 0.518179
[epoch12, step8]: loss 0.395900
[epoch12, step9]: loss 0.427207
[epoch12, step10]: loss 0.558588
[epoch12, step11]: loss 0.514257
[epoch12, step12]: loss 0.458613
[epoch12, step13]: loss 0.577830
[epoch12, step14]: loss 0.486625
[epoch12, step15]: loss 0.366318
[epoch12, step16]: loss 0.695421
[epoch12, step17]: loss 0.599385
[epoch12, step18]: loss 0.619014
[epoch12, step19]: loss 0.411286
[epoch12, step20]: loss 0.511886
[epoch12, step21]: loss 0.621223
[epoch12, step22]: loss 0.320849
[epoch12, step23]: loss 0.600299
[epoch12, step24]: loss 0.544617
[epoch12, step25]: loss 0.388903
[epoch12, step26]: loss 0.510379
[epoch12, step27]: loss 0.380922
[epoch12, step28]: loss 0.408498
[epoch12, step29]: loss 0.423579
[epoch12, step30]: loss 0.420405
[epoch12, step31]: loss 0.571626
[epoch12, step32]: loss 0.117931
[epoch12, step33]: loss 0.518540
[epoch12, step34]: loss 0.638713
[epoch12, step35]: loss 0.540451
[epoch12, step36]: loss 0.512079
[epoch12, step37]: loss 0.569386
[epoch12, step38]: loss 0.566016
[epoch12, step39]: loss 0.513907
[epoch12, step40]: loss 0.475023
[epoch12, step41]: loss 0.575173
[epoch12, step42]: loss 0.545940
[epoch12, step43]: loss 0.546340
[epoch12, step44]: loss 0.519441
[epoch12, step45]: loss 0.678419
[epoch12, step46]: loss 0.371657
[epoch12, step47]: loss 0.165117
[epoch12, step48]: loss 0.687832
[epoch12, step49]: loss 0.541841
[epoch12, step50]: loss 0.590326
[epoch12, step51]: loss 0.276288
[epoch12, step52]: loss 0.611006
[epoch12, step53]: loss 0.387436
[epoch12, step54]: loss 0.280378
[epoch12, step55]: loss 0.445381
[epoch12, step56]: loss 0.359378
[epoch12, step57]: loss 0.581667
[epoch12, step58]: loss 0.608351
[epoch12, step59]: loss 0.586540
[epoch12, step60]: loss 0.586720
[epoch12, step61]: loss 0.538636
[epoch12, step62]: loss 0.314440
[epoch12, step63]: loss 0.295024
[epoch12, step64]: loss 0.597739
[epoch12, step65]: loss 0.404320
[epoch12, step66]: loss 0.259058
[epoch12, step67]: loss 0.395565
[epoch12, step68]: loss 0.623185
[epoch12, step69]: loss 0.477502
[epoch12, step70]: loss 0.600435
[epoch12, step71]: loss 0.516238
[epoch12, step72]: loss 0.629255
[epoch12, step73]: loss 0.544751
[epoch12, step74]: loss 0.567840
[epoch12, step75]: loss 0.381577
[epoch12, step76]: loss 0.412180
[epoch12, step77]: loss 0.446433
[epoch12, step78]: loss 0.636265
[epoch12, step79]: loss 0.341368
[epoch12, step80]: loss 0.549069
[epoch12, step81]: loss 0.345621
[epoch12, step82]: loss 0.439850
[epoch12, step83]: loss 0.462094
[epoch12, step84]: loss 0.459207
[epoch12, step85]: loss 0.709930
[epoch12, step86]: loss 0.362207
[epoch12, step87]: loss 0.586604
[epoch12, step88]: loss 0.353634
[epoch12, step89]: loss 0.423733
[epoch12, step90]: loss 0.534440
[epoch12, step91]: loss 0.249796
[epoch12, step92]: loss 0.560689
[epoch12, step93]: loss 0.424463
[epoch12, step94]: loss 0.437526
[epoch12, step95]: loss 0.688245
[epoch12, step96]: loss 0.455946
[epoch12, step97]: loss 0.670491
[epoch12, step98]: loss 0.541727
[epoch12, step99]: loss 0.493318
[epoch12, step100]: loss 0.298487
[epoch12, step101]: loss 0.690248
[epoch12, step102]: loss 0.640402
[epoch12, step103]: loss 0.407155
[epoch12, step104]: loss 0.612177
[epoch12, step105]: loss 0.273915
[epoch12, step106]: loss 0.483739
[epoch12, step107]: loss 0.361860
[epoch12, step108]: loss 0.490005
[epoch12, step109]: loss 0.182429
[epoch12, step110]: loss 0.514508
[epoch12, step111]: loss 0.624961
[epoch12, step112]: loss 0.452603
[epoch12, step113]: loss 0.471038
[epoch12, step114]: loss 0.489694
[epoch12, step115]: loss 0.380799
[epoch12, step116]: loss 0.545730
[epoch12, step117]: loss 0.350607
[epoch12, step118]: loss 0.438982
[epoch12, step119]: loss 0.501965
[epoch12, step120]: loss 0.540908
[epoch12, step121]: loss 0.434350
[epoch12, step122]: loss 0.558819
[epoch12, step123]: loss 0.476386
[epoch12, step124]: loss 0.766827
[epoch12, step125]: loss 0.392506
[epoch12, step126]: loss 0.517910
[epoch12, step127]: loss 0.344686
[epoch12, step128]: loss 0.569198
[epoch12, step129]: loss 0.605823
[epoch12, step130]: loss 0.281926
[epoch12, step131]: loss 0.372001
[epoch12, step132]: loss 0.401076
[epoch12, step133]: loss 0.380970
[epoch12, step134]: loss 0.506680
[epoch12, step135]: loss 0.391946
[epoch12, step136]: loss 0.485695
[epoch12, step137]: loss 0.581365
[epoch12, step138]: loss 0.557944
[epoch12, step139]: loss 0.646935
[epoch12, step140]: loss 0.566892
[epoch12, step141]: loss 0.354832
[epoch12, step142]: loss 0.520619
[epoch12, step143]: loss 0.706357
[epoch12, step144]: loss 0.500951
[epoch12, step145]: loss 0.441774
[epoch12, step146]: loss 0.515360
[epoch12, step147]: loss 0.591485
[epoch12, step148]: loss 0.443684
[epoch12, step149]: loss 0.703852
[epoch12, step150]: loss 0.613544
[epoch12, step151]: loss 0.514152
[epoch12, step152]: loss 0.481777
[epoch12, step153]: loss 0.452206
[epoch12, step154]: loss 0.367444
[epoch12, step155]: loss 0.357351
[epoch12, step156]: loss 0.624628
[epoch12, step157]: loss 0.411804
[epoch12, step158]: loss 0.397588
[epoch12, step159]: loss 0.335429
[epoch12, step160]: loss 0.584588
[epoch12, step161]: loss 0.551107
[epoch12, step162]: loss 0.517056
[epoch12, step163]: loss 0.498924
[epoch12, step164]: loss 0.740571
[epoch12, step165]: loss 0.349473
[epoch12, step166]: loss 0.693082
[epoch12, step167]: loss 0.572469
[epoch12, step168]: loss 0.503721
[epoch12, step169]: loss 0.431356
[epoch12, step170]: loss 0.348948
[epoch12, step171]: loss 0.414589
[epoch12, step172]: loss 0.539476
[epoch12, step173]: loss 0.581343
[epoch12, step174]: loss 0.732322
[epoch12, step175]: loss 0.423602
[epoch12, step176]: loss 0.656550
[epoch12, step177]: loss 0.574516
[epoch12, step178]: loss 0.437818
[epoch12, step179]: loss 0.535057
[epoch12, step180]: loss 0.658796
[epoch12, step181]: loss 0.627991
[epoch12, step182]: loss 0.517912
[epoch12, step183]: loss 0.604381
[epoch12, step184]: loss 0.517810
[epoch12, step185]: loss 0.541195
[epoch12, step186]: loss 0.623254
[epoch12, step187]: loss 0.319670
[epoch12, step188]: loss 0.602983
[epoch12, step189]: loss 0.361413
[epoch12, step190]: loss 0.158309
[epoch12, step191]: loss 0.436232
[epoch12, step192]: loss 0.560153
[epoch12, step193]: loss 0.484927
[epoch12, step194]: loss 0.434235
[epoch12, step195]: loss 0.385231
[epoch12, step196]: loss 0.551466
[epoch12, step197]: loss 0.483690
[epoch12, step198]: loss 0.731314
[epoch12, step199]: loss 0.659514
[epoch12, step200]: loss 0.497335
[epoch12, step201]: loss 0.429352
[epoch12, step202]: loss 0.282680
[epoch12, step203]: loss 0.577368
[epoch12, step204]: loss 0.338906
[epoch12, step205]: loss 0.225055
[epoch12, step206]: loss 0.625282
[epoch12, step207]: loss 0.688199
[epoch12, step208]: loss 0.602954
[epoch12, step209]: loss 0.460975
[epoch12, step210]: loss 0.427120
[epoch12, step211]: loss 0.507276
[epoch12, step212]: loss 0.536422
[epoch12, step213]: loss 0.532673
[epoch12, step214]: loss 0.690691
[epoch12, step215]: loss 0.382118
[epoch12, step216]: loss 0.621250
[epoch12, step217]: loss 0.653339
[epoch12, step218]: loss 0.513350
[epoch12, step219]: loss 0.488475
[epoch12, step220]: loss 0.613838
[epoch12, step221]: loss 0.560459
[epoch12, step222]: loss 0.614623
[epoch12, step223]: loss 0.356553
[epoch12, step224]: loss 0.430662
[epoch12, step225]: loss 0.505687
[epoch12, step226]: loss 0.350366
[epoch12, step227]: loss 0.336061
[epoch12, step228]: loss 0.588823
[epoch12, step229]: loss 0.438309
[epoch12, step230]: loss 0.664060
[epoch12, step231]: loss 0.555950
[epoch12, step232]: loss 0.379335
[epoch12, step233]: loss 0.589673
[epoch12, step234]: loss 0.630478
[epoch12, step235]: loss 0.249841
[epoch12, step236]: loss 0.572968
[epoch12, step237]: loss 0.154967
[epoch12, step238]: loss 0.500098
[epoch12, step239]: loss 0.381141
[epoch12, step240]: loss 0.421807
[epoch12, step241]: loss 0.291652
[epoch12, step242]: loss 0.494530
[epoch12, step243]: loss 0.379865
[epoch12, step244]: loss 0.572097
[epoch12, step245]: loss 0.420822
[epoch12, step246]: loss 0.292574
[epoch12, step247]: loss 0.721806
[epoch12, step248]: loss 0.570628
[epoch12, step249]: loss 0.682328
[epoch12, step250]: loss 0.555942
[epoch12, step251]: loss 0.493112
[epoch12, step252]: loss 0.385276
[epoch12, step253]: loss 0.745503
[epoch12, step254]: loss 0.330855
[epoch12, step255]: loss 0.490413
[epoch12, step256]: loss 0.412989
[epoch12, step257]: loss 0.266721
[epoch12, step258]: loss 0.659233
[epoch12, step259]: loss 0.721769
[epoch12, step260]: loss 0.232725
[epoch12, step261]: loss 0.524670
[epoch12, step262]: loss 0.680204
[epoch12, step263]: loss 0.261733
[epoch12, step264]: loss 0.411194
[epoch12, step265]: loss 0.639280
[epoch12, step266]: loss 0.359286
[epoch12, step267]: loss 0.420281
[epoch12, step268]: loss 0.502217
[epoch12, step269]: loss 0.594398
[epoch12, step270]: loss 0.444694
[epoch12, step271]: loss 0.574185
[epoch12, step272]: loss 0.601799
[epoch12, step273]: loss 0.416119
[epoch12, step274]: loss 0.350042
[epoch12, step275]: loss 0.695382
[epoch12, step276]: loss 0.459183
[epoch12, step277]: loss 0.586309
[epoch12, step278]: loss 0.587636
[epoch12, step279]: loss 0.613407
[epoch12, step280]: loss 0.579056
[epoch12, step281]: loss 0.642953
[epoch12, step282]: loss 0.379968
[epoch12, step283]: loss 0.605882
[epoch12, step284]: loss 0.418457
[epoch12, step285]: loss 0.445261
[epoch12, step286]: loss 0.524973
[epoch12, step287]: loss 0.709523
[epoch12, step288]: loss 0.663690
[epoch12, step289]: loss 0.698783
[epoch12, step290]: loss 0.572379
[epoch12, step291]: loss 0.412162
[epoch12, step292]: loss 0.700944
[epoch12, step293]: loss 0.565430
[epoch12, step294]: loss 0.580871
[epoch12, step295]: loss 0.518327
[epoch12, step296]: loss 0.587371
[epoch12, step297]: loss 0.493907
[epoch12, step298]: loss 0.536902
[epoch12, step299]: loss 0.488982
[epoch12, step300]: loss 0.304924
[epoch12, step301]: loss 0.444989
[epoch12, step302]: loss 0.634651
[epoch12, step303]: loss 0.390677
[epoch12, step304]: loss 0.382716
[epoch12, step305]: loss 0.150633
[epoch12, step306]: loss 0.648815
[epoch12, step307]: loss 0.391350
[epoch12, step308]: loss 0.556522
[epoch12, step309]: loss 0.564583
[epoch12, step310]: loss 0.590507
[epoch12, step311]: loss 0.428486
[epoch12, step312]: loss 0.502361
[epoch12, step313]: loss 0.408832
[epoch12, step314]: loss 0.454495
[epoch12, step315]: loss 0.759777
[epoch12, step316]: loss 0.567073
[epoch12, step317]: loss 0.671127
[epoch12, step318]: loss 0.642764
[epoch12, step319]: loss 0.526145
[epoch12, step320]: loss 0.540618
[epoch12, step321]: loss 0.502791
[epoch12, step322]: loss 0.479667
[epoch12, step323]: loss 0.592583
[epoch12, step324]: loss 0.472322
[epoch12, step325]: loss 0.569828
[epoch12, step326]: loss 0.543895
[epoch12, step327]: loss 0.353275
[epoch12, step328]: loss 0.662265
[epoch12, step329]: loss 0.589490
[epoch12, step330]: loss 0.670983
[epoch12, step331]: loss 0.708203
[epoch12, step332]: loss 0.421376
[epoch12, step333]: loss 0.382693
[epoch12, step334]: loss 0.314884
[epoch12, step335]: loss 0.442952
[epoch12, step336]: loss 0.438807
[epoch12, step337]: loss 0.564095
[epoch12, step338]: loss 0.542790
[epoch12, step339]: loss 0.618445
[epoch12, step340]: loss 0.541268
[epoch12, step341]: loss 0.554940
[epoch12, step342]: loss 0.563204
[epoch12, step343]: loss 0.481113
[epoch12, step344]: loss 0.510200
[epoch12, step345]: loss 0.503617
[epoch12, step346]: loss 0.525011
[epoch12, step347]: loss 0.376270
[epoch12, step348]: loss 0.295041
[epoch12, step349]: loss 0.459162
[epoch12, step350]: loss 0.523742
[epoch12, step351]: loss 0.442373
[epoch12, step352]: loss 0.594130
[epoch12, step353]: loss 0.460845
[epoch12, step354]: loss 0.658399
[epoch12, step355]: loss 0.121927
[epoch12, step356]: loss 0.500897
[epoch12, step357]: loss 0.501696
[epoch12, step358]: loss 0.466133
[epoch12, step359]: loss 0.389855
[epoch12, step360]: loss 0.438750
[epoch12, step361]: loss 0.414719
[epoch12, step362]: loss 0.266895
[epoch12, step363]: loss 0.365622
[epoch12, step364]: loss 0.436556
[epoch12, step365]: loss 0.487452
[epoch12, step366]: loss 0.394521
[epoch12, step367]: loss 0.568655
[epoch12, step368]: loss 0.560499
[epoch12, step369]: loss 0.282531
[epoch12, step370]: loss 0.282491
[epoch12, step371]: loss 0.511447
[epoch12, step372]: loss 0.357014
[epoch12, step373]: loss 0.759989
[epoch12, step374]: loss 0.427025
[epoch12, step375]: loss 0.591180
[epoch12, step376]: loss 0.531720
[epoch12, step377]: loss 0.364955
[epoch12, step378]: loss 0.438772
[epoch12, step379]: loss 0.558567
[epoch12, step380]: loss 0.478248
[epoch12, step381]: loss 0.555598
[epoch12, step382]: loss 0.530885
[epoch12, step383]: loss 0.535908
[epoch12, step384]: loss 0.580471
[epoch12, step385]: loss 0.692132
[epoch12, step386]: loss 0.502236
[epoch12, step387]: loss 0.578523
[epoch12, step388]: loss 0.407311
[epoch12, step389]: loss 0.466254
[epoch12, step390]: loss 0.262840
[epoch12, step391]: loss 0.513434
[epoch12, step392]: loss 0.721413
[epoch12, step393]: loss 0.395447
[epoch12, step394]: loss 0.339045
[epoch12, step395]: loss 0.395160
[epoch12, step396]: loss 0.433493
[epoch12, step397]: loss 0.585398
[epoch12, step398]: loss 0.518751
[epoch12, step399]: loss 0.636470
[epoch12, step400]: loss 0.369179
[epoch12, step401]: loss 0.616048
[epoch12, step402]: loss 0.655474
[epoch12, step403]: loss 0.698757
[epoch12, step404]: loss 0.614776
[epoch12, step405]: loss 0.449581
[epoch12, step406]: loss 0.450589
[epoch12, step407]: loss 0.495750
[epoch12, step408]: loss 0.268406
[epoch12, step409]: loss 0.396192
[epoch12, step410]: loss 0.521694
[epoch12, step411]: loss 0.299233
[epoch12, step412]: loss 0.534447
[epoch12, step413]: loss 0.425925
[epoch12, step414]: loss 0.553263
[epoch12, step415]: loss 0.482220
[epoch12, step416]: loss 0.579637
[epoch12, step417]: loss 0.523546
[epoch12, step418]: loss 0.564681
[epoch12, step419]: loss 0.500943
[epoch12, step420]: loss 0.247345
[epoch12, step421]: loss 0.308494
[epoch12, step422]: loss 0.486341
[epoch12, step423]: loss 0.496960
[epoch12, step424]: loss 0.713268
[epoch12, step425]: loss 0.405048
[epoch12, step426]: loss 0.631423
[epoch12, step427]: loss 0.257420
[epoch12, step428]: loss 0.529294
[epoch12, step429]: loss 0.516349
[epoch12, step430]: loss 0.549179
[epoch12, step431]: loss 0.284129
[epoch12, step432]: loss 0.366454
[epoch12, step433]: loss 0.583035
[epoch12, step434]: loss 0.588523
[epoch12, step435]: loss 0.623330
[epoch12, step436]: loss 0.630095
[epoch12, step437]: loss 0.563163
[epoch12, step438]: loss 0.446809
[epoch12, step439]: loss 0.592174
[epoch12, step440]: loss 0.483192
[epoch12, step441]: loss 0.324100
[epoch12, step442]: loss 0.660475
[epoch12, step443]: loss 0.470230
[epoch12, step444]: loss 0.462094
[epoch12, step445]: loss 0.323931
[epoch12, step446]: loss 0.304487
[epoch12, step447]: loss 0.527403
[epoch12, step448]: loss 0.468986
[epoch12, step449]: loss 0.514004
[epoch12, step450]: loss 0.300336
[epoch12, step451]: loss 0.555349
[epoch12, step452]: loss 0.662771
[epoch12, step453]: loss 0.511505
[epoch12, step454]: loss 0.532372
[epoch12, step455]: loss 0.531602
[epoch12, step456]: loss 0.516708
[epoch12, step457]: loss 0.282329
[epoch12, step458]: loss 0.303243
[epoch12, step459]: loss 0.457529
[epoch12, step460]: loss 0.738208
[epoch12, step461]: loss 0.365904
[epoch12, step462]: loss 0.602280
[epoch12, step463]: loss 0.506543
[epoch12, step464]: loss 0.529608
[epoch12, step465]: loss 0.468696
[epoch12, step466]: loss 0.461156
[epoch12, step467]: loss 0.563271
[epoch12, step468]: loss 0.490234
[epoch12, step469]: loss 0.496626
[epoch12, step470]: loss 0.677600
[epoch12, step471]: loss 0.572351
[epoch12, step472]: loss 0.551853
[epoch12, step473]: loss 0.616316
[epoch12, step474]: loss 0.721512
[epoch12, step475]: loss 0.425836
[epoch12, step476]: loss 0.455580
[epoch12, step477]: loss 0.457575
[epoch12, step478]: loss 0.676266
[epoch12, step479]: loss 0.360984
[epoch12, step480]: loss 0.577595
[epoch12, step481]: loss 0.461801
[epoch12, step482]: loss 0.323499
[epoch12, step483]: loss 0.636676
[epoch12, step484]: loss 0.458417
[epoch12, step485]: loss 0.509443
[epoch12, step486]: loss 0.466111
[epoch12, step487]: loss 0.668307
[epoch12, step488]: loss 0.541546
[epoch12, step489]: loss 0.581621
[epoch12, step490]: loss 0.478300
[epoch12, step491]: loss 0.535534
[epoch12, step492]: loss 0.732018
[epoch12, step493]: loss 0.277734
[epoch12, step494]: loss 0.117543
[epoch12, step495]: loss 0.632887
[epoch12, step496]: loss 0.618048
[epoch12, step497]: loss 0.368206
[epoch12, step498]: loss 0.608809
[epoch12, step499]: loss 0.441923
[epoch12, step500]: loss 0.523131
[epoch12, step501]: loss 0.477466
[epoch12, step502]: loss 0.586939
[epoch12, step503]: loss 0.386756
[epoch12, step504]: loss 0.424769
[epoch12, step505]: loss 0.375854
[epoch12, step506]: loss 0.765383
[epoch12, step507]: loss 0.698518
[epoch12, step508]: loss 0.524888
[epoch12, step509]: loss 0.427282
[epoch12, step510]: loss 0.306317
[epoch12, step511]: loss 0.475623
[epoch12, step512]: loss 0.109199
[epoch12, step513]: loss 0.375563
[epoch12, step514]: loss 0.361038
[epoch12, step515]: loss 0.665649
[epoch12, step516]: loss 0.365155
[epoch12, step517]: loss 0.311084
[epoch12, step518]: loss 0.434430
[epoch12, step519]: loss 0.444730
[epoch12, step520]: loss 0.391405
[epoch12, step521]: loss 0.695355
[epoch12, step522]: loss 0.467130
[epoch12, step523]: loss 0.471532
[epoch12, step524]: loss 0.554237
[epoch12, step525]: loss 0.452783
[epoch12, step526]: loss 0.334641
[epoch12, step527]: loss 0.497532
[epoch12, step528]: loss 0.605845
[epoch12, step529]: loss 0.297040
[epoch12, step530]: loss 0.498406
[epoch12, step531]: loss 0.352350
[epoch12, step532]: loss 0.381686
[epoch12, step533]: loss 0.441166
[epoch12, step534]: loss 0.540589
[epoch12, step535]: loss 0.377015
[epoch12, step536]: loss 0.400448
[epoch12, step537]: loss 0.657539
[epoch12, step538]: loss 0.490326
[epoch12, step539]: loss 0.518950
[epoch12, step540]: loss 0.398315
[epoch12, step541]: loss 0.575260
[epoch12, step542]: loss 0.342913
[epoch12, step543]: loss 0.385474
[epoch12, step544]: loss 0.503945
[epoch12, step545]: loss 0.581319
[epoch12, step546]: loss 0.492764
[epoch12, step547]: loss 0.660227
[epoch12, step548]: loss 0.378809
[epoch12, step549]: loss 0.553342
[epoch12, step550]: loss 0.556687
[epoch12, step551]: loss 0.387785
[epoch12, step552]: loss 0.577368
[epoch12, step553]: loss 0.569222
[epoch12, step554]: loss 0.425345
[epoch12, step555]: loss 0.398439
[epoch12, step556]: loss 0.657065
[epoch12, step557]: loss 0.557838
[epoch12, step558]: loss 0.564540
[epoch12, step559]: loss 0.407117
[epoch12, step560]: loss 0.699405
[epoch12, step561]: loss 0.484442
[epoch12, step562]: loss 0.600658
[epoch12, step563]: loss 0.252702
[epoch12, step564]: loss 0.357619
[epoch12, step565]: loss 0.489388
[epoch12, step566]: loss 0.605268
[epoch12, step567]: loss 0.593508
[epoch12, step568]: loss 0.429662
[epoch12, step569]: loss 0.430208
[epoch12, step570]: loss 0.417200
[epoch12, step571]: loss 0.510256
[epoch12, step572]: loss 0.642720
[epoch12, step573]: loss 0.616100
[epoch12, step574]: loss 0.514350
[epoch12, step575]: loss 0.431976
[epoch12, step576]: loss 0.180920
[epoch12, step577]: loss 0.717516
[epoch12, step578]: loss 0.232254
[epoch12, step579]: loss 0.345285
[epoch12, step580]: loss 0.419131
[epoch12, step581]: loss 0.520500
[epoch12, step582]: loss 0.466349
[epoch12, step583]: loss 0.403818
[epoch12, step584]: loss 0.257529
[epoch12, step585]: loss 0.426490
[epoch12, step586]: loss 0.654632
[epoch12, step587]: loss 0.395189
[epoch12, step588]: loss 0.513792
[epoch12, step589]: loss 0.632048
[epoch12, step590]: loss 0.493918
[epoch12, step591]: loss 0.509043
[epoch12, step592]: loss 0.269071
[epoch12, step593]: loss 0.423639
[epoch12, step594]: loss 0.494187
[epoch12, step595]: loss 0.119801
[epoch12, step596]: loss 0.514877
[epoch12, step597]: loss 0.389465
[epoch12, step598]: loss 0.300209
[epoch12, step599]: loss 0.360138
[epoch12, step600]: loss 0.359583
[epoch12, step601]: loss 0.423986
[epoch12, step602]: loss 0.389188
[epoch12, step603]: loss 0.541821
[epoch12, step604]: loss 0.516355
[epoch12, step605]: loss 0.576089
[epoch12, step606]: loss 0.408328
[epoch12, step607]: loss 0.597517
[epoch12, step608]: loss 0.332211
[epoch12, step609]: loss 0.404491
[epoch12, step610]: loss 0.276628
[epoch12, step611]: loss 0.597660
[epoch12, step612]: loss 0.554251
[epoch12, step613]: loss 0.303761
[epoch12, step614]: loss 0.446389
[epoch12, step615]: loss 0.458288
[epoch12, step616]: loss 0.303333
[epoch12, step617]: loss 0.757488
[epoch12, step618]: loss 0.551591
[epoch12, step619]: loss 0.310135
[epoch12, step620]: loss 0.593619
[epoch12, step621]: loss 0.531495
[epoch12, step622]: loss 0.627819
[epoch12, step623]: loss 0.481798
[epoch12, step624]: loss 0.411944
[epoch12, step625]: loss 0.526797
[epoch12, step626]: loss 0.482587
[epoch12, step627]: loss 0.490451
[epoch12, step628]: loss 0.370299
[epoch12, step629]: loss 0.721937
[epoch12, step630]: loss 0.742796
[epoch12, step631]: loss 0.515143
[epoch12, step632]: loss 0.379635
[epoch12, step633]: loss 0.532892
[epoch12, step634]: loss 0.518920
[epoch12, step635]: loss 0.260055
[epoch12, step636]: loss 0.670410
[epoch12, step637]: loss 0.563678
[epoch12, step638]: loss 0.445517
[epoch12, step639]: loss 0.741177
[epoch12, step640]: loss 0.450305
[epoch12, step641]: loss 0.610507
[epoch12, step642]: loss 0.576084
[epoch12, step643]: loss 0.695938
[epoch12, step644]: loss 0.282191
[epoch12, step645]: loss 0.640664
[epoch12, step646]: loss 0.589569
[epoch12, step647]: loss 0.616473
[epoch12, step648]: loss 0.566119
[epoch12, step649]: loss 0.747748
[epoch12, step650]: loss 0.697681
[epoch12, step651]: loss 0.288884
[epoch12, step652]: loss 0.377587
[epoch12, step653]: loss 0.634745
[epoch12, step654]: loss 0.551502
[epoch12, step655]: loss 0.636528
[epoch12, step656]: loss 0.476029
[epoch12, step657]: loss 0.498056
[epoch12, step658]: loss 0.591457
[epoch12, step659]: loss 0.413366
[epoch12, step660]: loss 0.649263
[epoch12, step661]: loss 0.290074
[epoch12, step662]: loss 0.397656
[epoch12, step663]: loss 0.510768
[epoch12, step664]: loss 0.569680
[epoch12, step665]: loss 0.112586
[epoch12, step666]: loss 0.421228
[epoch12, step667]: loss 0.586018
[epoch12, step668]: loss 0.662272
[epoch12, step669]: loss 0.421893
[epoch12, step670]: loss 0.472539
[epoch12, step671]: loss 0.588098
[epoch12, step672]: loss 0.534672
[epoch12, step673]: loss 0.291397
[epoch12, step674]: loss 0.497398
[epoch12, step675]: loss 0.716449
[epoch12, step676]: loss 0.506726
[epoch12, step677]: loss 0.262823
[epoch12, step678]: loss 0.613384
[epoch12, step679]: loss 0.635813
[epoch12, step680]: loss 0.579471
[epoch12, step681]: loss 0.628878
[epoch12, step682]: loss 0.652008
[epoch12, step683]: loss 0.525505
[epoch12, step684]: loss 0.419181
[epoch12, step685]: loss 0.675958
[epoch12, step686]: loss 0.400037
[epoch12, step687]: loss 0.524833
[epoch12, step688]: loss 0.760094
[epoch12, step689]: loss 0.606970
[epoch12, step690]: loss 0.622429
[epoch12, step691]: loss 0.294363
[epoch12, step692]: loss 0.286591
[epoch12, step693]: loss 0.583415
[epoch12, step694]: loss 0.397283
[epoch12, step695]: loss 0.380684
[epoch12, step696]: loss 0.627053
[epoch12, step697]: loss 0.633735
[epoch12, step698]: loss 0.498070
[epoch12, step699]: loss 0.535642
[epoch12, step700]: loss 0.475290
[epoch12, step701]: loss 0.408259
[epoch12, step702]: loss 0.449204
[epoch12, step703]: loss 0.645718
[epoch12, step704]: loss 0.250142
[epoch12, step705]: loss 0.443581
[epoch12, step706]: loss 0.312005
[epoch12, step707]: loss 0.436568
[epoch12, step708]: loss 0.459535
[epoch12, step709]: loss 0.586452
[epoch12, step710]: loss 0.482138
[epoch12, step711]: loss 0.563162
[epoch12, step712]: loss 0.451958
[epoch12, step713]: loss 0.471935
[epoch12, step714]: loss 0.122392
[epoch12, step715]: loss 0.468211
[epoch12, step716]: loss 0.371694
[epoch12, step717]: loss 0.693611
[epoch12, step718]: loss 0.403948
[epoch12, step719]: loss 0.578058
[epoch12, step720]: loss 0.472202
[epoch12, step721]: loss 0.829742
[epoch12, step722]: loss 0.610974
[epoch12, step723]: loss 0.484598
[epoch12, step724]: loss 0.279467
[epoch12, step725]: loss 0.513918
[epoch12, step726]: loss 0.605014
[epoch12, step727]: loss 0.397251
[epoch12, step728]: loss 0.540716
[epoch12, step729]: loss 0.505832
[epoch12, step730]: loss 0.600264
[epoch12, step731]: loss 0.459489
[epoch12, step732]: loss 0.660864
[epoch12, step733]: loss 0.662979
[epoch12, step734]: loss 0.646569
[epoch12, step735]: loss 0.350800
[epoch12, step736]: loss 0.264668
[epoch12, step737]: loss 0.321016
[epoch12, step738]: loss 0.674470
[epoch12, step739]: loss 0.661768
[epoch12, step740]: loss 0.279633
[epoch12, step741]: loss 0.706353
[epoch12, step742]: loss 0.483237
[epoch12, step743]: loss 0.352101
[epoch12, step744]: loss 0.315932
[epoch12, step745]: loss 0.313874
[epoch12, step746]: loss 0.442176
[epoch12, step747]: loss 0.505173
[epoch12, step748]: loss 0.464357
[epoch12, step749]: loss 0.476893
[epoch12, step750]: loss 0.685088
[epoch12, step751]: loss 0.576913
[epoch12, step752]: loss 0.461382
[epoch12, step753]: loss 0.435583
[epoch12, step754]: loss 0.492839
[epoch12, step755]: loss 0.335609
[epoch12, step756]: loss 0.130297
[epoch12, step757]: loss 0.528717
[epoch12, step758]: loss 0.277435
[epoch12, step759]: loss 0.584627
[epoch12, step760]: loss 0.612613
[epoch12, step761]: loss 0.271206
[epoch12, step762]: loss 0.816042
[epoch12, step763]: loss 0.676507
[epoch12, step764]: loss 0.352804
[epoch12, step765]: loss 0.566510
[epoch12, step766]: loss 0.510615
[epoch12, step767]: loss 0.514949
[epoch12, step768]: loss 0.472068
[epoch12, step769]: loss 0.356232
[epoch12, step770]: loss 0.746149
[epoch12, step771]: loss 0.505664
[epoch12, step772]: loss 0.548160
[epoch12, step773]: loss 0.459113
[epoch12, step774]: loss 0.633677
[epoch12, step775]: loss 0.550453
[epoch12, step776]: loss 0.709961
[epoch12, step777]: loss 0.366867
[epoch12, step778]: loss 0.565478
[epoch12, step779]: loss 0.484291
[epoch12, step780]: loss 0.516656
[epoch12, step781]: loss 0.340296
[epoch12, step782]: loss 0.627493
[epoch12, step783]: loss 0.380997
[epoch12, step784]: loss 0.385848
[epoch12, step785]: loss 0.378935
[epoch12, step786]: loss 0.791720
[epoch12, step787]: loss 0.531080
[epoch12, step788]: loss 0.579951
[epoch12, step789]: loss 0.618050
[epoch12, step790]: loss 0.400986
[epoch12, step791]: loss 0.279553
[epoch12, step792]: loss 0.714841
[epoch12, step793]: loss 0.545048
[epoch12, step794]: loss 0.382201
[epoch12, step795]: loss 0.484224
[epoch12, step796]: loss 0.370504
[epoch12, step797]: loss 0.600075
[epoch12, step798]: loss 0.656055
[epoch12, step799]: loss 0.330595
[epoch12, step800]: loss 0.537270
[epoch12, step801]: loss 0.479294
[epoch12, step802]: loss 0.322797
[epoch12, step803]: loss 0.574596
[epoch12, step804]: loss 0.548224
[epoch12, step805]: loss 0.571220
[epoch12, step806]: loss 0.485108
[epoch12, step807]: loss 0.410950
[epoch12, step808]: loss 0.612484
[epoch12, step809]: loss 0.371670
[epoch12, step810]: loss 0.499462
[epoch12, step811]: loss 0.662219
[epoch12, step812]: loss 0.640979
[epoch12, step813]: loss 0.325312
[epoch12, step814]: loss 0.546542
[epoch12, step815]: loss 0.666334
[epoch12, step816]: loss 0.431193
[epoch12, step817]: loss 0.506082
[epoch12, step818]: loss 0.334815
[epoch12, step819]: loss 0.577672
[epoch12, step820]: loss 0.507344
[epoch12, step821]: loss 0.412443
[epoch12, step822]: loss 0.703640
[epoch12, step823]: loss 0.323484
[epoch12, step824]: loss 0.640195
[epoch12, step825]: loss 0.607044
[epoch12, step826]: loss 0.364128
[epoch12, step827]: loss 0.504988
[epoch12, step828]: loss 0.563139
[epoch12, step829]: loss 0.318715
[epoch12, step830]: loss 0.371799
[epoch12, step831]: loss 0.571790
[epoch12, step832]: loss 0.302101
[epoch12, step833]: loss 0.809601
[epoch12, step834]: loss 0.303428
[epoch12, step835]: loss 0.547055
[epoch12, step836]: loss 0.470529
[epoch12, step837]: loss 0.173359
[epoch12, step838]: loss 0.418585
[epoch12, step839]: loss 0.192825
[epoch12, step840]: loss 0.434418
[epoch12, step841]: loss 0.525265
[epoch12, step842]: loss 0.451901
[epoch12, step843]: loss 0.514409
[epoch12, step844]: loss 0.390880
[epoch12, step845]: loss 0.719418
[epoch12, step846]: loss 0.377821
[epoch12, step847]: loss 0.438602
[epoch12, step848]: loss 0.489197
[epoch12, step849]: loss 0.500632
[epoch12, step850]: loss 0.570508
[epoch12, step851]: loss 0.617986
[epoch12, step852]: loss 0.431173
[epoch12, step853]: loss 0.632176
[epoch12, step854]: loss 0.607988
[epoch12, step855]: loss 0.427152
[epoch12, step856]: loss 0.532411
[epoch12, step857]: loss 0.371918
[epoch12, step858]: loss 0.386898
[epoch12, step859]: loss 0.648574
[epoch12, step860]: loss 0.654673
[epoch12, step861]: loss 0.359346
[epoch12, step862]: loss 0.481626
[epoch12, step863]: loss 0.401891
[epoch12, step864]: loss 0.672978
[epoch12, step865]: loss 0.273289
[epoch12, step866]: loss 0.440515
[epoch12, step867]: loss 0.413758
[epoch12, step868]: loss 0.462693
[epoch12, step869]: loss 0.382280
[epoch12, step870]: loss 0.387439
[epoch12, step871]: loss 0.320421
[epoch12, step872]: loss 0.526447
[epoch12, step873]: loss 0.387225
[epoch12, step874]: loss 0.624644
[epoch12, step875]: loss 0.726383
[epoch12, step876]: loss 0.608732
[epoch12, step877]: loss 0.496013
[epoch12, step878]: loss 0.496455
[epoch12, step879]: loss 0.450485
[epoch12, step880]: loss 0.460951
[epoch12, step881]: loss 0.549845
[epoch12, step882]: loss 0.340150
[epoch12, step883]: loss 0.398808
[epoch12, step884]: loss 0.486850
[epoch12, step885]: loss 0.518173
[epoch12, step886]: loss 0.380094
[epoch12, step887]: loss 0.657668
[epoch12, step888]: loss 0.371042
[epoch12, step889]: loss 0.505352
[epoch12, step890]: loss 0.357291
[epoch12, step891]: loss 0.693417
[epoch12, step892]: loss 0.613204
[epoch12, step893]: loss 0.435699
[epoch12, step894]: loss 0.357593
[epoch12, step895]: loss 0.643328
[epoch12, step896]: loss 0.438349
[epoch12, step897]: loss 0.533559
[epoch12, step898]: loss 0.721442
[epoch12, step899]: loss 0.477936
[epoch12, step900]: loss 0.409101
[epoch12, step901]: loss 0.471766
[epoch12, step902]: loss 0.587670
[epoch12, step903]: loss 0.204692
[epoch12, step904]: loss 0.405886
[epoch12, step905]: loss 0.494334
[epoch12, step906]: loss 0.604737
[epoch12, step907]: loss 0.416943
[epoch12, step908]: loss 0.335544
[epoch12, step909]: loss 0.398912
[epoch12, step910]: loss 0.576549
[epoch12, step911]: loss 0.303129
[epoch12, step912]: loss 0.352107
[epoch12, step913]: loss 0.352537
[epoch12, step914]: loss 0.402343
[epoch12, step915]: loss 0.560800
[epoch12, step916]: loss 0.392428
[epoch12, step917]: loss 0.505551
[epoch12, step918]: loss 0.743903
[epoch12, step919]: loss 0.663996
[epoch12, step920]: loss 0.256847
[epoch12, step921]: loss 0.707580
[epoch12, step922]: loss 0.573106
[epoch12, step923]: loss 0.600048
[epoch12, step924]: loss 0.371770
[epoch12, step925]: loss 0.462883
[epoch12, step926]: loss 0.508088
[epoch12, step927]: loss 0.654849
[epoch12, step928]: loss 0.504446
[epoch12, step929]: loss 0.402519
[epoch12, step930]: loss 0.515298
[epoch12, step931]: loss 0.613560
[epoch12, step932]: loss 0.228506
[epoch12, step933]: loss 0.477612
[epoch12, step934]: loss 0.363085
[epoch12, step935]: loss 0.521932
[epoch12, step936]: loss 0.469281
[epoch12, step937]: loss 0.419131
[epoch12, step938]: loss 0.263980
[epoch12, step939]: loss 0.620755
[epoch12, step940]: loss 0.748060
[epoch12, step941]: loss 0.536393
[epoch12, step942]: loss 0.442435
[epoch12, step943]: loss 0.555677
[epoch12, step944]: loss 0.711756
[epoch12, step945]: loss 0.761583
[epoch12, step946]: loss 0.613104
[epoch12, step947]: loss 0.442347
[epoch12, step948]: loss 0.399835
[epoch12, step949]: loss 0.427475
[epoch12, step950]: loss 0.653855
[epoch12, step951]: loss 0.425628
[epoch12, step952]: loss 0.356934
[epoch12, step953]: loss 0.536867
[epoch12, step954]: loss 0.398127
[epoch12, step955]: loss 0.310813
[epoch12, step956]: loss 0.380385
[epoch12, step957]: loss 0.577664
[epoch12, step958]: loss 0.534888
[epoch12, step959]: loss 0.508121
[epoch12, step960]: loss 0.445095
[epoch12, step961]: loss 0.574832
[epoch12, step962]: loss 0.467943
[epoch12, step963]: loss 0.526845
[epoch12, step964]: loss 0.670332
[epoch12, step965]: loss 0.238491
[epoch12, step966]: loss 0.472627
[epoch12, step967]: loss 0.489017
[epoch12, step968]: loss 0.649534
[epoch12, step969]: loss 0.536974
[epoch12, step970]: loss 0.622080
[epoch12, step971]: loss 0.696830
[epoch12, step972]: loss 0.497636
[epoch12, step973]: loss 0.336991
[epoch12, step974]: loss 0.733562
[epoch12, step975]: loss 0.362139
[epoch12, step976]: loss 0.669912
[epoch12, step977]: loss 0.369098
[epoch12, step978]: loss 0.523177
[epoch12, step979]: loss 0.350920
[epoch12, step980]: loss 0.283125
[epoch12, step981]: loss 0.351474
[epoch12, step982]: loss 0.475209
[epoch12, step983]: loss 0.251865
[epoch12, step984]: loss 0.519216
[epoch12, step985]: loss 0.668864
[epoch12, step986]: loss 0.699277
[epoch12, step987]: loss 0.572844
[epoch12, step988]: loss 0.420708
[epoch12, step989]: loss 0.628570
[epoch12, step990]: loss 0.540984
[epoch12, step991]: loss 0.462950
[epoch12, step992]: loss 0.522475
[epoch12, step993]: loss 0.221310
[epoch12, step994]: loss 0.255244
[epoch12, step995]: loss 0.508435
[epoch12, step996]: loss 0.598332
[epoch12, step997]: loss 0.535413
[epoch12, step998]: loss 0.250407
[epoch12, step999]: loss 0.550386
[epoch12, step1000]: loss 0.370109
[epoch12, step1001]: loss 0.600675
[epoch12, step1002]: loss 0.563211
[epoch12, step1003]: loss 0.406781
[epoch12, step1004]: loss 0.601691
[epoch12, step1005]: loss 0.451516
[epoch12, step1006]: loss 0.414910
[epoch12, step1007]: loss 0.413007
[epoch12, step1008]: loss 0.394113
[epoch12, step1009]: loss 0.366625
[epoch12, step1010]: loss 0.485645
[epoch12, step1011]: loss 0.475867
[epoch12, step1012]: loss 0.501842
[epoch12, step1013]: loss 0.624056
[epoch12, step1014]: loss 0.484957
[epoch12, step1015]: loss 0.323861
[epoch12, step1016]: loss 0.304963
[epoch12, step1017]: loss 0.703010
[epoch12, step1018]: loss 0.455693
[epoch12, step1019]: loss 0.608219
[epoch12, step1020]: loss 0.367572
[epoch12, step1021]: loss 0.444974
[epoch12, step1022]: loss 0.596990
[epoch12, step1023]: loss 0.404019
[epoch12, step1024]: loss 0.604107
[epoch12, step1025]: loss 0.284781
[epoch12, step1026]: loss 0.512002
[epoch12, step1027]: loss 0.367078
[epoch12, step1028]: loss 0.424676
[epoch12, step1029]: loss 0.601505
[epoch12, step1030]: loss 0.595379
[epoch12, step1031]: loss 0.365128
[epoch12, step1032]: loss 0.530935
[epoch12, step1033]: loss 0.640524
[epoch12, step1034]: loss 0.271702
[epoch12, step1035]: loss 0.519916
[epoch12, step1036]: loss 0.627183
[epoch12, step1037]: loss 0.498139
[epoch12, step1038]: loss 0.434346
[epoch12, step1039]: loss 0.467336
[epoch12, step1040]: loss 0.268193
[epoch12, step1041]: loss 0.607236
[epoch12, step1042]: loss 0.401170
[epoch12, step1043]: loss 0.521640
[epoch12, step1044]: loss 0.528521
[epoch12, step1045]: loss 0.695689
[epoch12, step1046]: loss 0.426367
[epoch12, step1047]: loss 0.468539
[epoch12, step1048]: loss 0.462507
[epoch12, step1049]: loss 0.436367
[epoch12, step1050]: loss 0.503838
[epoch12, step1051]: loss 0.461142
[epoch12, step1052]: loss 0.430617
[epoch12, step1053]: loss 0.365484
[epoch12, step1054]: loss 0.726674
[epoch12, step1055]: loss 0.398056
[epoch12, step1056]: loss 0.591440
[epoch12, step1057]: loss 0.646267
[epoch12, step1058]: loss 0.468342
[epoch12, step1059]: loss 0.676458
[epoch12, step1060]: loss 0.317796
[epoch12, step1061]: loss 0.617600
[epoch12, step1062]: loss 0.260327
[epoch12, step1063]: loss 0.649305
[epoch12, step1064]: loss 0.669785
[epoch12, step1065]: loss 0.623733
[epoch12, step1066]: loss 0.333152
[epoch12, step1067]: loss 0.423570
[epoch12, step1068]: loss 0.486142
[epoch12, step1069]: loss 0.629347
[epoch12, step1070]: loss 0.530037
[epoch12, step1071]: loss 0.341588
[epoch12, step1072]: loss 0.531692
[epoch12, step1073]: loss 0.248213
[epoch12, step1074]: loss 0.477743
[epoch12, step1075]: loss 0.622240
[epoch12, step1076]: loss 0.577463
[epoch12, step1077]: loss 0.527704
[epoch12, step1078]: loss 0.471200
[epoch12, step1079]: loss 0.533413
[epoch12, step1080]: loss 0.611683
[epoch12, step1081]: loss 0.441109
[epoch12, step1082]: loss 0.606304
[epoch12, step1083]: loss 0.386665
[epoch12, step1084]: loss 0.633732
[epoch12, step1085]: loss 0.386727
[epoch12, step1086]: loss 0.674606
[epoch12, step1087]: loss 0.515258
[epoch12, step1088]: loss 0.270444
[epoch12, step1089]: loss 0.621629
[epoch12, step1090]: loss 0.482866
[epoch12, step1091]: loss 0.426993
[epoch12, step1092]: loss 0.530871
[epoch12, step1093]: loss 0.570322
[epoch12, step1094]: loss 0.593413
[epoch12, step1095]: loss 0.575868
[epoch12, step1096]: loss 0.576131
[epoch12, step1097]: loss 0.551161
[epoch12, step1098]: loss 0.557127
[epoch12, step1099]: loss 0.668423
[epoch12, step1100]: loss 0.636645
[epoch12, step1101]: loss 0.674186
[epoch12, step1102]: loss 0.636821
[epoch12, step1103]: loss 0.395000
[epoch12, step1104]: loss 0.447284
[epoch12, step1105]: loss 0.647904
[epoch12, step1106]: loss 0.343693
[epoch12, step1107]: loss 0.739858
[epoch12, step1108]: loss 0.664085
[epoch12, step1109]: loss 0.411158
[epoch12, step1110]: loss 0.450087
[epoch12, step1111]: loss 0.595247
[epoch12, step1112]: loss 0.386734
[epoch12, step1113]: loss 0.274641
[epoch12, step1114]: loss 0.337984
[epoch12, step1115]: loss 0.485679
[epoch12, step1116]: loss 0.532568
[epoch12, step1117]: loss 0.442304
[epoch12, step1118]: loss 0.320815
[epoch12, step1119]: loss 0.317584
[epoch12, step1120]: loss 0.304972
[epoch12, step1121]: loss 0.475684
[epoch12, step1122]: loss 0.609530
[epoch12, step1123]: loss 0.429962
[epoch12, step1124]: loss 0.347351
[epoch12, step1125]: loss 0.472761
[epoch12, step1126]: loss 0.461005
[epoch12, step1127]: loss 0.439192
[epoch12, step1128]: loss 0.552992
[epoch12, step1129]: loss 0.467824
[epoch12, step1130]: loss 0.307343
[epoch12, step1131]: loss 0.601299
[epoch12, step1132]: loss 0.441869
[epoch12, step1133]: loss 0.447725
[epoch12, step1134]: loss 0.348552
[epoch12, step1135]: loss 0.192170
[epoch12, step1136]: loss 0.438728
[epoch12, step1137]: loss 0.533626
[epoch12, step1138]: loss 0.509834
[epoch12, step1139]: loss 0.399588
[epoch12, step1140]: loss 0.510816
[epoch12, step1141]: loss 0.379040
[epoch12, step1142]: loss 0.732638
[epoch12, step1143]: loss 0.629242
[epoch12, step1144]: loss 0.402987
[epoch12, step1145]: loss 0.114008
[epoch12, step1146]: loss 0.500577
[epoch12, step1147]: loss 0.705009
[epoch12, step1148]: loss 0.673112
[epoch12, step1149]: loss 0.452952
[epoch12, step1150]: loss 0.711959
[epoch12, step1151]: loss 0.389001
[epoch12, step1152]: loss 0.445779
[epoch12, step1153]: loss 0.495792
[epoch12, step1154]: loss 0.473797
[epoch12, step1155]: loss 0.509330
[epoch12, step1156]: loss 0.527159
[epoch12, step1157]: loss 0.780257
[epoch12, step1158]: loss 0.304677
[epoch12, step1159]: loss 0.566060
[epoch12, step1160]: loss 0.431087
[epoch12, step1161]: loss 0.486964
[epoch12, step1162]: loss 0.335106
[epoch12, step1163]: loss 0.147554
[epoch12, step1164]: loss 0.510486
[epoch12, step1165]: loss 0.257908
[epoch12, step1166]: loss 0.606427
[epoch12, step1167]: loss 0.407674
[epoch12, step1168]: loss 0.544302
[epoch12, step1169]: loss 0.611377
[epoch12, step1170]: loss 0.574052
[epoch12, step1171]: loss 0.506712
[epoch12, step1172]: loss 0.404641
[epoch12, step1173]: loss 0.568638
[epoch12, step1174]: loss 0.730312
[epoch12, step1175]: loss 0.351112
[epoch12, step1176]: loss 0.476570
[epoch12, step1177]: loss 0.521099
[epoch12, step1178]: loss 0.655160
[epoch12, step1179]: loss 0.565741
[epoch12, step1180]: loss 0.433911
[epoch12, step1181]: loss 0.452459
[epoch12, step1182]: loss 0.407387
[epoch12, step1183]: loss 0.355281
[epoch12, step1184]: loss 0.426481
[epoch12, step1185]: loss 0.609506
[epoch12, step1186]: loss 0.533712
[epoch12, step1187]: loss 0.712954
[epoch12, step1188]: loss 0.627843
[epoch12, step1189]: loss 0.473333
[epoch12, step1190]: loss 0.387814
[epoch12, step1191]: loss 0.525289
[epoch12, step1192]: loss 0.527544
[epoch12, step1193]: loss 0.680312
[epoch12, step1194]: loss 0.270495
[epoch12, step1195]: loss 0.466046
[epoch12, step1196]: loss 0.628842
[epoch12, step1197]: loss 0.618150
[epoch12, step1198]: loss 0.523382
[epoch12, step1199]: loss 0.519847
[epoch12, step1200]: loss 0.564199
[epoch12, step1201]: loss 0.777635
[epoch12, step1202]: loss 0.592571
[epoch12, step1203]: loss 0.350501
[epoch12, step1204]: loss 0.386541
[epoch12, step1205]: loss 0.315198
[epoch12, step1206]: loss 0.327077
[epoch12, step1207]: loss 0.712131
[epoch12, step1208]: loss 0.379887
[epoch12, step1209]: loss 0.430005
[epoch12, step1210]: loss 0.526300
[epoch12, step1211]: loss 0.447193
[epoch12, step1212]: loss 0.553255
[epoch12, step1213]: loss 0.680772
[epoch12, step1214]: loss 0.158091
[epoch12, step1215]: loss 0.730628
[epoch12, step1216]: loss 0.438993
[epoch12, step1217]: loss 0.142102
[epoch12, step1218]: loss 0.577919
[epoch12, step1219]: loss 0.352842
[epoch12, step1220]: loss 0.382090
[epoch12, step1221]: loss 0.399736
[epoch12, step1222]: loss 0.530834
[epoch12, step1223]: loss 0.583126
[epoch12, step1224]: loss 0.530828
[epoch12, step1225]: loss 0.566912
[epoch12, step1226]: loss 0.475542
[epoch12, step1227]: loss 0.447530
[epoch12, step1228]: loss 0.380945
[epoch12, step1229]: loss 0.406363
[epoch12, step1230]: loss 0.560563
[epoch12, step1231]: loss 0.599149
[epoch12, step1232]: loss 0.156054
[epoch12, step1233]: loss 0.278897
[epoch12, step1234]: loss 0.419984
[epoch12, step1235]: loss 0.630747
[epoch12, step1236]: loss 0.585114
[epoch12, step1237]: loss 0.647710
[epoch12, step1238]: loss 0.271126
[epoch12, step1239]: loss 0.705221
[epoch12, step1240]: loss 0.426982
[epoch12, step1241]: loss 0.486400
[epoch12, step1242]: loss 0.427107
[epoch12, step1243]: loss 0.617765
[epoch12, step1244]: loss 0.111251
[epoch12, step1245]: loss 0.524203
[epoch12, step1246]: loss 0.680033
[epoch12, step1247]: loss 0.394066
[epoch12, step1248]: loss 0.687989
[epoch12, step1249]: loss 0.580217
[epoch12, step1250]: loss 0.596591
[epoch12, step1251]: loss 0.570383
[epoch12, step1252]: loss 0.637435
[epoch12, step1253]: loss 0.575825
[epoch12, step1254]: loss 0.394566
[epoch12, step1255]: loss 0.558460
[epoch12, step1256]: loss 0.569318
[epoch12, step1257]: loss 0.464779
[epoch12, step1258]: loss 0.757185
[epoch12, step1259]: loss 0.452632
[epoch12, step1260]: loss 0.532277
[epoch12, step1261]: loss 0.486297
[epoch12, step1262]: loss 0.384370
[epoch12, step1263]: loss 0.463000
[epoch12, step1264]: loss 0.564667
[epoch12, step1265]: loss 0.618913
[epoch12, step1266]: loss 0.408509
[epoch12, step1267]: loss 0.485371
[epoch12, step1268]: loss 0.309727
[epoch12, step1269]: loss 0.496864
[epoch12, step1270]: loss 0.393685
[epoch12, step1271]: loss 0.559041
[epoch12, step1272]: loss 0.616025
[epoch12, step1273]: loss 0.385343
[epoch12, step1274]: loss 0.616270
[epoch12, step1275]: loss 0.723891
[epoch12, step1276]: loss 0.434303
[epoch12, step1277]: loss 0.505151
[epoch12, step1278]: loss 0.636179
[epoch12, step1279]: loss 0.684799
[epoch12, step1280]: loss 0.594868
[epoch12, step1281]: loss 0.422524
[epoch12, step1282]: loss 0.612329
[epoch12, step1283]: loss 0.381838
[epoch12, step1284]: loss 0.407963
[epoch12, step1285]: loss 0.606759
[epoch12, step1286]: loss 0.485716
[epoch12, step1287]: loss 0.321167
[epoch12, step1288]: loss 0.547991
[epoch12, step1289]: loss 0.597887
[epoch12, step1290]: loss 0.590147
[epoch12, step1291]: loss 0.549249
[epoch12, step1292]: loss 0.519017
[epoch12, step1293]: loss 0.456617
[epoch12, step1294]: loss 0.750485
[epoch12, step1295]: loss 0.563581
[epoch12, step1296]: loss 0.726100
[epoch12, step1297]: loss 0.450870
[epoch12, step1298]: loss 0.407561
[epoch12, step1299]: loss 0.555008
[epoch12, step1300]: loss 0.434947
[epoch12, step1301]: loss 0.538679
[epoch12, step1302]: loss 0.574047
[epoch12, step1303]: loss 0.587527
[epoch12, step1304]: loss 0.697421
[epoch12, step1305]: loss 0.240840
[epoch12, step1306]: loss 0.618612
[epoch12, step1307]: loss 0.472579
[epoch12, step1308]: loss 0.389301
[epoch12, step1309]: loss 0.497766
[epoch12, step1310]: loss 0.354243
[epoch12, step1311]: loss 0.621248
[epoch12, step1312]: loss 0.631423
[epoch12, step1313]: loss 0.308948
[epoch12, step1314]: loss 0.283616
[epoch12, step1315]: loss 0.490460
[epoch12, step1316]: loss 0.673867
[epoch12, step1317]: loss 0.533671
[epoch12, step1318]: loss 0.429179
[epoch12, step1319]: loss 0.464537
[epoch12, step1320]: loss 0.399903
[epoch12, step1321]: loss 0.572788
[epoch12, step1322]: loss 0.451746
[epoch12, step1323]: loss 0.438826
[epoch12, step1324]: loss 0.670913
[epoch12, step1325]: loss 0.609678
[epoch12, step1326]: loss 0.757605
[epoch12, step1327]: loss 0.438631
[epoch12, step1328]: loss 0.532875
[epoch12, step1329]: loss 0.696088
[epoch12, step1330]: loss 0.330521
[epoch12, step1331]: loss 0.592189
[epoch12, step1332]: loss 0.434901
[epoch12, step1333]: loss 0.340009
[epoch12, step1334]: loss 0.375464
[epoch12, step1335]: loss 0.635087
[epoch12, step1336]: loss 0.407030
[epoch12, step1337]: loss 0.340992
[epoch12, step1338]: loss 0.623970
[epoch12, step1339]: loss 0.600797
[epoch12, step1340]: loss 0.502556
[epoch12, step1341]: loss 0.717738
[epoch12, step1342]: loss 0.625882
[epoch12, step1343]: loss 0.546586
[epoch12, step1344]: loss 0.353331
[epoch12, step1345]: loss 0.484720
[epoch12, step1346]: loss 0.429879
[epoch12, step1347]: loss 0.383166
[epoch12, step1348]: loss 0.535564
[epoch12, step1349]: loss 0.528409
[epoch12, step1350]: loss 0.492177
[epoch12, step1351]: loss 0.589208
[epoch12, step1352]: loss 0.545131
[epoch12, step1353]: loss 0.220014
[epoch12, step1354]: loss 0.669379
[epoch12, step1355]: loss 0.613475
[epoch12, step1356]: loss 0.545366
[epoch12, step1357]: loss 0.516552
[epoch12, step1358]: loss 0.374044
[epoch12, step1359]: loss 0.515565
[epoch12, step1360]: loss 0.180822
[epoch12, step1361]: loss 0.382119
[epoch12, step1362]: loss 0.436990
[epoch12, step1363]: loss 0.500705
[epoch12, step1364]: loss 0.477501
[epoch12, step1365]: loss 0.473408
[epoch12, step1366]: loss 0.489631
[epoch12, step1367]: loss 0.389981
[epoch12, step1368]: loss 0.442415
[epoch12, step1369]: loss 0.762050
[epoch12, step1370]: loss 0.335265
[epoch12, step1371]: loss 0.473874
[epoch12, step1372]: loss 0.562468
[epoch12, step1373]: loss 0.501532
[epoch12, step1374]: loss 0.534867
[epoch12, step1375]: loss 0.506222
[epoch12, step1376]: loss 0.596861
[epoch12, step1377]: loss 0.648335
[epoch12, step1378]: loss 0.559529
[epoch12, step1379]: loss 0.620801
[epoch12, step1380]: loss 0.258094
[epoch12, step1381]: loss 0.432007
[epoch12, step1382]: loss 0.379473
[epoch12, step1383]: loss 0.152831
[epoch12, step1384]: loss 0.463979
[epoch12, step1385]: loss 0.460078
[epoch12, step1386]: loss 0.545345
[epoch12, step1387]: loss 0.298246
[epoch12, step1388]: loss 0.474208
[epoch12, step1389]: loss 0.668352
[epoch12, step1390]: loss 0.552227
[epoch12, step1391]: loss 0.307639
[epoch12, step1392]: loss 0.413952
[epoch12, step1393]: loss 0.634359
[epoch12, step1394]: loss 0.670623
[epoch12, step1395]: loss 0.735947
[epoch12, step1396]: loss 0.470045
[epoch12, step1397]: loss 0.475067
[epoch12, step1398]: loss 0.469177
[epoch12, step1399]: loss 0.506270
[epoch12, step1400]: loss 0.437598
[epoch12, step1401]: loss 0.425203
[epoch12, step1402]: loss 0.551074
[epoch12, step1403]: loss 0.299787
[epoch12, step1404]: loss 0.520064
[epoch12, step1405]: loss 0.535773
[epoch12, step1406]: loss 0.577843
[epoch12, step1407]: loss 0.598855
[epoch12, step1408]: loss 0.304773
[epoch12, step1409]: loss 0.438972
[epoch12, step1410]: loss 0.352459
[epoch12, step1411]: loss 0.560735
[epoch12, step1412]: loss 0.459807
[epoch12, step1413]: loss 0.316244
[epoch12, step1414]: loss 0.346562
[epoch12, step1415]: loss 0.586424
[epoch12, step1416]: loss 0.637617
[epoch12, step1417]: loss 0.696302
[epoch12, step1418]: loss 0.645569
[epoch12, step1419]: loss 0.486471
[epoch12, step1420]: loss 0.441572
[epoch12, step1421]: loss 0.421645
[epoch12, step1422]: loss 0.258578
[epoch12, step1423]: loss 0.671343
[epoch12, step1424]: loss 0.586897
[epoch12, step1425]: loss 0.326467
[epoch12, step1426]: loss 0.331885
[epoch12, step1427]: loss 0.380595
[epoch12, step1428]: loss 0.382362
[epoch12, step1429]: loss 0.655324
[epoch12, step1430]: loss 0.448800
[epoch12, step1431]: loss 0.539677
[epoch12, step1432]: loss 0.536388
[epoch12, step1433]: loss 0.582940
[epoch12, step1434]: loss 0.525144
[epoch12, step1435]: loss 0.441180
[epoch12, step1436]: loss 0.544213
[epoch12, step1437]: loss 0.525320
[epoch12, step1438]: loss 0.508227
[epoch12, step1439]: loss 0.324630
[epoch12, step1440]: loss 0.445582
[epoch12, step1441]: loss 0.610297
[epoch12, step1442]: loss 0.322951
[epoch12, step1443]: loss 0.559783
[epoch12, step1444]: loss 0.462143
[epoch12, step1445]: loss 0.495344
[epoch12, step1446]: loss 0.270199
[epoch12, step1447]: loss 0.641301
[epoch12, step1448]: loss 0.274245
[epoch12, step1449]: loss 0.523061
[epoch12, step1450]: loss 0.601608
[epoch12, step1451]: loss 0.477802
[epoch12, step1452]: loss 0.425802
[epoch12, step1453]: loss 0.513030
[epoch12, step1454]: loss 0.556949
[epoch12, step1455]: loss 0.551935
[epoch12, step1456]: loss 0.370081
[epoch12, step1457]: loss 0.402447
[epoch12, step1458]: loss 0.527176
[epoch12, step1459]: loss 0.480201
[epoch12, step1460]: loss 0.594977
[epoch12, step1461]: loss 0.278807
[epoch12, step1462]: loss 0.461965
[epoch12, step1463]: loss 0.461565
[epoch12, step1464]: loss 0.350001
[epoch12, step1465]: loss 0.476933
[epoch12, step1466]: loss 0.664001
[epoch12, step1467]: loss 0.346404
[epoch12, step1468]: loss 0.620533
[epoch12, step1469]: loss 0.553257
[epoch12, step1470]: loss 0.413948
[epoch12, step1471]: loss 0.609334
[epoch12, step1472]: loss 0.375564
[epoch12, step1473]: loss 0.730727
[epoch12, step1474]: loss 0.459376
[epoch12, step1475]: loss 0.538845
[epoch12, step1476]: loss 0.573214
[epoch12, step1477]: loss 0.362789
[epoch12, step1478]: loss 0.273736
[epoch12, step1479]: loss 0.578827
[epoch12, step1480]: loss 0.581411
[epoch12, step1481]: loss 0.538727
[epoch12, step1482]: loss 0.564664
[epoch12, step1483]: loss 0.593526
[epoch12, step1484]: loss 0.258422
[epoch12, step1485]: loss 0.633659
[epoch12, step1486]: loss 0.543424
[epoch12, step1487]: loss 0.523065
[epoch12, step1488]: loss 0.527049
[epoch12, step1489]: loss 0.619041
[epoch12, step1490]: loss 0.252408
[epoch12, step1491]: loss 0.306199
[epoch12, step1492]: loss 0.366723
[epoch12, step1493]: loss 0.338179
[epoch12, step1494]: loss 0.358796
[epoch12, step1495]: loss 0.534156
[epoch12, step1496]: loss 0.243363
[epoch12, step1497]: loss 0.450103
[epoch12, step1498]: loss 0.529840
[epoch12, step1499]: loss 0.692376
[epoch12, step1500]: loss 0.678660
[epoch12, step1501]: loss 0.614569
[epoch12, step1502]: loss 0.523224
[epoch12, step1503]: loss 0.466811
[epoch12, step1504]: loss 0.409923
[epoch12, step1505]: loss 0.550048
[epoch12, step1506]: loss 0.536962
[epoch12, step1507]: loss 0.768447
[epoch12, step1508]: loss 0.642993
[epoch12, step1509]: loss 0.499451
[epoch12, step1510]: loss 0.471183
[epoch12, step1511]: loss 0.616132
[epoch12, step1512]: loss 0.569280
[epoch12, step1513]: loss 0.728640
[epoch12, step1514]: loss 0.620504
[epoch12, step1515]: loss 0.366205
[epoch12, step1516]: loss 0.677728
[epoch12, step1517]: loss 0.582598
[epoch12, step1518]: loss 0.411241
[epoch12, step1519]: loss 0.648315
[epoch12, step1520]: loss 0.519525
[epoch12, step1521]: loss 0.635279
[epoch12, step1522]: loss 0.341679
[epoch12, step1523]: loss 0.523322
[epoch12, step1524]: loss 0.512293
[epoch12, step1525]: loss 0.682672
[epoch12, step1526]: loss 0.453495
[epoch12, step1527]: loss 0.778215
[epoch12, step1528]: loss 0.527529
[epoch12, step1529]: loss 0.442692
[epoch12, step1530]: loss 0.490647
[epoch12, step1531]: loss 0.368129
[epoch12, step1532]: loss 0.570793
[epoch12, step1533]: loss 0.516624
[epoch12, step1534]: loss 0.531472
[epoch12, step1535]: loss 0.554255
[epoch12, step1536]: loss 0.535760
[epoch12, step1537]: loss 0.475874
[epoch12, step1538]: loss 0.486881
[epoch12, step1539]: loss 0.364911
[epoch12, step1540]: loss 0.545924
[epoch12, step1541]: loss 0.404633
[epoch12, step1542]: loss 0.513333
[epoch12, step1543]: loss 0.508704
[epoch12, step1544]: loss 0.516229
[epoch12, step1545]: loss 0.493117
[epoch12, step1546]: loss 0.343268
[epoch12, step1547]: loss 0.334452
[epoch12, step1548]: loss 0.122545
[epoch12, step1549]: loss 0.634678
[epoch12, step1550]: loss 0.468371
[epoch12, step1551]: loss 0.574403
[epoch12, step1552]: loss 0.386595
[epoch12, step1553]: loss 0.716592
[epoch12, step1554]: loss 0.554129
[epoch12, step1555]: loss 0.592547
[epoch12, step1556]: loss 0.507021
[epoch12, step1557]: loss 0.490760
[epoch12, step1558]: loss 0.543574
[epoch12, step1559]: loss 0.807113
[epoch12, step1560]: loss 0.447797
[epoch12, step1561]: loss 0.432637
[epoch12, step1562]: loss 0.313097
[epoch12, step1563]: loss 0.556275
[epoch12, step1564]: loss 0.581715
[epoch12, step1565]: loss 0.430045
[epoch12, step1566]: loss 0.492842
[epoch12, step1567]: loss 0.451194
[epoch12, step1568]: loss 0.628695
[epoch12, step1569]: loss 0.564781
[epoch12, step1570]: loss 0.577335
[epoch12, step1571]: loss 0.434939
[epoch12, step1572]: loss 0.639748
[epoch12, step1573]: loss 0.525719
[epoch12, step1574]: loss 0.512388
[epoch12, step1575]: loss 0.281017
[epoch12, step1576]: loss 0.475500
[epoch12, step1577]: loss 0.627301
[epoch12, step1578]: loss 0.538184
[epoch12, step1579]: loss 0.500017
[epoch12, step1580]: loss 0.612355
[epoch12, step1581]: loss 0.726327
[epoch12, step1582]: loss 0.242222
[epoch12, step1583]: loss 0.410299
[epoch12, step1584]: loss 0.449254
[epoch12, step1585]: loss 0.450758
[epoch12, step1586]: loss 0.580860
[epoch12, step1587]: loss 0.496588
[epoch12, step1588]: loss 0.569380
[epoch12, step1589]: loss 0.475472
[epoch12, step1590]: loss 0.500715
[epoch12, step1591]: loss 0.690830
[epoch12, step1592]: loss 0.561977
[epoch12, step1593]: loss 0.410058
[epoch12, step1594]: loss 0.443799
[epoch12, step1595]: loss 0.514420
[epoch12, step1596]: loss 0.498408
[epoch12, step1597]: loss 0.275026
[epoch12, step1598]: loss 0.567956
[epoch12, step1599]: loss 0.436375
[epoch12, step1600]: loss 0.360391
[epoch12, step1601]: loss 0.517833
[epoch12, step1602]: loss 0.470415
[epoch12, step1603]: loss 0.425914
[epoch12, step1604]: loss 0.635588
[epoch12, step1605]: loss 0.689799
[epoch12, step1606]: loss 0.640255
[epoch12, step1607]: loss 0.354440
[epoch12, step1608]: loss 0.382689
[epoch12, step1609]: loss 0.113044
[epoch12, step1610]: loss 0.264962
[epoch12, step1611]: loss 0.572775
[epoch12, step1612]: loss 0.413434
[epoch12, step1613]: loss 0.639938
[epoch12, step1614]: loss 0.261640
[epoch12, step1615]: loss 0.303505
[epoch12, step1616]: loss 0.524144
[epoch12, step1617]: loss 0.293297
[epoch12, step1618]: loss 0.537478
[epoch12, step1619]: loss 0.363914
[epoch12, step1620]: loss 0.212363
[epoch12, step1621]: loss 0.111355
[epoch12, step1622]: loss 0.512371
[epoch12, step1623]: loss 0.735234
[epoch12, step1624]: loss 0.327424
[epoch12, step1625]: loss 0.786983
[epoch12, step1626]: loss 0.269455
[epoch12, step1627]: loss 0.526841
[epoch12, step1628]: loss 0.510760
[epoch12, step1629]: loss 0.470740
[epoch12, step1630]: loss 0.432470
[epoch12, step1631]: loss 0.502178
[epoch12, step1632]: loss 0.368572
[epoch12, step1633]: loss 0.605317
[epoch12, step1634]: loss 0.251992
[epoch12, step1635]: loss 0.465910
[epoch12, step1636]: loss 0.445177
[epoch12, step1637]: loss 0.511248
[epoch12, step1638]: loss 0.291930
[epoch12, step1639]: loss 0.616356
[epoch12, step1640]: loss 0.508161
[epoch12, step1641]: loss 0.527157
[epoch12, step1642]: loss 0.650206
[epoch12, step1643]: loss 0.493002
[epoch12, step1644]: loss 0.780385
[epoch12, step1645]: loss 0.587682
[epoch12, step1646]: loss 0.453072
[epoch12, step1647]: loss 0.444803
[epoch12, step1648]: loss 0.382321
[epoch12, step1649]: loss 0.534529
[epoch12, step1650]: loss 0.601976
[epoch12, step1651]: loss 0.571777
[epoch12, step1652]: loss 0.408427
[epoch12, step1653]: loss 0.616337
[epoch12, step1654]: loss 0.621273
[epoch12, step1655]: loss 0.556744
[epoch12, step1656]: loss 0.345845
[epoch12, step1657]: loss 0.655964
[epoch12, step1658]: loss 0.609880
[epoch12, step1659]: loss 0.566650
[epoch12, step1660]: loss 0.369824
[epoch12, step1661]: loss 0.631930
[epoch12, step1662]: loss 0.523820
[epoch12, step1663]: loss 0.350919
[epoch12, step1664]: loss 0.391439
[epoch12, step1665]: loss 0.761135
[epoch12, step1666]: loss 0.610441
[epoch12, step1667]: loss 0.553608
[epoch12, step1668]: loss 0.436999
[epoch12, step1669]: loss 0.423533
[epoch12, step1670]: loss 0.291029
[epoch12, step1671]: loss 0.737298
[epoch12, step1672]: loss 0.471228
[epoch12, step1673]: loss 0.365692
[epoch12, step1674]: loss 0.436908
[epoch12, step1675]: loss 0.505423
[epoch12, step1676]: loss 0.544387
[epoch12, step1677]: loss 0.523627
[epoch12, step1678]: loss 0.246142
[epoch12, step1679]: loss 0.120115
[epoch12, step1680]: loss 0.503919
[epoch12, step1681]: loss 0.430040
[epoch12, step1682]: loss 0.462525
[epoch12, step1683]: loss 0.426852
[epoch12, step1684]: loss 0.690933
[epoch12, step1685]: loss 0.457124
[epoch12, step1686]: loss 0.590355
[epoch12, step1687]: loss 0.398238
[epoch12, step1688]: loss 0.544605
[epoch12, step1689]: loss 0.351323
[epoch12, step1690]: loss 0.566464
[epoch12, step1691]: loss 0.326982
[epoch12, step1692]: loss 0.390450
[epoch12, step1693]: loss 0.315275
[epoch12, step1694]: loss 0.143064
[epoch12, step1695]: loss 0.474476
[epoch12, step1696]: loss 0.492166
[epoch12, step1697]: loss 0.580029
[epoch12, step1698]: loss 0.536950
[epoch12, step1699]: loss 0.641920
[epoch12, step1700]: loss 0.419155
[epoch12, step1701]: loss 0.339809
[epoch12, step1702]: loss 0.670474
[epoch12, step1703]: loss 0.570186
[epoch12, step1704]: loss 0.501522
[epoch12, step1705]: loss 0.501040
[epoch12, step1706]: loss 0.575030
[epoch12, step1707]: loss 0.256940
[epoch12, step1708]: loss 0.519276
[epoch12, step1709]: loss 0.645144
[epoch12, step1710]: loss 0.514804
[epoch12, step1711]: loss 0.352799
[epoch12, step1712]: loss 0.493247
[epoch12, step1713]: loss 0.444823
[epoch12, step1714]: loss 0.503612
[epoch12, step1715]: loss 0.380039
[epoch12, step1716]: loss 0.414389
[epoch12, step1717]: loss 0.642680
[epoch12, step1718]: loss 0.401734
[epoch12, step1719]: loss 0.445084
[epoch12, step1720]: loss 0.635363
[epoch12, step1721]: loss 0.609498
[epoch12, step1722]: loss 0.620773
[epoch12, step1723]: loss 0.528316
[epoch12, step1724]: loss 0.501176
[epoch12, step1725]: loss 0.316852
[epoch12, step1726]: loss 0.488393
[epoch12, step1727]: loss 0.654061
[epoch12, step1728]: loss 0.715271
[epoch12, step1729]: loss 0.371668
[epoch12, step1730]: loss 0.472113
[epoch12, step1731]: loss 0.528511
[epoch12, step1732]: loss 0.514060
[epoch12, step1733]: loss 0.434542
[epoch12, step1734]: loss 0.654021
[epoch12, step1735]: loss 0.392228
[epoch12, step1736]: loss 0.661203
[epoch12, step1737]: loss 0.341667
[epoch12, step1738]: loss 0.624910
[epoch12, step1739]: loss 0.351676
[epoch12, step1740]: loss 0.663041
[epoch12, step1741]: loss 0.475646
[epoch12, step1742]: loss 0.475332
[epoch12, step1743]: loss 0.477906
[epoch12, step1744]: loss 0.549945
[epoch12, step1745]: loss 0.643610
[epoch12, step1746]: loss 0.500531
[epoch12, step1747]: loss 0.607649
[epoch12, step1748]: loss 0.564474
[epoch12, step1749]: loss 0.416149
[epoch12, step1750]: loss 0.386545
[epoch12, step1751]: loss 0.508664
[epoch12, step1752]: loss 0.573200
[epoch12, step1753]: loss 0.638511
[epoch12, step1754]: loss 0.612879
[epoch12, step1755]: loss 0.445583
[epoch12, step1756]: loss 0.287162
[epoch12, step1757]: loss 0.591694
[epoch12, step1758]: loss 0.714097
[epoch12, step1759]: loss 0.414923
[epoch12, step1760]: loss 0.521326
[epoch12, step1761]: loss 0.233413
[epoch12, step1762]: loss 0.519973
[epoch12, step1763]: loss 0.515325
[epoch12, step1764]: loss 0.518206
[epoch12, step1765]: loss 0.528722
[epoch12, step1766]: loss 0.360419
[epoch12, step1767]: loss 0.540346
[epoch12, step1768]: loss 0.354393
[epoch12, step1769]: loss 0.736719
[epoch12, step1770]: loss 0.460183
[epoch12, step1771]: loss 0.497576
[epoch12, step1772]: loss 0.373527
[epoch12, step1773]: loss 0.501628
[epoch12, step1774]: loss 0.473854
[epoch12, step1775]: loss 0.539822
[epoch12, step1776]: loss 0.460231
[epoch12, step1777]: loss 0.345790
[epoch12, step1778]: loss 0.545089
[epoch12, step1779]: loss 0.424517
[epoch12, step1780]: loss 0.467301
[epoch12, step1781]: loss 0.446906
[epoch12, step1782]: loss 0.434317
[epoch12, step1783]: loss 0.376671
[epoch12, step1784]: loss 0.533707
[epoch12, step1785]: loss 0.215458
[epoch12, step1786]: loss 0.522842
[epoch12, step1787]: loss 0.648611
[epoch12, step1788]: loss 0.403831
[epoch12, step1789]: loss 0.537672
[epoch12, step1790]: loss 0.261780
[epoch12, step1791]: loss 0.723693
[epoch12, step1792]: loss 0.596492
[epoch12, step1793]: loss 0.645953
[epoch12, step1794]: loss 0.591401
[epoch12, step1795]: loss 0.515030
[epoch12, step1796]: loss 0.298971
[epoch12, step1797]: loss 0.278158
[epoch12, step1798]: loss 0.473027
[epoch12, step1799]: loss 0.523099
[epoch12, step1800]: loss 0.384012
[epoch12, step1801]: loss 0.327831
[epoch12, step1802]: loss 0.419359
[epoch12, step1803]: loss 0.511255
[epoch12, step1804]: loss 0.224023
[epoch12, step1805]: loss 0.696456
[epoch12, step1806]: loss 0.613819
[epoch12, step1807]: loss 0.544239
[epoch12, step1808]: loss 0.664018
[epoch12, step1809]: loss 0.536580
[epoch12, step1810]: loss 0.353804
[epoch12, step1811]: loss 0.492250
[epoch12, step1812]: loss 0.269490
[epoch12, step1813]: loss 0.488752
[epoch12, step1814]: loss 0.655680
[epoch12, step1815]: loss 0.365634
[epoch12, step1816]: loss 0.513439
[epoch12, step1817]: loss 0.438832
[epoch12, step1818]: loss 0.506405
[epoch12, step1819]: loss 0.507346
[epoch12, step1820]: loss 0.485340
[epoch12, step1821]: loss 0.348688
[epoch12, step1822]: loss 0.587989
[epoch12, step1823]: loss 0.636596
[epoch12, step1824]: loss 0.587066
[epoch12, step1825]: loss 0.474264
[epoch12, step1826]: loss 0.254550
[epoch12, step1827]: loss 0.494291
[epoch12, step1828]: loss 0.431581
[epoch12, step1829]: loss 0.429562
[epoch12, step1830]: loss 0.384810
[epoch12, step1831]: loss 0.454886
[epoch12, step1832]: loss 0.392778
[epoch12, step1833]: loss 0.503406
[epoch12, step1834]: loss 0.461744
[epoch12, step1835]: loss 0.597294
[epoch12, step1836]: loss 0.520577
[epoch12, step1837]: loss 0.910290
[epoch12, step1838]: loss 0.528017
[epoch12, step1839]: loss 0.499884
[epoch12, step1840]: loss 0.249310
[epoch12, step1841]: loss 0.440061
[epoch12, step1842]: loss 0.444919
[epoch12, step1843]: loss 0.519187
[epoch12, step1844]: loss 0.268855
[epoch12, step1845]: loss 0.575638
[epoch12, step1846]: loss 0.680838
[epoch12, step1847]: loss 0.553365
[epoch12, step1848]: loss 0.360391
[epoch12, step1849]: loss 0.470487
[epoch12, step1850]: loss 0.341477
[epoch12, step1851]: loss 0.536491
[epoch12, step1852]: loss 0.330551
[epoch12, step1853]: loss 0.432476
[epoch12, step1854]: loss 0.254152
[epoch12, step1855]: loss 0.355110
[epoch12, step1856]: loss 0.559457
[epoch12, step1857]: loss 0.349691
[epoch12, step1858]: loss 0.506767
[epoch12, step1859]: loss 0.644048
[epoch12, step1860]: loss 0.350643
[epoch12, step1861]: loss 0.389246
[epoch12, step1862]: loss 0.773408
[epoch12, step1863]: loss 0.588770
[epoch12, step1864]: loss 0.560545
[epoch12, step1865]: loss 0.635311
[epoch12, step1866]: loss 0.318730
[epoch12, step1867]: loss 0.446385
[epoch12, step1868]: loss 0.523709
[epoch12, step1869]: loss 0.535163
[epoch12, step1870]: loss 0.536186
[epoch12, step1871]: loss 0.470743
[epoch12, step1872]: loss 0.365418
[epoch12, step1873]: loss 0.537263
[epoch12, step1874]: loss 0.370957
[epoch12, step1875]: loss 0.459532
[epoch12, step1876]: loss 0.428462
[epoch12, step1877]: loss 0.269110
[epoch12, step1878]: loss 0.675434
[epoch12, step1879]: loss 0.416307
[epoch12, step1880]: loss 0.589930
[epoch12, step1881]: loss 0.447131
[epoch12, step1882]: loss 0.701619
[epoch12, step1883]: loss 0.362822
[epoch12, step1884]: loss 0.600585
[epoch12, step1885]: loss 0.397303
[epoch12, step1886]: loss 0.625363
[epoch12, step1887]: loss 0.584661
[epoch12, step1888]: loss 0.362113
[epoch12, step1889]: loss 0.618666
[epoch12, step1890]: loss 0.417900
[epoch12, step1891]: loss 0.517702
[epoch12, step1892]: loss 0.349861
[epoch12, step1893]: loss 0.404536
[epoch12, step1894]: loss 0.403375
[epoch12, step1895]: loss 0.596564
[epoch12, step1896]: loss 0.270816
[epoch12, step1897]: loss 0.561389
[epoch12, step1898]: loss 0.450389
[epoch12, step1899]: loss 0.323907
[epoch12, step1900]: loss 0.576206
[epoch12, step1901]: loss 0.599088
[epoch12, step1902]: loss 0.796486
[epoch12, step1903]: loss 0.440595
[epoch12, step1904]: loss 0.618084
[epoch12, step1905]: loss 0.670078
[epoch12, step1906]: loss 0.448070
[epoch12, step1907]: loss 0.618493
[epoch12, step1908]: loss 0.320069
[epoch12, step1909]: loss 0.454449
[epoch12, step1910]: loss 0.485713
[epoch12, step1911]: loss 0.549368
[epoch12, step1912]: loss 0.598288
[epoch12, step1913]: loss 0.377863
[epoch12, step1914]: loss 0.625574
[epoch12, step1915]: loss 0.413797
[epoch12, step1916]: loss 0.585927
[epoch12, step1917]: loss 0.391792
[epoch12, step1918]: loss 0.524253
[epoch12, step1919]: loss 0.336520
[epoch12, step1920]: loss 0.553455
[epoch12, step1921]: loss 0.571609
[epoch12, step1922]: loss 0.379620
[epoch12, step1923]: loss 0.484583
[epoch12, step1924]: loss 0.517187
[epoch12, step1925]: loss 0.696257
[epoch12, step1926]: loss 0.654614
[epoch12, step1927]: loss 0.642200
[epoch12, step1928]: loss 0.536647
[epoch12, step1929]: loss 0.434421
[epoch12, step1930]: loss 0.602822
[epoch12, step1931]: loss 0.468358
[epoch12, step1932]: loss 0.428780
[epoch12, step1933]: loss 0.584701
[epoch12, step1934]: loss 0.490693
[epoch12, step1935]: loss 0.648452
[epoch12, step1936]: loss 0.785541
[epoch12, step1937]: loss 0.669213
[epoch12, step1938]: loss 0.544939
[epoch12, step1939]: loss 0.490541
[epoch12, step1940]: loss 0.619769
[epoch12, step1941]: loss 0.392538
[epoch12, step1942]: loss 0.621378
[epoch12, step1943]: loss 0.518715
[epoch12, step1944]: loss 0.632341
[epoch12, step1945]: loss 0.524882
[epoch12, step1946]: loss 0.157195
[epoch12, step1947]: loss 0.551143
[epoch12, step1948]: loss 0.717367
[epoch12, step1949]: loss 0.493167
[epoch12, step1950]: loss 0.791928
[epoch12, step1951]: loss 0.565460
[epoch12, step1952]: loss 0.110230
[epoch12, step1953]: loss 0.351575
[epoch12, step1954]: loss 0.397822
[epoch12, step1955]: loss 0.488589
[epoch12, step1956]: loss 0.501564
[epoch12, step1957]: loss 0.618753
[epoch12, step1958]: loss 0.348286
[epoch12, step1959]: loss 0.522819
[epoch12, step1960]: loss 0.599805
[epoch12, step1961]: loss 0.588836
[epoch12, step1962]: loss 0.388514
[epoch12, step1963]: loss 0.527065
[epoch12, step1964]: loss 0.414819
[epoch12, step1965]: loss 0.233464
[epoch12, step1966]: loss 0.556959
[epoch12, step1967]: loss 0.585511
[epoch12, step1968]: loss 0.368388
[epoch12, step1969]: loss 0.455471
[epoch12, step1970]: loss 0.374205
[epoch12, step1971]: loss 0.431810
[epoch12, step1972]: loss 0.562231
[epoch12, step1973]: loss 0.265421
[epoch12, step1974]: loss 0.229750
[epoch12, step1975]: loss 0.525047
[epoch12, step1976]: loss 0.320633
[epoch12, step1977]: loss 0.368412
[epoch12, step1978]: loss 0.476446
[epoch12, step1979]: loss 0.600432
[epoch12, step1980]: loss 0.589232
[epoch12, step1981]: loss 0.609220
[epoch12, step1982]: loss 0.252127
[epoch12, step1983]: loss 0.652788
[epoch12, step1984]: loss 0.261982
[epoch12, step1985]: loss 0.106537
[epoch12, step1986]: loss 0.343482
[epoch12, step1987]: loss 0.427449
[epoch12, step1988]: loss 0.496103
[epoch12, step1989]: loss 0.567798
[epoch12, step1990]: loss 0.517180
[epoch12, step1991]: loss 0.314464
[epoch12, step1992]: loss 0.438082
[epoch12, step1993]: loss 0.247830
[epoch12, step1994]: loss 0.166675
[epoch12, step1995]: loss 0.611176
[epoch12, step1996]: loss 0.299179
[epoch12, step1997]: loss 0.405584
[epoch12, step1998]: loss 0.679618
[epoch12, step1999]: loss 0.253687
[epoch12, step2000]: loss 0.327449
[epoch12, step2001]: loss 0.222003
[epoch12, step2002]: loss 0.637852
[epoch12, step2003]: loss 0.228037
[epoch12, step2004]: loss 0.474594
[epoch12, step2005]: loss 0.599363
[epoch12, step2006]: loss 0.502627
[epoch12, step2007]: loss 0.250637
[epoch12, step2008]: loss 0.643947
[epoch12, step2009]: loss 0.402976
[epoch12, step2010]: loss 0.405808
[epoch12, step2011]: loss 0.282920
[epoch12, step2012]: loss 0.630290
[epoch12, step2013]: loss 0.375697
[epoch12, step2014]: loss 0.616672
[epoch12, step2015]: loss 0.637903
[epoch12, step2016]: loss 0.382532
[epoch12, step2017]: loss 0.656259
[epoch12, step2018]: loss 0.398072
[epoch12, step2019]: loss 0.585145
[epoch12, step2020]: loss 0.476984
[epoch12, step2021]: loss 0.628619
[epoch12, step2022]: loss 0.469970
[epoch12, step2023]: loss 0.439445
[epoch12, step2024]: loss 0.462959
[epoch12, step2025]: loss 0.355668
[epoch12, step2026]: loss 0.450974
[epoch12, step2027]: loss 0.421163
[epoch12, step2028]: loss 0.618323
[epoch12, step2029]: loss 0.476037
[epoch12, step2030]: loss 0.379502
[epoch12, step2031]: loss 0.577206
[epoch12, step2032]: loss 0.517076
[epoch12, step2033]: loss 0.563594
[epoch12, step2034]: loss 0.418126
[epoch12, step2035]: loss 0.673410
[epoch12, step2036]: loss 0.617343
[epoch12, step2037]: loss 0.669351
[epoch12, step2038]: loss 0.308846
[epoch12, step2039]: loss 0.445032
[epoch12, step2040]: loss 0.633563
[epoch12, step2041]: loss 0.653576
[epoch12, step2042]: loss 0.324539
[epoch12, step2043]: loss 0.593549
[epoch12, step2044]: loss 0.540967
[epoch12, step2045]: loss 0.523277
[epoch12, step2046]: loss 0.597637
[epoch12, step2047]: loss 0.286765
[epoch12, step2048]: loss 0.418158
[epoch12, step2049]: loss 0.478466
[epoch12, step2050]: loss 0.306202
[epoch12, step2051]: loss 0.522920
[epoch12, step2052]: loss 0.374763
[epoch12, step2053]: loss 0.211481
[epoch12, step2054]: loss 0.417722
[epoch12, step2055]: loss 0.522085
[epoch12, step2056]: loss 0.145361
[epoch12, step2057]: loss 0.423981
[epoch12, step2058]: loss 0.460363
[epoch12, step2059]: loss 0.260173
[epoch12, step2060]: loss 0.392688
[epoch12, step2061]: loss 0.561772
[epoch12, step2062]: loss 0.452037
[epoch12, step2063]: loss 0.396079
[epoch12, step2064]: loss 0.443096
[epoch12, step2065]: loss 0.527327
[epoch12, step2066]: loss 0.164349
[epoch12, step2067]: loss 0.566168
[epoch12, step2068]: loss 0.280308
[epoch12, step2069]: loss 0.530031
[epoch12, step2070]: loss 0.549604
[epoch12, step2071]: loss 0.200750
[epoch12, step2072]: loss 0.590793
[epoch12, step2073]: loss 0.335052
[epoch12, step2074]: loss 0.688723
[epoch12, step2075]: loss 0.387193
[epoch12, step2076]: loss 0.472179
[epoch12, step2077]: loss 0.377401
[epoch12, step2078]: loss 0.495758
[epoch12, step2079]: loss 0.420813
[epoch12, step2080]: loss 0.510781
[epoch12, step2081]: loss 0.513498
[epoch12, step2082]: loss 0.488175
[epoch12, step2083]: loss 0.453433
[epoch12, step2084]: loss 0.563377
[epoch12, step2085]: loss 0.342711
[epoch12, step2086]: loss 0.695709
[epoch12, step2087]: loss 0.659783
[epoch12, step2088]: loss 0.384128
[epoch12, step2089]: loss 0.577018
[epoch12, step2090]: loss 0.411525
[epoch12, step2091]: loss 0.539764
[epoch12, step2092]: loss 0.541967
[epoch12, step2093]: loss 0.333227
[epoch12, step2094]: loss 0.490836
[epoch12, step2095]: loss 0.475759
[epoch12, step2096]: loss 0.334605
[epoch12, step2097]: loss 0.333727
[epoch12, step2098]: loss 0.638110
[epoch12, step2099]: loss 0.693180
[epoch12, step2100]: loss 0.438308
[epoch12, step2101]: loss 0.340488
[epoch12, step2102]: loss 0.603161
[epoch12, step2103]: loss 0.586247
[epoch12, step2104]: loss 0.603347
[epoch12, step2105]: loss 0.548895
[epoch12, step2106]: loss 0.504685
[epoch12, step2107]: loss 0.263605
[epoch12, step2108]: loss 0.237014
[epoch12, step2109]: loss 0.472398
[epoch12, step2110]: loss 0.631869
[epoch12, step2111]: loss 0.497528
[epoch12, step2112]: loss 0.328171
[epoch12, step2113]: loss 0.368497
[epoch12, step2114]: loss 0.498373
[epoch12, step2115]: loss 0.440357
[epoch12, step2116]: loss 0.211043
[epoch12, step2117]: loss 0.507818
[epoch12, step2118]: loss 0.595802
[epoch12, step2119]: loss 0.436253
[epoch12, step2120]: loss 0.364623
[epoch12, step2121]: loss 0.496216
[epoch12, step2122]: loss 0.608119
[epoch12, step2123]: loss 0.634116
[epoch12, step2124]: loss 0.860000
[epoch12, step2125]: loss 0.657966
[epoch12, step2126]: loss 0.533280
[epoch12, step2127]: loss 0.339763
[epoch12, step2128]: loss 0.669470
[epoch12, step2129]: loss 0.644340
[epoch12, step2130]: loss 0.685908
[epoch12, step2131]: loss 0.312302
[epoch12, step2132]: loss 0.555488
[epoch12, step2133]: loss 0.638323
[epoch12, step2134]: loss 0.615382
[epoch12, step2135]: loss 0.519644
[epoch12, step2136]: loss 0.386390
[epoch12, step2137]: loss 0.739204
[epoch12, step2138]: loss 0.388903
[epoch12, step2139]: loss 0.456466
[epoch12, step2140]: loss 0.514597
[epoch12, step2141]: loss 0.503914
[epoch12, step2142]: loss 0.748033
[epoch12, step2143]: loss 0.492356
[epoch12, step2144]: loss 0.645110
[epoch12, step2145]: loss 0.381651
[epoch12, step2146]: loss 0.496036
[epoch12, step2147]: loss 0.254308
[epoch12, step2148]: loss 0.460282
[epoch12, step2149]: loss 0.582782
[epoch12, step2150]: loss 0.405565
[epoch12, step2151]: loss 0.613641
[epoch12, step2152]: loss 0.341561
[epoch12, step2153]: loss 0.503527
[epoch12, step2154]: loss 0.434379
[epoch12, step2155]: loss 0.471101
[epoch12, step2156]: loss 0.573834
[epoch12, step2157]: loss 0.472691
[epoch12, step2158]: loss 0.367237
[epoch12, step2159]: loss 0.453560
[epoch12, step2160]: loss 0.468455
[epoch12, step2161]: loss 0.435178
[epoch12, step2162]: loss 0.624864
[epoch12, step2163]: loss 0.445438
[epoch12, step2164]: loss 0.588997
[epoch12, step2165]: loss 0.472195
[epoch12, step2166]: loss 0.507644
[epoch12, step2167]: loss 0.367781
[epoch12, step2168]: loss 0.404061
[epoch12, step2169]: loss 0.640260
[epoch12, step2170]: loss 0.420756
[epoch12, step2171]: loss 0.398308
[epoch12, step2172]: loss 0.765177
[epoch12, step2173]: loss 0.522330
[epoch12, step2174]: loss 0.363859
[epoch12, step2175]: loss 0.536923
[epoch12, step2176]: loss 0.536990
[epoch12, step2177]: loss 0.452133
[epoch12, step2178]: loss 0.322174
[epoch12, step2179]: loss 0.449522
[epoch12, step2180]: loss 0.436400
[epoch12, step2181]: loss 0.490692
[epoch12, step2182]: loss 0.628783
[epoch12, step2183]: loss 0.179609
[epoch12, step2184]: loss 0.203162
[epoch12, step2185]: loss 0.639273
[epoch12, step2186]: loss 0.550493
[epoch12, step2187]: loss 0.487778
[epoch12, step2188]: loss 0.443364
[epoch12, step2189]: loss 0.580697
[epoch12, step2190]: loss 0.804581
[epoch12, step2191]: loss 0.426853
[epoch12, step2192]: loss 0.498125
[epoch12, step2193]: loss 0.404392
[epoch12, step2194]: loss 0.497947
[epoch12, step2195]: loss 0.475707
[epoch12, step2196]: loss 0.526997
[epoch12, step2197]: loss 0.404725
[epoch12, step2198]: loss 0.450839
[epoch12, step2199]: loss 0.551448
[epoch12, step2200]: loss 0.231603
[epoch12, step2201]: loss 0.335854
[epoch12, step2202]: loss 0.535227
[epoch12, step2203]: loss 0.288595
[epoch12, step2204]: loss 0.568705
[epoch12, step2205]: loss 0.578050
[epoch12, step2206]: loss 0.467432
[epoch12, step2207]: loss 0.473666
[epoch12, step2208]: loss 0.585584
[epoch12, step2209]: loss 0.601372
[epoch12, step2210]: loss 0.571873
[epoch12, step2211]: loss 0.589033
[epoch12, step2212]: loss 0.499473
[epoch12, step2213]: loss 0.466012
[epoch12, step2214]: loss 0.525643
[epoch12, step2215]: loss 0.518097
[epoch12, step2216]: loss 0.694231
[epoch12, step2217]: loss 0.545905
[epoch12, step2218]: loss 0.437930
[epoch12, step2219]: loss 0.284631
[epoch12, step2220]: loss 0.595767
[epoch12, step2221]: loss 0.618735
[epoch12, step2222]: loss 0.603756
[epoch12, step2223]: loss 0.465358
[epoch12, step2224]: loss 0.520314
[epoch12, step2225]: loss 0.625060
[epoch12, step2226]: loss 0.578756
[epoch12, step2227]: loss 0.342833
[epoch12, step2228]: loss 0.375199
[epoch12, step2229]: loss 0.687292
[epoch12, step2230]: loss 0.395745
[epoch12, step2231]: loss 0.604592
[epoch12, step2232]: loss 0.376260
[epoch12, step2233]: loss 0.233117
[epoch12, step2234]: loss 0.454880
[epoch12, step2235]: loss 0.688785
[epoch12, step2236]: loss 0.442461
[epoch12, step2237]: loss 0.274721
[epoch12, step2238]: loss 0.268966
[epoch12, step2239]: loss 0.330301
[epoch12, step2240]: loss 0.518115
[epoch12, step2241]: loss 0.681453
[epoch12, step2242]: loss 0.505322
[epoch12, step2243]: loss 0.366101
[epoch12, step2244]: loss 0.505452
[epoch12, step2245]: loss 0.361823
[epoch12, step2246]: loss 0.338923
[epoch12, step2247]: loss 0.508948
[epoch12, step2248]: loss 0.291888
[epoch12, step2249]: loss 0.503320
[epoch12, step2250]: loss 0.462300
[epoch12, step2251]: loss 0.316822
[epoch12, step2252]: loss 0.343648
[epoch12, step2253]: loss 0.375922
[epoch12, step2254]: loss 0.278823
[epoch12, step2255]: loss 0.442565
[epoch12, step2256]: loss 0.627320
[epoch12, step2257]: loss 0.359693
[epoch12, step2258]: loss 0.587984
[epoch12, step2259]: loss 0.438513
[epoch12, step2260]: loss 0.451108
[epoch12, step2261]: loss 0.600657
[epoch12, step2262]: loss 0.504648
[epoch12, step2263]: loss 0.643906
[epoch12, step2264]: loss 0.592254
[epoch12, step2265]: loss 0.258547
[epoch12, step2266]: loss 0.585543
[epoch12, step2267]: loss 0.683066
[epoch12, step2268]: loss 0.592011
[epoch12, step2269]: loss 0.481895
[epoch12, step2270]: loss 0.540297
[epoch12, step2271]: loss 0.549215
[epoch12, step2272]: loss 0.478642
[epoch12, step2273]: loss 0.530421
[epoch12, step2274]: loss 0.430967
[epoch12, step2275]: loss 0.524212
[epoch12, step2276]: loss 0.461505
[epoch12, step2277]: loss 0.570067
[epoch12, step2278]: loss 0.577488
[epoch12, step2279]: loss 0.500099
[epoch12, step2280]: loss 0.536343
[epoch12, step2281]: loss 0.558422
[epoch12, step2282]: loss 0.450424
[epoch12, step2283]: loss 0.374000
[epoch12, step2284]: loss 0.539151
[epoch12, step2285]: loss 0.434377
[epoch12, step2286]: loss 0.655826
[epoch12, step2287]: loss 0.525786
[epoch12, step2288]: loss 0.391210
[epoch12, step2289]: loss 0.549631
[epoch12, step2290]: loss 0.410047
[epoch12, step2291]: loss 0.681066
[epoch12, step2292]: loss 0.566643
[epoch12, step2293]: loss 0.326170
[epoch12, step2294]: loss 0.398176
[epoch12, step2295]: loss 0.547818
[epoch12, step2296]: loss 0.714161
[epoch12, step2297]: loss 0.440350
[epoch12, step2298]: loss 0.431231
[epoch12, step2299]: loss 0.633325
[epoch12, step2300]: loss 0.685097
[epoch12, step2301]: loss 0.583780
[epoch12, step2302]: loss 0.366513
[epoch12, step2303]: loss 0.634102
[epoch12, step2304]: loss 0.564934
[epoch12, step2305]: loss 0.489310
[epoch12, step2306]: loss 0.573969
[epoch12, step2307]: loss 0.506442
[epoch12, step2308]: loss 0.667036
[epoch12, step2309]: loss 0.509213
[epoch12, step2310]: loss 0.583311
[epoch12, step2311]: loss 0.346307
[epoch12, step2312]: loss 0.451896
[epoch12, step2313]: loss 0.273913
[epoch12, step2314]: loss 0.629703
[epoch12, step2315]: loss 0.403757
[epoch12, step2316]: loss 0.334659
[epoch12, step2317]: loss 0.507405
[epoch12, step2318]: loss 0.356225
[epoch12, step2319]: loss 0.531914
[epoch12, step2320]: loss 0.344604
[epoch12, step2321]: loss 0.548755
[epoch12, step2322]: loss 0.397563
[epoch12, step2323]: loss 0.677770
[epoch12, step2324]: loss 0.313665
[epoch12, step2325]: loss 0.441647
[epoch12, step2326]: loss 0.691375
[epoch12, step2327]: loss 0.433780
[epoch12, step2328]: loss 0.556748
[epoch12, step2329]: loss 0.624203
[epoch12, step2330]: loss 0.630496
[epoch12, step2331]: loss 0.729011
[epoch12, step2332]: loss 0.488749
[epoch12, step2333]: loss 0.553966
[epoch12, step2334]: loss 0.425811
[epoch12, step2335]: loss 0.432280
[epoch12, step2336]: loss 0.464546
[epoch12, step2337]: loss 0.617851
[epoch12, step2338]: loss 0.524958
[epoch12, step2339]: loss 0.617319
[epoch12, step2340]: loss 0.506109
[epoch12, step2341]: loss 0.149792
[epoch12, step2342]: loss 0.250779
[epoch12, step2343]: loss 0.242635
[epoch12, step2344]: loss 0.285927
[epoch12, step2345]: loss 0.166986
[epoch12, step2346]: loss 0.535474
[epoch12, step2347]: loss 0.485951
[epoch12, step2348]: loss 0.481953
[epoch12, step2349]: loss 0.329018
[epoch12, step2350]: loss 0.517838
[epoch12, step2351]: loss 0.602258
[epoch12, step2352]: loss 0.479006
[epoch12, step2353]: loss 0.406279
[epoch12, step2354]: loss 0.491337
[epoch12, step2355]: loss 0.550745
[epoch12, step2356]: loss 0.315158
[epoch12, step2357]: loss 0.475689
[epoch12, step2358]: loss 0.316531
[epoch12, step2359]: loss 0.435846
[epoch12, step2360]: loss 0.458914
[epoch12, step2361]: loss 0.113054
[epoch12, step2362]: loss 0.607893
[epoch12, step2363]: loss 0.395724
[epoch12, step2364]: loss 0.616518
[epoch12, step2365]: loss 0.477907
[epoch12, step2366]: loss 0.439287
[epoch12, step2367]: loss 0.583502
[epoch12, step2368]: loss 0.445056
[epoch12, step2369]: loss 0.406800
[epoch12, step2370]: loss 0.597030
[epoch12, step2371]: loss 0.454696
[epoch12, step2372]: loss 0.468476
[epoch12, step2373]: loss 0.480183
[epoch12, step2374]: loss 0.383759
[epoch12, step2375]: loss 0.435249
[epoch12, step2376]: loss 0.316764
[epoch12, step2377]: loss 0.447799
[epoch12, step2378]: loss 0.577873
[epoch12, step2379]: loss 0.674629
[epoch12, step2380]: loss 0.749558
[epoch12, step2381]: loss 0.281191
[epoch12, step2382]: loss 0.540317
[epoch12, step2383]: loss 0.590189
[epoch12, step2384]: loss 0.284223
[epoch12, step2385]: loss 0.608736
[epoch12, step2386]: loss 0.424528
[epoch12, step2387]: loss 0.378750
[epoch12, step2388]: loss 0.545097
[epoch12, step2389]: loss 0.454542
[epoch12, step2390]: loss 0.333128
[epoch12, step2391]: loss 0.537530
[epoch12, step2392]: loss 0.597407
[epoch12, step2393]: loss 0.549910
[epoch12, step2394]: loss 0.371035
[epoch12, step2395]: loss 0.463524
[epoch12, step2396]: loss 0.664127
[epoch12, step2397]: loss 0.603222
[epoch12, step2398]: loss 0.269428
[epoch12, step2399]: loss 0.652866
[epoch12, step2400]: loss 0.431821
[epoch12, step2401]: loss 0.522196
[epoch12, step2402]: loss 0.778976
[epoch12, step2403]: loss 0.509167
[epoch12, step2404]: loss 0.455437
[epoch12, step2405]: loss 0.582193
[epoch12, step2406]: loss 0.656146
[epoch12, step2407]: loss 0.317204
[epoch12, step2408]: loss 0.433490
[epoch12, step2409]: loss 0.486394
[epoch12, step2410]: loss 0.360811
[epoch12, step2411]: loss 0.296366
[epoch12, step2412]: loss 0.291434
[epoch12, step2413]: loss 0.414298
[epoch12, step2414]: loss 0.399643
[epoch12, step2415]: loss 0.501474
[epoch12, step2416]: loss 0.451854
[epoch12, step2417]: loss 0.322159
[epoch12, step2418]: loss 0.289718
[epoch12, step2419]: loss 0.218266
[epoch12, step2420]: loss 0.443726
[epoch12, step2421]: loss 0.571683
[epoch12, step2422]: loss 0.548928
[epoch12, step2423]: loss 0.420082
[epoch12, step2424]: loss 0.558218
[epoch12, step2425]: loss 0.725020
[epoch12, step2426]: loss 0.621266
[epoch12, step2427]: loss 0.545276
[epoch12, step2428]: loss 0.710735
[epoch12, step2429]: loss 0.605454
[epoch12, step2430]: loss 0.378713
[epoch12, step2431]: loss 0.353494
[epoch12, step2432]: loss 0.703786
[epoch12, step2433]: loss 0.166738
[epoch12, step2434]: loss 0.527459
[epoch12, step2435]: loss 0.554883
[epoch12, step2436]: loss 0.381657
[epoch12, step2437]: loss 0.607078
[epoch12, step2438]: loss 0.524072
[epoch12, step2439]: loss 0.447295
[epoch12, step2440]: loss 0.466746
[epoch12, step2441]: loss 0.540556
[epoch12, step2442]: loss 0.493779
[epoch12, step2443]: loss 0.513213
[epoch12, step2444]: loss 0.252389
[epoch12, step2445]: loss 0.658470
[epoch12, step2446]: loss 0.441170
[epoch12, step2447]: loss 0.276572
[epoch12, step2448]: loss 0.343373
[epoch12, step2449]: loss 0.517843
[epoch12, step2450]: loss 0.472739
[epoch12, step2451]: loss 0.544441
[epoch12, step2452]: loss 0.552164
[epoch12, step2453]: loss 0.530315
[epoch12, step2454]: loss 0.770765
[epoch12, step2455]: loss 0.714241
[epoch12, step2456]: loss 0.480582
[epoch12, step2457]: loss 0.556856
[epoch12, step2458]: loss 0.457889
[epoch12, step2459]: loss 0.620288
[epoch12, step2460]: loss 0.373673
[epoch12, step2461]: loss 0.537944
[epoch12, step2462]: loss 0.488006
[epoch12, step2463]: loss 0.497758
[epoch12, step2464]: loss 0.409269
[epoch12, step2465]: loss 0.454453
[epoch12, step2466]: loss 0.299508
[epoch12, step2467]: loss 0.352174
[epoch12, step2468]: loss 0.379275
[epoch12, step2469]: loss 0.507607
[epoch12, step2470]: loss 0.643710
[epoch12, step2471]: loss 0.379336
[epoch12, step2472]: loss 0.683728
[epoch12, step2473]: loss 0.481765
[epoch12, step2474]: loss 0.569844
[epoch12, step2475]: loss 0.481116
[epoch12, step2476]: loss 0.469490
[epoch12, step2477]: loss 0.423499
[epoch12, step2478]: loss 0.682349
[epoch12, step2479]: loss 0.601313
[epoch12, step2480]: loss 0.446314
[epoch12, step2481]: loss 0.541053
[epoch12, step2482]: loss 0.400642
[epoch12, step2483]: loss 0.596461
[epoch12, step2484]: loss 0.335211
[epoch12, step2485]: loss 0.518061
[epoch12, step2486]: loss 0.519927
[epoch12, step2487]: loss 0.531762
[epoch12, step2488]: loss 0.340618
[epoch12, step2489]: loss 0.406834
[epoch12, step2490]: loss 0.500376
[epoch12, step2491]: loss 0.430007
[epoch12, step2492]: loss 0.621489
[epoch12, step2493]: loss 0.124980
[epoch12, step2494]: loss 0.485867
[epoch12, step2495]: loss 0.502819
[epoch12, step2496]: loss 0.471499
[epoch12, step2497]: loss 0.591593
[epoch12, step2498]: loss 0.583832
[epoch12, step2499]: loss 0.483881
[epoch12, step2500]: loss 0.654799
[epoch12, step2501]: loss 0.363900
[epoch12, step2502]: loss 0.359994
[epoch12, step2503]: loss 0.575905
[epoch12, step2504]: loss 0.595558
[epoch12, step2505]: loss 0.472137
[epoch12, step2506]: loss 0.640924
[epoch12, step2507]: loss 0.245766
[epoch12, step2508]: loss 0.464191
[epoch12, step2509]: loss 0.252817
[epoch12, step2510]: loss 0.357496
[epoch12, step2511]: loss 0.491215
[epoch12, step2512]: loss 0.438657
[epoch12, step2513]: loss 0.760247
[epoch12, step2514]: loss 0.584738
[epoch12, step2515]: loss 0.520208
[epoch12, step2516]: loss 0.449192
[epoch12, step2517]: loss 0.204309
[epoch12, step2518]: loss 0.501293
[epoch12, step2519]: loss 0.447891
[epoch12, step2520]: loss 0.570061
[epoch12, step2521]: loss 0.279894
[epoch12, step2522]: loss 0.409743
[epoch12, step2523]: loss 0.479857
[epoch12, step2524]: loss 0.476998
[epoch12, step2525]: loss 0.537012
[epoch12, step2526]: loss 0.337003
[epoch12, step2527]: loss 0.341100
[epoch12, step2528]: loss 0.552486
[epoch12, step2529]: loss 0.553966
[epoch12, step2530]: loss 0.299514
[epoch12, step2531]: loss 0.521898
[epoch12, step2532]: loss 0.690940
[epoch12, step2533]: loss 0.484094
[epoch12, step2534]: loss 0.322663
[epoch12, step2535]: loss 0.366721
[epoch12, step2536]: loss 0.629823
[epoch12, step2537]: loss 0.290961
[epoch12, step2538]: loss 0.465952
[epoch12, step2539]: loss 0.305854
[epoch12, step2540]: loss 0.418465
[epoch12, step2541]: loss 0.342183
[epoch12, step2542]: loss 0.663764
[epoch12, step2543]: loss 0.630900
[epoch12, step2544]: loss 0.373711
[epoch12, step2545]: loss 0.352862
[epoch12, step2546]: loss 0.383613
[epoch12, step2547]: loss 0.592133
[epoch12, step2548]: loss 0.592859
[epoch12, step2549]: loss 0.467311
[epoch12, step2550]: loss 0.451100
[epoch12, step2551]: loss 0.532427
[epoch12, step2552]: loss 0.506182
[epoch12, step2553]: loss 0.649114
[epoch12, step2554]: loss 0.355879
[epoch12, step2555]: loss 0.633817
[epoch12, step2556]: loss 0.452644
[epoch12, step2557]: loss 0.583625
[epoch12, step2558]: loss 0.533389
[epoch12, step2559]: loss 0.465097
[epoch12, step2560]: loss 0.289283
[epoch12, step2561]: loss 0.289002
[epoch12, step2562]: loss 0.548160
[epoch12, step2563]: loss 0.472735
[epoch12, step2564]: loss 0.460089
[epoch12, step2565]: loss 0.541761
[epoch12, step2566]: loss 0.567695
[epoch12, step2567]: loss 0.663889
[epoch12, step2568]: loss 0.354759
[epoch12, step2569]: loss 0.617857
[epoch12, step2570]: loss 0.412972
[epoch12, step2571]: loss 0.373750
[epoch12, step2572]: loss 0.449906
[epoch12, step2573]: loss 0.548353
[epoch12, step2574]: loss 0.442765
[epoch12, step2575]: loss 0.393382
[epoch12, step2576]: loss 0.548871
[epoch12, step2577]: loss 0.588049
[epoch12, step2578]: loss 0.355901
[epoch12, step2579]: loss 0.614394
[epoch12, step2580]: loss 0.361183
[epoch12, step2581]: loss 0.452037
[epoch12, step2582]: loss 0.639217
[epoch12, step2583]: loss 0.277091
[epoch12, step2584]: loss 0.619209
[epoch12, step2585]: loss 0.549714
[epoch12, step2586]: loss 0.559238
[epoch12, step2587]: loss 0.340176
[epoch12, step2588]: loss 0.339851
[epoch12, step2589]: loss 0.601540
[epoch12, step2590]: loss 0.659157
[epoch12, step2591]: loss 0.520482
[epoch12, step2592]: loss 0.480814
[epoch12, step2593]: loss 0.508085
[epoch12, step2594]: loss 0.374422
[epoch12, step2595]: loss 0.521090
[epoch12, step2596]: loss 0.445703
[epoch12, step2597]: loss 0.450603
[epoch12, step2598]: loss 0.440699
[epoch12, step2599]: loss 0.601478
[epoch12, step2600]: loss 0.529928
[epoch12, step2601]: loss 0.453502
[epoch12, step2602]: loss 0.499710
[epoch12, step2603]: loss 0.304511
[epoch12, step2604]: loss 0.366282
[epoch12, step2605]: loss 0.639668
[epoch12, step2606]: loss 0.701179
[epoch12, step2607]: loss 0.498221
[epoch12, step2608]: loss 0.367557
[epoch12, step2609]: loss 0.573135
[epoch12, step2610]: loss 0.632245
[epoch12, step2611]: loss 0.481373
[epoch12, step2612]: loss 0.604288
[epoch12, step2613]: loss 0.334333
[epoch12, step2614]: loss 0.728460
[epoch12, step2615]: loss 0.501021
[epoch12, step2616]: loss 0.333506
[epoch12, step2617]: loss 0.582187
[epoch12, step2618]: loss 0.471725
[epoch12, step2619]: loss 0.349156
[epoch12, step2620]: loss 0.542565
[epoch12, step2621]: loss 0.375421
[epoch12, step2622]: loss 0.382813
[epoch12, step2623]: loss 0.304897
[epoch12, step2624]: loss 0.554187
[epoch12, step2625]: loss 0.649638
[epoch12, step2626]: loss 0.141300
[epoch12, step2627]: loss 0.517013
[epoch12, step2628]: loss 0.526069
[epoch12, step2629]: loss 0.495226
[epoch12, step2630]: loss 0.396455
[epoch12, step2631]: loss 0.392762
[epoch12, step2632]: loss 0.298059
[epoch12, step2633]: loss 0.591887
[epoch12, step2634]: loss 0.402475
[epoch12, step2635]: loss 0.496100
[epoch12, step2636]: loss 0.667658
[epoch12, step2637]: loss 0.349253
[epoch12, step2638]: loss 0.584149
[epoch12, step2639]: loss 0.532355
[epoch12, step2640]: loss 0.399301
[epoch12, step2641]: loss 0.247695
[epoch12, step2642]: loss 0.470338
[epoch12, step2643]: loss 0.444448
[epoch12, step2644]: loss 0.377194
[epoch12, step2645]: loss 0.470731
[epoch12, step2646]: loss 0.764254
[epoch12, step2647]: loss 0.573724
[epoch12, step2648]: loss 0.653410
[epoch12, step2649]: loss 0.653122
[epoch12, step2650]: loss 0.362563
[epoch12, step2651]: loss 0.553989
[epoch12, step2652]: loss 0.361052
[epoch12, step2653]: loss 0.533274
[epoch12, step2654]: loss 0.522540
[epoch12, step2655]: loss 0.716577
[epoch12, step2656]: loss 0.655702
[epoch12, step2657]: loss 0.619804
[epoch12, step2658]: loss 0.446549
[epoch12, step2659]: loss 0.504958
[epoch12, step2660]: loss 0.367660
[epoch12, step2661]: loss 0.402268
[epoch12, step2662]: loss 0.457014
[epoch12, step2663]: loss 0.618655
[epoch12, step2664]: loss 0.678113
[epoch12, step2665]: loss 0.422137
[epoch12, step2666]: loss 0.725558
[epoch12, step2667]: loss 0.560240
[epoch12, step2668]: loss 0.487125
[epoch12, step2669]: loss 0.605162
[epoch12, step2670]: loss 0.584109
[epoch12, step2671]: loss 0.569957
[epoch12, step2672]: loss 0.399382
[epoch12, step2673]: loss 0.224041
[epoch12, step2674]: loss 0.751818
[epoch12, step2675]: loss 0.582930
[epoch12, step2676]: loss 0.661092
[epoch12, step2677]: loss 0.495171
[epoch12, step2678]: loss 0.446126
[epoch12, step2679]: loss 0.348139
[epoch12, step2680]: loss 0.424778
[epoch12, step2681]: loss 0.422190
[epoch12, step2682]: loss 0.421381
[epoch12, step2683]: loss 0.459163
[epoch12, step2684]: loss 0.320061
[epoch12, step2685]: loss 0.536008
[epoch12, step2686]: loss 0.464835
[epoch12, step2687]: loss 0.575752
[epoch12, step2688]: loss 0.603542
[epoch12, step2689]: loss 0.504024
[epoch12, step2690]: loss 0.540400
[epoch12, step2691]: loss 0.472034
[epoch12, step2692]: loss 0.542596
[epoch12, step2693]: loss 0.445174
[epoch12, step2694]: loss 0.158662
[epoch12, step2695]: loss 0.157371
[epoch12, step2696]: loss 0.433943
[epoch12, step2697]: loss 0.821649
[epoch12, step2698]: loss 0.154259
[epoch12, step2699]: loss 0.473961
[epoch12, step2700]: loss 0.761959
[epoch12, step2701]: loss 0.564310
[epoch12, step2702]: loss 0.534237
[epoch12, step2703]: loss 0.420063
[epoch12, step2704]: loss 0.379841
[epoch12, step2705]: loss 0.571853
[epoch12, step2706]: loss 0.567472
[epoch12, step2707]: loss 0.280386
[epoch12, step2708]: loss 0.245383
[epoch12, step2709]: loss 0.281730
[epoch12, step2710]: loss 0.458525
[epoch12, step2711]: loss 0.445152
[epoch12, step2712]: loss 0.488588
[epoch12, step2713]: loss 0.566435
[epoch12, step2714]: loss 0.590271
[epoch12, step2715]: loss 0.466159
[epoch12, step2716]: loss 0.441553
[epoch12, step2717]: loss 0.511396
[epoch12, step2718]: loss 0.350116
[epoch12, step2719]: loss 0.494057
[epoch12, step2720]: loss 0.341604
[epoch12, step2721]: loss 0.549705
[epoch12, step2722]: loss 0.579597
[epoch12, step2723]: loss 0.603181
[epoch12, step2724]: loss 0.464132
[epoch12, step2725]: loss 0.457485
[epoch12, step2726]: loss 0.419485
[epoch12, step2727]: loss 0.656034
[epoch12, step2728]: loss 0.314053
[epoch12, step2729]: loss 0.585557
[epoch12, step2730]: loss 0.460041
[epoch12, step2731]: loss 0.298363
[epoch12, step2732]: loss 0.459194
[epoch12, step2733]: loss 0.469631
[epoch12, step2734]: loss 0.373970
[epoch12, step2735]: loss 0.581772
[epoch12, step2736]: loss 0.603683
[epoch12, step2737]: loss 0.500421
[epoch12, step2738]: loss 0.603842
[epoch12, step2739]: loss 0.499174
[epoch12, step2740]: loss 0.260748
[epoch12, step2741]: loss 0.623468
[epoch12, step2742]: loss 0.517036
[epoch12, step2743]: loss 0.573332
[epoch12, step2744]: loss 0.282990
[epoch12, step2745]: loss 0.551983
[epoch12, step2746]: loss 0.257763
[epoch12, step2747]: loss 0.277436
[epoch12, step2748]: loss 0.364977
[epoch12, step2749]: loss 0.668571
[epoch12, step2750]: loss 0.539739
[epoch12, step2751]: loss 0.429075
[epoch12, step2752]: loss 0.152530
[epoch12, step2753]: loss 0.598930
[epoch12, step2754]: loss 0.401165
[epoch12, step2755]: loss 0.416955
[epoch12, step2756]: loss 0.551452
[epoch12, step2757]: loss 0.440295
[epoch12, step2758]: loss 0.270121
[epoch12, step2759]: loss 0.164928
[epoch12, step2760]: loss 0.509049
[epoch12, step2761]: loss 0.352014
[epoch12, step2762]: loss 0.644728
[epoch12, step2763]: loss 0.526019
[epoch12, step2764]: loss 0.471488
[epoch12, step2765]: loss 0.284752
[epoch12, step2766]: loss 0.470279
[epoch12, step2767]: loss 0.511503
[epoch12, step2768]: loss 0.520339
[epoch12, step2769]: loss 0.229142
[epoch12, step2770]: loss 0.443622
[epoch12, step2771]: loss 0.587828
[epoch12, step2772]: loss 0.487978
[epoch12, step2773]: loss 0.264655
[epoch12, step2774]: loss 0.744255
[epoch12, step2775]: loss 0.343531
[epoch12, step2776]: loss 0.309173
[epoch12, step2777]: loss 0.236397
[epoch12, step2778]: loss 0.571814
[epoch12, step2779]: loss 0.647264
[epoch12, step2780]: loss 0.504195
[epoch12, step2781]: loss 0.624015
[epoch12, step2782]: loss 0.533865
[epoch12, step2783]: loss 0.633953
[epoch12, step2784]: loss 0.503181
[epoch12, step2785]: loss 0.275060
[epoch12, step2786]: loss 0.278387
[epoch12, step2787]: loss 0.508192
[epoch12, step2788]: loss 0.442429
[epoch12, step2789]: loss 0.422471
[epoch12, step2790]: loss 0.604869
[epoch12, step2791]: loss 0.818486
[epoch12, step2792]: loss 0.423135
[epoch12, step2793]: loss 0.452272
[epoch12, step2794]: loss 0.244904
[epoch12, step2795]: loss 0.427319
[epoch12, step2796]: loss 0.358358
[epoch12, step2797]: loss 0.354110
[epoch12, step2798]: loss 0.393016
[epoch12, step2799]: loss 0.465323
[epoch12, step2800]: loss 0.699516
[epoch12, step2801]: loss 0.402501
[epoch12, step2802]: loss 0.322173
[epoch12, step2803]: loss 0.582396
[epoch12, step2804]: loss 0.538653
[epoch12, step2805]: loss 0.401643
[epoch12, step2806]: loss 0.373734
[epoch12, step2807]: loss 0.610784
[epoch12, step2808]: loss 0.534088
[epoch12, step2809]: loss 0.503090
[epoch12, step2810]: loss 0.479798
[epoch12, step2811]: loss 0.663017
[epoch12, step2812]: loss 0.459266
[epoch12, step2813]: loss 0.371928
[epoch12, step2814]: loss 0.605411
[epoch12, step2815]: loss 0.572488
[epoch12, step2816]: loss 0.432918
[epoch12, step2817]: loss 0.308882
[epoch12, step2818]: loss 0.396341
[epoch12, step2819]: loss 0.304891
[epoch12, step2820]: loss 0.596146
[epoch12, step2821]: loss 0.472790
[epoch12, step2822]: loss 0.311916
[epoch12, step2823]: loss 0.561905
[epoch12, step2824]: loss 0.341158
[epoch12, step2825]: loss 0.484249
[epoch12, step2826]: loss 0.131589
[epoch12, step2827]: loss 0.375482
[epoch12, step2828]: loss 0.597718
[epoch12, step2829]: loss 0.496186
[epoch12, step2830]: loss 0.628975
[epoch12, step2831]: loss 0.507046
[epoch12, step2832]: loss 0.482290
[epoch12, step2833]: loss 0.322570
[epoch12, step2834]: loss 0.485400
[epoch12, step2835]: loss 0.441910
[epoch12, step2836]: loss 0.571251
[epoch12, step2837]: loss 0.321048
[epoch12, step2838]: loss 0.413597
[epoch12, step2839]: loss 0.589462
[epoch12, step2840]: loss 0.484152
[epoch12, step2841]: loss 0.588203
[epoch12, step2842]: loss 0.316279
[epoch12, step2843]: loss 0.544891
[epoch12, step2844]: loss 0.453037
[epoch12, step2845]: loss 0.739743
[epoch12, step2846]: loss 0.623997
[epoch12, step2847]: loss 0.347253
[epoch12, step2848]: loss 0.254749
[epoch12, step2849]: loss 0.389423
[epoch12, step2850]: loss 0.553957
[epoch12, step2851]: loss 0.400769
[epoch12, step2852]: loss 0.482432
[epoch12, step2853]: loss 0.461102
[epoch12, step2854]: loss 0.599100
[epoch12, step2855]: loss 0.652495
[epoch12, step2856]: loss 0.615854
[epoch12, step2857]: loss 0.513503
[epoch12, step2858]: loss 0.641529
[epoch12, step2859]: loss 0.504169
[epoch12, step2860]: loss 0.413548
[epoch12, step2861]: loss 0.363708
[epoch12, step2862]: loss 0.574709
[epoch12, step2863]: loss 0.498117
[epoch12, step2864]: loss 0.508233
[epoch12, step2865]: loss 0.361419
[epoch12, step2866]: loss 0.649468
[epoch12, step2867]: loss 0.489426
[epoch12, step2868]: loss 0.455154
[epoch12, step2869]: loss 0.643737
[epoch12, step2870]: loss 0.255403
[epoch12, step2871]: loss 0.369204
[epoch12, step2872]: loss 0.403561
[epoch12, step2873]: loss 0.321422
[epoch12, step2874]: loss 0.306763
[epoch12, step2875]: loss 0.649839
[epoch12, step2876]: loss 0.351375
[epoch12, step2877]: loss 0.482146
[epoch12, step2878]: loss 0.585194
[epoch12, step2879]: loss 0.268963
[epoch12, step2880]: loss 0.342876
[epoch12, step2881]: loss 0.519199
[epoch12, step2882]: loss 0.432667
[epoch12, step2883]: loss 0.457915
[epoch12, step2884]: loss 0.465200
[epoch12, step2885]: loss 0.418769
[epoch12, step2886]: loss 0.273300
[epoch12, step2887]: loss 0.126205
[epoch12, step2888]: loss 0.457245
[epoch12, step2889]: loss 0.470580
[epoch12, step2890]: loss 0.552387
[epoch12, step2891]: loss 0.635296
[epoch12, step2892]: loss 0.617530
[epoch12, step2893]: loss 0.463354
[epoch12, step2894]: loss 0.396203
[epoch12, step2895]: loss 0.566349
[epoch12, step2896]: loss 0.165991
[epoch12, step2897]: loss 0.545104
[epoch12, step2898]: loss 0.589764
[epoch12, step2899]: loss 0.574326
[epoch12, step2900]: loss 0.446792
[epoch12, step2901]: loss 0.586092
[epoch12, step2902]: loss 0.348457
[epoch12, step2903]: loss 0.539603
[epoch12, step2904]: loss 0.482790
[epoch12, step2905]: loss 0.570579
[epoch12, step2906]: loss 0.327619
[epoch12, step2907]: loss 0.347597
[epoch12, step2908]: loss 0.485039
[epoch12, step2909]: loss 0.639971
[epoch12, step2910]: loss 0.635900
[epoch12, step2911]: loss 0.486384
[epoch12, step2912]: loss 0.312720
[epoch12, step2913]: loss 0.393717
[epoch12, step2914]: loss 0.488576
[epoch12, step2915]: loss 0.289299
[epoch12, step2916]: loss 0.460545
[epoch12, step2917]: loss 0.499839
[epoch12, step2918]: loss 0.606830
[epoch12, step2919]: loss 0.428289
[epoch12, step2920]: loss 0.499589
[epoch12, step2921]: loss 0.441590
[epoch12, step2922]: loss 0.568849
[epoch12, step2923]: loss 0.500675
[epoch12, step2924]: loss 0.697330
[epoch12, step2925]: loss 0.516271
[epoch12, step2926]: loss 0.390744
[epoch12, step2927]: loss 0.345264
[epoch12, step2928]: loss 0.498102
[epoch12, step2929]: loss 0.438132
[epoch12, step2930]: loss 0.714141
[epoch12, step2931]: loss 0.449667
[epoch12, step2932]: loss 0.486876
[epoch12, step2933]: loss 0.297028
[epoch12, step2934]: loss 0.409687
[epoch12, step2935]: loss 0.361384
[epoch12, step2936]: loss 0.448570
[epoch12, step2937]: loss 0.316905
[epoch12, step2938]: loss 0.517152
[epoch12, step2939]: loss 0.397467
[epoch12, step2940]: loss 0.732466
[epoch12, step2941]: loss 0.385438
[epoch12, step2942]: loss 0.321592
[epoch12, step2943]: loss 0.264734
[epoch12, step2944]: loss 0.537339
[epoch12, step2945]: loss 0.587524
[epoch12, step2946]: loss 0.400232
[epoch12, step2947]: loss 0.241104
[epoch12, step2948]: loss 0.577278
[epoch12, step2949]: loss 0.455134
[epoch12, step2950]: loss 0.659282
[epoch12, step2951]: loss 0.688423
[epoch12, step2952]: loss 0.487301
[epoch12, step2953]: loss 0.526094
[epoch12, step2954]: loss 0.611697
[epoch12, step2955]: loss 0.650430
[epoch12, step2956]: loss 0.656271
[epoch12, step2957]: loss 0.503558
[epoch12, step2958]: loss 0.573073
[epoch12, step2959]: loss 0.282976
[epoch12, step2960]: loss 0.444543
[epoch12, step2961]: loss 0.757625
[epoch12, step2962]: loss 0.707237
[epoch12, step2963]: loss 0.364870
[epoch12, step2964]: loss 0.662997
[epoch12, step2965]: loss 0.443826
[epoch12, step2966]: loss 0.689461
[epoch12, step2967]: loss 0.342160
[epoch12, step2968]: loss 0.611076
[epoch12, step2969]: loss 0.542673
[epoch12, step2970]: loss 0.526944
[epoch12, step2971]: loss 0.319985
[epoch12, step2972]: loss 0.679444
[epoch12, step2973]: loss 0.282262
[epoch12, step2974]: loss 0.690362
[epoch12, step2975]: loss 0.435681
[epoch12, step2976]: loss 0.512873
[epoch12, step2977]: loss 0.566496
[epoch12, step2978]: loss 0.508199
[epoch12, step2979]: loss 0.661456
[epoch12, step2980]: loss 0.692013
[epoch12, step2981]: loss 0.235886
[epoch12, step2982]: loss 0.718237
[epoch12, step2983]: loss 0.476069
[epoch12, step2984]: loss 0.453873
[epoch12, step2985]: loss 0.524439
[epoch12, step2986]: loss 0.517985
[epoch12, step2987]: loss 0.371830
[epoch12, step2988]: loss 0.646178
[epoch12, step2989]: loss 0.496342
[epoch12, step2990]: loss 0.596352
[epoch12, step2991]: loss 0.705140
[epoch12, step2992]: loss 0.527436
[epoch12, step2993]: loss 0.574623
[epoch12, step2994]: loss 0.389825
[epoch12, step2995]: loss 0.637424
[epoch12, step2996]: loss 0.354761
[epoch12, step2997]: loss 0.592655
[epoch12, step2998]: loss 0.508156
[epoch12, step2999]: loss 0.195355
[epoch12, step3000]: loss 0.527736
[epoch12, step3001]: loss 0.674618
[epoch12, step3002]: loss 0.477788
[epoch12, step3003]: loss 0.552029
[epoch12, step3004]: loss 0.363408
[epoch12, step3005]: loss 0.391474
[epoch12, step3006]: loss 0.723910
[epoch12, step3007]: loss 0.250171
[epoch12, step3008]: loss 0.685727
[epoch12, step3009]: loss 0.646914
[epoch12, step3010]: loss 0.556237
[epoch12, step3011]: loss 0.546522
[epoch12, step3012]: loss 0.495864
[epoch12, step3013]: loss 0.417716
[epoch12, step3014]: loss 0.475687
[epoch12, step3015]: loss 0.260077
[epoch12, step3016]: loss 0.248333
[epoch12, step3017]: loss 0.506294
[epoch12, step3018]: loss 0.258832
[epoch12, step3019]: loss 0.429411
[epoch12, step3020]: loss 0.553181
[epoch12, step3021]: loss 0.345213
[epoch12, step3022]: loss 0.636153
[epoch12, step3023]: loss 0.169063
[epoch12, step3024]: loss 0.389886
[epoch12, step3025]: loss 0.367863
[epoch12, step3026]: loss 0.629893
[epoch12, step3027]: loss 0.529375
[epoch12, step3028]: loss 0.567422
[epoch12, step3029]: loss 0.518372
[epoch12, step3030]: loss 0.747505
[epoch12, step3031]: loss 0.381574
[epoch12, step3032]: loss 0.374097
[epoch12, step3033]: loss 0.480443
[epoch12, step3034]: loss 0.458512
[epoch12, step3035]: loss 0.405894
[epoch12, step3036]: loss 0.567187
[epoch12, step3037]: loss 0.552813
[epoch12, step3038]: loss 0.416738
[epoch12, step3039]: loss 0.565706
[epoch12, step3040]: loss 0.494228
[epoch12, step3041]: loss 0.397029
[epoch12, step3042]: loss 0.378017
[epoch12, step3043]: loss 0.358808
[epoch12, step3044]: loss 0.456913
[epoch12, step3045]: loss 0.301386
[epoch12, step3046]: loss 0.607064
[epoch12, step3047]: loss 0.378077
[epoch12, step3048]: loss 0.496496
[epoch12, step3049]: loss 0.374905
[epoch12, step3050]: loss 0.370172
[epoch12, step3051]: loss 0.640841
[epoch12, step3052]: loss 0.360445
[epoch12, step3053]: loss 0.472748
[epoch12, step3054]: loss 0.628955
[epoch12, step3055]: loss 0.500578
[epoch12, step3056]: loss 0.513361
[epoch12, step3057]: loss 0.703438
[epoch12, step3058]: loss 0.460709
[epoch12, step3059]: loss 0.559297
[epoch12, step3060]: loss 0.572799
[epoch12, step3061]: loss 0.393304
[epoch12, step3062]: loss 0.522757
[epoch12, step3063]: loss 0.285084
[epoch12, step3064]: loss 0.471951
[epoch12, step3065]: loss 0.678224
[epoch12, step3066]: loss 0.530541
[epoch12, step3067]: loss 0.263791
[epoch12, step3068]: loss 0.615778
[epoch12, step3069]: loss 0.657144
[epoch12, step3070]: loss 0.561988
[epoch12, step3071]: loss 0.498666
[epoch12, step3072]: loss 0.393598
[epoch12, step3073]: loss 0.566672
[epoch12, step3074]: loss 0.466279
[epoch12, step3075]: loss 0.502786
[epoch12, step3076]: loss 0.680609

[epoch12]: avg loss 0.680609

[epoch13, step1]: loss 0.368900
[epoch13, step2]: loss 0.473995
[epoch13, step3]: loss 0.581286
[epoch13, step4]: loss 0.518626
[epoch13, step5]: loss 0.621226
[epoch13, step6]: loss 0.376427
[epoch13, step7]: loss 0.408504
[epoch13, step8]: loss 0.359261
[epoch13, step9]: loss 0.370533
[epoch13, step10]: loss 0.591716
[epoch13, step11]: loss 0.128365
[epoch13, step12]: loss 0.577430
[epoch13, step13]: loss 0.650845
[epoch13, step14]: loss 0.426696
[epoch13, step15]: loss 0.563824
[epoch13, step16]: loss 0.545329
[epoch13, step17]: loss 0.487507
[epoch13, step18]: loss 0.470424
[epoch13, step19]: loss 0.496800
[epoch13, step20]: loss 0.240271
[epoch13, step21]: loss 0.543686
[epoch13, step22]: loss 0.263133
[epoch13, step23]: loss 0.233218
[epoch13, step24]: loss 0.443666
[epoch13, step25]: loss 0.389659
[epoch13, step26]: loss 0.590900
[epoch13, step27]: loss 0.520564
[epoch13, step28]: loss 0.473630
[epoch13, step29]: loss 0.569692
[epoch13, step30]: loss 0.530885
[epoch13, step31]: loss 0.353326
[epoch13, step32]: loss 0.353356
[epoch13, step33]: loss 0.570856
[epoch13, step34]: loss 0.465903
[epoch13, step35]: loss 0.501660
[epoch13, step36]: loss 0.535737
[epoch13, step37]: loss 0.774590
[epoch13, step38]: loss 0.596582
[epoch13, step39]: loss 0.367188
[epoch13, step40]: loss 0.671371
[epoch13, step41]: loss 0.489097
[epoch13, step42]: loss 0.343481
[epoch13, step43]: loss 0.521011
[epoch13, step44]: loss 0.532954
[epoch13, step45]: loss 0.572860
[epoch13, step46]: loss 0.354818
[epoch13, step47]: loss 0.510294
[epoch13, step48]: loss 0.203794
[epoch13, step49]: loss 0.372628
[epoch13, step50]: loss 0.509245
[epoch13, step51]: loss 0.517090
[epoch13, step52]: loss 0.759093
[epoch13, step53]: loss 0.547506
[epoch13, step54]: loss 0.545779
[epoch13, step55]: loss 0.419703
[epoch13, step56]: loss 0.352048
[epoch13, step57]: loss 0.433776
[epoch13, step58]: loss 0.387027
[epoch13, step59]: loss 0.578196
[epoch13, step60]: loss 0.368923
[epoch13, step61]: loss 0.497612
[epoch13, step62]: loss 0.584943
[epoch13, step63]: loss 0.477470
[epoch13, step64]: loss 0.496395
[epoch13, step65]: loss 0.618909
[epoch13, step66]: loss 0.490416
[epoch13, step67]: loss 0.424749
[epoch13, step68]: loss 0.406596
[epoch13, step69]: loss 0.388004
[epoch13, step70]: loss 0.350460
[epoch13, step71]: loss 0.435669
[epoch13, step72]: loss 0.523120
[epoch13, step73]: loss 0.734923
[epoch13, step74]: loss 0.387377
[epoch13, step75]: loss 0.460652
[epoch13, step76]: loss 0.317804
[epoch13, step77]: loss 0.344106
[epoch13, step78]: loss 0.576382
[epoch13, step79]: loss 0.515937
[epoch13, step80]: loss 0.515832
[epoch13, step81]: loss 0.587587
[epoch13, step82]: loss 0.496286
[epoch13, step83]: loss 0.719558
[epoch13, step84]: loss 0.292141
[epoch13, step85]: loss 0.333508
[epoch13, step86]: loss 0.414398
[epoch13, step87]: loss 0.547165
[epoch13, step88]: loss 0.711370
[epoch13, step89]: loss 0.536124
[epoch13, step90]: loss 0.655688
[epoch13, step91]: loss 0.492203
[epoch13, step92]: loss 0.363800
[epoch13, step93]: loss 0.311137
[epoch13, step94]: loss 0.461409
[epoch13, step95]: loss 0.690271
[epoch13, step96]: loss 0.551921
[epoch13, step97]: loss 0.426555
[epoch13, step98]: loss 0.244317
[epoch13, step99]: loss 0.461246
[epoch13, step100]: loss 0.293977
[epoch13, step101]: loss 0.500636
[epoch13, step102]: loss 0.579896
[epoch13, step103]: loss 0.425778
[epoch13, step104]: loss 0.543668
[epoch13, step105]: loss 0.251644
[epoch13, step106]: loss 0.559130
[epoch13, step107]: loss 0.547449
[epoch13, step108]: loss 0.399875
[epoch13, step109]: loss 0.533872
[epoch13, step110]: loss 0.573533
[epoch13, step111]: loss 0.441571
[epoch13, step112]: loss 0.438317
[epoch13, step113]: loss 0.352548
[epoch13, step114]: loss 0.546073
[epoch13, step115]: loss 0.549729
[epoch13, step116]: loss 0.495371
[epoch13, step117]: loss 0.349751
[epoch13, step118]: loss 0.600989
[epoch13, step119]: loss 0.565525
[epoch13, step120]: loss 0.484650
[epoch13, step121]: loss 0.601146
[epoch13, step122]: loss 0.258178
[epoch13, step123]: loss 0.252536
[epoch13, step124]: loss 0.129103
[epoch13, step125]: loss 0.583404
[epoch13, step126]: loss 0.653762
[epoch13, step127]: loss 0.471961
[epoch13, step128]: loss 0.478622
[epoch13, step129]: loss 0.521841
[epoch13, step130]: loss 0.586756
[epoch13, step131]: loss 0.694242
[epoch13, step132]: loss 0.525306
[epoch13, step133]: loss 0.707405
[epoch13, step134]: loss 0.446792
[epoch13, step135]: loss 0.693100
[epoch13, step136]: loss 0.425809
[epoch13, step137]: loss 0.456483
[epoch13, step138]: loss 0.434112
[epoch13, step139]: loss 0.702985
[epoch13, step140]: loss 0.279743
[epoch13, step141]: loss 0.609111
[epoch13, step142]: loss 0.752035
[epoch13, step143]: loss 0.599089
[epoch13, step144]: loss 0.424841
[epoch13, step145]: loss 0.510467
[epoch13, step146]: loss 0.231853
[epoch13, step147]: loss 0.631261
[epoch13, step148]: loss 0.588888
[epoch13, step149]: loss 0.462083
[epoch13, step150]: loss 0.427395
[epoch13, step151]: loss 0.363490
[epoch13, step152]: loss 0.692566
[epoch13, step153]: loss 0.531704
[epoch13, step154]: loss 0.284734
[epoch13, step155]: loss 0.419679
[epoch13, step156]: loss 0.359469
[epoch13, step157]: loss 0.657198
[epoch13, step158]: loss 0.457341
[epoch13, step159]: loss 0.231092
[epoch13, step160]: loss 0.379484
[epoch13, step161]: loss 0.750757
[epoch13, step162]: loss 0.664139
[epoch13, step163]: loss 0.367747
[epoch13, step164]: loss 0.421634
[epoch13, step165]: loss 0.629886
[epoch13, step166]: loss 0.659638
[epoch13, step167]: loss 0.320764
[epoch13, step168]: loss 0.670110
[epoch13, step169]: loss 0.459462
[epoch13, step170]: loss 0.419783
[epoch13, step171]: loss 0.499964
[epoch13, step172]: loss 0.317480
[epoch13, step173]: loss 0.413690
[epoch13, step174]: loss 0.700590
[epoch13, step175]: loss 0.605218
[epoch13, step176]: loss 0.597267
[epoch13, step177]: loss 0.710652
[epoch13, step178]: loss 0.625818
[epoch13, step179]: loss 0.479082
[epoch13, step180]: loss 0.625217
[epoch13, step181]: loss 0.525483
[epoch13, step182]: loss 0.673618
[epoch13, step183]: loss 0.526669
[epoch13, step184]: loss 0.433972
[epoch13, step185]: loss 0.230807
[epoch13, step186]: loss 0.464456
[epoch13, step187]: loss 0.273752
[epoch13, step188]: loss 0.289475
[epoch13, step189]: loss 0.627206
[epoch13, step190]: loss 0.559768
[epoch13, step191]: loss 0.545065
[epoch13, step192]: loss 0.589788
[epoch13, step193]: loss 0.662834
[epoch13, step194]: loss 0.519315
[epoch13, step195]: loss 0.669533
[epoch13, step196]: loss 0.406468
[epoch13, step197]: loss 0.398135
[epoch13, step198]: loss 0.387134
[epoch13, step199]: loss 0.562176
[epoch13, step200]: loss 0.632584
[epoch13, step201]: loss 0.612044
[epoch13, step202]: loss 0.332442
[epoch13, step203]: loss 0.687090
[epoch13, step204]: loss 0.357804
[epoch13, step205]: loss 0.507525
[epoch13, step206]: loss 0.271422
[epoch13, step207]: loss 0.528141
[epoch13, step208]: loss 0.576216
[epoch13, step209]: loss 0.270394
[epoch13, step210]: loss 0.491029
[epoch13, step211]: loss 0.471051
[epoch13, step212]: loss 0.495260
[epoch13, step213]: loss 0.431087
[epoch13, step214]: loss 0.458448
[epoch13, step215]: loss 0.623744
[epoch13, step216]: loss 0.570321
[epoch13, step217]: loss 0.283498
[epoch13, step218]: loss 0.491008
[epoch13, step219]: loss 0.236991
[epoch13, step220]: loss 0.359907
[epoch13, step221]: loss 0.418406
[epoch13, step222]: loss 0.467885
[epoch13, step223]: loss 0.446425
[epoch13, step224]: loss 0.672005
[epoch13, step225]: loss 0.548161
[epoch13, step226]: loss 0.724153
[epoch13, step227]: loss 0.476239
[epoch13, step228]: loss 0.454071
[epoch13, step229]: loss 0.545893
[epoch13, step230]: loss 0.661914
[epoch13, step231]: loss 0.397671
[epoch13, step232]: loss 0.583830
[epoch13, step233]: loss 0.536542
[epoch13, step234]: loss 0.493050
[epoch13, step235]: loss 0.559728
[epoch13, step236]: loss 0.354380
[epoch13, step237]: loss 0.380793
[epoch13, step238]: loss 0.495501
[epoch13, step239]: loss 0.337092
[epoch13, step240]: loss 0.389025
[epoch13, step241]: loss 0.431430
[epoch13, step242]: loss 0.430534
[epoch13, step243]: loss 0.251969
[epoch13, step244]: loss 0.484808
[epoch13, step245]: loss 0.578747
[epoch13, step246]: loss 0.392796
[epoch13, step247]: loss 0.630366
[epoch13, step248]: loss 0.679899
[epoch13, step249]: loss 0.535413
[epoch13, step250]: loss 0.413987
[epoch13, step251]: loss 0.620616
[epoch13, step252]: loss 0.343504
[epoch13, step253]: loss 0.501108
[epoch13, step254]: loss 0.528474
[epoch13, step255]: loss 0.517704
[epoch13, step256]: loss 0.530820
[epoch13, step257]: loss 0.629073
[epoch13, step258]: loss 0.567904
[epoch13, step259]: loss 0.333313
[epoch13, step260]: loss 0.447752
[epoch13, step261]: loss 0.636894
[epoch13, step262]: loss 0.351095
[epoch13, step263]: loss 0.517554
[epoch13, step264]: loss 0.536438
[epoch13, step265]: loss 0.664920
[epoch13, step266]: loss 0.631852
[epoch13, step267]: loss 0.505459
[epoch13, step268]: loss 0.517797
[epoch13, step269]: loss 0.459764
[epoch13, step270]: loss 0.539086
[epoch13, step271]: loss 0.592915
[epoch13, step272]: loss 0.293196
[epoch13, step273]: loss 0.395845
[epoch13, step274]: loss 0.480152
[epoch13, step275]: loss 0.363851
[epoch13, step276]: loss 0.291058
[epoch13, step277]: loss 0.538557
[epoch13, step278]: loss 0.531576
[epoch13, step279]: loss 0.386192
[epoch13, step280]: loss 0.560575
[epoch13, step281]: loss 0.486161
[epoch13, step282]: loss 0.488914
[epoch13, step283]: loss 0.553632
[epoch13, step284]: loss 0.350073
[epoch13, step285]: loss 0.367529
[epoch13, step286]: loss 0.575969
[epoch13, step287]: loss 0.324306
[epoch13, step288]: loss 0.509163
[epoch13, step289]: loss 0.212198
[epoch13, step290]: loss 0.643883
[epoch13, step291]: loss 0.406406
[epoch13, step292]: loss 0.592008
[epoch13, step293]: loss 0.331887
[epoch13, step294]: loss 0.547790
[epoch13, step295]: loss 0.544754
[epoch13, step296]: loss 0.396552
[epoch13, step297]: loss 0.524238
[epoch13, step298]: loss 0.325824
[epoch13, step299]: loss 0.258357
[epoch13, step300]: loss 0.571170
[epoch13, step301]: loss 0.221738
[epoch13, step302]: loss 0.449880
[epoch13, step303]: loss 0.349444
[epoch13, step304]: loss 0.504615
[epoch13, step305]: loss 0.417756
[epoch13, step306]: loss 0.561966
[epoch13, step307]: loss 0.486036
[epoch13, step308]: loss 0.256660
[epoch13, step309]: loss 0.500095
[epoch13, step310]: loss 0.226761
[epoch13, step311]: loss 0.377970
[epoch13, step312]: loss 0.473362
[epoch13, step313]: loss 0.659061
[epoch13, step314]: loss 0.288218
[epoch13, step315]: loss 0.481463
[epoch13, step316]: loss 0.354821
[epoch13, step317]: loss 0.456312
[epoch13, step318]: loss 0.289385
[epoch13, step319]: loss 0.542008
[epoch13, step320]: loss 0.446088
[epoch13, step321]: loss 0.282305
[epoch13, step322]: loss 0.271184
[epoch13, step323]: loss 0.410661
[epoch13, step324]: loss 0.471280
[epoch13, step325]: loss 0.608604
[epoch13, step326]: loss 0.556131
[epoch13, step327]: loss 0.356972
[epoch13, step328]: loss 0.383458
[epoch13, step329]: loss 0.439398
[epoch13, step330]: loss 0.452458
[epoch13, step331]: loss 0.230905
[epoch13, step332]: loss 0.487390
[epoch13, step333]: loss 0.345762
[epoch13, step334]: loss 0.618402
[epoch13, step335]: loss 0.543923
[epoch13, step336]: loss 0.468021
[epoch13, step337]: loss 0.549373
[epoch13, step338]: loss 0.547762
[epoch13, step339]: loss 0.728854
[epoch13, step340]: loss 0.530953
[epoch13, step341]: loss 0.636276
[epoch13, step342]: loss 0.359214
[epoch13, step343]: loss 0.367630
[epoch13, step344]: loss 0.448017
[epoch13, step345]: loss 0.481181
[epoch13, step346]: loss 0.558048
[epoch13, step347]: loss 0.546310
[epoch13, step348]: loss 0.532381
[epoch13, step349]: loss 0.389125
[epoch13, step350]: loss 0.361563
[epoch13, step351]: loss 0.451135
[epoch13, step352]: loss 0.367642
[epoch13, step353]: loss 0.375404
[epoch13, step354]: loss 0.478512
[epoch13, step355]: loss 0.692185
[epoch13, step356]: loss 0.496786
[epoch13, step357]: loss 0.159691
[epoch13, step358]: loss 0.387813
[epoch13, step359]: loss 0.362789
[epoch13, step360]: loss 0.643486
[epoch13, step361]: loss 0.593819
[epoch13, step362]: loss 0.438266
[epoch13, step363]: loss 0.394794
[epoch13, step364]: loss 0.283031
[epoch13, step365]: loss 0.519691
[epoch13, step366]: loss 0.601260
[epoch13, step367]: loss 0.649411
[epoch13, step368]: loss 0.558820
[epoch13, step369]: loss 0.380472
[epoch13, step370]: loss 0.356863
[epoch13, step371]: loss 0.567887
[epoch13, step372]: loss 0.580704
[epoch13, step373]: loss 0.667630
[epoch13, step374]: loss 0.680318
[epoch13, step375]: loss 0.278937
[epoch13, step376]: loss 0.473115
[epoch13, step377]: loss 0.699396
[epoch13, step378]: loss 0.470904
[epoch13, step379]: loss 0.443583
[epoch13, step380]: loss 0.549242
[epoch13, step381]: loss 0.703230
[epoch13, step382]: loss 0.226435
[epoch13, step383]: loss 0.715080
[epoch13, step384]: loss 0.319146
[epoch13, step385]: loss 0.457771
[epoch13, step386]: loss 0.546004
[epoch13, step387]: loss 0.450659
[epoch13, step388]: loss 0.502942
[epoch13, step389]: loss 0.442255
[epoch13, step390]: loss 0.353908
[epoch13, step391]: loss 0.321904
[epoch13, step392]: loss 0.511510
[epoch13, step393]: loss 0.471943
[epoch13, step394]: loss 0.433939
[epoch13, step395]: loss 0.397627
[epoch13, step396]: loss 0.396474
[epoch13, step397]: loss 0.474327
[epoch13, step398]: loss 0.662978
[epoch13, step399]: loss 0.383452
[epoch13, step400]: loss 0.520554
[epoch13, step401]: loss 0.536236
[epoch13, step402]: loss 0.592265
[epoch13, step403]: loss 0.398867
[epoch13, step404]: loss 0.624582
[epoch13, step405]: loss 0.392895
[epoch13, step406]: loss 0.665146
[epoch13, step407]: loss 0.296137
[epoch13, step408]: loss 0.609974
[epoch13, step409]: loss 0.567184
[epoch13, step410]: loss 0.223087
[epoch13, step411]: loss 0.641405
[epoch13, step412]: loss 0.282793
[epoch13, step413]: loss 0.689558
[epoch13, step414]: loss 0.368922
[epoch13, step415]: loss 0.326822
[epoch13, step416]: loss 0.459634
[epoch13, step417]: loss 0.569533
[epoch13, step418]: loss 0.326054
[epoch13, step419]: loss 0.688620
[epoch13, step420]: loss 0.440859
[epoch13, step421]: loss 0.596576
[epoch13, step422]: loss 0.530877
[epoch13, step423]: loss 0.263606
[epoch13, step424]: loss 0.429749
[epoch13, step425]: loss 0.570891
[epoch13, step426]: loss 0.588329
[epoch13, step427]: loss 0.756121
[epoch13, step428]: loss 0.531090
[epoch13, step429]: loss 0.626543
[epoch13, step430]: loss 0.348313
[epoch13, step431]: loss 0.582766
[epoch13, step432]: loss 0.369798
[epoch13, step433]: loss 0.627605
[epoch13, step434]: loss 0.614933
[epoch13, step435]: loss 0.278219
[epoch13, step436]: loss 0.564948
[epoch13, step437]: loss 0.562274
[epoch13, step438]: loss 0.457158
[epoch13, step439]: loss 0.506377
[epoch13, step440]: loss 0.612499
[epoch13, step441]: loss 0.518833
[epoch13, step442]: loss 0.599595
[epoch13, step443]: loss 0.672599
[epoch13, step444]: loss 0.389611
[epoch13, step445]: loss 0.409196
[epoch13, step446]: loss 0.239738
[epoch13, step447]: loss 0.427194
[epoch13, step448]: loss 0.346690
[epoch13, step449]: loss 0.565923
[epoch13, step450]: loss 0.429576
[epoch13, step451]: loss 0.466495
[epoch13, step452]: loss 0.581308
[epoch13, step453]: loss 0.175161
[epoch13, step454]: loss 0.358466
[epoch13, step455]: loss 0.440561
[epoch13, step456]: loss 0.434725
[epoch13, step457]: loss 0.609883
[epoch13, step458]: loss 0.283623
[epoch13, step459]: loss 0.535679
[epoch13, step460]: loss 0.590895
[epoch13, step461]: loss 0.455000
[epoch13, step462]: loss 0.602024
[epoch13, step463]: loss 0.454858
[epoch13, step464]: loss 0.617039
[epoch13, step465]: loss 0.440461
[epoch13, step466]: loss 0.575251
[epoch13, step467]: loss 0.559855
[epoch13, step468]: loss 0.415426
[epoch13, step469]: loss 0.427283
[epoch13, step470]: loss 0.431503
[epoch13, step471]: loss 0.571026
[epoch13, step472]: loss 0.463256
[epoch13, step473]: loss 0.712069
[epoch13, step474]: loss 0.580092
[epoch13, step475]: loss 0.371588
[epoch13, step476]: loss 0.698909
[epoch13, step477]: loss 0.387995
[epoch13, step478]: loss 0.392995
[epoch13, step479]: loss 0.549486
[epoch13, step480]: loss 0.468961
[epoch13, step481]: loss 0.263753
[epoch13, step482]: loss 0.521931
[epoch13, step483]: loss 0.461114
[epoch13, step484]: loss 0.668035
[epoch13, step485]: loss 0.432290
[epoch13, step486]: loss 0.406992
[epoch13, step487]: loss 0.346667
[epoch13, step488]: loss 0.725989
[epoch13, step489]: loss 0.410419
[epoch13, step490]: loss 0.558367
[epoch13, step491]: loss 0.409216
[epoch13, step492]: loss 0.647092
[epoch13, step493]: loss 0.523679
[epoch13, step494]: loss 0.660243
[epoch13, step495]: loss 0.328919
[epoch13, step496]: loss 0.418950
[epoch13, step497]: loss 0.556134
[epoch13, step498]: loss 0.562015
[epoch13, step499]: loss 0.337987
[epoch13, step500]: loss 0.500027
[epoch13, step501]: loss 0.466809
[epoch13, step502]: loss 0.574678
[epoch13, step503]: loss 0.570484
[epoch13, step504]: loss 0.439272
[epoch13, step505]: loss 0.496270
[epoch13, step506]: loss 0.346392
[epoch13, step507]: loss 0.579882
[epoch13, step508]: loss 0.500798
[epoch13, step509]: loss 0.609665
[epoch13, step510]: loss 0.562389
[epoch13, step511]: loss 0.523339
[epoch13, step512]: loss 0.412105
[epoch13, step513]: loss 0.380189
[epoch13, step514]: loss 0.420094
[epoch13, step515]: loss 0.281903
[epoch13, step516]: loss 0.487815
[epoch13, step517]: loss 0.471582
[epoch13, step518]: loss 0.125137
[epoch13, step519]: loss 0.542058
[epoch13, step520]: loss 0.439620
[epoch13, step521]: loss 0.277830
[epoch13, step522]: loss 0.430498
[epoch13, step523]: loss 0.754993
[epoch13, step524]: loss 0.399826
[epoch13, step525]: loss 0.489048
[epoch13, step526]: loss 0.473803
[epoch13, step527]: loss 0.411127
[epoch13, step528]: loss 0.363283
[epoch13, step529]: loss 0.455723
[epoch13, step530]: loss 0.328141
[epoch13, step531]: loss 0.393376
[epoch13, step532]: loss 0.343480
[epoch13, step533]: loss 0.574945
[epoch13, step534]: loss 0.441037
[epoch13, step535]: loss 0.437584
[epoch13, step536]: loss 0.643272
[epoch13, step537]: loss 0.223307
[epoch13, step538]: loss 0.383298
[epoch13, step539]: loss 0.558167
[epoch13, step540]: loss 0.427240
[epoch13, step541]: loss 0.375257
[epoch13, step542]: loss 0.296061
[epoch13, step543]: loss 0.585189
[epoch13, step544]: loss 0.730314
[epoch13, step545]: loss 0.569456
[epoch13, step546]: loss 0.257007
[epoch13, step547]: loss 0.599316
[epoch13, step548]: loss 0.238935
[epoch13, step549]: loss 0.342798
[epoch13, step550]: loss 0.613766
[epoch13, step551]: loss 0.299414
[epoch13, step552]: loss 0.483924
[epoch13, step553]: loss 0.378655
[epoch13, step554]: loss 0.380040
[epoch13, step555]: loss 0.449526
[epoch13, step556]: loss 0.526412
[epoch13, step557]: loss 0.432983
[epoch13, step558]: loss 0.705064
[epoch13, step559]: loss 0.577032
[epoch13, step560]: loss 0.592398
[epoch13, step561]: loss 0.243201
[epoch13, step562]: loss 0.607098
[epoch13, step563]: loss 0.535323
[epoch13, step564]: loss 0.560496
[epoch13, step565]: loss 0.493661
[epoch13, step566]: loss 0.569276
[epoch13, step567]: loss 0.615315
[epoch13, step568]: loss 0.411245
[epoch13, step569]: loss 0.450058
[epoch13, step570]: loss 0.259597
[epoch13, step571]: loss 0.503529
[epoch13, step572]: loss 0.631069
[epoch13, step573]: loss 0.574656
[epoch13, step574]: loss 0.539945
[epoch13, step575]: loss 0.280605
[epoch13, step576]: loss 0.518374
[epoch13, step577]: loss 0.464821
[epoch13, step578]: loss 0.794129
[epoch13, step579]: loss 0.504793
[epoch13, step580]: loss 0.438109
[epoch13, step581]: loss 0.564180
[epoch13, step582]: loss 0.621131
[epoch13, step583]: loss 0.158625
[epoch13, step584]: loss 0.432205
[epoch13, step585]: loss 0.356538
[epoch13, step586]: loss 0.573287
[epoch13, step587]: loss 0.464446
[epoch13, step588]: loss 0.613739
[epoch13, step589]: loss 0.556406
[epoch13, step590]: loss 0.420192
[epoch13, step591]: loss 0.468982
[epoch13, step592]: loss 0.468808
[epoch13, step593]: loss 0.383307
[epoch13, step594]: loss 0.605080
[epoch13, step595]: loss 0.509304
[epoch13, step596]: loss 0.552090
[epoch13, step597]: loss 0.602953
[epoch13, step598]: loss 0.471251
[epoch13, step599]: loss 0.604714
[epoch13, step600]: loss 0.504849
[epoch13, step601]: loss 0.713401
[epoch13, step602]: loss 0.276604
[epoch13, step603]: loss 0.554110
[epoch13, step604]: loss 0.379256
[epoch13, step605]: loss 0.500977
[epoch13, step606]: loss 0.364142
[epoch13, step607]: loss 0.238079
[epoch13, step608]: loss 0.309131
[epoch13, step609]: loss 0.497961
[epoch13, step610]: loss 0.801785
[epoch13, step611]: loss 0.654930
[epoch13, step612]: loss 0.202597
[epoch13, step613]: loss 0.352491
[epoch13, step614]: loss 0.503335
[epoch13, step615]: loss 0.404856
[epoch13, step616]: loss 0.435022
[epoch13, step617]: loss 0.512599
[epoch13, step618]: loss 0.374960
[epoch13, step619]: loss 0.508133
[epoch13, step620]: loss 0.729911
[epoch13, step621]: loss 0.267286
[epoch13, step622]: loss 0.693727
[epoch13, step623]: loss 0.464630
[epoch13, step624]: loss 0.295212
[epoch13, step625]: loss 0.386056
[epoch13, step626]: loss 0.566691
[epoch13, step627]: loss 0.255229
[epoch13, step628]: loss 0.356532
[epoch13, step629]: loss 0.546491
[epoch13, step630]: loss 0.484807
[epoch13, step631]: loss 0.544985
[epoch13, step632]: loss 0.446385
[epoch13, step633]: loss 0.387831
[epoch13, step634]: loss 0.556259
[epoch13, step635]: loss 0.540003
[epoch13, step636]: loss 0.521654
[epoch13, step637]: loss 0.466040
[epoch13, step638]: loss 0.668899
[epoch13, step639]: loss 0.821453
[epoch13, step640]: loss 0.657516
[epoch13, step641]: loss 0.458318
[epoch13, step642]: loss 0.475919
[epoch13, step643]: loss 0.613824
[epoch13, step644]: loss 0.510188
[epoch13, step645]: loss 0.313382
[epoch13, step646]: loss 0.534770
[epoch13, step647]: loss 0.485471
[epoch13, step648]: loss 0.443111
[epoch13, step649]: loss 0.779699
[epoch13, step650]: loss 0.321678
[epoch13, step651]: loss 0.511171
[epoch13, step652]: loss 0.529656
[epoch13, step653]: loss 0.537835
[epoch13, step654]: loss 0.533170
[epoch13, step655]: loss 0.468181
[epoch13, step656]: loss 0.352721
[epoch13, step657]: loss 0.392161
[epoch13, step658]: loss 0.337714
[epoch13, step659]: loss 0.530984
[epoch13, step660]: loss 0.112647
[epoch13, step661]: loss 0.480247
[epoch13, step662]: loss 0.419116
[epoch13, step663]: loss 0.676229
[epoch13, step664]: loss 0.447978
[epoch13, step665]: loss 0.678929
[epoch13, step666]: loss 0.664894
[epoch13, step667]: loss 0.361627
[epoch13, step668]: loss 0.587631
[epoch13, step669]: loss 0.583584
[epoch13, step670]: loss 0.634835
[epoch13, step671]: loss 0.512522
[epoch13, step672]: loss 0.363575
[epoch13, step673]: loss 0.343128
[epoch13, step674]: loss 0.595748
[epoch13, step675]: loss 0.277730
[epoch13, step676]: loss 0.457199
[epoch13, step677]: loss 0.529093
[epoch13, step678]: loss 0.628724
[epoch13, step679]: loss 0.585563
[epoch13, step680]: loss 0.419361
[epoch13, step681]: loss 0.554476
[epoch13, step682]: loss 0.247990
[epoch13, step683]: loss 0.537789
[epoch13, step684]: loss 0.370450
[epoch13, step685]: loss 0.136993
[epoch13, step686]: loss 0.567828
[epoch13, step687]: loss 0.633973
[epoch13, step688]: loss 0.485820
[epoch13, step689]: loss 0.150124
[epoch13, step690]: loss 0.628510
[epoch13, step691]: loss 0.591692
[epoch13, step692]: loss 0.506401
[epoch13, step693]: loss 0.507020
[epoch13, step694]: loss 0.417310
[epoch13, step695]: loss 0.405815
[epoch13, step696]: loss 0.559718
[epoch13, step697]: loss 0.569283
[epoch13, step698]: loss 0.116526
[epoch13, step699]: loss 0.519303
[epoch13, step700]: loss 0.406211
[epoch13, step701]: loss 0.601820
[epoch13, step702]: loss 0.283649
[epoch13, step703]: loss 0.506360
[epoch13, step704]: loss 0.391923
[epoch13, step705]: loss 0.404495
[epoch13, step706]: loss 0.508380
[epoch13, step707]: loss 0.294286
[epoch13, step708]: loss 0.615917
[epoch13, step709]: loss 0.421807
[epoch13, step710]: loss 0.482302
[epoch13, step711]: loss 0.459204
[epoch13, step712]: loss 0.672024
[epoch13, step713]: loss 0.487637
[epoch13, step714]: loss 0.526770
[epoch13, step715]: loss 0.339267
[epoch13, step716]: loss 0.260364
[epoch13, step717]: loss 0.263262
[epoch13, step718]: loss 0.381436
[epoch13, step719]: loss 0.565080
[epoch13, step720]: loss 0.547355
[epoch13, step721]: loss 0.691214
[epoch13, step722]: loss 0.473169
[epoch13, step723]: loss 0.303231
[epoch13, step724]: loss 0.437859
[epoch13, step725]: loss 0.527180
[epoch13, step726]: loss 0.447966
[epoch13, step727]: loss 0.680276
[epoch13, step728]: loss 0.433201
[epoch13, step729]: loss 0.604926
[epoch13, step730]: loss 0.302846
[epoch13, step731]: loss 0.494281
[epoch13, step732]: loss 0.497240
[epoch13, step733]: loss 0.343100
[epoch13, step734]: loss 0.569404
[epoch13, step735]: loss 0.392460
[epoch13, step736]: loss 0.380313
[epoch13, step737]: loss 0.601741
[epoch13, step738]: loss 0.407479
[epoch13, step739]: loss 0.369274
[epoch13, step740]: loss 0.451034
[epoch13, step741]: loss 0.412488
[epoch13, step742]: loss 0.311776
[epoch13, step743]: loss 0.500637
[epoch13, step744]: loss 0.616545
[epoch13, step745]: loss 0.243018
[epoch13, step746]: loss 0.483334
[epoch13, step747]: loss 0.369046
[epoch13, step748]: loss 0.318187
[epoch13, step749]: loss 0.430858
[epoch13, step750]: loss 0.336015
[epoch13, step751]: loss 0.532653
[epoch13, step752]: loss 0.424602
[epoch13, step753]: loss 0.422911
[epoch13, step754]: loss 0.268344
[epoch13, step755]: loss 0.570761
[epoch13, step756]: loss 0.297344
[epoch13, step757]: loss 0.601847
[epoch13, step758]: loss 0.247943
[epoch13, step759]: loss 0.519402
[epoch13, step760]: loss 0.640372
[epoch13, step761]: loss 0.565487
[epoch13, step762]: loss 0.559550
[epoch13, step763]: loss 0.563061
[epoch13, step764]: loss 0.479996
[epoch13, step765]: loss 0.508380
[epoch13, step766]: loss 0.356881
[epoch13, step767]: loss 0.363025
[epoch13, step768]: loss 0.417102
[epoch13, step769]: loss 0.465635
[epoch13, step770]: loss 0.411906
[epoch13, step771]: loss 0.512606
[epoch13, step772]: loss 0.431772
[epoch13, step773]: loss 0.559070
[epoch13, step774]: loss 0.496539
[epoch13, step775]: loss 0.565043
[epoch13, step776]: loss 0.469942
[epoch13, step777]: loss 0.591815
[epoch13, step778]: loss 0.394810
[epoch13, step779]: loss 0.646947
[epoch13, step780]: loss 0.421308
[epoch13, step781]: loss 0.624984
[epoch13, step782]: loss 0.227180
[epoch13, step783]: loss 0.281395
[epoch13, step784]: loss 0.576045
[epoch13, step785]: loss 0.572311
[epoch13, step786]: loss 0.376701
[epoch13, step787]: loss 0.254650
[epoch13, step788]: loss 0.473080
[epoch13, step789]: loss 0.300372
[epoch13, step790]: loss 0.558401
[epoch13, step791]: loss 0.381852
[epoch13, step792]: loss 0.616956
[epoch13, step793]: loss 0.299505
[epoch13, step794]: loss 0.510355
[epoch13, step795]: loss 0.583631
[epoch13, step796]: loss 0.471451
[epoch13, step797]: loss 0.639394
[epoch13, step798]: loss 0.747375
[epoch13, step799]: loss 0.607504
[epoch13, step800]: loss 0.586764
[epoch13, step801]: loss 0.423275
[epoch13, step802]: loss 0.568225
[epoch13, step803]: loss 0.406044
[epoch13, step804]: loss 0.497002
[epoch13, step805]: loss 0.577378
[epoch13, step806]: loss 0.244611
[epoch13, step807]: loss 0.566276
[epoch13, step808]: loss 0.637555
[epoch13, step809]: loss 0.576037
[epoch13, step810]: loss 0.565398
[epoch13, step811]: loss 0.462823
[epoch13, step812]: loss 0.341712
[epoch13, step813]: loss 0.505406
[epoch13, step814]: loss 0.428439
[epoch13, step815]: loss 0.577041
[epoch13, step816]: loss 0.424885
[epoch13, step817]: loss 0.552529
[epoch13, step818]: loss 0.467454
[epoch13, step819]: loss 0.634436
[epoch13, step820]: loss 0.286125
[epoch13, step821]: loss 0.523225
[epoch13, step822]: loss 0.532391
[epoch13, step823]: loss 0.632392
[epoch13, step824]: loss 0.746484
[epoch13, step825]: loss 0.727917
[epoch13, step826]: loss 0.566751
[epoch13, step827]: loss 0.577232
[epoch13, step828]: loss 0.372860
[epoch13, step829]: loss 0.623466
[epoch13, step830]: loss 0.378249
[epoch13, step831]: loss 0.463011
[epoch13, step832]: loss 0.638174
[epoch13, step833]: loss 0.365181
[epoch13, step834]: loss 0.757722
[epoch13, step835]: loss 0.527971
[epoch13, step836]: loss 0.703231
[epoch13, step837]: loss 0.465714
[epoch13, step838]: loss 0.459316
[epoch13, step839]: loss 0.518605
[epoch13, step840]: loss 0.733460
[epoch13, step841]: loss 0.539031
[epoch13, step842]: loss 0.640678
[epoch13, step843]: loss 0.428380
[epoch13, step844]: loss 0.274342
[epoch13, step845]: loss 0.618235
[epoch13, step846]: loss 0.343182
[epoch13, step847]: loss 0.614901
[epoch13, step848]: loss 0.394442
[epoch13, step849]: loss 0.486958
[epoch13, step850]: loss 0.361668
[epoch13, step851]: loss 0.255510
[epoch13, step852]: loss 0.424707
[epoch13, step853]: loss 0.236645
[epoch13, step854]: loss 0.393695
[epoch13, step855]: loss 0.400428
[epoch13, step856]: loss 0.509525
[epoch13, step857]: loss 0.544092
[epoch13, step858]: loss 0.599703
[epoch13, step859]: loss 0.501570
[epoch13, step860]: loss 0.651636
[epoch13, step861]: loss 0.490463
[epoch13, step862]: loss 0.379246
[epoch13, step863]: loss 0.740430
[epoch13, step864]: loss 0.558316
[epoch13, step865]: loss 0.309013
[epoch13, step866]: loss 0.626223
[epoch13, step867]: loss 0.404031
[epoch13, step868]: loss 0.685629
[epoch13, step869]: loss 0.444297
[epoch13, step870]: loss 0.378959
[epoch13, step871]: loss 0.469321
[epoch13, step872]: loss 0.552368
[epoch13, step873]: loss 0.551336
[epoch13, step874]: loss 0.164128
[epoch13, step875]: loss 0.114098
[epoch13, step876]: loss 0.569280
[epoch13, step877]: loss 0.699323
[epoch13, step878]: loss 0.465774
[epoch13, step879]: loss 0.213899
[epoch13, step880]: loss 0.384507
[epoch13, step881]: loss 0.455911
[epoch13, step882]: loss 0.151902
[epoch13, step883]: loss 0.169146
[epoch13, step884]: loss 0.343206
[epoch13, step885]: loss 0.543467
[epoch13, step886]: loss 0.528603
[epoch13, step887]: loss 0.337149
[epoch13, step888]: loss 0.311432
[epoch13, step889]: loss 0.448926
[epoch13, step890]: loss 0.637516
[epoch13, step891]: loss 0.508263
[epoch13, step892]: loss 0.681125
[epoch13, step893]: loss 0.408109
[epoch13, step894]: loss 0.592973
[epoch13, step895]: loss 0.464610
[epoch13, step896]: loss 0.474676
[epoch13, step897]: loss 0.614278
[epoch13, step898]: loss 0.376886
[epoch13, step899]: loss 0.540708
[epoch13, step900]: loss 0.586482
[epoch13, step901]: loss 0.462718
[epoch13, step902]: loss 0.736776
[epoch13, step903]: loss 0.472587
[epoch13, step904]: loss 0.681770
[epoch13, step905]: loss 0.367000
[epoch13, step906]: loss 0.430871
[epoch13, step907]: loss 0.603361
[epoch13, step908]: loss 0.515964
[epoch13, step909]: loss 0.550812
[epoch13, step910]: loss 0.450976
[epoch13, step911]: loss 0.577777
[epoch13, step912]: loss 0.542285
[epoch13, step913]: loss 0.251553
[epoch13, step914]: loss 0.403874
[epoch13, step915]: loss 0.539985
[epoch13, step916]: loss 0.556842
[epoch13, step917]: loss 0.423048
[epoch13, step918]: loss 0.632026
[epoch13, step919]: loss 0.472877
[epoch13, step920]: loss 0.491078
[epoch13, step921]: loss 0.410948
[epoch13, step922]: loss 0.352842
[epoch13, step923]: loss 0.557513
[epoch13, step924]: loss 0.398273
[epoch13, step925]: loss 0.390007
[epoch13, step926]: loss 0.605767
[epoch13, step927]: loss 0.643232
[epoch13, step928]: loss 0.653590
[epoch13, step929]: loss 0.510256
[epoch13, step930]: loss 0.547373
[epoch13, step931]: loss 0.514371
[epoch13, step932]: loss 0.419490
[epoch13, step933]: loss 0.446951
[epoch13, step934]: loss 0.537781
[epoch13, step935]: loss 0.371632
[epoch13, step936]: loss 0.610875
[epoch13, step937]: loss 0.554843
[epoch13, step938]: loss 0.507322
[epoch13, step939]: loss 0.452645
[epoch13, step940]: loss 0.386859
[epoch13, step941]: loss 0.407869
[epoch13, step942]: loss 0.231385
[epoch13, step943]: loss 0.417959
[epoch13, step944]: loss 0.158650
[epoch13, step945]: loss 0.525562
[epoch13, step946]: loss 0.589838
[epoch13, step947]: loss 0.398544
[epoch13, step948]: loss 0.599884
[epoch13, step949]: loss 0.143406
[epoch13, step950]: loss 0.561498
[epoch13, step951]: loss 0.169355
[epoch13, step952]: loss 0.567649
[epoch13, step953]: loss 0.350390
[epoch13, step954]: loss 0.708134
[epoch13, step955]: loss 0.609434
[epoch13, step956]: loss 0.444445
[epoch13, step957]: loss 0.359740
[epoch13, step958]: loss 0.384169
[epoch13, step959]: loss 0.277154
[epoch13, step960]: loss 0.558064
[epoch13, step961]: loss 0.568554
[epoch13, step962]: loss 0.454246
[epoch13, step963]: loss 0.593841
[epoch13, step964]: loss 0.456714
[epoch13, step965]: loss 0.436733
[epoch13, step966]: loss 0.608474
[epoch13, step967]: loss 0.612063
[epoch13, step968]: loss 0.503143
[epoch13, step969]: loss 0.432337
[epoch13, step970]: loss 0.440898
[epoch13, step971]: loss 0.459397
[epoch13, step972]: loss 0.585763
[epoch13, step973]: loss 0.636974
[epoch13, step974]: loss 0.615218
[epoch13, step975]: loss 0.471251
[epoch13, step976]: loss 0.637021
[epoch13, step977]: loss 0.517985
[epoch13, step978]: loss 0.465071
[epoch13, step979]: loss 0.401967
[epoch13, step980]: loss 0.548137
[epoch13, step981]: loss 0.445201
[epoch13, step982]: loss 0.527454
[epoch13, step983]: loss 0.705257
[epoch13, step984]: loss 0.546917
[epoch13, step985]: loss 0.453647
[epoch13, step986]: loss 0.461940
[epoch13, step987]: loss 0.665817
[epoch13, step988]: loss 0.444430
[epoch13, step989]: loss 0.600187
[epoch13, step990]: loss 0.473161
[epoch13, step991]: loss 0.515770
[epoch13, step992]: loss 0.393240
[epoch13, step993]: loss 0.421359
[epoch13, step994]: loss 0.351683
[epoch13, step995]: loss 0.480924
[epoch13, step996]: loss 0.559647
[epoch13, step997]: loss 0.343640
[epoch13, step998]: loss 0.547344
[epoch13, step999]: loss 0.467535
[epoch13, step1000]: loss 0.533577
[epoch13, step1001]: loss 0.624000
[epoch13, step1002]: loss 0.424348
[epoch13, step1003]: loss 0.482615
[epoch13, step1004]: loss 0.703074
[epoch13, step1005]: loss 0.408411
[epoch13, step1006]: loss 0.417650
[epoch13, step1007]: loss 0.609527
[epoch13, step1008]: loss 0.325158
[epoch13, step1009]: loss 0.519813
[epoch13, step1010]: loss 0.739217
[epoch13, step1011]: loss 0.531851
[epoch13, step1012]: loss 0.443001
[epoch13, step1013]: loss 0.602942
[epoch13, step1014]: loss 0.442819
[epoch13, step1015]: loss 0.156380
[epoch13, step1016]: loss 0.594310
[epoch13, step1017]: loss 0.618374
[epoch13, step1018]: loss 0.484823
[epoch13, step1019]: loss 0.521284
[epoch13, step1020]: loss 0.321937
[epoch13, step1021]: loss 0.623813
[epoch13, step1022]: loss 0.391710
[epoch13, step1023]: loss 0.615978
[epoch13, step1024]: loss 0.595086
[epoch13, step1025]: loss 0.301613
[epoch13, step1026]: loss 0.310756
[epoch13, step1027]: loss 0.624609
[epoch13, step1028]: loss 0.531919
[epoch13, step1029]: loss 0.429401
[epoch13, step1030]: loss 0.457661
[epoch13, step1031]: loss 0.495306
[epoch13, step1032]: loss 0.509136
[epoch13, step1033]: loss 0.650081
[epoch13, step1034]: loss 0.541667
[epoch13, step1035]: loss 0.448539
[epoch13, step1036]: loss 0.281791
[epoch13, step1037]: loss 0.497310
[epoch13, step1038]: loss 0.579256
[epoch13, step1039]: loss 0.610383
[epoch13, step1040]: loss 0.427835
[epoch13, step1041]: loss 0.744891
[epoch13, step1042]: loss 0.635286
[epoch13, step1043]: loss 0.520209
[epoch13, step1044]: loss 0.298576
[epoch13, step1045]: loss 0.559403
[epoch13, step1046]: loss 0.388779
[epoch13, step1047]: loss 0.397925
[epoch13, step1048]: loss 0.345279
[epoch13, step1049]: loss 0.409991
[epoch13, step1050]: loss 0.360583
[epoch13, step1051]: loss 0.595362
[epoch13, step1052]: loss 0.316049
[epoch13, step1053]: loss 0.510581
[epoch13, step1054]: loss 0.344955
[epoch13, step1055]: loss 0.377871
[epoch13, step1056]: loss 0.369854
[epoch13, step1057]: loss 0.367211
[epoch13, step1058]: loss 0.371505
[epoch13, step1059]: loss 0.622949
[epoch13, step1060]: loss 0.501580
[epoch13, step1061]: loss 0.311185
[epoch13, step1062]: loss 0.386493
[epoch13, step1063]: loss 0.499659
[epoch13, step1064]: loss 0.272399
[epoch13, step1065]: loss 0.537406
[epoch13, step1066]: loss 0.459441
[epoch13, step1067]: loss 0.381380
[epoch13, step1068]: loss 0.161215
[epoch13, step1069]: loss 0.528737
[epoch13, step1070]: loss 0.432187
[epoch13, step1071]: loss 0.591082
[epoch13, step1072]: loss 0.640125
[epoch13, step1073]: loss 0.251472
[epoch13, step1074]: loss 0.592567
[epoch13, step1075]: loss 0.526305
[epoch13, step1076]: loss 0.645920
[epoch13, step1077]: loss 0.370798
[epoch13, step1078]: loss 0.548185
[epoch13, step1079]: loss 0.553338
[epoch13, step1080]: loss 0.660419
[epoch13, step1081]: loss 0.640889
[epoch13, step1082]: loss 0.266584
[epoch13, step1083]: loss 0.562906
[epoch13, step1084]: loss 0.350382
[epoch13, step1085]: loss 0.795861
[epoch13, step1086]: loss 0.470871
[epoch13, step1087]: loss 0.557366
[epoch13, step1088]: loss 0.232381
[epoch13, step1089]: loss 0.704845
[epoch13, step1090]: loss 0.234842
[epoch13, step1091]: loss 0.475910
[epoch13, step1092]: loss 0.505035
[epoch13, step1093]: loss 0.515697
[epoch13, step1094]: loss 0.662803
[epoch13, step1095]: loss 0.289420
[epoch13, step1096]: loss 0.388904
[epoch13, step1097]: loss 0.142438
[epoch13, step1098]: loss 0.155837
[epoch13, step1099]: loss 0.394787
[epoch13, step1100]: loss 0.422430
[epoch13, step1101]: loss 0.670511
[epoch13, step1102]: loss 0.417265
[epoch13, step1103]: loss 0.392704
[epoch13, step1104]: loss 0.427764
[epoch13, step1105]: loss 0.506187
[epoch13, step1106]: loss 0.390864
[epoch13, step1107]: loss 0.601100
[epoch13, step1108]: loss 0.471030
[epoch13, step1109]: loss 0.476235
[epoch13, step1110]: loss 0.240161
[epoch13, step1111]: loss 0.416786
[epoch13, step1112]: loss 0.711433
[epoch13, step1113]: loss 0.638803
[epoch13, step1114]: loss 0.316102
[epoch13, step1115]: loss 0.503428
[epoch13, step1116]: loss 0.631654
[epoch13, step1117]: loss 0.282710
[epoch13, step1118]: loss 0.571135
[epoch13, step1119]: loss 0.558898
[epoch13, step1120]: loss 0.423624
[epoch13, step1121]: loss 0.544551
[epoch13, step1122]: loss 0.662440
[epoch13, step1123]: loss 0.587947
[epoch13, step1124]: loss 0.332492
[epoch13, step1125]: loss 0.518062
[epoch13, step1126]: loss 0.405560
[epoch13, step1127]: loss 0.716171
[epoch13, step1128]: loss 0.474148
[epoch13, step1129]: loss 0.578721
[epoch13, step1130]: loss 0.553785
[epoch13, step1131]: loss 0.610677
[epoch13, step1132]: loss 0.541956
[epoch13, step1133]: loss 0.401865
[epoch13, step1134]: loss 0.535550
[epoch13, step1135]: loss 0.601590
[epoch13, step1136]: loss 0.267802
[epoch13, step1137]: loss 0.485011
[epoch13, step1138]: loss 0.560986
[epoch13, step1139]: loss 0.474214
[epoch13, step1140]: loss 0.406844
[epoch13, step1141]: loss 0.560413
[epoch13, step1142]: loss 0.523263
[epoch13, step1143]: loss 0.413342
[epoch13, step1144]: loss 0.594280
[epoch13, step1145]: loss 0.138697
[epoch13, step1146]: loss 0.565214
[epoch13, step1147]: loss 0.498112
[epoch13, step1148]: loss 0.481114
[epoch13, step1149]: loss 0.596865
[epoch13, step1150]: loss 0.504725
[epoch13, step1151]: loss 0.324284
[epoch13, step1152]: loss 0.538444
[epoch13, step1153]: loss 0.474314
[epoch13, step1154]: loss 0.742390
[epoch13, step1155]: loss 0.325475
[epoch13, step1156]: loss 0.373250
[epoch13, step1157]: loss 0.367888
[epoch13, step1158]: loss 0.342465
[epoch13, step1159]: loss 0.465537
[epoch13, step1160]: loss 0.618159
[epoch13, step1161]: loss 0.555626
[epoch13, step1162]: loss 0.588200
[epoch13, step1163]: loss 0.550817
[epoch13, step1164]: loss 0.695858
[epoch13, step1165]: loss 0.469176
[epoch13, step1166]: loss 0.343896
[epoch13, step1167]: loss 0.542059
[epoch13, step1168]: loss 0.652930
[epoch13, step1169]: loss 0.441438
[epoch13, step1170]: loss 0.449089
[epoch13, step1171]: loss 0.492283
[epoch13, step1172]: loss 0.611806
[epoch13, step1173]: loss 0.592494
[epoch13, step1174]: loss 0.495095
[epoch13, step1175]: loss 0.594396
[epoch13, step1176]: loss 0.675357
[epoch13, step1177]: loss 0.497691
[epoch13, step1178]: loss 0.476252
[epoch13, step1179]: loss 0.592450
[epoch13, step1180]: loss 0.501633
[epoch13, step1181]: loss 0.410553
[epoch13, step1182]: loss 0.787865
[epoch13, step1183]: loss 0.422978
[epoch13, step1184]: loss 0.395787
[epoch13, step1185]: loss 0.482668
[epoch13, step1186]: loss 0.424153
[epoch13, step1187]: loss 0.434224
[epoch13, step1188]: loss 0.246265
[epoch13, step1189]: loss 0.539138
[epoch13, step1190]: loss 0.236447
[epoch13, step1191]: loss 0.417569
[epoch13, step1192]: loss 0.381100
[epoch13, step1193]: loss 0.428429
[epoch13, step1194]: loss 0.357845
[epoch13, step1195]: loss 0.471989
[epoch13, step1196]: loss 0.397738
[epoch13, step1197]: loss 0.270919
[epoch13, step1198]: loss 0.634799
[epoch13, step1199]: loss 0.364676
[epoch13, step1200]: loss 0.539733
[epoch13, step1201]: loss 0.545593
[epoch13, step1202]: loss 0.295953
[epoch13, step1203]: loss 0.632457
[epoch13, step1204]: loss 0.461279
[epoch13, step1205]: loss 0.226472
[epoch13, step1206]: loss 0.691726
[epoch13, step1207]: loss 0.519789
[epoch13, step1208]: loss 0.636217
[epoch13, step1209]: loss 0.500081
[epoch13, step1210]: loss 0.443988
[epoch13, step1211]: loss 0.414973
[epoch13, step1212]: loss 0.525996
[epoch13, step1213]: loss 0.405688
[epoch13, step1214]: loss 0.477180
[epoch13, step1215]: loss 0.608128
[epoch13, step1216]: loss 0.614222
[epoch13, step1217]: loss 0.671122
[epoch13, step1218]: loss 0.645103
[epoch13, step1219]: loss 0.710876
[epoch13, step1220]: loss 0.313309
[epoch13, step1221]: loss 0.466792
[epoch13, step1222]: loss 0.219857
[epoch13, step1223]: loss 0.430721
[epoch13, step1224]: loss 0.701494
[epoch13, step1225]: loss 0.656303
[epoch13, step1226]: loss 0.506142
[epoch13, step1227]: loss 0.450079
[epoch13, step1228]: loss 0.611304
[epoch13, step1229]: loss 0.571373
[epoch13, step1230]: loss 0.462770
[epoch13, step1231]: loss 0.143030
[epoch13, step1232]: loss 0.443632
[epoch13, step1233]: loss 0.525229
[epoch13, step1234]: loss 0.454925
[epoch13, step1235]: loss 0.381200
[epoch13, step1236]: loss 0.669773
[epoch13, step1237]: loss 0.460878
[epoch13, step1238]: loss 0.473303
[epoch13, step1239]: loss 0.570393
[epoch13, step1240]: loss 0.546346
[epoch13, step1241]: loss 0.404892
[epoch13, step1242]: loss 0.567154
[epoch13, step1243]: loss 0.529502
[epoch13, step1244]: loss 0.398787
[epoch13, step1245]: loss 0.598897
[epoch13, step1246]: loss 0.338670
[epoch13, step1247]: loss 0.393913
[epoch13, step1248]: loss 0.594525
[epoch13, step1249]: loss 0.545100
[epoch13, step1250]: loss 0.687511
[epoch13, step1251]: loss 0.517847
[epoch13, step1252]: loss 0.420031
[epoch13, step1253]: loss 0.483930
[epoch13, step1254]: loss 0.430521
[epoch13, step1255]: loss 0.718328
[epoch13, step1256]: loss 0.344000
[epoch13, step1257]: loss 0.405223
[epoch13, step1258]: loss 0.473188
[epoch13, step1259]: loss 0.292871
[epoch13, step1260]: loss 0.599067
[epoch13, step1261]: loss 0.383269
[epoch13, step1262]: loss 0.515467
[epoch13, step1263]: loss 0.224859
[epoch13, step1264]: loss 0.365814
[epoch13, step1265]: loss 0.428349
[epoch13, step1266]: loss 0.515396
[epoch13, step1267]: loss 0.368667
[epoch13, step1268]: loss 0.534234
[epoch13, step1269]: loss 0.528002
[epoch13, step1270]: loss 0.301803
[epoch13, step1271]: loss 0.645351
[epoch13, step1272]: loss 0.270141
[epoch13, step1273]: loss 0.505946
[epoch13, step1274]: loss 0.413681
[epoch13, step1275]: loss 0.457700
[epoch13, step1276]: loss 0.458118
[epoch13, step1277]: loss 0.258291
[epoch13, step1278]: loss 0.630931
[epoch13, step1279]: loss 0.707570
[epoch13, step1280]: loss 0.604432
[epoch13, step1281]: loss 0.450062
[epoch13, step1282]: loss 0.198670
[epoch13, step1283]: loss 0.376932
[epoch13, step1284]: loss 0.427752
[epoch13, step1285]: loss 0.223125
[epoch13, step1286]: loss 0.432117
[epoch13, step1287]: loss 0.347574
[epoch13, step1288]: loss 0.469907
[epoch13, step1289]: loss 0.498837
[epoch13, step1290]: loss 0.423087
[epoch13, step1291]: loss 0.473554
[epoch13, step1292]: loss 0.340437
[epoch13, step1293]: loss 0.661731
[epoch13, step1294]: loss 0.426008
[epoch13, step1295]: loss 0.407502
[epoch13, step1296]: loss 0.486647
[epoch13, step1297]: loss 0.530357
[epoch13, step1298]: loss 0.352683
[epoch13, step1299]: loss 0.487263
[epoch13, step1300]: loss 0.410944
[epoch13, step1301]: loss 0.237256
[epoch13, step1302]: loss 0.636954
[epoch13, step1303]: loss 0.496348
[epoch13, step1304]: loss 0.472671
[epoch13, step1305]: loss 0.357540
[epoch13, step1306]: loss 0.364376
[epoch13, step1307]: loss 0.343782
[epoch13, step1308]: loss 0.560522
[epoch13, step1309]: loss 0.391081
[epoch13, step1310]: loss 0.485102
[epoch13, step1311]: loss 0.440216
[epoch13, step1312]: loss 0.529541
[epoch13, step1313]: loss 0.275888
[epoch13, step1314]: loss 0.258170
[epoch13, step1315]: loss 0.445542
[epoch13, step1316]: loss 0.707665
[epoch13, step1317]: loss 0.356959
[epoch13, step1318]: loss 0.426679
[epoch13, step1319]: loss 0.635872
[epoch13, step1320]: loss 0.410998
[epoch13, step1321]: loss 0.571037
[epoch13, step1322]: loss 0.437815
[epoch13, step1323]: loss 0.362712
[epoch13, step1324]: loss 0.543919
[epoch13, step1325]: loss 0.421120
[epoch13, step1326]: loss 0.640591
[epoch13, step1327]: loss 0.485115
[epoch13, step1328]: loss 0.475061
[epoch13, step1329]: loss 0.543333
[epoch13, step1330]: loss 0.458821
[epoch13, step1331]: loss 0.223091
[epoch13, step1332]: loss 0.539119
[epoch13, step1333]: loss 0.274134
[epoch13, step1334]: loss 0.487385
[epoch13, step1335]: loss 0.565924
[epoch13, step1336]: loss 0.502327
[epoch13, step1337]: loss 0.658914
[epoch13, step1338]: loss 0.627895
[epoch13, step1339]: loss 0.351899
[epoch13, step1340]: loss 0.460604
[epoch13, step1341]: loss 0.540221
[epoch13, step1342]: loss 0.417507
[epoch13, step1343]: loss 0.387041
[epoch13, step1344]: loss 0.555555
[epoch13, step1345]: loss 0.558743
[epoch13, step1346]: loss 0.658844
[epoch13, step1347]: loss 0.707792
[epoch13, step1348]: loss 0.587139
[epoch13, step1349]: loss 0.433879
[epoch13, step1350]: loss 0.345254
[epoch13, step1351]: loss 0.601719
[epoch13, step1352]: loss 0.406274
[epoch13, step1353]: loss 0.439479
[epoch13, step1354]: loss 0.490009
[epoch13, step1355]: loss 0.224142
[epoch13, step1356]: loss 0.482913
[epoch13, step1357]: loss 0.418067
[epoch13, step1358]: loss 0.461779
[epoch13, step1359]: loss 0.352024
[epoch13, step1360]: loss 0.646128
[epoch13, step1361]: loss 0.665165
[epoch13, step1362]: loss 0.460584
[epoch13, step1363]: loss 0.434060
[epoch13, step1364]: loss 0.427442
[epoch13, step1365]: loss 0.419823
[epoch13, step1366]: loss 0.504708
[epoch13, step1367]: loss 0.505519
[epoch13, step1368]: loss 0.371855
[epoch13, step1369]: loss 0.403661
[epoch13, step1370]: loss 0.470666
[epoch13, step1371]: loss 0.416510
[epoch13, step1372]: loss 0.546640
[epoch13, step1373]: loss 0.666306
[epoch13, step1374]: loss 0.627110
[epoch13, step1375]: loss 0.616391
[epoch13, step1376]: loss 0.208861
[epoch13, step1377]: loss 0.518370
[epoch13, step1378]: loss 0.653228
[epoch13, step1379]: loss 0.327811
[epoch13, step1380]: loss 0.467153
[epoch13, step1381]: loss 0.491559
[epoch13, step1382]: loss 0.549695
[epoch13, step1383]: loss 0.485164
[epoch13, step1384]: loss 0.509422
[epoch13, step1385]: loss 0.578630
[epoch13, step1386]: loss 0.406608
[epoch13, step1387]: loss 0.344215
[epoch13, step1388]: loss 0.502388
[epoch13, step1389]: loss 0.528074
[epoch13, step1390]: loss 0.615635
[epoch13, step1391]: loss 0.479543
[epoch13, step1392]: loss 0.267141
[epoch13, step1393]: loss 0.756538
[epoch13, step1394]: loss 0.206082
[epoch13, step1395]: loss 0.460123
[epoch13, step1396]: loss 0.487105
[epoch13, step1397]: loss 0.438994
[epoch13, step1398]: loss 0.464973
[epoch13, step1399]: loss 0.487656
[epoch13, step1400]: loss 0.526408
[epoch13, step1401]: loss 0.390381
[epoch13, step1402]: loss 0.257328
[epoch13, step1403]: loss 0.333253
[epoch13, step1404]: loss 0.689067
[epoch13, step1405]: loss 0.474403
[epoch13, step1406]: loss 0.529612
[epoch13, step1407]: loss 0.473523
[epoch13, step1408]: loss 0.509704
[epoch13, step1409]: loss 0.514253
[epoch13, step1410]: loss 0.426336
[epoch13, step1411]: loss 0.269604
[epoch13, step1412]: loss 0.725564
[epoch13, step1413]: loss 0.501177
[epoch13, step1414]: loss 0.500275
[epoch13, step1415]: loss 0.508194
[epoch13, step1416]: loss 0.607576
[epoch13, step1417]: loss 0.531228
[epoch13, step1418]: loss 0.511498
[epoch13, step1419]: loss 0.663081
[epoch13, step1420]: loss 0.571899
[epoch13, step1421]: loss 0.502843
[epoch13, step1422]: loss 0.331819
[epoch13, step1423]: loss 0.748487
[epoch13, step1424]: loss 0.474226
[epoch13, step1425]: loss 0.554844
[epoch13, step1426]: loss 0.397934
[epoch13, step1427]: loss 0.507890
[epoch13, step1428]: loss 0.446648
[epoch13, step1429]: loss 0.447217
[epoch13, step1430]: loss 0.507689
[epoch13, step1431]: loss 0.578898
[epoch13, step1432]: loss 0.449423
[epoch13, step1433]: loss 0.630310
[epoch13, step1434]: loss 0.509190
[epoch13, step1435]: loss 0.324486
[epoch13, step1436]: loss 0.498656
[epoch13, step1437]: loss 0.489060
[epoch13, step1438]: loss 0.575716
[epoch13, step1439]: loss 0.420835
[epoch13, step1440]: loss 0.544680
[epoch13, step1441]: loss 0.372066
[epoch13, step1442]: loss 0.242211
[epoch13, step1443]: loss 0.606889
[epoch13, step1444]: loss 0.515027
[epoch13, step1445]: loss 0.553330
[epoch13, step1446]: loss 0.563079
[epoch13, step1447]: loss 0.687293
[epoch13, step1448]: loss 0.438690
[epoch13, step1449]: loss 0.477169
[epoch13, step1450]: loss 0.617726
[epoch13, step1451]: loss 0.217971
[epoch13, step1452]: loss 0.573482
[epoch13, step1453]: loss 0.696747
[epoch13, step1454]: loss 0.653595
[epoch13, step1455]: loss 0.512262
[epoch13, step1456]: loss 0.404823
[epoch13, step1457]: loss 0.586294
[epoch13, step1458]: loss 0.388475
[epoch13, step1459]: loss 0.493137
[epoch13, step1460]: loss 0.490820
[epoch13, step1461]: loss 0.606219
[epoch13, step1462]: loss 0.559471
[epoch13, step1463]: loss 0.400958
[epoch13, step1464]: loss 0.595047
[epoch13, step1465]: loss 0.619479
[epoch13, step1466]: loss 0.521843
[epoch13, step1467]: loss 0.586510
[epoch13, step1468]: loss 0.119622
[epoch13, step1469]: loss 0.606886
[epoch13, step1470]: loss 0.620842
[epoch13, step1471]: loss 0.579262
[epoch13, step1472]: loss 0.358727
[epoch13, step1473]: loss 0.244713
[epoch13, step1474]: loss 0.544315
[epoch13, step1475]: loss 0.448579
[epoch13, step1476]: loss 0.325522
[epoch13, step1477]: loss 0.659064
[epoch13, step1478]: loss 0.457602
[epoch13, step1479]: loss 0.521449
[epoch13, step1480]: loss 0.657600
[epoch13, step1481]: loss 0.607280
[epoch13, step1482]: loss 0.447588
[epoch13, step1483]: loss 0.496879
[epoch13, step1484]: loss 0.610026
[epoch13, step1485]: loss 0.348848
[epoch13, step1486]: loss 0.389813
[epoch13, step1487]: loss 0.609034
[epoch13, step1488]: loss 0.457282
[epoch13, step1489]: loss 0.265680
[epoch13, step1490]: loss 0.388629
[epoch13, step1491]: loss 0.405824
[epoch13, step1492]: loss 0.498251
[epoch13, step1493]: loss 0.581618
[epoch13, step1494]: loss 0.367661
[epoch13, step1495]: loss 0.518613
[epoch13, step1496]: loss 0.273041
[epoch13, step1497]: loss 0.474598
[epoch13, step1498]: loss 0.455418
[epoch13, step1499]: loss 0.392751
[epoch13, step1500]: loss 0.642246
[epoch13, step1501]: loss 0.633087
[epoch13, step1502]: loss 0.515986
[epoch13, step1503]: loss 0.518433
[epoch13, step1504]: loss 0.416250
[epoch13, step1505]: loss 0.714139
[epoch13, step1506]: loss 0.593972
[epoch13, step1507]: loss 0.482784
[epoch13, step1508]: loss 0.679371
[epoch13, step1509]: loss 0.604097
[epoch13, step1510]: loss 0.633697
[epoch13, step1511]: loss 0.602085
[epoch13, step1512]: loss 0.637412
[epoch13, step1513]: loss 0.540119
[epoch13, step1514]: loss 0.473789
[epoch13, step1515]: loss 0.655188
[epoch13, step1516]: loss 0.514605
[epoch13, step1517]: loss 0.399510
[epoch13, step1518]: loss 0.466098
[epoch13, step1519]: loss 0.450956
[epoch13, step1520]: loss 0.442056
[epoch13, step1521]: loss 0.323907
[epoch13, step1522]: loss 0.719346
[epoch13, step1523]: loss 0.450775
[epoch13, step1524]: loss 0.563949
[epoch13, step1525]: loss 0.668523
[epoch13, step1526]: loss 0.376480
[epoch13, step1527]: loss 0.705494
[epoch13, step1528]: loss 0.495890
[epoch13, step1529]: loss 0.444921
[epoch13, step1530]: loss 0.449975
[epoch13, step1531]: loss 0.412328
[epoch13, step1532]: loss 0.601632
[epoch13, step1533]: loss 0.416938
[epoch13, step1534]: loss 0.251146
[epoch13, step1535]: loss 0.485969
[epoch13, step1536]: loss 0.260690
[epoch13, step1537]: loss 0.371794
[epoch13, step1538]: loss 0.435533
[epoch13, step1539]: loss 0.702973
[epoch13, step1540]: loss 0.565043
[epoch13, step1541]: loss 0.470861
[epoch13, step1542]: loss 0.333979
[epoch13, step1543]: loss 0.657582
[epoch13, step1544]: loss 0.502391
[epoch13, step1545]: loss 0.143779
[epoch13, step1546]: loss 0.812585
[epoch13, step1547]: loss 0.390572
[epoch13, step1548]: loss 0.426990
[epoch13, step1549]: loss 0.394271
[epoch13, step1550]: loss 0.575726
[epoch13, step1551]: loss 0.155552
[epoch13, step1552]: loss 0.715757
[epoch13, step1553]: loss 0.531959
[epoch13, step1554]: loss 0.385938
[epoch13, step1555]: loss 0.280852
[epoch13, step1556]: loss 0.630131
[epoch13, step1557]: loss 0.506294
[epoch13, step1558]: loss 0.667231
[epoch13, step1559]: loss 0.435176
[epoch13, step1560]: loss 0.622684
[epoch13, step1561]: loss 0.251657
[epoch13, step1562]: loss 0.446589
[epoch13, step1563]: loss 0.351017
[epoch13, step1564]: loss 0.439481
[epoch13, step1565]: loss 0.311690
[epoch13, step1566]: loss 0.487104
[epoch13, step1567]: loss 0.502607
[epoch13, step1568]: loss 0.779349
[epoch13, step1569]: loss 0.522897
[epoch13, step1570]: loss 0.494507
[epoch13, step1571]: loss 0.552364
[epoch13, step1572]: loss 0.277073
[epoch13, step1573]: loss 0.436455
[epoch13, step1574]: loss 0.441911
[epoch13, step1575]: loss 0.744571
[epoch13, step1576]: loss 0.606898
[epoch13, step1577]: loss 0.533650
[epoch13, step1578]: loss 0.435895
[epoch13, step1579]: loss 0.257453
[epoch13, step1580]: loss 0.234747
[epoch13, step1581]: loss 0.393856
[epoch13, step1582]: loss 0.442769
[epoch13, step1583]: loss 0.563622
[epoch13, step1584]: loss 0.435296
[epoch13, step1585]: loss 0.374036
[epoch13, step1586]: loss 0.525867
[epoch13, step1587]: loss 0.364458
[epoch13, step1588]: loss 0.551336
[epoch13, step1589]: loss 0.646228
[epoch13, step1590]: loss 0.648217
[epoch13, step1591]: loss 0.608289
[epoch13, step1592]: loss 0.509807
[epoch13, step1593]: loss 0.558929
[epoch13, step1594]: loss 0.348597
[epoch13, step1595]: loss 0.391085
[epoch13, step1596]: loss 0.400535
[epoch13, step1597]: loss 0.654949
[epoch13, step1598]: loss 0.438363
[epoch13, step1599]: loss 0.436672
[epoch13, step1600]: loss 0.554917
[epoch13, step1601]: loss 0.331362
[epoch13, step1602]: loss 0.478099
[epoch13, step1603]: loss 0.621240
[epoch13, step1604]: loss 0.560515
[epoch13, step1605]: loss 0.511570
[epoch13, step1606]: loss 0.393021
[epoch13, step1607]: loss 0.677393
[epoch13, step1608]: loss 0.521886
[epoch13, step1609]: loss 0.552657
[epoch13, step1610]: loss 0.524440
[epoch13, step1611]: loss 0.270560
[epoch13, step1612]: loss 0.451214
[epoch13, step1613]: loss 0.673927
[epoch13, step1614]: loss 0.282847
[epoch13, step1615]: loss 0.626503
[epoch13, step1616]: loss 0.719449
[epoch13, step1617]: loss 0.571453
[epoch13, step1618]: loss 0.496858
[epoch13, step1619]: loss 0.420216
[epoch13, step1620]: loss 0.278310
[epoch13, step1621]: loss 0.427099
[epoch13, step1622]: loss 0.316952
[epoch13, step1623]: loss 0.373959
[epoch13, step1624]: loss 0.334615
[epoch13, step1625]: loss 0.341890
[epoch13, step1626]: loss 0.486873
[epoch13, step1627]: loss 0.454048
[epoch13, step1628]: loss 0.470671
[epoch13, step1629]: loss 0.276700
[epoch13, step1630]: loss 0.598541
[epoch13, step1631]: loss 0.280605
[epoch13, step1632]: loss 0.401925
[epoch13, step1633]: loss 0.530278
[epoch13, step1634]: loss 0.408091
[epoch13, step1635]: loss 0.501924
[epoch13, step1636]: loss 0.478813
[epoch13, step1637]: loss 0.484683
[epoch13, step1638]: loss 0.489524
[epoch13, step1639]: loss 0.698165
[epoch13, step1640]: loss 0.556943
[epoch13, step1641]: loss 0.380320
[epoch13, step1642]: loss 0.507102
[epoch13, step1643]: loss 0.406913
[epoch13, step1644]: loss 0.549874
[epoch13, step1645]: loss 0.545982
[epoch13, step1646]: loss 0.431821
[epoch13, step1647]: loss 0.421842
[epoch13, step1648]: loss 0.390573
[epoch13, step1649]: loss 0.340720
[epoch13, step1650]: loss 0.283844
[epoch13, step1651]: loss 0.751768
[epoch13, step1652]: loss 0.242148
[epoch13, step1653]: loss 0.173576
[epoch13, step1654]: loss 0.587947
[epoch13, step1655]: loss 0.503046
[epoch13, step1656]: loss 0.506210
[epoch13, step1657]: loss 0.320630
[epoch13, step1658]: loss 0.349963
[epoch13, step1659]: loss 0.263420
[epoch13, step1660]: loss 0.513962
[epoch13, step1661]: loss 0.463226
[epoch13, step1662]: loss 0.360633
[epoch13, step1663]: loss 0.437806
[epoch13, step1664]: loss 0.417327
[epoch13, step1665]: loss 0.513863
[epoch13, step1666]: loss 0.572790
[epoch13, step1667]: loss 0.784748
[epoch13, step1668]: loss 0.591879
[epoch13, step1669]: loss 0.498694
[epoch13, step1670]: loss 0.345148
[epoch13, step1671]: loss 0.392116
[epoch13, step1672]: loss 0.492448
[epoch13, step1673]: loss 0.639111
[epoch13, step1674]: loss 0.627769
[epoch13, step1675]: loss 0.470613
[epoch13, step1676]: loss 0.436467
[epoch13, step1677]: loss 0.587123
[epoch13, step1678]: loss 0.439644
[epoch13, step1679]: loss 0.570775
[epoch13, step1680]: loss 0.516852
[epoch13, step1681]: loss 0.397930
[epoch13, step1682]: loss 0.566711
[epoch13, step1683]: loss 0.271032
[epoch13, step1684]: loss 0.291020
[epoch13, step1685]: loss 0.517976
[epoch13, step1686]: loss 0.398764
[epoch13, step1687]: loss 0.507452
[epoch13, step1688]: loss 0.322206
[epoch13, step1689]: loss 0.276218
[epoch13, step1690]: loss 0.772432
[epoch13, step1691]: loss 0.337201
[epoch13, step1692]: loss 0.301888
[epoch13, step1693]: loss 0.269621
[epoch13, step1694]: loss 0.478437
[epoch13, step1695]: loss 0.440262
[epoch13, step1696]: loss 0.688627
[epoch13, step1697]: loss 0.535928
[epoch13, step1698]: loss 0.431260
[epoch13, step1699]: loss 0.489788
[epoch13, step1700]: loss 0.601299
[epoch13, step1701]: loss 0.526983
[epoch13, step1702]: loss 0.356465
[epoch13, step1703]: loss 0.381145
[epoch13, step1704]: loss 0.509586
[epoch13, step1705]: loss 0.434581
[epoch13, step1706]: loss 0.443205
[epoch13, step1707]: loss 0.596455
[epoch13, step1708]: loss 0.556288
[epoch13, step1709]: loss 0.378219
[epoch13, step1710]: loss 0.434291
[epoch13, step1711]: loss 0.398568
[epoch13, step1712]: loss 0.666638
[epoch13, step1713]: loss 0.424160
[epoch13, step1714]: loss 0.505753
[epoch13, step1715]: loss 0.346793
[epoch13, step1716]: loss 0.125271
[epoch13, step1717]: loss 0.537482
[epoch13, step1718]: loss 0.690221
[epoch13, step1719]: loss 0.625517
[epoch13, step1720]: loss 0.337265
[epoch13, step1721]: loss 0.442053
[epoch13, step1722]: loss 0.334899
[epoch13, step1723]: loss 0.605725
[epoch13, step1724]: loss 0.564669
[epoch13, step1725]: loss 0.147520
[epoch13, step1726]: loss 0.633583
[epoch13, step1727]: loss 0.432705
[epoch13, step1728]: loss 0.524493
[epoch13, step1729]: loss 0.579077
[epoch13, step1730]: loss 0.627032
[epoch13, step1731]: loss 0.174792
[epoch13, step1732]: loss 0.707655
[epoch13, step1733]: loss 0.806502
[epoch13, step1734]: loss 0.468019
[epoch13, step1735]: loss 0.441731
[epoch13, step1736]: loss 0.536154
[epoch13, step1737]: loss 0.413027
[epoch13, step1738]: loss 0.594299
[epoch13, step1739]: loss 0.564148
[epoch13, step1740]: loss 0.513378
[epoch13, step1741]: loss 0.438873
[epoch13, step1742]: loss 0.484251
[epoch13, step1743]: loss 0.558184
[epoch13, step1744]: loss 0.578761
[epoch13, step1745]: loss 0.534254
[epoch13, step1746]: loss 0.457004
[epoch13, step1747]: loss 0.414601
[epoch13, step1748]: loss 0.477284
[epoch13, step1749]: loss 0.492569
[epoch13, step1750]: loss 0.411050
[epoch13, step1751]: loss 0.637798
[epoch13, step1752]: loss 0.613590
[epoch13, step1753]: loss 0.616885
[epoch13, step1754]: loss 0.254070
[epoch13, step1755]: loss 0.575030
[epoch13, step1756]: loss 0.289119
[epoch13, step1757]: loss 0.661364
[epoch13, step1758]: loss 0.619244
[epoch13, step1759]: loss 0.451164
[epoch13, step1760]: loss 0.361809
[epoch13, step1761]: loss 0.556091
[epoch13, step1762]: loss 0.523894
[epoch13, step1763]: loss 0.501621
[epoch13, step1764]: loss 0.524967
[epoch13, step1765]: loss 0.416411
[epoch13, step1766]: loss 0.456325
[epoch13, step1767]: loss 0.593120
[epoch13, step1768]: loss 0.541326
[epoch13, step1769]: loss 0.608850
[epoch13, step1770]: loss 0.443491
[epoch13, step1771]: loss 0.527408
[epoch13, step1772]: loss 0.357978
[epoch13, step1773]: loss 0.570826
[epoch13, step1774]: loss 0.668857
[epoch13, step1775]: loss 0.453749
[epoch13, step1776]: loss 0.304138
[epoch13, step1777]: loss 0.412827
[epoch13, step1778]: loss 0.450238
[epoch13, step1779]: loss 0.567189
[epoch13, step1780]: loss 0.498021
[epoch13, step1781]: loss 0.620614
[epoch13, step1782]: loss 0.613784
[epoch13, step1783]: loss 0.568748
[epoch13, step1784]: loss 0.417786
[epoch13, step1785]: loss 0.431283
[epoch13, step1786]: loss 0.541388
[epoch13, step1787]: loss 0.621032
[epoch13, step1788]: loss 0.571148
[epoch13, step1789]: loss 0.396517
[epoch13, step1790]: loss 0.429571
[epoch13, step1791]: loss 0.579860
[epoch13, step1792]: loss 0.203254
[epoch13, step1793]: loss 0.238410
[epoch13, step1794]: loss 0.515912
[epoch13, step1795]: loss 0.363103
[epoch13, step1796]: loss 0.601543
[epoch13, step1797]: loss 0.537243
[epoch13, step1798]: loss 0.322793
[epoch13, step1799]: loss 0.341767
[epoch13, step1800]: loss 0.376138
[epoch13, step1801]: loss 0.412703
[epoch13, step1802]: loss 0.546702
[epoch13, step1803]: loss 0.409322
[epoch13, step1804]: loss 0.478671
[epoch13, step1805]: loss 0.508709
[epoch13, step1806]: loss 0.400095
[epoch13, step1807]: loss 0.693750
[epoch13, step1808]: loss 0.491652
[epoch13, step1809]: loss 0.449708
[epoch13, step1810]: loss 0.586344
[epoch13, step1811]: loss 0.368184
[epoch13, step1812]: loss 0.348032
[epoch13, step1813]: loss 0.618068
[epoch13, step1814]: loss 0.586405
[epoch13, step1815]: loss 0.534017
[epoch13, step1816]: loss 0.540159
[epoch13, step1817]: loss 0.517264
[epoch13, step1818]: loss 0.534750
[epoch13, step1819]: loss 0.439006
[epoch13, step1820]: loss 0.360090
[epoch13, step1821]: loss 0.366166
[epoch13, step1822]: loss 0.488294
[epoch13, step1823]: loss 0.437418
[epoch13, step1824]: loss 0.410701
[epoch13, step1825]: loss 0.286841
[epoch13, step1826]: loss 0.646910
[epoch13, step1827]: loss 0.681645
[epoch13, step1828]: loss 0.533216
[epoch13, step1829]: loss 0.640791
[epoch13, step1830]: loss 0.451706
[epoch13, step1831]: loss 0.580910
[epoch13, step1832]: loss 0.544303
[epoch13, step1833]: loss 0.431866
[epoch13, step1834]: loss 0.612337
[epoch13, step1835]: loss 0.555423
[epoch13, step1836]: loss 0.421524
[epoch13, step1837]: loss 0.477441
[epoch13, step1838]: loss 0.469819
[epoch13, step1839]: loss 0.342753
[epoch13, step1840]: loss 0.522178
[epoch13, step1841]: loss 0.375165
[epoch13, step1842]: loss 0.546860
[epoch13, step1843]: loss 0.495337
[epoch13, step1844]: loss 0.597167
[epoch13, step1845]: loss 0.558946
[epoch13, step1846]: loss 0.628697
[epoch13, step1847]: loss 0.305043
[epoch13, step1848]: loss 0.248166
[epoch13, step1849]: loss 0.651215
[epoch13, step1850]: loss 0.392214
[epoch13, step1851]: loss 0.432976
[epoch13, step1852]: loss 0.442110
[epoch13, step1853]: loss 0.585353
[epoch13, step1854]: loss 0.313841
[epoch13, step1855]: loss 0.307824
[epoch13, step1856]: loss 0.540375
[epoch13, step1857]: loss 0.555191
[epoch13, step1858]: loss 0.428334
[epoch13, step1859]: loss 0.547628
[epoch13, step1860]: loss 0.442597
[epoch13, step1861]: loss 0.533062
[epoch13, step1862]: loss 0.408834
[epoch13, step1863]: loss 0.608841
[epoch13, step1864]: loss 0.552345
[epoch13, step1865]: loss 0.569554
[epoch13, step1866]: loss 0.442503
[epoch13, step1867]: loss 0.299008
[epoch13, step1868]: loss 0.425532
[epoch13, step1869]: loss 0.619558
[epoch13, step1870]: loss 0.443257
[epoch13, step1871]: loss 0.371426
[epoch13, step1872]: loss 0.542506
[epoch13, step1873]: loss 0.402328
[epoch13, step1874]: loss 0.625648
[epoch13, step1875]: loss 0.446538
[epoch13, step1876]: loss 0.488762
[epoch13, step1877]: loss 0.583720
[epoch13, step1878]: loss 0.510149
[epoch13, step1879]: loss 0.557786
[epoch13, step1880]: loss 0.484927
[epoch13, step1881]: loss 0.292418
[epoch13, step1882]: loss 0.650237
[epoch13, step1883]: loss 0.378002
[epoch13, step1884]: loss 0.454755
[epoch13, step1885]: loss 0.380032
[epoch13, step1886]: loss 0.591947
[epoch13, step1887]: loss 0.296213
[epoch13, step1888]: loss 0.473808
[epoch13, step1889]: loss 0.526828
[epoch13, step1890]: loss 0.526935
[epoch13, step1891]: loss 0.563836
[epoch13, step1892]: loss 0.400832
[epoch13, step1893]: loss 0.522408
[epoch13, step1894]: loss 0.609746
[epoch13, step1895]: loss 0.530805
[epoch13, step1896]: loss 0.381442
[epoch13, step1897]: loss 0.448032
[epoch13, step1898]: loss 0.274781
[epoch13, step1899]: loss 0.279455
[epoch13, step1900]: loss 0.511348
[epoch13, step1901]: loss 0.457432
[epoch13, step1902]: loss 0.616043
[epoch13, step1903]: loss 0.651058
[epoch13, step1904]: loss 0.402723
[epoch13, step1905]: loss 0.355310
[epoch13, step1906]: loss 0.510369
[epoch13, step1907]: loss 0.266378
[epoch13, step1908]: loss 0.604911
[epoch13, step1909]: loss 0.236061
[epoch13, step1910]: loss 0.407565
[epoch13, step1911]: loss 0.628805
[epoch13, step1912]: loss 0.505558
[epoch13, step1913]: loss 0.532572
[epoch13, step1914]: loss 0.515358
[epoch13, step1915]: loss 0.712085
[epoch13, step1916]: loss 0.325641
[epoch13, step1917]: loss 0.285729
[epoch13, step1918]: loss 0.588494
[epoch13, step1919]: loss 0.413443
[epoch13, step1920]: loss 0.444464
[epoch13, step1921]: loss 0.600773
[epoch13, step1922]: loss 0.314568
[epoch13, step1923]: loss 0.530830
[epoch13, step1924]: loss 0.513217
[epoch13, step1925]: loss 0.539658
[epoch13, step1926]: loss 0.398038
[epoch13, step1927]: loss 0.474990
[epoch13, step1928]: loss 0.538591
[epoch13, step1929]: loss 0.535041
[epoch13, step1930]: loss 0.596005
[epoch13, step1931]: loss 0.548717
[epoch13, step1932]: loss 0.362915
[epoch13, step1933]: loss 0.621696
[epoch13, step1934]: loss 0.385994
[epoch13, step1935]: loss 0.683377
[epoch13, step1936]: loss 0.277717
[epoch13, step1937]: loss 0.358302
[epoch13, step1938]: loss 0.391695
[epoch13, step1939]: loss 0.705440
[epoch13, step1940]: loss 0.355220
[epoch13, step1941]: loss 0.610870
[epoch13, step1942]: loss 0.544452
[epoch13, step1943]: loss 0.452364
[epoch13, step1944]: loss 0.615836
[epoch13, step1945]: loss 0.637563
[epoch13, step1946]: loss 0.589278
[epoch13, step1947]: loss 0.422962
[epoch13, step1948]: loss 0.493326
[epoch13, step1949]: loss 0.402673
[epoch13, step1950]: loss 0.515024
[epoch13, step1951]: loss 0.572088
[epoch13, step1952]: loss 0.595649
[epoch13, step1953]: loss 0.538786
[epoch13, step1954]: loss 0.473168
[epoch13, step1955]: loss 0.524605
[epoch13, step1956]: loss 0.685790
[epoch13, step1957]: loss 0.343976
[epoch13, step1958]: loss 0.692649
[epoch13, step1959]: loss 0.509306
[epoch13, step1960]: loss 0.283486
[epoch13, step1961]: loss 0.437479
[epoch13, step1962]: loss 0.750822
[epoch13, step1963]: loss 0.537209
[epoch13, step1964]: loss 0.284772
[epoch13, step1965]: loss 0.528989
[epoch13, step1966]: loss 0.539323
[epoch13, step1967]: loss 0.400123
[epoch13, step1968]: loss 0.503801
[epoch13, step1969]: loss 0.612708
[epoch13, step1970]: loss 0.403577
[epoch13, step1971]: loss 0.416321
[epoch13, step1972]: loss 0.639522
[epoch13, step1973]: loss 0.505348
[epoch13, step1974]: loss 0.624685
[epoch13, step1975]: loss 0.603358
[epoch13, step1976]: loss 0.517329
[epoch13, step1977]: loss 0.391859
[epoch13, step1978]: loss 0.556354
[epoch13, step1979]: loss 0.368448
[epoch13, step1980]: loss 0.582362
[epoch13, step1981]: loss 0.657621
[epoch13, step1982]: loss 0.544999
[epoch13, step1983]: loss 0.554813
[epoch13, step1984]: loss 0.474545
[epoch13, step1985]: loss 0.497236
[epoch13, step1986]: loss 0.500797
[epoch13, step1987]: loss 0.346859
[epoch13, step1988]: loss 0.266630
[epoch13, step1989]: loss 0.667947
[epoch13, step1990]: loss 0.681334
[epoch13, step1991]: loss 0.395378
[epoch13, step1992]: loss 0.402843
[epoch13, step1993]: loss 0.416868
[epoch13, step1994]: loss 0.656249
[epoch13, step1995]: loss 0.458508
[epoch13, step1996]: loss 0.542504
[epoch13, step1997]: loss 0.332710
[epoch13, step1998]: loss 0.540396
[epoch13, step1999]: loss 0.432900
[epoch13, step2000]: loss 0.431088
[epoch13, step2001]: loss 0.536458
[epoch13, step2002]: loss 0.619236
[epoch13, step2003]: loss 0.521927
[epoch13, step2004]: loss 0.503348
[epoch13, step2005]: loss 0.435107
[epoch13, step2006]: loss 0.546578
[epoch13, step2007]: loss 0.538686
[epoch13, step2008]: loss 0.365191
[epoch13, step2009]: loss 0.518718
[epoch13, step2010]: loss 0.571563
[epoch13, step2011]: loss 0.446506
[epoch13, step2012]: loss 0.192395
[epoch13, step2013]: loss 0.493895
[epoch13, step2014]: loss 0.891452
[epoch13, step2015]: loss 0.281534
[epoch13, step2016]: loss 0.520353
[epoch13, step2017]: loss 0.392179
[epoch13, step2018]: loss 0.374569
[epoch13, step2019]: loss 0.537831
[epoch13, step2020]: loss 0.694254
[epoch13, step2021]: loss 0.454632
[epoch13, step2022]: loss 0.617489
[epoch13, step2023]: loss 0.413162
[epoch13, step2024]: loss 0.444796
[epoch13, step2025]: loss 0.269067
[epoch13, step2026]: loss 0.516412
[epoch13, step2027]: loss 0.518043
[epoch13, step2028]: loss 0.576455
[epoch13, step2029]: loss 0.384798
[epoch13, step2030]: loss 0.862736
[epoch13, step2031]: loss 0.376027
[epoch13, step2032]: loss 0.196361
[epoch13, step2033]: loss 0.415994
[epoch13, step2034]: loss 0.536153
[epoch13, step2035]: loss 0.370515
[epoch13, step2036]: loss 0.623338
[epoch13, step2037]: loss 0.570336
[epoch13, step2038]: loss 0.460000
[epoch13, step2039]: loss 0.520492
[epoch13, step2040]: loss 0.419231
[epoch13, step2041]: loss 0.488235
[epoch13, step2042]: loss 0.377037
[epoch13, step2043]: loss 0.660041
[epoch13, step2044]: loss 0.672236
[epoch13, step2045]: loss 0.325525
[epoch13, step2046]: loss 0.407378
[epoch13, step2047]: loss 0.753456
[epoch13, step2048]: loss 0.655936
[epoch13, step2049]: loss 0.564497
[epoch13, step2050]: loss 0.280439
[epoch13, step2051]: loss 0.613115
[epoch13, step2052]: loss 0.545629
[epoch13, step2053]: loss 0.688840
[epoch13, step2054]: loss 0.592802
[epoch13, step2055]: loss 0.559490
[epoch13, step2056]: loss 0.605783
[epoch13, step2057]: loss 0.569920
[epoch13, step2058]: loss 0.513384
[epoch13, step2059]: loss 0.375711
[epoch13, step2060]: loss 0.493899
[epoch13, step2061]: loss 0.552621
[epoch13, step2062]: loss 0.396078
[epoch13, step2063]: loss 0.756399
[epoch13, step2064]: loss 0.378279
[epoch13, step2065]: loss 0.561618
[epoch13, step2066]: loss 0.254471
[epoch13, step2067]: loss 0.475660
[epoch13, step2068]: loss 0.452006
[epoch13, step2069]: loss 0.615665
[epoch13, step2070]: loss 0.621993
[epoch13, step2071]: loss 0.676476
[epoch13, step2072]: loss 0.476259
[epoch13, step2073]: loss 0.520781
[epoch13, step2074]: loss 0.448135
[epoch13, step2075]: loss 0.126142
[epoch13, step2076]: loss 0.327881
[epoch13, step2077]: loss 0.501715
[epoch13, step2078]: loss 0.592518
[epoch13, step2079]: loss 0.492399
[epoch13, step2080]: loss 0.534476
[epoch13, step2081]: loss 0.278702
[epoch13, step2082]: loss 0.363120
[epoch13, step2083]: loss 0.322610
[epoch13, step2084]: loss 0.534737
[epoch13, step2085]: loss 0.330373
[epoch13, step2086]: loss 0.446158
[epoch13, step2087]: loss 0.455665
[epoch13, step2088]: loss 0.638619
[epoch13, step2089]: loss 0.465624
[epoch13, step2090]: loss 0.509223
[epoch13, step2091]: loss 0.532433
[epoch13, step2092]: loss 0.458697
[epoch13, step2093]: loss 0.637192
[epoch13, step2094]: loss 0.264499
[epoch13, step2095]: loss 0.373464
[epoch13, step2096]: loss 0.667581
[epoch13, step2097]: loss 0.439094
[epoch13, step2098]: loss 0.513575
[epoch13, step2099]: loss 0.588849
[epoch13, step2100]: loss 0.760574
[epoch13, step2101]: loss 0.604879
[epoch13, step2102]: loss 0.615484
[epoch13, step2103]: loss 0.554475
[epoch13, step2104]: loss 0.400947
[epoch13, step2105]: loss 0.452311
[epoch13, step2106]: loss 0.462915
[epoch13, step2107]: loss 0.515791
[epoch13, step2108]: loss 0.398789
[epoch13, step2109]: loss 0.432123
[epoch13, step2110]: loss 0.391440
[epoch13, step2111]: loss 0.622851
[epoch13, step2112]: loss 0.293968
[epoch13, step2113]: loss 0.423218
[epoch13, step2114]: loss 0.296117
[epoch13, step2115]: loss 0.558732
[epoch13, step2116]: loss 0.259828
[epoch13, step2117]: loss 0.260449
[epoch13, step2118]: loss 0.536431
[epoch13, step2119]: loss 0.514138
[epoch13, step2120]: loss 0.290655
[epoch13, step2121]: loss 0.444023
[epoch13, step2122]: loss 0.513821
[epoch13, step2123]: loss 0.576131
[epoch13, step2124]: loss 0.195121
[epoch13, step2125]: loss 0.464914
[epoch13, step2126]: loss 0.609732
[epoch13, step2127]: loss 0.525764
[epoch13, step2128]: loss 0.303286
[epoch13, step2129]: loss 0.506589
[epoch13, step2130]: loss 0.360314
[epoch13, step2131]: loss 0.652058
[epoch13, step2132]: loss 0.552621
[epoch13, step2133]: loss 0.603147
[epoch13, step2134]: loss 0.655519
[epoch13, step2135]: loss 0.498017
[epoch13, step2136]: loss 0.567379
[epoch13, step2137]: loss 0.375814
[epoch13, step2138]: loss 0.504823
[epoch13, step2139]: loss 0.567491
[epoch13, step2140]: loss 0.435054
[epoch13, step2141]: loss 0.433392
[epoch13, step2142]: loss 0.618565
[epoch13, step2143]: loss 0.694916
[epoch13, step2144]: loss 0.527496
[epoch13, step2145]: loss 0.732601
[epoch13, step2146]: loss 0.669144
[epoch13, step2147]: loss 0.380014
[epoch13, step2148]: loss 0.631318
[epoch13, step2149]: loss 0.406604
[epoch13, step2150]: loss 0.547860
[epoch13, step2151]: loss 0.496828
[epoch13, step2152]: loss 0.188679
[epoch13, step2153]: loss 0.594092
[epoch13, step2154]: loss 0.452145
[epoch13, step2155]: loss 0.526031
[epoch13, step2156]: loss 0.519799
[epoch13, step2157]: loss 0.606098
[epoch13, step2158]: loss 0.571787
[epoch13, step2159]: loss 0.497144
[epoch13, step2160]: loss 0.485858
[epoch13, step2161]: loss 0.492672
[epoch13, step2162]: loss 0.608790
[epoch13, step2163]: loss 0.440029
[epoch13, step2164]: loss 0.532130
[epoch13, step2165]: loss 0.402758
[epoch13, step2166]: loss 0.573364
[epoch13, step2167]: loss 0.762500
[epoch13, step2168]: loss 0.790485
[epoch13, step2169]: loss 0.342388
[epoch13, step2170]: loss 0.456406
[epoch13, step2171]: loss 0.431474
[epoch13, step2172]: loss 0.282064
[epoch13, step2173]: loss 0.450194
[epoch13, step2174]: loss 0.544467
[epoch13, step2175]: loss 0.661219
[epoch13, step2176]: loss 0.387958
[epoch13, step2177]: loss 0.371292
[epoch13, step2178]: loss 0.438198
[epoch13, step2179]: loss 0.572335
[epoch13, step2180]: loss 0.715699
[epoch13, step2181]: loss 0.706012
[epoch13, step2182]: loss 0.546127
[epoch13, step2183]: loss 0.560849
[epoch13, step2184]: loss 0.541134
[epoch13, step2185]: loss 0.555749
[epoch13, step2186]: loss 0.618317
[epoch13, step2187]: loss 0.585589
[epoch13, step2188]: loss 0.618640
[epoch13, step2189]: loss 0.628274
[epoch13, step2190]: loss 0.441494
[epoch13, step2191]: loss 0.362306
[epoch13, step2192]: loss 0.385563
[epoch13, step2193]: loss 0.422309
[epoch13, step2194]: loss 0.603234
[epoch13, step2195]: loss 0.617192
[epoch13, step2196]: loss 0.518353
[epoch13, step2197]: loss 0.460643
[epoch13, step2198]: loss 0.438156
[epoch13, step2199]: loss 0.485021
[epoch13, step2200]: loss 0.572936
[epoch13, step2201]: loss 0.501245
[epoch13, step2202]: loss 0.333139
[epoch13, step2203]: loss 0.306105
[epoch13, step2204]: loss 0.553826
[epoch13, step2205]: loss 0.670685
[epoch13, step2206]: loss 0.155563
[epoch13, step2207]: loss 0.407804
[epoch13, step2208]: loss 0.441856
[epoch13, step2209]: loss 0.454233
[epoch13, step2210]: loss 0.592332
[epoch13, step2211]: loss 0.575048
[epoch13, step2212]: loss 0.355352
[epoch13, step2213]: loss 0.316488
[epoch13, step2214]: loss 0.594232
[epoch13, step2215]: loss 0.582548
[epoch13, step2216]: loss 0.676845
[epoch13, step2217]: loss 0.543965
[epoch13, step2218]: loss 0.384054
[epoch13, step2219]: loss 0.410256
[epoch13, step2220]: loss 0.357709
[epoch13, step2221]: loss 0.325658
[epoch13, step2222]: loss 0.367902
[epoch13, step2223]: loss 0.450662
[epoch13, step2224]: loss 0.348790
[epoch13, step2225]: loss 0.602312
[epoch13, step2226]: loss 0.480023
[epoch13, step2227]: loss 0.477768
[epoch13, step2228]: loss 0.616309
[epoch13, step2229]: loss 0.372455
[epoch13, step2230]: loss 0.389072
[epoch13, step2231]: loss 0.579110
[epoch13, step2232]: loss 0.309892
[epoch13, step2233]: loss 0.620729
[epoch13, step2234]: loss 0.549359
[epoch13, step2235]: loss 0.342546
[epoch13, step2236]: loss 0.578193
[epoch13, step2237]: loss 0.494775
[epoch13, step2238]: loss 0.476442
[epoch13, step2239]: loss 0.364241
[epoch13, step2240]: loss 0.485796
[epoch13, step2241]: loss 0.450331
[epoch13, step2242]: loss 0.598015
[epoch13, step2243]: loss 0.264726
[epoch13, step2244]: loss 0.294155
[epoch13, step2245]: loss 0.271105
[epoch13, step2246]: loss 0.480007
[epoch13, step2247]: loss 0.265285
[epoch13, step2248]: loss 0.433107
[epoch13, step2249]: loss 0.514208
[epoch13, step2250]: loss 0.343768
[epoch13, step2251]: loss 0.512069
[epoch13, step2252]: loss 0.702196
[epoch13, step2253]: loss 0.263605
[epoch13, step2254]: loss 0.311644
[epoch13, step2255]: loss 0.440987
[epoch13, step2256]: loss 0.413124
[epoch13, step2257]: loss 0.642523
[epoch13, step2258]: loss 0.367660
[epoch13, step2259]: loss 0.508734
[epoch13, step2260]: loss 0.345184
[epoch13, step2261]: loss 0.796003
[epoch13, step2262]: loss 0.106496
[epoch13, step2263]: loss 0.361981
[epoch13, step2264]: loss 0.487968
[epoch13, step2265]: loss 0.571661
[epoch13, step2266]: loss 0.679342
[epoch13, step2267]: loss 0.388479
[epoch13, step2268]: loss 0.513332
[epoch13, step2269]: loss 0.511963
[epoch13, step2270]: loss 0.565409
[epoch13, step2271]: loss 0.549321
[epoch13, step2272]: loss 0.545363
[epoch13, step2273]: loss 0.496795
[epoch13, step2274]: loss 0.547813
[epoch13, step2275]: loss 0.645348
[epoch13, step2276]: loss 0.368058
[epoch13, step2277]: loss 0.304568
[epoch13, step2278]: loss 0.470935
[epoch13, step2279]: loss 0.452960
[epoch13, step2280]: loss 0.714114
[epoch13, step2281]: loss 0.692386
[epoch13, step2282]: loss 0.584296
[epoch13, step2283]: loss 0.527793
[epoch13, step2284]: loss 0.562636
[epoch13, step2285]: loss 0.447270
[epoch13, step2286]: loss 0.810623
[epoch13, step2287]: loss 0.456975
[epoch13, step2288]: loss 0.677969
[epoch13, step2289]: loss 0.325655
[epoch13, step2290]: loss 0.440239
[epoch13, step2291]: loss 0.585737
[epoch13, step2292]: loss 0.503650
[epoch13, step2293]: loss 0.628395
[epoch13, step2294]: loss 0.437029
[epoch13, step2295]: loss 0.586859
[epoch13, step2296]: loss 0.594276
[epoch13, step2297]: loss 0.372394
[epoch13, step2298]: loss 0.460432
[epoch13, step2299]: loss 0.533055
[epoch13, step2300]: loss 0.545728
[epoch13, step2301]: loss 0.319293
[epoch13, step2302]: loss 0.423255
[epoch13, step2303]: loss 0.392276
[epoch13, step2304]: loss 0.770621
[epoch13, step2305]: loss 0.548565
[epoch13, step2306]: loss 0.554586
[epoch13, step2307]: loss 0.587204
[epoch13, step2308]: loss 0.626089
[epoch13, step2309]: loss 0.510153
[epoch13, step2310]: loss 0.481723
[epoch13, step2311]: loss 0.303952
[epoch13, step2312]: loss 0.523674
[epoch13, step2313]: loss 0.572322
[epoch13, step2314]: loss 0.633000
[epoch13, step2315]: loss 0.239878
[epoch13, step2316]: loss 0.650913
[epoch13, step2317]: loss 0.505263
[epoch13, step2318]: loss 0.427648
[epoch13, step2319]: loss 0.273408
[epoch13, step2320]: loss 0.605926
[epoch13, step2321]: loss 0.163826
[epoch13, step2322]: loss 0.536241
[epoch13, step2323]: loss 0.389686
[epoch13, step2324]: loss 0.498191
[epoch13, step2325]: loss 0.541027
[epoch13, step2326]: loss 0.701902
[epoch13, step2327]: loss 0.581420
[epoch13, step2328]: loss 0.538677
[epoch13, step2329]: loss 0.685376
[epoch13, step2330]: loss 0.538391
[epoch13, step2331]: loss 0.393684
[epoch13, step2332]: loss 0.148885
[epoch13, step2333]: loss 0.510585
[epoch13, step2334]: loss 0.640462
[epoch13, step2335]: loss 0.391843
[epoch13, step2336]: loss 0.609191
[epoch13, step2337]: loss 0.577363
[epoch13, step2338]: loss 0.288669
[epoch13, step2339]: loss 0.664524
[epoch13, step2340]: loss 0.429522
[epoch13, step2341]: loss 0.527383
[epoch13, step2342]: loss 0.613023
[epoch13, step2343]: loss 0.445230
[epoch13, step2344]: loss 0.423268
[epoch13, step2345]: loss 0.490849
[epoch13, step2346]: loss 0.374962
[epoch13, step2347]: loss 0.195515
[epoch13, step2348]: loss 0.532062
[epoch13, step2349]: loss 0.443269
[epoch13, step2350]: loss 0.577789
[epoch13, step2351]: loss 0.471279
[epoch13, step2352]: loss 0.649767
[epoch13, step2353]: loss 0.479607
[epoch13, step2354]: loss 0.417808
[epoch13, step2355]: loss 0.536841
[epoch13, step2356]: loss 0.240497
[epoch13, step2357]: loss 0.429796
[epoch13, step2358]: loss 0.530984
[epoch13, step2359]: loss 0.616066
[epoch13, step2360]: loss 0.253680
[epoch13, step2361]: loss 0.561795
[epoch13, step2362]: loss 0.431896
[epoch13, step2363]: loss 0.524580
[epoch13, step2364]: loss 0.524605
[epoch13, step2365]: loss 0.280981
[epoch13, step2366]: loss 0.580806
[epoch13, step2367]: loss 0.415940
[epoch13, step2368]: loss 0.445868
[epoch13, step2369]: loss 0.584098
[epoch13, step2370]: loss 0.584787
[epoch13, step2371]: loss 0.287465
[epoch13, step2372]: loss 0.682511
[epoch13, step2373]: loss 0.789595
[epoch13, step2374]: loss 0.485557
[epoch13, step2375]: loss 0.589704
[epoch13, step2376]: loss 0.202252
[epoch13, step2377]: loss 0.455077
[epoch13, step2378]: loss 0.606430
[epoch13, step2379]: loss 0.499958
[epoch13, step2380]: loss 0.596950
[epoch13, step2381]: loss 0.491377
[epoch13, step2382]: loss 0.326126
[epoch13, step2383]: loss 0.469331
[epoch13, step2384]: loss 0.596327
[epoch13, step2385]: loss 0.322635
[epoch13, step2386]: loss 0.133576
[epoch13, step2387]: loss 0.424356
[epoch13, step2388]: loss 0.406206
[epoch13, step2389]: loss 0.362965
[epoch13, step2390]: loss 0.448576
[epoch13, step2391]: loss 0.429347
[epoch13, step2392]: loss 0.426748
[epoch13, step2393]: loss 0.473038
[epoch13, step2394]: loss 0.573476
[epoch13, step2395]: loss 0.551973
[epoch13, step2396]: loss 0.510480
[epoch13, step2397]: loss 0.260888
[epoch13, step2398]: loss 0.378214
[epoch13, step2399]: loss 0.567385
[epoch13, step2400]: loss 0.444542
[epoch13, step2401]: loss 0.502752
[epoch13, step2402]: loss 0.458990
[epoch13, step2403]: loss 0.424353
[epoch13, step2404]: loss 0.472041
[epoch13, step2405]: loss 0.567604
[epoch13, step2406]: loss 0.503867
[epoch13, step2407]: loss 0.385296
[epoch13, step2408]: loss 0.596260
[epoch13, step2409]: loss 0.555015
[epoch13, step2410]: loss 0.675565
[epoch13, step2411]: loss 0.647219
[epoch13, step2412]: loss 0.503520
[epoch13, step2413]: loss 0.378228
[epoch13, step2414]: loss 0.420976
[epoch13, step2415]: loss 0.240341
[epoch13, step2416]: loss 0.448890
[epoch13, step2417]: loss 0.457593
[epoch13, step2418]: loss 0.627312
[epoch13, step2419]: loss 0.371069
[epoch13, step2420]: loss 0.485287
[epoch13, step2421]: loss 0.421799
[epoch13, step2422]: loss 0.545651
[epoch13, step2423]: loss 0.462525
[epoch13, step2424]: loss 0.578851
[epoch13, step2425]: loss 0.375659
[epoch13, step2426]: loss 0.581756
[epoch13, step2427]: loss 0.557731
[epoch13, step2428]: loss 0.406502
[epoch13, step2429]: loss 0.343602
[epoch13, step2430]: loss 0.412318
[epoch13, step2431]: loss 0.437108
[epoch13, step2432]: loss 0.354691
[epoch13, step2433]: loss 0.605832
[epoch13, step2434]: loss 0.520978
[epoch13, step2435]: loss 0.393337
[epoch13, step2436]: loss 0.367328
[epoch13, step2437]: loss 0.701736
[epoch13, step2438]: loss 0.644175
[epoch13, step2439]: loss 0.398003
[epoch13, step2440]: loss 0.529184
[epoch13, step2441]: loss 0.358863
[epoch13, step2442]: loss 0.581782
[epoch13, step2443]: loss 0.801733
[epoch13, step2444]: loss 0.614602
[epoch13, step2445]: loss 0.371576
[epoch13, step2446]: loss 0.503469
[epoch13, step2447]: loss 0.501217
[epoch13, step2448]: loss 0.439015
[epoch13, step2449]: loss 0.568671
[epoch13, step2450]: loss 0.384345
[epoch13, step2451]: loss 0.479374
[epoch13, step2452]: loss 0.516717
[epoch13, step2453]: loss 0.289047
[epoch13, step2454]: loss 0.449347
[epoch13, step2455]: loss 0.473876
[epoch13, step2456]: loss 0.628615
[epoch13, step2457]: loss 0.666249
[epoch13, step2458]: loss 0.420018
[epoch13, step2459]: loss 0.470805
[epoch13, step2460]: loss 0.631047
[epoch13, step2461]: loss 0.715790
[epoch13, step2462]: loss 0.347226
[epoch13, step2463]: loss 0.601538
[epoch13, step2464]: loss 0.598480
[epoch13, step2465]: loss 0.568232
[epoch13, step2466]: loss 0.421700
[epoch13, step2467]: loss 0.510656
[epoch13, step2468]: loss 0.558584
[epoch13, step2469]: loss 0.253213
[epoch13, step2470]: loss 0.648807
[epoch13, step2471]: loss 0.561567
[epoch13, step2472]: loss 0.390939
[epoch13, step2473]: loss 0.443885
[epoch13, step2474]: loss 0.661166
[epoch13, step2475]: loss 0.542860
[epoch13, step2476]: loss 0.490780
[epoch13, step2477]: loss 0.614418
[epoch13, step2478]: loss 0.595677
[epoch13, step2479]: loss 0.617803
[epoch13, step2480]: loss 0.545701
[epoch13, step2481]: loss 0.642451
[epoch13, step2482]: loss 0.545926
[epoch13, step2483]: loss 0.248558
[epoch13, step2484]: loss 0.686524
[epoch13, step2485]: loss 0.179143
[epoch13, step2486]: loss 0.412464
[epoch13, step2487]: loss 0.550065
[epoch13, step2488]: loss 0.536158
[epoch13, step2489]: loss 0.549236
[epoch13, step2490]: loss 0.604152
[epoch13, step2491]: loss 0.516277
[epoch13, step2492]: loss 0.826017
[epoch13, step2493]: loss 0.518081
[epoch13, step2494]: loss 0.694111
[epoch13, step2495]: loss 0.576785
[epoch13, step2496]: loss 0.359363
[epoch13, step2497]: loss 0.324949
[epoch13, step2498]: loss 0.391637
[epoch13, step2499]: loss 0.444018
[epoch13, step2500]: loss 0.521592
[epoch13, step2501]: loss 0.688207
[epoch13, step2502]: loss 0.304480
[epoch13, step2503]: loss 0.480395
[epoch13, step2504]: loss 0.562676
[epoch13, step2505]: loss 0.579167
[epoch13, step2506]: loss 0.577329
[epoch13, step2507]: loss 0.441246
[epoch13, step2508]: loss 0.552565
[epoch13, step2509]: loss 0.553756
[epoch13, step2510]: loss 0.650148
[epoch13, step2511]: loss 0.284880
[epoch13, step2512]: loss 0.544850
[epoch13, step2513]: loss 0.322055
[epoch13, step2514]: loss 0.462310
[epoch13, step2515]: loss 0.527765
[epoch13, step2516]: loss 0.503282
[epoch13, step2517]: loss 0.449139
[epoch13, step2518]: loss 0.438779
[epoch13, step2519]: loss 0.731690
[epoch13, step2520]: loss 0.569664
[epoch13, step2521]: loss 0.358270
[epoch13, step2522]: loss 0.711884
[epoch13, step2523]: loss 0.597017
[epoch13, step2524]: loss 0.598418
[epoch13, step2525]: loss 0.598701
[epoch13, step2526]: loss 0.500461
[epoch13, step2527]: loss 0.460627
[epoch13, step2528]: loss 0.362638
[epoch13, step2529]: loss 0.438150
[epoch13, step2530]: loss 0.418938
[epoch13, step2531]: loss 0.578107
[epoch13, step2532]: loss 0.439481
[epoch13, step2533]: loss 0.466887
[epoch13, step2534]: loss 0.674664
[epoch13, step2535]: loss 0.266159
[epoch13, step2536]: loss 0.672664
[epoch13, step2537]: loss 0.595386
[epoch13, step2538]: loss 0.476862
[epoch13, step2539]: loss 0.391989
[epoch13, step2540]: loss 0.395812
[epoch13, step2541]: loss 0.628513
[epoch13, step2542]: loss 0.446305
[epoch13, step2543]: loss 0.588828
[epoch13, step2544]: loss 0.335301
[epoch13, step2545]: loss 0.559587
[epoch13, step2546]: loss 0.485806
[epoch13, step2547]: loss 0.546870
[epoch13, step2548]: loss 0.608619
[epoch13, step2549]: loss 0.639678
[epoch13, step2550]: loss 0.588114
[epoch13, step2551]: loss 0.435276
[epoch13, step2552]: loss 0.232393
[epoch13, step2553]: loss 0.454543
[epoch13, step2554]: loss 0.548981
[epoch13, step2555]: loss 0.347951
[epoch13, step2556]: loss 0.342681
[epoch13, step2557]: loss 0.341024
[epoch13, step2558]: loss 0.588985
[epoch13, step2559]: loss 0.435195
[epoch13, step2560]: loss 0.606852
[epoch13, step2561]: loss 0.491438
[epoch13, step2562]: loss 0.587598
[epoch13, step2563]: loss 0.263362
[epoch13, step2564]: loss 0.554264
[epoch13, step2565]: loss 0.575376
[epoch13, step2566]: loss 0.351061
[epoch13, step2567]: loss 0.684915
[epoch13, step2568]: loss 0.474430
[epoch13, step2569]: loss 0.405088
[epoch13, step2570]: loss 0.572540
[epoch13, step2571]: loss 0.377190
[epoch13, step2572]: loss 0.521922
[epoch13, step2573]: loss 0.258990
[epoch13, step2574]: loss 0.515830
[epoch13, step2575]: loss 0.651349
[epoch13, step2576]: loss 0.566515
[epoch13, step2577]: loss 0.430054
[epoch13, step2578]: loss 0.378939
[epoch13, step2579]: loss 0.376705
[epoch13, step2580]: loss 0.451100
[epoch13, step2581]: loss 0.359122
[epoch13, step2582]: loss 0.557858
[epoch13, step2583]: loss 0.535713
[epoch13, step2584]: loss 0.618391
[epoch13, step2585]: loss 0.731362
[epoch13, step2586]: loss 0.455672
[epoch13, step2587]: loss 0.566209
[epoch13, step2588]: loss 0.583302
[epoch13, step2589]: loss 0.327743
[epoch13, step2590]: loss 0.680908
[epoch13, step2591]: loss 0.423719
[epoch13, step2592]: loss 0.393025
[epoch13, step2593]: loss 0.579849
[epoch13, step2594]: loss 0.294179
[epoch13, step2595]: loss 0.526574
[epoch13, step2596]: loss 0.159994
[epoch13, step2597]: loss 0.208394
[epoch13, step2598]: loss 0.641978
[epoch13, step2599]: loss 0.529007
[epoch13, step2600]: loss 0.518471
[epoch13, step2601]: loss 0.280938
[epoch13, step2602]: loss 0.469959
[epoch13, step2603]: loss 0.239990
[epoch13, step2604]: loss 0.551358
[epoch13, step2605]: loss 0.480713
[epoch13, step2606]: loss 0.577245
[epoch13, step2607]: loss 0.250314
[epoch13, step2608]: loss 0.568933
[epoch13, step2609]: loss 0.316241
[epoch13, step2610]: loss 0.567113
[epoch13, step2611]: loss 0.421745
[epoch13, step2612]: loss 0.552637
[epoch13, step2613]: loss 0.285566
[epoch13, step2614]: loss 0.192894
[epoch13, step2615]: loss 0.447806
[epoch13, step2616]: loss 0.536079
[epoch13, step2617]: loss 0.557585
[epoch13, step2618]: loss 0.548977
[epoch13, step2619]: loss 0.330500
[epoch13, step2620]: loss 0.431505
[epoch13, step2621]: loss 0.423435
[epoch13, step2622]: loss 0.694180
[epoch13, step2623]: loss 0.455247
[epoch13, step2624]: loss 0.480012
[epoch13, step2625]: loss 0.435586
[epoch13, step2626]: loss 0.512447
[epoch13, step2627]: loss 0.592834
[epoch13, step2628]: loss 0.355170
[epoch13, step2629]: loss 0.650121
[epoch13, step2630]: loss 0.441853
[epoch13, step2631]: loss 0.388859
[epoch13, step2632]: loss 0.406483
[epoch13, step2633]: loss 0.253950
[epoch13, step2634]: loss 0.543200
[epoch13, step2635]: loss 0.497745
[epoch13, step2636]: loss 0.474039
[epoch13, step2637]: loss 0.492111
[epoch13, step2638]: loss 0.385808
[epoch13, step2639]: loss 0.481470
[epoch13, step2640]: loss 0.432878
[epoch13, step2641]: loss 0.464608
[epoch13, step2642]: loss 0.355061
[epoch13, step2643]: loss 0.423572
[epoch13, step2644]: loss 0.529418
[epoch13, step2645]: loss 0.585828
[epoch13, step2646]: loss 0.539986
[epoch13, step2647]: loss 0.369165
[epoch13, step2648]: loss 0.433402
[epoch13, step2649]: loss 0.206630
[epoch13, step2650]: loss 0.333665
[epoch13, step2651]: loss 0.458991
[epoch13, step2652]: loss 0.633240
[epoch13, step2653]: loss 0.480781
[epoch13, step2654]: loss 0.456063
[epoch13, step2655]: loss 0.608806
[epoch13, step2656]: loss 0.237730
[epoch13, step2657]: loss 0.590238
[epoch13, step2658]: loss 0.460658
[epoch13, step2659]: loss 0.720540
[epoch13, step2660]: loss 0.512682
[epoch13, step2661]: loss 0.516192
[epoch13, step2662]: loss 0.352141
[epoch13, step2663]: loss 0.515949
[epoch13, step2664]: loss 0.373219
[epoch13, step2665]: loss 0.638130
[epoch13, step2666]: loss 0.630387
[epoch13, step2667]: loss 0.453268
[epoch13, step2668]: loss 0.467164
[epoch13, step2669]: loss 0.586505
[epoch13, step2670]: loss 0.594693
[epoch13, step2671]: loss 0.286648
[epoch13, step2672]: loss 0.421757
[epoch13, step2673]: loss 0.341201
[epoch13, step2674]: loss 0.679507
[epoch13, step2675]: loss 0.381741
[epoch13, step2676]: loss 0.463098
[epoch13, step2677]: loss 0.685006
[epoch13, step2678]: loss 0.327208
[epoch13, step2679]: loss 0.547945
[epoch13, step2680]: loss 0.296920
[epoch13, step2681]: loss 0.251697
[epoch13, step2682]: loss 0.459525
[epoch13, step2683]: loss 0.434071
[epoch13, step2684]: loss 0.479453
[epoch13, step2685]: loss 0.422649
[epoch13, step2686]: loss 0.490932
[epoch13, step2687]: loss 0.407846
[epoch13, step2688]: loss 0.565249
[epoch13, step2689]: loss 0.380260
[epoch13, step2690]: loss 0.546904
[epoch13, step2691]: loss 0.551858
[epoch13, step2692]: loss 0.313052
[epoch13, step2693]: loss 0.594420
[epoch13, step2694]: loss 0.525457
[epoch13, step2695]: loss 0.446616
[epoch13, step2696]: loss 0.593137
[epoch13, step2697]: loss 0.483684
[epoch13, step2698]: loss 0.538289
[epoch13, step2699]: loss 0.406040
[epoch13, step2700]: loss 0.560554
[epoch13, step2701]: loss 0.607815
[epoch13, step2702]: loss 0.535576
[epoch13, step2703]: loss 0.327533
[epoch13, step2704]: loss 0.437979
[epoch13, step2705]: loss 0.501125
[epoch13, step2706]: loss 0.666817
[epoch13, step2707]: loss 0.468759
[epoch13, step2708]: loss 0.306149
[epoch13, step2709]: loss 0.554939
[epoch13, step2710]: loss 0.399966
[epoch13, step2711]: loss 0.464195
[epoch13, step2712]: loss 0.639501
[epoch13, step2713]: loss 0.474144
[epoch13, step2714]: loss 0.462381
[epoch13, step2715]: loss 0.522394
[epoch13, step2716]: loss 0.617430
[epoch13, step2717]: loss 0.531873
[epoch13, step2718]: loss 0.591489
[epoch13, step2719]: loss 0.517541
[epoch13, step2720]: loss 0.587778
[epoch13, step2721]: loss 0.505147
[epoch13, step2722]: loss 0.441797
[epoch13, step2723]: loss 0.650019
[epoch13, step2724]: loss 0.427287
[epoch13, step2725]: loss 0.421220
[epoch13, step2726]: loss 0.454306
[epoch13, step2727]: loss 0.481336
[epoch13, step2728]: loss 0.431201
[epoch13, step2729]: loss 0.390767
[epoch13, step2730]: loss 0.395602
[epoch13, step2731]: loss 0.356423
[epoch13, step2732]: loss 0.481829
[epoch13, step2733]: loss 0.544840
[epoch13, step2734]: loss 0.515591
[epoch13, step2735]: loss 0.421511
[epoch13, step2736]: loss 0.642051
[epoch13, step2737]: loss 0.506887
[epoch13, step2738]: loss 0.590845
[epoch13, step2739]: loss 0.521885
[epoch13, step2740]: loss 0.590914
[epoch13, step2741]: loss 0.500780
[epoch13, step2742]: loss 0.412819
[epoch13, step2743]: loss 0.282037
[epoch13, step2744]: loss 0.414789
[epoch13, step2745]: loss 0.493025
[epoch13, step2746]: loss 0.421045
[epoch13, step2747]: loss 0.322237
[epoch13, step2748]: loss 0.632507
[epoch13, step2749]: loss 0.469456
[epoch13, step2750]: loss 0.521029
[epoch13, step2751]: loss 0.367842
[epoch13, step2752]: loss 0.479519
[epoch13, step2753]: loss 0.399931
[epoch13, step2754]: loss 0.405234
[epoch13, step2755]: loss 0.746740
[epoch13, step2756]: loss 0.513273
[epoch13, step2757]: loss 0.372932
[epoch13, step2758]: loss 0.597022
[epoch13, step2759]: loss 0.506726
[epoch13, step2760]: loss 0.540885
[epoch13, step2761]: loss 0.559671
[epoch13, step2762]: loss 0.645253
[epoch13, step2763]: loss 0.546076
[epoch13, step2764]: loss 0.450274
[epoch13, step2765]: loss 0.601955
[epoch13, step2766]: loss 0.262010
[epoch13, step2767]: loss 0.705600
[epoch13, step2768]: loss 0.373922
[epoch13, step2769]: loss 0.241157
[epoch13, step2770]: loss 0.517561
[epoch13, step2771]: loss 0.395104
[epoch13, step2772]: loss 0.404363
[epoch13, step2773]: loss 0.565592
[epoch13, step2774]: loss 0.298458
[epoch13, step2775]: loss 0.478773
[epoch13, step2776]: loss 0.615391
[epoch13, step2777]: loss 0.395552
[epoch13, step2778]: loss 0.298171
[epoch13, step2779]: loss 0.448079
[epoch13, step2780]: loss 0.483910
[epoch13, step2781]: loss 0.460956
[epoch13, step2782]: loss 0.602771
[epoch13, step2783]: loss 0.409851
[epoch13, step2784]: loss 0.761678
[epoch13, step2785]: loss 0.488561
[epoch13, step2786]: loss 0.506185
[epoch13, step2787]: loss 0.599545
[epoch13, step2788]: loss 0.687845
[epoch13, step2789]: loss 0.588518
[epoch13, step2790]: loss 0.328602
[epoch13, step2791]: loss 0.472503
[epoch13, step2792]: loss 0.482802
[epoch13, step2793]: loss 0.331618
[epoch13, step2794]: loss 0.447978
[epoch13, step2795]: loss 0.507863
[epoch13, step2796]: loss 0.623480
[epoch13, step2797]: loss 0.670572
[epoch13, step2798]: loss 0.454691
[epoch13, step2799]: loss 0.422501
[epoch13, step2800]: loss 0.467734
[epoch13, step2801]: loss 0.415201
[epoch13, step2802]: loss 0.820620
[epoch13, step2803]: loss 0.407743
[epoch13, step2804]: loss 0.498698
[epoch13, step2805]: loss 0.469589
[epoch13, step2806]: loss 0.586379
[epoch13, step2807]: loss 0.600605
[epoch13, step2808]: loss 0.361737
[epoch13, step2809]: loss 0.510071
[epoch13, step2810]: loss 0.377945
[epoch13, step2811]: loss 0.555541
[epoch13, step2812]: loss 0.587073
[epoch13, step2813]: loss 0.560956
[epoch13, step2814]: loss 0.592056
[epoch13, step2815]: loss 0.479420
[epoch13, step2816]: loss 0.470526
[epoch13, step2817]: loss 0.376146
[epoch13, step2818]: loss 0.384372
[epoch13, step2819]: loss 0.514703
[epoch13, step2820]: loss 0.450813
[epoch13, step2821]: loss 0.676410
[epoch13, step2822]: loss 0.491509
[epoch13, step2823]: loss 0.423543
[epoch13, step2824]: loss 0.267471
[epoch13, step2825]: loss 0.683801
[epoch13, step2826]: loss 0.485736
[epoch13, step2827]: loss 0.234429
[epoch13, step2828]: loss 0.345450
[epoch13, step2829]: loss 0.403358
[epoch13, step2830]: loss 0.536696
[epoch13, step2831]: loss 0.400644
[epoch13, step2832]: loss 0.629531
[epoch13, step2833]: loss 0.392408
[epoch13, step2834]: loss 0.711390
[epoch13, step2835]: loss 0.602652
[epoch13, step2836]: loss 0.568367
[epoch13, step2837]: loss 0.680189
[epoch13, step2838]: loss 0.543228
[epoch13, step2839]: loss 0.432808
[epoch13, step2840]: loss 0.261109
[epoch13, step2841]: loss 0.657041
[epoch13, step2842]: loss 0.638804
[epoch13, step2843]: loss 0.399531
[epoch13, step2844]: loss 0.443412
[epoch13, step2845]: loss 0.481638
[epoch13, step2846]: loss 0.580071
[epoch13, step2847]: loss 0.492941
[epoch13, step2848]: loss 0.840709
[epoch13, step2849]: loss 0.523636
[epoch13, step2850]: loss 0.529635
[epoch13, step2851]: loss 0.442373
[epoch13, step2852]: loss 0.280449
[epoch13, step2853]: loss 0.167474
[epoch13, step2854]: loss 0.311743
[epoch13, step2855]: loss 0.524834
[epoch13, step2856]: loss 0.348581
[epoch13, step2857]: loss 0.445786
[epoch13, step2858]: loss 0.557949
[epoch13, step2859]: loss 0.304324
[epoch13, step2860]: loss 0.463461
[epoch13, step2861]: loss 0.399860
[epoch13, step2862]: loss 0.569088
[epoch13, step2863]: loss 0.374249
[epoch13, step2864]: loss 0.563298
[epoch13, step2865]: loss 0.618445
[epoch13, step2866]: loss 0.556020
[epoch13, step2867]: loss 0.295562
[epoch13, step2868]: loss 0.463482
[epoch13, step2869]: loss 0.614058
[epoch13, step2870]: loss 0.583022
[epoch13, step2871]: loss 0.311768
[epoch13, step2872]: loss 0.409266
[epoch13, step2873]: loss 0.442377
[epoch13, step2874]: loss 0.786536
[epoch13, step2875]: loss 0.273086
[epoch13, step2876]: loss 0.504056
[epoch13, step2877]: loss 0.518309
[epoch13, step2878]: loss 0.655218
[epoch13, step2879]: loss 0.598355
[epoch13, step2880]: loss 0.529493
[epoch13, step2881]: loss 0.486825
[epoch13, step2882]: loss 0.671891
[epoch13, step2883]: loss 0.631712
[epoch13, step2884]: loss 0.363323
[epoch13, step2885]: loss 0.527435
[epoch13, step2886]: loss 0.410121
[epoch13, step2887]: loss 0.501073
[epoch13, step2888]: loss 0.502381
[epoch13, step2889]: loss 0.321817
[epoch13, step2890]: loss 0.375954
[epoch13, step2891]: loss 0.434622
[epoch13, step2892]: loss 0.433721
[epoch13, step2893]: loss 0.448098
[epoch13, step2894]: loss 0.505504
[epoch13, step2895]: loss 0.595356
[epoch13, step2896]: loss 0.414903
[epoch13, step2897]: loss 0.637973
[epoch13, step2898]: loss 0.241089
[epoch13, step2899]: loss 0.524439
[epoch13, step2900]: loss 0.552244
[epoch13, step2901]: loss 0.458916
[epoch13, step2902]: loss 0.364410
[epoch13, step2903]: loss 0.261060
[epoch13, step2904]: loss 0.466119
[epoch13, step2905]: loss 0.448304
[epoch13, step2906]: loss 0.552494
[epoch13, step2907]: loss 0.474597
[epoch13, step2908]: loss 0.596727
[epoch13, step2909]: loss 0.466229
[epoch13, step2910]: loss 0.551299
[epoch13, step2911]: loss 0.402135
[epoch13, step2912]: loss 0.462487
[epoch13, step2913]: loss 0.572014
[epoch13, step2914]: loss 0.476429
[epoch13, step2915]: loss 0.727622
[epoch13, step2916]: loss 0.383236
[epoch13, step2917]: loss 0.387385
[epoch13, step2918]: loss 0.240229
[epoch13, step2919]: loss 0.434530
[epoch13, step2920]: loss 0.451240
[epoch13, step2921]: loss 0.610143
[epoch13, step2922]: loss 0.110352
[epoch13, step2923]: loss 0.540916
[epoch13, step2924]: loss 0.469238
[epoch13, step2925]: loss 0.427167
[epoch13, step2926]: loss 0.473590
[epoch13, step2927]: loss 0.346776
[epoch13, step2928]: loss 0.343305
[epoch13, step2929]: loss 0.574351
[epoch13, step2930]: loss 0.523658
[epoch13, step2931]: loss 0.504177
[epoch13, step2932]: loss 0.525147
[epoch13, step2933]: loss 0.603389
[epoch13, step2934]: loss 0.453927
[epoch13, step2935]: loss 0.381141
[epoch13, step2936]: loss 0.576158
[epoch13, step2937]: loss 0.442101
[epoch13, step2938]: loss 0.251052
[epoch13, step2939]: loss 0.612844
[epoch13, step2940]: loss 0.243020
[epoch13, step2941]: loss 0.506395
[epoch13, step2942]: loss 0.609200
[epoch13, step2943]: loss 0.496436
[epoch13, step2944]: loss 0.451788
[epoch13, step2945]: loss 0.607019
[epoch13, step2946]: loss 0.731357
[epoch13, step2947]: loss 0.506655
[epoch13, step2948]: loss 0.720812
[epoch13, step2949]: loss 0.398830
[epoch13, step2950]: loss 0.450806
[epoch13, step2951]: loss 0.417562
[epoch13, step2952]: loss 0.699390
[epoch13, step2953]: loss 0.604315
[epoch13, step2954]: loss 0.445628
[epoch13, step2955]: loss 0.524616
[epoch13, step2956]: loss 0.358519
[epoch13, step2957]: loss 0.504550
[epoch13, step2958]: loss 0.461909
[epoch13, step2959]: loss 0.687045
[epoch13, step2960]: loss 0.493498
[epoch13, step2961]: loss 0.520376
[epoch13, step2962]: loss 0.662493
[epoch13, step2963]: loss 0.414636
[epoch13, step2964]: loss 0.556451
[epoch13, step2965]: loss 0.595328
[epoch13, step2966]: loss 0.651650
[epoch13, step2967]: loss 0.415423
[epoch13, step2968]: loss 0.649942
[epoch13, step2969]: loss 0.408596
[epoch13, step2970]: loss 0.378491
[epoch13, step2971]: loss 0.671363
[epoch13, step2972]: loss 0.387703
[epoch13, step2973]: loss 0.604600
[epoch13, step2974]: loss 0.607286
[epoch13, step2975]: loss 0.594024
[epoch13, step2976]: loss 0.367062
[epoch13, step2977]: loss 0.532032
[epoch13, step2978]: loss 0.449951
[epoch13, step2979]: loss 0.581549
[epoch13, step2980]: loss 0.621108
[epoch13, step2981]: loss 0.349745
[epoch13, step2982]: loss 0.550940
[epoch13, step2983]: loss 0.248312
[epoch13, step2984]: loss 0.617913
[epoch13, step2985]: loss 0.484719
[epoch13, step2986]: loss 0.655790
[epoch13, step2987]: loss 0.229167
[epoch13, step2988]: loss 0.680001
[epoch13, step2989]: loss 0.349335
[epoch13, step2990]: loss 0.464964
[epoch13, step2991]: loss 0.557193
[epoch13, step2992]: loss 0.621286
[epoch13, step2993]: loss 0.480888
[epoch13, step2994]: loss 0.432693
[epoch13, step2995]: loss 0.611571
[epoch13, step2996]: loss 0.501591
[epoch13, step2997]: loss 0.332592
[epoch13, step2998]: loss 0.484154
[epoch13, step2999]: loss 0.316991
[epoch13, step3000]: loss 0.419882
[epoch13, step3001]: loss 0.378802
[epoch13, step3002]: loss 0.422560
[epoch13, step3003]: loss 0.572074
[epoch13, step3004]: loss 0.603968
[epoch13, step3005]: loss 0.546431
[epoch13, step3006]: loss 0.546160
[epoch13, step3007]: loss 0.304785
[epoch13, step3008]: loss 0.407907
[epoch13, step3009]: loss 0.605738
[epoch13, step3010]: loss 0.367734
[epoch13, step3011]: loss 0.443637
[epoch13, step3012]: loss 0.448991
[epoch13, step3013]: loss 0.439260
[epoch13, step3014]: loss 0.569783
[epoch13, step3015]: loss 0.385099
[epoch13, step3016]: loss 0.285710
[epoch13, step3017]: loss 0.536547
[epoch13, step3018]: loss 0.238065
[epoch13, step3019]: loss 0.629075
[epoch13, step3020]: loss 0.583669
[epoch13, step3021]: loss 0.430811
[epoch13, step3022]: loss 0.351365
[epoch13, step3023]: loss 0.649719
[epoch13, step3024]: loss 0.495844
[epoch13, step3025]: loss 0.297117
[epoch13, step3026]: loss 0.413050
[epoch13, step3027]: loss 0.479862
[epoch13, step3028]: loss 0.389380
[epoch13, step3029]: loss 0.772947
[epoch13, step3030]: loss 0.439945
[epoch13, step3031]: loss 0.466878
[epoch13, step3032]: loss 0.383635
[epoch13, step3033]: loss 0.452726
[epoch13, step3034]: loss 0.679213
[epoch13, step3035]: loss 0.393767
[epoch13, step3036]: loss 0.443202
[epoch13, step3037]: loss 0.461259
[epoch13, step3038]: loss 0.164355
[epoch13, step3039]: loss 0.633659
[epoch13, step3040]: loss 0.591635
[epoch13, step3041]: loss 0.526653
[epoch13, step3042]: loss 0.526262
[epoch13, step3043]: loss 0.618741
[epoch13, step3044]: loss 0.686827
[epoch13, step3045]: loss 0.430788
[epoch13, step3046]: loss 0.528309
[epoch13, step3047]: loss 0.144024
[epoch13, step3048]: loss 0.405515
[epoch13, step3049]: loss 0.103382
[epoch13, step3050]: loss 0.517819
[epoch13, step3051]: loss 0.550325
[epoch13, step3052]: loss 0.589504
[epoch13, step3053]: loss 0.589148
[epoch13, step3054]: loss 0.461024
[epoch13, step3055]: loss 0.542394
[epoch13, step3056]: loss 0.272653
[epoch13, step3057]: loss 0.343901
[epoch13, step3058]: loss 0.417744
[epoch13, step3059]: loss 0.456825
[epoch13, step3060]: loss 0.435817
[epoch13, step3061]: loss 0.589809
[epoch13, step3062]: loss 0.442848
[epoch13, step3063]: loss 0.705059
[epoch13, step3064]: loss 0.451005
[epoch13, step3065]: loss 0.551593
[epoch13, step3066]: loss 0.479698
[epoch13, step3067]: loss 0.585643
[epoch13, step3068]: loss 0.506343
[epoch13, step3069]: loss 0.448772
[epoch13, step3070]: loss 0.597240
[epoch13, step3071]: loss 0.675762
[epoch13, step3072]: loss 0.543063
[epoch13, step3073]: loss 0.576691
[epoch13, step3074]: loss 0.301195
[epoch13, step3075]: loss 0.585619
[epoch13, step3076]: loss 0.154312

[epoch13]: avg loss 0.154312

[epoch14, step1]: loss 0.393788
[epoch14, step2]: loss 0.274207
[epoch14, step3]: loss 0.449687
[epoch14, step4]: loss 0.361873
[epoch14, step5]: loss 0.584331
[epoch14, step6]: loss 0.263770
[epoch14, step7]: loss 0.329284
[epoch14, step8]: loss 0.424015
[epoch14, step9]: loss 0.698326
[epoch14, step10]: loss 0.564966
[epoch14, step11]: loss 0.489878
[epoch14, step12]: loss 0.346087
[epoch14, step13]: loss 0.686057
[epoch14, step14]: loss 0.441705
[epoch14, step15]: loss 0.464273
[epoch14, step16]: loss 0.515568
[epoch14, step17]: loss 0.287102
[epoch14, step18]: loss 0.636477
[epoch14, step19]: loss 0.535180
[epoch14, step20]: loss 0.577258
[epoch14, step21]: loss 0.396646
[epoch14, step22]: loss 0.664434
[epoch14, step23]: loss 0.676049
[epoch14, step24]: loss 0.724885
[epoch14, step25]: loss 0.537960
[epoch14, step26]: loss 0.273263
[epoch14, step27]: loss 0.552734
[epoch14, step28]: loss 0.528998
[epoch14, step29]: loss 0.301758
[epoch14, step30]: loss 0.677190
[epoch14, step31]: loss 0.350289
[epoch14, step32]: loss 0.682438
[epoch14, step33]: loss 0.578756
[epoch14, step34]: loss 0.311098
[epoch14, step35]: loss 0.616589
[epoch14, step36]: loss 0.582489
[epoch14, step37]: loss 0.692076
[epoch14, step38]: loss 0.606058
[epoch14, step39]: loss 0.554475
[epoch14, step40]: loss 0.525219
[epoch14, step41]: loss 0.589919
[epoch14, step42]: loss 0.423289
[epoch14, step43]: loss 0.566882
[epoch14, step44]: loss 0.300005
[epoch14, step45]: loss 0.435899
[epoch14, step46]: loss 0.274698
[epoch14, step47]: loss 0.393055
[epoch14, step48]: loss 0.516935
[epoch14, step49]: loss 0.325245
[epoch14, step50]: loss 0.443766
[epoch14, step51]: loss 0.647993
[epoch14, step52]: loss 0.356413
[epoch14, step53]: loss 0.254318
[epoch14, step54]: loss 0.192463
[epoch14, step55]: loss 0.347400
[epoch14, step56]: loss 0.724284
[epoch14, step57]: loss 0.430178
[epoch14, step58]: loss 0.403187
[epoch14, step59]: loss 0.623855
[epoch14, step60]: loss 0.418245
[epoch14, step61]: loss 0.701310
[epoch14, step62]: loss 0.493288
[epoch14, step63]: loss 0.415568
[epoch14, step64]: loss 0.431333
[epoch14, step65]: loss 0.608378
[epoch14, step66]: loss 0.599050
[epoch14, step67]: loss 0.435866
[epoch14, step68]: loss 0.424385
[epoch14, step69]: loss 0.447453
[epoch14, step70]: loss 0.551704
[epoch14, step71]: loss 0.451730
[epoch14, step72]: loss 0.635460
[epoch14, step73]: loss 0.456125
[epoch14, step74]: loss 0.534420
[epoch14, step75]: loss 0.713313
[epoch14, step76]: loss 0.417372
[epoch14, step77]: loss 0.503476
[epoch14, step78]: loss 0.399051
[epoch14, step79]: loss 0.519030
[epoch14, step80]: loss 0.290406
[epoch14, step81]: loss 0.485144
[epoch14, step82]: loss 0.531175
[epoch14, step83]: loss 0.452229
[epoch14, step84]: loss 0.446493
[epoch14, step85]: loss 0.767222
[epoch14, step86]: loss 0.684549
[epoch14, step87]: loss 0.150284
[epoch14, step88]: loss 0.350698
[epoch14, step89]: loss 0.255191
[epoch14, step90]: loss 0.486154
[epoch14, step91]: loss 0.353711
[epoch14, step92]: loss 0.534099
[epoch14, step93]: loss 0.502703
[epoch14, step94]: loss 0.439992
[epoch14, step95]: loss 0.234715
[epoch14, step96]: loss 0.364572
[epoch14, step97]: loss 0.381758
[epoch14, step98]: loss 0.675688
[epoch14, step99]: loss 0.229033
[epoch14, step100]: loss 0.569768
[epoch14, step101]: loss 0.482298
[epoch14, step102]: loss 0.420250
[epoch14, step103]: loss 0.255046
[epoch14, step104]: loss 0.728210
[epoch14, step105]: loss 0.449238
[epoch14, step106]: loss 0.329765
[epoch14, step107]: loss 0.531863
[epoch14, step108]: loss 0.212899
[epoch14, step109]: loss 0.328284
[epoch14, step110]: loss 0.364938
[epoch14, step111]: loss 0.530236
[epoch14, step112]: loss 0.496709
[epoch14, step113]: loss 0.618199
[epoch14, step114]: loss 0.302422
[epoch14, step115]: loss 0.578125
[epoch14, step116]: loss 0.526188
[epoch14, step117]: loss 0.404054
[epoch14, step118]: loss 0.540699
[epoch14, step119]: loss 0.500620
[epoch14, step120]: loss 0.740955
[epoch14, step121]: loss 0.609284
[epoch14, step122]: loss 0.487334
[epoch14, step123]: loss 0.749576
[epoch14, step124]: loss 0.619499
[epoch14, step125]: loss 0.420623
[epoch14, step126]: loss 0.287418
[epoch14, step127]: loss 0.724829
[epoch14, step128]: loss 0.349069
[epoch14, step129]: loss 0.275062
[epoch14, step130]: loss 0.437909
[epoch14, step131]: loss 0.423913
[epoch14, step132]: loss 0.287476
[epoch14, step133]: loss 0.463731
[epoch14, step134]: loss 0.468471
[epoch14, step135]: loss 0.357629
[epoch14, step136]: loss 0.179622
[epoch14, step137]: loss 0.533716
[epoch14, step138]: loss 0.469327
[epoch14, step139]: loss 0.474222
[epoch14, step140]: loss 0.413115
[epoch14, step141]: loss 0.409581
[epoch14, step142]: loss 0.762112
[epoch14, step143]: loss 0.413999
[epoch14, step144]: loss 0.481081
[epoch14, step145]: loss 0.288519
[epoch14, step146]: loss 0.500969
[epoch14, step147]: loss 0.274077
[epoch14, step148]: loss 0.695445
[epoch14, step149]: loss 0.199419
[epoch14, step150]: loss 0.493946
[epoch14, step151]: loss 0.617718
[epoch14, step152]: loss 0.535252
[epoch14, step153]: loss 0.479378
[epoch14, step154]: loss 0.643723
[epoch14, step155]: loss 0.374078
[epoch14, step156]: loss 0.254060
[epoch14, step157]: loss 0.268245
[epoch14, step158]: loss 0.511493
[epoch14, step159]: loss 0.656662
[epoch14, step160]: loss 0.646192
[epoch14, step161]: loss 0.558848
[epoch14, step162]: loss 0.552570
[epoch14, step163]: loss 0.153549
[epoch14, step164]: loss 0.711350
[epoch14, step165]: loss 0.529853
[epoch14, step166]: loss 0.479409
[epoch14, step167]: loss 0.234765
[epoch14, step168]: loss 0.468476
[epoch14, step169]: loss 0.151633
[epoch14, step170]: loss 0.560550
[epoch14, step171]: loss 0.541771
[epoch14, step172]: loss 0.614015
[epoch14, step173]: loss 0.619922
[epoch14, step174]: loss 0.631174
[epoch14, step175]: loss 0.496754
[epoch14, step176]: loss 0.397426
[epoch14, step177]: loss 0.546149
[epoch14, step178]: loss 0.448647
[epoch14, step179]: loss 0.525074
[epoch14, step180]: loss 0.524033
[epoch14, step181]: loss 0.427528
[epoch14, step182]: loss 0.346011
[epoch14, step183]: loss 0.573547
[epoch14, step184]: loss 0.446065
[epoch14, step185]: loss 0.464925
[epoch14, step186]: loss 0.442543
[epoch14, step187]: loss 0.118674
[epoch14, step188]: loss 0.129549
[epoch14, step189]: loss 0.621169
[epoch14, step190]: loss 0.246893
[epoch14, step191]: loss 0.256129
[epoch14, step192]: loss 0.611392
[epoch14, step193]: loss 0.552432
[epoch14, step194]: loss 0.484676
[epoch14, step195]: loss 0.196921
[epoch14, step196]: loss 0.587651
[epoch14, step197]: loss 0.520087
[epoch14, step198]: loss 0.569911
[epoch14, step199]: loss 0.658351
[epoch14, step200]: loss 0.192973
[epoch14, step201]: loss 0.442795
[epoch14, step202]: loss 0.487373
[epoch14, step203]: loss 0.677359
[epoch14, step204]: loss 0.617101
[epoch14, step205]: loss 0.508770
[epoch14, step206]: loss 0.472014
[epoch14, step207]: loss 0.612393
[epoch14, step208]: loss 0.475718
[epoch14, step209]: loss 0.566172
[epoch14, step210]: loss 0.352726
[epoch14, step211]: loss 0.598828
[epoch14, step212]: loss 0.354212
[epoch14, step213]: loss 0.498041
[epoch14, step214]: loss 0.465347
[epoch14, step215]: loss 0.570722
[epoch14, step216]: loss 0.277857
[epoch14, step217]: loss 0.679859
[epoch14, step218]: loss 0.308829
[epoch14, step219]: loss 0.487434
[epoch14, step220]: loss 0.211927
[epoch14, step221]: loss 0.505710
[epoch14, step222]: loss 0.695059
[epoch14, step223]: loss 0.515262
[epoch14, step224]: loss 0.406344
[epoch14, step225]: loss 0.659200
[epoch14, step226]: loss 0.418429
[epoch14, step227]: loss 0.649627
[epoch14, step228]: loss 0.239482
[epoch14, step229]: loss 0.579108
[epoch14, step230]: loss 0.546921
[epoch14, step231]: loss 0.361750
[epoch14, step232]: loss 0.589767
[epoch14, step233]: loss 0.700716
[epoch14, step234]: loss 0.458327
[epoch14, step235]: loss 0.459060
[epoch14, step236]: loss 0.383904
[epoch14, step237]: loss 0.411204
[epoch14, step238]: loss 0.517533
[epoch14, step239]: loss 0.432568
[epoch14, step240]: loss 0.746441
[epoch14, step241]: loss 0.334823
[epoch14, step242]: loss 0.333619
[epoch14, step243]: loss 0.621821
[epoch14, step244]: loss 0.626918
[epoch14, step245]: loss 0.483098
[epoch14, step246]: loss 0.576999
[epoch14, step247]: loss 0.439543
[epoch14, step248]: loss 0.582939
[epoch14, step249]: loss 0.587652
[epoch14, step250]: loss 0.253741
[epoch14, step251]: loss 0.664008
[epoch14, step252]: loss 0.491023
[epoch14, step253]: loss 0.577425
[epoch14, step254]: loss 0.263355
[epoch14, step255]: loss 0.532139
[epoch14, step256]: loss 0.562088
[epoch14, step257]: loss 0.446209
[epoch14, step258]: loss 0.537497
[epoch14, step259]: loss 0.592204
[epoch14, step260]: loss 0.295679
[epoch14, step261]: loss 0.524308
[epoch14, step262]: loss 0.696840
[epoch14, step263]: loss 0.786988
[epoch14, step264]: loss 0.436404
[epoch14, step265]: loss 0.516455
[epoch14, step266]: loss 0.379722
[epoch14, step267]: loss 0.519797
[epoch14, step268]: loss 0.257557
[epoch14, step269]: loss 0.631091
[epoch14, step270]: loss 0.524864
[epoch14, step271]: loss 0.436407
[epoch14, step272]: loss 0.588582
[epoch14, step273]: loss 0.672334
[epoch14, step274]: loss 0.643140
[epoch14, step275]: loss 0.341238
[epoch14, step276]: loss 0.420964
[epoch14, step277]: loss 0.481593
[epoch14, step278]: loss 0.408026
[epoch14, step279]: loss 0.606708
[epoch14, step280]: loss 0.344325
[epoch14, step281]: loss 0.757480
[epoch14, step282]: loss 0.483264
[epoch14, step283]: loss 0.506838
[epoch14, step284]: loss 0.429520
[epoch14, step285]: loss 0.367064
[epoch14, step286]: loss 0.469614
[epoch14, step287]: loss 0.544303
[epoch14, step288]: loss 0.678424
[epoch14, step289]: loss 0.434364
[epoch14, step290]: loss 0.594324
[epoch14, step291]: loss 0.469527
[epoch14, step292]: loss 0.430472
[epoch14, step293]: loss 0.384745
[epoch14, step294]: loss 0.548368
[epoch14, step295]: loss 0.554365
[epoch14, step296]: loss 0.402069
[epoch14, step297]: loss 0.443324
[epoch14, step298]: loss 0.441890
[epoch14, step299]: loss 0.473192
[epoch14, step300]: loss 0.406943
[epoch14, step301]: loss 0.372508
[epoch14, step302]: loss 0.561711
[epoch14, step303]: loss 0.412751
[epoch14, step304]: loss 0.402934
[epoch14, step305]: loss 0.594738
[epoch14, step306]: loss 0.710513
[epoch14, step307]: loss 0.351865
[epoch14, step308]: loss 0.441433
[epoch14, step309]: loss 0.338601
[epoch14, step310]: loss 0.441073
[epoch14, step311]: loss 0.434667
[epoch14, step312]: loss 0.453465
[epoch14, step313]: loss 0.509129
[epoch14, step314]: loss 0.516809
[epoch14, step315]: loss 0.493784
[epoch14, step316]: loss 0.370299
[epoch14, step317]: loss 0.449821
[epoch14, step318]: loss 0.626006
[epoch14, step319]: loss 0.507176
[epoch14, step320]: loss 0.413798
[epoch14, step321]: loss 0.521149
[epoch14, step322]: loss 0.496459
[epoch14, step323]: loss 0.406472
[epoch14, step324]: loss 0.474114
[epoch14, step325]: loss 0.419262
[epoch14, step326]: loss 0.656669
[epoch14, step327]: loss 0.567114
[epoch14, step328]: loss 0.532441
[epoch14, step329]: loss 0.497247
[epoch14, step330]: loss 0.485973
[epoch14, step331]: loss 0.430516
[epoch14, step332]: loss 0.617223
[epoch14, step333]: loss 0.502345
[epoch14, step334]: loss 0.566823
[epoch14, step335]: loss 0.493620
[epoch14, step336]: loss 0.386294
[epoch14, step337]: loss 0.718828
[epoch14, step338]: loss 0.596133
[epoch14, step339]: loss 0.483556
[epoch14, step340]: loss 0.618085
[epoch14, step341]: loss 0.657983
[epoch14, step342]: loss 0.605214
[epoch14, step343]: loss 0.471961
[epoch14, step344]: loss 0.440847
[epoch14, step345]: loss 0.471158
[epoch14, step346]: loss 0.489014
[epoch14, step347]: loss 0.455521
[epoch14, step348]: loss 0.564170
[epoch14, step349]: loss 0.609159
[epoch14, step350]: loss 0.555681
[epoch14, step351]: loss 0.355557
[epoch14, step352]: loss 0.507402
[epoch14, step353]: loss 0.359171
[epoch14, step354]: loss 0.622890
[epoch14, step355]: loss 0.546770
[epoch14, step356]: loss 0.442251
[epoch14, step357]: loss 0.248240
[epoch14, step358]: loss 0.282020
[epoch14, step359]: loss 0.309260
[epoch14, step360]: loss 0.424742
[epoch14, step361]: loss 0.607300
[epoch14, step362]: loss 0.434728
[epoch14, step363]: loss 0.529066
[epoch14, step364]: loss 0.516654
[epoch14, step365]: loss 0.568623
[epoch14, step366]: loss 0.650927
[epoch14, step367]: loss 0.161606
[epoch14, step368]: loss 0.208669
[epoch14, step369]: loss 0.472527
[epoch14, step370]: loss 0.534813
[epoch14, step371]: loss 0.502030
[epoch14, step372]: loss 0.496860
[epoch14, step373]: loss 0.126113
[epoch14, step374]: loss 0.537900
[epoch14, step375]: loss 0.420281
[epoch14, step376]: loss 0.325482
[epoch14, step377]: loss 0.538637
[epoch14, step378]: loss 0.573543
[epoch14, step379]: loss 0.605887
[epoch14, step380]: loss 0.563988
[epoch14, step381]: loss 0.449066
[epoch14, step382]: loss 0.561128
[epoch14, step383]: loss 0.573345
[epoch14, step384]: loss 0.466430
[epoch14, step385]: loss 0.449468
[epoch14, step386]: loss 0.669619
[epoch14, step387]: loss 0.479271
[epoch14, step388]: loss 0.649970
[epoch14, step389]: loss 0.564827
[epoch14, step390]: loss 0.331667
[epoch14, step391]: loss 0.669069
[epoch14, step392]: loss 0.343234
[epoch14, step393]: loss 0.405879
[epoch14, step394]: loss 0.480132
[epoch14, step395]: loss 0.348169
[epoch14, step396]: loss 0.417401
[epoch14, step397]: loss 0.453407
[epoch14, step398]: loss 0.732027
[epoch14, step399]: loss 0.162084
[epoch14, step400]: loss 0.377828
[epoch14, step401]: loss 0.368649
[epoch14, step402]: loss 0.348981
[epoch14, step403]: loss 0.520726
[epoch14, step404]: loss 0.560486
[epoch14, step405]: loss 0.390981
[epoch14, step406]: loss 0.437549
[epoch14, step407]: loss 0.499364
[epoch14, step408]: loss 0.346173
[epoch14, step409]: loss 0.587780
[epoch14, step410]: loss 0.353574
[epoch14, step411]: loss 0.242234
[epoch14, step412]: loss 0.464330
[epoch14, step413]: loss 0.369717
[epoch14, step414]: loss 0.448706
[epoch14, step415]: loss 0.571556
[epoch14, step416]: loss 0.569378
[epoch14, step417]: loss 0.514781
[epoch14, step418]: loss 0.483453
[epoch14, step419]: loss 0.478461
[epoch14, step420]: loss 0.341293
[epoch14, step421]: loss 0.591612
[epoch14, step422]: loss 0.498849
[epoch14, step423]: loss 0.377190
[epoch14, step424]: loss 0.490815
[epoch14, step425]: loss 0.245346
[epoch14, step426]: loss 0.753425
[epoch14, step427]: loss 0.624775
[epoch14, step428]: loss 0.467921
[epoch14, step429]: loss 0.559903
[epoch14, step430]: loss 0.744407
[epoch14, step431]: loss 0.477133
[epoch14, step432]: loss 0.449810
[epoch14, step433]: loss 0.417313
[epoch14, step434]: loss 0.358009
[epoch14, step435]: loss 0.593795
[epoch14, step436]: loss 0.507002
[epoch14, step437]: loss 0.648300
[epoch14, step438]: loss 0.409034
[epoch14, step439]: loss 0.520170
[epoch14, step440]: loss 0.640941
[epoch14, step441]: loss 0.308752
[epoch14, step442]: loss 0.286444
[epoch14, step443]: loss 0.499591
[epoch14, step444]: loss 0.605612
[epoch14, step445]: loss 0.478495
[epoch14, step446]: loss 0.604622
[epoch14, step447]: loss 0.653921
[epoch14, step448]: loss 0.424876
[epoch14, step449]: loss 0.454742
[epoch14, step450]: loss 0.517167
[epoch14, step451]: loss 0.171687
[epoch14, step452]: loss 0.419189
[epoch14, step453]: loss 0.272525
[epoch14, step454]: loss 0.552486
[epoch14, step455]: loss 0.723415
[epoch14, step456]: loss 0.382340
[epoch14, step457]: loss 0.712287
[epoch14, step458]: loss 0.499751
[epoch14, step459]: loss 0.507755
[epoch14, step460]: loss 0.298031
[epoch14, step461]: loss 0.497020
[epoch14, step462]: loss 0.278464
[epoch14, step463]: loss 0.538891
[epoch14, step464]: loss 0.548313
[epoch14, step465]: loss 0.430539
[epoch14, step466]: loss 0.557584
[epoch14, step467]: loss 0.247503
[epoch14, step468]: loss 0.625180
[epoch14, step469]: loss 0.631084
[epoch14, step470]: loss 0.447675
[epoch14, step471]: loss 0.485815
[epoch14, step472]: loss 0.470943
[epoch14, step473]: loss 0.402698
[epoch14, step474]: loss 0.449736
[epoch14, step475]: loss 0.568340
[epoch14, step476]: loss 0.466532
[epoch14, step477]: loss 0.522917
[epoch14, step478]: loss 0.540714
[epoch14, step479]: loss 0.567623
[epoch14, step480]: loss 0.275390
[epoch14, step481]: loss 0.636338
[epoch14, step482]: loss 0.481875
[epoch14, step483]: loss 0.554530
[epoch14, step484]: loss 0.338534
[epoch14, step485]: loss 0.620781
[epoch14, step486]: loss 0.282390
[epoch14, step487]: loss 0.496376
[epoch14, step488]: loss 0.347357
[epoch14, step489]: loss 0.356450
[epoch14, step490]: loss 0.586857
[epoch14, step491]: loss 0.483531
[epoch14, step492]: loss 0.508316
[epoch14, step493]: loss 0.536864
[epoch14, step494]: loss 0.488779
[epoch14, step495]: loss 0.663117
[epoch14, step496]: loss 0.260740
[epoch14, step497]: loss 0.547687
[epoch14, step498]: loss 0.489102
[epoch14, step499]: loss 0.565271
[epoch14, step500]: loss 0.416010
[epoch14, step501]: loss 0.414278
[epoch14, step502]: loss 0.545064
[epoch14, step503]: loss 0.581062
[epoch14, step504]: loss 0.534683
[epoch14, step505]: loss 0.333972
[epoch14, step506]: loss 0.653342
[epoch14, step507]: loss 0.560444
[epoch14, step508]: loss 0.382664
[epoch14, step509]: loss 0.630816
[epoch14, step510]: loss 0.333007
[epoch14, step511]: loss 0.708371
[epoch14, step512]: loss 0.500322
[epoch14, step513]: loss 0.754723
[epoch14, step514]: loss 0.462746
[epoch14, step515]: loss 0.499430
[epoch14, step516]: loss 0.439346
[epoch14, step517]: loss 0.668871
[epoch14, step518]: loss 0.382966
[epoch14, step519]: loss 0.284263
[epoch14, step520]: loss 0.438015
[epoch14, step521]: loss 0.702087
[epoch14, step522]: loss 0.445740
[epoch14, step523]: loss 0.520050
[epoch14, step524]: loss 0.498970
[epoch14, step525]: loss 0.468097
[epoch14, step526]: loss 0.455368
[epoch14, step527]: loss 0.502521
[epoch14, step528]: loss 0.459453
[epoch14, step529]: loss 0.533475
[epoch14, step530]: loss 0.537717
[epoch14, step531]: loss 0.632108
[epoch14, step532]: loss 0.384021
[epoch14, step533]: loss 0.524976
[epoch14, step534]: loss 0.519290
[epoch14, step535]: loss 0.215626
[epoch14, step536]: loss 0.392440
[epoch14, step537]: loss 0.656815
[epoch14, step538]: loss 0.409369
[epoch14, step539]: loss 0.310705
[epoch14, step540]: loss 0.150689
[epoch14, step541]: loss 0.603456
[epoch14, step542]: loss 0.506128
[epoch14, step543]: loss 0.497360
[epoch14, step544]: loss 0.448171
[epoch14, step545]: loss 0.500153
[epoch14, step546]: loss 0.450073
[epoch14, step547]: loss 0.351747
[epoch14, step548]: loss 0.384297
[epoch14, step549]: loss 0.402496
[epoch14, step550]: loss 0.567136
[epoch14, step551]: loss 0.584027
[epoch14, step552]: loss 0.568822
[epoch14, step553]: loss 0.278037
[epoch14, step554]: loss 0.631506
[epoch14, step555]: loss 0.382232
[epoch14, step556]: loss 0.675327
[epoch14, step557]: loss 0.588675
[epoch14, step558]: loss 0.390516
[epoch14, step559]: loss 0.573721
[epoch14, step560]: loss 0.345215
[epoch14, step561]: loss 0.270966
[epoch14, step562]: loss 0.380413
[epoch14, step563]: loss 0.267870
[epoch14, step564]: loss 0.554266
[epoch14, step565]: loss 0.504294
[epoch14, step566]: loss 0.526235
[epoch14, step567]: loss 0.451102
[epoch14, step568]: loss 0.353632
[epoch14, step569]: loss 0.658336
[epoch14, step570]: loss 0.606199
[epoch14, step571]: loss 0.418034
[epoch14, step572]: loss 0.522712
[epoch14, step573]: loss 0.425537
[epoch14, step574]: loss 0.310940
[epoch14, step575]: loss 0.532455
[epoch14, step576]: loss 0.602222
[epoch14, step577]: loss 0.275784
[epoch14, step578]: loss 0.437350
[epoch14, step579]: loss 0.544033
[epoch14, step580]: loss 0.548244
[epoch14, step581]: loss 0.476055
[epoch14, step582]: loss 0.472514
[epoch14, step583]: loss 0.483484
[epoch14, step584]: loss 0.577015
[epoch14, step585]: loss 0.650061
[epoch14, step586]: loss 0.452457
[epoch14, step587]: loss 0.466279
[epoch14, step588]: loss 0.320800
[epoch14, step589]: loss 0.467280
[epoch14, step590]: loss 0.300654
[epoch14, step591]: loss 0.717741
[epoch14, step592]: loss 0.489225
[epoch14, step593]: loss 0.494079
[epoch14, step594]: loss 0.392317
[epoch14, step595]: loss 0.758279
[epoch14, step596]: loss 0.580221
[epoch14, step597]: loss 0.519707
[epoch14, step598]: loss 0.481569
[epoch14, step599]: loss 0.334845
[epoch14, step600]: loss 0.615289
[epoch14, step601]: loss 0.379540
[epoch14, step602]: loss 0.252245
[epoch14, step603]: loss 0.547402
[epoch14, step604]: loss 0.586785
[epoch14, step605]: loss 0.530922
[epoch14, step606]: loss 0.371321
[epoch14, step607]: loss 0.562715
[epoch14, step608]: loss 0.395693
[epoch14, step609]: loss 0.277469
[epoch14, step610]: loss 0.633592
[epoch14, step611]: loss 0.402544
[epoch14, step612]: loss 0.287108
[epoch14, step613]: loss 0.368398
[epoch14, step614]: loss 0.623299
[epoch14, step615]: loss 0.457192
[epoch14, step616]: loss 0.537644
[epoch14, step617]: loss 0.471128
[epoch14, step618]: loss 0.298622
[epoch14, step619]: loss 0.520068
[epoch14, step620]: loss 0.487117
[epoch14, step621]: loss 0.377676
[epoch14, step622]: loss 0.519054
[epoch14, step623]: loss 0.595589
[epoch14, step624]: loss 0.753090
[epoch14, step625]: loss 0.323474
[epoch14, step626]: loss 0.242198
[epoch14, step627]: loss 0.488032
[epoch14, step628]: loss 0.549409
[epoch14, step629]: loss 0.410395
[epoch14, step630]: loss 0.623727
[epoch14, step631]: loss 0.652166
[epoch14, step632]: loss 0.291336
[epoch14, step633]: loss 0.529269
[epoch14, step634]: loss 0.665540
[epoch14, step635]: loss 0.588425
[epoch14, step636]: loss 0.643782
[epoch14, step637]: loss 0.223362
[epoch14, step638]: loss 0.441500
[epoch14, step639]: loss 0.317087
[epoch14, step640]: loss 0.492404
[epoch14, step641]: loss 0.622718
[epoch14, step642]: loss 0.366131
[epoch14, step643]: loss 0.533698
[epoch14, step644]: loss 0.578504
[epoch14, step645]: loss 0.542542
[epoch14, step646]: loss 0.485105
[epoch14, step647]: loss 0.441640
[epoch14, step648]: loss 0.247438
[epoch14, step649]: loss 0.456247
[epoch14, step650]: loss 0.616310
[epoch14, step651]: loss 0.573313
[epoch14, step652]: loss 0.638325
[epoch14, step653]: loss 0.602133
[epoch14, step654]: loss 0.539206
[epoch14, step655]: loss 0.474043
[epoch14, step656]: loss 0.748128
[epoch14, step657]: loss 0.426865
[epoch14, step658]: loss 0.530557
[epoch14, step659]: loss 0.285132
[epoch14, step660]: loss 0.281545
[epoch14, step661]: loss 0.182859
[epoch14, step662]: loss 0.403950
[epoch14, step663]: loss 0.336380
[epoch14, step664]: loss 0.380803
[epoch14, step665]: loss 0.338085
[epoch14, step666]: loss 0.522934
[epoch14, step667]: loss 0.455979
[epoch14, step668]: loss 0.413303
[epoch14, step669]: loss 0.607901
[epoch14, step670]: loss 0.593060
[epoch14, step671]: loss 0.400996
[epoch14, step672]: loss 0.491499
[epoch14, step673]: loss 0.491674
[epoch14, step674]: loss 0.630872
[epoch14, step675]: loss 0.565199
[epoch14, step676]: loss 0.648654
[epoch14, step677]: loss 0.363038
[epoch14, step678]: loss 0.519855
[epoch14, step679]: loss 0.649774
[epoch14, step680]: loss 0.420310
[epoch14, step681]: loss 0.581214
[epoch14, step682]: loss 0.733878
[epoch14, step683]: loss 0.594873
[epoch14, step684]: loss 0.450478
[epoch14, step685]: loss 0.456199
[epoch14, step686]: loss 0.589924
[epoch14, step687]: loss 0.377659
[epoch14, step688]: loss 0.444404
[epoch14, step689]: loss 0.597028
[epoch14, step690]: loss 0.395008
[epoch14, step691]: loss 0.404530
[epoch14, step692]: loss 0.261510
[epoch14, step693]: loss 0.546415
[epoch14, step694]: loss 0.558162
[epoch14, step695]: loss 0.525220
[epoch14, step696]: loss 0.320362
[epoch14, step697]: loss 0.339809
[epoch14, step698]: loss 0.704454
[epoch14, step699]: loss 0.496140
[epoch14, step700]: loss 0.339524
[epoch14, step701]: loss 0.428532
[epoch14, step702]: loss 0.369511
[epoch14, step703]: loss 0.416096
[epoch14, step704]: loss 0.518065
[epoch14, step705]: loss 0.514024
[epoch14, step706]: loss 0.299596
[epoch14, step707]: loss 0.511399
[epoch14, step708]: loss 0.566057
[epoch14, step709]: loss 0.599951
[epoch14, step710]: loss 0.257046
[epoch14, step711]: loss 0.549858
[epoch14, step712]: loss 0.319690
[epoch14, step713]: loss 0.351350
[epoch14, step714]: loss 0.448382
[epoch14, step715]: loss 0.534943
[epoch14, step716]: loss 0.475946
[epoch14, step717]: loss 0.506354
[epoch14, step718]: loss 0.616037
[epoch14, step719]: loss 0.417670
[epoch14, step720]: loss 0.525677
[epoch14, step721]: loss 0.332936
[epoch14, step722]: loss 0.354871
[epoch14, step723]: loss 0.428446
[epoch14, step724]: loss 0.499300
[epoch14, step725]: loss 0.331207
[epoch14, step726]: loss 0.444965
[epoch14, step727]: loss 0.208554
[epoch14, step728]: loss 0.260783
[epoch14, step729]: loss 0.475289
[epoch14, step730]: loss 0.456661
[epoch14, step731]: loss 0.505176
[epoch14, step732]: loss 0.393177
[epoch14, step733]: loss 0.793220
[epoch14, step734]: loss 0.458612
[epoch14, step735]: loss 0.665737
[epoch14, step736]: loss 0.398524
[epoch14, step737]: loss 0.588941
[epoch14, step738]: loss 0.647308
[epoch14, step739]: loss 0.320081
[epoch14, step740]: loss 0.476612
[epoch14, step741]: loss 0.596674
[epoch14, step742]: loss 0.611712
[epoch14, step743]: loss 0.655308
[epoch14, step744]: loss 0.590950
[epoch14, step745]: loss 0.310473
[epoch14, step746]: loss 0.525160
[epoch14, step747]: loss 0.579926
[epoch14, step748]: loss 0.560186
[epoch14, step749]: loss 0.507324
[epoch14, step750]: loss 0.516254
[epoch14, step751]: loss 0.430010
[epoch14, step752]: loss 0.383795
[epoch14, step753]: loss 0.654304
[epoch14, step754]: loss 0.275805
[epoch14, step755]: loss 0.360198
[epoch14, step756]: loss 0.381069
[epoch14, step757]: loss 0.299366
[epoch14, step758]: loss 0.588801
[epoch14, step759]: loss 0.356419
[epoch14, step760]: loss 0.602318
[epoch14, step761]: loss 0.343010
[epoch14, step762]: loss 0.408092
[epoch14, step763]: loss 0.478626
[epoch14, step764]: loss 0.402844
[epoch14, step765]: loss 0.387676
[epoch14, step766]: loss 0.288005
[epoch14, step767]: loss 0.447429
[epoch14, step768]: loss 0.335070
[epoch14, step769]: loss 0.414473
[epoch14, step770]: loss 0.514533
[epoch14, step771]: loss 0.376208
[epoch14, step772]: loss 0.427166
[epoch14, step773]: loss 0.440805
[epoch14, step774]: loss 0.259282
[epoch14, step775]: loss 0.486963
[epoch14, step776]: loss 0.639232
[epoch14, step777]: loss 0.338785
[epoch14, step778]: loss 0.353899
[epoch14, step779]: loss 0.348979
[epoch14, step780]: loss 0.747158
[epoch14, step781]: loss 0.664744
[epoch14, step782]: loss 0.627680
[epoch14, step783]: loss 0.459202
[epoch14, step784]: loss 0.417825
[epoch14, step785]: loss 0.676659
[epoch14, step786]: loss 0.659279
[epoch14, step787]: loss 0.353883
[epoch14, step788]: loss 0.463427
[epoch14, step789]: loss 0.414192
[epoch14, step790]: loss 0.393983
[epoch14, step791]: loss 0.456919
[epoch14, step792]: loss 0.552131
[epoch14, step793]: loss 0.531938
[epoch14, step794]: loss 0.639849
[epoch14, step795]: loss 0.532481
[epoch14, step796]: loss 0.484311
[epoch14, step797]: loss 0.502323
[epoch14, step798]: loss 0.549720
[epoch14, step799]: loss 0.464434
[epoch14, step800]: loss 0.461009
[epoch14, step801]: loss 0.498559
[epoch14, step802]: loss 0.322371
[epoch14, step803]: loss 0.361398
[epoch14, step804]: loss 0.434180
[epoch14, step805]: loss 0.477126
[epoch14, step806]: loss 0.653822
[epoch14, step807]: loss 0.246188
[epoch14, step808]: loss 0.378778
[epoch14, step809]: loss 0.585412
[epoch14, step810]: loss 0.748846
[epoch14, step811]: loss 0.464867
[epoch14, step812]: loss 0.424283
[epoch14, step813]: loss 0.279494
[epoch14, step814]: loss 0.513365
[epoch14, step815]: loss 0.550385
[epoch14, step816]: loss 0.679534
[epoch14, step817]: loss 0.346239
[epoch14, step818]: loss 0.439254
[epoch14, step819]: loss 0.527172
[epoch14, step820]: loss 0.500599
[epoch14, step821]: loss 0.397687
[epoch14, step822]: loss 0.256556
[epoch14, step823]: loss 0.280259
[epoch14, step824]: loss 0.354396
[epoch14, step825]: loss 0.507973
[epoch14, step826]: loss 0.660459
[epoch14, step827]: loss 0.387172
[epoch14, step828]: loss 0.470609
[epoch14, step829]: loss 0.599599
[epoch14, step830]: loss 0.409001
[epoch14, step831]: loss 0.446628
[epoch14, step832]: loss 0.461876
[epoch14, step833]: loss 0.377525
[epoch14, step834]: loss 0.442638
[epoch14, step835]: loss 0.580552
[epoch14, step836]: loss 0.324857
[epoch14, step837]: loss 0.505725
[epoch14, step838]: loss 0.480871
[epoch14, step839]: loss 0.332543
[epoch14, step840]: loss 0.302778
[epoch14, step841]: loss 0.640493
[epoch14, step842]: loss 0.574096
[epoch14, step843]: loss 0.326187
[epoch14, step844]: loss 0.366053
[epoch14, step845]: loss 0.448532
[epoch14, step846]: loss 0.652821
[epoch14, step847]: loss 0.534334
[epoch14, step848]: loss 0.569467
[epoch14, step849]: loss 0.544102
[epoch14, step850]: loss 0.299634
[epoch14, step851]: loss 0.392087
[epoch14, step852]: loss 0.479410
[epoch14, step853]: loss 0.502438
[epoch14, step854]: loss 0.499598
[epoch14, step855]: loss 0.515907
[epoch14, step856]: loss 0.408136
[epoch14, step857]: loss 0.612492
[epoch14, step858]: loss 0.535611
[epoch14, step859]: loss 0.462034
[epoch14, step860]: loss 0.704056
[epoch14, step861]: loss 0.506060
[epoch14, step862]: loss 0.585842
[epoch14, step863]: loss 0.669142
[epoch14, step864]: loss 0.544234
[epoch14, step865]: loss 0.648708
[epoch14, step866]: loss 0.648349
[epoch14, step867]: loss 0.541067
[epoch14, step868]: loss 0.488538
[epoch14, step869]: loss 0.382203
[epoch14, step870]: loss 0.518484
[epoch14, step871]: loss 0.354145
[epoch14, step872]: loss 0.393647
[epoch14, step873]: loss 0.610765
[epoch14, step874]: loss 0.365404
[epoch14, step875]: loss 0.509080
[epoch14, step876]: loss 0.678419
[epoch14, step877]: loss 0.451545
[epoch14, step878]: loss 0.338440
[epoch14, step879]: loss 0.632458
[epoch14, step880]: loss 0.100635
[epoch14, step881]: loss 0.500480
[epoch14, step882]: loss 0.507813
[epoch14, step883]: loss 0.491231
[epoch14, step884]: loss 0.247493
[epoch14, step885]: loss 0.291976
[epoch14, step886]: loss 0.649413
[epoch14, step887]: loss 0.384866
[epoch14, step888]: loss 0.664457
[epoch14, step889]: loss 0.386409
[epoch14, step890]: loss 0.312874
[epoch14, step891]: loss 0.498917
[epoch14, step892]: loss 0.526323
[epoch14, step893]: loss 0.514606
[epoch14, step894]: loss 0.454964
[epoch14, step895]: loss 0.439025
[epoch14, step896]: loss 0.202359
[epoch14, step897]: loss 0.537365
[epoch14, step898]: loss 0.173979
[epoch14, step899]: loss 0.533671
[epoch14, step900]: loss 0.671322
[epoch14, step901]: loss 0.542213
[epoch14, step902]: loss 0.541267
[epoch14, step903]: loss 0.719517
[epoch14, step904]: loss 0.325073
[epoch14, step905]: loss 0.324493
[epoch14, step906]: loss 0.657866
[epoch14, step907]: loss 0.545351
[epoch14, step908]: loss 0.586755
[epoch14, step909]: loss 0.768634
[epoch14, step910]: loss 0.509115
[epoch14, step911]: loss 0.424546
[epoch14, step912]: loss 0.624205
[epoch14, step913]: loss 0.528375
[epoch14, step914]: loss 0.668821
[epoch14, step915]: loss 0.625774
[epoch14, step916]: loss 0.357501
[epoch14, step917]: loss 0.108855
[epoch14, step918]: loss 0.702987
[epoch14, step919]: loss 0.437099
[epoch14, step920]: loss 0.561006
[epoch14, step921]: loss 0.532005
[epoch14, step922]: loss 0.489703
[epoch14, step923]: loss 0.509492
[epoch14, step924]: loss 0.467997
[epoch14, step925]: loss 0.480525
[epoch14, step926]: loss 0.699717
[epoch14, step927]: loss 0.469871
[epoch14, step928]: loss 0.505982
[epoch14, step929]: loss 0.608442
[epoch14, step930]: loss 0.446533
[epoch14, step931]: loss 0.517371
[epoch14, step932]: loss 0.460481
[epoch14, step933]: loss 0.407234
[epoch14, step934]: loss 0.329874
[epoch14, step935]: loss 0.476454
[epoch14, step936]: loss 0.433147
[epoch14, step937]: loss 0.506694
[epoch14, step938]: loss 0.388779
[epoch14, step939]: loss 0.583219
[epoch14, step940]: loss 0.490405
[epoch14, step941]: loss 0.596997
[epoch14, step942]: loss 0.416309
[epoch14, step943]: loss 0.651617
[epoch14, step944]: loss 0.369818
[epoch14, step945]: loss 0.588329
[epoch14, step946]: loss 0.340758
[epoch14, step947]: loss 0.382011
[epoch14, step948]: loss 0.539063
[epoch14, step949]: loss 0.519072
[epoch14, step950]: loss 0.331021
[epoch14, step951]: loss 0.534104
[epoch14, step952]: loss 0.406572
[epoch14, step953]: loss 0.483884
[epoch14, step954]: loss 0.684631
[epoch14, step955]: loss 0.503016
[epoch14, step956]: loss 0.557573
[epoch14, step957]: loss 0.473070
[epoch14, step958]: loss 0.531891
[epoch14, step959]: loss 0.328298
[epoch14, step960]: loss 0.599331
[epoch14, step961]: loss 0.124430
[epoch14, step962]: loss 0.212597
[epoch14, step963]: loss 0.499304
[epoch14, step964]: loss 0.726852
[epoch14, step965]: loss 0.527554
[epoch14, step966]: loss 0.447199
[epoch14, step967]: loss 0.596037
[epoch14, step968]: loss 0.463900
[epoch14, step969]: loss 0.594688
[epoch14, step970]: loss 0.578773
[epoch14, step971]: loss 0.222215
[epoch14, step972]: loss 0.377416
[epoch14, step973]: loss 0.376925
[epoch14, step974]: loss 0.531080
[epoch14, step975]: loss 0.587490
[epoch14, step976]: loss 0.330685
[epoch14, step977]: loss 0.394801
[epoch14, step978]: loss 0.332414
[epoch14, step979]: loss 0.407047
[epoch14, step980]: loss 0.449265
[epoch14, step981]: loss 0.511423
[epoch14, step982]: loss 0.745157
[epoch14, step983]: loss 0.371704
[epoch14, step984]: loss 0.446580
[epoch14, step985]: loss 0.224434
[epoch14, step986]: loss 0.570849
[epoch14, step987]: loss 0.419291
[epoch14, step988]: loss 0.537863
[epoch14, step989]: loss 0.691016
[epoch14, step990]: loss 0.295979
[epoch14, step991]: loss 0.558724
[epoch14, step992]: loss 0.554707
[epoch14, step993]: loss 0.360362
[epoch14, step994]: loss 0.389679
[epoch14, step995]: loss 0.688553
[epoch14, step996]: loss 0.439553
[epoch14, step997]: loss 0.469390
[epoch14, step998]: loss 0.598564
[epoch14, step999]: loss 0.621932
[epoch14, step1000]: loss 0.535012
[epoch14, step1001]: loss 0.411091
[epoch14, step1002]: loss 0.144117
[epoch14, step1003]: loss 0.553023
[epoch14, step1004]: loss 0.647174
[epoch14, step1005]: loss 0.425534
[epoch14, step1006]: loss 0.182847
[epoch14, step1007]: loss 0.440229
[epoch14, step1008]: loss 0.523482
[epoch14, step1009]: loss 0.433199
[epoch14, step1010]: loss 0.405326
[epoch14, step1011]: loss 0.468360
[epoch14, step1012]: loss 0.567780
[epoch14, step1013]: loss 0.566166
[epoch14, step1014]: loss 0.424827
[epoch14, step1015]: loss 0.440401
[epoch14, step1016]: loss 0.527350
[epoch14, step1017]: loss 0.713484
[epoch14, step1018]: loss 0.518281
[epoch14, step1019]: loss 0.478795
[epoch14, step1020]: loss 0.598742
[epoch14, step1021]: loss 0.603239
[epoch14, step1022]: loss 0.567394
[epoch14, step1023]: loss 0.576264
[epoch14, step1024]: loss 0.528225
[epoch14, step1025]: loss 0.713065
[epoch14, step1026]: loss 0.650209
[epoch14, step1027]: loss 0.405605
[epoch14, step1028]: loss 0.453495
[epoch14, step1029]: loss 0.417525
[epoch14, step1030]: loss 0.361323
[epoch14, step1031]: loss 0.582607
[epoch14, step1032]: loss 0.405160
[epoch14, step1033]: loss 0.543720
[epoch14, step1034]: loss 0.621716
[epoch14, step1035]: loss 0.355259
[epoch14, step1036]: loss 0.578344
[epoch14, step1037]: loss 0.421742
[epoch14, step1038]: loss 0.468935
[epoch14, step1039]: loss 0.504894
[epoch14, step1040]: loss 0.392610
[epoch14, step1041]: loss 0.502559
[epoch14, step1042]: loss 0.540046
[epoch14, step1043]: loss 0.446491
[epoch14, step1044]: loss 0.500075
[epoch14, step1045]: loss 0.351102
[epoch14, step1046]: loss 0.563739
[epoch14, step1047]: loss 0.176276
[epoch14, step1048]: loss 0.411766
[epoch14, step1049]: loss 0.545254
[epoch14, step1050]: loss 0.547318
[epoch14, step1051]: loss 0.565572
[epoch14, step1052]: loss 0.389844
[epoch14, step1053]: loss 0.545760
[epoch14, step1054]: loss 0.508931
[epoch14, step1055]: loss 0.590509
[epoch14, step1056]: loss 0.423772
[epoch14, step1057]: loss 0.565793
[epoch14, step1058]: loss 0.524173
[epoch14, step1059]: loss 0.151600
[epoch14, step1060]: loss 0.561254
[epoch14, step1061]: loss 0.287314
[epoch14, step1062]: loss 0.465489
[epoch14, step1063]: loss 0.451499
[epoch14, step1064]: loss 0.551939
[epoch14, step1065]: loss 0.533824
[epoch14, step1066]: loss 0.420779
[epoch14, step1067]: loss 0.464284
[epoch14, step1068]: loss 0.373858
[epoch14, step1069]: loss 0.799188
[epoch14, step1070]: loss 0.547585
[epoch14, step1071]: loss 0.276196
[epoch14, step1072]: loss 0.315733
[epoch14, step1073]: loss 0.531562
[epoch14, step1074]: loss 0.448501
[epoch14, step1075]: loss 0.505104
[epoch14, step1076]: loss 0.711771
[epoch14, step1077]: loss 0.582637
[epoch14, step1078]: loss 0.512375
[epoch14, step1079]: loss 0.436444
[epoch14, step1080]: loss 0.365340
[epoch14, step1081]: loss 0.501295
[epoch14, step1082]: loss 0.271589
[epoch14, step1083]: loss 0.351246
[epoch14, step1084]: loss 0.501081
[epoch14, step1085]: loss 0.479739
[epoch14, step1086]: loss 0.462707
[epoch14, step1087]: loss 0.565686
[epoch14, step1088]: loss 0.428616
[epoch14, step1089]: loss 0.267623
[epoch14, step1090]: loss 0.542782
[epoch14, step1091]: loss 0.592655
[epoch14, step1092]: loss 0.325622
[epoch14, step1093]: loss 0.434254
[epoch14, step1094]: loss 0.654683
[epoch14, step1095]: loss 0.409508
[epoch14, step1096]: loss 0.524922
[epoch14, step1097]: loss 0.550834
[epoch14, step1098]: loss 0.630339
[epoch14, step1099]: loss 0.282550
[epoch14, step1100]: loss 0.542750
[epoch14, step1101]: loss 0.630307
[epoch14, step1102]: loss 0.717729
[epoch14, step1103]: loss 0.493395
[epoch14, step1104]: loss 0.425837
[epoch14, step1105]: loss 0.447993
[epoch14, step1106]: loss 0.487930
[epoch14, step1107]: loss 0.312391
[epoch14, step1108]: loss 0.330048
[epoch14, step1109]: loss 0.472314
[epoch14, step1110]: loss 0.208176
[epoch14, step1111]: loss 0.502762
[epoch14, step1112]: loss 0.536332
[epoch14, step1113]: loss 0.500286
[epoch14, step1114]: loss 0.288804
[epoch14, step1115]: loss 0.526481
[epoch14, step1116]: loss 0.544243
[epoch14, step1117]: loss 0.740456
[epoch14, step1118]: loss 0.505061
[epoch14, step1119]: loss 0.485884
[epoch14, step1120]: loss 0.406555
[epoch14, step1121]: loss 0.433173
[epoch14, step1122]: loss 0.350166
[epoch14, step1123]: loss 0.576291
[epoch14, step1124]: loss 0.410031
[epoch14, step1125]: loss 0.313194
[epoch14, step1126]: loss 0.247500
[epoch14, step1127]: loss 0.445289
[epoch14, step1128]: loss 0.469780
[epoch14, step1129]: loss 0.449962
[epoch14, step1130]: loss 0.424375
[epoch14, step1131]: loss 0.440316
[epoch14, step1132]: loss 0.570918
[epoch14, step1133]: loss 0.592524
[epoch14, step1134]: loss 0.315257
[epoch14, step1135]: loss 0.547386
[epoch14, step1136]: loss 0.255255
[epoch14, step1137]: loss 0.437022
[epoch14, step1138]: loss 0.281996
[epoch14, step1139]: loss 0.318217
[epoch14, step1140]: loss 0.431539
[epoch14, step1141]: loss 0.467256
[epoch14, step1142]: loss 0.526865
[epoch14, step1143]: loss 0.498281
[epoch14, step1144]: loss 0.373363
[epoch14, step1145]: loss 0.295772
[epoch14, step1146]: loss 0.505514
[epoch14, step1147]: loss 0.312870
[epoch14, step1148]: loss 0.540624
[epoch14, step1149]: loss 0.422187
[epoch14, step1150]: loss 0.650092
[epoch14, step1151]: loss 0.309285
[epoch14, step1152]: loss 0.517601
[epoch14, step1153]: loss 0.505234
[epoch14, step1154]: loss 0.628331
[epoch14, step1155]: loss 0.505796
[epoch14, step1156]: loss 0.392012
[epoch14, step1157]: loss 0.716099
[epoch14, step1158]: loss 0.418980
[epoch14, step1159]: loss 0.421319
[epoch14, step1160]: loss 0.523914
[epoch14, step1161]: loss 0.483781
[epoch14, step1162]: loss 0.508647
[epoch14, step1163]: loss 0.298709
[epoch14, step1164]: loss 0.472437
[epoch14, step1165]: loss 0.270803
[epoch14, step1166]: loss 0.293921
[epoch14, step1167]: loss 0.524659
[epoch14, step1168]: loss 0.452526
[epoch14, step1169]: loss 0.487760
[epoch14, step1170]: loss 0.467713
[epoch14, step1171]: loss 0.386072
[epoch14, step1172]: loss 0.377082
[epoch14, step1173]: loss 0.478232
[epoch14, step1174]: loss 0.328785
[epoch14, step1175]: loss 0.629418
[epoch14, step1176]: loss 0.450835
[epoch14, step1177]: loss 0.462153
[epoch14, step1178]: loss 0.348502
[epoch14, step1179]: loss 0.501632
[epoch14, step1180]: loss 0.628289
[epoch14, step1181]: loss 0.525900
[epoch14, step1182]: loss 0.411771
[epoch14, step1183]: loss 0.718064
[epoch14, step1184]: loss 0.560644
[epoch14, step1185]: loss 0.606615
[epoch14, step1186]: loss 0.394845
[epoch14, step1187]: loss 0.590712
[epoch14, step1188]: loss 0.539049
[epoch14, step1189]: loss 0.566476
[epoch14, step1190]: loss 0.565810
[epoch14, step1191]: loss 0.450103
[epoch14, step1192]: loss 0.615799
[epoch14, step1193]: loss 0.574912
[epoch14, step1194]: loss 0.478916
[epoch14, step1195]: loss 0.501459
[epoch14, step1196]: loss 0.276473
[epoch14, step1197]: loss 0.595488
[epoch14, step1198]: loss 0.433230
[epoch14, step1199]: loss 0.591851
[epoch14, step1200]: loss 0.221934
[epoch14, step1201]: loss 0.447752
[epoch14, step1202]: loss 0.451815
[epoch14, step1203]: loss 0.581320
[epoch14, step1204]: loss 0.480054
[epoch14, step1205]: loss 0.593895
[epoch14, step1206]: loss 0.391313
[epoch14, step1207]: loss 0.362800
[epoch14, step1208]: loss 0.428004
[epoch14, step1209]: loss 0.327783
[epoch14, step1210]: loss 0.470009
[epoch14, step1211]: loss 0.271563
[epoch14, step1212]: loss 0.608541
[epoch14, step1213]: loss 0.380429
[epoch14, step1214]: loss 0.364697
[epoch14, step1215]: loss 0.347525
[epoch14, step1216]: loss 0.467504
[epoch14, step1217]: loss 0.486773
[epoch14, step1218]: loss 0.440594
[epoch14, step1219]: loss 0.497076
[epoch14, step1220]: loss 0.407373
[epoch14, step1221]: loss 0.310256
[epoch14, step1222]: loss 0.442158
[epoch14, step1223]: loss 0.253083
[epoch14, step1224]: loss 0.589277
[epoch14, step1225]: loss 0.333255
[epoch14, step1226]: loss 0.709362
[epoch14, step1227]: loss 0.292542
[epoch14, step1228]: loss 0.493220
[epoch14, step1229]: loss 0.372136
[epoch14, step1230]: loss 0.525859
[epoch14, step1231]: loss 0.409048
[epoch14, step1232]: loss 0.439895
[epoch14, step1233]: loss 0.437920
[epoch14, step1234]: loss 0.493354
[epoch14, step1235]: loss 0.460595
[epoch14, step1236]: loss 0.299259
[epoch14, step1237]: loss 0.638983
[epoch14, step1238]: loss 0.512522
[epoch14, step1239]: loss 0.329888
[epoch14, step1240]: loss 0.683113
[epoch14, step1241]: loss 0.500629
[epoch14, step1242]: loss 0.340933
[epoch14, step1243]: loss 0.385268
[epoch14, step1244]: loss 0.399958
[epoch14, step1245]: loss 0.492689
[epoch14, step1246]: loss 0.487077
[epoch14, step1247]: loss 0.203451
[epoch14, step1248]: loss 0.377257
[epoch14, step1249]: loss 0.407335
[epoch14, step1250]: loss 0.601969
[epoch14, step1251]: loss 0.482555
[epoch14, step1252]: loss 0.394250
[epoch14, step1253]: loss 0.473109
[epoch14, step1254]: loss 0.529735
[epoch14, step1255]: loss 0.469424
[epoch14, step1256]: loss 0.364270
[epoch14, step1257]: loss 0.520383
[epoch14, step1258]: loss 0.368861
[epoch14, step1259]: loss 0.163466
[epoch14, step1260]: loss 0.451833
[epoch14, step1261]: loss 0.473186
[epoch14, step1262]: loss 0.515814
[epoch14, step1263]: loss 0.631826
[epoch14, step1264]: loss 0.683496
[epoch14, step1265]: loss 0.479137
[epoch14, step1266]: loss 0.665049
[epoch14, step1267]: loss 0.355411
[epoch14, step1268]: loss 0.442280
[epoch14, step1269]: loss 0.283903
[epoch14, step1270]: loss 0.449429
[epoch14, step1271]: loss 0.707332
[epoch14, step1272]: loss 0.302719
[epoch14, step1273]: loss 0.142604
[epoch14, step1274]: loss 0.514325
[epoch14, step1275]: loss 0.462757
[epoch14, step1276]: loss 0.659667
[epoch14, step1277]: loss 0.621210
[epoch14, step1278]: loss 0.598262
[epoch14, step1279]: loss 0.436478
[epoch14, step1280]: loss 0.498700
[epoch14, step1281]: loss 0.560693
[epoch14, step1282]: loss 0.247536
[epoch14, step1283]: loss 0.633477
[epoch14, step1284]: loss 0.606982
[epoch14, step1285]: loss 0.553634
[epoch14, step1286]: loss 0.434130
[epoch14, step1287]: loss 0.486946
[epoch14, step1288]: loss 0.620838
[epoch14, step1289]: loss 0.500692
[epoch14, step1290]: loss 0.688677
[epoch14, step1291]: loss 0.455693
[epoch14, step1292]: loss 0.390448
[epoch14, step1293]: loss 0.628938
[epoch14, step1294]: loss 0.512442
[epoch14, step1295]: loss 0.536745
[epoch14, step1296]: loss 0.257432
[epoch14, step1297]: loss 0.314175
[epoch14, step1298]: loss 0.296046
[epoch14, step1299]: loss 0.636144
[epoch14, step1300]: loss 0.402962
[epoch14, step1301]: loss 0.527322
[epoch14, step1302]: loss 0.436353
[epoch14, step1303]: loss 0.418012
[epoch14, step1304]: loss 0.531373
[epoch14, step1305]: loss 0.451291
[epoch14, step1306]: loss 0.333905
[epoch14, step1307]: loss 0.583992
[epoch14, step1308]: loss 0.178824
[epoch14, step1309]: loss 0.366523
[epoch14, step1310]: loss 0.416090
[epoch14, step1311]: loss 0.283171
[epoch14, step1312]: loss 0.289905
[epoch14, step1313]: loss 0.620001
[epoch14, step1314]: loss 0.436120
[epoch14, step1315]: loss 0.548027
[epoch14, step1316]: loss 0.489990
[epoch14, step1317]: loss 0.540131
[epoch14, step1318]: loss 0.344662
[epoch14, step1319]: loss 0.422670
[epoch14, step1320]: loss 0.572996
[epoch14, step1321]: loss 0.509955
[epoch14, step1322]: loss 0.518155
[epoch14, step1323]: loss 0.227082
[epoch14, step1324]: loss 0.567321
[epoch14, step1325]: loss 0.356319
[epoch14, step1326]: loss 0.541035
[epoch14, step1327]: loss 0.605587
[epoch14, step1328]: loss 0.459691
[epoch14, step1329]: loss 0.558256
[epoch14, step1330]: loss 0.482714
[epoch14, step1331]: loss 0.480444
[epoch14, step1332]: loss 0.455922
[epoch14, step1333]: loss 0.449263
[epoch14, step1334]: loss 0.359132
[epoch14, step1335]: loss 0.578591
[epoch14, step1336]: loss 0.433002
[epoch14, step1337]: loss 0.532160
[epoch14, step1338]: loss 0.758462
[epoch14, step1339]: loss 0.219139
[epoch14, step1340]: loss 0.384351
[epoch14, step1341]: loss 0.354404
[epoch14, step1342]: loss 0.481095
[epoch14, step1343]: loss 0.559596
[epoch14, step1344]: loss 0.513452
[epoch14, step1345]: loss 0.401893
[epoch14, step1346]: loss 0.576285
[epoch14, step1347]: loss 0.407074
[epoch14, step1348]: loss 0.385069
[epoch14, step1349]: loss 0.634476
[epoch14, step1350]: loss 0.463435
[epoch14, step1351]: loss 0.635166
[epoch14, step1352]: loss 0.258996
[epoch14, step1353]: loss 0.366213
[epoch14, step1354]: loss 0.466307
[epoch14, step1355]: loss 0.423630
[epoch14, step1356]: loss 0.509926
[epoch14, step1357]: loss 0.292450
[epoch14, step1358]: loss 0.443792
[epoch14, step1359]: loss 0.559705
[epoch14, step1360]: loss 0.359707
[epoch14, step1361]: loss 0.617601
[epoch14, step1362]: loss 0.766454
[epoch14, step1363]: loss 0.655869
[epoch14, step1364]: loss 0.645921
[epoch14, step1365]: loss 0.540809
[epoch14, step1366]: loss 0.322621
[epoch14, step1367]: loss 0.266462
[epoch14, step1368]: loss 0.416545
[epoch14, step1369]: loss 0.585584
[epoch14, step1370]: loss 0.319425
[epoch14, step1371]: loss 0.358880
[epoch14, step1372]: loss 0.731729
[epoch14, step1373]: loss 0.469359
[epoch14, step1374]: loss 0.397573
[epoch14, step1375]: loss 0.410662
[epoch14, step1376]: loss 0.530652
[epoch14, step1377]: loss 0.690419
[epoch14, step1378]: loss 0.506069
[epoch14, step1379]: loss 0.576778
[epoch14, step1380]: loss 0.456789
[epoch14, step1381]: loss 0.473798
[epoch14, step1382]: loss 0.604072
[epoch14, step1383]: loss 0.473957
[epoch14, step1384]: loss 0.689221
[epoch14, step1385]: loss 0.556613
[epoch14, step1386]: loss 0.372841
[epoch14, step1387]: loss 0.595751
[epoch14, step1388]: loss 0.633139
[epoch14, step1389]: loss 0.569959
[epoch14, step1390]: loss 0.601566
[epoch14, step1391]: loss 0.569282
[epoch14, step1392]: loss 0.615279
[epoch14, step1393]: loss 0.487191
[epoch14, step1394]: loss 0.376136
[epoch14, step1395]: loss 0.547392
[epoch14, step1396]: loss 0.465933
[epoch14, step1397]: loss 0.543581
[epoch14, step1398]: loss 0.534196
[epoch14, step1399]: loss 0.512513
[epoch14, step1400]: loss 0.387693
[epoch14, step1401]: loss 0.594493
[epoch14, step1402]: loss 0.361079
[epoch14, step1403]: loss 0.640633
[epoch14, step1404]: loss 0.659647
[epoch14, step1405]: loss 0.394609
[epoch14, step1406]: loss 0.564648
[epoch14, step1407]: loss 0.454054
[epoch14, step1408]: loss 0.551997
[epoch14, step1409]: loss 0.500515
[epoch14, step1410]: loss 0.573552
[epoch14, step1411]: loss 0.618770
[epoch14, step1412]: loss 0.482813
[epoch14, step1413]: loss 0.346695
[epoch14, step1414]: loss 0.389079
[epoch14, step1415]: loss 0.720848
[epoch14, step1416]: loss 0.480969
[epoch14, step1417]: loss 0.640398
[epoch14, step1418]: loss 0.465451
[epoch14, step1419]: loss 0.592909
[epoch14, step1420]: loss 0.643561
[epoch14, step1421]: loss 0.582929
[epoch14, step1422]: loss 0.249940
[epoch14, step1423]: loss 0.641768
[epoch14, step1424]: loss 0.537315
[epoch14, step1425]: loss 0.612073
[epoch14, step1426]: loss 0.496022
[epoch14, step1427]: loss 0.508030
[epoch14, step1428]: loss 0.440085
[epoch14, step1429]: loss 0.640907
[epoch14, step1430]: loss 0.659724
[epoch14, step1431]: loss 0.528098
[epoch14, step1432]: loss 0.468253
[epoch14, step1433]: loss 0.256847
[epoch14, step1434]: loss 0.621972
[epoch14, step1435]: loss 0.467662
[epoch14, step1436]: loss 0.116359
[epoch14, step1437]: loss 0.640953
[epoch14, step1438]: loss 0.571004
[epoch14, step1439]: loss 0.690853
[epoch14, step1440]: loss 0.527249
[epoch14, step1441]: loss 0.710593
[epoch14, step1442]: loss 0.633300
[epoch14, step1443]: loss 0.553461
[epoch14, step1444]: loss 0.442915
[epoch14, step1445]: loss 0.477955
[epoch14, step1446]: loss 0.431482
[epoch14, step1447]: loss 0.490451
[epoch14, step1448]: loss 0.464170
[epoch14, step1449]: loss 0.191188
[epoch14, step1450]: loss 0.465914
[epoch14, step1451]: loss 0.505388
[epoch14, step1452]: loss 0.405417
[epoch14, step1453]: loss 0.261072
[epoch14, step1454]: loss 0.632554
[epoch14, step1455]: loss 0.497226
[epoch14, step1456]: loss 0.496171
[epoch14, step1457]: loss 0.264001
[epoch14, step1458]: loss 0.454504
[epoch14, step1459]: loss 0.554025
[epoch14, step1460]: loss 0.409077
[epoch14, step1461]: loss 0.593137
[epoch14, step1462]: loss 0.401222
[epoch14, step1463]: loss 0.548667
[epoch14, step1464]: loss 0.562689
[epoch14, step1465]: loss 0.630054
[epoch14, step1466]: loss 0.428652
[epoch14, step1467]: loss 0.601969
[epoch14, step1468]: loss 0.539555
[epoch14, step1469]: loss 0.682227
[epoch14, step1470]: loss 0.549966
[epoch14, step1471]: loss 0.449434
[epoch14, step1472]: loss 0.621755
[epoch14, step1473]: loss 0.598600
[epoch14, step1474]: loss 0.619433
[epoch14, step1475]: loss 0.227895
[epoch14, step1476]: loss 0.530129
[epoch14, step1477]: loss 0.444373
[epoch14, step1478]: loss 0.447418
[epoch14, step1479]: loss 0.506751
[epoch14, step1480]: loss 0.446235
[epoch14, step1481]: loss 0.425086
[epoch14, step1482]: loss 0.597108
[epoch14, step1483]: loss 0.393820
[epoch14, step1484]: loss 0.688061
[epoch14, step1485]: loss 0.130255
[epoch14, step1486]: loss 0.597416
[epoch14, step1487]: loss 0.552919
[epoch14, step1488]: loss 0.376605
[epoch14, step1489]: loss 0.446603
[epoch14, step1490]: loss 0.434670
[epoch14, step1491]: loss 0.479580
[epoch14, step1492]: loss 0.489033
[epoch14, step1493]: loss 0.380200
[epoch14, step1494]: loss 0.362829
[epoch14, step1495]: loss 0.568396
[epoch14, step1496]: loss 0.374094
[epoch14, step1497]: loss 0.430539
[epoch14, step1498]: loss 0.434757
[epoch14, step1499]: loss 0.466879
[epoch14, step1500]: loss 0.637656
[epoch14, step1501]: loss 0.416216
[epoch14, step1502]: loss 0.425627
[epoch14, step1503]: loss 0.599932
[epoch14, step1504]: loss 0.452610
[epoch14, step1505]: loss 0.615309
[epoch14, step1506]: loss 0.505025
[epoch14, step1507]: loss 0.419221
[epoch14, step1508]: loss 0.640664
[epoch14, step1509]: loss 0.644059
[epoch14, step1510]: loss 0.329093
[epoch14, step1511]: loss 0.378696
[epoch14, step1512]: loss 0.495999
[epoch14, step1513]: loss 0.689581
[epoch14, step1514]: loss 0.360629
[epoch14, step1515]: loss 0.608121
[epoch14, step1516]: loss 0.549993
[epoch14, step1517]: loss 0.345842
[epoch14, step1518]: loss 0.431079
[epoch14, step1519]: loss 0.584379
[epoch14, step1520]: loss 0.496914
[epoch14, step1521]: loss 0.632135
[epoch14, step1522]: loss 0.636650
[epoch14, step1523]: loss 0.536041
[epoch14, step1524]: loss 0.550061
[epoch14, step1525]: loss 0.651882
[epoch14, step1526]: loss 0.130221
[epoch14, step1527]: loss 0.438366
[epoch14, step1528]: loss 0.449144
[epoch14, step1529]: loss 0.433976
[epoch14, step1530]: loss 0.708743
[epoch14, step1531]: loss 0.552415
[epoch14, step1532]: loss 0.415781
[epoch14, step1533]: loss 0.455890
[epoch14, step1534]: loss 0.477311
[epoch14, step1535]: loss 0.477751
[epoch14, step1536]: loss 0.646164
[epoch14, step1537]: loss 0.667256
[epoch14, step1538]: loss 0.592759
[epoch14, step1539]: loss 0.268774
[epoch14, step1540]: loss 0.299688
[epoch14, step1541]: loss 0.674803
[epoch14, step1542]: loss 0.630589
[epoch14, step1543]: loss 0.575600
[epoch14, step1544]: loss 0.684824
[epoch14, step1545]: loss 0.186678
[epoch14, step1546]: loss 0.760689
[epoch14, step1547]: loss 0.638142
[epoch14, step1548]: loss 0.450348
[epoch14, step1549]: loss 0.484938
[epoch14, step1550]: loss 0.282142
[epoch14, step1551]: loss 0.470513
[epoch14, step1552]: loss 0.559425
[epoch14, step1553]: loss 0.354855
[epoch14, step1554]: loss 0.541266
[epoch14, step1555]: loss 0.655899
[epoch14, step1556]: loss 0.638095
[epoch14, step1557]: loss 0.709247
[epoch14, step1558]: loss 0.411507
[epoch14, step1559]: loss 0.289786
[epoch14, step1560]: loss 0.540960
[epoch14, step1561]: loss 0.424894
[epoch14, step1562]: loss 0.377653
[epoch14, step1563]: loss 0.316151
[epoch14, step1564]: loss 0.524944
[epoch14, step1565]: loss 0.648672
[epoch14, step1566]: loss 0.390133
[epoch14, step1567]: loss 0.344087
[epoch14, step1568]: loss 0.545085
[epoch14, step1569]: loss 0.403179
[epoch14, step1570]: loss 0.456784
[epoch14, step1571]: loss 0.602668
[epoch14, step1572]: loss 0.359986
[epoch14, step1573]: loss 0.494645
[epoch14, step1574]: loss 0.400481
[epoch14, step1575]: loss 0.484432
[epoch14, step1576]: loss 0.568007
[epoch14, step1577]: loss 0.499914
[epoch14, step1578]: loss 0.546595
[epoch14, step1579]: loss 0.581262
[epoch14, step1580]: loss 0.647992
[epoch14, step1581]: loss 0.686747
[epoch14, step1582]: loss 0.431669
[epoch14, step1583]: loss 0.313706
[epoch14, step1584]: loss 0.685561
[epoch14, step1585]: loss 0.634605
[epoch14, step1586]: loss 0.561644
[epoch14, step1587]: loss 0.544572
[epoch14, step1588]: loss 0.426228
[epoch14, step1589]: loss 0.274242
[epoch14, step1590]: loss 0.579826
[epoch14, step1591]: loss 0.222040
[epoch14, step1592]: loss 0.444545
[epoch14, step1593]: loss 0.468448
[epoch14, step1594]: loss 0.599657
[epoch14, step1595]: loss 0.394681
[epoch14, step1596]: loss 0.500811
[epoch14, step1597]: loss 0.555333
[epoch14, step1598]: loss 0.438294
[epoch14, step1599]: loss 0.488129
[epoch14, step1600]: loss 0.668607
[epoch14, step1601]: loss 0.466904
[epoch14, step1602]: loss 0.480825
[epoch14, step1603]: loss 0.476206
[epoch14, step1604]: loss 0.385857
[epoch14, step1605]: loss 0.277962
[epoch14, step1606]: loss 0.580952
[epoch14, step1607]: loss 0.537016
[epoch14, step1608]: loss 0.507564
[epoch14, step1609]: loss 0.684730
[epoch14, step1610]: loss 0.600892
[epoch14, step1611]: loss 0.480636
[epoch14, step1612]: loss 0.448418
[epoch14, step1613]: loss 0.235039
[epoch14, step1614]: loss 0.270983
[epoch14, step1615]: loss 0.608695
[epoch14, step1616]: loss 0.435851
[epoch14, step1617]: loss 0.347788
[epoch14, step1618]: loss 0.250070
[epoch14, step1619]: loss 0.177391
[epoch14, step1620]: loss 0.616788
[epoch14, step1621]: loss 0.312543
[epoch14, step1622]: loss 0.698747
[epoch14, step1623]: loss 0.567391
[epoch14, step1624]: loss 0.413739
[epoch14, step1625]: loss 0.468073
[epoch14, step1626]: loss 0.708041
[epoch14, step1627]: loss 0.529720
[epoch14, step1628]: loss 0.374330
[epoch14, step1629]: loss 0.383378
[epoch14, step1630]: loss 0.345457
[epoch14, step1631]: loss 0.534141
[epoch14, step1632]: loss 0.390013
[epoch14, step1633]: loss 0.580458
[epoch14, step1634]: loss 0.449252
[epoch14, step1635]: loss 0.390369
[epoch14, step1636]: loss 0.343709
[epoch14, step1637]: loss 0.348550
[epoch14, step1638]: loss 0.454982
[epoch14, step1639]: loss 0.599097
[epoch14, step1640]: loss 0.599312
[epoch14, step1641]: loss 0.264216
[epoch14, step1642]: loss 0.564478
[epoch14, step1643]: loss 0.547551
[epoch14, step1644]: loss 0.653548
[epoch14, step1645]: loss 0.603310
[epoch14, step1646]: loss 0.792493
[epoch14, step1647]: loss 0.443020
[epoch14, step1648]: loss 0.438140
[epoch14, step1649]: loss 0.577489
[epoch14, step1650]: loss 0.466563
[epoch14, step1651]: loss 0.720104
[epoch14, step1652]: loss 0.536141
[epoch14, step1653]: loss 0.584190
[epoch14, step1654]: loss 0.618779
[epoch14, step1655]: loss 0.573514
[epoch14, step1656]: loss 0.416718
[epoch14, step1657]: loss 0.301288
[epoch14, step1658]: loss 0.533417
[epoch14, step1659]: loss 0.450598
[epoch14, step1660]: loss 0.452213
[epoch14, step1661]: loss 0.366400
[epoch14, step1662]: loss 0.384896
[epoch14, step1663]: loss 0.518209
[epoch14, step1664]: loss 0.306444
[epoch14, step1665]: loss 0.426332
[epoch14, step1666]: loss 0.604223
[epoch14, step1667]: loss 0.578593
[epoch14, step1668]: loss 0.569549
[epoch14, step1669]: loss 0.582634
[epoch14, step1670]: loss 0.534414
[epoch14, step1671]: loss 0.310170
[epoch14, step1672]: loss 0.711698
[epoch14, step1673]: loss 0.634070
[epoch14, step1674]: loss 0.454215
[epoch14, step1675]: loss 0.200300
[epoch14, step1676]: loss 0.516326
[epoch14, step1677]: loss 0.133719
[epoch14, step1678]: loss 0.505099
[epoch14, step1679]: loss 0.568961
[epoch14, step1680]: loss 0.547850
[epoch14, step1681]: loss 0.555835
[epoch14, step1682]: loss 0.696911
[epoch14, step1683]: loss 0.307201
[epoch14, step1684]: loss 0.456414
[epoch14, step1685]: loss 0.632876
[epoch14, step1686]: loss 0.659273
[epoch14, step1687]: loss 0.338013
[epoch14, step1688]: loss 0.323600
[epoch14, step1689]: loss 0.460838
[epoch14, step1690]: loss 0.296707
[epoch14, step1691]: loss 0.487875
[epoch14, step1692]: loss 0.439345
[epoch14, step1693]: loss 0.383151
[epoch14, step1694]: loss 0.408501
[epoch14, step1695]: loss 0.473739
[epoch14, step1696]: loss 0.598688
[epoch14, step1697]: loss 0.606536
[epoch14, step1698]: loss 0.580982
[epoch14, step1699]: loss 0.480400
[epoch14, step1700]: loss 0.420039
[epoch14, step1701]: loss 0.661514
[epoch14, step1702]: loss 0.395860
[epoch14, step1703]: loss 0.406019
[epoch14, step1704]: loss 0.333407
[epoch14, step1705]: loss 0.622408
[epoch14, step1706]: loss 0.272023
[epoch14, step1707]: loss 0.524288
[epoch14, step1708]: loss 0.358971
[epoch14, step1709]: loss 0.540775
[epoch14, step1710]: loss 0.550893
[epoch14, step1711]: loss 0.471458
[epoch14, step1712]: loss 0.454465
[epoch14, step1713]: loss 0.267514
[epoch14, step1714]: loss 0.643541
[epoch14, step1715]: loss 0.452303
[epoch14, step1716]: loss 0.683718
[epoch14, step1717]: loss 0.470050
[epoch14, step1718]: loss 0.553570
[epoch14, step1719]: loss 0.478423
[epoch14, step1720]: loss 0.616169
[epoch14, step1721]: loss 0.404295
[epoch14, step1722]: loss 0.640270
[epoch14, step1723]: loss 0.482905
[epoch14, step1724]: loss 0.510009
[epoch14, step1725]: loss 0.395749
[epoch14, step1726]: loss 0.544384
[epoch14, step1727]: loss 0.507959
[epoch14, step1728]: loss 0.568697
[epoch14, step1729]: loss 0.428145
[epoch14, step1730]: loss 0.346309
[epoch14, step1731]: loss 0.438928
[epoch14, step1732]: loss 0.358421
[epoch14, step1733]: loss 0.625070
[epoch14, step1734]: loss 0.434570
[epoch14, step1735]: loss 0.106548
[epoch14, step1736]: loss 0.457537
[epoch14, step1737]: loss 0.334946
[epoch14, step1738]: loss 0.697448
[epoch14, step1739]: loss 0.563942
[epoch14, step1740]: loss 0.625113
[epoch14, step1741]: loss 0.505201
[epoch14, step1742]: loss 0.408716
[epoch14, step1743]: loss 0.479264
[epoch14, step1744]: loss 0.476937
[epoch14, step1745]: loss 0.393839
[epoch14, step1746]: loss 0.584174
[epoch14, step1747]: loss 0.328560
[epoch14, step1748]: loss 0.425326
[epoch14, step1749]: loss 0.644790
[epoch14, step1750]: loss 0.479358
[epoch14, step1751]: loss 0.510644
[epoch14, step1752]: loss 0.679154
[epoch14, step1753]: loss 0.466583
[epoch14, step1754]: loss 0.351921
[epoch14, step1755]: loss 0.366722
[epoch14, step1756]: loss 0.685014
[epoch14, step1757]: loss 0.404869
[epoch14, step1758]: loss 0.367110
[epoch14, step1759]: loss 0.260331
[epoch14, step1760]: loss 0.438216
[epoch14, step1761]: loss 0.420701
[epoch14, step1762]: loss 0.533404
[epoch14, step1763]: loss 0.591134
[epoch14, step1764]: loss 0.741278
[epoch14, step1765]: loss 0.494627
[epoch14, step1766]: loss 0.703910
[epoch14, step1767]: loss 0.315612
[epoch14, step1768]: loss 0.507094
[epoch14, step1769]: loss 0.507693
[epoch14, step1770]: loss 0.539479
[epoch14, step1771]: loss 0.574484
[epoch14, step1772]: loss 0.383193
[epoch14, step1773]: loss 0.335871
[epoch14, step1774]: loss 0.241873
[epoch14, step1775]: loss 0.490701
[epoch14, step1776]: loss 0.315891
[epoch14, step1777]: loss 0.610204
[epoch14, step1778]: loss 0.608074
[epoch14, step1779]: loss 0.258394
[epoch14, step1780]: loss 0.570780
[epoch14, step1781]: loss 0.411974
[epoch14, step1782]: loss 0.566419
[epoch14, step1783]: loss 0.693746
[epoch14, step1784]: loss 0.555178
[epoch14, step1785]: loss 0.495275
[epoch14, step1786]: loss 0.516337
[epoch14, step1787]: loss 0.635594
[epoch14, step1788]: loss 0.459435
[epoch14, step1789]: loss 0.589980
[epoch14, step1790]: loss 0.426291
[epoch14, step1791]: loss 0.386721
[epoch14, step1792]: loss 0.600092
[epoch14, step1793]: loss 0.390340
[epoch14, step1794]: loss 0.514188
[epoch14, step1795]: loss 0.663702
[epoch14, step1796]: loss 0.525269
[epoch14, step1797]: loss 0.448028
[epoch14, step1798]: loss 0.542640
[epoch14, step1799]: loss 0.503702
[epoch14, step1800]: loss 0.362942
[epoch14, step1801]: loss 0.548577
[epoch14, step1802]: loss 0.684919
[epoch14, step1803]: loss 0.352374
[epoch14, step1804]: loss 0.585232
[epoch14, step1805]: loss 0.418432
[epoch14, step1806]: loss 0.559143
[epoch14, step1807]: loss 0.575388
[epoch14, step1808]: loss 0.610944
[epoch14, step1809]: loss 0.710512
[epoch14, step1810]: loss 0.507893
[epoch14, step1811]: loss 0.636769
[epoch14, step1812]: loss 0.514181
[epoch14, step1813]: loss 0.622301
[epoch14, step1814]: loss 0.512658
[epoch14, step1815]: loss 0.345624
[epoch14, step1816]: loss 0.528561
[epoch14, step1817]: loss 0.252230
[epoch14, step1818]: loss 0.627983
[epoch14, step1819]: loss 0.438442
[epoch14, step1820]: loss 0.240383
[epoch14, step1821]: loss 0.571343
[epoch14, step1822]: loss 0.675709
[epoch14, step1823]: loss 0.377376
[epoch14, step1824]: loss 0.362457
[epoch14, step1825]: loss 0.604802
[epoch14, step1826]: loss 0.431427
[epoch14, step1827]: loss 0.393299
[epoch14, step1828]: loss 0.538125
[epoch14, step1829]: loss 0.376167
[epoch14, step1830]: loss 0.515083
[epoch14, step1831]: loss 0.628454
[epoch14, step1832]: loss 0.423711
[epoch14, step1833]: loss 0.364442
[epoch14, step1834]: loss 0.595229
[epoch14, step1835]: loss 0.519090
[epoch14, step1836]: loss 0.402800
[epoch14, step1837]: loss 0.666969
[epoch14, step1838]: loss 0.414790
[epoch14, step1839]: loss 0.638511
[epoch14, step1840]: loss 0.610494
[epoch14, step1841]: loss 0.663172
[epoch14, step1842]: loss 0.315224
[epoch14, step1843]: loss 0.409555
[epoch14, step1844]: loss 0.417859
[epoch14, step1845]: loss 0.696721
[epoch14, step1846]: loss 0.530381
[epoch14, step1847]: loss 0.380091
[epoch14, step1848]: loss 0.494213
[epoch14, step1849]: loss 0.407070
[epoch14, step1850]: loss 0.478709
[epoch14, step1851]: loss 0.538930
[epoch14, step1852]: loss 0.389829
[epoch14, step1853]: loss 0.423374
[epoch14, step1854]: loss 0.654396
[epoch14, step1855]: loss 0.406262
[epoch14, step1856]: loss 0.364218
[epoch14, step1857]: loss 0.501138
[epoch14, step1858]: loss 0.544131
[epoch14, step1859]: loss 0.581410
[epoch14, step1860]: loss 0.430246
[epoch14, step1861]: loss 0.278455
[epoch14, step1862]: loss 0.518992
[epoch14, step1863]: loss 0.528487
[epoch14, step1864]: loss 0.495411
[epoch14, step1865]: loss 0.621455
[epoch14, step1866]: loss 0.522887
[epoch14, step1867]: loss 0.683836
[epoch14, step1868]: loss 0.374954
[epoch14, step1869]: loss 0.483367
[epoch14, step1870]: loss 0.518862
[epoch14, step1871]: loss 0.389401
[epoch14, step1872]: loss 0.249282
[epoch14, step1873]: loss 0.478091
[epoch14, step1874]: loss 0.469701
[epoch14, step1875]: loss 0.571343
[epoch14, step1876]: loss 0.593887
[epoch14, step1877]: loss 0.475469
[epoch14, step1878]: loss 0.611444
[epoch14, step1879]: loss 0.480707
[epoch14, step1880]: loss 0.501457
[epoch14, step1881]: loss 0.736787
[epoch14, step1882]: loss 0.450863
[epoch14, step1883]: loss 0.538225
[epoch14, step1884]: loss 0.478036
[epoch14, step1885]: loss 0.552086
[epoch14, step1886]: loss 0.358723
[epoch14, step1887]: loss 0.429218
[epoch14, step1888]: loss 0.308823
[epoch14, step1889]: loss 0.387941
[epoch14, step1890]: loss 0.578842
[epoch14, step1891]: loss 0.487363
[epoch14, step1892]: loss 0.487753
[epoch14, step1893]: loss 0.485698
[epoch14, step1894]: loss 0.706422
[epoch14, step1895]: loss 0.484130
[epoch14, step1896]: loss 0.371297
[epoch14, step1897]: loss 0.323021
[epoch14, step1898]: loss 0.544315
[epoch14, step1899]: loss 0.500154
[epoch14, step1900]: loss 0.560317
[epoch14, step1901]: loss 0.538969
[epoch14, step1902]: loss 0.424872
[epoch14, step1903]: loss 0.258006
[epoch14, step1904]: loss 0.498788
[epoch14, step1905]: loss 0.478206
[epoch14, step1906]: loss 0.408315
[epoch14, step1907]: loss 0.615673
[epoch14, step1908]: loss 0.648849
[epoch14, step1909]: loss 0.310171
[epoch14, step1910]: loss 0.444026
[epoch14, step1911]: loss 0.574257
[epoch14, step1912]: loss 0.648256
[epoch14, step1913]: loss 0.291646
[epoch14, step1914]: loss 0.261848
[epoch14, step1915]: loss 0.419743
[epoch14, step1916]: loss 0.584627
[epoch14, step1917]: loss 0.343049
[epoch14, step1918]: loss 0.392370
[epoch14, step1919]: loss 0.266165
[epoch14, step1920]: loss 0.350015
[epoch14, step1921]: loss 0.633224
[epoch14, step1922]: loss 0.369551
[epoch14, step1923]: loss 0.327333
[epoch14, step1924]: loss 0.615160
[epoch14, step1925]: loss 0.521884
[epoch14, step1926]: loss 0.443705
[epoch14, step1927]: loss 0.471617
[epoch14, step1928]: loss 0.287710
[epoch14, step1929]: loss 0.350219
[epoch14, step1930]: loss 0.383761
[epoch14, step1931]: loss 0.642049
[epoch14, step1932]: loss 0.460333
[epoch14, step1933]: loss 0.475933
[epoch14, step1934]: loss 0.401643
[epoch14, step1935]: loss 0.460848
[epoch14, step1936]: loss 0.500306
[epoch14, step1937]: loss 0.389222
[epoch14, step1938]: loss 0.561968
[epoch14, step1939]: loss 0.437659
[epoch14, step1940]: loss 0.567333
[epoch14, step1941]: loss 0.335891
[epoch14, step1942]: loss 0.241003
[epoch14, step1943]: loss 0.656630
[epoch14, step1944]: loss 0.462116
[epoch14, step1945]: loss 0.471102
[epoch14, step1946]: loss 0.459775
[epoch14, step1947]: loss 0.511897
[epoch14, step1948]: loss 0.262697
[epoch14, step1949]: loss 0.451420
[epoch14, step1950]: loss 0.720901
[epoch14, step1951]: loss 0.552270
[epoch14, step1952]: loss 0.233054
[epoch14, step1953]: loss 0.582616
[epoch14, step1954]: loss 0.482144
[epoch14, step1955]: loss 0.514204
[epoch14, step1956]: loss 0.331973
[epoch14, step1957]: loss 0.522063
[epoch14, step1958]: loss 0.574598
[epoch14, step1959]: loss 0.560507
[epoch14, step1960]: loss 0.451901
[epoch14, step1961]: loss 0.513797
[epoch14, step1962]: loss 0.443057
[epoch14, step1963]: loss 0.569323
[epoch14, step1964]: loss 0.095283
[epoch14, step1965]: loss 0.439897
[epoch14, step1966]: loss 0.263271
[epoch14, step1967]: loss 0.580517
[epoch14, step1968]: loss 0.454325
[epoch14, step1969]: loss 0.462038
[epoch14, step1970]: loss 0.541335
[epoch14, step1971]: loss 0.311562
[epoch14, step1972]: loss 0.520556
[epoch14, step1973]: loss 0.556716
[epoch14, step1974]: loss 0.372274
[epoch14, step1975]: loss 0.376952
[epoch14, step1976]: loss 0.460113
[epoch14, step1977]: loss 0.344831
[epoch14, step1978]: loss 0.647094
[epoch14, step1979]: loss 0.488453
[epoch14, step1980]: loss 0.353712
[epoch14, step1981]: loss 0.540682
[epoch14, step1982]: loss 0.647355
[epoch14, step1983]: loss 0.495279
[epoch14, step1984]: loss 0.619193
[epoch14, step1985]: loss 0.488580
[epoch14, step1986]: loss 0.385882
[epoch14, step1987]: loss 0.391853
[epoch14, step1988]: loss 0.153172
[epoch14, step1989]: loss 0.632773
[epoch14, step1990]: loss 0.392368
[epoch14, step1991]: loss 0.457317
[epoch14, step1992]: loss 0.476877
[epoch14, step1993]: loss 0.340501
[epoch14, step1994]: loss 0.278432
[epoch14, step1995]: loss 0.476245
[epoch14, step1996]: loss 0.583417
[epoch14, step1997]: loss 0.470486
[epoch14, step1998]: loss 0.293978
[epoch14, step1999]: loss 0.672364
[epoch14, step2000]: loss 0.622061
[epoch14, step2001]: loss 0.451112
[epoch14, step2002]: loss 0.464529
[epoch14, step2003]: loss 0.504664
[epoch14, step2004]: loss 0.435561
[epoch14, step2005]: loss 0.533130
[epoch14, step2006]: loss 0.323962
[epoch14, step2007]: loss 0.521420
[epoch14, step2008]: loss 0.638718
[epoch14, step2009]: loss 0.481766
[epoch14, step2010]: loss 0.478224
[epoch14, step2011]: loss 0.331442
[epoch14, step2012]: loss 0.548133
[epoch14, step2013]: loss 0.456880
[epoch14, step2014]: loss 0.424615
[epoch14, step2015]: loss 0.455947
[epoch14, step2016]: loss 0.446186
[epoch14, step2017]: loss 0.493317
[epoch14, step2018]: loss 0.212324
[epoch14, step2019]: loss 0.668675
[epoch14, step2020]: loss 0.523141
[epoch14, step2021]: loss 0.344156
[epoch14, step2022]: loss 0.576655
[epoch14, step2023]: loss 0.525689
[epoch14, step2024]: loss 0.365517
[epoch14, step2025]: loss 0.595149
[epoch14, step2026]: loss 0.456998
[epoch14, step2027]: loss 0.536719
[epoch14, step2028]: loss 0.339666
[epoch14, step2029]: loss 0.428327
[epoch14, step2030]: loss 0.241934
[epoch14, step2031]: loss 0.653267
[epoch14, step2032]: loss 0.520706
[epoch14, step2033]: loss 0.477610
[epoch14, step2034]: loss 0.643150
[epoch14, step2035]: loss 0.388818
[epoch14, step2036]: loss 0.346564
[epoch14, step2037]: loss 0.552912
[epoch14, step2038]: loss 0.550505
[epoch14, step2039]: loss 0.483497
[epoch14, step2040]: loss 0.418823
[epoch14, step2041]: loss 0.539258
[epoch14, step2042]: loss 0.279067
[epoch14, step2043]: loss 0.681991
[epoch14, step2044]: loss 0.637074
[epoch14, step2045]: loss 0.542502
[epoch14, step2046]: loss 0.600888
[epoch14, step2047]: loss 0.462215
[epoch14, step2048]: loss 0.369121
[epoch14, step2049]: loss 0.482523
[epoch14, step2050]: loss 0.491835
[epoch14, step2051]: loss 0.596912
[epoch14, step2052]: loss 0.586027
[epoch14, step2053]: loss 0.608764
[epoch14, step2054]: loss 0.481344
[epoch14, step2055]: loss 0.260148
[epoch14, step2056]: loss 0.496044
[epoch14, step2057]: loss 0.404504
[epoch14, step2058]: loss 0.630980
[epoch14, step2059]: loss 0.358149
[epoch14, step2060]: loss 0.595352
[epoch14, step2061]: loss 0.513608
[epoch14, step2062]: loss 0.304410
[epoch14, step2063]: loss 0.537583
[epoch14, step2064]: loss 0.564842
[epoch14, step2065]: loss 0.748724
[epoch14, step2066]: loss 0.405368
[epoch14, step2067]: loss 0.500631
[epoch14, step2068]: loss 0.368630
[epoch14, step2069]: loss 0.467650
[epoch14, step2070]: loss 0.304949
[epoch14, step2071]: loss 0.240181
[epoch14, step2072]: loss 0.551194
[epoch14, step2073]: loss 0.370380
[epoch14, step2074]: loss 0.223013
[epoch14, step2075]: loss 0.382734
[epoch14, step2076]: loss 0.515281
[epoch14, step2077]: loss 0.368092
[epoch14, step2078]: loss 0.535059
[epoch14, step2079]: loss 0.563053
[epoch14, step2080]: loss 0.548077
[epoch14, step2081]: loss 0.499973
[epoch14, step2082]: loss 0.457707
[epoch14, step2083]: loss 0.462868
[epoch14, step2084]: loss 0.518036
[epoch14, step2085]: loss 0.488062
[epoch14, step2086]: loss 0.540087
[epoch14, step2087]: loss 0.603697
[epoch14, step2088]: loss 0.480066
[epoch14, step2089]: loss 0.411852
[epoch14, step2090]: loss 0.662938
[epoch14, step2091]: loss 0.373267
[epoch14, step2092]: loss 0.435880
[epoch14, step2093]: loss 0.636728
[epoch14, step2094]: loss 0.452639
[epoch14, step2095]: loss 0.721445
[epoch14, step2096]: loss 0.609014
[epoch14, step2097]: loss 0.258478
[epoch14, step2098]: loss 0.545077
[epoch14, step2099]: loss 0.448633
[epoch14, step2100]: loss 0.621372
[epoch14, step2101]: loss 0.445942
[epoch14, step2102]: loss 0.579358
[epoch14, step2103]: loss 0.527392
[epoch14, step2104]: loss 0.655869
[epoch14, step2105]: loss 0.438710
[epoch14, step2106]: loss 0.553144
[epoch14, step2107]: loss 0.430522
[epoch14, step2108]: loss 0.555390
[epoch14, step2109]: loss 0.564831
[epoch14, step2110]: loss 0.193534
[epoch14, step2111]: loss 0.499168
[epoch14, step2112]: loss 0.331162
[epoch14, step2113]: loss 0.367070
[epoch14, step2114]: loss 0.273803
[epoch14, step2115]: loss 0.254371
[epoch14, step2116]: loss 0.623459
[epoch14, step2117]: loss 0.388643
[epoch14, step2118]: loss 0.731939
[epoch14, step2119]: loss 0.413399
[epoch14, step2120]: loss 0.088849
[epoch14, step2121]: loss 0.499732
[epoch14, step2122]: loss 0.631680
[epoch14, step2123]: loss 0.605195
[epoch14, step2124]: loss 0.389605
[epoch14, step2125]: loss 0.698315
[epoch14, step2126]: loss 0.681147
[epoch14, step2127]: loss 0.253618
[epoch14, step2128]: loss 0.529706
[epoch14, step2129]: loss 0.124546
[epoch14, step2130]: loss 0.673969
[epoch14, step2131]: loss 0.331175
[epoch14, step2132]: loss 0.437645
[epoch14, step2133]: loss 0.336868
[epoch14, step2134]: loss 0.586497
[epoch14, step2135]: loss 0.471317
[epoch14, step2136]: loss 0.490065
[epoch14, step2137]: loss 0.430588
[epoch14, step2138]: loss 0.446837
[epoch14, step2139]: loss 0.591766
[epoch14, step2140]: loss 0.535162
[epoch14, step2141]: loss 0.496323
[epoch14, step2142]: loss 0.491912
[epoch14, step2143]: loss 0.583797
[epoch14, step2144]: loss 0.563607
[epoch14, step2145]: loss 0.437766
[epoch14, step2146]: loss 0.381278
[epoch14, step2147]: loss 0.615095
[epoch14, step2148]: loss 0.311192
[epoch14, step2149]: loss 0.451828
[epoch14, step2150]: loss 0.508214
[epoch14, step2151]: loss 0.341700
[epoch14, step2152]: loss 0.432531
[epoch14, step2153]: loss 0.436364
[epoch14, step2154]: loss 0.194719
[epoch14, step2155]: loss 0.510402
[epoch14, step2156]: loss 0.255657
[epoch14, step2157]: loss 0.336647
[epoch14, step2158]: loss 0.444887
[epoch14, step2159]: loss 0.507410
[epoch14, step2160]: loss 0.292196
[epoch14, step2161]: loss 0.379020
[epoch14, step2162]: loss 0.240504
[epoch14, step2163]: loss 0.582733
[epoch14, step2164]: loss 0.479935
[epoch14, step2165]: loss 0.371374
[epoch14, step2166]: loss 0.495379
[epoch14, step2167]: loss 0.640325
[epoch14, step2168]: loss 0.621759
[epoch14, step2169]: loss 0.492866
[epoch14, step2170]: loss 0.407172
[epoch14, step2171]: loss 0.402308
[epoch14, step2172]: loss 0.599227
[epoch14, step2173]: loss 0.793606
[epoch14, step2174]: loss 0.277866
[epoch14, step2175]: loss 0.545902
[epoch14, step2176]: loss 0.413688
[epoch14, step2177]: loss 0.539782
[epoch14, step2178]: loss 0.394462
[epoch14, step2179]: loss 0.301180
[epoch14, step2180]: loss 0.512153
[epoch14, step2181]: loss 0.604472
[epoch14, step2182]: loss 0.286055
[epoch14, step2183]: loss 0.593023
[epoch14, step2184]: loss 0.545088
[epoch14, step2185]: loss 0.474333
[epoch14, step2186]: loss 0.689129
[epoch14, step2187]: loss 0.502324
[epoch14, step2188]: loss 0.563749
[epoch14, step2189]: loss 0.158636
[epoch14, step2190]: loss 0.456630
[epoch14, step2191]: loss 0.604248
[epoch14, step2192]: loss 0.426974
[epoch14, step2193]: loss 0.371570
[epoch14, step2194]: loss 0.577430
[epoch14, step2195]: loss 0.392811
[epoch14, step2196]: loss 0.410804
[epoch14, step2197]: loss 0.690251
[epoch14, step2198]: loss 0.441783
[epoch14, step2199]: loss 0.500152
[epoch14, step2200]: loss 0.400550
[epoch14, step2201]: loss 0.280411
[epoch14, step2202]: loss 0.323331
[epoch14, step2203]: loss 0.553051
[epoch14, step2204]: loss 0.748409
[epoch14, step2205]: loss 0.192312
[epoch14, step2206]: loss 0.187798
[epoch14, step2207]: loss 0.522456
[epoch14, step2208]: loss 0.605581
[epoch14, step2209]: loss 0.464928
[epoch14, step2210]: loss 0.259200
[epoch14, step2211]: loss 0.521670
[epoch14, step2212]: loss 0.290517
[epoch14, step2213]: loss 0.403940
[epoch14, step2214]: loss 0.524835
[epoch14, step2215]: loss 0.612141
[epoch14, step2216]: loss 0.591882
[epoch14, step2217]: loss 0.458467
[epoch14, step2218]: loss 0.555267
[epoch14, step2219]: loss 0.547622
[epoch14, step2220]: loss 0.601567
[epoch14, step2221]: loss 0.406830
[epoch14, step2222]: loss 0.596629
[epoch14, step2223]: loss 0.542369
[epoch14, step2224]: loss 0.353532
[epoch14, step2225]: loss 0.433844
[epoch14, step2226]: loss 0.670284
[epoch14, step2227]: loss 0.537133
[epoch14, step2228]: loss 0.578616
[epoch14, step2229]: loss 0.384074
[epoch14, step2230]: loss 0.467533
[epoch14, step2231]: loss 0.622945
[epoch14, step2232]: loss 0.436839
[epoch14, step2233]: loss 0.677652
[epoch14, step2234]: loss 0.453882
[epoch14, step2235]: loss 0.430702
[epoch14, step2236]: loss 0.458213
[epoch14, step2237]: loss 0.555527
[epoch14, step2238]: loss 0.375011
[epoch14, step2239]: loss 0.638883
[epoch14, step2240]: loss 0.549213
[epoch14, step2241]: loss 0.692020
[epoch14, step2242]: loss 0.698092
[epoch14, step2243]: loss 0.355523
[epoch14, step2244]: loss 0.491241
[epoch14, step2245]: loss 0.467257
[epoch14, step2246]: loss 0.311134
[epoch14, step2247]: loss 0.308607
[epoch14, step2248]: loss 0.550646
[epoch14, step2249]: loss 0.336291
[epoch14, step2250]: loss 0.396570
[epoch14, step2251]: loss 0.600193
[epoch14, step2252]: loss 0.352749
[epoch14, step2253]: loss 0.591333
[epoch14, step2254]: loss 0.494669
[epoch14, step2255]: loss 0.573203
[epoch14, step2256]: loss 0.276529
[epoch14, step2257]: loss 0.586878
[epoch14, step2258]: loss 0.703234
[epoch14, step2259]: loss 0.665257
[epoch14, step2260]: loss 0.374293
[epoch14, step2261]: loss 0.277748
[epoch14, step2262]: loss 0.680287
[epoch14, step2263]: loss 0.535883
[epoch14, step2264]: loss 0.628423
[epoch14, step2265]: loss 0.263030
[epoch14, step2266]: loss 0.710660
[epoch14, step2267]: loss 0.605502
[epoch14, step2268]: loss 0.484591
[epoch14, step2269]: loss 0.581417
[epoch14, step2270]: loss 0.283511
[epoch14, step2271]: loss 0.486301
[epoch14, step2272]: loss 0.473683
[epoch14, step2273]: loss 0.422782
[epoch14, step2274]: loss 0.350307
[epoch14, step2275]: loss 0.612441
[epoch14, step2276]: loss 0.585207
[epoch14, step2277]: loss 0.254490
[epoch14, step2278]: loss 0.558719
[epoch14, step2279]: loss 0.458107
[epoch14, step2280]: loss 0.424539
[epoch14, step2281]: loss 0.515161
[epoch14, step2282]: loss 0.499000
[epoch14, step2283]: loss 0.417981
[epoch14, step2284]: loss 0.500175
[epoch14, step2285]: loss 0.381932
[epoch14, step2286]: loss 0.627709
[epoch14, step2287]: loss 0.419374
[epoch14, step2288]: loss 0.333328
[epoch14, step2289]: loss 0.380646
[epoch14, step2290]: loss 0.451679
[epoch14, step2291]: loss 0.565428
[epoch14, step2292]: loss 0.347644
[epoch14, step2293]: loss 0.584532
[epoch14, step2294]: loss 0.553040
[epoch14, step2295]: loss 0.503126
[epoch14, step2296]: loss 0.663163
[epoch14, step2297]: loss 0.598327
[epoch14, step2298]: loss 0.409717
[epoch14, step2299]: loss 0.376740
[epoch14, step2300]: loss 0.281112
[epoch14, step2301]: loss 0.550442
[epoch14, step2302]: loss 0.309518
[epoch14, step2303]: loss 0.497201
[epoch14, step2304]: loss 0.482535
[epoch14, step2305]: loss 0.560991
[epoch14, step2306]: loss 0.771914
[epoch14, step2307]: loss 0.536106
[epoch14, step2308]: loss 0.622988
[epoch14, step2309]: loss 0.498744
[epoch14, step2310]: loss 0.560883
[epoch14, step2311]: loss 0.467226
[epoch14, step2312]: loss 0.656248
[epoch14, step2313]: loss 0.531627
[epoch14, step2314]: loss 0.633864
[epoch14, step2315]: loss 0.611677
[epoch14, step2316]: loss 0.352780
[epoch14, step2317]: loss 0.526067
[epoch14, step2318]: loss 0.583586
[epoch14, step2319]: loss 0.650160
[epoch14, step2320]: loss 0.558747
[epoch14, step2321]: loss 0.554644
[epoch14, step2322]: loss 0.424476
[epoch14, step2323]: loss 0.457052
[epoch14, step2324]: loss 0.268581
[epoch14, step2325]: loss 0.513574
[epoch14, step2326]: loss 0.620319
[epoch14, step2327]: loss 0.380543
[epoch14, step2328]: loss 0.498620
[epoch14, step2329]: loss 0.437837
[epoch14, step2330]: loss 0.784431
[epoch14, step2331]: loss 0.318588
[epoch14, step2332]: loss 0.404489
[epoch14, step2333]: loss 0.518299
[epoch14, step2334]: loss 0.556976
[epoch14, step2335]: loss 0.381232
[epoch14, step2336]: loss 0.534340
[epoch14, step2337]: loss 0.329055
[epoch14, step2338]: loss 0.514431
[epoch14, step2339]: loss 0.251753
[epoch14, step2340]: loss 0.414136
[epoch14, step2341]: loss 0.595870
[epoch14, step2342]: loss 0.475182
[epoch14, step2343]: loss 0.562760
[epoch14, step2344]: loss 0.459167
[epoch14, step2345]: loss 0.560443
[epoch14, step2346]: loss 0.501502
[epoch14, step2347]: loss 0.577534
[epoch14, step2348]: loss 0.416026
[epoch14, step2349]: loss 0.352827
[epoch14, step2350]: loss 0.490156
[epoch14, step2351]: loss 0.455453
[epoch14, step2352]: loss 0.533076
[epoch14, step2353]: loss 0.512187
[epoch14, step2354]: loss 0.657736
[epoch14, step2355]: loss 0.513355
[epoch14, step2356]: loss 0.350703
[epoch14, step2357]: loss 0.596039
[epoch14, step2358]: loss 0.433930
[epoch14, step2359]: loss 0.441979
[epoch14, step2360]: loss 0.536431
[epoch14, step2361]: loss 0.399930
[epoch14, step2362]: loss 0.296766
[epoch14, step2363]: loss 0.357324
[epoch14, step2364]: loss 0.583928
[epoch14, step2365]: loss 0.601498
[epoch14, step2366]: loss 0.318646
[epoch14, step2367]: loss 0.353483
[epoch14, step2368]: loss 0.454594
[epoch14, step2369]: loss 0.376810
[epoch14, step2370]: loss 0.415604
[epoch14, step2371]: loss 0.440243
[epoch14, step2372]: loss 0.356500
[epoch14, step2373]: loss 0.596984
[epoch14, step2374]: loss 0.598330
[epoch14, step2375]: loss 0.257865
[epoch14, step2376]: loss 0.508196
[epoch14, step2377]: loss 0.591560
[epoch14, step2378]: loss 0.339345
[epoch14, step2379]: loss 0.483700
[epoch14, step2380]: loss 0.524046
[epoch14, step2381]: loss 0.628317
[epoch14, step2382]: loss 0.420603
[epoch14, step2383]: loss 0.609069
[epoch14, step2384]: loss 0.306826
[epoch14, step2385]: loss 0.534883
[epoch14, step2386]: loss 0.359095
[epoch14, step2387]: loss 0.476207
[epoch14, step2388]: loss 0.264174
[epoch14, step2389]: loss 0.442521
[epoch14, step2390]: loss 0.700284
[epoch14, step2391]: loss 0.634659
[epoch14, step2392]: loss 0.623483
[epoch14, step2393]: loss 0.531922
[epoch14, step2394]: loss 0.335153
[epoch14, step2395]: loss 0.348794
[epoch14, step2396]: loss 0.513801
[epoch14, step2397]: loss 0.475080
[epoch14, step2398]: loss 0.549134
[epoch14, step2399]: loss 0.296659
[epoch14, step2400]: loss 0.511712
[epoch14, step2401]: loss 0.602177
[epoch14, step2402]: loss 0.515127
[epoch14, step2403]: loss 0.586499
[epoch14, step2404]: loss 0.533280
[epoch14, step2405]: loss 0.602142
[epoch14, step2406]: loss 0.425884
[epoch14, step2407]: loss 0.468979
[epoch14, step2408]: loss 0.587143
[epoch14, step2409]: loss 0.160998
[epoch14, step2410]: loss 0.600794
[epoch14, step2411]: loss 0.374547
[epoch14, step2412]: loss 0.630843
[epoch14, step2413]: loss 0.527145
[epoch14, step2414]: loss 0.603208
[epoch14, step2415]: loss 0.411757
[epoch14, step2416]: loss 0.582376
[epoch14, step2417]: loss 0.365621
[epoch14, step2418]: loss 0.563606
[epoch14, step2419]: loss 0.435642
[epoch14, step2420]: loss 0.494568
[epoch14, step2421]: loss 0.528451
[epoch14, step2422]: loss 0.400020
[epoch14, step2423]: loss 0.305990
[epoch14, step2424]: loss 0.500461
[epoch14, step2425]: loss 0.717430
[epoch14, step2426]: loss 0.518966
[epoch14, step2427]: loss 0.445682
[epoch14, step2428]: loss 0.562052
[epoch14, step2429]: loss 0.564581
[epoch14, step2430]: loss 0.527823
[epoch14, step2431]: loss 0.530336
[epoch14, step2432]: loss 0.478359
[epoch14, step2433]: loss 0.495368
[epoch14, step2434]: loss 0.270703
[epoch14, step2435]: loss 0.472407
[epoch14, step2436]: loss 0.570524
[epoch14, step2437]: loss 0.435461
[epoch14, step2438]: loss 0.555589
[epoch14, step2439]: loss 0.519484
[epoch14, step2440]: loss 0.673015
[epoch14, step2441]: loss 0.605816
[epoch14, step2442]: loss 0.581868
[epoch14, step2443]: loss 0.378385
[epoch14, step2444]: loss 0.453315
[epoch14, step2445]: loss 0.398092
[epoch14, step2446]: loss 0.484110
[epoch14, step2447]: loss 0.534740
[epoch14, step2448]: loss 0.565840
[epoch14, step2449]: loss 0.376575
[epoch14, step2450]: loss 0.636592
[epoch14, step2451]: loss 0.488016
[epoch14, step2452]: loss 0.574353
[epoch14, step2453]: loss 0.355980
[epoch14, step2454]: loss 0.275129
[epoch14, step2455]: loss 0.455428
[epoch14, step2456]: loss 0.496737
[epoch14, step2457]: loss 0.541596
[epoch14, step2458]: loss 0.507169
[epoch14, step2459]: loss 0.508643
[epoch14, step2460]: loss 0.235964
[epoch14, step2461]: loss 0.503427
[epoch14, step2462]: loss 0.371251
[epoch14, step2463]: loss 0.592055
[epoch14, step2464]: loss 0.683940
[epoch14, step2465]: loss 0.468491
[epoch14, step2466]: loss 0.727286
[epoch14, step2467]: loss 0.489760
[epoch14, step2468]: loss 0.581894
[epoch14, step2469]: loss 0.566781
[epoch14, step2470]: loss 0.379518
[epoch14, step2471]: loss 0.635045
[epoch14, step2472]: loss 0.441604
[epoch14, step2473]: loss 0.422984
[epoch14, step2474]: loss 0.341102
[epoch14, step2475]: loss 0.533769
[epoch14, step2476]: loss 0.359922
[epoch14, step2477]: loss 0.406580
[epoch14, step2478]: loss 0.581144
[epoch14, step2479]: loss 0.777703
[epoch14, step2480]: loss 0.626073
[epoch14, step2481]: loss 0.608567
[epoch14, step2482]: loss 0.597319
[epoch14, step2483]: loss 0.457180
[epoch14, step2484]: loss 0.562244
[epoch14, step2485]: loss 0.424782
[epoch14, step2486]: loss 0.481413
[epoch14, step2487]: loss 0.361556
[epoch14, step2488]: loss 0.411910
[epoch14, step2489]: loss 0.498442
[epoch14, step2490]: loss 0.501381
[epoch14, step2491]: loss 0.578954
[epoch14, step2492]: loss 0.482563
[epoch14, step2493]: loss 0.354226
[epoch14, step2494]: loss 0.498793
[epoch14, step2495]: loss 0.616031
[epoch14, step2496]: loss 0.448797
[epoch14, step2497]: loss 0.444050
[epoch14, step2498]: loss 0.742116
[epoch14, step2499]: loss 0.453438
[epoch14, step2500]: loss 0.535905
[epoch14, step2501]: loss 0.437408
[epoch14, step2502]: loss 0.561491
[epoch14, step2503]: loss 0.254736
[epoch14, step2504]: loss 0.445537
[epoch14, step2505]: loss 0.727147
[epoch14, step2506]: loss 0.524618
[epoch14, step2507]: loss 0.593958
[epoch14, step2508]: loss 0.632998
[epoch14, step2509]: loss 0.333326
[epoch14, step2510]: loss 0.270791
[epoch14, step2511]: loss 0.465210
[epoch14, step2512]: loss 0.305748
[epoch14, step2513]: loss 0.526576
[epoch14, step2514]: loss 0.374462
[epoch14, step2515]: loss 0.551055
[epoch14, step2516]: loss 0.383680
[epoch14, step2517]: loss 0.232038
[epoch14, step2518]: loss 0.484580
[epoch14, step2519]: loss 0.338802
[epoch14, step2520]: loss 0.423691
[epoch14, step2521]: loss 0.303126
[epoch14, step2522]: loss 0.500651
[epoch14, step2523]: loss 0.456054
[epoch14, step2524]: loss 0.413972
[epoch14, step2525]: loss 0.538889
[epoch14, step2526]: loss 0.451363
[epoch14, step2527]: loss 0.404298
[epoch14, step2528]: loss 0.485546
[epoch14, step2529]: loss 0.285600
[epoch14, step2530]: loss 0.457039
[epoch14, step2531]: loss 0.572459
[epoch14, step2532]: loss 0.447264
[epoch14, step2533]: loss 0.649244
[epoch14, step2534]: loss 0.495843
[epoch14, step2535]: loss 0.487326
[epoch14, step2536]: loss 0.560984
[epoch14, step2537]: loss 0.307889
[epoch14, step2538]: loss 0.353172
[epoch14, step2539]: loss 0.442501
[epoch14, step2540]: loss 0.291181
[epoch14, step2541]: loss 0.347050
[epoch14, step2542]: loss 0.703833
[epoch14, step2543]: loss 0.492684
[epoch14, step2544]: loss 0.445805
[epoch14, step2545]: loss 0.563456
[epoch14, step2546]: loss 0.491254
[epoch14, step2547]: loss 0.134908
[epoch14, step2548]: loss 0.557821
[epoch14, step2549]: loss 0.611124
[epoch14, step2550]: loss 0.242172
[epoch14, step2551]: loss 0.506303
[epoch14, step2552]: loss 0.382728
[epoch14, step2553]: loss 0.464179
[epoch14, step2554]: loss 0.338089
[epoch14, step2555]: loss 0.498644
[epoch14, step2556]: loss 0.616938
[epoch14, step2557]: loss 0.564005
[epoch14, step2558]: loss 0.400488
[epoch14, step2559]: loss 0.398681
[epoch14, step2560]: loss 0.301204
[epoch14, step2561]: loss 0.353695
[epoch14, step2562]: loss 0.437441
[epoch14, step2563]: loss 0.712504
[epoch14, step2564]: loss 0.771872
[epoch14, step2565]: loss 0.704621
[epoch14, step2566]: loss 0.514760
[epoch14, step2567]: loss 0.299942
[epoch14, step2568]: loss 0.363409
[epoch14, step2569]: loss 0.608648
[epoch14, step2570]: loss 0.339087
[epoch14, step2571]: loss 0.646284
[epoch14, step2572]: loss 0.598943
[epoch14, step2573]: loss 0.506226
[epoch14, step2574]: loss 0.639571
[epoch14, step2575]: loss 0.471307
[epoch14, step2576]: loss 0.509794
[epoch14, step2577]: loss 0.364300
[epoch14, step2578]: loss 0.508827
[epoch14, step2579]: loss 0.583092
[epoch14, step2580]: loss 0.441902
[epoch14, step2581]: loss 0.378462
[epoch14, step2582]: loss 0.577059
[epoch14, step2583]: loss 0.478147
[epoch14, step2584]: loss 0.472870
[epoch14, step2585]: loss 0.433631
[epoch14, step2586]: loss 0.551680
[epoch14, step2587]: loss 0.591148
[epoch14, step2588]: loss 0.411255
[epoch14, step2589]: loss 0.518852
[epoch14, step2590]: loss 0.411159
[epoch14, step2591]: loss 0.494041
[epoch14, step2592]: loss 0.632240
[epoch14, step2593]: loss 0.594529
[epoch14, step2594]: loss 0.567755
[epoch14, step2595]: loss 0.517386
[epoch14, step2596]: loss 0.480430
[epoch14, step2597]: loss 0.367954
[epoch14, step2598]: loss 0.553323
[epoch14, step2599]: loss 0.564586
[epoch14, step2600]: loss 0.466073
[epoch14, step2601]: loss 0.311590
[epoch14, step2602]: loss 0.464647
[epoch14, step2603]: loss 0.733718
[epoch14, step2604]: loss 0.647035
[epoch14, step2605]: loss 0.596569
[epoch14, step2606]: loss 0.506695
[epoch14, step2607]: loss 0.476415
[epoch14, step2608]: loss 0.639265
[epoch14, step2609]: loss 0.455159
[epoch14, step2610]: loss 0.587537
[epoch14, step2611]: loss 0.424288
[epoch14, step2612]: loss 0.466795
[epoch14, step2613]: loss 0.690702
[epoch14, step2614]: loss 0.663499
[epoch14, step2615]: loss 0.513953
[epoch14, step2616]: loss 0.472166
[epoch14, step2617]: loss 0.332725
[epoch14, step2618]: loss 0.353223
[epoch14, step2619]: loss 0.499251
[epoch14, step2620]: loss 0.354788
[epoch14, step2621]: loss 0.330176
[epoch14, step2622]: loss 0.357190
[epoch14, step2623]: loss 0.527539
[epoch14, step2624]: loss 0.465999
[epoch14, step2625]: loss 0.412778
[epoch14, step2626]: loss 0.395196
[epoch14, step2627]: loss 0.668279
[epoch14, step2628]: loss 0.759724
[epoch14, step2629]: loss 0.421766
[epoch14, step2630]: loss 0.612090
[epoch14, step2631]: loss 0.602087
[epoch14, step2632]: loss 0.535698
[epoch14, step2633]: loss 0.567003
[epoch14, step2634]: loss 0.354547
[epoch14, step2635]: loss 0.243083
[epoch14, step2636]: loss 0.450247
[epoch14, step2637]: loss 0.429757
[epoch14, step2638]: loss 0.632947
[epoch14, step2639]: loss 0.415320
[epoch14, step2640]: loss 0.602235
[epoch14, step2641]: loss 0.217381
[epoch14, step2642]: loss 0.772544
[epoch14, step2643]: loss 0.431175
[epoch14, step2644]: loss 0.684847
[epoch14, step2645]: loss 0.510537
[epoch14, step2646]: loss 0.372061
[epoch14, step2647]: loss 0.439641
[epoch14, step2648]: loss 0.386668
[epoch14, step2649]: loss 0.477429
[epoch14, step2650]: loss 0.503397
[epoch14, step2651]: loss 0.286089
[epoch14, step2652]: loss 0.375104
[epoch14, step2653]: loss 0.342853
[epoch14, step2654]: loss 0.575586
[epoch14, step2655]: loss 0.534055
[epoch14, step2656]: loss 0.390994
[epoch14, step2657]: loss 0.310397
[epoch14, step2658]: loss 0.447939
[epoch14, step2659]: loss 0.540608
[epoch14, step2660]: loss 0.359940
[epoch14, step2661]: loss 0.562280
[epoch14, step2662]: loss 0.628204
[epoch14, step2663]: loss 0.466329
[epoch14, step2664]: loss 0.236748
[epoch14, step2665]: loss 0.417214
[epoch14, step2666]: loss 0.348714
[epoch14, step2667]: loss 0.602400
[epoch14, step2668]: loss 0.449721
[epoch14, step2669]: loss 0.426388
[epoch14, step2670]: loss 0.354188
[epoch14, step2671]: loss 0.520851
[epoch14, step2672]: loss 0.305886
[epoch14, step2673]: loss 0.498742
[epoch14, step2674]: loss 0.487412
[epoch14, step2675]: loss 0.431253
[epoch14, step2676]: loss 0.500316
[epoch14, step2677]: loss 0.335786
[epoch14, step2678]: loss 0.412093
[epoch14, step2679]: loss 0.559700
[epoch14, step2680]: loss 0.435377
[epoch14, step2681]: loss 0.176990
[epoch14, step2682]: loss 0.371764
[epoch14, step2683]: loss 0.466536
[epoch14, step2684]: loss 0.232731
[epoch14, step2685]: loss 0.520006
[epoch14, step2686]: loss 0.453131
[epoch14, step2687]: loss 0.327403
[epoch14, step2688]: loss 0.373720
[epoch14, step2689]: loss 0.438443
[epoch14, step2690]: loss 0.531059
[epoch14, step2691]: loss 0.532147
[epoch14, step2692]: loss 0.387041
[epoch14, step2693]: loss 0.442853
[epoch14, step2694]: loss 0.435682
[epoch14, step2695]: loss 0.356435
[epoch14, step2696]: loss 0.442857
[epoch14, step2697]: loss 0.655606
[epoch14, step2698]: loss 0.456111
[epoch14, step2699]: loss 0.422839
[epoch14, step2700]: loss 0.626484
[epoch14, step2701]: loss 0.625858
[epoch14, step2702]: loss 0.398756
[epoch14, step2703]: loss 0.631561
[epoch14, step2704]: loss 0.376360
[epoch14, step2705]: loss 0.227505
[epoch14, step2706]: loss 0.297904
[epoch14, step2707]: loss 0.450634
[epoch14, step2708]: loss 0.527394
[epoch14, step2709]: loss 0.387577
[epoch14, step2710]: loss 0.466150
[epoch14, step2711]: loss 0.313210
[epoch14, step2712]: loss 0.547736
[epoch14, step2713]: loss 0.583431
[epoch14, step2714]: loss 0.573836
[epoch14, step2715]: loss 0.682467
[epoch14, step2716]: loss 0.513402
[epoch14, step2717]: loss 0.520899
[epoch14, step2718]: loss 0.545554
[epoch14, step2719]: loss 0.546555
[epoch14, step2720]: loss 0.509088
[epoch14, step2721]: loss 0.399286
[epoch14, step2722]: loss 0.472524
[epoch14, step2723]: loss 0.419707
[epoch14, step2724]: loss 0.628161
[epoch14, step2725]: loss 0.541267
[epoch14, step2726]: loss 0.619133
[epoch14, step2727]: loss 0.221684
[epoch14, step2728]: loss 0.573418
[epoch14, step2729]: loss 0.300165
[epoch14, step2730]: loss 0.508464
[epoch14, step2731]: loss 0.318837
[epoch14, step2732]: loss 0.494890
[epoch14, step2733]: loss 0.560404
[epoch14, step2734]: loss 0.433727
[epoch14, step2735]: loss 0.398834
[epoch14, step2736]: loss 0.633735
[epoch14, step2737]: loss 0.568279
[epoch14, step2738]: loss 0.366159
[epoch14, step2739]: loss 0.421607
[epoch14, step2740]: loss 0.544493
[epoch14, step2741]: loss 0.676019
[epoch14, step2742]: loss 0.499202
[epoch14, step2743]: loss 0.789963
[epoch14, step2744]: loss 0.364459
[epoch14, step2745]: loss 0.638956
[epoch14, step2746]: loss 0.686010
[epoch14, step2747]: loss 0.454777
[epoch14, step2748]: loss 0.315244
[epoch14, step2749]: loss 0.575713
[epoch14, step2750]: loss 0.325560
[epoch14, step2751]: loss 0.280288
[epoch14, step2752]: loss 0.541628
[epoch14, step2753]: loss 0.442099
[epoch14, step2754]: loss 0.587611
[epoch14, step2755]: loss 0.405597
[epoch14, step2756]: loss 0.254156
[epoch14, step2757]: loss 0.457431
[epoch14, step2758]: loss 0.664365
[epoch14, step2759]: loss 0.550238
[epoch14, step2760]: loss 0.675342
[epoch14, step2761]: loss 0.175122
[epoch14, step2762]: loss 0.426257
[epoch14, step2763]: loss 0.660945
[epoch14, step2764]: loss 0.530418
[epoch14, step2765]: loss 0.511375
[epoch14, step2766]: loss 0.458868
[epoch14, step2767]: loss 0.338708
[epoch14, step2768]: loss 0.318522
[epoch14, step2769]: loss 0.484344
[epoch14, step2770]: loss 0.595303
[epoch14, step2771]: loss 0.514393
[epoch14, step2772]: loss 0.504520
[epoch14, step2773]: loss 0.500877
[epoch14, step2774]: loss 0.610397
[epoch14, step2775]: loss 0.534115
[epoch14, step2776]: loss 0.383751
[epoch14, step2777]: loss 0.350804
[epoch14, step2778]: loss 0.527616
[epoch14, step2779]: loss 0.434743
[epoch14, step2780]: loss 0.306805
[epoch14, step2781]: loss 0.563107
[epoch14, step2782]: loss 0.374999
[epoch14, step2783]: loss 0.489148
[epoch14, step2784]: loss 0.426009
[epoch14, step2785]: loss 0.524952
[epoch14, step2786]: loss 0.454703
[epoch14, step2787]: loss 0.503322
[epoch14, step2788]: loss 0.358633
[epoch14, step2789]: loss 0.507279
[epoch14, step2790]: loss 0.401342
[epoch14, step2791]: loss 0.424496
[epoch14, step2792]: loss 0.574825
[epoch14, step2793]: loss 0.402951
[epoch14, step2794]: loss 0.696276
[epoch14, step2795]: loss 0.363016
[epoch14, step2796]: loss 0.477348
[epoch14, step2797]: loss 0.570224
[epoch14, step2798]: loss 0.396540
[epoch14, step2799]: loss 0.323943
[epoch14, step2800]: loss 0.620008
[epoch14, step2801]: loss 0.168202
[epoch14, step2802]: loss 0.379928
[epoch14, step2803]: loss 0.483367
[epoch14, step2804]: loss 0.652130
[epoch14, step2805]: loss 0.617500
[epoch14, step2806]: loss 0.417654
[epoch14, step2807]: loss 0.678542
[epoch14, step2808]: loss 0.539176
[epoch14, step2809]: loss 0.586570
[epoch14, step2810]: loss 0.466944
[epoch14, step2811]: loss 0.578835
[epoch14, step2812]: loss 0.435318
[epoch14, step2813]: loss 0.488406
[epoch14, step2814]: loss 0.390365
[epoch14, step2815]: loss 0.472015
[epoch14, step2816]: loss 0.500374
[epoch14, step2817]: loss 0.554796
[epoch14, step2818]: loss 0.412231
[epoch14, step2819]: loss 0.407193
[epoch14, step2820]: loss 0.623746
[epoch14, step2821]: loss 0.525747
[epoch14, step2822]: loss 0.719935
[epoch14, step2823]: loss 0.489585
[epoch14, step2824]: loss 0.482893
[epoch14, step2825]: loss 0.520823
[epoch14, step2826]: loss 0.674261
[epoch14, step2827]: loss 0.244523
[epoch14, step2828]: loss 0.555009
[epoch14, step2829]: loss 0.534132
[epoch14, step2830]: loss 0.241184
[epoch14, step2831]: loss 0.570707
[epoch14, step2832]: loss 0.214774
[epoch14, step2833]: loss 0.491758
[epoch14, step2834]: loss 0.467558
[epoch14, step2835]: loss 0.192700
[epoch14, step2836]: loss 0.249337
[epoch14, step2837]: loss 0.599826
[epoch14, step2838]: loss 0.439448
[epoch14, step2839]: loss 0.171900
[epoch14, step2840]: loss 0.532313
[epoch14, step2841]: loss 0.555574
[epoch14, step2842]: loss 0.466838
[epoch14, step2843]: loss 0.230420
[epoch14, step2844]: loss 0.591375
[epoch14, step2845]: loss 0.508934
[epoch14, step2846]: loss 0.548836
[epoch14, step2847]: loss 0.603621
[epoch14, step2848]: loss 0.534879
[epoch14, step2849]: loss 0.357513
[epoch14, step2850]: loss 0.298965
[epoch14, step2851]: loss 0.532700
[epoch14, step2852]: loss 0.326200
[epoch14, step2853]: loss 0.690634
[epoch14, step2854]: loss 0.203913
[epoch14, step2855]: loss 0.433297
[epoch14, step2856]: loss 0.412597
[epoch14, step2857]: loss 0.257554
[epoch14, step2858]: loss 0.410692
[epoch14, step2859]: loss 0.666005
[epoch14, step2860]: loss 0.281256
[epoch14, step2861]: loss 0.606332
[epoch14, step2862]: loss 0.483564
[epoch14, step2863]: loss 0.268955
[epoch14, step2864]: loss 0.338516
[epoch14, step2865]: loss 0.395605
[epoch14, step2866]: loss 0.461334
[epoch14, step2867]: loss 0.278972
[epoch14, step2868]: loss 0.573771
[epoch14, step2869]: loss 0.751793
[epoch14, step2870]: loss 0.463715
[epoch14, step2871]: loss 0.503025
[epoch14, step2872]: loss 0.611957
[epoch14, step2873]: loss 0.318367
[epoch14, step2874]: loss 0.370113
[epoch14, step2875]: loss 0.349327
[epoch14, step2876]: loss 0.340174
[epoch14, step2877]: loss 0.414531
[epoch14, step2878]: loss 0.472018
[epoch14, step2879]: loss 0.323648
[epoch14, step2880]: loss 0.478135
[epoch14, step2881]: loss 0.467013
[epoch14, step2882]: loss 0.303279
[epoch14, step2883]: loss 0.487635
[epoch14, step2884]: loss 0.437389
[epoch14, step2885]: loss 0.487210
[epoch14, step2886]: loss 0.547666
[epoch14, step2887]: loss 0.514798
[epoch14, step2888]: loss 0.342561
[epoch14, step2889]: loss 0.365282
[epoch14, step2890]: loss 0.497654
[epoch14, step2891]: loss 0.583289
[epoch14, step2892]: loss 0.389606
[epoch14, step2893]: loss 0.503569
[epoch14, step2894]: loss 0.521847
[epoch14, step2895]: loss 0.253870
[epoch14, step2896]: loss 0.618918
[epoch14, step2897]: loss 0.494651
[epoch14, step2898]: loss 0.285325
[epoch14, step2899]: loss 0.396013
[epoch14, step2900]: loss 0.290687
[epoch14, step2901]: loss 0.612779
[epoch14, step2902]: loss 0.617463
[epoch14, step2903]: loss 0.279120
[epoch14, step2904]: loss 0.256370
[epoch14, step2905]: loss 0.509301
[epoch14, step2906]: loss 0.592506
[epoch14, step2907]: loss 0.287877
[epoch14, step2908]: loss 0.625836
[epoch14, step2909]: loss 0.614565
[epoch14, step2910]: loss 0.417493
[epoch14, step2911]: loss 0.654852
[epoch14, step2912]: loss 0.507515
[epoch14, step2913]: loss 0.217447
[epoch14, step2914]: loss 0.689641
[epoch14, step2915]: loss 0.495519
[epoch14, step2916]: loss 0.534052
[epoch14, step2917]: loss 0.179483
[epoch14, step2918]: loss 0.470960
[epoch14, step2919]: loss 0.514935
[epoch14, step2920]: loss 0.194356
[epoch14, step2921]: loss 0.243654
[epoch14, step2922]: loss 0.454410
[epoch14, step2923]: loss 0.475518
[epoch14, step2924]: loss 0.505851
[epoch14, step2925]: loss 0.345290
[epoch14, step2926]: loss 0.640663
[epoch14, step2927]: loss 0.575425
[epoch14, step2928]: loss 0.324481
[epoch14, step2929]: loss 0.531109
[epoch14, step2930]: loss 0.547860
[epoch14, step2931]: loss 0.344696
[epoch14, step2932]: loss 0.585230
[epoch14, step2933]: loss 0.409160
[epoch14, step2934]: loss 0.387053
[epoch14, step2935]: loss 0.426219
[epoch14, step2936]: loss 0.617234
[epoch14, step2937]: loss 0.375375
[epoch14, step2938]: loss 0.454312
[epoch14, step2939]: loss 0.489384
[epoch14, step2940]: loss 0.730977
[epoch14, step2941]: loss 0.527601
[epoch14, step2942]: loss 0.712322
[epoch14, step2943]: loss 0.397205
[epoch14, step2944]: loss 0.549462
[epoch14, step2945]: loss 0.398598
[epoch14, step2946]: loss 0.341487
[epoch14, step2947]: loss 0.548457
[epoch14, step2948]: loss 0.680906
[epoch14, step2949]: loss 0.489210
[epoch14, step2950]: loss 0.557017
[epoch14, step2951]: loss 0.222948
[epoch14, step2952]: loss 0.774689
[epoch14, step2953]: loss 0.261111
[epoch14, step2954]: loss 0.614234
[epoch14, step2955]: loss 0.579162
[epoch14, step2956]: loss 0.426298
[epoch14, step2957]: loss 0.548489
[epoch14, step2958]: loss 0.483099
[epoch14, step2959]: loss 0.582288
[epoch14, step2960]: loss 0.687136
[epoch14, step2961]: loss 0.649745
[epoch14, step2962]: loss 0.402353
[epoch14, step2963]: loss 0.437799
[epoch14, step2964]: loss 0.561584
[epoch14, step2965]: loss 0.477882
[epoch14, step2966]: loss 0.486256
[epoch14, step2967]: loss 0.178633
[epoch14, step2968]: loss 0.420528
[epoch14, step2969]: loss 0.383607
[epoch14, step2970]: loss 0.331011
[epoch14, step2971]: loss 0.361268
[epoch14, step2972]: loss 0.447227
[epoch14, step2973]: loss 0.476652
[epoch14, step2974]: loss 0.573995
[epoch14, step2975]: loss 0.546514
[epoch14, step2976]: loss 0.439557
[epoch14, step2977]: loss 0.307630
[epoch14, step2978]: loss 0.569879
[epoch14, step2979]: loss 0.672935
[epoch14, step2980]: loss 0.432754
[epoch14, step2981]: loss 0.550548
[epoch14, step2982]: loss 0.423650
[epoch14, step2983]: loss 0.524003
[epoch14, step2984]: loss 0.495091
[epoch14, step2985]: loss 0.432584
[epoch14, step2986]: loss 0.633508
[epoch14, step2987]: loss 0.621406
[epoch14, step2988]: loss 0.406868
[epoch14, step2989]: loss 0.493078
[epoch14, step2990]: loss 0.683733
[epoch14, step2991]: loss 0.638839
[epoch14, step2992]: loss 0.680619
[epoch14, step2993]: loss 0.623207
[epoch14, step2994]: loss 0.457034
[epoch14, step2995]: loss 0.413400
[epoch14, step2996]: loss 0.559994
[epoch14, step2997]: loss 0.576524
[epoch14, step2998]: loss 0.376101
[epoch14, step2999]: loss 0.400157
[epoch14, step3000]: loss 0.422518
[epoch14, step3001]: loss 0.716727
[epoch14, step3002]: loss 0.432482
[epoch14, step3003]: loss 0.467957
[epoch14, step3004]: loss 0.502195
[epoch14, step3005]: loss 0.589042
[epoch14, step3006]: loss 0.418699
[epoch14, step3007]: loss 0.584338
[epoch14, step3008]: loss 0.298263
[epoch14, step3009]: loss 0.557924
[epoch14, step3010]: loss 0.682478
[epoch14, step3011]: loss 0.472754
[epoch14, step3012]: loss 0.610107
[epoch14, step3013]: loss 0.477055
[epoch14, step3014]: loss 0.697727
[epoch14, step3015]: loss 0.613030
[epoch14, step3016]: loss 0.466926
[epoch14, step3017]: loss 0.333985
[epoch14, step3018]: loss 0.604252
[epoch14, step3019]: loss 0.536811
[epoch14, step3020]: loss 0.518041
[epoch14, step3021]: loss 0.577685
[epoch14, step3022]: loss 0.224572
[epoch14, step3023]: loss 0.367410
[epoch14, step3024]: loss 0.368162
[epoch14, step3025]: loss 0.563440
[epoch14, step3026]: loss 0.354496
[epoch14, step3027]: loss 0.621364
[epoch14, step3028]: loss 0.588236
[epoch14, step3029]: loss 0.570189
[epoch14, step3030]: loss 0.290605
[epoch14, step3031]: loss 0.454152
[epoch14, step3032]: loss 0.638426
[epoch14, step3033]: loss 0.600533
[epoch14, step3034]: loss 0.489331
[epoch14, step3035]: loss 0.345735
[epoch14, step3036]: loss 0.433968
[epoch14, step3037]: loss 0.539311
[epoch14, step3038]: loss 0.173497
[epoch14, step3039]: loss 0.603514
[epoch14, step3040]: loss 0.699907
[epoch14, step3041]: loss 0.302651
[epoch14, step3042]: loss 0.542610
[epoch14, step3043]: loss 0.590355
[epoch14, step3044]: loss 0.612731
[epoch14, step3045]: loss 0.438519
[epoch14, step3046]: loss 0.456352
[epoch14, step3047]: loss 0.544480
[epoch14, step3048]: loss 0.493953
[epoch14, step3049]: loss 0.588102
[epoch14, step3050]: loss 0.521691
[epoch14, step3051]: loss 0.582050
[epoch14, step3052]: loss 0.155800
[epoch14, step3053]: loss 0.216828
[epoch14, step3054]: loss 0.556884
[epoch14, step3055]: loss 0.574444
[epoch14, step3056]: loss 0.298458
[epoch14, step3057]: loss 0.592320
[epoch14, step3058]: loss 0.138501
[epoch14, step3059]: loss 0.245953
[epoch14, step3060]: loss 0.249274
[epoch14, step3061]: loss 0.507181
[epoch14, step3062]: loss 0.122004
[epoch14, step3063]: loss 0.308125
[epoch14, step3064]: loss 0.500391
[epoch14, step3065]: loss 0.505935
[epoch14, step3066]: loss 0.392180
[epoch14, step3067]: loss 0.586609
[epoch14, step3068]: loss 0.102316
[epoch14, step3069]: loss 0.587217
[epoch14, step3070]: loss 0.436836
[epoch14, step3071]: loss 0.496374
[epoch14, step3072]: loss 0.410355
[epoch14, step3073]: loss 0.501141
[epoch14, step3074]: loss 0.558810
[epoch14, step3075]: loss 0.636700
[epoch14, step3076]: loss 0.590581

[epoch14]: avg loss 0.590581

[epoch15, step1]: loss 0.538455
[epoch15, step2]: loss 0.677563
[epoch15, step3]: loss 0.356392
[epoch15, step4]: loss 0.574034
[epoch15, step5]: loss 0.594292
[epoch15, step6]: loss 0.583502
[epoch15, step7]: loss 0.226150
[epoch15, step8]: loss 0.394074
[epoch15, step9]: loss 0.650753
[epoch15, step10]: loss 0.661901
[epoch15, step11]: loss 0.708552
[epoch15, step12]: loss 0.516055
[epoch15, step13]: loss 0.409788
[epoch15, step14]: loss 0.343890
[epoch15, step15]: loss 0.479187
[epoch15, step16]: loss 0.525819
[epoch15, step17]: loss 0.495155
[epoch15, step18]: loss 0.608671
[epoch15, step19]: loss 0.296835
[epoch15, step20]: loss 0.562361
[epoch15, step21]: loss 0.506206
[epoch15, step22]: loss 0.260742
[epoch15, step23]: loss 0.705594
[epoch15, step24]: loss 0.452730
[epoch15, step25]: loss 0.460797
[epoch15, step26]: loss 0.463215
[epoch15, step27]: loss 0.448943
[epoch15, step28]: loss 0.423599
[epoch15, step29]: loss 0.603201
[epoch15, step30]: loss 0.614465
[epoch15, step31]: loss 0.650007
[epoch15, step32]: loss 0.465477
[epoch15, step33]: loss 0.586473
[epoch15, step34]: loss 0.436547
[epoch15, step35]: loss 0.450700
[epoch15, step36]: loss 0.471904
[epoch15, step37]: loss 0.645931
[epoch15, step38]: loss 0.627984
[epoch15, step39]: loss 0.327337
[epoch15, step40]: loss 0.572340
[epoch15, step41]: loss 0.268940
[epoch15, step42]: loss 0.582048
[epoch15, step43]: loss 0.597195
[epoch15, step44]: loss 0.427954
[epoch15, step45]: loss 0.473281
[epoch15, step46]: loss 0.356815
[epoch15, step47]: loss 0.271587
[epoch15, step48]: loss 0.407423
[epoch15, step49]: loss 0.588255
[epoch15, step50]: loss 0.576513
[epoch15, step51]: loss 0.620822
[epoch15, step52]: loss 0.569807
[epoch15, step53]: loss 0.443492
[epoch15, step54]: loss 0.506083
[epoch15, step55]: loss 0.351039
[epoch15, step56]: loss 0.424220
[epoch15, step57]: loss 0.519288
[epoch15, step58]: loss 0.352322
[epoch15, step59]: loss 0.508417
[epoch15, step60]: loss 0.454932
[epoch15, step61]: loss 0.635426
[epoch15, step62]: loss 0.483403
[epoch15, step63]: loss 0.510464
[epoch15, step64]: loss 0.556718
[epoch15, step65]: loss 0.636366
[epoch15, step66]: loss 0.455096
[epoch15, step67]: loss 0.405645
[epoch15, step68]: loss 0.575659
[epoch15, step69]: loss 0.652680
[epoch15, step70]: loss 0.485994
[epoch15, step71]: loss 0.545294
[epoch15, step72]: loss 0.609220
[epoch15, step73]: loss 0.483752
[epoch15, step74]: loss 0.590207
[epoch15, step75]: loss 0.562031
[epoch15, step76]: loss 0.563710
[epoch15, step77]: loss 0.484539
[epoch15, step78]: loss 0.380493
[epoch15, step79]: loss 0.453329
[epoch15, step80]: loss 0.416261
[epoch15, step81]: loss 0.400857
[epoch15, step82]: loss 0.505259
[epoch15, step83]: loss 0.476058
[epoch15, step84]: loss 0.267493
[epoch15, step85]: loss 0.562120
[epoch15, step86]: loss 0.552192
[epoch15, step87]: loss 0.451150
[epoch15, step88]: loss 0.428771
[epoch15, step89]: loss 0.607831
[epoch15, step90]: loss 0.186288
[epoch15, step91]: loss 0.654980
[epoch15, step92]: loss 0.522456
[epoch15, step93]: loss 0.403273
[epoch15, step94]: loss 0.561468
[epoch15, step95]: loss 0.577621
[epoch15, step96]: loss 0.494915
[epoch15, step97]: loss 0.292730
[epoch15, step98]: loss 0.482246
[epoch15, step99]: loss 0.471574
[epoch15, step100]: loss 0.599750
[epoch15, step101]: loss 0.628745
[epoch15, step102]: loss 0.454736
[epoch15, step103]: loss 0.390327
[epoch15, step104]: loss 0.354832
[epoch15, step105]: loss 0.570669
[epoch15, step106]: loss 0.337981
[epoch15, step107]: loss 0.335588
[epoch15, step108]: loss 0.485978
[epoch15, step109]: loss 0.486115
[epoch15, step110]: loss 0.388606
[epoch15, step111]: loss 0.544115
[epoch15, step112]: loss 0.471046
[epoch15, step113]: loss 0.727073
[epoch15, step114]: loss 0.323896
[epoch15, step115]: loss 0.684313
[epoch15, step116]: loss 0.356593
[epoch15, step117]: loss 0.610758
[epoch15, step118]: loss 0.506004
[epoch15, step119]: loss 0.580851
[epoch15, step120]: loss 0.494014
[epoch15, step121]: loss 0.460723
[epoch15, step122]: loss 0.515304
[epoch15, step123]: loss 0.524332
[epoch15, step124]: loss 0.599242
[epoch15, step125]: loss 0.630236
[epoch15, step126]: loss 0.683804
[epoch15, step127]: loss 0.421305
[epoch15, step128]: loss 0.718655
[epoch15, step129]: loss 0.424530
[epoch15, step130]: loss 0.532750
[epoch15, step131]: loss 0.359488
[epoch15, step132]: loss 0.481344
[epoch15, step133]: loss 0.622529
[epoch15, step134]: loss 0.463676
[epoch15, step135]: loss 0.382294
[epoch15, step136]: loss 0.466122
[epoch15, step137]: loss 0.342155
[epoch15, step138]: loss 0.633844
[epoch15, step139]: loss 0.405446
[epoch15, step140]: loss 0.511365
[epoch15, step141]: loss 0.756528
[epoch15, step142]: loss 0.473129
[epoch15, step143]: loss 0.606522
[epoch15, step144]: loss 0.562772
[epoch15, step145]: loss 0.594066
[epoch15, step146]: loss 0.217973
[epoch15, step147]: loss 0.544242
[epoch15, step148]: loss 0.296031
[epoch15, step149]: loss 0.137009
[epoch15, step150]: loss 0.233622
[epoch15, step151]: loss 0.608487
[epoch15, step152]: loss 0.576239
[epoch15, step153]: loss 0.633316
[epoch15, step154]: loss 0.458561
[epoch15, step155]: loss 0.379734
[epoch15, step156]: loss 0.431754
[epoch15, step157]: loss 0.597247
[epoch15, step158]: loss 0.256074
[epoch15, step159]: loss 0.354124
[epoch15, step160]: loss 0.633811
[epoch15, step161]: loss 0.356966
[epoch15, step162]: loss 0.540574
[epoch15, step163]: loss 0.614613
[epoch15, step164]: loss 0.542241
[epoch15, step165]: loss 0.567378
[epoch15, step166]: loss 0.698214
[epoch15, step167]: loss 0.447631
[epoch15, step168]: loss 0.486645
[epoch15, step169]: loss 0.521078
[epoch15, step170]: loss 0.322574
[epoch15, step171]: loss 0.501165
[epoch15, step172]: loss 0.366576
[epoch15, step173]: loss 0.365307
[epoch15, step174]: loss 0.580234
[epoch15, step175]: loss 0.491812
[epoch15, step176]: loss 0.334295
[epoch15, step177]: loss 0.391047
[epoch15, step178]: loss 0.672592
[epoch15, step179]: loss 0.548334
[epoch15, step180]: loss 0.528665
[epoch15, step181]: loss 0.449129
[epoch15, step182]: loss 0.462068
[epoch15, step183]: loss 0.687373
[epoch15, step184]: loss 0.388625
[epoch15, step185]: loss 0.349860
[epoch15, step186]: loss 0.730747
[epoch15, step187]: loss 0.634425
[epoch15, step188]: loss 0.604297
[epoch15, step189]: loss 0.317716
[epoch15, step190]: loss 0.631501
[epoch15, step191]: loss 0.645140
[epoch15, step192]: loss 0.420545
[epoch15, step193]: loss 0.600892
[epoch15, step194]: loss 0.690800
[epoch15, step195]: loss 0.669806
[epoch15, step196]: loss 0.505152
[epoch15, step197]: loss 0.175690
[epoch15, step198]: loss 0.463891
[epoch15, step199]: loss 0.276074
[epoch15, step200]: loss 0.549761
[epoch15, step201]: loss 0.452555
[epoch15, step202]: loss 0.492154
[epoch15, step203]: loss 0.523354
[epoch15, step204]: loss 0.430675
[epoch15, step205]: loss 0.517642
[epoch15, step206]: loss 0.358035
[epoch15, step207]: loss 0.568383
[epoch15, step208]: loss 0.449555
[epoch15, step209]: loss 0.520516
[epoch15, step210]: loss 0.461021
[epoch15, step211]: loss 0.281142
[epoch15, step212]: loss 0.484817
[epoch15, step213]: loss 0.269425
[epoch15, step214]: loss 0.564230
[epoch15, step215]: loss 0.503115
[epoch15, step216]: loss 0.519902
[epoch15, step217]: loss 0.485984
[epoch15, step218]: loss 0.617544
[epoch15, step219]: loss 0.719565
[epoch15, step220]: loss 0.543024
[epoch15, step221]: loss 0.631693
[epoch15, step222]: loss 0.448346
[epoch15, step223]: loss 0.512631
[epoch15, step224]: loss 0.450722
[epoch15, step225]: loss 0.623722
[epoch15, step226]: loss 0.555426
[epoch15, step227]: loss 0.282357
[epoch15, step228]: loss 0.353946
[epoch15, step229]: loss 0.671866
[epoch15, step230]: loss 0.258583
[epoch15, step231]: loss 0.401351
[epoch15, step232]: loss 0.351833
[epoch15, step233]: loss 0.565809
[epoch15, step234]: loss 0.580514
[epoch15, step235]: loss 0.520906
[epoch15, step236]: loss 0.452757
[epoch15, step237]: loss 0.706056
[epoch15, step238]: loss 0.399005
[epoch15, step239]: loss 0.704570
[epoch15, step240]: loss 0.529972
[epoch15, step241]: loss 0.454110
[epoch15, step242]: loss 0.612485
[epoch15, step243]: loss 0.618468
[epoch15, step244]: loss 0.731357
[epoch15, step245]: loss 0.446914
[epoch15, step246]: loss 0.660603
[epoch15, step247]: loss 0.561963
[epoch15, step248]: loss 0.507378
[epoch15, step249]: loss 0.488258
[epoch15, step250]: loss 0.660747
[epoch15, step251]: loss 0.655061
[epoch15, step252]: loss 0.472306
[epoch15, step253]: loss 0.388949
[epoch15, step254]: loss 0.354891
[epoch15, step255]: loss 0.425543
[epoch15, step256]: loss 0.260284
[epoch15, step257]: loss 0.404888
[epoch15, step258]: loss 0.372321
[epoch15, step259]: loss 0.346493
[epoch15, step260]: loss 0.412026
[epoch15, step261]: loss 0.524638
[epoch15, step262]: loss 0.358126
[epoch15, step263]: loss 0.382500
[epoch15, step264]: loss 0.456779
[epoch15, step265]: loss 0.540398
[epoch15, step266]: loss 0.319374
[epoch15, step267]: loss 0.429528
[epoch15, step268]: loss 0.512464
[epoch15, step269]: loss 0.535245
[epoch15, step270]: loss 0.449617
[epoch15, step271]: loss 0.356273
[epoch15, step272]: loss 0.352568
[epoch15, step273]: loss 0.254733
[epoch15, step274]: loss 0.692133
[epoch15, step275]: loss 0.578144
[epoch15, step276]: loss 0.586157
[epoch15, step277]: loss 0.346695
[epoch15, step278]: loss 0.602352
[epoch15, step279]: loss 0.264417
[epoch15, step280]: loss 0.548023
[epoch15, step281]: loss 0.489579
[epoch15, step282]: loss 0.432276
[epoch15, step283]: loss 0.533381
[epoch15, step284]: loss 0.551247
[epoch15, step285]: loss 0.532136
[epoch15, step286]: loss 0.299116
[epoch15, step287]: loss 0.603317
[epoch15, step288]: loss 0.489353
[epoch15, step289]: loss 0.504385
[epoch15, step290]: loss 0.616934
[epoch15, step291]: loss 0.584450
[epoch15, step292]: loss 0.251198
[epoch15, step293]: loss 0.632958
[epoch15, step294]: loss 0.357106
[epoch15, step295]: loss 0.569859
[epoch15, step296]: loss 0.141565
[epoch15, step297]: loss 0.232522
[epoch15, step298]: loss 0.235161
[epoch15, step299]: loss 0.590376
[epoch15, step300]: loss 0.510826
[epoch15, step301]: loss 0.513229
[epoch15, step302]: loss 0.473998
[epoch15, step303]: loss 0.644052
[epoch15, step304]: loss 0.424831
[epoch15, step305]: loss 0.549966
[epoch15, step306]: loss 0.463386
[epoch15, step307]: loss 0.622800
[epoch15, step308]: loss 0.581179
[epoch15, step309]: loss 0.381018
[epoch15, step310]: loss 0.438764
[epoch15, step311]: loss 0.328002
[epoch15, step312]: loss 0.669310
[epoch15, step313]: loss 0.404117
[epoch15, step314]: loss 0.626744
[epoch15, step315]: loss 0.282934
[epoch15, step316]: loss 0.403485
[epoch15, step317]: loss 0.469744
[epoch15, step318]: loss 0.449426
[epoch15, step319]: loss 0.355102
[epoch15, step320]: loss 0.570096
[epoch15, step321]: loss 0.504563
[epoch15, step322]: loss 0.545252
[epoch15, step323]: loss 0.316693
[epoch15, step324]: loss 0.591534
[epoch15, step325]: loss 0.578645
[epoch15, step326]: loss 0.312865
[epoch15, step327]: loss 0.474290
[epoch15, step328]: loss 0.402578
[epoch15, step329]: loss 0.460651
[epoch15, step330]: loss 0.361844
[epoch15, step331]: loss 0.519401
[epoch15, step332]: loss 0.733022
[epoch15, step333]: loss 0.407892
[epoch15, step334]: loss 0.512775
[epoch15, step335]: loss 0.496451
[epoch15, step336]: loss 0.512755
[epoch15, step337]: loss 0.491693
[epoch15, step338]: loss 0.716574
[epoch15, step339]: loss 0.544147
[epoch15, step340]: loss 0.301849
[epoch15, step341]: loss 0.561622
[epoch15, step342]: loss 0.462116
[epoch15, step343]: loss 0.573158
[epoch15, step344]: loss 0.377772
[epoch15, step345]: loss 0.476350
[epoch15, step346]: loss 0.431481
[epoch15, step347]: loss 0.499991
[epoch15, step348]: loss 0.526979
[epoch15, step349]: loss 0.411164
[epoch15, step350]: loss 0.427226
[epoch15, step351]: loss 0.812666
[epoch15, step352]: loss 0.479600
[epoch15, step353]: loss 0.490866
[epoch15, step354]: loss 0.369797
[epoch15, step355]: loss 0.278937
[epoch15, step356]: loss 0.791789
[epoch15, step357]: loss 0.575502
[epoch15, step358]: loss 0.431924
[epoch15, step359]: loss 0.426093
[epoch15, step360]: loss 0.555033
[epoch15, step361]: loss 0.616055
[epoch15, step362]: loss 0.350419
[epoch15, step363]: loss 0.626566
[epoch15, step364]: loss 0.609714
[epoch15, step365]: loss 0.752960
[epoch15, step366]: loss 0.481304
[epoch15, step367]: loss 0.202729
[epoch15, step368]: loss 0.551208
[epoch15, step369]: loss 0.524640
[epoch15, step370]: loss 0.793780
[epoch15, step371]: loss 0.661593
[epoch15, step372]: loss 0.491206
[epoch15, step373]: loss 0.630055
[epoch15, step374]: loss 0.544207
[epoch15, step375]: loss 0.528918
[epoch15, step376]: loss 0.667344
[epoch15, step377]: loss 0.463600
[epoch15, step378]: loss 0.467803
[epoch15, step379]: loss 0.439459
[epoch15, step380]: loss 0.454990
[epoch15, step381]: loss 0.536413
[epoch15, step382]: loss 0.501942
[epoch15, step383]: loss 0.381905
[epoch15, step384]: loss 0.531649
[epoch15, step385]: loss 0.609427
[epoch15, step386]: loss 0.497755
[epoch15, step387]: loss 0.396740
[epoch15, step388]: loss 0.235190
[epoch15, step389]: loss 0.515001
[epoch15, step390]: loss 0.417171
[epoch15, step391]: loss 0.389347
[epoch15, step392]: loss 0.569067
[epoch15, step393]: loss 0.678152
[epoch15, step394]: loss 0.470551
[epoch15, step395]: loss 0.438604
[epoch15, step396]: loss 0.537332
[epoch15, step397]: loss 0.485620
[epoch15, step398]: loss 0.555426
[epoch15, step399]: loss 0.445079
[epoch15, step400]: loss 0.382734
[epoch15, step401]: loss 0.555355
[epoch15, step402]: loss 0.597131
[epoch15, step403]: loss 0.517527
[epoch15, step404]: loss 0.560702
[epoch15, step405]: loss 0.374897
[epoch15, step406]: loss 0.672825
[epoch15, step407]: loss 0.240388
[epoch15, step408]: loss 0.594765
[epoch15, step409]: loss 0.387869
[epoch15, step410]: loss 0.453464
[epoch15, step411]: loss 0.558222
[epoch15, step412]: loss 0.753979
[epoch15, step413]: loss 0.487472
[epoch15, step414]: loss 0.513429
[epoch15, step415]: loss 0.527572
[epoch15, step416]: loss 0.457755
[epoch15, step417]: loss 0.582754
[epoch15, step418]: loss 0.528210
[epoch15, step419]: loss 0.643121
[epoch15, step420]: loss 0.453516
[epoch15, step421]: loss 0.600465
[epoch15, step422]: loss 0.454100
[epoch15, step423]: loss 0.457946
[epoch15, step424]: loss 0.505633
[epoch15, step425]: loss 0.469131
[epoch15, step426]: loss 0.493624
[epoch15, step427]: loss 0.334340
[epoch15, step428]: loss 0.610902
[epoch15, step429]: loss 0.173516
[epoch15, step430]: loss 0.361568
[epoch15, step431]: loss 0.307561
[epoch15, step432]: loss 0.501977
[epoch15, step433]: loss 0.447904
[epoch15, step434]: loss 0.763343
[epoch15, step435]: loss 0.200416
[epoch15, step436]: loss 0.304277
[epoch15, step437]: loss 0.430597
[epoch15, step438]: loss 0.461499
[epoch15, step439]: loss 0.680825
[epoch15, step440]: loss 0.335899
[epoch15, step441]: loss 0.510040
[epoch15, step442]: loss 0.443313
[epoch15, step443]: loss 0.660839
[epoch15, step444]: loss 0.413097
[epoch15, step445]: loss 0.453465
[epoch15, step446]: loss 0.615091
[epoch15, step447]: loss 0.422230
[epoch15, step448]: loss 0.446783
[epoch15, step449]: loss 0.307388
[epoch15, step450]: loss 0.537139
[epoch15, step451]: loss 0.518249
[epoch15, step452]: loss 0.266731
[epoch15, step453]: loss 0.621758
[epoch15, step454]: loss 0.467844
[epoch15, step455]: loss 0.604179
[epoch15, step456]: loss 0.653885
[epoch15, step457]: loss 0.680839
[epoch15, step458]: loss 0.508475
[epoch15, step459]: loss 0.548625
[epoch15, step460]: loss 0.451157
[epoch15, step461]: loss 0.378567
[epoch15, step462]: loss 0.265905
[epoch15, step463]: loss 0.590884
[epoch15, step464]: loss 0.240680
[epoch15, step465]: loss 0.417940
[epoch15, step466]: loss 0.491531
[epoch15, step467]: loss 0.369918
[epoch15, step468]: loss 0.321145
[epoch15, step469]: loss 0.292237
[epoch15, step470]: loss 0.388168
[epoch15, step471]: loss 0.499425
[epoch15, step472]: loss 0.227434
[epoch15, step473]: loss 0.269005
[epoch15, step474]: loss 0.433705
[epoch15, step475]: loss 0.542952
[epoch15, step476]: loss 0.464044
[epoch15, step477]: loss 0.475516
[epoch15, step478]: loss 0.335750
[epoch15, step479]: loss 0.325086
[epoch15, step480]: loss 0.401647
[epoch15, step481]: loss 0.304738
[epoch15, step482]: loss 0.501378
[epoch15, step483]: loss 0.538486
[epoch15, step484]: loss 0.428185
[epoch15, step485]: loss 0.633902
[epoch15, step486]: loss 0.528762
[epoch15, step487]: loss 0.529778
[epoch15, step488]: loss 0.507130
[epoch15, step489]: loss 0.424747
[epoch15, step490]: loss 0.442111
[epoch15, step491]: loss 0.428508
[epoch15, step492]: loss 0.599363
[epoch15, step493]: loss 0.658611
[epoch15, step494]: loss 0.588764
[epoch15, step495]: loss 0.790690
[epoch15, step496]: loss 0.215421
[epoch15, step497]: loss 0.245600
[epoch15, step498]: loss 0.687159
[epoch15, step499]: loss 0.332003
[epoch15, step500]: loss 0.277653
[epoch15, step501]: loss 0.698951
[epoch15, step502]: loss 0.375250
[epoch15, step503]: loss 0.477951
[epoch15, step504]: loss 0.349864
[epoch15, step505]: loss 0.646734
[epoch15, step506]: loss 0.503421
[epoch15, step507]: loss 0.533635
[epoch15, step508]: loss 0.507872
[epoch15, step509]: loss 0.401625
[epoch15, step510]: loss 0.464043
[epoch15, step511]: loss 0.745179
[epoch15, step512]: loss 0.413528
[epoch15, step513]: loss 0.537985
[epoch15, step514]: loss 0.646043
[epoch15, step515]: loss 0.482713
[epoch15, step516]: loss 0.538636
[epoch15, step517]: loss 0.428956
[epoch15, step518]: loss 0.516456
[epoch15, step519]: loss 0.315105
[epoch15, step520]: loss 0.564852
[epoch15, step521]: loss 0.676550
[epoch15, step522]: loss 0.390166
[epoch15, step523]: loss 0.555463
[epoch15, step524]: loss 0.651659
[epoch15, step525]: loss 0.330895
[epoch15, step526]: loss 0.324581
[epoch15, step527]: loss 0.618558
[epoch15, step528]: loss 0.542318
[epoch15, step529]: loss 0.247884
[epoch15, step530]: loss 0.470218
[epoch15, step531]: loss 0.398262
[epoch15, step532]: loss 0.590019
[epoch15, step533]: loss 0.406238
[epoch15, step534]: loss 0.354343
[epoch15, step535]: loss 0.747052
[epoch15, step536]: loss 0.556311
[epoch15, step537]: loss 0.490593
[epoch15, step538]: loss 0.558554
[epoch15, step539]: loss 0.499666
[epoch15, step540]: loss 0.485708
[epoch15, step541]: loss 0.354917
[epoch15, step542]: loss 0.512892
[epoch15, step543]: loss 0.422874
[epoch15, step544]: loss 0.385704
[epoch15, step545]: loss 0.527678
[epoch15, step546]: loss 0.442510
[epoch15, step547]: loss 0.376210
[epoch15, step548]: loss 0.363429
[epoch15, step549]: loss 0.388573
[epoch15, step550]: loss 0.430492
[epoch15, step551]: loss 0.593642
[epoch15, step552]: loss 0.257822
[epoch15, step553]: loss 0.422450
[epoch15, step554]: loss 0.352939
[epoch15, step555]: loss 0.315557
[epoch15, step556]: loss 0.149204
[epoch15, step557]: loss 0.522633
[epoch15, step558]: loss 0.561647
[epoch15, step559]: loss 0.482145
[epoch15, step560]: loss 0.478012
[epoch15, step561]: loss 0.396779
[epoch15, step562]: loss 0.424006
[epoch15, step563]: loss 0.430993
[epoch15, step564]: loss 0.491909
[epoch15, step565]: loss 0.274607
[epoch15, step566]: loss 0.534362
[epoch15, step567]: loss 0.339989
[epoch15, step568]: loss 0.515955
[epoch15, step569]: loss 0.411173
[epoch15, step570]: loss 0.301208
[epoch15, step571]: loss 0.585125
[epoch15, step572]: loss 0.628159
[epoch15, step573]: loss 0.407114
[epoch15, step574]: loss 0.372585
[epoch15, step575]: loss 0.429850
[epoch15, step576]: loss 0.460807
[epoch15, step577]: loss 0.460402
[epoch15, step578]: loss 0.507079
[epoch15, step579]: loss 0.363122
[epoch15, step580]: loss 0.647746
[epoch15, step581]: loss 0.455597
[epoch15, step582]: loss 0.260641
[epoch15, step583]: loss 0.453755
[epoch15, step584]: loss 0.435904
[epoch15, step585]: loss 0.605406
[epoch15, step586]: loss 0.476044
[epoch15, step587]: loss 0.534175
[epoch15, step588]: loss 0.401067
[epoch15, step589]: loss 0.373760
[epoch15, step590]: loss 0.411681
[epoch15, step591]: loss 0.677110
[epoch15, step592]: loss 0.258876
[epoch15, step593]: loss 0.236319
[epoch15, step594]: loss 0.368768
[epoch15, step595]: loss 0.600806
[epoch15, step596]: loss 0.654045
[epoch15, step597]: loss 0.640238
[epoch15, step598]: loss 0.524549
[epoch15, step599]: loss 0.635829
[epoch15, step600]: loss 0.267739
[epoch15, step601]: loss 0.617290
[epoch15, step602]: loss 0.427825
[epoch15, step603]: loss 0.587030
[epoch15, step604]: loss 0.446312
[epoch15, step605]: loss 0.559824
[epoch15, step606]: loss 0.310846
[epoch15, step607]: loss 0.359143
[epoch15, step608]: loss 0.725692
[epoch15, step609]: loss 0.575656
[epoch15, step610]: loss 0.638076
[epoch15, step611]: loss 0.242770
[epoch15, step612]: loss 0.281954
[epoch15, step613]: loss 0.514285
[epoch15, step614]: loss 0.548972
[epoch15, step615]: loss 0.605607
[epoch15, step616]: loss 0.518619
[epoch15, step617]: loss 0.483724
[epoch15, step618]: loss 0.401169
[epoch15, step619]: loss 0.549195
[epoch15, step620]: loss 0.278106
[epoch15, step621]: loss 0.460900
[epoch15, step622]: loss 0.454693
[epoch15, step623]: loss 0.510601
[epoch15, step624]: loss 0.546164
[epoch15, step625]: loss 0.357099
[epoch15, step626]: loss 0.507285
[epoch15, step627]: loss 0.323789
[epoch15, step628]: loss 0.346739
[epoch15, step629]: loss 0.307962
[epoch15, step630]: loss 0.526619
[epoch15, step631]: loss 0.449057
[epoch15, step632]: loss 0.668881
[epoch15, step633]: loss 0.507278
[epoch15, step634]: loss 0.491912
[epoch15, step635]: loss 0.497894
[epoch15, step636]: loss 0.711398
[epoch15, step637]: loss 0.590369
[epoch15, step638]: loss 0.565448
[epoch15, step639]: loss 0.543342
[epoch15, step640]: loss 0.351352
[epoch15, step641]: loss 0.557083
[epoch15, step642]: loss 0.329558
[epoch15, step643]: loss 0.523825
[epoch15, step644]: loss 0.399581
[epoch15, step645]: loss 0.288398
[epoch15, step646]: loss 0.592736
[epoch15, step647]: loss 0.692112
[epoch15, step648]: loss 0.310261
[epoch15, step649]: loss 0.253152
[epoch15, step650]: loss 0.610033
[epoch15, step651]: loss 0.414040
[epoch15, step652]: loss 0.420767
[epoch15, step653]: loss 0.568448
[epoch15, step654]: loss 0.262928
[epoch15, step655]: loss 0.634041
[epoch15, step656]: loss 0.378151
[epoch15, step657]: loss 0.225446
[epoch15, step658]: loss 0.758863
[epoch15, step659]: loss 0.418699
[epoch15, step660]: loss 0.377782
[epoch15, step661]: loss 0.553974
[epoch15, step662]: loss 0.393992
[epoch15, step663]: loss 0.436414
[epoch15, step664]: loss 0.668194
[epoch15, step665]: loss 0.240483
[epoch15, step666]: loss 0.479960
[epoch15, step667]: loss 0.594891
[epoch15, step668]: loss 0.431564
[epoch15, step669]: loss 0.504761
[epoch15, step670]: loss 0.420554
[epoch15, step671]: loss 0.520757
[epoch15, step672]: loss 0.611009
[epoch15, step673]: loss 0.599284
[epoch15, step674]: loss 0.706916
[epoch15, step675]: loss 0.442604
[epoch15, step676]: loss 0.412495
[epoch15, step677]: loss 0.605491
[epoch15, step678]: loss 0.512870
[epoch15, step679]: loss 0.492126
[epoch15, step680]: loss 0.334535
[epoch15, step681]: loss 0.239453
[epoch15, step682]: loss 0.730410
[epoch15, step683]: loss 0.268013
[epoch15, step684]: loss 0.294294
[epoch15, step685]: loss 0.687312
[epoch15, step686]: loss 0.413390
[epoch15, step687]: loss 0.509628
[epoch15, step688]: loss 0.456798
[epoch15, step689]: loss 0.538825
[epoch15, step690]: loss 0.375491
[epoch15, step691]: loss 0.491841
[epoch15, step692]: loss 0.439649
[epoch15, step693]: loss 0.533543
[epoch15, step694]: loss 0.244460
[epoch15, step695]: loss 0.419959
[epoch15, step696]: loss 0.439430
[epoch15, step697]: loss 0.574585
[epoch15, step698]: loss 0.271829
[epoch15, step699]: loss 0.483056
[epoch15, step700]: loss 0.585133
[epoch15, step701]: loss 0.572086
[epoch15, step702]: loss 0.704634
[epoch15, step703]: loss 0.243598
[epoch15, step704]: loss 0.584183
[epoch15, step705]: loss 0.266287
[epoch15, step706]: loss 0.483099
[epoch15, step707]: loss 0.442668
[epoch15, step708]: loss 0.608225
[epoch15, step709]: loss 0.305161
[epoch15, step710]: loss 0.339353
[epoch15, step711]: loss 0.535118
[epoch15, step712]: loss 0.627330
[epoch15, step713]: loss 0.563981
[epoch15, step714]: loss 0.274756
[epoch15, step715]: loss 0.605778
[epoch15, step716]: loss 0.666795
[epoch15, step717]: loss 0.606723
[epoch15, step718]: loss 0.337662
[epoch15, step719]: loss 0.664458
[epoch15, step720]: loss 0.533090
[epoch15, step721]: loss 0.407103
[epoch15, step722]: loss 0.493381
[epoch15, step723]: loss 0.315583
[epoch15, step724]: loss 0.420009
[epoch15, step725]: loss 0.440771
[epoch15, step726]: loss 0.473406
[epoch15, step727]: loss 0.429135
[epoch15, step728]: loss 0.678013
[epoch15, step729]: loss 0.520490
[epoch15, step730]: loss 0.472139
[epoch15, step731]: loss 0.476035
[epoch15, step732]: loss 0.666493
[epoch15, step733]: loss 0.321026
[epoch15, step734]: loss 0.263816
[epoch15, step735]: loss 0.488673
[epoch15, step736]: loss 0.557245
[epoch15, step737]: loss 0.592418
[epoch15, step738]: loss 0.101123
[epoch15, step739]: loss 0.426381
[epoch15, step740]: loss 0.496506
[epoch15, step741]: loss 0.498532
[epoch15, step742]: loss 0.622659
[epoch15, step743]: loss 0.533964
[epoch15, step744]: loss 0.656684
[epoch15, step745]: loss 0.489367
[epoch15, step746]: loss 0.657751
[epoch15, step747]: loss 0.549710
[epoch15, step748]: loss 0.713223
[epoch15, step749]: loss 0.488621
[epoch15, step750]: loss 0.304466
[epoch15, step751]: loss 0.579093
[epoch15, step752]: loss 0.622415
[epoch15, step753]: loss 0.327683
[epoch15, step754]: loss 0.574416
[epoch15, step755]: loss 0.539493
[epoch15, step756]: loss 0.293757
[epoch15, step757]: loss 0.410277
[epoch15, step758]: loss 0.701528
[epoch15, step759]: loss 0.419182
[epoch15, step760]: loss 0.546064
[epoch15, step761]: loss 0.522683
[epoch15, step762]: loss 0.307888
[epoch15, step763]: loss 0.540482
[epoch15, step764]: loss 0.648784
[epoch15, step765]: loss 0.256862
[epoch15, step766]: loss 0.636039
[epoch15, step767]: loss 0.545563
[epoch15, step768]: loss 0.335058
[epoch15, step769]: loss 0.397155
[epoch15, step770]: loss 0.454953
[epoch15, step771]: loss 0.475117
[epoch15, step772]: loss 0.312370
[epoch15, step773]: loss 0.574544
[epoch15, step774]: loss 0.350818
[epoch15, step775]: loss 0.642355
[epoch15, step776]: loss 0.460132
[epoch15, step777]: loss 0.707746
[epoch15, step778]: loss 0.650833
[epoch15, step779]: loss 0.704596
[epoch15, step780]: loss 0.644854
[epoch15, step781]: loss 0.566181
[epoch15, step782]: loss 0.511261
[epoch15, step783]: loss 0.260750
[epoch15, step784]: loss 0.587661
[epoch15, step785]: loss 0.571099
[epoch15, step786]: loss 0.468442
[epoch15, step787]: loss 0.346971
[epoch15, step788]: loss 0.387221
[epoch15, step789]: loss 0.435794
[epoch15, step790]: loss 0.443963
[epoch15, step791]: loss 0.412112
[epoch15, step792]: loss 0.462176
[epoch15, step793]: loss 0.538614
[epoch15, step794]: loss 0.623801
[epoch15, step795]: loss 0.540356
[epoch15, step796]: loss 0.410148
[epoch15, step797]: loss 0.401409
[epoch15, step798]: loss 0.489617
[epoch15, step799]: loss 0.164111
[epoch15, step800]: loss 0.518420
[epoch15, step801]: loss 0.496037
[epoch15, step802]: loss 0.616785
[epoch15, step803]: loss 0.349656
[epoch15, step804]: loss 0.548728
[epoch15, step805]: loss 0.627517
[epoch15, step806]: loss 0.532204
[epoch15, step807]: loss 0.440587
[epoch15, step808]: loss 0.456760
[epoch15, step809]: loss 0.679216
[epoch15, step810]: loss 0.521083
[epoch15, step811]: loss 0.376077
[epoch15, step812]: loss 0.520241
[epoch15, step813]: loss 0.671352
[epoch15, step814]: loss 0.565276
[epoch15, step815]: loss 0.615137
[epoch15, step816]: loss 0.270745
[epoch15, step817]: loss 0.714915
[epoch15, step818]: loss 0.491006
[epoch15, step819]: loss 0.343803
[epoch15, step820]: loss 0.396596
[epoch15, step821]: loss 0.336342
[epoch15, step822]: loss 0.461163
[epoch15, step823]: loss 0.477618
[epoch15, step824]: loss 0.493893
[epoch15, step825]: loss 0.348236
[epoch15, step826]: loss 0.513389
[epoch15, step827]: loss 0.537070
[epoch15, step828]: loss 0.368211
[epoch15, step829]: loss 0.476666
[epoch15, step830]: loss 0.576036
[epoch15, step831]: loss 0.671170
[epoch15, step832]: loss 0.627267
[epoch15, step833]: loss 0.378827
[epoch15, step834]: loss 0.511992
[epoch15, step835]: loss 0.413989
[epoch15, step836]: loss 0.568413
[epoch15, step837]: loss 0.522078
[epoch15, step838]: loss 0.556563
[epoch15, step839]: loss 0.503779
[epoch15, step840]: loss 0.564239
[epoch15, step841]: loss 0.640915
[epoch15, step842]: loss 0.491384
[epoch15, step843]: loss 0.499726
[epoch15, step844]: loss 0.693301
[epoch15, step845]: loss 0.305868
[epoch15, step846]: loss 0.238795
[epoch15, step847]: loss 0.622586
[epoch15, step848]: loss 0.541213
[epoch15, step849]: loss 0.398282
[epoch15, step850]: loss 0.596029
[epoch15, step851]: loss 0.597314
[epoch15, step852]: loss 0.476872
[epoch15, step853]: loss 0.626420
[epoch15, step854]: loss 0.440795
[epoch15, step855]: loss 0.337754
[epoch15, step856]: loss 0.500067
[epoch15, step857]: loss 0.356917
[epoch15, step858]: loss 0.266916
[epoch15, step859]: loss 0.302647
[epoch15, step860]: loss 0.492255
[epoch15, step861]: loss 0.345674
[epoch15, step862]: loss 0.589779
[epoch15, step863]: loss 0.270187
[epoch15, step864]: loss 0.407545
[epoch15, step865]: loss 0.321288
[epoch15, step866]: loss 0.519517
[epoch15, step867]: loss 0.421502
[epoch15, step868]: loss 0.447069
[epoch15, step869]: loss 0.369136
[epoch15, step870]: loss 0.499982
[epoch15, step871]: loss 0.297602
[epoch15, step872]: loss 0.254522
[epoch15, step873]: loss 0.428984
[epoch15, step874]: loss 0.390885
[epoch15, step875]: loss 0.644316
[epoch15, step876]: loss 0.501335
[epoch15, step877]: loss 0.591755
[epoch15, step878]: loss 0.428756
[epoch15, step879]: loss 0.661642
[epoch15, step880]: loss 0.465240
[epoch15, step881]: loss 0.476362
[epoch15, step882]: loss 0.319264
[epoch15, step883]: loss 0.419543
[epoch15, step884]: loss 0.561739
[epoch15, step885]: loss 0.515276
[epoch15, step886]: loss 0.549753
[epoch15, step887]: loss 0.357515
[epoch15, step888]: loss 0.695430
[epoch15, step889]: loss 0.652585
[epoch15, step890]: loss 0.455873
[epoch15, step891]: loss 0.339272
[epoch15, step892]: loss 0.565508
[epoch15, step893]: loss 0.494617
[epoch15, step894]: loss 0.248229
[epoch15, step895]: loss 0.432068
[epoch15, step896]: loss 0.533978
[epoch15, step897]: loss 0.597715
[epoch15, step898]: loss 0.288235
[epoch15, step899]: loss 0.484688
[epoch15, step900]: loss 0.492146
[epoch15, step901]: loss 0.505564
[epoch15, step902]: loss 0.632060
[epoch15, step903]: loss 0.585297
[epoch15, step904]: loss 0.526895
[epoch15, step905]: loss 0.509959
[epoch15, step906]: loss 0.371600
[epoch15, step907]: loss 0.388640
[epoch15, step908]: loss 0.432376
[epoch15, step909]: loss 0.593904
[epoch15, step910]: loss 0.402690
[epoch15, step911]: loss 0.400489
[epoch15, step912]: loss 0.514706
[epoch15, step913]: loss 0.357077
[epoch15, step914]: loss 0.532042
[epoch15, step915]: loss 0.571172
[epoch15, step916]: loss 0.628457
[epoch15, step917]: loss 0.568217
[epoch15, step918]: loss 0.439307
[epoch15, step919]: loss 0.570719
[epoch15, step920]: loss 0.591521
[epoch15, step921]: loss 0.244385
[epoch15, step922]: loss 0.480288
[epoch15, step923]: loss 0.343664
[epoch15, step924]: loss 0.674901
[epoch15, step925]: loss 0.114857
[epoch15, step926]: loss 0.487924
[epoch15, step927]: loss 0.427510
[epoch15, step928]: loss 0.575585
[epoch15, step929]: loss 0.522899
[epoch15, step930]: loss 0.446988
[epoch15, step931]: loss 0.470362
[epoch15, step932]: loss 0.350564
[epoch15, step933]: loss 0.199092
[epoch15, step934]: loss 0.698314
[epoch15, step935]: loss 0.311998
[epoch15, step936]: loss 0.465774
[epoch15, step937]: loss 0.563219
[epoch15, step938]: loss 0.410107
[epoch15, step939]: loss 0.470250
[epoch15, step940]: loss 0.626276
[epoch15, step941]: loss 0.318200
[epoch15, step942]: loss 0.507059
[epoch15, step943]: loss 0.490665
[epoch15, step944]: loss 0.568857
[epoch15, step945]: loss 0.488982
[epoch15, step946]: loss 0.573629
[epoch15, step947]: loss 0.512933
[epoch15, step948]: loss 0.375621
[epoch15, step949]: loss 0.485799
[epoch15, step950]: loss 0.505490
[epoch15, step951]: loss 0.212375
[epoch15, step952]: loss 0.382675
[epoch15, step953]: loss 0.345966
[epoch15, step954]: loss 0.476524
[epoch15, step955]: loss 0.349573
[epoch15, step956]: loss 0.376533
[epoch15, step957]: loss 0.343262
[epoch15, step958]: loss 0.577847
[epoch15, step959]: loss 0.377994
[epoch15, step960]: loss 0.370691
[epoch15, step961]: loss 0.726251
[epoch15, step962]: loss 0.540871
[epoch15, step963]: loss 0.285168
[epoch15, step964]: loss 0.630765
[epoch15, step965]: loss 0.643890
[epoch15, step966]: loss 0.649102
[epoch15, step967]: loss 0.620622
[epoch15, step968]: loss 0.241189
[epoch15, step969]: loss 0.442616
[epoch15, step970]: loss 0.251300
[epoch15, step971]: loss 0.324100
[epoch15, step972]: loss 0.318549
[epoch15, step973]: loss 0.579948
[epoch15, step974]: loss 0.333312
[epoch15, step975]: loss 0.407208
[epoch15, step976]: loss 0.302496
[epoch15, step977]: loss 0.522506
[epoch15, step978]: loss 0.467356
[epoch15, step979]: loss 0.500285
[epoch15, step980]: loss 0.620809
[epoch15, step981]: loss 0.421918
[epoch15, step982]: loss 0.377831
[epoch15, step983]: loss 0.603352
[epoch15, step984]: loss 0.448039
[epoch15, step985]: loss 0.502051
[epoch15, step986]: loss 0.288941
[epoch15, step987]: loss 0.407468
[epoch15, step988]: loss 0.492729
[epoch15, step989]: loss 0.296664
[epoch15, step990]: loss 0.530523
[epoch15, step991]: loss 0.402859
[epoch15, step992]: loss 0.325506
[epoch15, step993]: loss 0.419560
[epoch15, step994]: loss 0.445274
[epoch15, step995]: loss 0.533959
[epoch15, step996]: loss 0.398372
[epoch15, step997]: loss 0.237900
[epoch15, step998]: loss 0.417891
[epoch15, step999]: loss 0.464276
[epoch15, step1000]: loss 0.636336
[epoch15, step1001]: loss 0.525442
[epoch15, step1002]: loss 0.488207
[epoch15, step1003]: loss 0.557836
[epoch15, step1004]: loss 0.566047
[epoch15, step1005]: loss 0.424197
[epoch15, step1006]: loss 0.429284
[epoch15, step1007]: loss 0.482210
[epoch15, step1008]: loss 0.416228
[epoch15, step1009]: loss 0.557725
[epoch15, step1010]: loss 0.291946
[epoch15, step1011]: loss 0.444479
[epoch15, step1012]: loss 0.116394
[epoch15, step1013]: loss 0.358810
[epoch15, step1014]: loss 0.336694
[epoch15, step1015]: loss 0.742839
[epoch15, step1016]: loss 0.557280
[epoch15, step1017]: loss 0.144086
[epoch15, step1018]: loss 0.390765
[epoch15, step1019]: loss 0.502503
[epoch15, step1020]: loss 0.589416
[epoch15, step1021]: loss 0.367815
[epoch15, step1022]: loss 0.448411
[epoch15, step1023]: loss 0.554125
[epoch15, step1024]: loss 0.300831
[epoch15, step1025]: loss 0.514457
[epoch15, step1026]: loss 0.611621
[epoch15, step1027]: loss 0.500964
[epoch15, step1028]: loss 0.456911
[epoch15, step1029]: loss 0.522565
[epoch15, step1030]: loss 0.447855
[epoch15, step1031]: loss 0.631947
[epoch15, step1032]: loss 0.432811
[epoch15, step1033]: loss 0.543996
[epoch15, step1034]: loss 0.632146
[epoch15, step1035]: loss 0.233565
[epoch15, step1036]: loss 0.442393
[epoch15, step1037]: loss 0.760239
[epoch15, step1038]: loss 0.428029
[epoch15, step1039]: loss 0.726803
[epoch15, step1040]: loss 0.766416
[epoch15, step1041]: loss 0.472608
[epoch15, step1042]: loss 0.518324
[epoch15, step1043]: loss 0.581329
[epoch15, step1044]: loss 0.360020
[epoch15, step1045]: loss 0.561916
[epoch15, step1046]: loss 0.267375
[epoch15, step1047]: loss 0.571013
[epoch15, step1048]: loss 0.647772
[epoch15, step1049]: loss 0.359298
[epoch15, step1050]: loss 0.472518
[epoch15, step1051]: loss 0.545560
[epoch15, step1052]: loss 0.500604
[epoch15, step1053]: loss 0.270336
[epoch15, step1054]: loss 0.530146
[epoch15, step1055]: loss 0.529240
[epoch15, step1056]: loss 0.362040
[epoch15, step1057]: loss 0.288075
[epoch15, step1058]: loss 0.539832
[epoch15, step1059]: loss 0.534348
[epoch15, step1060]: loss 0.497380
[epoch15, step1061]: loss 0.232064
[epoch15, step1062]: loss 0.460633
[epoch15, step1063]: loss 0.573373
[epoch15, step1064]: loss 0.520324
[epoch15, step1065]: loss 0.403616
[epoch15, step1066]: loss 0.546920
[epoch15, step1067]: loss 0.763592
[epoch15, step1068]: loss 0.464253
[epoch15, step1069]: loss 0.554108
[epoch15, step1070]: loss 0.626027
[epoch15, step1071]: loss 0.155414
[epoch15, step1072]: loss 0.440274
[epoch15, step1073]: loss 0.342198
[epoch15, step1074]: loss 0.472691
[epoch15, step1075]: loss 0.399475
[epoch15, step1076]: loss 0.298538
[epoch15, step1077]: loss 0.422583
[epoch15, step1078]: loss 0.444769
[epoch15, step1079]: loss 0.585587
[epoch15, step1080]: loss 0.334521
[epoch15, step1081]: loss 0.419052
[epoch15, step1082]: loss 0.688410
[epoch15, step1083]: loss 0.571514
[epoch15, step1084]: loss 0.544182
[epoch15, step1085]: loss 0.361717
[epoch15, step1086]: loss 0.090244
[epoch15, step1087]: loss 0.610220
[epoch15, step1088]: loss 0.389522
[epoch15, step1089]: loss 0.524444
[epoch15, step1090]: loss 0.493470
[epoch15, step1091]: loss 0.442980
[epoch15, step1092]: loss 0.521311
[epoch15, step1093]: loss 0.360704
[epoch15, step1094]: loss 0.354219
[epoch15, step1095]: loss 0.416997
[epoch15, step1096]: loss 0.426577
[epoch15, step1097]: loss 0.539603
[epoch15, step1098]: loss 0.727443
[epoch15, step1099]: loss 0.487964
[epoch15, step1100]: loss 0.480342
[epoch15, step1101]: loss 0.560501
[epoch15, step1102]: loss 0.721885
[epoch15, step1103]: loss 0.403260
[epoch15, step1104]: loss 0.415880
[epoch15, step1105]: loss 0.679604
[epoch15, step1106]: loss 0.445315
[epoch15, step1107]: loss 0.355811
[epoch15, step1108]: loss 0.432115
[epoch15, step1109]: loss 0.669447
[epoch15, step1110]: loss 0.505532
[epoch15, step1111]: loss 0.555148
[epoch15, step1112]: loss 0.115326
[epoch15, step1113]: loss 0.398422
[epoch15, step1114]: loss 0.460513
[epoch15, step1115]: loss 0.511816
[epoch15, step1116]: loss 0.347856
[epoch15, step1117]: loss 0.465173
[epoch15, step1118]: loss 0.496351
[epoch15, step1119]: loss 0.479718
[epoch15, step1120]: loss 0.273717
[epoch15, step1121]: loss 0.521585
[epoch15, step1122]: loss 0.395078
[epoch15, step1123]: loss 0.489563
[epoch15, step1124]: loss 0.401489
[epoch15, step1125]: loss 0.600328
[epoch15, step1126]: loss 0.357933
[epoch15, step1127]: loss 0.269613
[epoch15, step1128]: loss 0.382900
[epoch15, step1129]: loss 0.453228
[epoch15, step1130]: loss 0.215140
[epoch15, step1131]: loss 0.546014
[epoch15, step1132]: loss 0.463678
[epoch15, step1133]: loss 0.223987
[epoch15, step1134]: loss 0.422935
[epoch15, step1135]: loss 0.542797
[epoch15, step1136]: loss 0.605146
[epoch15, step1137]: loss 0.429009
[epoch15, step1138]: loss 0.363425
[epoch15, step1139]: loss 0.642757
[epoch15, step1140]: loss 0.393139
[epoch15, step1141]: loss 0.722644
[epoch15, step1142]: loss 0.614494
[epoch15, step1143]: loss 0.269808
[epoch15, step1144]: loss 0.606293
[epoch15, step1145]: loss 0.545052
[epoch15, step1146]: loss 0.487852
[epoch15, step1147]: loss 0.600885
[epoch15, step1148]: loss 0.574511
[epoch15, step1149]: loss 0.483460
[epoch15, step1150]: loss 0.461699
[epoch15, step1151]: loss 0.400806
[epoch15, step1152]: loss 0.314671
[epoch15, step1153]: loss 0.188600
[epoch15, step1154]: loss 0.588800
[epoch15, step1155]: loss 0.420745
[epoch15, step1156]: loss 0.493075
[epoch15, step1157]: loss 0.378817
[epoch15, step1158]: loss 0.481544
[epoch15, step1159]: loss 0.554219
[epoch15, step1160]: loss 0.436902
[epoch15, step1161]: loss 0.427052
[epoch15, step1162]: loss 0.532239
[epoch15, step1163]: loss 0.467851
[epoch15, step1164]: loss 0.397316
[epoch15, step1165]: loss 0.290145
[epoch15, step1166]: loss 0.574940
[epoch15, step1167]: loss 0.513846
[epoch15, step1168]: loss 0.422918
[epoch15, step1169]: loss 0.607528
[epoch15, step1170]: loss 0.515047
[epoch15, step1171]: loss 0.428654
[epoch15, step1172]: loss 0.435833
[epoch15, step1173]: loss 0.553378
[epoch15, step1174]: loss 0.471579
[epoch15, step1175]: loss 0.491359
[epoch15, step1176]: loss 0.463467
[epoch15, step1177]: loss 0.480657
[epoch15, step1178]: loss 0.488683
[epoch15, step1179]: loss 0.491220
[epoch15, step1180]: loss 0.587087
[epoch15, step1181]: loss 0.266473
[epoch15, step1182]: loss 0.339562
[epoch15, step1183]: loss 0.334171
[epoch15, step1184]: loss 0.439428
[epoch15, step1185]: loss 0.217328
[epoch15, step1186]: loss 0.355564
[epoch15, step1187]: loss 0.284481
[epoch15, step1188]: loss 0.233817
[epoch15, step1189]: loss 0.492656
[epoch15, step1190]: loss 0.380185
[epoch15, step1191]: loss 0.384000
[epoch15, step1192]: loss 0.489921
[epoch15, step1193]: loss 0.251560
[epoch15, step1194]: loss 0.215592
[epoch15, step1195]: loss 0.445741
[epoch15, step1196]: loss 0.546378
[epoch15, step1197]: loss 0.143509
[epoch15, step1198]: loss 0.549138
[epoch15, step1199]: loss 0.447632
[epoch15, step1200]: loss 0.530135
[epoch15, step1201]: loss 0.622529
[epoch15, step1202]: loss 0.429250
[epoch15, step1203]: loss 0.687836
[epoch15, step1204]: loss 0.634467
[epoch15, step1205]: loss 0.510544
[epoch15, step1206]: loss 0.617469
[epoch15, step1207]: loss 0.662617
[epoch15, step1208]: loss 0.698142
[epoch15, step1209]: loss 0.436589
[epoch15, step1210]: loss 0.515487
[epoch15, step1211]: loss 0.457904
[epoch15, step1212]: loss 0.551736
[epoch15, step1213]: loss 0.636115
[epoch15, step1214]: loss 0.389901
[epoch15, step1215]: loss 0.588018
[epoch15, step1216]: loss 0.660577
[epoch15, step1217]: loss 0.448289
[epoch15, step1218]: loss 0.441916
[epoch15, step1219]: loss 0.375128
[epoch15, step1220]: loss 0.636010
[epoch15, step1221]: loss 0.447262
[epoch15, step1222]: loss 0.637583
[epoch15, step1223]: loss 0.314863
[epoch15, step1224]: loss 0.337260
[epoch15, step1225]: loss 0.240729
[epoch15, step1226]: loss 0.448557
[epoch15, step1227]: loss 0.392793
[epoch15, step1228]: loss 0.662824
[epoch15, step1229]: loss 0.322424
[epoch15, step1230]: loss 0.342354
[epoch15, step1231]: loss 0.356569
[epoch15, step1232]: loss 0.514787
[epoch15, step1233]: loss 0.355940
[epoch15, step1234]: loss 0.567831
[epoch15, step1235]: loss 0.330382
[epoch15, step1236]: loss 0.555559
[epoch15, step1237]: loss 0.326279
[epoch15, step1238]: loss 0.448941
[epoch15, step1239]: loss 0.666502
[epoch15, step1240]: loss 0.121914
[epoch15, step1241]: loss 0.619648
[epoch15, step1242]: loss 0.495801
[epoch15, step1243]: loss 0.586459
[epoch15, step1244]: loss 0.525702
[epoch15, step1245]: loss 0.381476
[epoch15, step1246]: loss 0.216717
[epoch15, step1247]: loss 0.338965
[epoch15, step1248]: loss 0.480423
[epoch15, step1249]: loss 0.562572
[epoch15, step1250]: loss 0.577441
[epoch15, step1251]: loss 0.466332
[epoch15, step1252]: loss 0.625453
[epoch15, step1253]: loss 0.296276
[epoch15, step1254]: loss 0.629082
[epoch15, step1255]: loss 0.550922
[epoch15, step1256]: loss 0.421608
[epoch15, step1257]: loss 0.459676
[epoch15, step1258]: loss 0.732190
[epoch15, step1259]: loss 0.551102
[epoch15, step1260]: loss 0.523368
[epoch15, step1261]: loss 0.504424
[epoch15, step1262]: loss 0.234322
[epoch15, step1263]: loss 0.434390
[epoch15, step1264]: loss 0.393495
[epoch15, step1265]: loss 0.676713
[epoch15, step1266]: loss 0.421166
[epoch15, step1267]: loss 0.513375
[epoch15, step1268]: loss 0.154942
[epoch15, step1269]: loss 0.400366
[epoch15, step1270]: loss 0.540330
[epoch15, step1271]: loss 0.444517
[epoch15, step1272]: loss 0.522040
[epoch15, step1273]: loss 0.497631
[epoch15, step1274]: loss 0.384019
[epoch15, step1275]: loss 0.584064
[epoch15, step1276]: loss 0.303850
[epoch15, step1277]: loss 0.220454
[epoch15, step1278]: loss 0.646879
[epoch15, step1279]: loss 0.502330
[epoch15, step1280]: loss 0.562446
[epoch15, step1281]: loss 0.584791
[epoch15, step1282]: loss 0.220117
[epoch15, step1283]: loss 0.397865
[epoch15, step1284]: loss 0.395000
[epoch15, step1285]: loss 0.473861
[epoch15, step1286]: loss 0.422076
[epoch15, step1287]: loss 0.641852
[epoch15, step1288]: loss 0.382257
[epoch15, step1289]: loss 0.480722
[epoch15, step1290]: loss 0.615729
[epoch15, step1291]: loss 0.587169
[epoch15, step1292]: loss 0.276146
[epoch15, step1293]: loss 0.555897
[epoch15, step1294]: loss 0.467914
[epoch15, step1295]: loss 0.349641
[epoch15, step1296]: loss 0.341521
[epoch15, step1297]: loss 0.389004
[epoch15, step1298]: loss 0.683929
[epoch15, step1299]: loss 0.631729
[epoch15, step1300]: loss 0.278652
[epoch15, step1301]: loss 0.612885
[epoch15, step1302]: loss 0.334181
[epoch15, step1303]: loss 0.423743
[epoch15, step1304]: loss 0.513582
[epoch15, step1305]: loss 0.562080
[epoch15, step1306]: loss 0.518615
[epoch15, step1307]: loss 0.373639
[epoch15, step1308]: loss 0.653826
[epoch15, step1309]: loss 0.638954
[epoch15, step1310]: loss 0.400912
[epoch15, step1311]: loss 0.619460
[epoch15, step1312]: loss 0.429706
[epoch15, step1313]: loss 0.520421
[epoch15, step1314]: loss 0.674308
[epoch15, step1315]: loss 0.574039
[epoch15, step1316]: loss 0.619243
[epoch15, step1317]: loss 0.369221
[epoch15, step1318]: loss 0.436580
[epoch15, step1319]: loss 0.563309
[epoch15, step1320]: loss 0.499393
[epoch15, step1321]: loss 0.502268
[epoch15, step1322]: loss 0.679181
[epoch15, step1323]: loss 0.324009
[epoch15, step1324]: loss 0.362224
[epoch15, step1325]: loss 0.516185
[epoch15, step1326]: loss 0.423988
[epoch15, step1327]: loss 0.624542
[epoch15, step1328]: loss 0.361171
[epoch15, step1329]: loss 0.378210
[epoch15, step1330]: loss 0.729832
[epoch15, step1331]: loss 0.467801
[epoch15, step1332]: loss 0.703041
[epoch15, step1333]: loss 0.483463
[epoch15, step1334]: loss 0.472535
[epoch15, step1335]: loss 0.554453
[epoch15, step1336]: loss 0.389295
[epoch15, step1337]: loss 0.626609
[epoch15, step1338]: loss 0.608798
[epoch15, step1339]: loss 0.578957
[epoch15, step1340]: loss 0.595303
[epoch15, step1341]: loss 0.499329
[epoch15, step1342]: loss 0.432813
[epoch15, step1343]: loss 0.404169
[epoch15, step1344]: loss 0.415020
[epoch15, step1345]: loss 0.493720
[epoch15, step1346]: loss 0.505446
[epoch15, step1347]: loss 0.340141
[epoch15, step1348]: loss 0.534628
[epoch15, step1349]: loss 0.153356
[epoch15, step1350]: loss 0.201409
[epoch15, step1351]: loss 0.434462
[epoch15, step1352]: loss 0.544950
[epoch15, step1353]: loss 0.260955
[epoch15, step1354]: loss 0.484472
[epoch15, step1355]: loss 0.691117
[epoch15, step1356]: loss 0.487713
[epoch15, step1357]: loss 0.532199
[epoch15, step1358]: loss 0.120038
[epoch15, step1359]: loss 0.576674
[epoch15, step1360]: loss 0.492620
[epoch15, step1361]: loss 0.537658
[epoch15, step1362]: loss 0.701761
[epoch15, step1363]: loss 0.419433
[epoch15, step1364]: loss 0.509982
[epoch15, step1365]: loss 0.313397
[epoch15, step1366]: loss 0.509311
[epoch15, step1367]: loss 0.428160
[epoch15, step1368]: loss 0.407871
[epoch15, step1369]: loss 0.532717
[epoch15, step1370]: loss 0.446268
[epoch15, step1371]: loss 0.419367
[epoch15, step1372]: loss 0.518906
[epoch15, step1373]: loss 0.647903
[epoch15, step1374]: loss 0.502664
[epoch15, step1375]: loss 0.605599
[epoch15, step1376]: loss 0.607322
[epoch15, step1377]: loss 0.483130
[epoch15, step1378]: loss 0.374868
[epoch15, step1379]: loss 0.582015
[epoch15, step1380]: loss 0.504586
[epoch15, step1381]: loss 0.533034
[epoch15, step1382]: loss 0.479006
[epoch15, step1383]: loss 0.451054
[epoch15, step1384]: loss 0.489608
[epoch15, step1385]: loss 0.564301
[epoch15, step1386]: loss 0.542453
[epoch15, step1387]: loss 0.376610
[epoch15, step1388]: loss 0.379336
[epoch15, step1389]: loss 0.479426
[epoch15, step1390]: loss 0.441916
[epoch15, step1391]: loss 0.391082
[epoch15, step1392]: loss 0.511367
[epoch15, step1393]: loss 0.342541
[epoch15, step1394]: loss 0.613828
[epoch15, step1395]: loss 0.431346
[epoch15, step1396]: loss 0.534600
[epoch15, step1397]: loss 0.540855
[epoch15, step1398]: loss 0.691977
[epoch15, step1399]: loss 0.318664
[epoch15, step1400]: loss 0.487278
[epoch15, step1401]: loss 0.655621
[epoch15, step1402]: loss 0.434438
[epoch15, step1403]: loss 0.362733
[epoch15, step1404]: loss 0.584014
[epoch15, step1405]: loss 0.398101
[epoch15, step1406]: loss 0.516842
[epoch15, step1407]: loss 0.370815
[epoch15, step1408]: loss 0.377889
[epoch15, step1409]: loss 0.547488
[epoch15, step1410]: loss 0.692553
[epoch15, step1411]: loss 0.436923
[epoch15, step1412]: loss 0.289301
[epoch15, step1413]: loss 0.406291
[epoch15, step1414]: loss 0.376462
[epoch15, step1415]: loss 0.585673
[epoch15, step1416]: loss 0.528803
[epoch15, step1417]: loss 0.386959
[epoch15, step1418]: loss 0.391204
[epoch15, step1419]: loss 0.475225
[epoch15, step1420]: loss 0.521655
[epoch15, step1421]: loss 0.424999
[epoch15, step1422]: loss 0.441857
[epoch15, step1423]: loss 0.443527
[epoch15, step1424]: loss 0.721581
[epoch15, step1425]: loss 0.486972
[epoch15, step1426]: loss 0.464334
[epoch15, step1427]: loss 0.534069
[epoch15, step1428]: loss 0.461653
[epoch15, step1429]: loss 0.399153
[epoch15, step1430]: loss 0.406757
[epoch15, step1431]: loss 0.466415
[epoch15, step1432]: loss 0.558188
[epoch15, step1433]: loss 0.470532
[epoch15, step1434]: loss 0.397077
[epoch15, step1435]: loss 0.586010
[epoch15, step1436]: loss 0.365849
[epoch15, step1437]: loss 0.568613
[epoch15, step1438]: loss 0.473526
[epoch15, step1439]: loss 0.665796
[epoch15, step1440]: loss 0.399334
[epoch15, step1441]: loss 0.564703
[epoch15, step1442]: loss 0.342269
[epoch15, step1443]: loss 0.448365
[epoch15, step1444]: loss 0.414061
[epoch15, step1445]: loss 0.541858
[epoch15, step1446]: loss 0.493370
[epoch15, step1447]: loss 0.546157
[epoch15, step1448]: loss 0.297288
[epoch15, step1449]: loss 0.536779
[epoch15, step1450]: loss 0.406560
[epoch15, step1451]: loss 0.504948
[epoch15, step1452]: loss 0.612649
[epoch15, step1453]: loss 0.761435
[epoch15, step1454]: loss 0.574958
[epoch15, step1455]: loss 0.479442
[epoch15, step1456]: loss 0.624785
[epoch15, step1457]: loss 0.582399
[epoch15, step1458]: loss 0.478952
[epoch15, step1459]: loss 0.534508
[epoch15, step1460]: loss 0.430946
[epoch15, step1461]: loss 0.602406
[epoch15, step1462]: loss 0.367883
[epoch15, step1463]: loss 0.467843
[epoch15, step1464]: loss 0.492442
[epoch15, step1465]: loss 0.408310
[epoch15, step1466]: loss 0.497850
[epoch15, step1467]: loss 0.259906
[epoch15, step1468]: loss 0.356878
[epoch15, step1469]: loss 0.635981
[epoch15, step1470]: loss 0.508225
[epoch15, step1471]: loss 0.594909
[epoch15, step1472]: loss 0.608788
[epoch15, step1473]: loss 0.596669
[epoch15, step1474]: loss 0.401289
[epoch15, step1475]: loss 0.479950
[epoch15, step1476]: loss 0.491992
[epoch15, step1477]: loss 0.574434
[epoch15, step1478]: loss 0.526452
[epoch15, step1479]: loss 0.486450
[epoch15, step1480]: loss 0.495033
[epoch15, step1481]: loss 0.385731
[epoch15, step1482]: loss 0.603454
[epoch15, step1483]: loss 0.462621
[epoch15, step1484]: loss 0.274868
[epoch15, step1485]: loss 0.328105
[epoch15, step1486]: loss 0.458736
[epoch15, step1487]: loss 0.638544
[epoch15, step1488]: loss 0.521298
[epoch15, step1489]: loss 0.677857
[epoch15, step1490]: loss 0.613518
[epoch15, step1491]: loss 0.662929
[epoch15, step1492]: loss 0.595997
[epoch15, step1493]: loss 0.588348
[epoch15, step1494]: loss 0.430526
[epoch15, step1495]: loss 0.468073
[epoch15, step1496]: loss 0.527060
[epoch15, step1497]: loss 0.591021
[epoch15, step1498]: loss 0.618582
[epoch15, step1499]: loss 0.450995
[epoch15, step1500]: loss 0.596546
[epoch15, step1501]: loss 0.512828
[epoch15, step1502]: loss 0.494227
[epoch15, step1503]: loss 0.554047
[epoch15, step1504]: loss 0.654336
[epoch15, step1505]: loss 0.523618
[epoch15, step1506]: loss 0.375331
[epoch15, step1507]: loss 0.508222
[epoch15, step1508]: loss 0.642237
[epoch15, step1509]: loss 0.231137
[epoch15, step1510]: loss 0.611300
[epoch15, step1511]: loss 0.499688
[epoch15, step1512]: loss 0.540811
[epoch15, step1513]: loss 0.468134
[epoch15, step1514]: loss 0.324933
[epoch15, step1515]: loss 0.472628
[epoch15, step1516]: loss 0.469821
[epoch15, step1517]: loss 0.505024
[epoch15, step1518]: loss 0.300063
[epoch15, step1519]: loss 0.424814
[epoch15, step1520]: loss 0.502476
[epoch15, step1521]: loss 0.399938
[epoch15, step1522]: loss 0.353406
[epoch15, step1523]: loss 0.607019
[epoch15, step1524]: loss 0.532500
[epoch15, step1525]: loss 0.416497
[epoch15, step1526]: loss 0.564387
[epoch15, step1527]: loss 0.442568
[epoch15, step1528]: loss 0.559329
[epoch15, step1529]: loss 0.520214
[epoch15, step1530]: loss 0.629121
[epoch15, step1531]: loss 0.524332
[epoch15, step1532]: loss 0.437442
[epoch15, step1533]: loss 0.587306
[epoch15, step1534]: loss 0.567411
[epoch15, step1535]: loss 0.290457
[epoch15, step1536]: loss 0.411715
[epoch15, step1537]: loss 0.447835
[epoch15, step1538]: loss 0.228747
[epoch15, step1539]: loss 0.489476
[epoch15, step1540]: loss 0.530816
[epoch15, step1541]: loss 0.260678
[epoch15, step1542]: loss 0.219414
[epoch15, step1543]: loss 0.243830
[epoch15, step1544]: loss 0.707044
[epoch15, step1545]: loss 0.406438
[epoch15, step1546]: loss 0.497130
[epoch15, step1547]: loss 0.527489
[epoch15, step1548]: loss 0.267022
[epoch15, step1549]: loss 0.369408
[epoch15, step1550]: loss 0.291768
[epoch15, step1551]: loss 0.524754
[epoch15, step1552]: loss 0.439546
[epoch15, step1553]: loss 0.240688
[epoch15, step1554]: loss 0.320079
[epoch15, step1555]: loss 0.382752
[epoch15, step1556]: loss 0.374519
[epoch15, step1557]: loss 0.351719
[epoch15, step1558]: loss 0.564348
[epoch15, step1559]: loss 0.281036
[epoch15, step1560]: loss 0.578885
[epoch15, step1561]: loss 0.558790
[epoch15, step1562]: loss 0.613176
[epoch15, step1563]: loss 0.542527
[epoch15, step1564]: loss 0.267404
[epoch15, step1565]: loss 0.632372
[epoch15, step1566]: loss 0.429191
[epoch15, step1567]: loss 0.341888
[epoch15, step1568]: loss 0.538481
[epoch15, step1569]: loss 0.442258
[epoch15, step1570]: loss 0.639074
[epoch15, step1571]: loss 0.509466
[epoch15, step1572]: loss 0.554376
[epoch15, step1573]: loss 0.328591
[epoch15, step1574]: loss 0.553876
[epoch15, step1575]: loss 0.464961
[epoch15, step1576]: loss 0.634664
[epoch15, step1577]: loss 0.243222
[epoch15, step1578]: loss 0.449139
[epoch15, step1579]: loss 0.487838
[epoch15, step1580]: loss 0.307052
[epoch15, step1581]: loss 0.467396
[epoch15, step1582]: loss 0.550580
[epoch15, step1583]: loss 0.467876
[epoch15, step1584]: loss 0.264685
[epoch15, step1585]: loss 0.530453
[epoch15, step1586]: loss 0.497535
[epoch15, step1587]: loss 0.372325
[epoch15, step1588]: loss 0.647176
[epoch15, step1589]: loss 0.596345
[epoch15, step1590]: loss 0.285881
[epoch15, step1591]: loss 0.487604
[epoch15, step1592]: loss 0.570188
[epoch15, step1593]: loss 0.289467
[epoch15, step1594]: loss 0.615515
[epoch15, step1595]: loss 0.450168
[epoch15, step1596]: loss 0.544683
[epoch15, step1597]: loss 0.330424
[epoch15, step1598]: loss 0.450268
[epoch15, step1599]: loss 0.443622
[epoch15, step1600]: loss 0.517020
[epoch15, step1601]: loss 0.566505
[epoch15, step1602]: loss 0.611151
[epoch15, step1603]: loss 0.537974
[epoch15, step1604]: loss 0.591901
[epoch15, step1605]: loss 0.546041
[epoch15, step1606]: loss 0.246334
[epoch15, step1607]: loss 0.622988
[epoch15, step1608]: loss 0.550475
[epoch15, step1609]: loss 0.549927
[epoch15, step1610]: loss 0.445144
[epoch15, step1611]: loss 0.419049
[epoch15, step1612]: loss 0.648070
[epoch15, step1613]: loss 0.468127
[epoch15, step1614]: loss 0.463226
[epoch15, step1615]: loss 0.560759
[epoch15, step1616]: loss 0.749558
[epoch15, step1617]: loss 0.698985
[epoch15, step1618]: loss 0.441507
[epoch15, step1619]: loss 0.273829
[epoch15, step1620]: loss 0.586961
[epoch15, step1621]: loss 0.403909
[epoch15, step1622]: loss 0.461167
[epoch15, step1623]: loss 0.401822
[epoch15, step1624]: loss 0.662811
[epoch15, step1625]: loss 0.592447
[epoch15, step1626]: loss 0.630172
[epoch15, step1627]: loss 0.719926
[epoch15, step1628]: loss 0.247887
[epoch15, step1629]: loss 0.527283
[epoch15, step1630]: loss 0.483767
[epoch15, step1631]: loss 0.500798
[epoch15, step1632]: loss 0.375361
[epoch15, step1633]: loss 0.694036
[epoch15, step1634]: loss 0.175528
[epoch15, step1635]: loss 0.516571
[epoch15, step1636]: loss 0.419599
[epoch15, step1637]: loss 0.580087
[epoch15, step1638]: loss 0.549256
[epoch15, step1639]: loss 0.522762
[epoch15, step1640]: loss 0.552698
[epoch15, step1641]: loss 0.515556
[epoch15, step1642]: loss 0.585262
[epoch15, step1643]: loss 0.428460
[epoch15, step1644]: loss 0.518943
[epoch15, step1645]: loss 0.433555
[epoch15, step1646]: loss 0.555136
[epoch15, step1647]: loss 0.482247
[epoch15, step1648]: loss 0.390134
[epoch15, step1649]: loss 0.426497
[epoch15, step1650]: loss 0.143366
[epoch15, step1651]: loss 0.513264
[epoch15, step1652]: loss 0.429638
[epoch15, step1653]: loss 0.710361
[epoch15, step1654]: loss 0.693339
[epoch15, step1655]: loss 0.451606
[epoch15, step1656]: loss 0.360757
[epoch15, step1657]: loss 0.518109
[epoch15, step1658]: loss 0.506627
[epoch15, step1659]: loss 0.481995
[epoch15, step1660]: loss 0.493722
[epoch15, step1661]: loss 0.478887
[epoch15, step1662]: loss 0.347194
[epoch15, step1663]: loss 0.446744
[epoch15, step1664]: loss 0.601545
[epoch15, step1665]: loss 0.350365
[epoch15, step1666]: loss 0.522413
[epoch15, step1667]: loss 0.532405
[epoch15, step1668]: loss 0.139756
[epoch15, step1669]: loss 0.452860
[epoch15, step1670]: loss 0.553227
[epoch15, step1671]: loss 0.283357
[epoch15, step1672]: loss 0.517329
[epoch15, step1673]: loss 0.509858
[epoch15, step1674]: loss 0.390078
[epoch15, step1675]: loss 0.379232
[epoch15, step1676]: loss 0.533786
[epoch15, step1677]: loss 0.294328
[epoch15, step1678]: loss 0.701653
[epoch15, step1679]: loss 0.254929
[epoch15, step1680]: loss 0.757692
[epoch15, step1681]: loss 0.499354
[epoch15, step1682]: loss 0.612493
[epoch15, step1683]: loss 0.437183
[epoch15, step1684]: loss 0.151840
[epoch15, step1685]: loss 0.490154
[epoch15, step1686]: loss 0.565048
[epoch15, step1687]: loss 0.625583
[epoch15, step1688]: loss 0.233651
[epoch15, step1689]: loss 0.594893
[epoch15, step1690]: loss 0.340710
[epoch15, step1691]: loss 0.433714
[epoch15, step1692]: loss 0.526912
[epoch15, step1693]: loss 0.578744
[epoch15, step1694]: loss 0.474264
[epoch15, step1695]: loss 0.408127
[epoch15, step1696]: loss 0.343529
[epoch15, step1697]: loss 0.315561
[epoch15, step1698]: loss 0.469610
[epoch15, step1699]: loss 0.456991
[epoch15, step1700]: loss 0.508633
[epoch15, step1701]: loss 0.478475
[epoch15, step1702]: loss 0.471900
[epoch15, step1703]: loss 0.308555
[epoch15, step1704]: loss 0.569187
[epoch15, step1705]: loss 0.484118
[epoch15, step1706]: loss 0.354252
[epoch15, step1707]: loss 0.349469
[epoch15, step1708]: loss 0.491551
[epoch15, step1709]: loss 0.485801
[epoch15, step1710]: loss 0.649919
[epoch15, step1711]: loss 0.216566
[epoch15, step1712]: loss 0.496997
[epoch15, step1713]: loss 0.478182
[epoch15, step1714]: loss 0.756287
[epoch15, step1715]: loss 0.424947
[epoch15, step1716]: loss 0.519307
[epoch15, step1717]: loss 0.375730
[epoch15, step1718]: loss 0.292031
[epoch15, step1719]: loss 0.507307
[epoch15, step1720]: loss 0.246149
[epoch15, step1721]: loss 0.486334
[epoch15, step1722]: loss 0.626658
[epoch15, step1723]: loss 0.394444
[epoch15, step1724]: loss 0.282196
[epoch15, step1725]: loss 0.301517
[epoch15, step1726]: loss 0.576801
[epoch15, step1727]: loss 0.349873
[epoch15, step1728]: loss 0.366860
[epoch15, step1729]: loss 0.438483
[epoch15, step1730]: loss 0.272832
[epoch15, step1731]: loss 0.456804
[epoch15, step1732]: loss 0.362329
[epoch15, step1733]: loss 0.256937
[epoch15, step1734]: loss 0.209576
[epoch15, step1735]: loss 0.355455
[epoch15, step1736]: loss 0.612424
[epoch15, step1737]: loss 0.648907
[epoch15, step1738]: loss 0.533431
[epoch15, step1739]: loss 0.475243
[epoch15, step1740]: loss 0.555547
[epoch15, step1741]: loss 0.528735
[epoch15, step1742]: loss 0.588537
[epoch15, step1743]: loss 0.582608
[epoch15, step1744]: loss 0.273439
[epoch15, step1745]: loss 0.351230
[epoch15, step1746]: loss 0.525454
[epoch15, step1747]: loss 0.188213
[epoch15, step1748]: loss 0.575715
[epoch15, step1749]: loss 0.518690
[epoch15, step1750]: loss 0.220249
[epoch15, step1751]: loss 0.493823
[epoch15, step1752]: loss 0.620705
[epoch15, step1753]: loss 0.348531
[epoch15, step1754]: loss 0.456306
[epoch15, step1755]: loss 0.601655
[epoch15, step1756]: loss 0.591429
[epoch15, step1757]: loss 0.528623
[epoch15, step1758]: loss 0.337812
[epoch15, step1759]: loss 0.507851
[epoch15, step1760]: loss 0.522509
[epoch15, step1761]: loss 0.078264
[epoch15, step1762]: loss 0.523013
[epoch15, step1763]: loss 0.559692
[epoch15, step1764]: loss 0.543485
[epoch15, step1765]: loss 0.498004
[epoch15, step1766]: loss 0.380060
[epoch15, step1767]: loss 0.391024
[epoch15, step1768]: loss 0.487071
[epoch15, step1769]: loss 0.453601
[epoch15, step1770]: loss 0.407149
[epoch15, step1771]: loss 0.336794
[epoch15, step1772]: loss 0.489295
[epoch15, step1773]: loss 0.501626
[epoch15, step1774]: loss 0.623341
[epoch15, step1775]: loss 0.384464
[epoch15, step1776]: loss 0.356262
[epoch15, step1777]: loss 0.448762
[epoch15, step1778]: loss 0.496396
[epoch15, step1779]: loss 0.352125
[epoch15, step1780]: loss 0.346682
[epoch15, step1781]: loss 0.371758
[epoch15, step1782]: loss 0.564375
[epoch15, step1783]: loss 0.507955
[epoch15, step1784]: loss 0.543787
[epoch15, step1785]: loss 0.503812
[epoch15, step1786]: loss 0.396235
[epoch15, step1787]: loss 0.676145
[epoch15, step1788]: loss 0.425153
[epoch15, step1789]: loss 0.440429
[epoch15, step1790]: loss 0.337538
[epoch15, step1791]: loss 0.572752
[epoch15, step1792]: loss 0.422246
[epoch15, step1793]: loss 0.172265
[epoch15, step1794]: loss 0.278787
[epoch15, step1795]: loss 0.385943
[epoch15, step1796]: loss 0.690791
[epoch15, step1797]: loss 0.416337
[epoch15, step1798]: loss 0.504236
[epoch15, step1799]: loss 0.581568
[epoch15, step1800]: loss 0.389089
[epoch15, step1801]: loss 0.170461
[epoch15, step1802]: loss 0.485074
[epoch15, step1803]: loss 0.536724
[epoch15, step1804]: loss 0.348042
[epoch15, step1805]: loss 0.679756
[epoch15, step1806]: loss 0.235587
[epoch15, step1807]: loss 0.602870
[epoch15, step1808]: loss 0.460595
[epoch15, step1809]: loss 0.581216
[epoch15, step1810]: loss 0.373702
[epoch15, step1811]: loss 0.470442
[epoch15, step1812]: loss 0.352840
[epoch15, step1813]: loss 0.353965
[epoch15, step1814]: loss 0.460305
[epoch15, step1815]: loss 0.512319
[epoch15, step1816]: loss 0.547445
[epoch15, step1817]: loss 0.240022
[epoch15, step1818]: loss 0.590758
[epoch15, step1819]: loss 0.572970
[epoch15, step1820]: loss 0.675333
[epoch15, step1821]: loss 0.247434
[epoch15, step1822]: loss 0.455851
[epoch15, step1823]: loss 0.480119
[epoch15, step1824]: loss 0.739072
[epoch15, step1825]: loss 0.241380
[epoch15, step1826]: loss 0.646925
[epoch15, step1827]: loss 0.486844
[epoch15, step1828]: loss 0.446637
[epoch15, step1829]: loss 0.421477
[epoch15, step1830]: loss 0.479576
[epoch15, step1831]: loss 0.609106
[epoch15, step1832]: loss 0.366809
[epoch15, step1833]: loss 0.478118
[epoch15, step1834]: loss 0.357577
[epoch15, step1835]: loss 0.352929
[epoch15, step1836]: loss 0.459356
[epoch15, step1837]: loss 0.237019
[epoch15, step1838]: loss 0.438376
[epoch15, step1839]: loss 0.544007
[epoch15, step1840]: loss 0.526371
[epoch15, step1841]: loss 0.596576
[epoch15, step1842]: loss 0.250671
[epoch15, step1843]: loss 0.411652
[epoch15, step1844]: loss 0.373167
[epoch15, step1845]: loss 0.500299
[epoch15, step1846]: loss 0.368843
[epoch15, step1847]: loss 0.673989
[epoch15, step1848]: loss 0.496864
[epoch15, step1849]: loss 0.537441
[epoch15, step1850]: loss 0.413584
[epoch15, step1851]: loss 0.254339
[epoch15, step1852]: loss 0.431817
[epoch15, step1853]: loss 0.260351
[epoch15, step1854]: loss 0.494350
[epoch15, step1855]: loss 0.483496
[epoch15, step1856]: loss 0.667417
[epoch15, step1857]: loss 0.595775
[epoch15, step1858]: loss 0.550537
[epoch15, step1859]: loss 0.466200
[epoch15, step1860]: loss 0.609782
[epoch15, step1861]: loss 0.440888
[epoch15, step1862]: loss 0.409447
[epoch15, step1863]: loss 0.303594
[epoch15, step1864]: loss 0.683874
[epoch15, step1865]: loss 0.370393
[epoch15, step1866]: loss 0.468336
[epoch15, step1867]: loss 0.449374
[epoch15, step1868]: loss 0.357423
[epoch15, step1869]: loss 0.410762
[epoch15, step1870]: loss 0.669778
[epoch15, step1871]: loss 0.355217
[epoch15, step1872]: loss 0.549120
[epoch15, step1873]: loss 0.457961
[epoch15, step1874]: loss 0.430786
[epoch15, step1875]: loss 0.392660
[epoch15, step1876]: loss 0.460916
[epoch15, step1877]: loss 0.422588
[epoch15, step1878]: loss 0.453402
[epoch15, step1879]: loss 0.526025
[epoch15, step1880]: loss 0.524215
[epoch15, step1881]: loss 0.326667
[epoch15, step1882]: loss 0.368207
[epoch15, step1883]: loss 0.596776
[epoch15, step1884]: loss 0.718314
[epoch15, step1885]: loss 0.496760
[epoch15, step1886]: loss 0.126925
[epoch15, step1887]: loss 0.414027
[epoch15, step1888]: loss 0.538563
[epoch15, step1889]: loss 0.547579
[epoch15, step1890]: loss 0.637960
[epoch15, step1891]: loss 0.146921
[epoch15, step1892]: loss 0.229048
[epoch15, step1893]: loss 0.491245
[epoch15, step1894]: loss 0.169787
[epoch15, step1895]: loss 0.535089
[epoch15, step1896]: loss 0.432011
[epoch15, step1897]: loss 0.495313
[epoch15, step1898]: loss 0.412939
[epoch15, step1899]: loss 0.295013
[epoch15, step1900]: loss 0.495887
[epoch15, step1901]: loss 0.528220
[epoch15, step1902]: loss 0.668999
[epoch15, step1903]: loss 0.240693
[epoch15, step1904]: loss 0.469094
[epoch15, step1905]: loss 0.547437
[epoch15, step1906]: loss 0.453943
[epoch15, step1907]: loss 0.484980
[epoch15, step1908]: loss 0.491324
[epoch15, step1909]: loss 0.649550
[epoch15, step1910]: loss 0.518875
[epoch15, step1911]: loss 0.216965
[epoch15, step1912]: loss 0.434005
[epoch15, step1913]: loss 0.400610
[epoch15, step1914]: loss 0.501227
[epoch15, step1915]: loss 0.273647
[epoch15, step1916]: loss 0.360740
[epoch15, step1917]: loss 0.387750
[epoch15, step1918]: loss 0.432139
[epoch15, step1919]: loss 0.361021
[epoch15, step1920]: loss 0.393323
[epoch15, step1921]: loss 0.710105
[epoch15, step1922]: loss 0.329951
[epoch15, step1923]: loss 0.507463
[epoch15, step1924]: loss 0.459257
[epoch15, step1925]: loss 0.575349
[epoch15, step1926]: loss 0.514299
[epoch15, step1927]: loss 0.374379
[epoch15, step1928]: loss 0.372495
[epoch15, step1929]: loss 0.338693
[epoch15, step1930]: loss 0.578442
[epoch15, step1931]: loss 0.472057
[epoch15, step1932]: loss 0.501362
[epoch15, step1933]: loss 0.435760
[epoch15, step1934]: loss 0.605519
[epoch15, step1935]: loss 0.419823
[epoch15, step1936]: loss 0.528512
[epoch15, step1937]: loss 0.434061
[epoch15, step1938]: loss 0.505236
[epoch15, step1939]: loss 0.512579
[epoch15, step1940]: loss 0.413797
[epoch15, step1941]: loss 0.260028
[epoch15, step1942]: loss 0.475358
[epoch15, step1943]: loss 0.377946
[epoch15, step1944]: loss 0.580914
[epoch15, step1945]: loss 0.497718
[epoch15, step1946]: loss 0.381959
[epoch15, step1947]: loss 0.342959
[epoch15, step1948]: loss 0.474371
[epoch15, step1949]: loss 0.323811
[epoch15, step1950]: loss 0.110335
[epoch15, step1951]: loss 0.611853
[epoch15, step1952]: loss 0.686044
[epoch15, step1953]: loss 0.437199
[epoch15, step1954]: loss 0.321887
[epoch15, step1955]: loss 0.589017
[epoch15, step1956]: loss 0.617687
[epoch15, step1957]: loss 0.743002
[epoch15, step1958]: loss 0.511076
[epoch15, step1959]: loss 0.175960
[epoch15, step1960]: loss 0.468172
[epoch15, step1961]: loss 0.242378
[epoch15, step1962]: loss 0.429424
[epoch15, step1963]: loss 0.148984
[epoch15, step1964]: loss 0.419686
[epoch15, step1965]: loss 0.550783
[epoch15, step1966]: loss 0.356207
[epoch15, step1967]: loss 0.419229
[epoch15, step1968]: loss 0.468864
[epoch15, step1969]: loss 0.413443
[epoch15, step1970]: loss 0.407098
[epoch15, step1971]: loss 0.523146
[epoch15, step1972]: loss 0.626842
[epoch15, step1973]: loss 0.496099
[epoch15, step1974]: loss 0.227846
[epoch15, step1975]: loss 0.570329
[epoch15, step1976]: loss 0.527547
[epoch15, step1977]: loss 0.486094
[epoch15, step1978]: loss 0.432799
[epoch15, step1979]: loss 0.331710
[epoch15, step1980]: loss 0.557768
[epoch15, step1981]: loss 0.525840
[epoch15, step1982]: loss 0.361889
[epoch15, step1983]: loss 0.426005
[epoch15, step1984]: loss 0.546888
[epoch15, step1985]: loss 0.414078
[epoch15, step1986]: loss 0.578799
[epoch15, step1987]: loss 0.682769
[epoch15, step1988]: loss 0.579850
[epoch15, step1989]: loss 0.556298
[epoch15, step1990]: loss 0.533610
[epoch15, step1991]: loss 0.472085
[epoch15, step1992]: loss 0.526187
[epoch15, step1993]: loss 0.489133
[epoch15, step1994]: loss 0.469967
[epoch15, step1995]: loss 0.428545
[epoch15, step1996]: loss 0.428796
[epoch15, step1997]: loss 0.527219
[epoch15, step1998]: loss 0.394489
[epoch15, step1999]: loss 0.344550
[epoch15, step2000]: loss 0.581297
[epoch15, step2001]: loss 0.421320
[epoch15, step2002]: loss 0.454541
[epoch15, step2003]: loss 0.608108
[epoch15, step2004]: loss 0.374491
[epoch15, step2005]: loss 0.524725
[epoch15, step2006]: loss 0.490413
[epoch15, step2007]: loss 0.268736
[epoch15, step2008]: loss 0.781306
[epoch15, step2009]: loss 0.426510
[epoch15, step2010]: loss 0.398113
[epoch15, step2011]: loss 0.417683
[epoch15, step2012]: loss 0.550152
[epoch15, step2013]: loss 0.289376
[epoch15, step2014]: loss 0.596823
[epoch15, step2015]: loss 0.169409
[epoch15, step2016]: loss 0.637533
[epoch15, step2017]: loss 0.511557
[epoch15, step2018]: loss 0.390387
[epoch15, step2019]: loss 0.366678
[epoch15, step2020]: loss 0.246194
[epoch15, step2021]: loss 0.374147
[epoch15, step2022]: loss 0.611210
[epoch15, step2023]: loss 0.358927
[epoch15, step2024]: loss 0.667776
[epoch15, step2025]: loss 0.593201
[epoch15, step2026]: loss 0.661932
[epoch15, step2027]: loss 0.317069
[epoch15, step2028]: loss 0.613871
[epoch15, step2029]: loss 0.474663
[epoch15, step2030]: loss 0.701156
[epoch15, step2031]: loss 0.562225
[epoch15, step2032]: loss 0.432902
[epoch15, step2033]: loss 0.473623
[epoch15, step2034]: loss 0.271063
[epoch15, step2035]: loss 0.657834
[epoch15, step2036]: loss 0.427628
[epoch15, step2037]: loss 0.654057
[epoch15, step2038]: loss 0.526832
[epoch15, step2039]: loss 0.297980
[epoch15, step2040]: loss 0.551481
[epoch15, step2041]: loss 0.401422
[epoch15, step2042]: loss 0.458850
[epoch15, step2043]: loss 0.558314
[epoch15, step2044]: loss 0.412378
[epoch15, step2045]: loss 0.581803
[epoch15, step2046]: loss 0.437063
[epoch15, step2047]: loss 0.633328
[epoch15, step2048]: loss 0.619829
[epoch15, step2049]: loss 0.447172
[epoch15, step2050]: loss 0.579148
[epoch15, step2051]: loss 0.452672
[epoch15, step2052]: loss 0.663481
[epoch15, step2053]: loss 0.310136
[epoch15, step2054]: loss 0.371746
[epoch15, step2055]: loss 0.213232
[epoch15, step2056]: loss 0.420978
[epoch15, step2057]: loss 0.411206
[epoch15, step2058]: loss 0.661361
[epoch15, step2059]: loss 0.468488
[epoch15, step2060]: loss 0.482321
[epoch15, step2061]: loss 0.535050
[epoch15, step2062]: loss 0.431438
[epoch15, step2063]: loss 0.562652
[epoch15, step2064]: loss 0.561550
[epoch15, step2065]: loss 0.418408
[epoch15, step2066]: loss 0.391450
[epoch15, step2067]: loss 0.399989
[epoch15, step2068]: loss 0.731842
[epoch15, step2069]: loss 0.305435
[epoch15, step2070]: loss 0.609909
[epoch15, step2071]: loss 0.295742
[epoch15, step2072]: loss 0.258742
[epoch15, step2073]: loss 0.641324
[epoch15, step2074]: loss 0.390984
[epoch15, step2075]: loss 0.484007
[epoch15, step2076]: loss 0.388402
[epoch15, step2077]: loss 0.518346
[epoch15, step2078]: loss 0.501893
[epoch15, step2079]: loss 0.511485
[epoch15, step2080]: loss 0.474193
[epoch15, step2081]: loss 0.411913
[epoch15, step2082]: loss 0.672280
[epoch15, step2083]: loss 0.258152
[epoch15, step2084]: loss 0.571002
[epoch15, step2085]: loss 0.442838
[epoch15, step2086]: loss 0.508448
[epoch15, step2087]: loss 0.602307
[epoch15, step2088]: loss 0.684662
[epoch15, step2089]: loss 0.344401
[epoch15, step2090]: loss 0.287755
[epoch15, step2091]: loss 0.689547
[epoch15, step2092]: loss 0.455922
[epoch15, step2093]: loss 0.516548
[epoch15, step2094]: loss 0.653007
[epoch15, step2095]: loss 0.562069
[epoch15, step2096]: loss 0.485889
[epoch15, step2097]: loss 0.493340
[epoch15, step2098]: loss 0.486950
[epoch15, step2099]: loss 0.194794
[epoch15, step2100]: loss 0.465007
[epoch15, step2101]: loss 0.570381
[epoch15, step2102]: loss 0.359803
[epoch15, step2103]: loss 0.496467
[epoch15, step2104]: loss 0.340069
[epoch15, step2105]: loss 0.504799
[epoch15, step2106]: loss 0.461316
[epoch15, step2107]: loss 0.436938
[epoch15, step2108]: loss 0.659559
[epoch15, step2109]: loss 0.620748
[epoch15, step2110]: loss 0.526815
[epoch15, step2111]: loss 0.357785
[epoch15, step2112]: loss 0.281221
[epoch15, step2113]: loss 0.488644
[epoch15, step2114]: loss 0.513690
[epoch15, step2115]: loss 0.268891
[epoch15, step2116]: loss 0.461411
[epoch15, step2117]: loss 0.239355
[epoch15, step2118]: loss 0.452871
[epoch15, step2119]: loss 0.534074
[epoch15, step2120]: loss 0.538805
[epoch15, step2121]: loss 0.412876
[epoch15, step2122]: loss 0.438855
[epoch15, step2123]: loss 0.557802
[epoch15, step2124]: loss 0.488725
[epoch15, step2125]: loss 0.346900
[epoch15, step2126]: loss 0.584773
[epoch15, step2127]: loss 0.651834
[epoch15, step2128]: loss 0.547006
[epoch15, step2129]: loss 0.206012
[epoch15, step2130]: loss 0.513882
[epoch15, step2131]: loss 0.398679
[epoch15, step2132]: loss 0.593602
[epoch15, step2133]: loss 0.626359
[epoch15, step2134]: loss 0.471943
[epoch15, step2135]: loss 0.614602
[epoch15, step2136]: loss 0.331286
[epoch15, step2137]: loss 0.151858
[epoch15, step2138]: loss 0.395700
[epoch15, step2139]: loss 0.613495
[epoch15, step2140]: loss 0.493173
[epoch15, step2141]: loss 0.612289
[epoch15, step2142]: loss 0.341517
[epoch15, step2143]: loss 0.684589
[epoch15, step2144]: loss 0.281646
[epoch15, step2145]: loss 0.440635
[epoch15, step2146]: loss 0.596614
[epoch15, step2147]: loss 0.553317
[epoch15, step2148]: loss 0.527651
[epoch15, step2149]: loss 0.515313
[epoch15, step2150]: loss 0.468254
[epoch15, step2151]: loss 0.353240
[epoch15, step2152]: loss 0.494335
[epoch15, step2153]: loss 0.356899
[epoch15, step2154]: loss 0.586133
[epoch15, step2155]: loss 0.695067
[epoch15, step2156]: loss 0.408664
[epoch15, step2157]: loss 0.655899
[epoch15, step2158]: loss 0.120915
[epoch15, step2159]: loss 0.263214
[epoch15, step2160]: loss 0.545641
[epoch15, step2161]: loss 0.484755
[epoch15, step2162]: loss 0.577314
[epoch15, step2163]: loss 0.484170
[epoch15, step2164]: loss 0.377729
[epoch15, step2165]: loss 0.413993
[epoch15, step2166]: loss 0.415943
[epoch15, step2167]: loss 0.572202
[epoch15, step2168]: loss 0.465108
[epoch15, step2169]: loss 0.400010
[epoch15, step2170]: loss 0.618957
[epoch15, step2171]: loss 0.488717
[epoch15, step2172]: loss 0.467391
[epoch15, step2173]: loss 0.486624
[epoch15, step2174]: loss 0.522536
[epoch15, step2175]: loss 0.501450
[epoch15, step2176]: loss 0.434308
[epoch15, step2177]: loss 0.370226
[epoch15, step2178]: loss 0.538312
[epoch15, step2179]: loss 0.598803
[epoch15, step2180]: loss 0.530361
[epoch15, step2181]: loss 0.587936
[epoch15, step2182]: loss 0.512173
[epoch15, step2183]: loss 0.212602
[epoch15, step2184]: loss 0.617905
[epoch15, step2185]: loss 0.586276
[epoch15, step2186]: loss 0.625257
[epoch15, step2187]: loss 0.372291
[epoch15, step2188]: loss 0.610731
[epoch15, step2189]: loss 0.441186
[epoch15, step2190]: loss 0.225220
[epoch15, step2191]: loss 0.384021
[epoch15, step2192]: loss 0.512421
[epoch15, step2193]: loss 0.423885
[epoch15, step2194]: loss 0.592208
[epoch15, step2195]: loss 0.292829
[epoch15, step2196]: loss 0.445812
[epoch15, step2197]: loss 0.259284
[epoch15, step2198]: loss 0.592364
[epoch15, step2199]: loss 0.586901
[epoch15, step2200]: loss 0.524038
[epoch15, step2201]: loss 0.459441
[epoch15, step2202]: loss 0.244528
[epoch15, step2203]: loss 0.495915
[epoch15, step2204]: loss 0.613524
[epoch15, step2205]: loss 0.506453
[epoch15, step2206]: loss 0.445285
[epoch15, step2207]: loss 0.428601
[epoch15, step2208]: loss 0.502089
[epoch15, step2209]: loss 0.422035
[epoch15, step2210]: loss 0.661717
[epoch15, step2211]: loss 0.367617
[epoch15, step2212]: loss 0.538355
[epoch15, step2213]: loss 0.494978
[epoch15, step2214]: loss 0.585428
[epoch15, step2215]: loss 0.548042
[epoch15, step2216]: loss 0.617480
[epoch15, step2217]: loss 0.130938
[epoch15, step2218]: loss 0.478979
[epoch15, step2219]: loss 0.637840
[epoch15, step2220]: loss 0.517285
[epoch15, step2221]: loss 0.497929
[epoch15, step2222]: loss 0.574187
[epoch15, step2223]: loss 0.497392
[epoch15, step2224]: loss 0.533091
[epoch15, step2225]: loss 0.498802
[epoch15, step2226]: loss 0.576528
[epoch15, step2227]: loss 0.573648
[epoch15, step2228]: loss 0.601894
[epoch15, step2229]: loss 0.313370
[epoch15, step2230]: loss 0.740722
[epoch15, step2231]: loss 0.487505
[epoch15, step2232]: loss 0.513152
[epoch15, step2233]: loss 0.344663
[epoch15, step2234]: loss 0.371762
[epoch15, step2235]: loss 0.514895
[epoch15, step2236]: loss 0.547045
[epoch15, step2237]: loss 0.580680
[epoch15, step2238]: loss 0.564608
[epoch15, step2239]: loss 0.502665
[epoch15, step2240]: loss 0.618523
[epoch15, step2241]: loss 0.411163
[epoch15, step2242]: loss 0.647840
[epoch15, step2243]: loss 0.119776
[epoch15, step2244]: loss 0.500140
[epoch15, step2245]: loss 0.559234
[epoch15, step2246]: loss 0.792739
[epoch15, step2247]: loss 0.521587
[epoch15, step2248]: loss 0.310914
[epoch15, step2249]: loss 0.615642
[epoch15, step2250]: loss 0.451137
[epoch15, step2251]: loss 0.517189
[epoch15, step2252]: loss 0.446860
[epoch15, step2253]: loss 0.408747
[epoch15, step2254]: loss 0.565921
[epoch15, step2255]: loss 0.488716
[epoch15, step2256]: loss 0.579535
[epoch15, step2257]: loss 0.501455
[epoch15, step2258]: loss 0.371192
[epoch15, step2259]: loss 0.592157
[epoch15, step2260]: loss 0.209132
[epoch15, step2261]: loss 0.372168
[epoch15, step2262]: loss 0.457081
[epoch15, step2263]: loss 0.599246
[epoch15, step2264]: loss 0.367454
[epoch15, step2265]: loss 0.469561
[epoch15, step2266]: loss 0.437015
[epoch15, step2267]: loss 0.451853
[epoch15, step2268]: loss 0.606030
[epoch15, step2269]: loss 0.571758
[epoch15, step2270]: loss 0.557540
[epoch15, step2271]: loss 0.256182
[epoch15, step2272]: loss 0.345109
[epoch15, step2273]: loss 0.436861
[epoch15, step2274]: loss 0.450264
[epoch15, step2275]: loss 0.567471
[epoch15, step2276]: loss 0.583055
[epoch15, step2277]: loss 0.553658
[epoch15, step2278]: loss 0.582289
[epoch15, step2279]: loss 0.366743
[epoch15, step2280]: loss 0.414508
[epoch15, step2281]: loss 0.580257
[epoch15, step2282]: loss 0.513381
[epoch15, step2283]: loss 0.465943
[epoch15, step2284]: loss 0.759742
[epoch15, step2285]: loss 0.665573
[epoch15, step2286]: loss 0.710240
[epoch15, step2287]: loss 0.511104
[epoch15, step2288]: loss 0.727403
[epoch15, step2289]: loss 0.501005
[epoch15, step2290]: loss 0.365320
[epoch15, step2291]: loss 0.628592
[epoch15, step2292]: loss 0.272020
[epoch15, step2293]: loss 0.384092
[epoch15, step2294]: loss 0.592489
[epoch15, step2295]: loss 0.488255
[epoch15, step2296]: loss 0.635313
[epoch15, step2297]: loss 0.572336
[epoch15, step2298]: loss 0.415983
[epoch15, step2299]: loss 0.527776
[epoch15, step2300]: loss 0.484704
[epoch15, step2301]: loss 0.334199
[epoch15, step2302]: loss 0.415632
[epoch15, step2303]: loss 0.430564
[epoch15, step2304]: loss 0.533107
[epoch15, step2305]: loss 0.177554
[epoch15, step2306]: loss 0.401896
[epoch15, step2307]: loss 0.279418
[epoch15, step2308]: loss 0.475657
[epoch15, step2309]: loss 0.550392
[epoch15, step2310]: loss 0.462815
[epoch15, step2311]: loss 0.262770
[epoch15, step2312]: loss 0.584943
[epoch15, step2313]: loss 0.437916
[epoch15, step2314]: loss 0.715869
[epoch15, step2315]: loss 0.521091
[epoch15, step2316]: loss 0.336312
[epoch15, step2317]: loss 0.610900
[epoch15, step2318]: loss 0.694287
[epoch15, step2319]: loss 0.662423
[epoch15, step2320]: loss 0.313333
[epoch15, step2321]: loss 0.455251
[epoch15, step2322]: loss 0.652913
[epoch15, step2323]: loss 0.421694
[epoch15, step2324]: loss 0.576883
[epoch15, step2325]: loss 0.504669
[epoch15, step2326]: loss 0.568503
[epoch15, step2327]: loss 0.584320
[epoch15, step2328]: loss 0.128226
[epoch15, step2329]: loss 0.404304
[epoch15, step2330]: loss 0.461215
[epoch15, step2331]: loss 0.074121
[epoch15, step2332]: loss 0.471901
[epoch15, step2333]: loss 0.472090
[epoch15, step2334]: loss 0.446962
[epoch15, step2335]: loss 0.611514
[epoch15, step2336]: loss 0.677068
[epoch15, step2337]: loss 0.570383
[epoch15, step2338]: loss 0.572846
[epoch15, step2339]: loss 0.619963
[epoch15, step2340]: loss 0.549117
[epoch15, step2341]: loss 0.335479
[epoch15, step2342]: loss 0.516465
[epoch15, step2343]: loss 0.380600
[epoch15, step2344]: loss 0.418932
[epoch15, step2345]: loss 0.387729
[epoch15, step2346]: loss 0.535770
[epoch15, step2347]: loss 0.403879
[epoch15, step2348]: loss 0.533855
[epoch15, step2349]: loss 0.376996
[epoch15, step2350]: loss 0.566135
[epoch15, step2351]: loss 0.581549
[epoch15, step2352]: loss 0.260765
[epoch15, step2353]: loss 0.486072
[epoch15, step2354]: loss 0.685818
[epoch15, step2355]: loss 0.527384
[epoch15, step2356]: loss 0.414753
[epoch15, step2357]: loss 0.670209
[epoch15, step2358]: loss 0.470427
[epoch15, step2359]: loss 0.416613
[epoch15, step2360]: loss 0.574607
[epoch15, step2361]: loss 0.425437
[epoch15, step2362]: loss 0.654882
[epoch15, step2363]: loss 0.354606
[epoch15, step2364]: loss 0.428257
[epoch15, step2365]: loss 0.673286
[epoch15, step2366]: loss 0.431978
[epoch15, step2367]: loss 0.510139
[epoch15, step2368]: loss 0.763553
[epoch15, step2369]: loss 0.532650
[epoch15, step2370]: loss 0.391984
[epoch15, step2371]: loss 0.326372
[epoch15, step2372]: loss 0.399677
[epoch15, step2373]: loss 0.410417
[epoch15, step2374]: loss 0.128771
[epoch15, step2375]: loss 0.394258
[epoch15, step2376]: loss 0.677865
[epoch15, step2377]: loss 0.483629
[epoch15, step2378]: loss 0.437395
[epoch15, step2379]: loss 0.619027
[epoch15, step2380]: loss 0.421851
[epoch15, step2381]: loss 0.525801
[epoch15, step2382]: loss 0.540766
[epoch15, step2383]: loss 0.121863
[epoch15, step2384]: loss 0.285383
[epoch15, step2385]: loss 0.606786
[epoch15, step2386]: loss 0.463635
[epoch15, step2387]: loss 0.364406
[epoch15, step2388]: loss 0.514836
[epoch15, step2389]: loss 0.478685
[epoch15, step2390]: loss 0.346188
[epoch15, step2391]: loss 0.528493
[epoch15, step2392]: loss 0.499750
[epoch15, step2393]: loss 0.340866
[epoch15, step2394]: loss 0.526887
[epoch15, step2395]: loss 0.250475
[epoch15, step2396]: loss 0.564955
[epoch15, step2397]: loss 0.524878
[epoch15, step2398]: loss 0.334594
[epoch15, step2399]: loss 0.533247
[epoch15, step2400]: loss 0.273824
[epoch15, step2401]: loss 0.349966
[epoch15, step2402]: loss 0.300454
[epoch15, step2403]: loss 0.360612
[epoch15, step2404]: loss 0.410071
[epoch15, step2405]: loss 0.696203
[epoch15, step2406]: loss 0.672379
[epoch15, step2407]: loss 0.616600
[epoch15, step2408]: loss 0.388571
[epoch15, step2409]: loss 0.669666
[epoch15, step2410]: loss 0.551125
[epoch15, step2411]: loss 0.297397
[epoch15, step2412]: loss 0.619737
[epoch15, step2413]: loss 0.713043
[epoch15, step2414]: loss 0.582732
[epoch15, step2415]: loss 0.452993
[epoch15, step2416]: loss 0.538790
[epoch15, step2417]: loss 0.632951
[epoch15, step2418]: loss 0.563044
[epoch15, step2419]: loss 0.401730
[epoch15, step2420]: loss 0.528450
[epoch15, step2421]: loss 0.528590
[epoch15, step2422]: loss 0.591485
[epoch15, step2423]: loss 0.521487
[epoch15, step2424]: loss 0.577070
[epoch15, step2425]: loss 0.530927
[epoch15, step2426]: loss 0.365662
[epoch15, step2427]: loss 0.280374
[epoch15, step2428]: loss 0.767839
[epoch15, step2429]: loss 0.205701
[epoch15, step2430]: loss 0.239186
[epoch15, step2431]: loss 0.649149
[epoch15, step2432]: loss 0.204593
[epoch15, step2433]: loss 0.277738
[epoch15, step2434]: loss 0.488805
[epoch15, step2435]: loss 0.567532
[epoch15, step2436]: loss 0.401315
[epoch15, step2437]: loss 0.349909
[epoch15, step2438]: loss 0.509040
[epoch15, step2439]: loss 0.517000
[epoch15, step2440]: loss 0.568415
[epoch15, step2441]: loss 0.595153
[epoch15, step2442]: loss 0.524533
[epoch15, step2443]: loss 0.247569
[epoch15, step2444]: loss 0.477506
[epoch15, step2445]: loss 0.342768
[epoch15, step2446]: loss 0.459176
[epoch15, step2447]: loss 0.426236
[epoch15, step2448]: loss 0.610848
[epoch15, step2449]: loss 0.379211
[epoch15, step2450]: loss 0.470673
[epoch15, step2451]: loss 0.524908
[epoch15, step2452]: loss 0.412553
[epoch15, step2453]: loss 0.485522
[epoch15, step2454]: loss 0.531163
[epoch15, step2455]: loss 0.635689
[epoch15, step2456]: loss 0.603143
[epoch15, step2457]: loss 0.361702
[epoch15, step2458]: loss 0.316445
[epoch15, step2459]: loss 0.698469
[epoch15, step2460]: loss 0.566476
[epoch15, step2461]: loss 0.455123
[epoch15, step2462]: loss 0.579137
[epoch15, step2463]: loss 0.663037
[epoch15, step2464]: loss 0.595105
[epoch15, step2465]: loss 0.354306
[epoch15, step2466]: loss 0.516000
[epoch15, step2467]: loss 0.653914
[epoch15, step2468]: loss 0.381390
[epoch15, step2469]: loss 0.468041
[epoch15, step2470]: loss 0.605745
[epoch15, step2471]: loss 0.318931
[epoch15, step2472]: loss 0.485895
[epoch15, step2473]: loss 0.700739
[epoch15, step2474]: loss 0.336642
[epoch15, step2475]: loss 0.508951
[epoch15, step2476]: loss 0.480785
[epoch15, step2477]: loss 0.268344
[epoch15, step2478]: loss 0.321915
[epoch15, step2479]: loss 0.330705
[epoch15, step2480]: loss 0.501205
[epoch15, step2481]: loss 0.588557
[epoch15, step2482]: loss 0.257108
[epoch15, step2483]: loss 0.623508
[epoch15, step2484]: loss 0.356997
[epoch15, step2485]: loss 0.372703
[epoch15, step2486]: loss 0.440103
[epoch15, step2487]: loss 0.496367
[epoch15, step2488]: loss 0.406301
[epoch15, step2489]: loss 0.508459
[epoch15, step2490]: loss 0.518638
[epoch15, step2491]: loss 0.386443
[epoch15, step2492]: loss 0.521259
[epoch15, step2493]: loss 0.499957
[epoch15, step2494]: loss 0.218052
[epoch15, step2495]: loss 0.588359
[epoch15, step2496]: loss 0.496566
[epoch15, step2497]: loss 0.350942
[epoch15, step2498]: loss 0.590309
[epoch15, step2499]: loss 0.478063
[epoch15, step2500]: loss 0.543208
[epoch15, step2501]: loss 0.360229
[epoch15, step2502]: loss 0.567989
[epoch15, step2503]: loss 0.602797
[epoch15, step2504]: loss 0.445607
[epoch15, step2505]: loss 0.464786
[epoch15, step2506]: loss 0.713674
[epoch15, step2507]: loss 0.621836
[epoch15, step2508]: loss 0.610903
[epoch15, step2509]: loss 0.682907
[epoch15, step2510]: loss 0.323969
[epoch15, step2511]: loss 0.399011
[epoch15, step2512]: loss 0.486770
[epoch15, step2513]: loss 0.158114
[epoch15, step2514]: loss 0.645660
[epoch15, step2515]: loss 0.585409
[epoch15, step2516]: loss 0.332953
[epoch15, step2517]: loss 0.258112
[epoch15, step2518]: loss 0.609034
[epoch15, step2519]: loss 0.511438
[epoch15, step2520]: loss 0.541987
[epoch15, step2521]: loss 0.365962
[epoch15, step2522]: loss 0.417006
[epoch15, step2523]: loss 0.610401
[epoch15, step2524]: loss 0.575477
[epoch15, step2525]: loss 0.535992
[epoch15, step2526]: loss 0.398178
[epoch15, step2527]: loss 0.528413
[epoch15, step2528]: loss 0.499761
[epoch15, step2529]: loss 0.450497
[epoch15, step2530]: loss 0.254319
[epoch15, step2531]: loss 0.402209
[epoch15, step2532]: loss 0.586003
[epoch15, step2533]: loss 0.304541
[epoch15, step2534]: loss 0.258534
[epoch15, step2535]: loss 0.354898
[epoch15, step2536]: loss 0.575846
[epoch15, step2537]: loss 0.516219
[epoch15, step2538]: loss 0.341659
[epoch15, step2539]: loss 0.651493
[epoch15, step2540]: loss 0.233798
[epoch15, step2541]: loss 0.504712
[epoch15, step2542]: loss 0.426753
[epoch15, step2543]: loss 0.558573
[epoch15, step2544]: loss 0.512076
[epoch15, step2545]: loss 0.453395
[epoch15, step2546]: loss 0.454399
[epoch15, step2547]: loss 0.417977
[epoch15, step2548]: loss 0.445973
[epoch15, step2549]: loss 0.369389
[epoch15, step2550]: loss 0.419014
[epoch15, step2551]: loss 0.337569
[epoch15, step2552]: loss 0.463403
[epoch15, step2553]: loss 0.565126
[epoch15, step2554]: loss 0.472263
[epoch15, step2555]: loss 0.360625
[epoch15, step2556]: loss 0.376682
[epoch15, step2557]: loss 0.432874
[epoch15, step2558]: loss 0.449530
[epoch15, step2559]: loss 0.523211
[epoch15, step2560]: loss 0.465931
[epoch15, step2561]: loss 0.577769
[epoch15, step2562]: loss 0.389111
[epoch15, step2563]: loss 0.493971
[epoch15, step2564]: loss 0.659482
[epoch15, step2565]: loss 0.646166
[epoch15, step2566]: loss 0.485315
[epoch15, step2567]: loss 0.520134
[epoch15, step2568]: loss 0.409650
[epoch15, step2569]: loss 0.483157
[epoch15, step2570]: loss 0.518095
[epoch15, step2571]: loss 0.617384
[epoch15, step2572]: loss 0.335148
[epoch15, step2573]: loss 0.654818
[epoch15, step2574]: loss 0.242694
[epoch15, step2575]: loss 0.457390
[epoch15, step2576]: loss 0.367346
[epoch15, step2577]: loss 0.381109
[epoch15, step2578]: loss 0.396170
[epoch15, step2579]: loss 0.322249
[epoch15, step2580]: loss 0.736409
[epoch15, step2581]: loss 0.492019
[epoch15, step2582]: loss 0.373429
[epoch15, step2583]: loss 0.335106
[epoch15, step2584]: loss 0.475457
[epoch15, step2585]: loss 0.379724
[epoch15, step2586]: loss 0.450021
[epoch15, step2587]: loss 0.646003
[epoch15, step2588]: loss 0.519403
[epoch15, step2589]: loss 0.296040
[epoch15, step2590]: loss 0.465479
[epoch15, step2591]: loss 0.455736
[epoch15, step2592]: loss 0.465016
[epoch15, step2593]: loss 0.593539
[epoch15, step2594]: loss 0.408955
[epoch15, step2595]: loss 0.629373
[epoch15, step2596]: loss 0.331211
[epoch15, step2597]: loss 0.497022
[epoch15, step2598]: loss 0.329559
[epoch15, step2599]: loss 0.612877
[epoch15, step2600]: loss 0.360009
[epoch15, step2601]: loss 0.522792
[epoch15, step2602]: loss 0.484603
[epoch15, step2603]: loss 0.360694
[epoch15, step2604]: loss 0.490634
[epoch15, step2605]: loss 0.446170
[epoch15, step2606]: loss 0.682264
[epoch15, step2607]: loss 0.763656
[epoch15, step2608]: loss 0.650212
[epoch15, step2609]: loss 0.568247
[epoch15, step2610]: loss 0.654434
[epoch15, step2611]: loss 0.545670
[epoch15, step2612]: loss 0.189537
[epoch15, step2613]: loss 0.458100
[epoch15, step2614]: loss 0.515806
[epoch15, step2615]: loss 0.260159
[epoch15, step2616]: loss 0.436064
[epoch15, step2617]: loss 0.582195
[epoch15, step2618]: loss 0.427350
[epoch15, step2619]: loss 0.577326
[epoch15, step2620]: loss 0.627526
[epoch15, step2621]: loss 0.475196
[epoch15, step2622]: loss 0.495421
[epoch15, step2623]: loss 0.472716
[epoch15, step2624]: loss 0.306245
[epoch15, step2625]: loss 0.476848
[epoch15, step2626]: loss 0.619727
[epoch15, step2627]: loss 0.670522
[epoch15, step2628]: loss 0.511449
[epoch15, step2629]: loss 0.561734
[epoch15, step2630]: loss 0.452381
[epoch15, step2631]: loss 0.249272
[epoch15, step2632]: loss 0.454240
[epoch15, step2633]: loss 0.299441
[epoch15, step2634]: loss 0.430670
[epoch15, step2635]: loss 0.387637
[epoch15, step2636]: loss 0.365020
[epoch15, step2637]: loss 0.100956
[epoch15, step2638]: loss 0.487552
[epoch15, step2639]: loss 0.426149
[epoch15, step2640]: loss 0.585322
[epoch15, step2641]: loss 0.492968
[epoch15, step2642]: loss 0.456449
[epoch15, step2643]: loss 0.349979
[epoch15, step2644]: loss 0.719796
[epoch15, step2645]: loss 0.470872
[epoch15, step2646]: loss 0.456685
[epoch15, step2647]: loss 0.328564
[epoch15, step2648]: loss 0.475960
[epoch15, step2649]: loss 0.377355
[epoch15, step2650]: loss 0.587948
[epoch15, step2651]: loss 0.636213
[epoch15, step2652]: loss 0.640206
[epoch15, step2653]: loss 0.274916
[epoch15, step2654]: loss 0.347477
[epoch15, step2655]: loss 0.424523
[epoch15, step2656]: loss 0.584388
[epoch15, step2657]: loss 0.450422
[epoch15, step2658]: loss 0.537305
[epoch15, step2659]: loss 0.638569
[epoch15, step2660]: loss 0.378664
[epoch15, step2661]: loss 0.334730
[epoch15, step2662]: loss 0.415766
[epoch15, step2663]: loss 0.541425
[epoch15, step2664]: loss 0.261933
[epoch15, step2665]: loss 0.354312
[epoch15, step2666]: loss 0.511428
[epoch15, step2667]: loss 0.622411
[epoch15, step2668]: loss 0.564716
[epoch15, step2669]: loss 0.403034
[epoch15, step2670]: loss 0.556448
[epoch15, step2671]: loss 0.536994
[epoch15, step2672]: loss 0.565285
[epoch15, step2673]: loss 0.319137
[epoch15, step2674]: loss 0.377306
[epoch15, step2675]: loss 0.532115
[epoch15, step2676]: loss 0.618601
[epoch15, step2677]: loss 0.525769
[epoch15, step2678]: loss 0.587082
[epoch15, step2679]: loss 0.253532
[epoch15, step2680]: loss 0.234399
[epoch15, step2681]: loss 0.485054
[epoch15, step2682]: loss 0.497126
[epoch15, step2683]: loss 0.528989
[epoch15, step2684]: loss 0.509165
[epoch15, step2685]: loss 0.638778
[epoch15, step2686]: loss 0.527402
[epoch15, step2687]: loss 0.687953
[epoch15, step2688]: loss 0.574538
[epoch15, step2689]: loss 0.471921
[epoch15, step2690]: loss 0.411251
[epoch15, step2691]: loss 0.436785
[epoch15, step2692]: loss 0.534804
[epoch15, step2693]: loss 0.595800
[epoch15, step2694]: loss 0.684098
[epoch15, step2695]: loss 0.609083
[epoch15, step2696]: loss 0.298243
[epoch15, step2697]: loss 0.400792
[epoch15, step2698]: loss 0.694753
[epoch15, step2699]: loss 0.442256
[epoch15, step2700]: loss 0.478758
[epoch15, step2701]: loss 0.732257
[epoch15, step2702]: loss 0.547526
[epoch15, step2703]: loss 0.446128
[epoch15, step2704]: loss 0.567275
[epoch15, step2705]: loss 0.548533
[epoch15, step2706]: loss 0.477485
[epoch15, step2707]: loss 0.609518
[epoch15, step2708]: loss 0.468292
[epoch15, step2709]: loss 0.419302
[epoch15, step2710]: loss 0.541179
[epoch15, step2711]: loss 0.499536
[epoch15, step2712]: loss 0.444062
[epoch15, step2713]: loss 0.557760
[epoch15, step2714]: loss 0.616650
[epoch15, step2715]: loss 0.398797
[epoch15, step2716]: loss 0.563701
[epoch15, step2717]: loss 0.158114
[epoch15, step2718]: loss 0.387624
[epoch15, step2719]: loss 0.515765
[epoch15, step2720]: loss 0.632297
[epoch15, step2721]: loss 0.412017
[epoch15, step2722]: loss 0.485526
[epoch15, step2723]: loss 0.481307
[epoch15, step2724]: loss 0.368270
[epoch15, step2725]: loss 0.537281
[epoch15, step2726]: loss 0.375459
[epoch15, step2727]: loss 0.563323
[epoch15, step2728]: loss 0.662540
[epoch15, step2729]: loss 0.382460
[epoch15, step2730]: loss 0.529751
[epoch15, step2731]: loss 0.423911
[epoch15, step2732]: loss 0.169772
[epoch15, step2733]: loss 0.526487
[epoch15, step2734]: loss 0.526614
[epoch15, step2735]: loss 0.324914
[epoch15, step2736]: loss 0.209089
[epoch15, step2737]: loss 0.367174
[epoch15, step2738]: loss 0.277428
[epoch15, step2739]: loss 0.435202
[epoch15, step2740]: loss 0.422328
[epoch15, step2741]: loss 0.412974
[epoch15, step2742]: loss 0.493674
[epoch15, step2743]: loss 0.362118
[epoch15, step2744]: loss 0.549757
[epoch15, step2745]: loss 0.593107
[epoch15, step2746]: loss 0.636259
[epoch15, step2747]: loss 0.420966
[epoch15, step2748]: loss 0.668371
[epoch15, step2749]: loss 0.631463
[epoch15, step2750]: loss 0.366412
[epoch15, step2751]: loss 0.387886
[epoch15, step2752]: loss 0.187643
[epoch15, step2753]: loss 0.442212
[epoch15, step2754]: loss 0.351164
[epoch15, step2755]: loss 0.679628
[epoch15, step2756]: loss 0.353239
[epoch15, step2757]: loss 0.562529
[epoch15, step2758]: loss 0.509859
[epoch15, step2759]: loss 0.446666
[epoch15, step2760]: loss 0.644193
[epoch15, step2761]: loss 0.557159
[epoch15, step2762]: loss 0.411629
[epoch15, step2763]: loss 0.348854
[epoch15, step2764]: loss 0.523457
[epoch15, step2765]: loss 0.469616
[epoch15, step2766]: loss 0.483568
[epoch15, step2767]: loss 0.702446
[epoch15, step2768]: loss 0.571401
[epoch15, step2769]: loss 0.314752
[epoch15, step2770]: loss 0.599415
[epoch15, step2771]: loss 0.395463
[epoch15, step2772]: loss 0.333514
[epoch15, step2773]: loss 0.426786
[epoch15, step2774]: loss 0.365745
[epoch15, step2775]: loss 0.454889
[epoch15, step2776]: loss 0.336358
[epoch15, step2777]: loss 0.558156
[epoch15, step2778]: loss 0.569697
[epoch15, step2779]: loss 0.519949
[epoch15, step2780]: loss 0.515669
[epoch15, step2781]: loss 0.531602
[epoch15, step2782]: loss 0.616039
[epoch15, step2783]: loss 0.383725
[epoch15, step2784]: loss 0.537249
[epoch15, step2785]: loss 0.261798
[epoch15, step2786]: loss 0.231752
[epoch15, step2787]: loss 0.519201
[epoch15, step2788]: loss 0.528141
[epoch15, step2789]: loss 0.726011
[epoch15, step2790]: loss 0.513731
[epoch15, step2791]: loss 0.543352
[epoch15, step2792]: loss 0.515389
[epoch15, step2793]: loss 0.332175
[epoch15, step2794]: loss 0.548414
[epoch15, step2795]: loss 0.512653
[epoch15, step2796]: loss 0.556322
[epoch15, step2797]: loss 0.599698
[epoch15, step2798]: loss 0.563170
[epoch15, step2799]: loss 0.144903
[epoch15, step2800]: loss 0.493111
[epoch15, step2801]: loss 0.458660
[epoch15, step2802]: loss 0.439885
[epoch15, step2803]: loss 0.600324
[epoch15, step2804]: loss 0.261714
[epoch15, step2805]: loss 0.550220
[epoch15, step2806]: loss 0.480280
[epoch15, step2807]: loss 0.323196
[epoch15, step2808]: loss 0.433587
[epoch15, step2809]: loss 0.374016
[epoch15, step2810]: loss 0.488169
[epoch15, step2811]: loss 0.643741
[epoch15, step2812]: loss 0.559364
[epoch15, step2813]: loss 0.327080
[epoch15, step2814]: loss 0.465366
[epoch15, step2815]: loss 0.462435
[epoch15, step2816]: loss 0.286463
[epoch15, step2817]: loss 0.305829
[epoch15, step2818]: loss 0.329724
[epoch15, step2819]: loss 0.445142
[epoch15, step2820]: loss 0.498681
[epoch15, step2821]: loss 0.395308
[epoch15, step2822]: loss 0.509199
[epoch15, step2823]: loss 0.288192
[epoch15, step2824]: loss 0.528105
[epoch15, step2825]: loss 0.314068
[epoch15, step2826]: loss 0.431643
[epoch15, step2827]: loss 0.429105
[epoch15, step2828]: loss 0.437524
[epoch15, step2829]: loss 0.540723
[epoch15, step2830]: loss 0.468053
[epoch15, step2831]: loss 0.589418
[epoch15, step2832]: loss 0.338636
[epoch15, step2833]: loss 0.759518
[epoch15, step2834]: loss 0.437883
[epoch15, step2835]: loss 0.344916
[epoch15, step2836]: loss 0.605523
[epoch15, step2837]: loss 0.148404
[epoch15, step2838]: loss 0.230157
[epoch15, step2839]: loss 0.427218
[epoch15, step2840]: loss 0.517560
[epoch15, step2841]: loss 0.658546
[epoch15, step2842]: loss 0.358924
[epoch15, step2843]: loss 0.630460
[epoch15, step2844]: loss 0.390121
[epoch15, step2845]: loss 0.173229
[epoch15, step2846]: loss 0.528203
[epoch15, step2847]: loss 0.722181
[epoch15, step2848]: loss 0.381858
[epoch15, step2849]: loss 0.752102
[epoch15, step2850]: loss 0.476314
[epoch15, step2851]: loss 0.275322
[epoch15, step2852]: loss 0.498249
[epoch15, step2853]: loss 0.469503
[epoch15, step2854]: loss 0.517617
[epoch15, step2855]: loss 0.506422
[epoch15, step2856]: loss 0.469607
[epoch15, step2857]: loss 0.423312
[epoch15, step2858]: loss 0.482788
[epoch15, step2859]: loss 0.634790
[epoch15, step2860]: loss 0.787816
[epoch15, step2861]: loss 0.521586
[epoch15, step2862]: loss 0.403818
[epoch15, step2863]: loss 0.489777
[epoch15, step2864]: loss 0.531702
[epoch15, step2865]: loss 0.577296
[epoch15, step2866]: loss 0.772359
[epoch15, step2867]: loss 0.638442
[epoch15, step2868]: loss 0.417793
[epoch15, step2869]: loss 0.376432
[epoch15, step2870]: loss 0.512763
[epoch15, step2871]: loss 0.412851
[epoch15, step2872]: loss 0.440941
[epoch15, step2873]: loss 0.571754
[epoch15, step2874]: loss 0.433779
[epoch15, step2875]: loss 0.347081
[epoch15, step2876]: loss 0.553576
[epoch15, step2877]: loss 0.505249
[epoch15, step2878]: loss 0.488961
[epoch15, step2879]: loss 0.444990
[epoch15, step2880]: loss 0.628563
[epoch15, step2881]: loss 0.646755
[epoch15, step2882]: loss 0.445593
[epoch15, step2883]: loss 0.078657
[epoch15, step2884]: loss 0.421311
[epoch15, step2885]: loss 0.674476
[epoch15, step2886]: loss 0.525903
[epoch15, step2887]: loss 0.350756
[epoch15, step2888]: loss 0.528902
[epoch15, step2889]: loss 0.636022
[epoch15, step2890]: loss 0.485158
[epoch15, step2891]: loss 0.596574
[epoch15, step2892]: loss 0.414814
[epoch15, step2893]: loss 0.470546
[epoch15, step2894]: loss 0.415551
[epoch15, step2895]: loss 0.572506
[epoch15, step2896]: loss 0.572407
[epoch15, step2897]: loss 0.246995
[epoch15, step2898]: loss 0.531933
[epoch15, step2899]: loss 0.597298
[epoch15, step2900]: loss 0.578562
[epoch15, step2901]: loss 0.531852
[epoch15, step2902]: loss 0.431689
[epoch15, step2903]: loss 0.538534
[epoch15, step2904]: loss 0.636097
[epoch15, step2905]: loss 0.574233
[epoch15, step2906]: loss 0.105614
[epoch15, step2907]: loss 0.367532
[epoch15, step2908]: loss 0.273403
[epoch15, step2909]: loss 0.497671
[epoch15, step2910]: loss 0.438818
[epoch15, step2911]: loss 0.458985
[epoch15, step2912]: loss 0.579367
[epoch15, step2913]: loss 0.595951
[epoch15, step2914]: loss 0.506312
[epoch15, step2915]: loss 0.416161
[epoch15, step2916]: loss 0.537351
[epoch15, step2917]: loss 0.428585
[epoch15, step2918]: loss 0.466140
[epoch15, step2919]: loss 0.613540
[epoch15, step2920]: loss 0.482256
[epoch15, step2921]: loss 0.476648
[epoch15, step2922]: loss 0.337062
[epoch15, step2923]: loss 0.624176
[epoch15, step2924]: loss 0.587349
[epoch15, step2925]: loss 0.451965
[epoch15, step2926]: loss 0.591484
[epoch15, step2927]: loss 0.276045
[epoch15, step2928]: loss 0.449928
[epoch15, step2929]: loss 0.437288
[epoch15, step2930]: loss 0.737525
[epoch15, step2931]: loss 0.540436
[epoch15, step2932]: loss 0.572066
[epoch15, step2933]: loss 0.466000
[epoch15, step2934]: loss 0.383773
[epoch15, step2935]: loss 0.390127
[epoch15, step2936]: loss 0.736024
[epoch15, step2937]: loss 0.438079
[epoch15, step2938]: loss 0.367843
[epoch15, step2939]: loss 0.619747
[epoch15, step2940]: loss 0.586524
[epoch15, step2941]: loss 0.512282
[epoch15, step2942]: loss 0.365421
[epoch15, step2943]: loss 0.651077
[epoch15, step2944]: loss 0.441977
[epoch15, step2945]: loss 0.541801
[epoch15, step2946]: loss 0.282952
[epoch15, step2947]: loss 0.487214
[epoch15, step2948]: loss 0.480860
[epoch15, step2949]: loss 0.348386
[epoch15, step2950]: loss 0.358491
[epoch15, step2951]: loss 0.451139
[epoch15, step2952]: loss 0.158097
[epoch15, step2953]: loss 0.436591
[epoch15, step2954]: loss 0.175193
[epoch15, step2955]: loss 0.436086
[epoch15, step2956]: loss 0.506909
[epoch15, step2957]: loss 0.635756
[epoch15, step2958]: loss 0.663483
[epoch15, step2959]: loss 0.519537
[epoch15, step2960]: loss 0.452064
[epoch15, step2961]: loss 0.227027
[epoch15, step2962]: loss 0.493806
[epoch15, step2963]: loss 0.703007
[epoch15, step2964]: loss 0.408733
[epoch15, step2965]: loss 0.296891
[epoch15, step2966]: loss 0.356149
[epoch15, step2967]: loss 0.259095
[epoch15, step2968]: loss 0.537699
[epoch15, step2969]: loss 0.150467
[epoch15, step2970]: loss 0.093499
[epoch15, step2971]: loss 0.262331
[epoch15, step2972]: loss 0.250143
[epoch15, step2973]: loss 0.558909
[epoch15, step2974]: loss 0.361382
[epoch15, step2975]: loss 0.273908
[epoch15, step2976]: loss 0.417285
[epoch15, step2977]: loss 0.279628
[epoch15, step2978]: loss 0.392173
[epoch15, step2979]: loss 0.577588
[epoch15, step2980]: loss 0.452771
[epoch15, step2981]: loss 0.523263
[epoch15, step2982]: loss 0.659780
[epoch15, step2983]: loss 0.381711
[epoch15, step2984]: loss 0.449756
[epoch15, step2985]: loss 0.442251
[epoch15, step2986]: loss 0.714852
[epoch15, step2987]: loss 0.484725
[epoch15, step2988]: loss 0.541304
[epoch15, step2989]: loss 0.678431
[epoch15, step2990]: loss 0.605531
[epoch15, step2991]: loss 0.606569
[epoch15, step2992]: loss 0.482137
[epoch15, step2993]: loss 0.503389
[epoch15, step2994]: loss 0.537248
[epoch15, step2995]: loss 0.355300
[epoch15, step2996]: loss 0.651786
[epoch15, step2997]: loss 0.453858
[epoch15, step2998]: loss 0.348952
[epoch15, step2999]: loss 0.507757
[epoch15, step3000]: loss 0.528604
[epoch15, step3001]: loss 0.492807
[epoch15, step3002]: loss 0.548401
[epoch15, step3003]: loss 0.625099
[epoch15, step3004]: loss 0.426180
[epoch15, step3005]: loss 0.443484
[epoch15, step3006]: loss 0.339877
[epoch15, step3007]: loss 0.342937
[epoch15, step3008]: loss 0.684756
[epoch15, step3009]: loss 0.533070
[epoch15, step3010]: loss 0.549333
[epoch15, step3011]: loss 0.434954
[epoch15, step3012]: loss 0.470274
[epoch15, step3013]: loss 0.735750
[epoch15, step3014]: loss 0.407095
[epoch15, step3015]: loss 0.335063
[epoch15, step3016]: loss 0.359786
[epoch15, step3017]: loss 0.639489
[epoch15, step3018]: loss 0.568614
[epoch15, step3019]: loss 0.514373
[epoch15, step3020]: loss 0.578991
[epoch15, step3021]: loss 0.485979
[epoch15, step3022]: loss 0.378258
[epoch15, step3023]: loss 0.404463
[epoch15, step3024]: loss 0.584603
[epoch15, step3025]: loss 0.451338
[epoch15, step3026]: loss 0.320164
[epoch15, step3027]: loss 0.622513
[epoch15, step3028]: loss 0.504920
[epoch15, step3029]: loss 0.613297
[epoch15, step3030]: loss 0.244065
[epoch15, step3031]: loss 0.547306
[epoch15, step3032]: loss 0.518244
[epoch15, step3033]: loss 0.277126
[epoch15, step3034]: loss 0.551564
[epoch15, step3035]: loss 0.457470
[epoch15, step3036]: loss 0.613805
[epoch15, step3037]: loss 0.466596
[epoch15, step3038]: loss 0.444189
[epoch15, step3039]: loss 0.279266
[epoch15, step3040]: loss 0.489303
[epoch15, step3041]: loss 0.401929
[epoch15, step3042]: loss 0.591830
[epoch15, step3043]: loss 0.604793
[epoch15, step3044]: loss 0.531985
[epoch15, step3045]: loss 0.503884
[epoch15, step3046]: loss 0.638200
[epoch15, step3047]: loss 0.372356
[epoch15, step3048]: loss 0.142440
[epoch15, step3049]: loss 0.466820
[epoch15, step3050]: loss 0.536884
[epoch15, step3051]: loss 0.562825
[epoch15, step3052]: loss 0.361930
[epoch15, step3053]: loss 0.556227
[epoch15, step3054]: loss 0.613916
[epoch15, step3055]: loss 0.575568
[epoch15, step3056]: loss 0.465980
[epoch15, step3057]: loss 0.347650
[epoch15, step3058]: loss 0.593302
[epoch15, step3059]: loss 0.203386
[epoch15, step3060]: loss 0.631732
[epoch15, step3061]: loss 0.510816
[epoch15, step3062]: loss 0.538034
[epoch15, step3063]: loss 0.566452
[epoch15, step3064]: loss 0.744649
[epoch15, step3065]: loss 0.502459
[epoch15, step3066]: loss 0.518834
[epoch15, step3067]: loss 0.349903
[epoch15, step3068]: loss 0.530411
[epoch15, step3069]: loss 0.414583
[epoch15, step3070]: loss 0.635155
[epoch15, step3071]: loss 0.453931
[epoch15, step3072]: loss 0.444515
[epoch15, step3073]: loss 0.381900
[epoch15, step3074]: loss 0.385925
[epoch15, step3075]: loss 0.488052
[epoch15, step3076]: loss 0.246699

[epoch15]: avg loss 0.246699

[epoch16, step1]: loss 0.450802
[epoch16, step2]: loss 0.550448
[epoch16, step3]: loss 0.583531
[epoch16, step4]: loss 0.616361
[epoch16, step5]: loss 0.615468
[epoch16, step6]: loss 0.503578
[epoch16, step7]: loss 0.531969
[epoch16, step8]: loss 0.628140
[epoch16, step9]: loss 0.152672
[epoch16, step10]: loss 0.271047
[epoch16, step11]: loss 0.136223
[epoch16, step12]: loss 0.567178
[epoch16, step13]: loss 0.477192
[epoch16, step14]: loss 0.413569
[epoch16, step15]: loss 0.368351
[epoch16, step16]: loss 0.314084
[epoch16, step17]: loss 0.562268
[epoch16, step18]: loss 0.308637
[epoch16, step19]: loss 0.593473
[epoch16, step20]: loss 0.453765
[epoch16, step21]: loss 0.471764
[epoch16, step22]: loss 0.603387
[epoch16, step23]: loss 0.509785
[epoch16, step24]: loss 0.410290
[epoch16, step25]: loss 0.493303
[epoch16, step26]: loss 0.512396
[epoch16, step27]: loss 0.560043
[epoch16, step28]: loss 0.243655
[epoch16, step29]: loss 0.350798
[epoch16, step30]: loss 0.650655
[epoch16, step31]: loss 0.362015
[epoch16, step32]: loss 0.219696
[epoch16, step33]: loss 0.343460
[epoch16, step34]: loss 0.355717
[epoch16, step35]: loss 0.417687
[epoch16, step36]: loss 0.520252
[epoch16, step37]: loss 0.632025
[epoch16, step38]: loss 0.751553
[epoch16, step39]: loss 0.485621
[epoch16, step40]: loss 0.458159
[epoch16, step41]: loss 0.457017
[epoch16, step42]: loss 0.375532
[epoch16, step43]: loss 0.524594
[epoch16, step44]: loss 0.475108
[epoch16, step45]: loss 0.443787
[epoch16, step46]: loss 0.389245
[epoch16, step47]: loss 0.429119
[epoch16, step48]: loss 0.319433
[epoch16, step49]: loss 0.646439
[epoch16, step50]: loss 0.497604
[epoch16, step51]: loss 0.331257
[epoch16, step52]: loss 0.574721
[epoch16, step53]: loss 0.303569
[epoch16, step54]: loss 0.404966
[epoch16, step55]: loss 0.517116
[epoch16, step56]: loss 0.341389
[epoch16, step57]: loss 0.744278
[epoch16, step58]: loss 0.503088
[epoch16, step59]: loss 0.474219
[epoch16, step60]: loss 0.404528
[epoch16, step61]: loss 0.555182
[epoch16, step62]: loss 0.539999
[epoch16, step63]: loss 0.431219
[epoch16, step64]: loss 0.325447
[epoch16, step65]: loss 0.627160
[epoch16, step66]: loss 0.455338
[epoch16, step67]: loss 0.538158
[epoch16, step68]: loss 0.494386
[epoch16, step69]: loss 0.403841
[epoch16, step70]: loss 0.281393
[epoch16, step71]: loss 0.287562
[epoch16, step72]: loss 0.443275
[epoch16, step73]: loss 0.298910
[epoch16, step74]: loss 0.535566
[epoch16, step75]: loss 0.376284
[epoch16, step76]: loss 0.660948
[epoch16, step77]: loss 0.468105
[epoch16, step78]: loss 0.428453
[epoch16, step79]: loss 0.392986
[epoch16, step80]: loss 0.210690
[epoch16, step81]: loss 0.399697
[epoch16, step82]: loss 0.231819
[epoch16, step83]: loss 0.549708
[epoch16, step84]: loss 0.508898
[epoch16, step85]: loss 0.440479
[epoch16, step86]: loss 0.700861
[epoch16, step87]: loss 0.759831
[epoch16, step88]: loss 0.575966
[epoch16, step89]: loss 0.221930
[epoch16, step90]: loss 0.591668
[epoch16, step91]: loss 0.503505
[epoch16, step92]: loss 0.538884
[epoch16, step93]: loss 0.644153
[epoch16, step94]: loss 0.478193
[epoch16, step95]: loss 0.511608
[epoch16, step96]: loss 0.424100
[epoch16, step97]: loss 0.530953
[epoch16, step98]: loss 0.406932
[epoch16, step99]: loss 0.473807
[epoch16, step100]: loss 0.476892
[epoch16, step101]: loss 0.521059
[epoch16, step102]: loss 0.680554
[epoch16, step103]: loss 0.564413
[epoch16, step104]: loss 0.266978
[epoch16, step105]: loss 0.430754
[epoch16, step106]: loss 0.533524
[epoch16, step107]: loss 0.561419
[epoch16, step108]: loss 0.442498
[epoch16, step109]: loss 0.364699
[epoch16, step110]: loss 0.406136
[epoch16, step111]: loss 0.616619
[epoch16, step112]: loss 0.523685
[epoch16, step113]: loss 0.536676
[epoch16, step114]: loss 0.357520
[epoch16, step115]: loss 0.618099
[epoch16, step116]: loss 0.419181
[epoch16, step117]: loss 0.613899
[epoch16, step118]: loss 0.446279
[epoch16, step119]: loss 0.409955
[epoch16, step120]: loss 0.538297
[epoch16, step121]: loss 0.146575
[epoch16, step122]: loss 0.369229
[epoch16, step123]: loss 0.247126
[epoch16, step124]: loss 0.349714
[epoch16, step125]: loss 0.602613
[epoch16, step126]: loss 0.707429
[epoch16, step127]: loss 0.432171
[epoch16, step128]: loss 0.545895
[epoch16, step129]: loss 0.464865
[epoch16, step130]: loss 0.468514
[epoch16, step131]: loss 0.482374
[epoch16, step132]: loss 0.289463
[epoch16, step133]: loss 0.305701
[epoch16, step134]: loss 0.507246
[epoch16, step135]: loss 0.518230
[epoch16, step136]: loss 0.397324
[epoch16, step137]: loss 0.660685
[epoch16, step138]: loss 0.329258
[epoch16, step139]: loss 0.295749
[epoch16, step140]: loss 0.518643
[epoch16, step141]: loss 0.386585
[epoch16, step142]: loss 0.534456
[epoch16, step143]: loss 0.487819
[epoch16, step144]: loss 0.552181
[epoch16, step145]: loss 0.461967
[epoch16, step146]: loss 0.509017
[epoch16, step147]: loss 0.588018
[epoch16, step148]: loss 0.382764
[epoch16, step149]: loss 0.374198
[epoch16, step150]: loss 0.408209
[epoch16, step151]: loss 0.372746
[epoch16, step152]: loss 0.435055
[epoch16, step153]: loss 0.647676
[epoch16, step154]: loss 0.397459
[epoch16, step155]: loss 0.596195
[epoch16, step156]: loss 0.688453
[epoch16, step157]: loss 0.546405
[epoch16, step158]: loss 0.511800
[epoch16, step159]: loss 0.438286
[epoch16, step160]: loss 0.478471
[epoch16, step161]: loss 0.383200
[epoch16, step162]: loss 0.356287
[epoch16, step163]: loss 0.465002
[epoch16, step164]: loss 0.572212
[epoch16, step165]: loss 0.423341
[epoch16, step166]: loss 0.550423
[epoch16, step167]: loss 0.449753
[epoch16, step168]: loss 0.587370
[epoch16, step169]: loss 0.360177
[epoch16, step170]: loss 0.678252
[epoch16, step171]: loss 0.527195
[epoch16, step172]: loss 0.556057
[epoch16, step173]: loss 0.218871
[epoch16, step174]: loss 0.500888
[epoch16, step175]: loss 0.539779
[epoch16, step176]: loss 0.488000
[epoch16, step177]: loss 0.241410
[epoch16, step178]: loss 0.521317
[epoch16, step179]: loss 0.514737
[epoch16, step180]: loss 0.524774
[epoch16, step181]: loss 0.499849
[epoch16, step182]: loss 0.515212
[epoch16, step183]: loss 0.217747
[epoch16, step184]: loss 0.220942
[epoch16, step185]: loss 0.322187
[epoch16, step186]: loss 0.481447
[epoch16, step187]: loss 0.766017
[epoch16, step188]: loss 0.487399
[epoch16, step189]: loss 0.482994
[epoch16, step190]: loss 0.515336
[epoch16, step191]: loss 0.430822
[epoch16, step192]: loss 0.468535
[epoch16, step193]: loss 0.594583
[epoch16, step194]: loss 0.530348
[epoch16, step195]: loss 0.350821
[epoch16, step196]: loss 0.454755
[epoch16, step197]: loss 0.573722
[epoch16, step198]: loss 0.606494
[epoch16, step199]: loss 0.199054
[epoch16, step200]: loss 0.464924
[epoch16, step201]: loss 0.370253
[epoch16, step202]: loss 0.485997
[epoch16, step203]: loss 0.466229
[epoch16, step204]: loss 0.505495
[epoch16, step205]: loss 0.588753
[epoch16, step206]: loss 0.344026
[epoch16, step207]: loss 0.411851
[epoch16, step208]: loss 0.673184
[epoch16, step209]: loss 0.330688
[epoch16, step210]: loss 0.473749
[epoch16, step211]: loss 0.640622
[epoch16, step212]: loss 0.431437
[epoch16, step213]: loss 0.579765
[epoch16, step214]: loss 0.407661
[epoch16, step215]: loss 0.482081
[epoch16, step216]: loss 0.453636
[epoch16, step217]: loss 0.427734
[epoch16, step218]: loss 0.401003
[epoch16, step219]: loss 0.590041
[epoch16, step220]: loss 0.541788
[epoch16, step221]: loss 0.282219
[epoch16, step222]: loss 0.584570
[epoch16, step223]: loss 0.477579
[epoch16, step224]: loss 0.685543
[epoch16, step225]: loss 0.584966
[epoch16, step226]: loss 0.279507
[epoch16, step227]: loss 0.343466
[epoch16, step228]: loss 0.306500
[epoch16, step229]: loss 0.593986
[epoch16, step230]: loss 0.387198
[epoch16, step231]: loss 0.381889
[epoch16, step232]: loss 0.561085
[epoch16, step233]: loss 0.586745
[epoch16, step234]: loss 0.469668
[epoch16, step235]: loss 0.488031
[epoch16, step236]: loss 0.325244
[epoch16, step237]: loss 0.441572
[epoch16, step238]: loss 0.453565
[epoch16, step239]: loss 0.516098
[epoch16, step240]: loss 0.383246
[epoch16, step241]: loss 0.384570
[epoch16, step242]: loss 0.592286
[epoch16, step243]: loss 0.416157
[epoch16, step244]: loss 0.574022
[epoch16, step245]: loss 0.423377
[epoch16, step246]: loss 0.501746
[epoch16, step247]: loss 0.551115
[epoch16, step248]: loss 0.568526
[epoch16, step249]: loss 0.520798
[epoch16, step250]: loss 0.162690
[epoch16, step251]: loss 0.628839
[epoch16, step252]: loss 0.625211
[epoch16, step253]: loss 0.436535
[epoch16, step254]: loss 0.527042
[epoch16, step255]: loss 0.410422
[epoch16, step256]: loss 0.499193
[epoch16, step257]: loss 0.428033
[epoch16, step258]: loss 0.561757
[epoch16, step259]: loss 0.506223
[epoch16, step260]: loss 0.428272
[epoch16, step261]: loss 0.348657
[epoch16, step262]: loss 0.609973
[epoch16, step263]: loss 0.631502
[epoch16, step264]: loss 0.416974
[epoch16, step265]: loss 0.344498
[epoch16, step266]: loss 0.370452
[epoch16, step267]: loss 0.417071
[epoch16, step268]: loss 0.531820
[epoch16, step269]: loss 0.268692
[epoch16, step270]: loss 0.298846
[epoch16, step271]: loss 0.232706
[epoch16, step272]: loss 0.539113
[epoch16, step273]: loss 0.342679
[epoch16, step274]: loss 0.650833
[epoch16, step275]: loss 0.588562
[epoch16, step276]: loss 0.615960
[epoch16, step277]: loss 0.592842
[epoch16, step278]: loss 0.474325
[epoch16, step279]: loss 0.360371
[epoch16, step280]: loss 0.618889
[epoch16, step281]: loss 0.371094
[epoch16, step282]: loss 0.406371
[epoch16, step283]: loss 0.683788
[epoch16, step284]: loss 0.250197
[epoch16, step285]: loss 0.633189
[epoch16, step286]: loss 0.596715
[epoch16, step287]: loss 0.316472
[epoch16, step288]: loss 0.534341
[epoch16, step289]: loss 0.458012
[epoch16, step290]: loss 0.386696
[epoch16, step291]: loss 0.588767
[epoch16, step292]: loss 0.574410
[epoch16, step293]: loss 0.523188
[epoch16, step294]: loss 0.664937
[epoch16, step295]: loss 0.382732
[epoch16, step296]: loss 0.364758
[epoch16, step297]: loss 0.423663
[epoch16, step298]: loss 0.514821
[epoch16, step299]: loss 0.471536
[epoch16, step300]: loss 0.480494
[epoch16, step301]: loss 0.670325
[epoch16, step302]: loss 0.356953
[epoch16, step303]: loss 0.527552
[epoch16, step304]: loss 0.520981
[epoch16, step305]: loss 0.652088
[epoch16, step306]: loss 0.470909
[epoch16, step307]: loss 0.530858
[epoch16, step308]: loss 0.506712
[epoch16, step309]: loss 0.259631
[epoch16, step310]: loss 0.517726
[epoch16, step311]: loss 0.446197
[epoch16, step312]: loss 0.449770
[epoch16, step313]: loss 0.422002
[epoch16, step314]: loss 0.605671
[epoch16, step315]: loss 0.556041
[epoch16, step316]: loss 0.704376
[epoch16, step317]: loss 0.611403
[epoch16, step318]: loss 0.334300
[epoch16, step319]: loss 0.439907
[epoch16, step320]: loss 0.397786
[epoch16, step321]: loss 0.445117
[epoch16, step322]: loss 0.307006
[epoch16, step323]: loss 0.398462
[epoch16, step324]: loss 0.465237
[epoch16, step325]: loss 0.483506
[epoch16, step326]: loss 0.380052
[epoch16, step327]: loss 0.623409
[epoch16, step328]: loss 0.452179
[epoch16, step329]: loss 0.504919
[epoch16, step330]: loss 0.416057
[epoch16, step331]: loss 0.340226
[epoch16, step332]: loss 0.613410
[epoch16, step333]: loss 0.390257
[epoch16, step334]: loss 0.320453
[epoch16, step335]: loss 0.315693
[epoch16, step336]: loss 0.415554
[epoch16, step337]: loss 0.375721
[epoch16, step338]: loss 0.251388
[epoch16, step339]: loss 0.453953
[epoch16, step340]: loss 0.496382
[epoch16, step341]: loss 0.660245
[epoch16, step342]: loss 0.423855
[epoch16, step343]: loss 0.520333
[epoch16, step344]: loss 0.402608
[epoch16, step345]: loss 0.286018
[epoch16, step346]: loss 0.448040
[epoch16, step347]: loss 0.503325
[epoch16, step348]: loss 0.258272
[epoch16, step349]: loss 0.542124
[epoch16, step350]: loss 0.620819
[epoch16, step351]: loss 0.600009
[epoch16, step352]: loss 0.452461
[epoch16, step353]: loss 0.577577
[epoch16, step354]: loss 0.241500
[epoch16, step355]: loss 0.184240
[epoch16, step356]: loss 0.721747
[epoch16, step357]: loss 0.651180
[epoch16, step358]: loss 0.565729
[epoch16, step359]: loss 0.333762
[epoch16, step360]: loss 0.367311
[epoch16, step361]: loss 0.566073
[epoch16, step362]: loss 0.603785
[epoch16, step363]: loss 0.293327
[epoch16, step364]: loss 0.356950
[epoch16, step365]: loss 0.470560
[epoch16, step366]: loss 0.469191
[epoch16, step367]: loss 0.230958
[epoch16, step368]: loss 0.668048
[epoch16, step369]: loss 0.627922
[epoch16, step370]: loss 0.480771
[epoch16, step371]: loss 0.365183
[epoch16, step372]: loss 0.711506
[epoch16, step373]: loss 0.498454
[epoch16, step374]: loss 0.482441
[epoch16, step375]: loss 0.188995
[epoch16, step376]: loss 0.582253
[epoch16, step377]: loss 0.342484
[epoch16, step378]: loss 0.350669
[epoch16, step379]: loss 0.322604
[epoch16, step380]: loss 0.500045
[epoch16, step381]: loss 0.360680
[epoch16, step382]: loss 0.530796
[epoch16, step383]: loss 0.458750
[epoch16, step384]: loss 0.362471
[epoch16, step385]: loss 0.389288
[epoch16, step386]: loss 0.405405
[epoch16, step387]: loss 0.517336
[epoch16, step388]: loss 0.308553
[epoch16, step389]: loss 0.563607
[epoch16, step390]: loss 0.694066
[epoch16, step391]: loss 0.450704
[epoch16, step392]: loss 0.344646
[epoch16, step393]: loss 0.511735
[epoch16, step394]: loss 0.550001
[epoch16, step395]: loss 0.462009
[epoch16, step396]: loss 0.688447
[epoch16, step397]: loss 0.233436
[epoch16, step398]: loss 0.545394
[epoch16, step399]: loss 0.584031
[epoch16, step400]: loss 0.269106
[epoch16, step401]: loss 0.406795
[epoch16, step402]: loss 0.500253
[epoch16, step403]: loss 0.542896
[epoch16, step404]: loss 0.606190
[epoch16, step405]: loss 0.646537
[epoch16, step406]: loss 0.566406
[epoch16, step407]: loss 0.324497
[epoch16, step408]: loss 0.483858
[epoch16, step409]: loss 0.308369
[epoch16, step410]: loss 0.486332
[epoch16, step411]: loss 0.417311
[epoch16, step412]: loss 0.551871
[epoch16, step413]: loss 0.656025
[epoch16, step414]: loss 0.584254
[epoch16, step415]: loss 0.607422
[epoch16, step416]: loss 0.373418
[epoch16, step417]: loss 0.143179
[epoch16, step418]: loss 0.531502
[epoch16, step419]: loss 0.499893
[epoch16, step420]: loss 0.521956
[epoch16, step421]: loss 0.386364
[epoch16, step422]: loss 0.578976
[epoch16, step423]: loss 0.622851
[epoch16, step424]: loss 0.502322
[epoch16, step425]: loss 0.412231
[epoch16, step426]: loss 0.491331
[epoch16, step427]: loss 0.493942
[epoch16, step428]: loss 0.591325
[epoch16, step429]: loss 0.448038
[epoch16, step430]: loss 0.358014
[epoch16, step431]: loss 0.656499
[epoch16, step432]: loss 0.584805
[epoch16, step433]: loss 0.607485
[epoch16, step434]: loss 0.554729
[epoch16, step435]: loss 0.465841
[epoch16, step436]: loss 0.368642
[epoch16, step437]: loss 0.401627
[epoch16, step438]: loss 0.510252
[epoch16, step439]: loss 0.343196
[epoch16, step440]: loss 0.380938
[epoch16, step441]: loss 0.567648
[epoch16, step442]: loss 0.668425
[epoch16, step443]: loss 0.326078
[epoch16, step444]: loss 0.520886
[epoch16, step445]: loss 0.457455
[epoch16, step446]: loss 0.513534
[epoch16, step447]: loss 0.263154
[epoch16, step448]: loss 0.341443
[epoch16, step449]: loss 0.486126
[epoch16, step450]: loss 0.655002
[epoch16, step451]: loss 0.688971
[epoch16, step452]: loss 0.561347
[epoch16, step453]: loss 0.430483
[epoch16, step454]: loss 0.213594
[epoch16, step455]: loss 0.632910
[epoch16, step456]: loss 0.443502
[epoch16, step457]: loss 0.323426
[epoch16, step458]: loss 0.605355
[epoch16, step459]: loss 0.335669
[epoch16, step460]: loss 0.632766
[epoch16, step461]: loss 0.414463
[epoch16, step462]: loss 0.492430
[epoch16, step463]: loss 0.400032
[epoch16, step464]: loss 0.305358
[epoch16, step465]: loss 0.599715
[epoch16, step466]: loss 0.425749
[epoch16, step467]: loss 0.517855
[epoch16, step468]: loss 0.385656
[epoch16, step469]: loss 0.570800
[epoch16, step470]: loss 0.499467
[epoch16, step471]: loss 0.367162
[epoch16, step472]: loss 0.445569
[epoch16, step473]: loss 0.276538
[epoch16, step474]: loss 0.516650
[epoch16, step475]: loss 0.151495
[epoch16, step476]: loss 0.434627
[epoch16, step477]: loss 0.699925
[epoch16, step478]: loss 0.556255
[epoch16, step479]: loss 0.324972
[epoch16, step480]: loss 0.480425
[epoch16, step481]: loss 0.465679
[epoch16, step482]: loss 0.668209
[epoch16, step483]: loss 0.566950
[epoch16, step484]: loss 0.390949
[epoch16, step485]: loss 0.291789
[epoch16, step486]: loss 0.489028
[epoch16, step487]: loss 0.452745
[epoch16, step488]: loss 0.645330
[epoch16, step489]: loss 0.318324
[epoch16, step490]: loss 0.677851
[epoch16, step491]: loss 0.335021
[epoch16, step492]: loss 0.400291
[epoch16, step493]: loss 0.336136
[epoch16, step494]: loss 0.679296
[epoch16, step495]: loss 0.523535
[epoch16, step496]: loss 0.452264
[epoch16, step497]: loss 0.594878
[epoch16, step498]: loss 0.157651
[epoch16, step499]: loss 0.427100
[epoch16, step500]: loss 0.489856
[epoch16, step501]: loss 0.507659
[epoch16, step502]: loss 0.624862
[epoch16, step503]: loss 0.580498
[epoch16, step504]: loss 0.489321
[epoch16, step505]: loss 0.648729
[epoch16, step506]: loss 0.590862
[epoch16, step507]: loss 0.678110
[epoch16, step508]: loss 0.445199
[epoch16, step509]: loss 0.272021
[epoch16, step510]: loss 0.415110
[epoch16, step511]: loss 0.588126
[epoch16, step512]: loss 0.262652
[epoch16, step513]: loss 0.204542
[epoch16, step514]: loss 0.685003
[epoch16, step515]: loss 0.385308
[epoch16, step516]: loss 0.476222
[epoch16, step517]: loss 0.536271
[epoch16, step518]: loss 0.302075
[epoch16, step519]: loss 0.529538
[epoch16, step520]: loss 0.234342
[epoch16, step521]: loss 0.497358
[epoch16, step522]: loss 0.467249
[epoch16, step523]: loss 0.501228
[epoch16, step524]: loss 0.424018
[epoch16, step525]: loss 0.576113
[epoch16, step526]: loss 0.344415
[epoch16, step527]: loss 0.260572
[epoch16, step528]: loss 0.445489
[epoch16, step529]: loss 0.549868
[epoch16, step530]: loss 0.377675
[epoch16, step531]: loss 0.534422
[epoch16, step532]: loss 0.583250
[epoch16, step533]: loss 0.423604
[epoch16, step534]: loss 0.356218
[epoch16, step535]: loss 0.563598
[epoch16, step536]: loss 0.366756
[epoch16, step537]: loss 0.458440
[epoch16, step538]: loss 0.434040
[epoch16, step539]: loss 0.438710
[epoch16, step540]: loss 0.449342
[epoch16, step541]: loss 0.428047
[epoch16, step542]: loss 0.689411
[epoch16, step543]: loss 0.570691
[epoch16, step544]: loss 0.701741
[epoch16, step545]: loss 0.513766
[epoch16, step546]: loss 0.455389
[epoch16, step547]: loss 0.617094
[epoch16, step548]: loss 0.378231
[epoch16, step549]: loss 0.496704
[epoch16, step550]: loss 0.588954
[epoch16, step551]: loss 0.602255
[epoch16, step552]: loss 0.369961
[epoch16, step553]: loss 0.510564
[epoch16, step554]: loss 0.651676
[epoch16, step555]: loss 0.252918
[epoch16, step556]: loss 0.516537
[epoch16, step557]: loss 0.524495
[epoch16, step558]: loss 0.481974
[epoch16, step559]: loss 0.522472
[epoch16, step560]: loss 0.264455
[epoch16, step561]: loss 0.611136
[epoch16, step562]: loss 0.729425
[epoch16, step563]: loss 0.527805
[epoch16, step564]: loss 0.443286
[epoch16, step565]: loss 0.231757
[epoch16, step566]: loss 0.292710
[epoch16, step567]: loss 0.349903
[epoch16, step568]: loss 0.520512
[epoch16, step569]: loss 0.168008
[epoch16, step570]: loss 0.190954
[epoch16, step571]: loss 0.350552
[epoch16, step572]: loss 0.359428
[epoch16, step573]: loss 0.386728
[epoch16, step574]: loss 0.433548
[epoch16, step575]: loss 0.428054
[epoch16, step576]: loss 0.281071
[epoch16, step577]: loss 0.571324
[epoch16, step578]: loss 0.740995
[epoch16, step579]: loss 0.476703
[epoch16, step580]: loss 0.658473
[epoch16, step581]: loss 0.444732
[epoch16, step582]: loss 0.563049
[epoch16, step583]: loss 0.420307
[epoch16, step584]: loss 0.412425
[epoch16, step585]: loss 0.533412
[epoch16, step586]: loss 0.149311
[epoch16, step587]: loss 0.639372
[epoch16, step588]: loss 0.354446
[epoch16, step589]: loss 0.431135
[epoch16, step590]: loss 0.573141
[epoch16, step591]: loss 0.452743
[epoch16, step592]: loss 0.453491
[epoch16, step593]: loss 0.404303
[epoch16, step594]: loss 0.448270
[epoch16, step595]: loss 0.519521
[epoch16, step596]: loss 0.524829
[epoch16, step597]: loss 0.490562
[epoch16, step598]: loss 0.649856
[epoch16, step599]: loss 0.381095
[epoch16, step600]: loss 0.281082
[epoch16, step601]: loss 0.511078
[epoch16, step602]: loss 0.519604
[epoch16, step603]: loss 0.478457
[epoch16, step604]: loss 0.514211
[epoch16, step605]: loss 0.587855
[epoch16, step606]: loss 0.375210
[epoch16, step607]: loss 0.536805
[epoch16, step608]: loss 0.368075
[epoch16, step609]: loss 0.625771
[epoch16, step610]: loss 0.362519
[epoch16, step611]: loss 0.558255
[epoch16, step612]: loss 0.594207
[epoch16, step613]: loss 0.326773
[epoch16, step614]: loss 0.483054
[epoch16, step615]: loss 0.625015
[epoch16, step616]: loss 0.359895
[epoch16, step617]: loss 0.351369
[epoch16, step618]: loss 0.447652
[epoch16, step619]: loss 0.345205
[epoch16, step620]: loss 0.620222
[epoch16, step621]: loss 0.518068
[epoch16, step622]: loss 0.391951
[epoch16, step623]: loss 0.490119
[epoch16, step624]: loss 0.487164
[epoch16, step625]: loss 0.476545
[epoch16, step626]: loss 0.537863
[epoch16, step627]: loss 0.424526
[epoch16, step628]: loss 0.350929
[epoch16, step629]: loss 0.385858
[epoch16, step630]: loss 0.385070
[epoch16, step631]: loss 0.651126
[epoch16, step632]: loss 0.547038
[epoch16, step633]: loss 0.343475
[epoch16, step634]: loss 0.325204
[epoch16, step635]: loss 0.185015
[epoch16, step636]: loss 0.365036
[epoch16, step637]: loss 0.598891
[epoch16, step638]: loss 0.550074
[epoch16, step639]: loss 0.659584
[epoch16, step640]: loss 0.707611
[epoch16, step641]: loss 0.574763
[epoch16, step642]: loss 0.148325
[epoch16, step643]: loss 0.485474
[epoch16, step644]: loss 0.397201
[epoch16, step645]: loss 0.241399
[epoch16, step646]: loss 0.462333
[epoch16, step647]: loss 0.484142
[epoch16, step648]: loss 0.568339
[epoch16, step649]: loss 0.524567
[epoch16, step650]: loss 0.584872
[epoch16, step651]: loss 0.346191
[epoch16, step652]: loss 0.459537
[epoch16, step653]: loss 0.341732
[epoch16, step654]: loss 0.509184
[epoch16, step655]: loss 0.426163
[epoch16, step656]: loss 0.570673
[epoch16, step657]: loss 0.550106
[epoch16, step658]: loss 0.349738
[epoch16, step659]: loss 0.516995
[epoch16, step660]: loss 0.613885
[epoch16, step661]: loss 0.492951
[epoch16, step662]: loss 0.466147
[epoch16, step663]: loss 0.742246
[epoch16, step664]: loss 0.391432
[epoch16, step665]: loss 0.502096
[epoch16, step666]: loss 0.636251
[epoch16, step667]: loss 0.193037
[epoch16, step668]: loss 0.545073
[epoch16, step669]: loss 0.325589
[epoch16, step670]: loss 0.477502
[epoch16, step671]: loss 0.360652
[epoch16, step672]: loss 0.441864
[epoch16, step673]: loss 0.569616
[epoch16, step674]: loss 0.444741
[epoch16, step675]: loss 0.422743
[epoch16, step676]: loss 0.415844
[epoch16, step677]: loss 0.461534
[epoch16, step678]: loss 0.503524
[epoch16, step679]: loss 0.449788
[epoch16, step680]: loss 0.420669
[epoch16, step681]: loss 0.626742
[epoch16, step682]: loss 0.308214
[epoch16, step683]: loss 0.305222
[epoch16, step684]: loss 0.563955
[epoch16, step685]: loss 0.713288
[epoch16, step686]: loss 0.327272
[epoch16, step687]: loss 0.424906
[epoch16, step688]: loss 0.367681
[epoch16, step689]: loss 0.454731
[epoch16, step690]: loss 0.529376
[epoch16, step691]: loss 0.524553
[epoch16, step692]: loss 0.633482
[epoch16, step693]: loss 0.467667
[epoch16, step694]: loss 0.627964
[epoch16, step695]: loss 0.496893
[epoch16, step696]: loss 0.606930
[epoch16, step697]: loss 0.584811
[epoch16, step698]: loss 0.616833
[epoch16, step699]: loss 0.607446
[epoch16, step700]: loss 0.277091
[epoch16, step701]: loss 0.337262
[epoch16, step702]: loss 0.492860
[epoch16, step703]: loss 0.522667
[epoch16, step704]: loss 0.268562
[epoch16, step705]: loss 0.387501
[epoch16, step706]: loss 0.384000
[epoch16, step707]: loss 0.604255
[epoch16, step708]: loss 0.384276
[epoch16, step709]: loss 0.535385
[epoch16, step710]: loss 0.680243
[epoch16, step711]: loss 0.454781
[epoch16, step712]: loss 0.196192
[epoch16, step713]: loss 0.647886
[epoch16, step714]: loss 0.441798
[epoch16, step715]: loss 0.395096
[epoch16, step716]: loss 0.573419
[epoch16, step717]: loss 0.402736
[epoch16, step718]: loss 0.643282
[epoch16, step719]: loss 0.468638
[epoch16, step720]: loss 0.491976
[epoch16, step721]: loss 0.425088
[epoch16, step722]: loss 0.713722
[epoch16, step723]: loss 0.413840
[epoch16, step724]: loss 0.514967
[epoch16, step725]: loss 0.477231
[epoch16, step726]: loss 0.471408
[epoch16, step727]: loss 0.639108
[epoch16, step728]: loss 0.478707
[epoch16, step729]: loss 0.559091
[epoch16, step730]: loss 0.535616
[epoch16, step731]: loss 0.486527
[epoch16, step732]: loss 0.492098
[epoch16, step733]: loss 0.494471
[epoch16, step734]: loss 0.349996
[epoch16, step735]: loss 0.302216
[epoch16, step736]: loss 0.441426
[epoch16, step737]: loss 0.753301
[epoch16, step738]: loss 0.261437
[epoch16, step739]: loss 0.286828
[epoch16, step740]: loss 0.294534
[epoch16, step741]: loss 0.430513
[epoch16, step742]: loss 0.408624
[epoch16, step743]: loss 0.485641
[epoch16, step744]: loss 0.471793
[epoch16, step745]: loss 0.463402
[epoch16, step746]: loss 0.479822
[epoch16, step747]: loss 0.551785
[epoch16, step748]: loss 0.426459
[epoch16, step749]: loss 0.426691
[epoch16, step750]: loss 0.575237
[epoch16, step751]: loss 0.668142
[epoch16, step752]: loss 0.257242
[epoch16, step753]: loss 0.423092
[epoch16, step754]: loss 0.395715
[epoch16, step755]: loss 0.435182
[epoch16, step756]: loss 0.222110
[epoch16, step757]: loss 0.553925
[epoch16, step758]: loss 0.484300
[epoch16, step759]: loss 0.284940
[epoch16, step760]: loss 0.408894
[epoch16, step761]: loss 0.583937
[epoch16, step762]: loss 0.526645
[epoch16, step763]: loss 0.430574
[epoch16, step764]: loss 0.657556
[epoch16, step765]: loss 0.643224
[epoch16, step766]: loss 0.345258
[epoch16, step767]: loss 0.481628
[epoch16, step768]: loss 0.435932
[epoch16, step769]: loss 0.377724
[epoch16, step770]: loss 0.556758
[epoch16, step771]: loss 0.669073
[epoch16, step772]: loss 0.425540
[epoch16, step773]: loss 0.502867
[epoch16, step774]: loss 0.522102
[epoch16, step775]: loss 0.659947
[epoch16, step776]: loss 0.411199
[epoch16, step777]: loss 0.377593
[epoch16, step778]: loss 0.672852
[epoch16, step779]: loss 0.416782
[epoch16, step780]: loss 0.554840
[epoch16, step781]: loss 0.502081
[epoch16, step782]: loss 0.318794
[epoch16, step783]: loss 0.563803
[epoch16, step784]: loss 0.469390
[epoch16, step785]: loss 0.430114
[epoch16, step786]: loss 0.371011
[epoch16, step787]: loss 0.464520
[epoch16, step788]: loss 0.503194
[epoch16, step789]: loss 0.505024
[epoch16, step790]: loss 0.536775
[epoch16, step791]: loss 0.338799
[epoch16, step792]: loss 0.244313
[epoch16, step793]: loss 0.422910
[epoch16, step794]: loss 0.470274
[epoch16, step795]: loss 0.324342
[epoch16, step796]: loss 0.579780
[epoch16, step797]: loss 0.447381
[epoch16, step798]: loss 0.375124
[epoch16, step799]: loss 0.491075
[epoch16, step800]: loss 0.657839
[epoch16, step801]: loss 0.502982
[epoch16, step802]: loss 0.453876
[epoch16, step803]: loss 0.381267
[epoch16, step804]: loss 0.285344
[epoch16, step805]: loss 0.608338
[epoch16, step806]: loss 0.340728
[epoch16, step807]: loss 0.481039
[epoch16, step808]: loss 0.535795
[epoch16, step809]: loss 0.541261
[epoch16, step810]: loss 0.511973
[epoch16, step811]: loss 0.478302
[epoch16, step812]: loss 0.428694
[epoch16, step813]: loss 0.479058
[epoch16, step814]: loss 0.569723
[epoch16, step815]: loss 0.533565
[epoch16, step816]: loss 0.601503
[epoch16, step817]: loss 0.701489
[epoch16, step818]: loss 0.325639
[epoch16, step819]: loss 0.621595
[epoch16, step820]: loss 0.498394
[epoch16, step821]: loss 0.270149
[epoch16, step822]: loss 0.246334
[epoch16, step823]: loss 0.561752
[epoch16, step824]: loss 0.249488
[epoch16, step825]: loss 0.529200
[epoch16, step826]: loss 0.470889
[epoch16, step827]: loss 0.482226
[epoch16, step828]: loss 0.569303
[epoch16, step829]: loss 0.587428
[epoch16, step830]: loss 0.495554
[epoch16, step831]: loss 0.415040
[epoch16, step832]: loss 0.238423
[epoch16, step833]: loss 0.677116
[epoch16, step834]: loss 0.444612
[epoch16, step835]: loss 0.409397
[epoch16, step836]: loss 0.441931
[epoch16, step837]: loss 0.153380
[epoch16, step838]: loss 0.571691
[epoch16, step839]: loss 0.286000
[epoch16, step840]: loss 0.378015
[epoch16, step841]: loss 0.264059
[epoch16, step842]: loss 0.339947
[epoch16, step843]: loss 0.401495
[epoch16, step844]: loss 0.447791
[epoch16, step845]: loss 0.545362
[epoch16, step846]: loss 0.505890
[epoch16, step847]: loss 0.385132
[epoch16, step848]: loss 0.491195
[epoch16, step849]: loss 0.479254
[epoch16, step850]: loss 0.531405
[epoch16, step851]: loss 0.595863
[epoch16, step852]: loss 0.384192
[epoch16, step853]: loss 0.355950
[epoch16, step854]: loss 0.442265
[epoch16, step855]: loss 0.532833
[epoch16, step856]: loss 0.435308
[epoch16, step857]: loss 0.504413
[epoch16, step858]: loss 0.382680
[epoch16, step859]: loss 0.622907
[epoch16, step860]: loss 0.391338
[epoch16, step861]: loss 0.395538
[epoch16, step862]: loss 0.366707
[epoch16, step863]: loss 0.474515
[epoch16, step864]: loss 0.413217
[epoch16, step865]: loss 0.450721
[epoch16, step866]: loss 0.592193
[epoch16, step867]: loss 0.507058
[epoch16, step868]: loss 0.425126
[epoch16, step869]: loss 0.543295
[epoch16, step870]: loss 0.476693
[epoch16, step871]: loss 0.518995
[epoch16, step872]: loss 0.187840
[epoch16, step873]: loss 0.373875
[epoch16, step874]: loss 0.409657
[epoch16, step875]: loss 0.258900
[epoch16, step876]: loss 0.305959
[epoch16, step877]: loss 0.204468
[epoch16, step878]: loss 0.442474
[epoch16, step879]: loss 0.441441
[epoch16, step880]: loss 0.341079
[epoch16, step881]: loss 0.541779
[epoch16, step882]: loss 0.611686
[epoch16, step883]: loss 0.512631
[epoch16, step884]: loss 0.461315
[epoch16, step885]: loss 0.547178
[epoch16, step886]: loss 0.398577
[epoch16, step887]: loss 0.399329
[epoch16, step888]: loss 0.582265
[epoch16, step889]: loss 0.677434
[epoch16, step890]: loss 0.628765
[epoch16, step891]: loss 0.304630
[epoch16, step892]: loss 0.564515
[epoch16, step893]: loss 0.520242
[epoch16, step894]: loss 0.518306
[epoch16, step895]: loss 0.468366
[epoch16, step896]: loss 0.453521
[epoch16, step897]: loss 0.333590
[epoch16, step898]: loss 0.413639
[epoch16, step899]: loss 0.534538
[epoch16, step900]: loss 0.347056
[epoch16, step901]: loss 0.616125
[epoch16, step902]: loss 0.651825
[epoch16, step903]: loss 0.528915
[epoch16, step904]: loss 0.466399
[epoch16, step905]: loss 0.695260
[epoch16, step906]: loss 0.397808
[epoch16, step907]: loss 0.467549
[epoch16, step908]: loss 0.660429
[epoch16, step909]: loss 0.585610
[epoch16, step910]: loss 0.537362
[epoch16, step911]: loss 0.442330
[epoch16, step912]: loss 0.372130
[epoch16, step913]: loss 0.450757
[epoch16, step914]: loss 0.639244
[epoch16, step915]: loss 0.509583
[epoch16, step916]: loss 0.336509
[epoch16, step917]: loss 0.583228
[epoch16, step918]: loss 0.501929
[epoch16, step919]: loss 0.105661
[epoch16, step920]: loss 0.757047
[epoch16, step921]: loss 0.320297
[epoch16, step922]: loss 0.427351
[epoch16, step923]: loss 0.769566
[epoch16, step924]: loss 0.483224
[epoch16, step925]: loss 0.574856
[epoch16, step926]: loss 0.533082
[epoch16, step927]: loss 0.455148
[epoch16, step928]: loss 0.562111
[epoch16, step929]: loss 0.498933
[epoch16, step930]: loss 0.490817
[epoch16, step931]: loss 0.575209
[epoch16, step932]: loss 0.390412
[epoch16, step933]: loss 0.565836
[epoch16, step934]: loss 0.539796
[epoch16, step935]: loss 0.571808
[epoch16, step936]: loss 0.528041
[epoch16, step937]: loss 0.491112
[epoch16, step938]: loss 0.360310
[epoch16, step939]: loss 0.669381
[epoch16, step940]: loss 0.702557
[epoch16, step941]: loss 0.596494
[epoch16, step942]: loss 0.514211
[epoch16, step943]: loss 0.609841
[epoch16, step944]: loss 0.570738
[epoch16, step945]: loss 0.581234
[epoch16, step946]: loss 0.313643
[epoch16, step947]: loss 0.274124
[epoch16, step948]: loss 0.569055
[epoch16, step949]: loss 0.345030
[epoch16, step950]: loss 0.405829
[epoch16, step951]: loss 0.388125
[epoch16, step952]: loss 0.553711
[epoch16, step953]: loss 0.341093
[epoch16, step954]: loss 0.371722
[epoch16, step955]: loss 0.391748
[epoch16, step956]: loss 0.708496
[epoch16, step957]: loss 0.674181
[epoch16, step958]: loss 0.758514
[epoch16, step959]: loss 0.660222
[epoch16, step960]: loss 0.631097
[epoch16, step961]: loss 0.548522
[epoch16, step962]: loss 0.447593
[epoch16, step963]: loss 0.393677
[epoch16, step964]: loss 0.416844
[epoch16, step965]: loss 0.551311
[epoch16, step966]: loss 0.528754
[epoch16, step967]: loss 0.307106
[epoch16, step968]: loss 0.445293
[epoch16, step969]: loss 0.310599
[epoch16, step970]: loss 0.719641
[epoch16, step971]: loss 0.415936
[epoch16, step972]: loss 0.419170
[epoch16, step973]: loss 0.645189
[epoch16, step974]: loss 0.528122
[epoch16, step975]: loss 0.396542
[epoch16, step976]: loss 0.445619
[epoch16, step977]: loss 0.535791
[epoch16, step978]: loss 0.576188
[epoch16, step979]: loss 0.165142
[epoch16, step980]: loss 0.400495
[epoch16, step981]: loss 0.635337
[epoch16, step982]: loss 0.391477
[epoch16, step983]: loss 0.411675
[epoch16, step984]: loss 0.389855
[epoch16, step985]: loss 0.428662
[epoch16, step986]: loss 0.544920
[epoch16, step987]: loss 0.602235
[epoch16, step988]: loss 0.529023
[epoch16, step989]: loss 0.310008
[epoch16, step990]: loss 0.289284
[epoch16, step991]: loss 0.560786
[epoch16, step992]: loss 0.303439
[epoch16, step993]: loss 0.487512
[epoch16, step994]: loss 0.464213
[epoch16, step995]: loss 0.439704
[epoch16, step996]: loss 0.528257
[epoch16, step997]: loss 0.339715
[epoch16, step998]: loss 0.290276
[epoch16, step999]: loss 0.384294
[epoch16, step1000]: loss 0.511301
[epoch16, step1001]: loss 0.646324
[epoch16, step1002]: loss 0.280482
[epoch16, step1003]: loss 0.118390
[epoch16, step1004]: loss 0.548301
[epoch16, step1005]: loss 0.367193
[epoch16, step1006]: loss 0.310278
[epoch16, step1007]: loss 0.585249
[epoch16, step1008]: loss 0.451518
[epoch16, step1009]: loss 0.474238
[epoch16, step1010]: loss 0.519610
[epoch16, step1011]: loss 0.708310
[epoch16, step1012]: loss 0.565878
[epoch16, step1013]: loss 0.573134
[epoch16, step1014]: loss 0.535537
[epoch16, step1015]: loss 0.277733
[epoch16, step1016]: loss 0.631698
[epoch16, step1017]: loss 0.612097
[epoch16, step1018]: loss 0.557421
[epoch16, step1019]: loss 0.423017
[epoch16, step1020]: loss 0.673217
[epoch16, step1021]: loss 0.280462
[epoch16, step1022]: loss 0.473605
[epoch16, step1023]: loss 0.496747
[epoch16, step1024]: loss 0.475827
[epoch16, step1025]: loss 0.463190
[epoch16, step1026]: loss 0.481400
[epoch16, step1027]: loss 0.577911
[epoch16, step1028]: loss 0.418944
[epoch16, step1029]: loss 0.470511
[epoch16, step1030]: loss 0.503820
[epoch16, step1031]: loss 0.325372
[epoch16, step1032]: loss 0.609323
[epoch16, step1033]: loss 0.346683
[epoch16, step1034]: loss 0.447371
[epoch16, step1035]: loss 0.313303
[epoch16, step1036]: loss 0.469000
[epoch16, step1037]: loss 0.613273
[epoch16, step1038]: loss 0.146624
[epoch16, step1039]: loss 0.600668
[epoch16, step1040]: loss 0.679787
[epoch16, step1041]: loss 0.198495
[epoch16, step1042]: loss 0.435665
[epoch16, step1043]: loss 0.578342
[epoch16, step1044]: loss 0.605746
[epoch16, step1045]: loss 0.475806
[epoch16, step1046]: loss 0.717411
[epoch16, step1047]: loss 0.251971
[epoch16, step1048]: loss 0.635027
[epoch16, step1049]: loss 0.631491
[epoch16, step1050]: loss 0.438311
[epoch16, step1051]: loss 0.498030
[epoch16, step1052]: loss 0.322319
[epoch16, step1053]: loss 0.294059
[epoch16, step1054]: loss 0.555397
[epoch16, step1055]: loss 0.490094
[epoch16, step1056]: loss 0.392617
[epoch16, step1057]: loss 0.480703
[epoch16, step1058]: loss 0.506043
[epoch16, step1059]: loss 0.595758
[epoch16, step1060]: loss 0.613255
[epoch16, step1061]: loss 0.256549
[epoch16, step1062]: loss 0.548652
[epoch16, step1063]: loss 0.493428
[epoch16, step1064]: loss 0.709962
[epoch16, step1065]: loss 0.561675
[epoch16, step1066]: loss 0.567899
[epoch16, step1067]: loss 0.593845
[epoch16, step1068]: loss 0.486011
[epoch16, step1069]: loss 0.448908
[epoch16, step1070]: loss 0.398124
[epoch16, step1071]: loss 0.281104
[epoch16, step1072]: loss 0.313530
[epoch16, step1073]: loss 0.326548
[epoch16, step1074]: loss 0.376600
[epoch16, step1075]: loss 0.523894
[epoch16, step1076]: loss 0.407779
[epoch16, step1077]: loss 0.352316
[epoch16, step1078]: loss 0.449196
[epoch16, step1079]: loss 0.592892
[epoch16, step1080]: loss 0.544099
[epoch16, step1081]: loss 0.494991
[epoch16, step1082]: loss 0.563830
[epoch16, step1083]: loss 0.486882
[epoch16, step1084]: loss 0.294683
[epoch16, step1085]: loss 0.393595
[epoch16, step1086]: loss 0.340767
[epoch16, step1087]: loss 0.454625
[epoch16, step1088]: loss 0.722745
[epoch16, step1089]: loss 0.468550
[epoch16, step1090]: loss 0.410580
[epoch16, step1091]: loss 0.192814
[epoch16, step1092]: loss 0.425482
[epoch16, step1093]: loss 0.405855
[epoch16, step1094]: loss 0.513595
[epoch16, step1095]: loss 0.405740
[epoch16, step1096]: loss 0.237531
[epoch16, step1097]: loss 0.335537
[epoch16, step1098]: loss 0.479231
[epoch16, step1099]: loss 0.329780
[epoch16, step1100]: loss 0.298499
[epoch16, step1101]: loss 0.518254
[epoch16, step1102]: loss 0.398138
[epoch16, step1103]: loss 0.587982
[epoch16, step1104]: loss 0.234787
[epoch16, step1105]: loss 0.467607
[epoch16, step1106]: loss 0.424873
[epoch16, step1107]: loss 0.572416
[epoch16, step1108]: loss 0.513351
[epoch16, step1109]: loss 0.411505
[epoch16, step1110]: loss 0.373181
[epoch16, step1111]: loss 0.267704
[epoch16, step1112]: loss 0.478372
[epoch16, step1113]: loss 0.584737
[epoch16, step1114]: loss 0.424795
[epoch16, step1115]: loss 0.354952
[epoch16, step1116]: loss 0.428212
[epoch16, step1117]: loss 0.626703
[epoch16, step1118]: loss 0.536615
[epoch16, step1119]: loss 0.437504
[epoch16, step1120]: loss 0.454190
[epoch16, step1121]: loss 0.353281
[epoch16, step1122]: loss 0.499328
[epoch16, step1123]: loss 0.494747
[epoch16, step1124]: loss 0.404309
[epoch16, step1125]: loss 0.128991
[epoch16, step1126]: loss 0.574209
[epoch16, step1127]: loss 0.372700
[epoch16, step1128]: loss 0.380395
[epoch16, step1129]: loss 0.595062
[epoch16, step1130]: loss 0.520967
[epoch16, step1131]: loss 0.395895
[epoch16, step1132]: loss 0.801289
[epoch16, step1133]: loss 0.462469
[epoch16, step1134]: loss 0.583523
[epoch16, step1135]: loss 0.610358
[epoch16, step1136]: loss 0.578568
[epoch16, step1137]: loss 0.549649
[epoch16, step1138]: loss 0.358632
[epoch16, step1139]: loss 0.664537
[epoch16, step1140]: loss 0.682742
[epoch16, step1141]: loss 0.549229
[epoch16, step1142]: loss 0.263111
[epoch16, step1143]: loss 0.429483
[epoch16, step1144]: loss 0.478470
[epoch16, step1145]: loss 0.294422
[epoch16, step1146]: loss 0.438295
[epoch16, step1147]: loss 0.268683
[epoch16, step1148]: loss 0.541223
[epoch16, step1149]: loss 0.349200
[epoch16, step1150]: loss 0.313045
[epoch16, step1151]: loss 0.367673
[epoch16, step1152]: loss 0.443210
[epoch16, step1153]: loss 0.546171
[epoch16, step1154]: loss 0.706349
[epoch16, step1155]: loss 0.547780
[epoch16, step1156]: loss 0.356504
[epoch16, step1157]: loss 0.277066
[epoch16, step1158]: loss 0.477147
[epoch16, step1159]: loss 0.365647
[epoch16, step1160]: loss 0.667385
[epoch16, step1161]: loss 0.526173
[epoch16, step1162]: loss 0.409865
[epoch16, step1163]: loss 0.484446
[epoch16, step1164]: loss 0.544152
[epoch16, step1165]: loss 0.513782
[epoch16, step1166]: loss 0.620207
[epoch16, step1167]: loss 0.337025
[epoch16, step1168]: loss 0.461717
[epoch16, step1169]: loss 0.463039
[epoch16, step1170]: loss 0.596861
[epoch16, step1171]: loss 0.521843
[epoch16, step1172]: loss 0.536353
[epoch16, step1173]: loss 0.492054
[epoch16, step1174]: loss 0.402801
[epoch16, step1175]: loss 0.273883
[epoch16, step1176]: loss 0.525705
[epoch16, step1177]: loss 0.523939
[epoch16, step1178]: loss 0.399060
[epoch16, step1179]: loss 0.682152
[epoch16, step1180]: loss 0.445942
[epoch16, step1181]: loss 0.541912
[epoch16, step1182]: loss 0.402429
[epoch16, step1183]: loss 0.313415
[epoch16, step1184]: loss 0.532610
[epoch16, step1185]: loss 0.409892
[epoch16, step1186]: loss 0.336369
[epoch16, step1187]: loss 0.493079
[epoch16, step1188]: loss 0.576074
[epoch16, step1189]: loss 0.498629
[epoch16, step1190]: loss 0.501554
[epoch16, step1191]: loss 0.579158
[epoch16, step1192]: loss 0.624707
[epoch16, step1193]: loss 0.606468
[epoch16, step1194]: loss 0.476288
[epoch16, step1195]: loss 0.513931
[epoch16, step1196]: loss 0.510641
[epoch16, step1197]: loss 0.378464
[epoch16, step1198]: loss 0.618702
[epoch16, step1199]: loss 0.449718
[epoch16, step1200]: loss 0.525320
[epoch16, step1201]: loss 0.278487
[epoch16, step1202]: loss 0.403094
[epoch16, step1203]: loss 0.355043
[epoch16, step1204]: loss 0.370493
[epoch16, step1205]: loss 0.330161
[epoch16, step1206]: loss 0.585823
[epoch16, step1207]: loss 0.484290
[epoch16, step1208]: loss 0.516701
[epoch16, step1209]: loss 0.438086
[epoch16, step1210]: loss 0.498392
[epoch16, step1211]: loss 0.710357
[epoch16, step1212]: loss 0.405616
[epoch16, step1213]: loss 0.375586
[epoch16, step1214]: loss 0.480963
[epoch16, step1215]: loss 0.186255
[epoch16, step1216]: loss 0.675669
[epoch16, step1217]: loss 0.550700
[epoch16, step1218]: loss 0.524284
[epoch16, step1219]: loss 0.569404
[epoch16, step1220]: loss 0.535093
[epoch16, step1221]: loss 0.657825
[epoch16, step1222]: loss 0.592895
[epoch16, step1223]: loss 0.391048
[epoch16, step1224]: loss 0.378690
[epoch16, step1225]: loss 0.634158
[epoch16, step1226]: loss 0.478748
[epoch16, step1227]: loss 0.467129
[epoch16, step1228]: loss 0.602658
[epoch16, step1229]: loss 0.490751
[epoch16, step1230]: loss 0.459732
[epoch16, step1231]: loss 0.348744
[epoch16, step1232]: loss 0.445046
[epoch16, step1233]: loss 0.551888
[epoch16, step1234]: loss 0.449319
[epoch16, step1235]: loss 0.382545
[epoch16, step1236]: loss 0.450439
[epoch16, step1237]: loss 0.628325
[epoch16, step1238]: loss 0.433481
[epoch16, step1239]: loss 0.531747
[epoch16, step1240]: loss 0.422554
[epoch16, step1241]: loss 0.415117
[epoch16, step1242]: loss 0.347739
[epoch16, step1243]: loss 0.506402
[epoch16, step1244]: loss 0.559719
[epoch16, step1245]: loss 0.633339
[epoch16, step1246]: loss 0.408730
[epoch16, step1247]: loss 0.564263
[epoch16, step1248]: loss 0.409473
[epoch16, step1249]: loss 0.359303
[epoch16, step1250]: loss 0.233381
[epoch16, step1251]: loss 0.568916
[epoch16, step1252]: loss 0.499205
[epoch16, step1253]: loss 0.447798
[epoch16, step1254]: loss 0.349866
[epoch16, step1255]: loss 0.170621
[epoch16, step1256]: loss 0.240763
[epoch16, step1257]: loss 0.442816
[epoch16, step1258]: loss 0.529184
[epoch16, step1259]: loss 0.595950
[epoch16, step1260]: loss 0.555869
[epoch16, step1261]: loss 0.564670
[epoch16, step1262]: loss 0.455157
[epoch16, step1263]: loss 0.610207
[epoch16, step1264]: loss 0.173859
[epoch16, step1265]: loss 0.502193
[epoch16, step1266]: loss 0.367510
[epoch16, step1267]: loss 0.330666
[epoch16, step1268]: loss 0.539023
[epoch16, step1269]: loss 0.362061
[epoch16, step1270]: loss 0.660766
[epoch16, step1271]: loss 0.504201
[epoch16, step1272]: loss 0.334038
[epoch16, step1273]: loss 0.508348
[epoch16, step1274]: loss 0.469215
[epoch16, step1275]: loss 0.555417
[epoch16, step1276]: loss 0.228848
[epoch16, step1277]: loss 0.547712
[epoch16, step1278]: loss 0.475813
[epoch16, step1279]: loss 0.677851
[epoch16, step1280]: loss 0.700237
[epoch16, step1281]: loss 0.433791
[epoch16, step1282]: loss 0.433338
[epoch16, step1283]: loss 0.369663
[epoch16, step1284]: loss 0.455482
[epoch16, step1285]: loss 0.569021
[epoch16, step1286]: loss 0.259825
[epoch16, step1287]: loss 0.612015
[epoch16, step1288]: loss 0.408009
[epoch16, step1289]: loss 0.319167
[epoch16, step1290]: loss 0.562708
[epoch16, step1291]: loss 0.284217
[epoch16, step1292]: loss 0.480327
[epoch16, step1293]: loss 0.324840
[epoch16, step1294]: loss 0.520544
[epoch16, step1295]: loss 0.611374
[epoch16, step1296]: loss 0.503467
[epoch16, step1297]: loss 0.592780
[epoch16, step1298]: loss 0.248765
[epoch16, step1299]: loss 0.487082
[epoch16, step1300]: loss 0.565737
[epoch16, step1301]: loss 0.486585
[epoch16, step1302]: loss 0.609382
[epoch16, step1303]: loss 0.560435
[epoch16, step1304]: loss 0.526650
[epoch16, step1305]: loss 0.514256
[epoch16, step1306]: loss 0.367506
[epoch16, step1307]: loss 0.451113
[epoch16, step1308]: loss 0.425905
[epoch16, step1309]: loss 0.808174
[epoch16, step1310]: loss 0.263801
[epoch16, step1311]: loss 0.578450
[epoch16, step1312]: loss 0.562513
[epoch16, step1313]: loss 0.434977
[epoch16, step1314]: loss 0.428642
[epoch16, step1315]: loss 0.474238
[epoch16, step1316]: loss 0.348891
[epoch16, step1317]: loss 0.396419
[epoch16, step1318]: loss 0.252925
[epoch16, step1319]: loss 0.679264
[epoch16, step1320]: loss 0.673081
[epoch16, step1321]: loss 0.481294
[epoch16, step1322]: loss 0.764069
[epoch16, step1323]: loss 0.608863
[epoch16, step1324]: loss 0.364081
[epoch16, step1325]: loss 0.289382
[epoch16, step1326]: loss 0.487716
[epoch16, step1327]: loss 0.527761
[epoch16, step1328]: loss 0.599186
[epoch16, step1329]: loss 0.568129
[epoch16, step1330]: loss 0.362203
[epoch16, step1331]: loss 0.537581
[epoch16, step1332]: loss 0.616065
[epoch16, step1333]: loss 0.564845
[epoch16, step1334]: loss 0.357964
[epoch16, step1335]: loss 0.344226
[epoch16, step1336]: loss 0.386709
[epoch16, step1337]: loss 0.498307
[epoch16, step1338]: loss 0.659790
[epoch16, step1339]: loss 0.584079
[epoch16, step1340]: loss 0.446555
[epoch16, step1341]: loss 0.733944
[epoch16, step1342]: loss 0.433139
[epoch16, step1343]: loss 0.546216
[epoch16, step1344]: loss 0.667249
[epoch16, step1345]: loss 0.211636
[epoch16, step1346]: loss 0.464046
[epoch16, step1347]: loss 0.658982
[epoch16, step1348]: loss 0.519053
[epoch16, step1349]: loss 0.586676
[epoch16, step1350]: loss 0.648575
[epoch16, step1351]: loss 0.299900
[epoch16, step1352]: loss 0.593581
[epoch16, step1353]: loss 0.325466
[epoch16, step1354]: loss 0.581109
[epoch16, step1355]: loss 0.479544
[epoch16, step1356]: loss 0.447214
[epoch16, step1357]: loss 0.761290
[epoch16, step1358]: loss 0.759098
[epoch16, step1359]: loss 0.353441
[epoch16, step1360]: loss 0.568493
[epoch16, step1361]: loss 0.666598
[epoch16, step1362]: loss 0.484066
[epoch16, step1363]: loss 0.602092
[epoch16, step1364]: loss 0.567586
[epoch16, step1365]: loss 0.498596
[epoch16, step1366]: loss 0.432745
[epoch16, step1367]: loss 0.342697
[epoch16, step1368]: loss 0.641101
[epoch16, step1369]: loss 0.469521
[epoch16, step1370]: loss 0.420144
[epoch16, step1371]: loss 0.339717
[epoch16, step1372]: loss 0.372919
[epoch16, step1373]: loss 0.645481
[epoch16, step1374]: loss 0.474140
[epoch16, step1375]: loss 0.474639
[epoch16, step1376]: loss 0.550483
[epoch16, step1377]: loss 0.621076
[epoch16, step1378]: loss 0.341258
[epoch16, step1379]: loss 0.396845
[epoch16, step1380]: loss 0.691246
[epoch16, step1381]: loss 0.137348
[epoch16, step1382]: loss 0.484838
[epoch16, step1383]: loss 0.475522
[epoch16, step1384]: loss 0.391601
[epoch16, step1385]: loss 0.406591
[epoch16, step1386]: loss 0.533498
[epoch16, step1387]: loss 0.581858
[epoch16, step1388]: loss 0.483668
[epoch16, step1389]: loss 0.450353
[epoch16, step1390]: loss 0.574792
[epoch16, step1391]: loss 0.610158
[epoch16, step1392]: loss 0.339719
[epoch16, step1393]: loss 0.557758
[epoch16, step1394]: loss 0.537708
[epoch16, step1395]: loss 0.416998
[epoch16, step1396]: loss 0.396669
[epoch16, step1397]: loss 0.647641
[epoch16, step1398]: loss 0.591916
[epoch16, step1399]: loss 0.698976
[epoch16, step1400]: loss 0.584251
[epoch16, step1401]: loss 0.618918
[epoch16, step1402]: loss 0.623936
[epoch16, step1403]: loss 0.378436
[epoch16, step1404]: loss 0.510328
[epoch16, step1405]: loss 0.660403
[epoch16, step1406]: loss 0.556059
[epoch16, step1407]: loss 0.626834
[epoch16, step1408]: loss 0.574772
[epoch16, step1409]: loss 0.624438
[epoch16, step1410]: loss 0.377610
[epoch16, step1411]: loss 0.398888
[epoch16, step1412]: loss 0.435205
[epoch16, step1413]: loss 0.569220
[epoch16, step1414]: loss 0.414252
[epoch16, step1415]: loss 0.639474
[epoch16, step1416]: loss 0.326158
[epoch16, step1417]: loss 0.622810
[epoch16, step1418]: loss 0.380902
[epoch16, step1419]: loss 0.516668
[epoch16, step1420]: loss 0.337215
[epoch16, step1421]: loss 0.685385
[epoch16, step1422]: loss 0.524392
[epoch16, step1423]: loss 0.641442
[epoch16, step1424]: loss 0.359613
[epoch16, step1425]: loss 0.607534
[epoch16, step1426]: loss 0.532817
[epoch16, step1427]: loss 0.493881
[epoch16, step1428]: loss 0.434278
[epoch16, step1429]: loss 0.426572
[epoch16, step1430]: loss 0.385314
[epoch16, step1431]: loss 0.527534
[epoch16, step1432]: loss 0.588142
[epoch16, step1433]: loss 0.168329
[epoch16, step1434]: loss 0.564675
[epoch16, step1435]: loss 0.546919
[epoch16, step1436]: loss 0.471613
[epoch16, step1437]: loss 0.543497
[epoch16, step1438]: loss 0.337936
[epoch16, step1439]: loss 0.146474
[epoch16, step1440]: loss 0.544765
[epoch16, step1441]: loss 0.287156
[epoch16, step1442]: loss 0.656727
[epoch16, step1443]: loss 0.299965
[epoch16, step1444]: loss 0.442637
[epoch16, step1445]: loss 0.394412
[epoch16, step1446]: loss 0.411744
[epoch16, step1447]: loss 0.488915
[epoch16, step1448]: loss 0.506369
[epoch16, step1449]: loss 0.451889
[epoch16, step1450]: loss 0.425713
[epoch16, step1451]: loss 0.356898
[epoch16, step1452]: loss 0.253684
[epoch16, step1453]: loss 0.451979
[epoch16, step1454]: loss 0.326380
[epoch16, step1455]: loss 0.651972
[epoch16, step1456]: loss 0.503779
[epoch16, step1457]: loss 0.328738
[epoch16, step1458]: loss 0.599426
[epoch16, step1459]: loss 0.383711
[epoch16, step1460]: loss 0.586751
[epoch16, step1461]: loss 0.469053
[epoch16, step1462]: loss 0.477149
[epoch16, step1463]: loss 0.542219
[epoch16, step1464]: loss 0.315282
[epoch16, step1465]: loss 0.493738
[epoch16, step1466]: loss 0.584295
[epoch16, step1467]: loss 0.437721
[epoch16, step1468]: loss 0.399052
[epoch16, step1469]: loss 0.425126
[epoch16, step1470]: loss 0.369723
[epoch16, step1471]: loss 0.537015
[epoch16, step1472]: loss 0.333659
[epoch16, step1473]: loss 0.174658
[epoch16, step1474]: loss 0.495767
[epoch16, step1475]: loss 0.560393
[epoch16, step1476]: loss 0.706501
[epoch16, step1477]: loss 0.516147
[epoch16, step1478]: loss 0.434617
[epoch16, step1479]: loss 0.275868
[epoch16, step1480]: loss 0.514637
[epoch16, step1481]: loss 0.631341
[epoch16, step1482]: loss 0.482850
[epoch16, step1483]: loss 0.451666
[epoch16, step1484]: loss 0.420715
[epoch16, step1485]: loss 0.335616
[epoch16, step1486]: loss 0.500498
[epoch16, step1487]: loss 0.730317
[epoch16, step1488]: loss 0.458408
[epoch16, step1489]: loss 0.509074
[epoch16, step1490]: loss 0.673000
[epoch16, step1491]: loss 0.572902
[epoch16, step1492]: loss 0.422848
[epoch16, step1493]: loss 0.427590
[epoch16, step1494]: loss 0.474793
[epoch16, step1495]: loss 0.229505
[epoch16, step1496]: loss 0.452287
[epoch16, step1497]: loss 0.239622
[epoch16, step1498]: loss 0.548166
[epoch16, step1499]: loss 0.484465
[epoch16, step1500]: loss 0.443310
[epoch16, step1501]: loss 0.562762
[epoch16, step1502]: loss 0.258393
[epoch16, step1503]: loss 0.472675
[epoch16, step1504]: loss 0.386517
[epoch16, step1505]: loss 0.547975
[epoch16, step1506]: loss 0.554600
[epoch16, step1507]: loss 0.477488
[epoch16, step1508]: loss 0.530286
[epoch16, step1509]: loss 0.523435
[epoch16, step1510]: loss 0.412104
[epoch16, step1511]: loss 0.616594
[epoch16, step1512]: loss 0.642791
[epoch16, step1513]: loss 0.616511
[epoch16, step1514]: loss 0.533401
[epoch16, step1515]: loss 0.398530
[epoch16, step1516]: loss 0.379258
[epoch16, step1517]: loss 0.473423
[epoch16, step1518]: loss 0.462928
[epoch16, step1519]: loss 0.580864
[epoch16, step1520]: loss 0.490226
[epoch16, step1521]: loss 0.451106
[epoch16, step1522]: loss 0.590575
[epoch16, step1523]: loss 0.368633
[epoch16, step1524]: loss 0.503144
[epoch16, step1525]: loss 0.383548
[epoch16, step1526]: loss 0.646757
[epoch16, step1527]: loss 0.574985
[epoch16, step1528]: loss 0.325036
[epoch16, step1529]: loss 0.293632
[epoch16, step1530]: loss 0.500014
[epoch16, step1531]: loss 0.576544
[epoch16, step1532]: loss 0.565742
[epoch16, step1533]: loss 0.601496
[epoch16, step1534]: loss 0.594352
[epoch16, step1535]: loss 0.532933
[epoch16, step1536]: loss 0.273425
[epoch16, step1537]: loss 0.434717
[epoch16, step1538]: loss 0.747694
[epoch16, step1539]: loss 0.261022
[epoch16, step1540]: loss 0.236102
[epoch16, step1541]: loss 0.546367
[epoch16, step1542]: loss 0.489806
[epoch16, step1543]: loss 0.787701
[epoch16, step1544]: loss 0.375458
[epoch16, step1545]: loss 0.380835
[epoch16, step1546]: loss 0.507900
[epoch16, step1547]: loss 0.488605
[epoch16, step1548]: loss 0.503479
[epoch16, step1549]: loss 0.472953
[epoch16, step1550]: loss 0.424800
[epoch16, step1551]: loss 0.512149
[epoch16, step1552]: loss 0.512866
[epoch16, step1553]: loss 0.368938
[epoch16, step1554]: loss 0.454702
[epoch16, step1555]: loss 0.307117
[epoch16, step1556]: loss 0.425722
[epoch16, step1557]: loss 0.683552
[epoch16, step1558]: loss 0.617432
[epoch16, step1559]: loss 0.384087
[epoch16, step1560]: loss 0.345067
[epoch16, step1561]: loss 0.142115
[epoch16, step1562]: loss 0.653635
[epoch16, step1563]: loss 0.506724
[epoch16, step1564]: loss 0.298097
[epoch16, step1565]: loss 0.538202
[epoch16, step1566]: loss 0.577761
[epoch16, step1567]: loss 0.706811
[epoch16, step1568]: loss 0.571978
[epoch16, step1569]: loss 0.526823
[epoch16, step1570]: loss 0.525897
[epoch16, step1571]: loss 0.522764
[epoch16, step1572]: loss 0.707013
[epoch16, step1573]: loss 0.554016
[epoch16, step1574]: loss 0.300011
[epoch16, step1575]: loss 0.499246
[epoch16, step1576]: loss 0.597527
[epoch16, step1577]: loss 0.342915
[epoch16, step1578]: loss 0.492233
[epoch16, step1579]: loss 0.314263
[epoch16, step1580]: loss 0.400638
[epoch16, step1581]: loss 0.364201
[epoch16, step1582]: loss 0.678348
[epoch16, step1583]: loss 0.632308
[epoch16, step1584]: loss 0.403936
[epoch16, step1585]: loss 0.562114
[epoch16, step1586]: loss 0.227655
[epoch16, step1587]: loss 0.332704
[epoch16, step1588]: loss 0.466379
[epoch16, step1589]: loss 0.525311
[epoch16, step1590]: loss 0.404062
[epoch16, step1591]: loss 0.423466
[epoch16, step1592]: loss 0.402261
[epoch16, step1593]: loss 0.638153
[epoch16, step1594]: loss 0.377844
[epoch16, step1595]: loss 0.507225
[epoch16, step1596]: loss 0.542630
[epoch16, step1597]: loss 0.299254
[epoch16, step1598]: loss 0.618221
[epoch16, step1599]: loss 0.502628
[epoch16, step1600]: loss 0.351086
[epoch16, step1601]: loss 0.560818
[epoch16, step1602]: loss 0.371507
[epoch16, step1603]: loss 0.720386
[epoch16, step1604]: loss 0.619851
[epoch16, step1605]: loss 0.526575
[epoch16, step1606]: loss 0.407905
[epoch16, step1607]: loss 0.604243
[epoch16, step1608]: loss 0.459145
[epoch16, step1609]: loss 0.548594
[epoch16, step1610]: loss 0.498794
[epoch16, step1611]: loss 0.421876
[epoch16, step1612]: loss 0.386075
[epoch16, step1613]: loss 0.224726
[epoch16, step1614]: loss 0.436380
[epoch16, step1615]: loss 0.544708
[epoch16, step1616]: loss 0.553586
[epoch16, step1617]: loss 0.174998
[epoch16, step1618]: loss 0.301955
[epoch16, step1619]: loss 0.324567
[epoch16, step1620]: loss 0.494969
[epoch16, step1621]: loss 0.540497
[epoch16, step1622]: loss 0.337617
[epoch16, step1623]: loss 0.509608
[epoch16, step1624]: loss 0.544900
[epoch16, step1625]: loss 0.505190
[epoch16, step1626]: loss 0.563661
[epoch16, step1627]: loss 0.771896
[epoch16, step1628]: loss 0.352663
[epoch16, step1629]: loss 0.666045
[epoch16, step1630]: loss 0.468255
[epoch16, step1631]: loss 0.532991
[epoch16, step1632]: loss 0.244926
[epoch16, step1633]: loss 0.648762
[epoch16, step1634]: loss 0.596060
[epoch16, step1635]: loss 0.705312
[epoch16, step1636]: loss 0.635573
[epoch16, step1637]: loss 0.535005
[epoch16, step1638]: loss 0.266682
[epoch16, step1639]: loss 0.420935
[epoch16, step1640]: loss 0.530116
[epoch16, step1641]: loss 0.448697
[epoch16, step1642]: loss 0.631034
[epoch16, step1643]: loss 0.485665
[epoch16, step1644]: loss 0.545685
[epoch16, step1645]: loss 0.602586
[epoch16, step1646]: loss 0.445769
[epoch16, step1647]: loss 0.473664
[epoch16, step1648]: loss 0.319445
[epoch16, step1649]: loss 0.269465
[epoch16, step1650]: loss 0.217452
[epoch16, step1651]: loss 0.508118
[epoch16, step1652]: loss 0.399662
[epoch16, step1653]: loss 0.569410
[epoch16, step1654]: loss 0.540358
[epoch16, step1655]: loss 0.506849
[epoch16, step1656]: loss 0.461664
[epoch16, step1657]: loss 0.576440
[epoch16, step1658]: loss 0.601717
[epoch16, step1659]: loss 0.646818
[epoch16, step1660]: loss 0.593558
[epoch16, step1661]: loss 0.458069
[epoch16, step1662]: loss 0.441997
[epoch16, step1663]: loss 0.606457
[epoch16, step1664]: loss 0.589071
[epoch16, step1665]: loss 0.358993
[epoch16, step1666]: loss 0.500310
[epoch16, step1667]: loss 0.605737
[epoch16, step1668]: loss 0.550032
[epoch16, step1669]: loss 0.523184
[epoch16, step1670]: loss 0.412883
[epoch16, step1671]: loss 0.569165
[epoch16, step1672]: loss 0.395350
[epoch16, step1673]: loss 0.417544
[epoch16, step1674]: loss 0.338055
[epoch16, step1675]: loss 0.511523
[epoch16, step1676]: loss 0.195548
[epoch16, step1677]: loss 0.446136
[epoch16, step1678]: loss 0.357921
[epoch16, step1679]: loss 0.184011
[epoch16, step1680]: loss 0.429434
[epoch16, step1681]: loss 0.502289
[epoch16, step1682]: loss 0.436563
[epoch16, step1683]: loss 0.630347
[epoch16, step1684]: loss 0.645822
[epoch16, step1685]: loss 0.554161
[epoch16, step1686]: loss 0.499073
[epoch16, step1687]: loss 0.655124
[epoch16, step1688]: loss 0.363848
[epoch16, step1689]: loss 0.445261
[epoch16, step1690]: loss 0.481469
[epoch16, step1691]: loss 0.544661
[epoch16, step1692]: loss 0.440735
[epoch16, step1693]: loss 0.466038
[epoch16, step1694]: loss 0.609052
[epoch16, step1695]: loss 0.255451
[epoch16, step1696]: loss 0.459424
[epoch16, step1697]: loss 0.511521
[epoch16, step1698]: loss 0.302427
[epoch16, step1699]: loss 0.512594
[epoch16, step1700]: loss 0.626879
[epoch16, step1701]: loss 0.518947
[epoch16, step1702]: loss 0.554535
[epoch16, step1703]: loss 0.600210
[epoch16, step1704]: loss 0.526557
[epoch16, step1705]: loss 0.426637
[epoch16, step1706]: loss 0.583329
[epoch16, step1707]: loss 0.446946
[epoch16, step1708]: loss 0.547081
[epoch16, step1709]: loss 0.356775
[epoch16, step1710]: loss 0.426703
[epoch16, step1711]: loss 0.484992
[epoch16, step1712]: loss 0.524038
[epoch16, step1713]: loss 0.585854
[epoch16, step1714]: loss 0.381381
[epoch16, step1715]: loss 0.669377
[epoch16, step1716]: loss 0.407140
[epoch16, step1717]: loss 0.543718
[epoch16, step1718]: loss 0.443999
[epoch16, step1719]: loss 0.171380
[epoch16, step1720]: loss 0.500729
[epoch16, step1721]: loss 0.571951
[epoch16, step1722]: loss 0.520835
[epoch16, step1723]: loss 0.504693
[epoch16, step1724]: loss 0.358198
[epoch16, step1725]: loss 0.309898
[epoch16, step1726]: loss 0.484929
[epoch16, step1727]: loss 0.569261
[epoch16, step1728]: loss 0.359090
[epoch16, step1729]: loss 0.348252
[epoch16, step1730]: loss 0.562971
[epoch16, step1731]: loss 0.570946
[epoch16, step1732]: loss 0.492341
[epoch16, step1733]: loss 0.444069
[epoch16, step1734]: loss 0.717071
[epoch16, step1735]: loss 0.592948
[epoch16, step1736]: loss 0.403856
[epoch16, step1737]: loss 0.617609
[epoch16, step1738]: loss 0.529805
[epoch16, step1739]: loss 0.554588
[epoch16, step1740]: loss 0.558075
[epoch16, step1741]: loss 0.199359
[epoch16, step1742]: loss 0.498818
[epoch16, step1743]: loss 0.488487
[epoch16, step1744]: loss 0.310443
[epoch16, step1745]: loss 0.366241
[epoch16, step1746]: loss 0.425853
[epoch16, step1747]: loss 0.451150
[epoch16, step1748]: loss 0.247048
[epoch16, step1749]: loss 0.576089
[epoch16, step1750]: loss 0.118870
[epoch16, step1751]: loss 0.481145
[epoch16, step1752]: loss 0.674837
[epoch16, step1753]: loss 0.628253
[epoch16, step1754]: loss 0.510158
[epoch16, step1755]: loss 0.382938
[epoch16, step1756]: loss 0.598881
[epoch16, step1757]: loss 0.267606
[epoch16, step1758]: loss 0.509677
[epoch16, step1759]: loss 0.641441
[epoch16, step1760]: loss 0.429865
[epoch16, step1761]: loss 0.160300
[epoch16, step1762]: loss 0.732191
[epoch16, step1763]: loss 0.730574
[epoch16, step1764]: loss 0.479929
[epoch16, step1765]: loss 0.352706
[epoch16, step1766]: loss 0.298425
[epoch16, step1767]: loss 0.356422
[epoch16, step1768]: loss 0.504515
[epoch16, step1769]: loss 0.520595
[epoch16, step1770]: loss 0.677664
[epoch16, step1771]: loss 0.283100
[epoch16, step1772]: loss 0.576303
[epoch16, step1773]: loss 0.612949
[epoch16, step1774]: loss 0.710124
[epoch16, step1775]: loss 0.694157
[epoch16, step1776]: loss 0.417190
[epoch16, step1777]: loss 0.498410
[epoch16, step1778]: loss 0.441104
[epoch16, step1779]: loss 0.706225
[epoch16, step1780]: loss 0.665789
[epoch16, step1781]: loss 0.604448
[epoch16, step1782]: loss 0.462176
[epoch16, step1783]: loss 0.563104
[epoch16, step1784]: loss 0.333529
[epoch16, step1785]: loss 0.604676
[epoch16, step1786]: loss 0.364101
[epoch16, step1787]: loss 0.570273
[epoch16, step1788]: loss 0.635195
[epoch16, step1789]: loss 0.539217
[epoch16, step1790]: loss 0.612802
[epoch16, step1791]: loss 0.552994
[epoch16, step1792]: loss 0.377361
[epoch16, step1793]: loss 0.408457
[epoch16, step1794]: loss 0.488786
[epoch16, step1795]: loss 0.421771
[epoch16, step1796]: loss 0.505303
[epoch16, step1797]: loss 0.251860
[epoch16, step1798]: loss 0.277807
[epoch16, step1799]: loss 0.439542
[epoch16, step1800]: loss 0.604851
[epoch16, step1801]: loss 0.377541
[epoch16, step1802]: loss 0.411951
[epoch16, step1803]: loss 0.433148
[epoch16, step1804]: loss 0.482780
[epoch16, step1805]: loss 0.427499
[epoch16, step1806]: loss 0.606265
[epoch16, step1807]: loss 0.439557
[epoch16, step1808]: loss 0.410515
[epoch16, step1809]: loss 0.376499
[epoch16, step1810]: loss 0.148140
[epoch16, step1811]: loss 0.676500
[epoch16, step1812]: loss 0.494398
[epoch16, step1813]: loss 0.611057
[epoch16, step1814]: loss 0.243810
[epoch16, step1815]: loss 0.593633
[epoch16, step1816]: loss 0.268890
[epoch16, step1817]: loss 0.295335
[epoch16, step1818]: loss 0.660204
[epoch16, step1819]: loss 0.489285
[epoch16, step1820]: loss 0.274644
[epoch16, step1821]: loss 0.335356
[epoch16, step1822]: loss 0.396708
[epoch16, step1823]: loss 0.553429
[epoch16, step1824]: loss 0.410803
[epoch16, step1825]: loss 0.548363
[epoch16, step1826]: loss 0.461636
[epoch16, step1827]: loss 0.572544
[epoch16, step1828]: loss 0.518043
[epoch16, step1829]: loss 0.384604
[epoch16, step1830]: loss 0.599729
[epoch16, step1831]: loss 0.440019
[epoch16, step1832]: loss 0.504201
[epoch16, step1833]: loss 0.460796
[epoch16, step1834]: loss 0.561947
[epoch16, step1835]: loss 0.200256
[epoch16, step1836]: loss 0.633247
[epoch16, step1837]: loss 0.573026
[epoch16, step1838]: loss 0.550059
[epoch16, step1839]: loss 0.373061
[epoch16, step1840]: loss 0.681922
[epoch16, step1841]: loss 0.651141
[epoch16, step1842]: loss 0.522579
[epoch16, step1843]: loss 0.238439
[epoch16, step1844]: loss 0.321429
[epoch16, step1845]: loss 0.501388
[epoch16, step1846]: loss 0.469539
[epoch16, step1847]: loss 0.383583
[epoch16, step1848]: loss 0.403121
[epoch16, step1849]: loss 0.650748
[epoch16, step1850]: loss 0.495272
[epoch16, step1851]: loss 0.277617
[epoch16, step1852]: loss 0.337520
[epoch16, step1853]: loss 0.411105
[epoch16, step1854]: loss 0.513507
[epoch16, step1855]: loss 0.508227
[epoch16, step1856]: loss 0.452671
[epoch16, step1857]: loss 0.435672
[epoch16, step1858]: loss 0.403315
[epoch16, step1859]: loss 0.267614
[epoch16, step1860]: loss 0.421099
[epoch16, step1861]: loss 0.447588
[epoch16, step1862]: loss 0.556019
[epoch16, step1863]: loss 0.510600
[epoch16, step1864]: loss 0.429554
[epoch16, step1865]: loss 0.517431
[epoch16, step1866]: loss 0.632692
[epoch16, step1867]: loss 0.550885
[epoch16, step1868]: loss 0.557746
[epoch16, step1869]: loss 0.663046
[epoch16, step1870]: loss 0.561873
[epoch16, step1871]: loss 0.380898
[epoch16, step1872]: loss 0.283900
[epoch16, step1873]: loss 0.721864
[epoch16, step1874]: loss 0.440419
[epoch16, step1875]: loss 0.518256
[epoch16, step1876]: loss 0.515412
[epoch16, step1877]: loss 0.503462
[epoch16, step1878]: loss 0.512814
[epoch16, step1879]: loss 0.566385
[epoch16, step1880]: loss 0.506199
[epoch16, step1881]: loss 0.576698
[epoch16, step1882]: loss 0.497615
[epoch16, step1883]: loss 0.275552
[epoch16, step1884]: loss 0.391639
[epoch16, step1885]: loss 0.541928
[epoch16, step1886]: loss 0.696269
[epoch16, step1887]: loss 0.577803
[epoch16, step1888]: loss 0.374316
[epoch16, step1889]: loss 0.311862
[epoch16, step1890]: loss 0.641675
[epoch16, step1891]: loss 0.589257
[epoch16, step1892]: loss 0.505584
[epoch16, step1893]: loss 0.396834
[epoch16, step1894]: loss 0.452105
[epoch16, step1895]: loss 0.305166
[epoch16, step1896]: loss 0.543894
[epoch16, step1897]: loss 0.454703
[epoch16, step1898]: loss 0.593221
[epoch16, step1899]: loss 0.270985
[epoch16, step1900]: loss 0.455568
[epoch16, step1901]: loss 0.522295
[epoch16, step1902]: loss 0.398007
[epoch16, step1903]: loss 0.545988
[epoch16, step1904]: loss 0.643397
[epoch16, step1905]: loss 0.284103
[epoch16, step1906]: loss 0.417110
[epoch16, step1907]: loss 0.385360
[epoch16, step1908]: loss 0.503038
[epoch16, step1909]: loss 0.440146
[epoch16, step1910]: loss 0.465299
[epoch16, step1911]: loss 0.718813
[epoch16, step1912]: loss 0.571919
[epoch16, step1913]: loss 0.368023
[epoch16, step1914]: loss 0.411546
[epoch16, step1915]: loss 0.465478
[epoch16, step1916]: loss 0.247563
[epoch16, step1917]: loss 0.568068
[epoch16, step1918]: loss 0.560684
[epoch16, step1919]: loss 0.175821
[epoch16, step1920]: loss 0.250729
[epoch16, step1921]: loss 0.552137
[epoch16, step1922]: loss 0.686496
[epoch16, step1923]: loss 0.471151
[epoch16, step1924]: loss 0.636756
[epoch16, step1925]: loss 0.669860
[epoch16, step1926]: loss 0.620839
[epoch16, step1927]: loss 0.384599
[epoch16, step1928]: loss 0.586762
[epoch16, step1929]: loss 0.470857
[epoch16, step1930]: loss 0.480071
[epoch16, step1931]: loss 0.517970
[epoch16, step1932]: loss 0.299417
[epoch16, step1933]: loss 0.314035
[epoch16, step1934]: loss 0.432668
[epoch16, step1935]: loss 0.549290
[epoch16, step1936]: loss 0.463755
[epoch16, step1937]: loss 0.282814
[epoch16, step1938]: loss 0.501161
[epoch16, step1939]: loss 0.627277
[epoch16, step1940]: loss 0.414710
[epoch16, step1941]: loss 0.485439
[epoch16, step1942]: loss 0.233267
[epoch16, step1943]: loss 0.331349
[epoch16, step1944]: loss 0.263072
[epoch16, step1945]: loss 0.627412
[epoch16, step1946]: loss 0.369707
[epoch16, step1947]: loss 0.662571
[epoch16, step1948]: loss 0.645196
[epoch16, step1949]: loss 0.578463
[epoch16, step1950]: loss 0.366283
[epoch16, step1951]: loss 0.642449
[epoch16, step1952]: loss 0.438833
[epoch16, step1953]: loss 0.385169
[epoch16, step1954]: loss 0.339869
[epoch16, step1955]: loss 0.641729
[epoch16, step1956]: loss 0.497478
[epoch16, step1957]: loss 0.595155
[epoch16, step1958]: loss 0.220555
[epoch16, step1959]: loss 0.614097
[epoch16, step1960]: loss 0.502493
[epoch16, step1961]: loss 0.385218
[epoch16, step1962]: loss 0.555097
[epoch16, step1963]: loss 0.564937
[epoch16, step1964]: loss 0.651245
[epoch16, step1965]: loss 0.520472
[epoch16, step1966]: loss 0.439554
[epoch16, step1967]: loss 0.404703
[epoch16, step1968]: loss 0.586462
[epoch16, step1969]: loss 0.406790
[epoch16, step1970]: loss 0.493221
[epoch16, step1971]: loss 0.285063
[epoch16, step1972]: loss 0.565753
[epoch16, step1973]: loss 0.579318
[epoch16, step1974]: loss 0.312117
[epoch16, step1975]: loss 0.371400
[epoch16, step1976]: loss 0.687718
[epoch16, step1977]: loss 0.411750
[epoch16, step1978]: loss 0.655481
[epoch16, step1979]: loss 0.180146
[epoch16, step1980]: loss 0.566814
[epoch16, step1981]: loss 0.495093
[epoch16, step1982]: loss 0.512192
[epoch16, step1983]: loss 0.490409
[epoch16, step1984]: loss 0.296915
[epoch16, step1985]: loss 0.618851
[epoch16, step1986]: loss 0.371987
[epoch16, step1987]: loss 0.473749
[epoch16, step1988]: loss 0.530283
[epoch16, step1989]: loss 0.666005
[epoch16, step1990]: loss 0.344276
[epoch16, step1991]: loss 0.598253
[epoch16, step1992]: loss 0.480878
[epoch16, step1993]: loss 0.210757
[epoch16, step1994]: loss 0.499922
[epoch16, step1995]: loss 0.424484
[epoch16, step1996]: loss 0.675838
[epoch16, step1997]: loss 0.384912
[epoch16, step1998]: loss 0.535612
[epoch16, step1999]: loss 0.304016
[epoch16, step2000]: loss 0.526244
[epoch16, step2001]: loss 0.540051
[epoch16, step2002]: loss 0.571393
[epoch16, step2003]: loss 0.351031
[epoch16, step2004]: loss 0.156222
[epoch16, step2005]: loss 0.419618
[epoch16, step2006]: loss 0.512899
[epoch16, step2007]: loss 0.497237
[epoch16, step2008]: loss 0.583003
[epoch16, step2009]: loss 0.428085
[epoch16, step2010]: loss 0.353958
[epoch16, step2011]: loss 0.343217
[epoch16, step2012]: loss 0.583706
[epoch16, step2013]: loss 0.694707
[epoch16, step2014]: loss 0.573447
[epoch16, step2015]: loss 0.366245
[epoch16, step2016]: loss 0.398875
[epoch16, step2017]: loss 0.462749
[epoch16, step2018]: loss 0.397425
[epoch16, step2019]: loss 0.469986
[epoch16, step2020]: loss 0.442210
[epoch16, step2021]: loss 0.589335
[epoch16, step2022]: loss 0.579712
[epoch16, step2023]: loss 0.453706
[epoch16, step2024]: loss 0.577991
[epoch16, step2025]: loss 0.609339
[epoch16, step2026]: loss 0.314081
[epoch16, step2027]: loss 0.398195
[epoch16, step2028]: loss 0.285502
[epoch16, step2029]: loss 0.409505
[epoch16, step2030]: loss 0.402946
[epoch16, step2031]: loss 0.495877
[epoch16, step2032]: loss 0.232798
[epoch16, step2033]: loss 0.343615
[epoch16, step2034]: loss 0.473139
[epoch16, step2035]: loss 0.515083
[epoch16, step2036]: loss 0.475122
[epoch16, step2037]: loss 0.656458
[epoch16, step2038]: loss 0.479144
[epoch16, step2039]: loss 0.554365
[epoch16, step2040]: loss 0.519571
[epoch16, step2041]: loss 0.167994
[epoch16, step2042]: loss 0.544594
[epoch16, step2043]: loss 0.548836
[epoch16, step2044]: loss 0.493587
[epoch16, step2045]: loss 0.620002
[epoch16, step2046]: loss 0.257950
[epoch16, step2047]: loss 0.521361
[epoch16, step2048]: loss 0.768799
[epoch16, step2049]: loss 0.603359
[epoch16, step2050]: loss 0.262967
[epoch16, step2051]: loss 0.637528
[epoch16, step2052]: loss 0.662510
[epoch16, step2053]: loss 0.415201
[epoch16, step2054]: loss 0.495936
[epoch16, step2055]: loss 0.585509
[epoch16, step2056]: loss 0.453521
[epoch16, step2057]: loss 0.623572
[epoch16, step2058]: loss 0.422577
[epoch16, step2059]: loss 0.416867
[epoch16, step2060]: loss 0.359186
[epoch16, step2061]: loss 0.452772
[epoch16, step2062]: loss 0.419895
[epoch16, step2063]: loss 0.561606
[epoch16, step2064]: loss 0.482747
[epoch16, step2065]: loss 0.534698
[epoch16, step2066]: loss 0.445226
[epoch16, step2067]: loss 0.524181
[epoch16, step2068]: loss 0.546769
[epoch16, step2069]: loss 0.578221
[epoch16, step2070]: loss 0.238971
[epoch16, step2071]: loss 0.367917
[epoch16, step2072]: loss 0.348630
[epoch16, step2073]: loss 0.414727
[epoch16, step2074]: loss 0.334818
[epoch16, step2075]: loss 0.327859
[epoch16, step2076]: loss 0.620578
[epoch16, step2077]: loss 0.301182
[epoch16, step2078]: loss 0.587455
[epoch16, step2079]: loss 0.237938
[epoch16, step2080]: loss 0.556510
[epoch16, step2081]: loss 0.593014
[epoch16, step2082]: loss 0.231178
[epoch16, step2083]: loss 0.644095
[epoch16, step2084]: loss 0.761534
[epoch16, step2085]: loss 0.120285
[epoch16, step2086]: loss 0.447093
[epoch16, step2087]: loss 0.418538
[epoch16, step2088]: loss 0.336991
[epoch16, step2089]: loss 0.424962
[epoch16, step2090]: loss 0.424547
[epoch16, step2091]: loss 0.396173
[epoch16, step2092]: loss 0.447907
[epoch16, step2093]: loss 0.510249
[epoch16, step2094]: loss 0.330881
[epoch16, step2095]: loss 0.540904
[epoch16, step2096]: loss 0.522228
[epoch16, step2097]: loss 0.503390
[epoch16, step2098]: loss 0.218608
[epoch16, step2099]: loss 0.140364
[epoch16, step2100]: loss 0.344036
[epoch16, step2101]: loss 0.394896
[epoch16, step2102]: loss 0.338096
[epoch16, step2103]: loss 0.408844
[epoch16, step2104]: loss 0.599946
[epoch16, step2105]: loss 0.529851
[epoch16, step2106]: loss 0.292696
[epoch16, step2107]: loss 0.645014
[epoch16, step2108]: loss 0.436230
[epoch16, step2109]: loss 0.383559
[epoch16, step2110]: loss 0.293578
[epoch16, step2111]: loss 0.657237
[epoch16, step2112]: loss 0.460018
[epoch16, step2113]: loss 0.135088
[epoch16, step2114]: loss 0.471422
[epoch16, step2115]: loss 0.431526
[epoch16, step2116]: loss 0.323630
[epoch16, step2117]: loss 0.520249
[epoch16, step2118]: loss 0.669961
[epoch16, step2119]: loss 0.347369
[epoch16, step2120]: loss 0.415382
[epoch16, step2121]: loss 0.422585
[epoch16, step2122]: loss 0.567895
[epoch16, step2123]: loss 0.350328
[epoch16, step2124]: loss 0.571851
[epoch16, step2125]: loss 0.524919
[epoch16, step2126]: loss 0.610329
[epoch16, step2127]: loss 0.450919
[epoch16, step2128]: loss 0.296389
[epoch16, step2129]: loss 0.445675
[epoch16, step2130]: loss 0.420204
[epoch16, step2131]: loss 0.328046
[epoch16, step2132]: loss 0.502012
[epoch16, step2133]: loss 0.677114
[epoch16, step2134]: loss 0.481985
[epoch16, step2135]: loss 0.396594
[epoch16, step2136]: loss 0.766026
[epoch16, step2137]: loss 0.484831
[epoch16, step2138]: loss 0.398796
[epoch16, step2139]: loss 0.459865
[epoch16, step2140]: loss 0.456755
[epoch16, step2141]: loss 0.347722
[epoch16, step2142]: loss 0.549898
[epoch16, step2143]: loss 0.529121
[epoch16, step2144]: loss 0.510779
[epoch16, step2145]: loss 0.339679
[epoch16, step2146]: loss 0.561474
[epoch16, step2147]: loss 0.697939
[epoch16, step2148]: loss 0.416780
[epoch16, step2149]: loss 0.401905
[epoch16, step2150]: loss 0.326781
[epoch16, step2151]: loss 0.729417
[epoch16, step2152]: loss 0.515571
[epoch16, step2153]: loss 0.551995
[epoch16, step2154]: loss 0.503873
[epoch16, step2155]: loss 0.590636
[epoch16, step2156]: loss 0.589996
[epoch16, step2157]: loss 0.654915
[epoch16, step2158]: loss 0.505697
[epoch16, step2159]: loss 0.285778
[epoch16, step2160]: loss 0.583869
[epoch16, step2161]: loss 0.558812
[epoch16, step2162]: loss 0.266813
[epoch16, step2163]: loss 0.467677
[epoch16, step2164]: loss 0.451237
[epoch16, step2165]: loss 0.511056
[epoch16, step2166]: loss 0.401421
[epoch16, step2167]: loss 0.582720
[epoch16, step2168]: loss 0.478872
[epoch16, step2169]: loss 0.562973
[epoch16, step2170]: loss 0.371815
[epoch16, step2171]: loss 0.335066
[epoch16, step2172]: loss 0.557056
[epoch16, step2173]: loss 0.270958
[epoch16, step2174]: loss 0.532411
[epoch16, step2175]: loss 0.529800
[epoch16, step2176]: loss 0.633350
[epoch16, step2177]: loss 0.457039
[epoch16, step2178]: loss 0.522138
[epoch16, step2179]: loss 0.649240
[epoch16, step2180]: loss 0.464929
[epoch16, step2181]: loss 0.504903
[epoch16, step2182]: loss 0.748633
[epoch16, step2183]: loss 0.700144
[epoch16, step2184]: loss 0.457789
[epoch16, step2185]: loss 0.298386
[epoch16, step2186]: loss 0.329947
[epoch16, step2187]: loss 0.498305
[epoch16, step2188]: loss 0.389142
[epoch16, step2189]: loss 0.552360
[epoch16, step2190]: loss 0.595939
[epoch16, step2191]: loss 0.616015
[epoch16, step2192]: loss 0.494917
[epoch16, step2193]: loss 0.741947
[epoch16, step2194]: loss 0.484149
[epoch16, step2195]: loss 0.340772
[epoch16, step2196]: loss 0.547726
[epoch16, step2197]: loss 0.697650
[epoch16, step2198]: loss 0.426827
[epoch16, step2199]: loss 0.608077
[epoch16, step2200]: loss 0.444985
[epoch16, step2201]: loss 0.365529
[epoch16, step2202]: loss 0.699291
[epoch16, step2203]: loss 0.404439
[epoch16, step2204]: loss 0.570484
[epoch16, step2205]: loss 0.470385
[epoch16, step2206]: loss 0.632853
[epoch16, step2207]: loss 0.389671
[epoch16, step2208]: loss 0.319172
[epoch16, step2209]: loss 0.259790
[epoch16, step2210]: loss 0.636903
[epoch16, step2211]: loss 0.140962
[epoch16, step2212]: loss 0.676453
[epoch16, step2213]: loss 0.524638
[epoch16, step2214]: loss 0.352714
[epoch16, step2215]: loss 0.629597
[epoch16, step2216]: loss 0.512029
[epoch16, step2217]: loss 0.570239
[epoch16, step2218]: loss 0.660623
[epoch16, step2219]: loss 0.506323
[epoch16, step2220]: loss 0.253437
[epoch16, step2221]: loss 0.351763
[epoch16, step2222]: loss 0.451085
[epoch16, step2223]: loss 0.594840
[epoch16, step2224]: loss 0.341996
[epoch16, step2225]: loss 0.471482
[epoch16, step2226]: loss 0.329946
[epoch16, step2227]: loss 0.559465
[epoch16, step2228]: loss 0.386635
[epoch16, step2229]: loss 0.303914
[epoch16, step2230]: loss 0.647146
[epoch16, step2231]: loss 0.433687
[epoch16, step2232]: loss 0.457340
[epoch16, step2233]: loss 0.297578
[epoch16, step2234]: loss 0.446371
[epoch16, step2235]: loss 0.492053
[epoch16, step2236]: loss 0.366411
[epoch16, step2237]: loss 0.346314
[epoch16, step2238]: loss 0.579026
[epoch16, step2239]: loss 0.472784
[epoch16, step2240]: loss 0.301663
[epoch16, step2241]: loss 0.582457
[epoch16, step2242]: loss 0.409255
[epoch16, step2243]: loss 0.544590
[epoch16, step2244]: loss 0.352616
[epoch16, step2245]: loss 0.506983
[epoch16, step2246]: loss 0.636581
[epoch16, step2247]: loss 0.548180
[epoch16, step2248]: loss 0.536829
[epoch16, step2249]: loss 0.389961
[epoch16, step2250]: loss 0.585254
[epoch16, step2251]: loss 0.581863
[epoch16, step2252]: loss 0.260642
[epoch16, step2253]: loss 0.301916
[epoch16, step2254]: loss 0.571839
[epoch16, step2255]: loss 0.447691
[epoch16, step2256]: loss 0.626171
[epoch16, step2257]: loss 0.568310
[epoch16, step2258]: loss 0.611122
[epoch16, step2259]: loss 0.242809
[epoch16, step2260]: loss 0.386463
[epoch16, step2261]: loss 0.675442
[epoch16, step2262]: loss 0.248028
[epoch16, step2263]: loss 0.498131
[epoch16, step2264]: loss 0.499629
[epoch16, step2265]: loss 0.514134
[epoch16, step2266]: loss 0.587574
[epoch16, step2267]: loss 0.598686
[epoch16, step2268]: loss 0.433018
[epoch16, step2269]: loss 0.382206
[epoch16, step2270]: loss 0.440109
[epoch16, step2271]: loss 0.108856
[epoch16, step2272]: loss 0.495105
[epoch16, step2273]: loss 0.592363
[epoch16, step2274]: loss 0.520265
[epoch16, step2275]: loss 0.483222
[epoch16, step2276]: loss 0.442604
[epoch16, step2277]: loss 0.382397
[epoch16, step2278]: loss 0.416639
[epoch16, step2279]: loss 0.358028
[epoch16, step2280]: loss 0.607042
[epoch16, step2281]: loss 0.612763
[epoch16, step2282]: loss 0.426801
[epoch16, step2283]: loss 0.536558
[epoch16, step2284]: loss 0.610238
[epoch16, step2285]: loss 0.441320
[epoch16, step2286]: loss 0.488712
[epoch16, step2287]: loss 0.610510
[epoch16, step2288]: loss 0.458017
[epoch16, step2289]: loss 0.610667
[epoch16, step2290]: loss 0.631712
[epoch16, step2291]: loss 0.396016
[epoch16, step2292]: loss 0.539548
[epoch16, step2293]: loss 0.481819
[epoch16, step2294]: loss 0.478671
[epoch16, step2295]: loss 0.560246
[epoch16, step2296]: loss 0.416849
[epoch16, step2297]: loss 0.425766
[epoch16, step2298]: loss 0.411976
[epoch16, step2299]: loss 0.448208
[epoch16, step2300]: loss 0.364016
[epoch16, step2301]: loss 0.248377
[epoch16, step2302]: loss 0.485793
[epoch16, step2303]: loss 0.415603
[epoch16, step2304]: loss 0.235962
[epoch16, step2305]: loss 0.626381
[epoch16, step2306]: loss 0.612042
[epoch16, step2307]: loss 0.484458
[epoch16, step2308]: loss 0.503130
[epoch16, step2309]: loss 0.657422
[epoch16, step2310]: loss 0.356379
[epoch16, step2311]: loss 0.510221
[epoch16, step2312]: loss 0.618879
[epoch16, step2313]: loss 0.383090
[epoch16, step2314]: loss 0.485459
[epoch16, step2315]: loss 0.360166
[epoch16, step2316]: loss 0.580242
[epoch16, step2317]: loss 0.605762
[epoch16, step2318]: loss 0.331184
[epoch16, step2319]: loss 0.576130
[epoch16, step2320]: loss 0.554419
[epoch16, step2321]: loss 0.618777
[epoch16, step2322]: loss 0.532592
[epoch16, step2323]: loss 0.379546
[epoch16, step2324]: loss 0.654065
[epoch16, step2325]: loss 0.656552
[epoch16, step2326]: loss 0.444235
[epoch16, step2327]: loss 0.373012
[epoch16, step2328]: loss 0.530561
[epoch16, step2329]: loss 0.421319
[epoch16, step2330]: loss 0.514610
[epoch16, step2331]: loss 0.229665
[epoch16, step2332]: loss 0.539361
[epoch16, step2333]: loss 0.583276
[epoch16, step2334]: loss 0.269260
[epoch16, step2335]: loss 0.322619
[epoch16, step2336]: loss 0.450484
[epoch16, step2337]: loss 0.573454
[epoch16, step2338]: loss 0.146515
[epoch16, step2339]: loss 0.633378
[epoch16, step2340]: loss 0.520994
[epoch16, step2341]: loss 0.454077
[epoch16, step2342]: loss 0.561967
[epoch16, step2343]: loss 0.559770
[epoch16, step2344]: loss 0.537389
[epoch16, step2345]: loss 0.516822
[epoch16, step2346]: loss 0.616527
[epoch16, step2347]: loss 0.481282
[epoch16, step2348]: loss 0.538683
[epoch16, step2349]: loss 0.270907
[epoch16, step2350]: loss 0.163758
[epoch16, step2351]: loss 0.577872
[epoch16, step2352]: loss 0.540181
[epoch16, step2353]: loss 0.333813
[epoch16, step2354]: loss 0.682290
[epoch16, step2355]: loss 0.396406
[epoch16, step2356]: loss 0.543588
[epoch16, step2357]: loss 0.334284
[epoch16, step2358]: loss 0.574887
[epoch16, step2359]: loss 0.358494
[epoch16, step2360]: loss 0.338056
[epoch16, step2361]: loss 0.460728
[epoch16, step2362]: loss 0.650992
[epoch16, step2363]: loss 0.660168
[epoch16, step2364]: loss 0.408518
[epoch16, step2365]: loss 0.414648
[epoch16, step2366]: loss 0.559955
[epoch16, step2367]: loss 0.479702
[epoch16, step2368]: loss 0.352530
[epoch16, step2369]: loss 0.495641
[epoch16, step2370]: loss 0.150307
[epoch16, step2371]: loss 0.577598
[epoch16, step2372]: loss 0.549154
[epoch16, step2373]: loss 0.378315
[epoch16, step2374]: loss 0.436976
[epoch16, step2375]: loss 0.403820
[epoch16, step2376]: loss 0.610514
[epoch16, step2377]: loss 0.563551
[epoch16, step2378]: loss 0.508005
[epoch16, step2379]: loss 0.537113
[epoch16, step2380]: loss 0.484999
[epoch16, step2381]: loss 0.361274
[epoch16, step2382]: loss 0.560506
[epoch16, step2383]: loss 0.421075
[epoch16, step2384]: loss 0.492497
[epoch16, step2385]: loss 0.543606
[epoch16, step2386]: loss 0.464575
[epoch16, step2387]: loss 0.585771
[epoch16, step2388]: loss 0.540939
[epoch16, step2389]: loss 0.743321
[epoch16, step2390]: loss 0.544806
[epoch16, step2391]: loss 0.502197
[epoch16, step2392]: loss 0.445822
[epoch16, step2393]: loss 0.446765
[epoch16, step2394]: loss 0.469411
[epoch16, step2395]: loss 0.490237
[epoch16, step2396]: loss 0.437919
[epoch16, step2397]: loss 0.607752
[epoch16, step2398]: loss 0.559290
[epoch16, step2399]: loss 0.566027
[epoch16, step2400]: loss 0.573603
[epoch16, step2401]: loss 0.549469
[epoch16, step2402]: loss 0.677678
[epoch16, step2403]: loss 0.590236
[epoch16, step2404]: loss 0.460728
[epoch16, step2405]: loss 0.522969
[epoch16, step2406]: loss 0.412944
[epoch16, step2407]: loss 0.570784
[epoch16, step2408]: loss 0.413100
[epoch16, step2409]: loss 0.395038
[epoch16, step2410]: loss 0.489636
[epoch16, step2411]: loss 0.355754
[epoch16, step2412]: loss 0.245887
[epoch16, step2413]: loss 0.649108
[epoch16, step2414]: loss 0.447258
[epoch16, step2415]: loss 0.724423
[epoch16, step2416]: loss 0.691750
[epoch16, step2417]: loss 0.626437
[epoch16, step2418]: loss 0.343131
[epoch16, step2419]: loss 0.412370
[epoch16, step2420]: loss 0.621666
[epoch16, step2421]: loss 0.609321
[epoch16, step2422]: loss 0.200676
[epoch16, step2423]: loss 0.482746
[epoch16, step2424]: loss 0.483133
[epoch16, step2425]: loss 0.351502
[epoch16, step2426]: loss 0.536125
[epoch16, step2427]: loss 0.441391
[epoch16, step2428]: loss 0.275030
[epoch16, step2429]: loss 0.356815
[epoch16, step2430]: loss 0.431358
[epoch16, step2431]: loss 0.635229
[epoch16, step2432]: loss 0.167515
[epoch16, step2433]: loss 0.196795
[epoch16, step2434]: loss 0.324076
[epoch16, step2435]: loss 0.508269
[epoch16, step2436]: loss 0.482073
[epoch16, step2437]: loss 0.315898
[epoch16, step2438]: loss 0.417446
[epoch16, step2439]: loss 0.325990
[epoch16, step2440]: loss 0.426729
[epoch16, step2441]: loss 0.437138
[epoch16, step2442]: loss 0.461347
[epoch16, step2443]: loss 0.776773
[epoch16, step2444]: loss 0.409303
[epoch16, step2445]: loss 0.589700
[epoch16, step2446]: loss 0.389882
[epoch16, step2447]: loss 0.427208
[epoch16, step2448]: loss 0.571524
[epoch16, step2449]: loss 0.483770
[epoch16, step2450]: loss 0.653474
[epoch16, step2451]: loss 0.447177
[epoch16, step2452]: loss 0.480129
[epoch16, step2453]: loss 0.425978
[epoch16, step2454]: loss 0.288720
[epoch16, step2455]: loss 0.326640
[epoch16, step2456]: loss 0.456730
[epoch16, step2457]: loss 0.299734
[epoch16, step2458]: loss 0.354389
[epoch16, step2459]: loss 0.684202
[epoch16, step2460]: loss 0.497371
[epoch16, step2461]: loss 0.323219
[epoch16, step2462]: loss 0.499577
[epoch16, step2463]: loss 0.492205
[epoch16, step2464]: loss 0.487531
[epoch16, step2465]: loss 0.674025
[epoch16, step2466]: loss 0.628475
[epoch16, step2467]: loss 0.402586
[epoch16, step2468]: loss 0.695961
[epoch16, step2469]: loss 0.590765
[epoch16, step2470]: loss 0.537086
[epoch16, step2471]: loss 0.400947
[epoch16, step2472]: loss 0.365205
[epoch16, step2473]: loss 0.552576
[epoch16, step2474]: loss 0.362071
[epoch16, step2475]: loss 0.402796
[epoch16, step2476]: loss 0.574927
[epoch16, step2477]: loss 0.398468
[epoch16, step2478]: loss 0.538514
[epoch16, step2479]: loss 0.578692
[epoch16, step2480]: loss 0.342934
[epoch16, step2481]: loss 0.512102
[epoch16, step2482]: loss 0.441701
[epoch16, step2483]: loss 0.616056
[epoch16, step2484]: loss 0.265305
[epoch16, step2485]: loss 0.512009
[epoch16, step2486]: loss 0.148052
[epoch16, step2487]: loss 0.490176
[epoch16, step2488]: loss 0.467682
[epoch16, step2489]: loss 0.317872
[epoch16, step2490]: loss 0.266337
[epoch16, step2491]: loss 0.382097
[epoch16, step2492]: loss 0.557841
[epoch16, step2493]: loss 0.357746
[epoch16, step2494]: loss 0.479389
[epoch16, step2495]: loss 0.481252
[epoch16, step2496]: loss 0.665539
[epoch16, step2497]: loss 0.492848
[epoch16, step2498]: loss 0.630414
[epoch16, step2499]: loss 0.437298
[epoch16, step2500]: loss 0.636334
[epoch16, step2501]: loss 0.606562
[epoch16, step2502]: loss 0.472886
[epoch16, step2503]: loss 0.641991
[epoch16, step2504]: loss 0.422933
[epoch16, step2505]: loss 0.410116
[epoch16, step2506]: loss 0.461676
[epoch16, step2507]: loss 0.409082
[epoch16, step2508]: loss 0.426631
[epoch16, step2509]: loss 0.701226
[epoch16, step2510]: loss 0.380047
[epoch16, step2511]: loss 0.246117
[epoch16, step2512]: loss 0.502141
[epoch16, step2513]: loss 0.510229
[epoch16, step2514]: loss 0.369986
[epoch16, step2515]: loss 0.522161
[epoch16, step2516]: loss 0.748255
[epoch16, step2517]: loss 0.543731
[epoch16, step2518]: loss 0.429357
[epoch16, step2519]: loss 0.404382
[epoch16, step2520]: loss 0.377383
[epoch16, step2521]: loss 0.539953
[epoch16, step2522]: loss 0.432444
[epoch16, step2523]: loss 0.624685
[epoch16, step2524]: loss 0.304634
[epoch16, step2525]: loss 0.432107
[epoch16, step2526]: loss 0.685591
[epoch16, step2527]: loss 0.513642
[epoch16, step2528]: loss 0.451728
[epoch16, step2529]: loss 0.574777
[epoch16, step2530]: loss 0.435295
[epoch16, step2531]: loss 0.107722
[epoch16, step2532]: loss 0.644599
[epoch16, step2533]: loss 0.274677
[epoch16, step2534]: loss 0.414158
[epoch16, step2535]: loss 0.515690
[epoch16, step2536]: loss 0.554015
[epoch16, step2537]: loss 0.454636
[epoch16, step2538]: loss 0.257603
[epoch16, step2539]: loss 0.381921
[epoch16, step2540]: loss 0.703752
[epoch16, step2541]: loss 0.358882
[epoch16, step2542]: loss 0.258559
[epoch16, step2543]: loss 0.564106
[epoch16, step2544]: loss 0.557077
[epoch16, step2545]: loss 0.410949
[epoch16, step2546]: loss 0.625879
[epoch16, step2547]: loss 0.594749
[epoch16, step2548]: loss 0.492726
[epoch16, step2549]: loss 0.202280
[epoch16, step2550]: loss 0.383158
[epoch16, step2551]: loss 0.618826
[epoch16, step2552]: loss 0.439800
[epoch16, step2553]: loss 0.381795
[epoch16, step2554]: loss 0.634049
[epoch16, step2555]: loss 0.546577
[epoch16, step2556]: loss 0.532785
[epoch16, step2557]: loss 0.491829
[epoch16, step2558]: loss 0.162891
[epoch16, step2559]: loss 0.250863
[epoch16, step2560]: loss 0.319240
[epoch16, step2561]: loss 0.538356
[epoch16, step2562]: loss 0.564254
[epoch16, step2563]: loss 0.621836
[epoch16, step2564]: loss 0.644932
[epoch16, step2565]: loss 0.563817
[epoch16, step2566]: loss 0.678059
[epoch16, step2567]: loss 0.517572
[epoch16, step2568]: loss 0.474442
[epoch16, step2569]: loss 0.427016
[epoch16, step2570]: loss 0.546887
[epoch16, step2571]: loss 0.459591
[epoch16, step2572]: loss 0.414400
[epoch16, step2573]: loss 0.115372
[epoch16, step2574]: loss 0.453875
[epoch16, step2575]: loss 0.489866
[epoch16, step2576]: loss 0.688566
[epoch16, step2577]: loss 0.263100
[epoch16, step2578]: loss 0.372953
[epoch16, step2579]: loss 0.256321
[epoch16, step2580]: loss 0.414908
[epoch16, step2581]: loss 0.712505
[epoch16, step2582]: loss 0.418932
[epoch16, step2583]: loss 0.605713
[epoch16, step2584]: loss 0.297286
[epoch16, step2585]: loss 0.450682
[epoch16, step2586]: loss 0.427984
[epoch16, step2587]: loss 0.559475
[epoch16, step2588]: loss 0.513854
[epoch16, step2589]: loss 0.255286
[epoch16, step2590]: loss 0.646667
[epoch16, step2591]: loss 0.504333
[epoch16, step2592]: loss 0.246918
[epoch16, step2593]: loss 0.622643
[epoch16, step2594]: loss 0.339926
[epoch16, step2595]: loss 0.329853
[epoch16, step2596]: loss 0.414080
[epoch16, step2597]: loss 0.426937
[epoch16, step2598]: loss 0.643589
[epoch16, step2599]: loss 0.503214
[epoch16, step2600]: loss 0.590141
[epoch16, step2601]: loss 0.445302
[epoch16, step2602]: loss 0.360032
[epoch16, step2603]: loss 0.401447
[epoch16, step2604]: loss 0.235534
[epoch16, step2605]: loss 0.370691
[epoch16, step2606]: loss 0.541884
[epoch16, step2607]: loss 0.624289
[epoch16, step2608]: loss 0.511765
[epoch16, step2609]: loss 0.390478
[epoch16, step2610]: loss 0.418090
[epoch16, step2611]: loss 0.536533
[epoch16, step2612]: loss 0.627946
[epoch16, step2613]: loss 0.255625
[epoch16, step2614]: loss 0.607608
[epoch16, step2615]: loss 0.554649
[epoch16, step2616]: loss 0.452559
[epoch16, step2617]: loss 0.728882
[epoch16, step2618]: loss 0.505054
[epoch16, step2619]: loss 0.478232
[epoch16, step2620]: loss 0.244338
[epoch16, step2621]: loss 0.444115
[epoch16, step2622]: loss 0.388377
[epoch16, step2623]: loss 0.697024
[epoch16, step2624]: loss 0.343761
[epoch16, step2625]: loss 0.550495
[epoch16, step2626]: loss 0.515998
[epoch16, step2627]: loss 0.345435
[epoch16, step2628]: loss 0.554374
[epoch16, step2629]: loss 0.592754
[epoch16, step2630]: loss 0.459614
[epoch16, step2631]: loss 0.134242
[epoch16, step2632]: loss 0.463026
[epoch16, step2633]: loss 0.544867
[epoch16, step2634]: loss 0.330311
[epoch16, step2635]: loss 0.168537
[epoch16, step2636]: loss 0.297804
[epoch16, step2637]: loss 0.551135
[epoch16, step2638]: loss 0.592954
[epoch16, step2639]: loss 0.383649
[epoch16, step2640]: loss 0.424634
[epoch16, step2641]: loss 0.429352
[epoch16, step2642]: loss 0.335466
[epoch16, step2643]: loss 0.378987
[epoch16, step2644]: loss 0.566927
[epoch16, step2645]: loss 0.592293
[epoch16, step2646]: loss 0.469560
[epoch16, step2647]: loss 0.417255
[epoch16, step2648]: loss 0.539479
[epoch16, step2649]: loss 0.637174
[epoch16, step2650]: loss 0.265919
[epoch16, step2651]: loss 0.432958
[epoch16, step2652]: loss 0.642369
[epoch16, step2653]: loss 0.514066
[epoch16, step2654]: loss 0.531586
[epoch16, step2655]: loss 0.627283
[epoch16, step2656]: loss 0.348281
[epoch16, step2657]: loss 0.506436
[epoch16, step2658]: loss 0.434426
[epoch16, step2659]: loss 0.392838
[epoch16, step2660]: loss 0.511223
[epoch16, step2661]: loss 0.249064
[epoch16, step2662]: loss 0.365325
[epoch16, step2663]: loss 0.595415
[epoch16, step2664]: loss 0.379577
[epoch16, step2665]: loss 0.506793
[epoch16, step2666]: loss 0.365575
[epoch16, step2667]: loss 0.555216
[epoch16, step2668]: loss 0.558023
[epoch16, step2669]: loss 0.558543
[epoch16, step2670]: loss 0.571914
[epoch16, step2671]: loss 0.134773
[epoch16, step2672]: loss 0.368182
[epoch16, step2673]: loss 0.667734
[epoch16, step2674]: loss 0.705912
[epoch16, step2675]: loss 0.223350
[epoch16, step2676]: loss 0.511013
[epoch16, step2677]: loss 0.484629
[epoch16, step2678]: loss 0.444839
[epoch16, step2679]: loss 0.532848
[epoch16, step2680]: loss 0.334750
[epoch16, step2681]: loss 0.708050
[epoch16, step2682]: loss 0.508807
[epoch16, step2683]: loss 0.431630
[epoch16, step2684]: loss 0.150269
[epoch16, step2685]: loss 0.302379
[epoch16, step2686]: loss 0.523841
[epoch16, step2687]: loss 0.728613
[epoch16, step2688]: loss 0.476920
[epoch16, step2689]: loss 0.627425
[epoch16, step2690]: loss 0.558392
[epoch16, step2691]: loss 0.445567
[epoch16, step2692]: loss 0.629892
[epoch16, step2693]: loss 0.477858
[epoch16, step2694]: loss 0.604876
[epoch16, step2695]: loss 0.356715
[epoch16, step2696]: loss 0.438660
[epoch16, step2697]: loss 0.205342
[epoch16, step2698]: loss 0.582833
[epoch16, step2699]: loss 0.437133
[epoch16, step2700]: loss 0.569056
[epoch16, step2701]: loss 0.423104
[epoch16, step2702]: loss 0.488692
[epoch16, step2703]: loss 0.728814
[epoch16, step2704]: loss 0.422837
[epoch16, step2705]: loss 0.433068
[epoch16, step2706]: loss 0.452057
[epoch16, step2707]: loss 0.317287
[epoch16, step2708]: loss 0.544903
[epoch16, step2709]: loss 0.591873
[epoch16, step2710]: loss 0.447790
[epoch16, step2711]: loss 0.218785
[epoch16, step2712]: loss 0.357684
[epoch16, step2713]: loss 0.608283
[epoch16, step2714]: loss 0.284820
[epoch16, step2715]: loss 0.342170
[epoch16, step2716]: loss 0.448361
[epoch16, step2717]: loss 0.741582
[epoch16, step2718]: loss 0.431080
[epoch16, step2719]: loss 0.590101
[epoch16, step2720]: loss 0.630741
[epoch16, step2721]: loss 0.264666
[epoch16, step2722]: loss 0.112226
[epoch16, step2723]: loss 0.454283
[epoch16, step2724]: loss 0.384967
[epoch16, step2725]: loss 0.316067
[epoch16, step2726]: loss 0.484075
[epoch16, step2727]: loss 0.470773
[epoch16, step2728]: loss 0.438939
[epoch16, step2729]: loss 0.430496
[epoch16, step2730]: loss 0.521622
[epoch16, step2731]: loss 0.472860
[epoch16, step2732]: loss 0.314286
[epoch16, step2733]: loss 0.427217
[epoch16, step2734]: loss 0.399493
[epoch16, step2735]: loss 0.219668
[epoch16, step2736]: loss 0.424695
[epoch16, step2737]: loss 0.420202
[epoch16, step2738]: loss 0.597854
[epoch16, step2739]: loss 0.538504
[epoch16, step2740]: loss 0.351277
[epoch16, step2741]: loss 0.337097
[epoch16, step2742]: loss 0.500581
[epoch16, step2743]: loss 0.303968
[epoch16, step2744]: loss 0.551843
[epoch16, step2745]: loss 0.581630
[epoch16, step2746]: loss 0.523637
[epoch16, step2747]: loss 0.346051
[epoch16, step2748]: loss 0.618924
[epoch16, step2749]: loss 0.383620
[epoch16, step2750]: loss 0.464188
[epoch16, step2751]: loss 0.500764
[epoch16, step2752]: loss 0.539319
[epoch16, step2753]: loss 0.550980
[epoch16, step2754]: loss 0.434069
[epoch16, step2755]: loss 0.527321
[epoch16, step2756]: loss 0.504341
[epoch16, step2757]: loss 0.597723
[epoch16, step2758]: loss 0.271171
[epoch16, step2759]: loss 0.283405
[epoch16, step2760]: loss 0.551781
[epoch16, step2761]: loss 0.486590
[epoch16, step2762]: loss 0.608089
[epoch16, step2763]: loss 0.387672
[epoch16, step2764]: loss 0.089821
[epoch16, step2765]: loss 0.509169
[epoch16, step2766]: loss 0.478291
[epoch16, step2767]: loss 0.396009
[epoch16, step2768]: loss 0.431201
[epoch16, step2769]: loss 0.497764
[epoch16, step2770]: loss 0.339038
[epoch16, step2771]: loss 0.547881
[epoch16, step2772]: loss 0.396590
[epoch16, step2773]: loss 0.605942
[epoch16, step2774]: loss 0.393469
[epoch16, step2775]: loss 0.372222
[epoch16, step2776]: loss 0.422608
[epoch16, step2777]: loss 0.570849
[epoch16, step2778]: loss 0.590469
[epoch16, step2779]: loss 0.439366
[epoch16, step2780]: loss 0.731220
[epoch16, step2781]: loss 0.530215
[epoch16, step2782]: loss 0.469243
[epoch16, step2783]: loss 0.679311
[epoch16, step2784]: loss 0.477844
[epoch16, step2785]: loss 0.361936
[epoch16, step2786]: loss 0.340217
[epoch16, step2787]: loss 0.538872
[epoch16, step2788]: loss 0.490124
[epoch16, step2789]: loss 0.650267
[epoch16, step2790]: loss 0.351564
[epoch16, step2791]: loss 0.444815
[epoch16, step2792]: loss 0.484056
[epoch16, step2793]: loss 0.623722
[epoch16, step2794]: loss 0.442100
[epoch16, step2795]: loss 0.485288
[epoch16, step2796]: loss 0.381454
[epoch16, step2797]: loss 0.533472
[epoch16, step2798]: loss 0.231372
[epoch16, step2799]: loss 0.657762
[epoch16, step2800]: loss 0.719466
[epoch16, step2801]: loss 0.436554
[epoch16, step2802]: loss 0.588609
[epoch16, step2803]: loss 0.363635
[epoch16, step2804]: loss 0.519512
[epoch16, step2805]: loss 0.608366
[epoch16, step2806]: loss 0.470258
[epoch16, step2807]: loss 0.491428
[epoch16, step2808]: loss 0.514806
[epoch16, step2809]: loss 0.490287
[epoch16, step2810]: loss 0.296491
[epoch16, step2811]: loss 0.571641
[epoch16, step2812]: loss 0.287119
[epoch16, step2813]: loss 0.248529
[epoch16, step2814]: loss 0.585683
[epoch16, step2815]: loss 0.483654
[epoch16, step2816]: loss 0.226839
[epoch16, step2817]: loss 0.518703
[epoch16, step2818]: loss 0.448481
[epoch16, step2819]: loss 0.581973
[epoch16, step2820]: loss 0.497766
[epoch16, step2821]: loss 0.618526
[epoch16, step2822]: loss 0.507350
[epoch16, step2823]: loss 0.510898
[epoch16, step2824]: loss 0.752419
[epoch16, step2825]: loss 0.510151
[epoch16, step2826]: loss 0.513685
[epoch16, step2827]: loss 0.294954
[epoch16, step2828]: loss 0.257258
[epoch16, step2829]: loss 0.558852
[epoch16, step2830]: loss 0.543506
[epoch16, step2831]: loss 0.593512
[epoch16, step2832]: loss 0.520706
[epoch16, step2833]: loss 0.569611
[epoch16, step2834]: loss 0.418482
[epoch16, step2835]: loss 0.319766
[epoch16, step2836]: loss 0.332702
[epoch16, step2837]: loss 0.649844
[epoch16, step2838]: loss 0.453884
[epoch16, step2839]: loss 0.473805
[epoch16, step2840]: loss 0.446457
[epoch16, step2841]: loss 0.368889
[epoch16, step2842]: loss 0.586687
[epoch16, step2843]: loss 0.534854
[epoch16, step2844]: loss 0.531256
[epoch16, step2845]: loss 0.455561
[epoch16, step2846]: loss 0.379073
[epoch16, step2847]: loss 0.153553
[epoch16, step2848]: loss 0.598907
[epoch16, step2849]: loss 0.330164
[epoch16, step2850]: loss 0.426627
[epoch16, step2851]: loss 0.578364
[epoch16, step2852]: loss 0.493777
[epoch16, step2853]: loss 0.407308
[epoch16, step2854]: loss 0.353388
[epoch16, step2855]: loss 0.252901
[epoch16, step2856]: loss 0.571846
[epoch16, step2857]: loss 0.517428
[epoch16, step2858]: loss 0.605833
[epoch16, step2859]: loss 0.433627
[epoch16, step2860]: loss 0.621944
[epoch16, step2861]: loss 0.258339
[epoch16, step2862]: loss 0.501555
[epoch16, step2863]: loss 0.494243
[epoch16, step2864]: loss 0.530235
[epoch16, step2865]: loss 0.312514
[epoch16, step2866]: loss 0.667674
[epoch16, step2867]: loss 0.353510
[epoch16, step2868]: loss 0.356125
[epoch16, step2869]: loss 0.274849
[epoch16, step2870]: loss 0.438964
[epoch16, step2871]: loss 0.437029
[epoch16, step2872]: loss 0.493534
[epoch16, step2873]: loss 0.666757
[epoch16, step2874]: loss 0.371113
[epoch16, step2875]: loss 0.748159
[epoch16, step2876]: loss 0.412667
[epoch16, step2877]: loss 0.585332
[epoch16, step2878]: loss 0.536375
[epoch16, step2879]: loss 0.514896
[epoch16, step2880]: loss 0.339547
[epoch16, step2881]: loss 0.517092
[epoch16, step2882]: loss 0.439278
[epoch16, step2883]: loss 0.398534
[epoch16, step2884]: loss 0.335495
[epoch16, step2885]: loss 0.736444
[epoch16, step2886]: loss 0.527209
[epoch16, step2887]: loss 0.423234
[epoch16, step2888]: loss 0.587527
[epoch16, step2889]: loss 0.428364
[epoch16, step2890]: loss 0.452498
[epoch16, step2891]: loss 0.337895
[epoch16, step2892]: loss 0.488832
[epoch16, step2893]: loss 0.511555
[epoch16, step2894]: loss 0.395607
[epoch16, step2895]: loss 0.597598
[epoch16, step2896]: loss 0.631094
[epoch16, step2897]: loss 0.136349
[epoch16, step2898]: loss 0.486166
[epoch16, step2899]: loss 0.743080
[epoch16, step2900]: loss 0.548893
[epoch16, step2901]: loss 0.370108
[epoch16, step2902]: loss 0.325555
[epoch16, step2903]: loss 0.183341
[epoch16, step2904]: loss 0.271428
[epoch16, step2905]: loss 0.368396
[epoch16, step2906]: loss 0.443958
[epoch16, step2907]: loss 0.331777
[epoch16, step2908]: loss 0.561944
[epoch16, step2909]: loss 0.316177
[epoch16, step2910]: loss 0.658054
[epoch16, step2911]: loss 0.337109
[epoch16, step2912]: loss 0.513820
[epoch16, step2913]: loss 0.688956
[epoch16, step2914]: loss 0.524182
[epoch16, step2915]: loss 0.282615
[epoch16, step2916]: loss 0.332349
[epoch16, step2917]: loss 0.538851
[epoch16, step2918]: loss 0.618321
[epoch16, step2919]: loss 0.275194
[epoch16, step2920]: loss 0.505602
[epoch16, step2921]: loss 0.600729
[epoch16, step2922]: loss 0.450687
[epoch16, step2923]: loss 0.545732
[epoch16, step2924]: loss 0.334701
[epoch16, step2925]: loss 0.279846
[epoch16, step2926]: loss 0.425920
[epoch16, step2927]: loss 0.391731
[epoch16, step2928]: loss 0.591184
[epoch16, step2929]: loss 0.508175
[epoch16, step2930]: loss 0.586641
[epoch16, step2931]: loss 0.569541
[epoch16, step2932]: loss 0.496872
[epoch16, step2933]: loss 0.461707
[epoch16, step2934]: loss 0.177492
[epoch16, step2935]: loss 0.409296
[epoch16, step2936]: loss 0.638284
[epoch16, step2937]: loss 0.395247
[epoch16, step2938]: loss 0.477972
[epoch16, step2939]: loss 0.303397
[epoch16, step2940]: loss 0.539144
[epoch16, step2941]: loss 0.502099
[epoch16, step2942]: loss 0.249196
[epoch16, step2943]: loss 0.365022
[epoch16, step2944]: loss 0.464756
[epoch16, step2945]: loss 0.788992
[epoch16, step2946]: loss 0.162062
[epoch16, step2947]: loss 0.366628
[epoch16, step2948]: loss 0.336998
[epoch16, step2949]: loss 0.408521
[epoch16, step2950]: loss 0.368748
[epoch16, step2951]: loss 0.570798
[epoch16, step2952]: loss 0.557093
[epoch16, step2953]: loss 0.754798
[epoch16, step2954]: loss 0.522433
[epoch16, step2955]: loss 0.644354
[epoch16, step2956]: loss 0.450340
[epoch16, step2957]: loss 0.429308
[epoch16, step2958]: loss 0.417533
[epoch16, step2959]: loss 0.269820
[epoch16, step2960]: loss 0.586972
[epoch16, step2961]: loss 0.668429
[epoch16, step2962]: loss 0.328599
[epoch16, step2963]: loss 0.600334
[epoch16, step2964]: loss 0.536822
[epoch16, step2965]: loss 0.318773
[epoch16, step2966]: loss 0.221431
[epoch16, step2967]: loss 0.379261
[epoch16, step2968]: loss 0.505681
[epoch16, step2969]: loss 0.592081
[epoch16, step2970]: loss 0.439070
[epoch16, step2971]: loss 0.390233
[epoch16, step2972]: loss 0.483593
[epoch16, step2973]: loss 0.420635
[epoch16, step2974]: loss 0.431975
[epoch16, step2975]: loss 0.420624
[epoch16, step2976]: loss 0.647202
[epoch16, step2977]: loss 0.452454
[epoch16, step2978]: loss 0.447680
[epoch16, step2979]: loss 0.222269
[epoch16, step2980]: loss 0.537840
[epoch16, step2981]: loss 0.380632
[epoch16, step2982]: loss 0.366498
[epoch16, step2983]: loss 0.424263
[epoch16, step2984]: loss 0.380092
[epoch16, step2985]: loss 0.524061
[epoch16, step2986]: loss 0.617631
[epoch16, step2987]: loss 0.664636
[epoch16, step2988]: loss 0.323245
[epoch16, step2989]: loss 0.323257
[epoch16, step2990]: loss 0.585096
[epoch16, step2991]: loss 0.510775
[epoch16, step2992]: loss 0.437977
[epoch16, step2993]: loss 0.560690
[epoch16, step2994]: loss 0.431856
[epoch16, step2995]: loss 0.685573
[epoch16, step2996]: loss 0.627830
[epoch16, step2997]: loss 0.214485
[epoch16, step2998]: loss 0.695959
[epoch16, step2999]: loss 0.381856
[epoch16, step3000]: loss 0.635389
[epoch16, step3001]: loss 0.353999
[epoch16, step3002]: loss 0.444741
[epoch16, step3003]: loss 0.365860
[epoch16, step3004]: loss 0.441086
[epoch16, step3005]: loss 0.449800
[epoch16, step3006]: loss 0.318426
[epoch16, step3007]: loss 0.477001
[epoch16, step3008]: loss 0.377152
[epoch16, step3009]: loss 0.473914
[epoch16, step3010]: loss 0.594752
[epoch16, step3011]: loss 0.391631
[epoch16, step3012]: loss 0.344963
[epoch16, step3013]: loss 0.327029
[epoch16, step3014]: loss 0.622488
[epoch16, step3015]: loss 0.594435
[epoch16, step3016]: loss 0.172833
[epoch16, step3017]: loss 0.312327
[epoch16, step3018]: loss 0.375417
[epoch16, step3019]: loss 0.578903
[epoch16, step3020]: loss 0.593337
[epoch16, step3021]: loss 0.587935
[epoch16, step3022]: loss 0.463960
[epoch16, step3023]: loss 0.534331
[epoch16, step3024]: loss 0.405526
[epoch16, step3025]: loss 0.561040
[epoch16, step3026]: loss 0.284406
[epoch16, step3027]: loss 0.501130
[epoch16, step3028]: loss 0.581432
[epoch16, step3029]: loss 0.454382
[epoch16, step3030]: loss 0.507267
[epoch16, step3031]: loss 0.253979
[epoch16, step3032]: loss 0.707389
[epoch16, step3033]: loss 0.421418
[epoch16, step3034]: loss 0.377709
[epoch16, step3035]: loss 0.330909
[epoch16, step3036]: loss 0.404816
[epoch16, step3037]: loss 0.482036
[epoch16, step3038]: loss 0.479177
[epoch16, step3039]: loss 0.264397
[epoch16, step3040]: loss 0.481196
[epoch16, step3041]: loss 0.500722
[epoch16, step3042]: loss 0.442769
[epoch16, step3043]: loss 0.251124
[epoch16, step3044]: loss 0.628086
[epoch16, step3045]: loss 0.350630
[epoch16, step3046]: loss 0.645696
[epoch16, step3047]: loss 0.639009
[epoch16, step3048]: loss 0.476280
[epoch16, step3049]: loss 0.524269
[epoch16, step3050]: loss 0.331348
[epoch16, step3051]: loss 0.530991
[epoch16, step3052]: loss 0.448920
[epoch16, step3053]: loss 0.675876
[epoch16, step3054]: loss 0.625512
[epoch16, step3055]: loss 0.689201
[epoch16, step3056]: loss 0.847450
[epoch16, step3057]: loss 0.545355
[epoch16, step3058]: loss 0.351051
[epoch16, step3059]: loss 0.424452
[epoch16, step3060]: loss 0.411143
[epoch16, step3061]: loss 0.502086
[epoch16, step3062]: loss 0.442798
[epoch16, step3063]: loss 0.377470
[epoch16, step3064]: loss 0.380739
[epoch16, step3065]: loss 0.645346
[epoch16, step3066]: loss 0.654686
[epoch16, step3067]: loss 0.364564
[epoch16, step3068]: loss 0.449279
[epoch16, step3069]: loss 0.440705
[epoch16, step3070]: loss 0.144503
[epoch16, step3071]: loss 0.437339
[epoch16, step3072]: loss 0.424870
[epoch16, step3073]: loss 0.443919
[epoch16, step3074]: loss 0.396898
[epoch16, step3075]: loss 0.506485
[epoch16, step3076]: loss 0.342903

[epoch16]: avg loss 0.342903

[epoch17, step1]: loss 0.629050
[epoch17, step2]: loss 0.489642
[epoch17, step3]: loss 0.267127
[epoch17, step4]: loss 0.420753
[epoch17, step5]: loss 0.551692
[epoch17, step6]: loss 0.453799
[epoch17, step7]: loss 0.677797
[epoch17, step8]: loss 0.194346
[epoch17, step9]: loss 0.628609
[epoch17, step10]: loss 0.387870
[epoch17, step11]: loss 0.450790
[epoch17, step12]: loss 0.358828
[epoch17, step13]: loss 0.644399
[epoch17, step14]: loss 0.539809
[epoch17, step15]: loss 0.503674
[epoch17, step16]: loss 0.217162
[epoch17, step17]: loss 0.491001
[epoch17, step18]: loss 0.465684
[epoch17, step19]: loss 0.462582
[epoch17, step20]: loss 0.439919
[epoch17, step21]: loss 0.568177
[epoch17, step22]: loss 0.507657
[epoch17, step23]: loss 0.285051
[epoch17, step24]: loss 0.676751
[epoch17, step25]: loss 0.307960
[epoch17, step26]: loss 0.376379
[epoch17, step27]: loss 0.249076
[epoch17, step28]: loss 0.254182
[epoch17, step29]: loss 0.242789
[epoch17, step30]: loss 0.524880
[epoch17, step31]: loss 0.458390
[epoch17, step32]: loss 0.434177
[epoch17, step33]: loss 0.337626
[epoch17, step34]: loss 0.359125
[epoch17, step35]: loss 0.431848
[epoch17, step36]: loss 0.413814
[epoch17, step37]: loss 0.330317
[epoch17, step38]: loss 0.526326
[epoch17, step39]: loss 0.412347
[epoch17, step40]: loss 0.467613
[epoch17, step41]: loss 0.393377
[epoch17, step42]: loss 0.534489
[epoch17, step43]: loss 0.628235
[epoch17, step44]: loss 0.591427
[epoch17, step45]: loss 0.510162
[epoch17, step46]: loss 0.320489
[epoch17, step47]: loss 0.453544
[epoch17, step48]: loss 0.386242
[epoch17, step49]: loss 0.286905
[epoch17, step50]: loss 0.544970
[epoch17, step51]: loss 0.602183
[epoch17, step52]: loss 0.444011
[epoch17, step53]: loss 0.538885
[epoch17, step54]: loss 0.616048
[epoch17, step55]: loss 0.341915
[epoch17, step56]: loss 0.458951
[epoch17, step57]: loss 0.336132
[epoch17, step58]: loss 0.535623
[epoch17, step59]: loss 0.501545
[epoch17, step60]: loss 0.365676
[epoch17, step61]: loss 0.515605
[epoch17, step62]: loss 0.451283
[epoch17, step63]: loss 0.524489
[epoch17, step64]: loss 0.409033
[epoch17, step65]: loss 0.406735
[epoch17, step66]: loss 0.331653
[epoch17, step67]: loss 0.219991
[epoch17, step68]: loss 0.407653
[epoch17, step69]: loss 0.492844
[epoch17, step70]: loss 0.482833
[epoch17, step71]: loss 0.476965
[epoch17, step72]: loss 0.580825
[epoch17, step73]: loss 0.515461
[epoch17, step74]: loss 0.549377
[epoch17, step75]: loss 0.514485
[epoch17, step76]: loss 0.418774
[epoch17, step77]: loss 0.515429
[epoch17, step78]: loss 0.572728
[epoch17, step79]: loss 0.632142
[epoch17, step80]: loss 0.733539
[epoch17, step81]: loss 0.351719
[epoch17, step82]: loss 0.503892
[epoch17, step83]: loss 0.410705
[epoch17, step84]: loss 0.358787
[epoch17, step85]: loss 0.438787
[epoch17, step86]: loss 0.473734
[epoch17, step87]: loss 0.325052
[epoch17, step88]: loss 0.360128
[epoch17, step89]: loss 0.610567
[epoch17, step90]: loss 0.612349
[epoch17, step91]: loss 0.505625
[epoch17, step92]: loss 0.424684
[epoch17, step93]: loss 0.240528
[epoch17, step94]: loss 0.416454
[epoch17, step95]: loss 0.722366
[epoch17, step96]: loss 0.268161
[epoch17, step97]: loss 0.572467
[epoch17, step98]: loss 0.400261
[epoch17, step99]: loss 0.364489
[epoch17, step100]: loss 0.553973
[epoch17, step101]: loss 0.627732
[epoch17, step102]: loss 0.565091
[epoch17, step103]: loss 0.468295
[epoch17, step104]: loss 0.384359
[epoch17, step105]: loss 0.144900
[epoch17, step106]: loss 0.761120
[epoch17, step107]: loss 0.400510
[epoch17, step108]: loss 0.435799
[epoch17, step109]: loss 0.274092
[epoch17, step110]: loss 0.236531
[epoch17, step111]: loss 0.501102
[epoch17, step112]: loss 0.458812
[epoch17, step113]: loss 0.285598
[epoch17, step114]: loss 0.237678
[epoch17, step115]: loss 0.535156
[epoch17, step116]: loss 0.380507
[epoch17, step117]: loss 0.332019
[epoch17, step118]: loss 0.147916
[epoch17, step119]: loss 0.477467
[epoch17, step120]: loss 0.517595
[epoch17, step121]: loss 0.538709
[epoch17, step122]: loss 0.500730
[epoch17, step123]: loss 0.537632
[epoch17, step124]: loss 0.236767
[epoch17, step125]: loss 0.218155
[epoch17, step126]: loss 0.641567
[epoch17, step127]: loss 0.445977
[epoch17, step128]: loss 0.276773
[epoch17, step129]: loss 0.501572
[epoch17, step130]: loss 0.341978
[epoch17, step131]: loss 0.687730
[epoch17, step132]: loss 0.481328
[epoch17, step133]: loss 0.271964
[epoch17, step134]: loss 0.372678
[epoch17, step135]: loss 0.527427
[epoch17, step136]: loss 0.589189
[epoch17, step137]: loss 0.686985
[epoch17, step138]: loss 0.657424
[epoch17, step139]: loss 0.256153
[epoch17, step140]: loss 0.655388
[epoch17, step141]: loss 0.519813
[epoch17, step142]: loss 0.384793
[epoch17, step143]: loss 0.454946
[epoch17, step144]: loss 0.527997
[epoch17, step145]: loss 0.609181
[epoch17, step146]: loss 0.505135
[epoch17, step147]: loss 0.512916
[epoch17, step148]: loss 0.446722
[epoch17, step149]: loss 0.449324
[epoch17, step150]: loss 0.400349
[epoch17, step151]: loss 0.376286
[epoch17, step152]: loss 0.395069
[epoch17, step153]: loss 0.337203
[epoch17, step154]: loss 0.446291
[epoch17, step155]: loss 0.543773
[epoch17, step156]: loss 0.316370
[epoch17, step157]: loss 0.321224
[epoch17, step158]: loss 0.518873
[epoch17, step159]: loss 0.230550
[epoch17, step160]: loss 0.312460
[epoch17, step161]: loss 0.465620
[epoch17, step162]: loss 0.579720
[epoch17, step163]: loss 0.396713
[epoch17, step164]: loss 0.439270
[epoch17, step165]: loss 0.602565
[epoch17, step166]: loss 0.400616
[epoch17, step167]: loss 0.465690
[epoch17, step168]: loss 0.416505
[epoch17, step169]: loss 0.470532
[epoch17, step170]: loss 0.528018
[epoch17, step171]: loss 0.597569
[epoch17, step172]: loss 0.330547
[epoch17, step173]: loss 0.530733
[epoch17, step174]: loss 0.449769
[epoch17, step175]: loss 0.465737
[epoch17, step176]: loss 0.385394
[epoch17, step177]: loss 0.319761
[epoch17, step178]: loss 0.509872
[epoch17, step179]: loss 0.593780
[epoch17, step180]: loss 0.435477
[epoch17, step181]: loss 0.502510
[epoch17, step182]: loss 0.405919
[epoch17, step183]: loss 0.563709
[epoch17, step184]: loss 0.399218
[epoch17, step185]: loss 0.497526
[epoch17, step186]: loss 0.509194
[epoch17, step187]: loss 0.459102
[epoch17, step188]: loss 0.663164
[epoch17, step189]: loss 0.464516
[epoch17, step190]: loss 0.460034
[epoch17, step191]: loss 0.281295
[epoch17, step192]: loss 0.607433
[epoch17, step193]: loss 0.534994
[epoch17, step194]: loss 0.606803
[epoch17, step195]: loss 0.525761
[epoch17, step196]: loss 0.539203
[epoch17, step197]: loss 0.518639
[epoch17, step198]: loss 0.385681
[epoch17, step199]: loss 0.263571
[epoch17, step200]: loss 0.469547
[epoch17, step201]: loss 0.388255
[epoch17, step202]: loss 0.343483
[epoch17, step203]: loss 0.530925
[epoch17, step204]: loss 0.354614
[epoch17, step205]: loss 0.467674
[epoch17, step206]: loss 0.636464
[epoch17, step207]: loss 0.505069
[epoch17, step208]: loss 0.549921
[epoch17, step209]: loss 0.449577
[epoch17, step210]: loss 0.379230
[epoch17, step211]: loss 0.537214
[epoch17, step212]: loss 0.306015
[epoch17, step213]: loss 0.307807
[epoch17, step214]: loss 0.319805
[epoch17, step215]: loss 0.573843
[epoch17, step216]: loss 0.125619
[epoch17, step217]: loss 0.472062
[epoch17, step218]: loss 0.329672
[epoch17, step219]: loss 0.498817
[epoch17, step220]: loss 0.667075
[epoch17, step221]: loss 0.468999
[epoch17, step222]: loss 0.701705
[epoch17, step223]: loss 0.501815
[epoch17, step224]: loss 0.503582
[epoch17, step225]: loss 0.612365
[epoch17, step226]: loss 0.374357
[epoch17, step227]: loss 0.281081
[epoch17, step228]: loss 0.560855
[epoch17, step229]: loss 0.574181
[epoch17, step230]: loss 0.497855
[epoch17, step231]: loss 0.428563
[epoch17, step232]: loss 0.434594
[epoch17, step233]: loss 0.575365
[epoch17, step234]: loss 0.454585
[epoch17, step235]: loss 0.543571
[epoch17, step236]: loss 0.602970
[epoch17, step237]: loss 0.460237
[epoch17, step238]: loss 0.336639
[epoch17, step239]: loss 0.415035
[epoch17, step240]: loss 0.488551
[epoch17, step241]: loss 0.573943
[epoch17, step242]: loss 0.555052
[epoch17, step243]: loss 0.266833
[epoch17, step244]: loss 0.576450
[epoch17, step245]: loss 0.482522
[epoch17, step246]: loss 0.652614
[epoch17, step247]: loss 0.373706
[epoch17, step248]: loss 0.471249
[epoch17, step249]: loss 0.532027
[epoch17, step250]: loss 0.639060
[epoch17, step251]: loss 0.116885
[epoch17, step252]: loss 0.486796
[epoch17, step253]: loss 0.445299
[epoch17, step254]: loss 0.503337
[epoch17, step255]: loss 0.694008
[epoch17, step256]: loss 0.557069
[epoch17, step257]: loss 0.461112
[epoch17, step258]: loss 0.519863
[epoch17, step259]: loss 0.518624
[epoch17, step260]: loss 0.541063
[epoch17, step261]: loss 0.481739
[epoch17, step262]: loss 0.514732
[epoch17, step263]: loss 0.540705
[epoch17, step264]: loss 0.211458
[epoch17, step265]: loss 0.659081
[epoch17, step266]: loss 0.614280
[epoch17, step267]: loss 0.572581
[epoch17, step268]: loss 0.453124
[epoch17, step269]: loss 0.150982
[epoch17, step270]: loss 0.540538
[epoch17, step271]: loss 0.274976
[epoch17, step272]: loss 0.450667
[epoch17, step273]: loss 0.504480
[epoch17, step274]: loss 0.294696
[epoch17, step275]: loss 0.484077
[epoch17, step276]: loss 0.436364
[epoch17, step277]: loss 0.527018
[epoch17, step278]: loss 0.495313
[epoch17, step279]: loss 0.249648
[epoch17, step280]: loss 0.613270
[epoch17, step281]: loss 0.586814
[epoch17, step282]: loss 0.468647
[epoch17, step283]: loss 0.258027
[epoch17, step284]: loss 0.354543
[epoch17, step285]: loss 0.548753
[epoch17, step286]: loss 0.328646
[epoch17, step287]: loss 0.443382
[epoch17, step288]: loss 0.629900
[epoch17, step289]: loss 0.611445
[epoch17, step290]: loss 0.535845
[epoch17, step291]: loss 0.357678
[epoch17, step292]: loss 0.454098
[epoch17, step293]: loss 0.463617
[epoch17, step294]: loss 0.669786
[epoch17, step295]: loss 0.529783
[epoch17, step296]: loss 0.439733
[epoch17, step297]: loss 0.320158
[epoch17, step298]: loss 0.488380
[epoch17, step299]: loss 0.633379
[epoch17, step300]: loss 0.549285
[epoch17, step301]: loss 0.541552
[epoch17, step302]: loss 0.541536
[epoch17, step303]: loss 0.495156
[epoch17, step304]: loss 0.389493
[epoch17, step305]: loss 0.448521
[epoch17, step306]: loss 0.609761
[epoch17, step307]: loss 0.318419
[epoch17, step308]: loss 0.544702
[epoch17, step309]: loss 0.175468
[epoch17, step310]: loss 0.545050
[epoch17, step311]: loss 0.427300
[epoch17, step312]: loss 0.423804
[epoch17, step313]: loss 0.317581
[epoch17, step314]: loss 0.454165
[epoch17, step315]: loss 0.533762
[epoch17, step316]: loss 0.481409
[epoch17, step317]: loss 0.626938
[epoch17, step318]: loss 0.404810
[epoch17, step319]: loss 0.573815
[epoch17, step320]: loss 0.444941
[epoch17, step321]: loss 0.552993
[epoch17, step322]: loss 0.415485
[epoch17, step323]: loss 0.524649
[epoch17, step324]: loss 0.205045
[epoch17, step325]: loss 0.538438
[epoch17, step326]: loss 0.399043
[epoch17, step327]: loss 0.359651
[epoch17, step328]: loss 0.481842
[epoch17, step329]: loss 0.486573
[epoch17, step330]: loss 0.524417
[epoch17, step331]: loss 0.405156
[epoch17, step332]: loss 0.547187
[epoch17, step333]: loss 0.237746
[epoch17, step334]: loss 0.565250
[epoch17, step335]: loss 0.510705
[epoch17, step336]: loss 0.497561
[epoch17, step337]: loss 0.290345
[epoch17, step338]: loss 0.489039
[epoch17, step339]: loss 0.447974
[epoch17, step340]: loss 0.676982
[epoch17, step341]: loss 0.531643
[epoch17, step342]: loss 0.521811
[epoch17, step343]: loss 0.383004
[epoch17, step344]: loss 0.390011
[epoch17, step345]: loss 0.247792
[epoch17, step346]: loss 0.535763
[epoch17, step347]: loss 0.586844
[epoch17, step348]: loss 0.358181
[epoch17, step349]: loss 0.503843
[epoch17, step350]: loss 0.453031
[epoch17, step351]: loss 0.691053
[epoch17, step352]: loss 0.360532
[epoch17, step353]: loss 0.333796
[epoch17, step354]: loss 0.286314
[epoch17, step355]: loss 0.279401
[epoch17, step356]: loss 0.446256
[epoch17, step357]: loss 0.503412
[epoch17, step358]: loss 0.527883
[epoch17, step359]: loss 0.610162
[epoch17, step360]: loss 0.690262
[epoch17, step361]: loss 0.438535
[epoch17, step362]: loss 0.477294
[epoch17, step363]: loss 0.652719
[epoch17, step364]: loss 0.717668
[epoch17, step365]: loss 0.500148
[epoch17, step366]: loss 0.637317
[epoch17, step367]: loss 0.594454
[epoch17, step368]: loss 0.693317
[epoch17, step369]: loss 0.393168
[epoch17, step370]: loss 0.678273
[epoch17, step371]: loss 0.549978
[epoch17, step372]: loss 0.528531
[epoch17, step373]: loss 0.686452
[epoch17, step374]: loss 0.569923
[epoch17, step375]: loss 0.507468
[epoch17, step376]: loss 0.337966
[epoch17, step377]: loss 0.329659
[epoch17, step378]: loss 0.467955
[epoch17, step379]: loss 0.100876
[epoch17, step380]: loss 0.472620
[epoch17, step381]: loss 0.453540
[epoch17, step382]: loss 0.445632
[epoch17, step383]: loss 0.296642
[epoch17, step384]: loss 0.392498
[epoch17, step385]: loss 0.376723
[epoch17, step386]: loss 0.495234
[epoch17, step387]: loss 0.389356
[epoch17, step388]: loss 0.544613
[epoch17, step389]: loss 0.300333
[epoch17, step390]: loss 0.493102
[epoch17, step391]: loss 0.466102
[epoch17, step392]: loss 0.558422
[epoch17, step393]: loss 0.549185
[epoch17, step394]: loss 0.486994
[epoch17, step395]: loss 0.620096
[epoch17, step396]: loss 0.453857
[epoch17, step397]: loss 0.385325
[epoch17, step398]: loss 0.465838
[epoch17, step399]: loss 0.428110
[epoch17, step400]: loss 0.607469
[epoch17, step401]: loss 0.524148
[epoch17, step402]: loss 0.469716
[epoch17, step403]: loss 0.606762
[epoch17, step404]: loss 0.299639
[epoch17, step405]: loss 0.526008
[epoch17, step406]: loss 0.617420
[epoch17, step407]: loss 0.111657
[epoch17, step408]: loss 0.451541
[epoch17, step409]: loss 0.628816
[epoch17, step410]: loss 0.683439
[epoch17, step411]: loss 0.509202
[epoch17, step412]: loss 0.515445
[epoch17, step413]: loss 0.159687
[epoch17, step414]: loss 0.610195
[epoch17, step415]: loss 0.657556
[epoch17, step416]: loss 0.427345
[epoch17, step417]: loss 0.449356
[epoch17, step418]: loss 0.416452
[epoch17, step419]: loss 0.688775
[epoch17, step420]: loss 0.590709
[epoch17, step421]: loss 0.505248
[epoch17, step422]: loss 0.587390
[epoch17, step423]: loss 0.333928
[epoch17, step424]: loss 0.577009
[epoch17, step425]: loss 0.526444
[epoch17, step426]: loss 0.354940
[epoch17, step427]: loss 0.585038
[epoch17, step428]: loss 0.562330
[epoch17, step429]: loss 0.604063
[epoch17, step430]: loss 0.654858
[epoch17, step431]: loss 0.656211
[epoch17, step432]: loss 0.534581
[epoch17, step433]: loss 0.312684
[epoch17, step434]: loss 0.452513
[epoch17, step435]: loss 0.376618
[epoch17, step436]: loss 0.443769
[epoch17, step437]: loss 0.396085
[epoch17, step438]: loss 0.556869
[epoch17, step439]: loss 0.545192
[epoch17, step440]: loss 0.618408
[epoch17, step441]: loss 0.548300
[epoch17, step442]: loss 0.561589
[epoch17, step443]: loss 0.601991
[epoch17, step444]: loss 0.609065
[epoch17, step445]: loss 0.365640
[epoch17, step446]: loss 0.573145
[epoch17, step447]: loss 0.551339
[epoch17, step448]: loss 0.520538
[epoch17, step449]: loss 0.572075
[epoch17, step450]: loss 0.514920
[epoch17, step451]: loss 0.333444
[epoch17, step452]: loss 0.487841
[epoch17, step453]: loss 0.535858
[epoch17, step454]: loss 0.650120
[epoch17, step455]: loss 0.349410
[epoch17, step456]: loss 0.728591
[epoch17, step457]: loss 0.563341
[epoch17, step458]: loss 0.261929
[epoch17, step459]: loss 0.383380
[epoch17, step460]: loss 0.492253
[epoch17, step461]: loss 0.655060
[epoch17, step462]: loss 0.355109
[epoch17, step463]: loss 0.569058
[epoch17, step464]: loss 0.505807
[epoch17, step465]: loss 0.483734
[epoch17, step466]: loss 0.446034
[epoch17, step467]: loss 0.500663
[epoch17, step468]: loss 0.366759
[epoch17, step469]: loss 0.439744
[epoch17, step470]: loss 0.453729
[epoch17, step471]: loss 0.547872
[epoch17, step472]: loss 0.261937
[epoch17, step473]: loss 0.508985
[epoch17, step474]: loss 0.282008
[epoch17, step475]: loss 0.405570
[epoch17, step476]: loss 0.619277
[epoch17, step477]: loss 0.310229
[epoch17, step478]: loss 0.533477
[epoch17, step479]: loss 0.582866
[epoch17, step480]: loss 0.356907
[epoch17, step481]: loss 0.334618
[epoch17, step482]: loss 0.513788
[epoch17, step483]: loss 0.524585
[epoch17, step484]: loss 0.531492
[epoch17, step485]: loss 0.357674
[epoch17, step486]: loss 0.437135
[epoch17, step487]: loss 0.497055
[epoch17, step488]: loss 0.535661
[epoch17, step489]: loss 0.451024
[epoch17, step490]: loss 0.385778
[epoch17, step491]: loss 0.327818
[epoch17, step492]: loss 0.271702
[epoch17, step493]: loss 0.712015
[epoch17, step494]: loss 0.460739
[epoch17, step495]: loss 0.576598
[epoch17, step496]: loss 0.203990
[epoch17, step497]: loss 0.607325
[epoch17, step498]: loss 0.484026
[epoch17, step499]: loss 0.480111
[epoch17, step500]: loss 0.510157
[epoch17, step501]: loss 0.488119
[epoch17, step502]: loss 0.578031
[epoch17, step503]: loss 0.367235
[epoch17, step504]: loss 0.482309
[epoch17, step505]: loss 0.677012
[epoch17, step506]: loss 0.620798
[epoch17, step507]: loss 0.585365
[epoch17, step508]: loss 0.370000
[epoch17, step509]: loss 0.417850
[epoch17, step510]: loss 0.383451
[epoch17, step511]: loss 0.591950
[epoch17, step512]: loss 0.473843
[epoch17, step513]: loss 0.619783
[epoch17, step514]: loss 0.638991
[epoch17, step515]: loss 0.638230
[epoch17, step516]: loss 0.408746
[epoch17, step517]: loss 0.449146
[epoch17, step518]: loss 0.347811
[epoch17, step519]: loss 0.289477
[epoch17, step520]: loss 0.142367
[epoch17, step521]: loss 0.427417
[epoch17, step522]: loss 0.418028
[epoch17, step523]: loss 0.487591
[epoch17, step524]: loss 0.480352
[epoch17, step525]: loss 0.561504
[epoch17, step526]: loss 0.360855
[epoch17, step527]: loss 0.525633
[epoch17, step528]: loss 0.387646
[epoch17, step529]: loss 0.478721
[epoch17, step530]: loss 0.519263
[epoch17, step531]: loss 0.340405
[epoch17, step532]: loss 0.621255
[epoch17, step533]: loss 0.367346
[epoch17, step534]: loss 0.645044
[epoch17, step535]: loss 0.434716
[epoch17, step536]: loss 0.597046
[epoch17, step537]: loss 0.647372
[epoch17, step538]: loss 0.368079
[epoch17, step539]: loss 0.466116
[epoch17, step540]: loss 0.554331
[epoch17, step541]: loss 0.435402
[epoch17, step542]: loss 0.238463
[epoch17, step543]: loss 0.591415
[epoch17, step544]: loss 0.568911
[epoch17, step545]: loss 0.651848
[epoch17, step546]: loss 0.324706
[epoch17, step547]: loss 0.428347
[epoch17, step548]: loss 0.514047
[epoch17, step549]: loss 0.483019
[epoch17, step550]: loss 0.375530
[epoch17, step551]: loss 0.556944
[epoch17, step552]: loss 0.601279
[epoch17, step553]: loss 0.470406
[epoch17, step554]: loss 0.377056
[epoch17, step555]: loss 0.667186
[epoch17, step556]: loss 0.488469
[epoch17, step557]: loss 0.581162
[epoch17, step558]: loss 0.656177
[epoch17, step559]: loss 0.471755
[epoch17, step560]: loss 0.440044
[epoch17, step561]: loss 0.527415
[epoch17, step562]: loss 0.351239
[epoch17, step563]: loss 0.488383
[epoch17, step564]: loss 0.538935
[epoch17, step565]: loss 0.520802
[epoch17, step566]: loss 0.664220
[epoch17, step567]: loss 0.597533
[epoch17, step568]: loss 0.477834
[epoch17, step569]: loss 0.526152
[epoch17, step570]: loss 0.493301
[epoch17, step571]: loss 0.558755
[epoch17, step572]: loss 0.524203
[epoch17, step573]: loss 0.388220
[epoch17, step574]: loss 0.575341
[epoch17, step575]: loss 0.351255
[epoch17, step576]: loss 0.470892
[epoch17, step577]: loss 0.266443
[epoch17, step578]: loss 0.409773
[epoch17, step579]: loss 0.511449
[epoch17, step580]: loss 0.537414
[epoch17, step581]: loss 0.491731
[epoch17, step582]: loss 0.662208
[epoch17, step583]: loss 0.478739
[epoch17, step584]: loss 0.243729
[epoch17, step585]: loss 0.308286
[epoch17, step586]: loss 0.422997
[epoch17, step587]: loss 0.454802
[epoch17, step588]: loss 0.481239
[epoch17, step589]: loss 0.518779
[epoch17, step590]: loss 0.437052
[epoch17, step591]: loss 0.595416
[epoch17, step592]: loss 0.521696
[epoch17, step593]: loss 0.512341
[epoch17, step594]: loss 0.364021
[epoch17, step595]: loss 0.493767
[epoch17, step596]: loss 0.394011
[epoch17, step597]: loss 0.496543
[epoch17, step598]: loss 0.474070
[epoch17, step599]: loss 0.353971
[epoch17, step600]: loss 0.284758
[epoch17, step601]: loss 0.270602
[epoch17, step602]: loss 0.468880
[epoch17, step603]: loss 0.312659
[epoch17, step604]: loss 0.270652
[epoch17, step605]: loss 0.547101
[epoch17, step606]: loss 0.530991
[epoch17, step607]: loss 0.380540
[epoch17, step608]: loss 0.657635
[epoch17, step609]: loss 0.630314
[epoch17, step610]: loss 0.508852
[epoch17, step611]: loss 0.483004
[epoch17, step612]: loss 0.449630
[epoch17, step613]: loss 0.280709
[epoch17, step614]: loss 0.464310
[epoch17, step615]: loss 0.249004
[epoch17, step616]: loss 0.544051
[epoch17, step617]: loss 0.113740
[epoch17, step618]: loss 0.617083
[epoch17, step619]: loss 0.450264
[epoch17, step620]: loss 0.380254
[epoch17, step621]: loss 0.468287
[epoch17, step622]: loss 0.319625
[epoch17, step623]: loss 0.386689
[epoch17, step624]: loss 0.435117
[epoch17, step625]: loss 0.591347
[epoch17, step626]: loss 0.568630
[epoch17, step627]: loss 0.510285
[epoch17, step628]: loss 0.459226
[epoch17, step629]: loss 0.508493
[epoch17, step630]: loss 0.595837
[epoch17, step631]: loss 0.565280
[epoch17, step632]: loss 0.455963
[epoch17, step633]: loss 0.403226
[epoch17, step634]: loss 0.362966
[epoch17, step635]: loss 0.635077
[epoch17, step636]: loss 0.630190
[epoch17, step637]: loss 0.422588
[epoch17, step638]: loss 0.269662
[epoch17, step639]: loss 0.531873
[epoch17, step640]: loss 0.390417
[epoch17, step641]: loss 0.292215
[epoch17, step642]: loss 0.516072
[epoch17, step643]: loss 0.501811
[epoch17, step644]: loss 0.342649
[epoch17, step645]: loss 0.548371
[epoch17, step646]: loss 0.557368
[epoch17, step647]: loss 0.507122
[epoch17, step648]: loss 0.627020
[epoch17, step649]: loss 0.489281
[epoch17, step650]: loss 0.754393
[epoch17, step651]: loss 0.331602
[epoch17, step652]: loss 0.350063
[epoch17, step653]: loss 0.351556
[epoch17, step654]: loss 0.676687
[epoch17, step655]: loss 0.356951
[epoch17, step656]: loss 0.648134
[epoch17, step657]: loss 0.253385
[epoch17, step658]: loss 0.435750
[epoch17, step659]: loss 0.415043
[epoch17, step660]: loss 0.449298
[epoch17, step661]: loss 0.462418
[epoch17, step662]: loss 0.406187
[epoch17, step663]: loss 0.403112
[epoch17, step664]: loss 0.590898
[epoch17, step665]: loss 0.363856
[epoch17, step666]: loss 0.442062
[epoch17, step667]: loss 0.677229
[epoch17, step668]: loss 0.382625
[epoch17, step669]: loss 0.544740
[epoch17, step670]: loss 0.404291
[epoch17, step671]: loss 0.512845
[epoch17, step672]: loss 0.733614
[epoch17, step673]: loss 0.267074
[epoch17, step674]: loss 0.305463
[epoch17, step675]: loss 0.291761
[epoch17, step676]: loss 0.612629
[epoch17, step677]: loss 0.425164
[epoch17, step678]: loss 0.363764
[epoch17, step679]: loss 0.365635
[epoch17, step680]: loss 0.402518
[epoch17, step681]: loss 0.555615
[epoch17, step682]: loss 0.510961
[epoch17, step683]: loss 0.635541
[epoch17, step684]: loss 0.418398
[epoch17, step685]: loss 0.600628
[epoch17, step686]: loss 0.512024
[epoch17, step687]: loss 0.427479
[epoch17, step688]: loss 0.378858
[epoch17, step689]: loss 0.177561
[epoch17, step690]: loss 0.517443
[epoch17, step691]: loss 0.332293
[epoch17, step692]: loss 0.526545
[epoch17, step693]: loss 0.501047
[epoch17, step694]: loss 0.398228
[epoch17, step695]: loss 0.378265
[epoch17, step696]: loss 0.483485
[epoch17, step697]: loss 0.300379
[epoch17, step698]: loss 0.503428
[epoch17, step699]: loss 0.585192
[epoch17, step700]: loss 0.514321
[epoch17, step701]: loss 0.529370
[epoch17, step702]: loss 0.381155
[epoch17, step703]: loss 0.334918
[epoch17, step704]: loss 0.450585
[epoch17, step705]: loss 0.463079
[epoch17, step706]: loss 0.546826
[epoch17, step707]: loss 0.455887
[epoch17, step708]: loss 0.469412
[epoch17, step709]: loss 0.586393
[epoch17, step710]: loss 0.503351
[epoch17, step711]: loss 0.415235
[epoch17, step712]: loss 0.645917
[epoch17, step713]: loss 0.351925
[epoch17, step714]: loss 0.672287
[epoch17, step715]: loss 0.458283
[epoch17, step716]: loss 0.537333
[epoch17, step717]: loss 0.334419
[epoch17, step718]: loss 0.468511
[epoch17, step719]: loss 0.552609
[epoch17, step720]: loss 0.466866
[epoch17, step721]: loss 0.479564
[epoch17, step722]: loss 0.504634
[epoch17, step723]: loss 0.496862
[epoch17, step724]: loss 0.623638
[epoch17, step725]: loss 0.519787
[epoch17, step726]: loss 0.487933
[epoch17, step727]: loss 0.598340
[epoch17, step728]: loss 0.480977
[epoch17, step729]: loss 0.398890
[epoch17, step730]: loss 0.556339
[epoch17, step731]: loss 0.630581
[epoch17, step732]: loss 0.512304
[epoch17, step733]: loss 0.343692
[epoch17, step734]: loss 0.553782
[epoch17, step735]: loss 0.558732
[epoch17, step736]: loss 0.513179
[epoch17, step737]: loss 0.578611
[epoch17, step738]: loss 0.345557
[epoch17, step739]: loss 0.561695
[epoch17, step740]: loss 0.739194
[epoch17, step741]: loss 0.516042
[epoch17, step742]: loss 0.657158
[epoch17, step743]: loss 0.586952
[epoch17, step744]: loss 0.574971
[epoch17, step745]: loss 0.550480
[epoch17, step746]: loss 0.449569
[epoch17, step747]: loss 0.524767
[epoch17, step748]: loss 0.508195
[epoch17, step749]: loss 0.673244
[epoch17, step750]: loss 0.436008
[epoch17, step751]: loss 0.093179
[epoch17, step752]: loss 0.222267
[epoch17, step753]: loss 0.316759
[epoch17, step754]: loss 0.680546
[epoch17, step755]: loss 0.488830
[epoch17, step756]: loss 0.463397
[epoch17, step757]: loss 0.386702
[epoch17, step758]: loss 0.473516
[epoch17, step759]: loss 0.555398
[epoch17, step760]: loss 0.332368
[epoch17, step761]: loss 0.634612
[epoch17, step762]: loss 0.694893
[epoch17, step763]: loss 0.589644
[epoch17, step764]: loss 0.537347
[epoch17, step765]: loss 0.401505
[epoch17, step766]: loss 0.556266
[epoch17, step767]: loss 0.549774
[epoch17, step768]: loss 0.621812
[epoch17, step769]: loss 0.414771
[epoch17, step770]: loss 0.496177
[epoch17, step771]: loss 0.426519
[epoch17, step772]: loss 0.423118
[epoch17, step773]: loss 0.538368
[epoch17, step774]: loss 0.556799
[epoch17, step775]: loss 0.399955
[epoch17, step776]: loss 0.356011
[epoch17, step777]: loss 0.437618
[epoch17, step778]: loss 0.245052
[epoch17, step779]: loss 0.633510
[epoch17, step780]: loss 0.329554
[epoch17, step781]: loss 0.360466
[epoch17, step782]: loss 0.655560
[epoch17, step783]: loss 0.683213
[epoch17, step784]: loss 0.367363
[epoch17, step785]: loss 0.368249
[epoch17, step786]: loss 0.351445
[epoch17, step787]: loss 0.220417
[epoch17, step788]: loss 0.328220
[epoch17, step789]: loss 0.468384
[epoch17, step790]: loss 0.422770
[epoch17, step791]: loss 0.354560
[epoch17, step792]: loss 0.537130
[epoch17, step793]: loss 0.346630
[epoch17, step794]: loss 0.735335
[epoch17, step795]: loss 0.375095
[epoch17, step796]: loss 0.486108
[epoch17, step797]: loss 0.505862
[epoch17, step798]: loss 0.364157
[epoch17, step799]: loss 0.584665
[epoch17, step800]: loss 0.651007
[epoch17, step801]: loss 0.693629
[epoch17, step802]: loss 0.409619
[epoch17, step803]: loss 0.460912
[epoch17, step804]: loss 0.638320
[epoch17, step805]: loss 0.254404
[epoch17, step806]: loss 0.647159
[epoch17, step807]: loss 0.309667
[epoch17, step808]: loss 0.229126
[epoch17, step809]: loss 0.314639
[epoch17, step810]: loss 0.442701
[epoch17, step811]: loss 0.502573
[epoch17, step812]: loss 0.516629
[epoch17, step813]: loss 0.506462
[epoch17, step814]: loss 0.517120
[epoch17, step815]: loss 0.459668
[epoch17, step816]: loss 0.415466
[epoch17, step817]: loss 0.640248
[epoch17, step818]: loss 0.715159
[epoch17, step819]: loss 0.370888
[epoch17, step820]: loss 0.465477
[epoch17, step821]: loss 0.567228
[epoch17, step822]: loss 0.520412
[epoch17, step823]: loss 0.265784
[epoch17, step824]: loss 0.674438
[epoch17, step825]: loss 0.266170
[epoch17, step826]: loss 0.764132
[epoch17, step827]: loss 0.333950
[epoch17, step828]: loss 0.404504
[epoch17, step829]: loss 0.457283
[epoch17, step830]: loss 0.602689
[epoch17, step831]: loss 0.530990
[epoch17, step832]: loss 0.653464
[epoch17, step833]: loss 0.367829
[epoch17, step834]: loss 0.598183
[epoch17, step835]: loss 0.445176
[epoch17, step836]: loss 0.269528
[epoch17, step837]: loss 0.481586
[epoch17, step838]: loss 0.559364
[epoch17, step839]: loss 0.462982
[epoch17, step840]: loss 0.492129
[epoch17, step841]: loss 0.524289
[epoch17, step842]: loss 0.443447
[epoch17, step843]: loss 0.634950
[epoch17, step844]: loss 0.431890
[epoch17, step845]: loss 0.498432
[epoch17, step846]: loss 0.398054
[epoch17, step847]: loss 0.434565
[epoch17, step848]: loss 0.418338
[epoch17, step849]: loss 0.237348
[epoch17, step850]: loss 0.496636
[epoch17, step851]: loss 0.365001
[epoch17, step852]: loss 0.492814
[epoch17, step853]: loss 0.548285
[epoch17, step854]: loss 0.438983
[epoch17, step855]: loss 0.501753
[epoch17, step856]: loss 0.434377
[epoch17, step857]: loss 0.637028
[epoch17, step858]: loss 0.434565
[epoch17, step859]: loss 0.700277
[epoch17, step860]: loss 0.550739
[epoch17, step861]: loss 0.490816
[epoch17, step862]: loss 0.555742
[epoch17, step863]: loss 0.445707
[epoch17, step864]: loss 0.528909
[epoch17, step865]: loss 0.434011
[epoch17, step866]: loss 0.412376
[epoch17, step867]: loss 0.588567
[epoch17, step868]: loss 0.560578
[epoch17, step869]: loss 0.456510
[epoch17, step870]: loss 0.553079
[epoch17, step871]: loss 0.380318
[epoch17, step872]: loss 0.479024
[epoch17, step873]: loss 0.382815
[epoch17, step874]: loss 0.368665
[epoch17, step875]: loss 0.458864
[epoch17, step876]: loss 0.649396
[epoch17, step877]: loss 0.566219
[epoch17, step878]: loss 0.507241
[epoch17, step879]: loss 0.242210
[epoch17, step880]: loss 0.687259
[epoch17, step881]: loss 0.628844
[epoch17, step882]: loss 0.646332
[epoch17, step883]: loss 0.651052
[epoch17, step884]: loss 0.643432
[epoch17, step885]: loss 0.399021
[epoch17, step886]: loss 0.486386
[epoch17, step887]: loss 0.491942
[epoch17, step888]: loss 0.515801
[epoch17, step889]: loss 0.656187
[epoch17, step890]: loss 0.397674
[epoch17, step891]: loss 0.446830
[epoch17, step892]: loss 0.464286
[epoch17, step893]: loss 0.406365
[epoch17, step894]: loss 0.431707
[epoch17, step895]: loss 0.475848
[epoch17, step896]: loss 0.463011
[epoch17, step897]: loss 0.517833
[epoch17, step898]: loss 0.511211
[epoch17, step899]: loss 0.578270
[epoch17, step900]: loss 0.459727
[epoch17, step901]: loss 0.320142
[epoch17, step902]: loss 0.344112
[epoch17, step903]: loss 0.587439
[epoch17, step904]: loss 0.342465
[epoch17, step905]: loss 0.234112
[epoch17, step906]: loss 0.549891
[epoch17, step907]: loss 0.394495
[epoch17, step908]: loss 0.420221
[epoch17, step909]: loss 0.583896
[epoch17, step910]: loss 0.212780
[epoch17, step911]: loss 0.392858
[epoch17, step912]: loss 0.271619
[epoch17, step913]: loss 0.657227
[epoch17, step914]: loss 0.492596
[epoch17, step915]: loss 0.529173
[epoch17, step916]: loss 0.575437
[epoch17, step917]: loss 0.415762
[epoch17, step918]: loss 0.391357
[epoch17, step919]: loss 0.278079
[epoch17, step920]: loss 0.576906
[epoch17, step921]: loss 0.256595
[epoch17, step922]: loss 0.573626
[epoch17, step923]: loss 0.619404
[epoch17, step924]: loss 0.410311
[epoch17, step925]: loss 0.656722
[epoch17, step926]: loss 0.671610
[epoch17, step927]: loss 0.485842
[epoch17, step928]: loss 0.323231
[epoch17, step929]: loss 0.713357
[epoch17, step930]: loss 0.383412
[epoch17, step931]: loss 0.555972
[epoch17, step932]: loss 0.266761
[epoch17, step933]: loss 0.516094
[epoch17, step934]: loss 0.320397
[epoch17, step935]: loss 0.539761
[epoch17, step936]: loss 0.513513
[epoch17, step937]: loss 0.582308
[epoch17, step938]: loss 0.401441
[epoch17, step939]: loss 0.319652
[epoch17, step940]: loss 0.274296
[epoch17, step941]: loss 0.481328
[epoch17, step942]: loss 0.432723
[epoch17, step943]: loss 0.427770
[epoch17, step944]: loss 0.617758
[epoch17, step945]: loss 0.270689
[epoch17, step946]: loss 0.415904
[epoch17, step947]: loss 0.139754
[epoch17, step948]: loss 0.663485
[epoch17, step949]: loss 0.598851
[epoch17, step950]: loss 0.500703
[epoch17, step951]: loss 0.584482
[epoch17, step952]: loss 0.700973
[epoch17, step953]: loss 0.509529
[epoch17, step954]: loss 0.543914
[epoch17, step955]: loss 0.542471
[epoch17, step956]: loss 0.568995
[epoch17, step957]: loss 0.483742
[epoch17, step958]: loss 0.636306
[epoch17, step959]: loss 0.325439
[epoch17, step960]: loss 0.267373
[epoch17, step961]: loss 0.549239
[epoch17, step962]: loss 0.584552
[epoch17, step963]: loss 0.586597
[epoch17, step964]: loss 0.354939
[epoch17, step965]: loss 0.229106
[epoch17, step966]: loss 0.520333
[epoch17, step967]: loss 0.397177
[epoch17, step968]: loss 0.505375
[epoch17, step969]: loss 0.538779
[epoch17, step970]: loss 0.508603
[epoch17, step971]: loss 0.499885
[epoch17, step972]: loss 0.512220
[epoch17, step973]: loss 0.519219
[epoch17, step974]: loss 0.503719
[epoch17, step975]: loss 0.609648
[epoch17, step976]: loss 0.269002
[epoch17, step977]: loss 0.575778
[epoch17, step978]: loss 0.446310
[epoch17, step979]: loss 0.640025
[epoch17, step980]: loss 0.664962
[epoch17, step981]: loss 0.733649
[epoch17, step982]: loss 0.461484
[epoch17, step983]: loss 0.625564
[epoch17, step984]: loss 0.527926
[epoch17, step985]: loss 0.400547
[epoch17, step986]: loss 0.426450
[epoch17, step987]: loss 0.294507
[epoch17, step988]: loss 0.535287
[epoch17, step989]: loss 0.379126
[epoch17, step990]: loss 0.565756
[epoch17, step991]: loss 0.487780
[epoch17, step992]: loss 0.244907
[epoch17, step993]: loss 0.577627
[epoch17, step994]: loss 0.235454
[epoch17, step995]: loss 0.666192
[epoch17, step996]: loss 0.429671
[epoch17, step997]: loss 0.398371
[epoch17, step998]: loss 0.504980
[epoch17, step999]: loss 0.492422
[epoch17, step1000]: loss 0.272579
[epoch17, step1001]: loss 0.455889
[epoch17, step1002]: loss 0.403416
[epoch17, step1003]: loss 0.383062
[epoch17, step1004]: loss 0.649783
[epoch17, step1005]: loss 0.575136
[epoch17, step1006]: loss 0.350628
[epoch17, step1007]: loss 0.515464
[epoch17, step1008]: loss 0.655369
[epoch17, step1009]: loss 0.330414
[epoch17, step1010]: loss 0.431934
[epoch17, step1011]: loss 0.180156
[epoch17, step1012]: loss 0.623148
[epoch17, step1013]: loss 0.525285
[epoch17, step1014]: loss 0.515995
[epoch17, step1015]: loss 0.534637
[epoch17, step1016]: loss 0.659086
[epoch17, step1017]: loss 0.320571
[epoch17, step1018]: loss 0.563889
[epoch17, step1019]: loss 0.615696
[epoch17, step1020]: loss 0.511106
[epoch17, step1021]: loss 0.498028
[epoch17, step1022]: loss 0.309336
[epoch17, step1023]: loss 0.310295
[epoch17, step1024]: loss 0.579621
[epoch17, step1025]: loss 0.442671
[epoch17, step1026]: loss 0.358687
[epoch17, step1027]: loss 0.258478
[epoch17, step1028]: loss 0.391582
[epoch17, step1029]: loss 0.562441
[epoch17, step1030]: loss 0.524184
[epoch17, step1031]: loss 0.610076
[epoch17, step1032]: loss 0.439249
[epoch17, step1033]: loss 0.762410
[epoch17, step1034]: loss 0.453175
[epoch17, step1035]: loss 0.334867
[epoch17, step1036]: loss 0.263346
[epoch17, step1037]: loss 0.638419
[epoch17, step1038]: loss 0.437547
[epoch17, step1039]: loss 0.332954
[epoch17, step1040]: loss 0.701248
[epoch17, step1041]: loss 0.539582
[epoch17, step1042]: loss 0.490532
[epoch17, step1043]: loss 0.328987
[epoch17, step1044]: loss 0.335026
[epoch17, step1045]: loss 0.513008
[epoch17, step1046]: loss 0.404080
[epoch17, step1047]: loss 0.529959
[epoch17, step1048]: loss 0.444198
[epoch17, step1049]: loss 0.650839
[epoch17, step1050]: loss 0.506031
[epoch17, step1051]: loss 0.577329
[epoch17, step1052]: loss 0.462098
[epoch17, step1053]: loss 0.250980
[epoch17, step1054]: loss 0.673943
[epoch17, step1055]: loss 0.487699
[epoch17, step1056]: loss 0.384011
[epoch17, step1057]: loss 0.551442
[epoch17, step1058]: loss 0.757609
[epoch17, step1059]: loss 0.512622
[epoch17, step1060]: loss 0.588017
[epoch17, step1061]: loss 0.525143
[epoch17, step1062]: loss 0.610556
[epoch17, step1063]: loss 0.292485
[epoch17, step1064]: loss 0.151722
[epoch17, step1065]: loss 0.383020
[epoch17, step1066]: loss 0.527568
[epoch17, step1067]: loss 0.408781
[epoch17, step1068]: loss 0.509429
[epoch17, step1069]: loss 0.418483
[epoch17, step1070]: loss 0.228341
[epoch17, step1071]: loss 0.306476
[epoch17, step1072]: loss 0.368408
[epoch17, step1073]: loss 0.514847
[epoch17, step1074]: loss 0.530025
[epoch17, step1075]: loss 0.486652
[epoch17, step1076]: loss 0.477409
[epoch17, step1077]: loss 0.261875
[epoch17, step1078]: loss 0.487047
[epoch17, step1079]: loss 0.557721
[epoch17, step1080]: loss 0.177950
[epoch17, step1081]: loss 0.250396
[epoch17, step1082]: loss 0.306174
[epoch17, step1083]: loss 0.470546
[epoch17, step1084]: loss 0.354402
[epoch17, step1085]: loss 0.404494
[epoch17, step1086]: loss 0.645215
[epoch17, step1087]: loss 0.434278
[epoch17, step1088]: loss 0.623193
[epoch17, step1089]: loss 0.643488
[epoch17, step1090]: loss 0.470880
[epoch17, step1091]: loss 0.458984
[epoch17, step1092]: loss 0.514851
[epoch17, step1093]: loss 0.231896
[epoch17, step1094]: loss 0.311352
[epoch17, step1095]: loss 0.573336
[epoch17, step1096]: loss 0.474128
[epoch17, step1097]: loss 0.661727
[epoch17, step1098]: loss 0.630578
[epoch17, step1099]: loss 0.487311
[epoch17, step1100]: loss 0.623719
[epoch17, step1101]: loss 0.474383
[epoch17, step1102]: loss 0.504355
[epoch17, step1103]: loss 0.469451
[epoch17, step1104]: loss 0.658632
[epoch17, step1105]: loss 0.495614
[epoch17, step1106]: loss 0.270550
[epoch17, step1107]: loss 0.539973
[epoch17, step1108]: loss 0.381708
[epoch17, step1109]: loss 0.550844
[epoch17, step1110]: loss 0.354536
[epoch17, step1111]: loss 0.649870
[epoch17, step1112]: loss 0.251355
[epoch17, step1113]: loss 0.630148
[epoch17, step1114]: loss 0.340673
[epoch17, step1115]: loss 0.336156
[epoch17, step1116]: loss 0.252624
[epoch17, step1117]: loss 0.567769
[epoch17, step1118]: loss 0.669308
[epoch17, step1119]: loss 0.330239
[epoch17, step1120]: loss 0.542464
[epoch17, step1121]: loss 0.552424
[epoch17, step1122]: loss 0.374057
[epoch17, step1123]: loss 0.487607
[epoch17, step1124]: loss 0.302287
[epoch17, step1125]: loss 0.528901
[epoch17, step1126]: loss 0.519128
[epoch17, step1127]: loss 0.305060
[epoch17, step1128]: loss 0.316383
[epoch17, step1129]: loss 0.439844
[epoch17, step1130]: loss 0.354333
[epoch17, step1131]: loss 0.618680
[epoch17, step1132]: loss 0.585095
[epoch17, step1133]: loss 0.254316
[epoch17, step1134]: loss 0.849037
[epoch17, step1135]: loss 0.477142
[epoch17, step1136]: loss 0.523791
[epoch17, step1137]: loss 0.340974
[epoch17, step1138]: loss 0.540762
[epoch17, step1139]: loss 0.450083
[epoch17, step1140]: loss 0.315769
[epoch17, step1141]: loss 0.246574
[epoch17, step1142]: loss 0.605274
[epoch17, step1143]: loss 0.550795
[epoch17, step1144]: loss 0.506532
[epoch17, step1145]: loss 0.399737
[epoch17, step1146]: loss 0.329397
[epoch17, step1147]: loss 0.345412
[epoch17, step1148]: loss 0.328599
[epoch17, step1149]: loss 0.376294
[epoch17, step1150]: loss 0.312918
[epoch17, step1151]: loss 0.518148
[epoch17, step1152]: loss 0.633678
[epoch17, step1153]: loss 0.492251
[epoch17, step1154]: loss 0.535868
[epoch17, step1155]: loss 0.494415
[epoch17, step1156]: loss 0.254017
[epoch17, step1157]: loss 0.577069
[epoch17, step1158]: loss 0.466621
[epoch17, step1159]: loss 0.512860
[epoch17, step1160]: loss 0.595918
[epoch17, step1161]: loss 0.568018
[epoch17, step1162]: loss 0.593982
[epoch17, step1163]: loss 0.332058
[epoch17, step1164]: loss 0.675544
[epoch17, step1165]: loss 0.300745
[epoch17, step1166]: loss 0.367747
[epoch17, step1167]: loss 0.456537
[epoch17, step1168]: loss 0.198179
[epoch17, step1169]: loss 0.466873
[epoch17, step1170]: loss 0.368724
[epoch17, step1171]: loss 0.504268
[epoch17, step1172]: loss 0.742472
[epoch17, step1173]: loss 0.334638
[epoch17, step1174]: loss 0.452404
[epoch17, step1175]: loss 0.834547
[epoch17, step1176]: loss 0.431968
[epoch17, step1177]: loss 0.418016
[epoch17, step1178]: loss 0.320103
[epoch17, step1179]: loss 0.522912
[epoch17, step1180]: loss 0.435973
[epoch17, step1181]: loss 0.480158
[epoch17, step1182]: loss 0.333445
[epoch17, step1183]: loss 0.574111
[epoch17, step1184]: loss 0.481087
[epoch17, step1185]: loss 0.365255
[epoch17, step1186]: loss 0.721597
[epoch17, step1187]: loss 0.419416
[epoch17, step1188]: loss 0.400987
[epoch17, step1189]: loss 0.523672
[epoch17, step1190]: loss 0.613619
[epoch17, step1191]: loss 0.594387
[epoch17, step1192]: loss 0.484236
[epoch17, step1193]: loss 0.371996
[epoch17, step1194]: loss 0.413133
[epoch17, step1195]: loss 0.330170
[epoch17, step1196]: loss 0.439995
[epoch17, step1197]: loss 0.647919
[epoch17, step1198]: loss 0.349511
[epoch17, step1199]: loss 0.371513
[epoch17, step1200]: loss 0.146409
[epoch17, step1201]: loss 0.679641
[epoch17, step1202]: loss 0.273443
[epoch17, step1203]: loss 0.612299
[epoch17, step1204]: loss 0.408834
[epoch17, step1205]: loss 0.562308
[epoch17, step1206]: loss 0.230672
[epoch17, step1207]: loss 0.377421
[epoch17, step1208]: loss 0.474735
[epoch17, step1209]: loss 0.274837
[epoch17, step1210]: loss 0.584717
[epoch17, step1211]: loss 0.382051
[epoch17, step1212]: loss 0.409778
[epoch17, step1213]: loss 0.444165
[epoch17, step1214]: loss 0.624654
[epoch17, step1215]: loss 0.464791
[epoch17, step1216]: loss 0.520181
[epoch17, step1217]: loss 0.530008
[epoch17, step1218]: loss 0.613870
[epoch17, step1219]: loss 0.565916
[epoch17, step1220]: loss 0.518908
[epoch17, step1221]: loss 0.519020
[epoch17, step1222]: loss 0.460894
[epoch17, step1223]: loss 0.574149
[epoch17, step1224]: loss 0.660837
[epoch17, step1225]: loss 0.463641
[epoch17, step1226]: loss 0.286619
[epoch17, step1227]: loss 0.280578
[epoch17, step1228]: loss 0.446765
[epoch17, step1229]: loss 0.438917
[epoch17, step1230]: loss 0.193695
[epoch17, step1231]: loss 0.403308
[epoch17, step1232]: loss 0.195492
[epoch17, step1233]: loss 0.524785
[epoch17, step1234]: loss 0.455279
[epoch17, step1235]: loss 0.709383
[epoch17, step1236]: loss 0.629910
[epoch17, step1237]: loss 0.202350
[epoch17, step1238]: loss 0.415632
[epoch17, step1239]: loss 0.619106
[epoch17, step1240]: loss 0.320834
[epoch17, step1241]: loss 0.480861
[epoch17, step1242]: loss 0.617281
[epoch17, step1243]: loss 0.504703
[epoch17, step1244]: loss 0.512802
[epoch17, step1245]: loss 0.624092
[epoch17, step1246]: loss 0.490854
[epoch17, step1247]: loss 0.722786
[epoch17, step1248]: loss 0.538844
[epoch17, step1249]: loss 0.411928
[epoch17, step1250]: loss 0.498251
[epoch17, step1251]: loss 0.513043
[epoch17, step1252]: loss 0.690242
[epoch17, step1253]: loss 0.439890
[epoch17, step1254]: loss 0.377579
[epoch17, step1255]: loss 0.521285
[epoch17, step1256]: loss 0.351851
[epoch17, step1257]: loss 0.492239
[epoch17, step1258]: loss 0.412025
[epoch17, step1259]: loss 0.537689
[epoch17, step1260]: loss 0.453105
[epoch17, step1261]: loss 0.273377
[epoch17, step1262]: loss 0.239880
[epoch17, step1263]: loss 0.473124
[epoch17, step1264]: loss 0.438697
[epoch17, step1265]: loss 0.430734
[epoch17, step1266]: loss 0.441671
[epoch17, step1267]: loss 0.577701
[epoch17, step1268]: loss 0.380919
[epoch17, step1269]: loss 0.567942
[epoch17, step1270]: loss 0.659836
[epoch17, step1271]: loss 0.434387
[epoch17, step1272]: loss 0.664076
[epoch17, step1273]: loss 0.356760
[epoch17, step1274]: loss 0.426175
[epoch17, step1275]: loss 0.407889
[epoch17, step1276]: loss 0.537164
[epoch17, step1277]: loss 0.413604
[epoch17, step1278]: loss 0.512365
[epoch17, step1279]: loss 0.250594
[epoch17, step1280]: loss 0.329005
[epoch17, step1281]: loss 0.643114
[epoch17, step1282]: loss 0.244912
[epoch17, step1283]: loss 0.525712
[epoch17, step1284]: loss 0.452943
[epoch17, step1285]: loss 0.486775
[epoch17, step1286]: loss 0.433509
[epoch17, step1287]: loss 0.760049
[epoch17, step1288]: loss 0.331900
[epoch17, step1289]: loss 0.389974
[epoch17, step1290]: loss 0.530355
[epoch17, step1291]: loss 0.536298
[epoch17, step1292]: loss 0.429864
[epoch17, step1293]: loss 0.534319
[epoch17, step1294]: loss 0.348179
[epoch17, step1295]: loss 0.247686
[epoch17, step1296]: loss 0.396265
[epoch17, step1297]: loss 0.595447
[epoch17, step1298]: loss 0.343810
[epoch17, step1299]: loss 0.224595
[epoch17, step1300]: loss 0.511009
[epoch17, step1301]: loss 0.589403
[epoch17, step1302]: loss 0.401946
[epoch17, step1303]: loss 0.545503
[epoch17, step1304]: loss 0.556886
[epoch17, step1305]: loss 0.336721
[epoch17, step1306]: loss 0.579628
[epoch17, step1307]: loss 0.570404
[epoch17, step1308]: loss 0.456143
[epoch17, step1309]: loss 0.576361
[epoch17, step1310]: loss 0.459494
[epoch17, step1311]: loss 0.521060
[epoch17, step1312]: loss 0.264639
[epoch17, step1313]: loss 0.540020
[epoch17, step1314]: loss 0.214154
[epoch17, step1315]: loss 0.635722
[epoch17, step1316]: loss 0.333504
[epoch17, step1317]: loss 0.656188
[epoch17, step1318]: loss 0.327763
[epoch17, step1319]: loss 0.660148
[epoch17, step1320]: loss 0.449539
[epoch17, step1321]: loss 0.518275
[epoch17, step1322]: loss 0.475986
[epoch17, step1323]: loss 0.554292
[epoch17, step1324]: loss 0.371772
[epoch17, step1325]: loss 0.603379
[epoch17, step1326]: loss 0.460121
[epoch17, step1327]: loss 0.393586
[epoch17, step1328]: loss 0.398642
[epoch17, step1329]: loss 0.685284
[epoch17, step1330]: loss 0.302514
[epoch17, step1331]: loss 0.436332
[epoch17, step1332]: loss 0.337567
[epoch17, step1333]: loss 0.558173
[epoch17, step1334]: loss 0.662921
[epoch17, step1335]: loss 0.292086
[epoch17, step1336]: loss 0.419609
[epoch17, step1337]: loss 0.706809
[epoch17, step1338]: loss 0.617636
[epoch17, step1339]: loss 0.398149
[epoch17, step1340]: loss 0.700784
[epoch17, step1341]: loss 0.426882
[epoch17, step1342]: loss 0.587611
[epoch17, step1343]: loss 0.358900
[epoch17, step1344]: loss 0.524371
[epoch17, step1345]: loss 0.620514
[epoch17, step1346]: loss 0.337858
[epoch17, step1347]: loss 0.434887
[epoch17, step1348]: loss 0.510042
[epoch17, step1349]: loss 0.491229
[epoch17, step1350]: loss 0.647453
[epoch17, step1351]: loss 0.246286
[epoch17, step1352]: loss 0.660385
[epoch17, step1353]: loss 0.732139
[epoch17, step1354]: loss 0.532085
[epoch17, step1355]: loss 0.428796
[epoch17, step1356]: loss 0.482765
[epoch17, step1357]: loss 0.537430
[epoch17, step1358]: loss 0.530994
[epoch17, step1359]: loss 0.597721
[epoch17, step1360]: loss 0.535884
[epoch17, step1361]: loss 0.536298
[epoch17, step1362]: loss 0.518223
[epoch17, step1363]: loss 0.409424
[epoch17, step1364]: loss 0.352108
[epoch17, step1365]: loss 0.634713
[epoch17, step1366]: loss 0.446517
[epoch17, step1367]: loss 0.404632
[epoch17, step1368]: loss 0.467021
[epoch17, step1369]: loss 0.341822
[epoch17, step1370]: loss 0.488798
[epoch17, step1371]: loss 0.434446
[epoch17, step1372]: loss 0.639157
[epoch17, step1373]: loss 0.551463
[epoch17, step1374]: loss 0.728148
[epoch17, step1375]: loss 0.350712
[epoch17, step1376]: loss 0.662129
[epoch17, step1377]: loss 0.435945
[epoch17, step1378]: loss 0.536687
[epoch17, step1379]: loss 0.384405
[epoch17, step1380]: loss 0.529125
[epoch17, step1381]: loss 0.369388
[epoch17, step1382]: loss 0.233141
[epoch17, step1383]: loss 0.399771
[epoch17, step1384]: loss 0.550365
[epoch17, step1385]: loss 0.356497
[epoch17, step1386]: loss 0.469570
[epoch17, step1387]: loss 0.464122
[epoch17, step1388]: loss 0.673692
[epoch17, step1389]: loss 0.460711
[epoch17, step1390]: loss 0.500194
[epoch17, step1391]: loss 0.554324
[epoch17, step1392]: loss 0.478832
[epoch17, step1393]: loss 0.583211
[epoch17, step1394]: loss 0.446142
[epoch17, step1395]: loss 0.425242
[epoch17, step1396]: loss 0.540567
[epoch17, step1397]: loss 0.513142
[epoch17, step1398]: loss 0.459089
[epoch17, step1399]: loss 0.270322
[epoch17, step1400]: loss 0.485410
[epoch17, step1401]: loss 0.557075
[epoch17, step1402]: loss 0.493146
[epoch17, step1403]: loss 0.481410
[epoch17, step1404]: loss 0.520199
[epoch17, step1405]: loss 0.449256
[epoch17, step1406]: loss 0.309576
[epoch17, step1407]: loss 0.455731
[epoch17, step1408]: loss 0.285791
[epoch17, step1409]: loss 0.581623
[epoch17, step1410]: loss 0.370941
[epoch17, step1411]: loss 0.437561
[epoch17, step1412]: loss 0.550794
[epoch17, step1413]: loss 0.521746
[epoch17, step1414]: loss 0.389959
[epoch17, step1415]: loss 0.304945
[epoch17, step1416]: loss 0.777462
[epoch17, step1417]: loss 0.425853
[epoch17, step1418]: loss 0.673194
[epoch17, step1419]: loss 0.631982
[epoch17, step1420]: loss 0.572612
[epoch17, step1421]: loss 0.527057
[epoch17, step1422]: loss 0.512046
[epoch17, step1423]: loss 0.415988
[epoch17, step1424]: loss 0.265710
[epoch17, step1425]: loss 0.377220
[epoch17, step1426]: loss 0.436039
[epoch17, step1427]: loss 0.344228
[epoch17, step1428]: loss 0.488218
[epoch17, step1429]: loss 0.432406
[epoch17, step1430]: loss 0.380322
[epoch17, step1431]: loss 0.150008
[epoch17, step1432]: loss 0.467608
[epoch17, step1433]: loss 0.397453
[epoch17, step1434]: loss 0.553640
[epoch17, step1435]: loss 0.392397
[epoch17, step1436]: loss 0.369404
[epoch17, step1437]: loss 0.384576
[epoch17, step1438]: loss 0.371163
[epoch17, step1439]: loss 0.282228
[epoch17, step1440]: loss 0.345562
[epoch17, step1441]: loss 0.176115
[epoch17, step1442]: loss 0.541024
[epoch17, step1443]: loss 0.540598
[epoch17, step1444]: loss 0.385865
[epoch17, step1445]: loss 0.141826
[epoch17, step1446]: loss 0.549502
[epoch17, step1447]: loss 0.615761
[epoch17, step1448]: loss 0.540266
[epoch17, step1449]: loss 0.536376
[epoch17, step1450]: loss 0.754287
[epoch17, step1451]: loss 0.546638
[epoch17, step1452]: loss 0.433063
[epoch17, step1453]: loss 0.432623
[epoch17, step1454]: loss 0.471021
[epoch17, step1455]: loss 0.617291
[epoch17, step1456]: loss 0.444071
[epoch17, step1457]: loss 0.366335
[epoch17, step1458]: loss 0.577156
[epoch17, step1459]: loss 0.463868
[epoch17, step1460]: loss 0.488249
[epoch17, step1461]: loss 0.247768
[epoch17, step1462]: loss 0.569637
[epoch17, step1463]: loss 0.544857
[epoch17, step1464]: loss 0.344319
[epoch17, step1465]: loss 0.501362
[epoch17, step1466]: loss 0.451382
[epoch17, step1467]: loss 0.579761
[epoch17, step1468]: loss 0.555517
[epoch17, step1469]: loss 0.718666
[epoch17, step1470]: loss 0.375934
[epoch17, step1471]: loss 0.475378
[epoch17, step1472]: loss 0.607142
[epoch17, step1473]: loss 0.401942
[epoch17, step1474]: loss 0.491941
[epoch17, step1475]: loss 0.324909
[epoch17, step1476]: loss 0.551029
[epoch17, step1477]: loss 0.533226
[epoch17, step1478]: loss 0.659391
[epoch17, step1479]: loss 0.586060
[epoch17, step1480]: loss 0.385871
[epoch17, step1481]: loss 0.412232
[epoch17, step1482]: loss 0.425101
[epoch17, step1483]: loss 0.434350
[epoch17, step1484]: loss 0.619913
[epoch17, step1485]: loss 0.459361
[epoch17, step1486]: loss 0.468979
[epoch17, step1487]: loss 0.641504
[epoch17, step1488]: loss 0.571413
[epoch17, step1489]: loss 0.542894
[epoch17, step1490]: loss 0.629762
[epoch17, step1491]: loss 0.264713
[epoch17, step1492]: loss 0.391497
[epoch17, step1493]: loss 0.330529
[epoch17, step1494]: loss 0.522143
[epoch17, step1495]: loss 0.356210
[epoch17, step1496]: loss 0.554717
[epoch17, step1497]: loss 0.327635
[epoch17, step1498]: loss 0.421857
[epoch17, step1499]: loss 0.667503
[epoch17, step1500]: loss 0.386029
[epoch17, step1501]: loss 0.408469
[epoch17, step1502]: loss 0.380341
[epoch17, step1503]: loss 0.241640
[epoch17, step1504]: loss 0.235039
[epoch17, step1505]: loss 0.337013
[epoch17, step1506]: loss 0.305829
[epoch17, step1507]: loss 0.503705
[epoch17, step1508]: loss 0.262484
[epoch17, step1509]: loss 0.463904
[epoch17, step1510]: loss 0.420712
[epoch17, step1511]: loss 0.431670
[epoch17, step1512]: loss 0.504314
[epoch17, step1513]: loss 0.522149
[epoch17, step1514]: loss 0.500983
[epoch17, step1515]: loss 0.315485
[epoch17, step1516]: loss 0.522704
[epoch17, step1517]: loss 0.661114
[epoch17, step1518]: loss 0.262722
[epoch17, step1519]: loss 0.493465
[epoch17, step1520]: loss 0.577604
[epoch17, step1521]: loss 0.615371
[epoch17, step1522]: loss 0.477427
[epoch17, step1523]: loss 0.662301
[epoch17, step1524]: loss 0.518948
[epoch17, step1525]: loss 0.357626
[epoch17, step1526]: loss 0.649844
[epoch17, step1527]: loss 0.576936
[epoch17, step1528]: loss 0.456145
[epoch17, step1529]: loss 0.594489
[epoch17, step1530]: loss 0.428517
[epoch17, step1531]: loss 0.601431
[epoch17, step1532]: loss 0.524722
[epoch17, step1533]: loss 0.387910
[epoch17, step1534]: loss 0.247337
[epoch17, step1535]: loss 0.441876
[epoch17, step1536]: loss 0.493811
[epoch17, step1537]: loss 0.519090
[epoch17, step1538]: loss 0.572536
[epoch17, step1539]: loss 0.438025
[epoch17, step1540]: loss 0.260312
[epoch17, step1541]: loss 0.408799
[epoch17, step1542]: loss 0.677020
[epoch17, step1543]: loss 0.476268
[epoch17, step1544]: loss 0.489006
[epoch17, step1545]: loss 0.508659
[epoch17, step1546]: loss 0.704919
[epoch17, step1547]: loss 0.570361
[epoch17, step1548]: loss 0.223521
[epoch17, step1549]: loss 0.617247
[epoch17, step1550]: loss 0.274159
[epoch17, step1551]: loss 0.476046
[epoch17, step1552]: loss 0.640902
[epoch17, step1553]: loss 0.563024
[epoch17, step1554]: loss 0.302251
[epoch17, step1555]: loss 0.476727
[epoch17, step1556]: loss 0.601000
[epoch17, step1557]: loss 0.584092
[epoch17, step1558]: loss 0.444898
[epoch17, step1559]: loss 0.343707
[epoch17, step1560]: loss 0.524081
[epoch17, step1561]: loss 0.292718
[epoch17, step1562]: loss 0.525737
[epoch17, step1563]: loss 0.615902
[epoch17, step1564]: loss 0.149788
[epoch17, step1565]: loss 0.728630
[epoch17, step1566]: loss 0.657629
[epoch17, step1567]: loss 0.524608
[epoch17, step1568]: loss 0.700439
[epoch17, step1569]: loss 0.676705
[epoch17, step1570]: loss 0.577183
[epoch17, step1571]: loss 0.375350
[epoch17, step1572]: loss 0.381462
[epoch17, step1573]: loss 0.225195
[epoch17, step1574]: loss 0.403699
[epoch17, step1575]: loss 0.519533
[epoch17, step1576]: loss 0.603293
[epoch17, step1577]: loss 0.550469
[epoch17, step1578]: loss 0.371771
[epoch17, step1579]: loss 0.654815
[epoch17, step1580]: loss 0.529571
[epoch17, step1581]: loss 0.607848
[epoch17, step1582]: loss 0.666110
[epoch17, step1583]: loss 0.569974
[epoch17, step1584]: loss 0.464538
[epoch17, step1585]: loss 0.408742
[epoch17, step1586]: loss 0.398660
[epoch17, step1587]: loss 0.238052
[epoch17, step1588]: loss 0.443666
[epoch17, step1589]: loss 0.482644
[epoch17, step1590]: loss 0.408339
[epoch17, step1591]: loss 0.373894
[epoch17, step1592]: loss 0.432867
[epoch17, step1593]: loss 0.591341
[epoch17, step1594]: loss 0.252978
[epoch17, step1595]: loss 0.322166
[epoch17, step1596]: loss 0.514096
[epoch17, step1597]: loss 0.496171
[epoch17, step1598]: loss 0.254549
[epoch17, step1599]: loss 0.473189
[epoch17, step1600]: loss 0.320985
[epoch17, step1601]: loss 0.453803
[epoch17, step1602]: loss 0.330699
[epoch17, step1603]: loss 0.543783
[epoch17, step1604]: loss 0.467276
[epoch17, step1605]: loss 0.503138
[epoch17, step1606]: loss 0.555767
[epoch17, step1607]: loss 0.550663
[epoch17, step1608]: loss 0.379588
[epoch17, step1609]: loss 0.606628
[epoch17, step1610]: loss 0.575133
[epoch17, step1611]: loss 0.485967
[epoch17, step1612]: loss 0.371915
[epoch17, step1613]: loss 0.322263
[epoch17, step1614]: loss 0.458965
[epoch17, step1615]: loss 0.388667
[epoch17, step1616]: loss 0.498152
[epoch17, step1617]: loss 0.539495
[epoch17, step1618]: loss 0.146061
[epoch17, step1619]: loss 0.465534
[epoch17, step1620]: loss 0.427461
[epoch17, step1621]: loss 0.402751
[epoch17, step1622]: loss 0.503904
[epoch17, step1623]: loss 0.443051
[epoch17, step1624]: loss 0.518789
[epoch17, step1625]: loss 0.469206
[epoch17, step1626]: loss 0.490116
[epoch17, step1627]: loss 0.421796
[epoch17, step1628]: loss 0.434355
[epoch17, step1629]: loss 0.375875
[epoch17, step1630]: loss 0.598848
[epoch17, step1631]: loss 0.543133
[epoch17, step1632]: loss 0.494368
[epoch17, step1633]: loss 0.687827
[epoch17, step1634]: loss 0.433609
[epoch17, step1635]: loss 0.597810
[epoch17, step1636]: loss 0.603081
[epoch17, step1637]: loss 0.436494
[epoch17, step1638]: loss 0.324984
[epoch17, step1639]: loss 0.580958
[epoch17, step1640]: loss 0.398084
[epoch17, step1641]: loss 0.502858
[epoch17, step1642]: loss 0.502786
[epoch17, step1643]: loss 0.426238
[epoch17, step1644]: loss 0.495381
[epoch17, step1645]: loss 0.622957
[epoch17, step1646]: loss 0.420358
[epoch17, step1647]: loss 0.438596
[epoch17, step1648]: loss 0.493505
[epoch17, step1649]: loss 0.530184
[epoch17, step1650]: loss 0.445332
[epoch17, step1651]: loss 0.384052
[epoch17, step1652]: loss 0.292382
[epoch17, step1653]: loss 0.248804
[epoch17, step1654]: loss 0.480186
[epoch17, step1655]: loss 0.522508
[epoch17, step1656]: loss 0.508697
[epoch17, step1657]: loss 0.391416
[epoch17, step1658]: loss 0.604284
[epoch17, step1659]: loss 0.544997
[epoch17, step1660]: loss 0.415184
[epoch17, step1661]: loss 0.441163
[epoch17, step1662]: loss 0.527164
[epoch17, step1663]: loss 0.254093
[epoch17, step1664]: loss 0.098175
[epoch17, step1665]: loss 0.359972
[epoch17, step1666]: loss 0.365760
[epoch17, step1667]: loss 0.324675
[epoch17, step1668]: loss 0.432806
[epoch17, step1669]: loss 0.394147
[epoch17, step1670]: loss 0.512814
[epoch17, step1671]: loss 0.250424
[epoch17, step1672]: loss 0.480574
[epoch17, step1673]: loss 0.350538
[epoch17, step1674]: loss 0.483811
[epoch17, step1675]: loss 0.345330
[epoch17, step1676]: loss 0.505230
[epoch17, step1677]: loss 0.276732
[epoch17, step1678]: loss 0.462533
[epoch17, step1679]: loss 0.508047
[epoch17, step1680]: loss 0.590258
[epoch17, step1681]: loss 0.457190
[epoch17, step1682]: loss 0.454024
[epoch17, step1683]: loss 0.598202
[epoch17, step1684]: loss 0.553221
[epoch17, step1685]: loss 0.232074
[epoch17, step1686]: loss 0.139938
[epoch17, step1687]: loss 0.690041
[epoch17, step1688]: loss 0.279773
[epoch17, step1689]: loss 0.686361
[epoch17, step1690]: loss 0.430254
[epoch17, step1691]: loss 0.736763
[epoch17, step1692]: loss 0.302290
[epoch17, step1693]: loss 0.443076
[epoch17, step1694]: loss 0.574741
[epoch17, step1695]: loss 0.335521
[epoch17, step1696]: loss 0.472177
[epoch17, step1697]: loss 0.577689
[epoch17, step1698]: loss 0.419817
[epoch17, step1699]: loss 0.562579
[epoch17, step1700]: loss 0.235838
[epoch17, step1701]: loss 0.518435
[epoch17, step1702]: loss 0.449902
[epoch17, step1703]: loss 0.611218
[epoch17, step1704]: loss 0.542196
[epoch17, step1705]: loss 0.480822
[epoch17, step1706]: loss 0.348930
[epoch17, step1707]: loss 0.622108
[epoch17, step1708]: loss 0.217845
[epoch17, step1709]: loss 0.464781
[epoch17, step1710]: loss 0.252028
[epoch17, step1711]: loss 0.497965
[epoch17, step1712]: loss 0.462718
[epoch17, step1713]: loss 0.438353
[epoch17, step1714]: loss 0.518517
[epoch17, step1715]: loss 0.129309
[epoch17, step1716]: loss 0.346647
[epoch17, step1717]: loss 0.562054
[epoch17, step1718]: loss 0.514113
[epoch17, step1719]: loss 0.365241
[epoch17, step1720]: loss 0.418618
[epoch17, step1721]: loss 0.635308
[epoch17, step1722]: loss 0.465650
[epoch17, step1723]: loss 0.372783
[epoch17, step1724]: loss 0.459916
[epoch17, step1725]: loss 0.300872
[epoch17, step1726]: loss 0.493168
[epoch17, step1727]: loss 0.381048
[epoch17, step1728]: loss 0.560946
[epoch17, step1729]: loss 0.142453
[epoch17, step1730]: loss 0.380386
[epoch17, step1731]: loss 0.403738
[epoch17, step1732]: loss 0.404934
[epoch17, step1733]: loss 0.447815
[epoch17, step1734]: loss 0.400844
[epoch17, step1735]: loss 0.444648
[epoch17, step1736]: loss 0.575575
[epoch17, step1737]: loss 0.444454
[epoch17, step1738]: loss 0.618650
[epoch17, step1739]: loss 0.717069
[epoch17, step1740]: loss 0.360563
[epoch17, step1741]: loss 0.423883
[epoch17, step1742]: loss 0.437736
[epoch17, step1743]: loss 0.637043
[epoch17, step1744]: loss 0.237603
[epoch17, step1745]: loss 0.258679
[epoch17, step1746]: loss 0.496157
[epoch17, step1747]: loss 0.278937
[epoch17, step1748]: loss 0.520787
[epoch17, step1749]: loss 0.537369
[epoch17, step1750]: loss 0.628864
[epoch17, step1751]: loss 0.527748
[epoch17, step1752]: loss 0.430119
[epoch17, step1753]: loss 0.480264
[epoch17, step1754]: loss 0.507641
[epoch17, step1755]: loss 0.458363
[epoch17, step1756]: loss 0.396491
[epoch17, step1757]: loss 0.572308
[epoch17, step1758]: loss 0.307270
[epoch17, step1759]: loss 0.640133
[epoch17, step1760]: loss 0.634880
[epoch17, step1761]: loss 0.490932
[epoch17, step1762]: loss 0.497907
[epoch17, step1763]: loss 0.509933
[epoch17, step1764]: loss 0.619080
[epoch17, step1765]: loss 0.450699
[epoch17, step1766]: loss 0.442503
[epoch17, step1767]: loss 0.582446
[epoch17, step1768]: loss 0.591311
[epoch17, step1769]: loss 0.617973
[epoch17, step1770]: loss 0.498377
[epoch17, step1771]: loss 0.544454
[epoch17, step1772]: loss 0.515656
[epoch17, step1773]: loss 0.549645
[epoch17, step1774]: loss 0.418355
[epoch17, step1775]: loss 0.300828
[epoch17, step1776]: loss 0.417359
[epoch17, step1777]: loss 0.545571
[epoch17, step1778]: loss 0.381884
[epoch17, step1779]: loss 0.557090
[epoch17, step1780]: loss 0.384652
[epoch17, step1781]: loss 0.619405
[epoch17, step1782]: loss 0.506861
[epoch17, step1783]: loss 0.530835
[epoch17, step1784]: loss 0.325092
[epoch17, step1785]: loss 0.425130
[epoch17, step1786]: loss 0.653971
[epoch17, step1787]: loss 0.488469
[epoch17, step1788]: loss 0.543084
[epoch17, step1789]: loss 0.492284
[epoch17, step1790]: loss 0.356615
[epoch17, step1791]: loss 0.605077
[epoch17, step1792]: loss 0.364646
[epoch17, step1793]: loss 0.318118
[epoch17, step1794]: loss 0.424964
[epoch17, step1795]: loss 0.426133
[epoch17, step1796]: loss 0.496658
[epoch17, step1797]: loss 0.339225
[epoch17, step1798]: loss 0.609486
[epoch17, step1799]: loss 0.462173
[epoch17, step1800]: loss 0.447828
[epoch17, step1801]: loss 0.324451
[epoch17, step1802]: loss 0.526757
[epoch17, step1803]: loss 0.485880
[epoch17, step1804]: loss 0.561621
[epoch17, step1805]: loss 0.605660
[epoch17, step1806]: loss 0.614158
[epoch17, step1807]: loss 0.351402
[epoch17, step1808]: loss 0.740176
[epoch17, step1809]: loss 0.314469
[epoch17, step1810]: loss 0.502957
[epoch17, step1811]: loss 0.545651
[epoch17, step1812]: loss 0.591008
[epoch17, step1813]: loss 0.456287
[epoch17, step1814]: loss 0.236452
[epoch17, step1815]: loss 0.618925
[epoch17, step1816]: loss 0.454000
[epoch17, step1817]: loss 0.330645
[epoch17, step1818]: loss 0.606410
[epoch17, step1819]: loss 0.444124
[epoch17, step1820]: loss 0.231847
[epoch17, step1821]: loss 0.566920
[epoch17, step1822]: loss 0.543048
[epoch17, step1823]: loss 0.522274
[epoch17, step1824]: loss 0.448013
[epoch17, step1825]: loss 0.732823
[epoch17, step1826]: loss 0.488935
[epoch17, step1827]: loss 0.509475
[epoch17, step1828]: loss 0.517985
[epoch17, step1829]: loss 0.516055
[epoch17, step1830]: loss 0.561705
[epoch17, step1831]: loss 0.446776
[epoch17, step1832]: loss 0.462800
[epoch17, step1833]: loss 0.710259
[epoch17, step1834]: loss 0.512085
[epoch17, step1835]: loss 0.408363
[epoch17, step1836]: loss 0.496375
[epoch17, step1837]: loss 0.285478
[epoch17, step1838]: loss 0.477785
[epoch17, step1839]: loss 0.534831
[epoch17, step1840]: loss 0.571391
[epoch17, step1841]: loss 0.405753
[epoch17, step1842]: loss 0.595736
[epoch17, step1843]: loss 0.421239
[epoch17, step1844]: loss 0.530178
[epoch17, step1845]: loss 0.592516
[epoch17, step1846]: loss 0.250623
[epoch17, step1847]: loss 0.328556
[epoch17, step1848]: loss 0.468080
[epoch17, step1849]: loss 0.456244
[epoch17, step1850]: loss 0.482750
[epoch17, step1851]: loss 0.470862
[epoch17, step1852]: loss 0.384618
[epoch17, step1853]: loss 0.366704
[epoch17, step1854]: loss 0.429495
[epoch17, step1855]: loss 0.483342
[epoch17, step1856]: loss 0.328089
[epoch17, step1857]: loss 0.351743
[epoch17, step1858]: loss 0.549825
[epoch17, step1859]: loss 0.446984
[epoch17, step1860]: loss 0.491828
[epoch17, step1861]: loss 0.439884
[epoch17, step1862]: loss 0.562283
[epoch17, step1863]: loss 0.443824
[epoch17, step1864]: loss 0.255590
[epoch17, step1865]: loss 0.243991
[epoch17, step1866]: loss 0.410161
[epoch17, step1867]: loss 0.408375
[epoch17, step1868]: loss 0.666673
[epoch17, step1869]: loss 0.354781
[epoch17, step1870]: loss 0.448524
[epoch17, step1871]: loss 0.538691
[epoch17, step1872]: loss 0.429712
[epoch17, step1873]: loss 0.385101
[epoch17, step1874]: loss 0.484834
[epoch17, step1875]: loss 0.534015
[epoch17, step1876]: loss 0.671989
[epoch17, step1877]: loss 0.532510
[epoch17, step1878]: loss 0.369573
[epoch17, step1879]: loss 0.570461
[epoch17, step1880]: loss 0.553721
[epoch17, step1881]: loss 0.614143
[epoch17, step1882]: loss 0.392194
[epoch17, step1883]: loss 0.260874
[epoch17, step1884]: loss 0.623461
[epoch17, step1885]: loss 0.606269
[epoch17, step1886]: loss 0.470502
[epoch17, step1887]: loss 0.350997
[epoch17, step1888]: loss 0.481034
[epoch17, step1889]: loss 0.531513
[epoch17, step1890]: loss 0.226009
[epoch17, step1891]: loss 0.298896
[epoch17, step1892]: loss 0.572975
[epoch17, step1893]: loss 0.335615
[epoch17, step1894]: loss 0.214107
[epoch17, step1895]: loss 0.489101
[epoch17, step1896]: loss 0.463559
[epoch17, step1897]: loss 0.499586
[epoch17, step1898]: loss 0.600047
[epoch17, step1899]: loss 0.433120
[epoch17, step1900]: loss 0.351837
[epoch17, step1901]: loss 0.433443
[epoch17, step1902]: loss 0.453514
[epoch17, step1903]: loss 0.369344
[epoch17, step1904]: loss 0.595671
[epoch17, step1905]: loss 0.628171
[epoch17, step1906]: loss 0.446913
[epoch17, step1907]: loss 0.418285
[epoch17, step1908]: loss 0.342594
[epoch17, step1909]: loss 0.445300
[epoch17, step1910]: loss 0.460016
[epoch17, step1911]: loss 0.236975
[epoch17, step1912]: loss 0.397277
[epoch17, step1913]: loss 0.203042
[epoch17, step1914]: loss 0.308428
[epoch17, step1915]: loss 0.153415
[epoch17, step1916]: loss 0.368601
[epoch17, step1917]: loss 0.490645
[epoch17, step1918]: loss 0.668106
[epoch17, step1919]: loss 0.429952
[epoch17, step1920]: loss 0.303346
[epoch17, step1921]: loss 0.495832
[epoch17, step1922]: loss 0.402790
[epoch17, step1923]: loss 0.377066
[epoch17, step1924]: loss 0.481242
[epoch17, step1925]: loss 0.437230
[epoch17, step1926]: loss 0.508214
[epoch17, step1927]: loss 0.550017
[epoch17, step1928]: loss 0.107586
[epoch17, step1929]: loss 0.553797
[epoch17, step1930]: loss 0.116254
[epoch17, step1931]: loss 0.531597
[epoch17, step1932]: loss 0.652551
[epoch17, step1933]: loss 0.446100
[epoch17, step1934]: loss 0.603128
[epoch17, step1935]: loss 0.570999
[epoch17, step1936]: loss 0.650569
[epoch17, step1937]: loss 0.485582
[epoch17, step1938]: loss 0.455722
[epoch17, step1939]: loss 0.364486
[epoch17, step1940]: loss 0.454598
[epoch17, step1941]: loss 0.274101
[epoch17, step1942]: loss 0.623320
[epoch17, step1943]: loss 0.542841
[epoch17, step1944]: loss 0.448174
[epoch17, step1945]: loss 0.471783
[epoch17, step1946]: loss 0.605271
[epoch17, step1947]: loss 0.452428
[epoch17, step1948]: loss 0.653394
[epoch17, step1949]: loss 0.297165
[epoch17, step1950]: loss 0.713872
[epoch17, step1951]: loss 0.221594
[epoch17, step1952]: loss 0.626917
[epoch17, step1953]: loss 0.353823
[epoch17, step1954]: loss 0.437591
[epoch17, step1955]: loss 0.578876
[epoch17, step1956]: loss 0.441344
[epoch17, step1957]: loss 0.459399
[epoch17, step1958]: loss 0.357473
[epoch17, step1959]: loss 0.197618
[epoch17, step1960]: loss 0.137610
[epoch17, step1961]: loss 0.446999
[epoch17, step1962]: loss 0.480701
[epoch17, step1963]: loss 0.476762
[epoch17, step1964]: loss 0.299434
[epoch17, step1965]: loss 0.442043
[epoch17, step1966]: loss 0.341204
[epoch17, step1967]: loss 0.432299
[epoch17, step1968]: loss 0.467028
[epoch17, step1969]: loss 0.574305
[epoch17, step1970]: loss 0.664217
[epoch17, step1971]: loss 0.406338
[epoch17, step1972]: loss 0.615494
[epoch17, step1973]: loss 0.360744
[epoch17, step1974]: loss 0.648084
[epoch17, step1975]: loss 0.407232
[epoch17, step1976]: loss 0.631323
[epoch17, step1977]: loss 0.520482
[epoch17, step1978]: loss 0.630436
[epoch17, step1979]: loss 0.347370
[epoch17, step1980]: loss 0.386318
[epoch17, step1981]: loss 0.591022
[epoch17, step1982]: loss 0.537361
[epoch17, step1983]: loss 0.329240
[epoch17, step1984]: loss 0.601359
[epoch17, step1985]: loss 0.518881
[epoch17, step1986]: loss 0.410795
[epoch17, step1987]: loss 0.528430
[epoch17, step1988]: loss 0.359758
[epoch17, step1989]: loss 0.526860
[epoch17, step1990]: loss 0.470470
[epoch17, step1991]: loss 0.491777
[epoch17, step1992]: loss 0.472811
[epoch17, step1993]: loss 0.646188
[epoch17, step1994]: loss 0.561406
[epoch17, step1995]: loss 0.500783
[epoch17, step1996]: loss 0.457898
[epoch17, step1997]: loss 0.508217
[epoch17, step1998]: loss 0.321763
[epoch17, step1999]: loss 0.563591
[epoch17, step2000]: loss 0.461639
[epoch17, step2001]: loss 0.668436
[epoch17, step2002]: loss 0.431798
[epoch17, step2003]: loss 0.294469
[epoch17, step2004]: loss 0.404286
[epoch17, step2005]: loss 0.526493
[epoch17, step2006]: loss 0.409008
[epoch17, step2007]: loss 0.199494
[epoch17, step2008]: loss 0.502676
[epoch17, step2009]: loss 0.536875
[epoch17, step2010]: loss 0.509439
[epoch17, step2011]: loss 0.592681
[epoch17, step2012]: loss 0.215466
[epoch17, step2013]: loss 0.438089
[epoch17, step2014]: loss 0.378763
[epoch17, step2015]: loss 0.488231
[epoch17, step2016]: loss 0.467461
[epoch17, step2017]: loss 0.691003
[epoch17, step2018]: loss 0.364500
[epoch17, step2019]: loss 0.209216
[epoch17, step2020]: loss 0.487904
[epoch17, step2021]: loss 0.558197
[epoch17, step2022]: loss 0.470698
[epoch17, step2023]: loss 0.421544
[epoch17, step2024]: loss 0.471007
[epoch17, step2025]: loss 0.549481
[epoch17, step2026]: loss 0.563109
[epoch17, step2027]: loss 0.420759
[epoch17, step2028]: loss 0.546143
[epoch17, step2029]: loss 0.247411
[epoch17, step2030]: loss 0.665538
[epoch17, step2031]: loss 0.359237
[epoch17, step2032]: loss 0.544736
[epoch17, step2033]: loss 0.210767
[epoch17, step2034]: loss 0.516223
[epoch17, step2035]: loss 0.286590
[epoch17, step2036]: loss 0.605319
[epoch17, step2037]: loss 0.377372
[epoch17, step2038]: loss 0.304599
[epoch17, step2039]: loss 0.716244
[epoch17, step2040]: loss 0.470940
[epoch17, step2041]: loss 0.175029
[epoch17, step2042]: loss 0.513485
[epoch17, step2043]: loss 0.297190
[epoch17, step2044]: loss 0.190504
[epoch17, step2045]: loss 0.370929
[epoch17, step2046]: loss 0.297758
[epoch17, step2047]: loss 0.660032
[epoch17, step2048]: loss 0.493828
[epoch17, step2049]: loss 0.232505
[epoch17, step2050]: loss 0.665625
[epoch17, step2051]: loss 0.349132
[epoch17, step2052]: loss 0.441897
[epoch17, step2053]: loss 0.542402
[epoch17, step2054]: loss 0.408669
[epoch17, step2055]: loss 0.544456
[epoch17, step2056]: loss 0.480274
[epoch17, step2057]: loss 0.599153
[epoch17, step2058]: loss 0.372388
[epoch17, step2059]: loss 0.444065
[epoch17, step2060]: loss 0.547266
[epoch17, step2061]: loss 0.494322
[epoch17, step2062]: loss 0.427925
[epoch17, step2063]: loss 0.517916
[epoch17, step2064]: loss 0.396646
[epoch17, step2065]: loss 0.530404
[epoch17, step2066]: loss 0.575799
[epoch17, step2067]: loss 0.330331
[epoch17, step2068]: loss 0.273119
[epoch17, step2069]: loss 0.404682
[epoch17, step2070]: loss 0.531260
[epoch17, step2071]: loss 0.356786
[epoch17, step2072]: loss 0.636274
[epoch17, step2073]: loss 0.400380
[epoch17, step2074]: loss 0.464549
[epoch17, step2075]: loss 0.116122
[epoch17, step2076]: loss 0.468993
[epoch17, step2077]: loss 0.473737
[epoch17, step2078]: loss 0.485823
[epoch17, step2079]: loss 0.570172
[epoch17, step2080]: loss 0.641517
[epoch17, step2081]: loss 0.404073
[epoch17, step2082]: loss 0.545463
[epoch17, step2083]: loss 0.539303
[epoch17, step2084]: loss 0.501453
[epoch17, step2085]: loss 0.295650
[epoch17, step2086]: loss 0.383802
[epoch17, step2087]: loss 0.422449
[epoch17, step2088]: loss 0.256901
[epoch17, step2089]: loss 0.488657
[epoch17, step2090]: loss 0.324165
[epoch17, step2091]: loss 0.195564
[epoch17, step2092]: loss 0.449960
[epoch17, step2093]: loss 0.504626
[epoch17, step2094]: loss 0.357467
[epoch17, step2095]: loss 0.295745
[epoch17, step2096]: loss 0.618471
[epoch17, step2097]: loss 0.645968
[epoch17, step2098]: loss 0.389498
[epoch17, step2099]: loss 0.426276
[epoch17, step2100]: loss 0.303373
[epoch17, step2101]: loss 0.339801
[epoch17, step2102]: loss 0.432441
[epoch17, step2103]: loss 0.370341
[epoch17, step2104]: loss 0.471135
[epoch17, step2105]: loss 0.339593
[epoch17, step2106]: loss 0.606241
[epoch17, step2107]: loss 0.710894
[epoch17, step2108]: loss 0.604977
[epoch17, step2109]: loss 0.489028
[epoch17, step2110]: loss 0.493453
[epoch17, step2111]: loss 0.447102
[epoch17, step2112]: loss 0.546889
[epoch17, step2113]: loss 0.556432
[epoch17, step2114]: loss 0.651891
[epoch17, step2115]: loss 0.465804
[epoch17, step2116]: loss 0.475400
[epoch17, step2117]: loss 0.447773
[epoch17, step2118]: loss 0.573460
[epoch17, step2119]: loss 0.381925
[epoch17, step2120]: loss 0.485575
[epoch17, step2121]: loss 0.581594
[epoch17, step2122]: loss 0.454553
[epoch17, step2123]: loss 0.488688
[epoch17, step2124]: loss 0.496888
[epoch17, step2125]: loss 0.341824
[epoch17, step2126]: loss 0.485024
[epoch17, step2127]: loss 0.447763
[epoch17, step2128]: loss 0.385014
[epoch17, step2129]: loss 0.219487
[epoch17, step2130]: loss 0.622063
[epoch17, step2131]: loss 0.480967
[epoch17, step2132]: loss 0.243947
[epoch17, step2133]: loss 0.527238
[epoch17, step2134]: loss 0.742669
[epoch17, step2135]: loss 0.667426
[epoch17, step2136]: loss 0.412557
[epoch17, step2137]: loss 0.423086
[epoch17, step2138]: loss 0.490003
[epoch17, step2139]: loss 0.539809
[epoch17, step2140]: loss 0.399847
[epoch17, step2141]: loss 0.564212
[epoch17, step2142]: loss 0.498618
[epoch17, step2143]: loss 0.465554
[epoch17, step2144]: loss 0.613769
[epoch17, step2145]: loss 0.654597
[epoch17, step2146]: loss 0.476469
[epoch17, step2147]: loss 0.368727
[epoch17, step2148]: loss 0.580562
[epoch17, step2149]: loss 0.372478
[epoch17, step2150]: loss 0.480931
[epoch17, step2151]: loss 0.297406
[epoch17, step2152]: loss 0.680900
[epoch17, step2153]: loss 0.499478
[epoch17, step2154]: loss 0.527613
[epoch17, step2155]: loss 0.381068
[epoch17, step2156]: loss 0.117916
[epoch17, step2157]: loss 0.487892
[epoch17, step2158]: loss 0.450962
[epoch17, step2159]: loss 0.605665
[epoch17, step2160]: loss 0.365207
[epoch17, step2161]: loss 0.422600
[epoch17, step2162]: loss 0.293690
[epoch17, step2163]: loss 0.579361
[epoch17, step2164]: loss 0.534482
[epoch17, step2165]: loss 0.534325
[epoch17, step2166]: loss 0.412465
[epoch17, step2167]: loss 0.629172
[epoch17, step2168]: loss 0.535784
[epoch17, step2169]: loss 0.528349
[epoch17, step2170]: loss 0.353078
[epoch17, step2171]: loss 0.524707
[epoch17, step2172]: loss 0.587990
[epoch17, step2173]: loss 0.330983
[epoch17, step2174]: loss 0.534949
[epoch17, step2175]: loss 0.591065
[epoch17, step2176]: loss 0.450584
[epoch17, step2177]: loss 0.671665
[epoch17, step2178]: loss 0.415310
[epoch17, step2179]: loss 0.582073
[epoch17, step2180]: loss 0.413037
[epoch17, step2181]: loss 0.529922
[epoch17, step2182]: loss 0.516163
[epoch17, step2183]: loss 0.339160
[epoch17, step2184]: loss 0.526317
[epoch17, step2185]: loss 0.419829
[epoch17, step2186]: loss 0.552774
[epoch17, step2187]: loss 0.424116
[epoch17, step2188]: loss 0.748025
[epoch17, step2189]: loss 0.250161
[epoch17, step2190]: loss 0.450756
[epoch17, step2191]: loss 0.335390
[epoch17, step2192]: loss 0.333282
[epoch17, step2193]: loss 0.602123
[epoch17, step2194]: loss 0.598519
[epoch17, step2195]: loss 0.517276
[epoch17, step2196]: loss 0.545178
[epoch17, step2197]: loss 0.482258
[epoch17, step2198]: loss 0.567553
[epoch17, step2199]: loss 0.323689
[epoch17, step2200]: loss 0.550821
[epoch17, step2201]: loss 0.356844
[epoch17, step2202]: loss 0.469278
[epoch17, step2203]: loss 0.535649
[epoch17, step2204]: loss 0.657801
[epoch17, step2205]: loss 0.443497
[epoch17, step2206]: loss 0.551232
[epoch17, step2207]: loss 0.355715
[epoch17, step2208]: loss 0.163851
[epoch17, step2209]: loss 0.452801
[epoch17, step2210]: loss 0.382319
[epoch17, step2211]: loss 0.613600
[epoch17, step2212]: loss 0.409390
[epoch17, step2213]: loss 0.507980
[epoch17, step2214]: loss 0.562267
[epoch17, step2215]: loss 0.538281
[epoch17, step2216]: loss 0.137937
[epoch17, step2217]: loss 0.658793
[epoch17, step2218]: loss 0.493516
[epoch17, step2219]: loss 0.432651
[epoch17, step2220]: loss 0.624581
[epoch17, step2221]: loss 0.327652
[epoch17, step2222]: loss 0.439491
[epoch17, step2223]: loss 0.550380
[epoch17, step2224]: loss 0.364213
[epoch17, step2225]: loss 0.289504
[epoch17, step2226]: loss 0.151565
[epoch17, step2227]: loss 0.284349
[epoch17, step2228]: loss 0.302491
[epoch17, step2229]: loss 0.145717
[epoch17, step2230]: loss 0.459035
[epoch17, step2231]: loss 0.434279
[epoch17, step2232]: loss 0.334153
[epoch17, step2233]: loss 0.339975
[epoch17, step2234]: loss 0.393253
[epoch17, step2235]: loss 0.475586
[epoch17, step2236]: loss 0.333858
[epoch17, step2237]: loss 0.351786
[epoch17, step2238]: loss 0.635742
[epoch17, step2239]: loss 0.516612
[epoch17, step2240]: loss 0.574735
[epoch17, step2241]: loss 0.466909
[epoch17, step2242]: loss 0.626900
[epoch17, step2243]: loss 0.398451
[epoch17, step2244]: loss 0.567215
[epoch17, step2245]: loss 0.399405
[epoch17, step2246]: loss 0.254651
[epoch17, step2247]: loss 0.410395
[epoch17, step2248]: loss 0.508684
[epoch17, step2249]: loss 0.534063
[epoch17, step2250]: loss 0.578264
[epoch17, step2251]: loss 0.413800
[epoch17, step2252]: loss 0.437338
[epoch17, step2253]: loss 0.522439
[epoch17, step2254]: loss 0.507570
[epoch17, step2255]: loss 0.410419
[epoch17, step2256]: loss 0.489275
[epoch17, step2257]: loss 0.690355
[epoch17, step2258]: loss 0.179392
[epoch17, step2259]: loss 0.617429
[epoch17, step2260]: loss 0.584582
[epoch17, step2261]: loss 0.568214
[epoch17, step2262]: loss 0.679516
[epoch17, step2263]: loss 0.447176
[epoch17, step2264]: loss 0.336654
[epoch17, step2265]: loss 0.584032
[epoch17, step2266]: loss 0.422663
[epoch17, step2267]: loss 0.355244
[epoch17, step2268]: loss 0.490646
[epoch17, step2269]: loss 0.614879
[epoch17, step2270]: loss 0.660309
[epoch17, step2271]: loss 0.345969
[epoch17, step2272]: loss 0.466534
[epoch17, step2273]: loss 0.619529
[epoch17, step2274]: loss 0.421894
[epoch17, step2275]: loss 0.454219
[epoch17, step2276]: loss 0.291912
[epoch17, step2277]: loss 0.473815
[epoch17, step2278]: loss 0.324878
[epoch17, step2279]: loss 0.520693
[epoch17, step2280]: loss 0.487565
[epoch17, step2281]: loss 0.445135
[epoch17, step2282]: loss 0.409510
[epoch17, step2283]: loss 0.336290
[epoch17, step2284]: loss 0.510955
[epoch17, step2285]: loss 0.519747
[epoch17, step2286]: loss 0.257879
[epoch17, step2287]: loss 0.282601
[epoch17, step2288]: loss 0.416013
[epoch17, step2289]: loss 0.620083
[epoch17, step2290]: loss 0.374680
[epoch17, step2291]: loss 0.506592
[epoch17, step2292]: loss 0.375392
[epoch17, step2293]: loss 0.725400
[epoch17, step2294]: loss 0.608433
[epoch17, step2295]: loss 0.533841
[epoch17, step2296]: loss 0.323906
[epoch17, step2297]: loss 0.473543
[epoch17, step2298]: loss 0.556676
[epoch17, step2299]: loss 0.313558
[epoch17, step2300]: loss 0.281034
[epoch17, step2301]: loss 0.767932
[epoch17, step2302]: loss 0.626771
[epoch17, step2303]: loss 0.210423
[epoch17, step2304]: loss 0.626956
[epoch17, step2305]: loss 0.236709
[epoch17, step2306]: loss 0.437878
[epoch17, step2307]: loss 0.702055
[epoch17, step2308]: loss 0.452729
[epoch17, step2309]: loss 0.473042
[epoch17, step2310]: loss 0.331751
[epoch17, step2311]: loss 0.392723
[epoch17, step2312]: loss 0.514516
[epoch17, step2313]: loss 0.523428
[epoch17, step2314]: loss 0.554233
[epoch17, step2315]: loss 0.440509
[epoch17, step2316]: loss 0.467073
[epoch17, step2317]: loss 0.369866
[epoch17, step2318]: loss 0.515288
[epoch17, step2319]: loss 0.130026
[epoch17, step2320]: loss 0.564320
[epoch17, step2321]: loss 0.414031
[epoch17, step2322]: loss 0.410937
[epoch17, step2323]: loss 0.283665
[epoch17, step2324]: loss 0.547453
[epoch17, step2325]: loss 0.664484
[epoch17, step2326]: loss 0.457974
[epoch17, step2327]: loss 0.522771
[epoch17, step2328]: loss 0.358767
[epoch17, step2329]: loss 0.404811
[epoch17, step2330]: loss 0.530086
[epoch17, step2331]: loss 0.278958
[epoch17, step2332]: loss 0.518040
[epoch17, step2333]: loss 0.620336
[epoch17, step2334]: loss 0.512692
[epoch17, step2335]: loss 0.580375
[epoch17, step2336]: loss 0.468065
[epoch17, step2337]: loss 0.538323
[epoch17, step2338]: loss 0.389961
[epoch17, step2339]: loss 0.565295
[epoch17, step2340]: loss 0.551107
[epoch17, step2341]: loss 0.416966
[epoch17, step2342]: loss 0.479365
[epoch17, step2343]: loss 0.405435
[epoch17, step2344]: loss 0.638155
[epoch17, step2345]: loss 0.279174
[epoch17, step2346]: loss 0.454062
[epoch17, step2347]: loss 0.536575
[epoch17, step2348]: loss 0.635181
[epoch17, step2349]: loss 0.327126
[epoch17, step2350]: loss 0.591470
[epoch17, step2351]: loss 0.401946
[epoch17, step2352]: loss 0.326271
[epoch17, step2353]: loss 0.329763
[epoch17, step2354]: loss 0.342593
[epoch17, step2355]: loss 0.459524
[epoch17, step2356]: loss 0.261838
[epoch17, step2357]: loss 0.114731
[epoch17, step2358]: loss 0.496037
[epoch17, step2359]: loss 0.515466
[epoch17, step2360]: loss 0.603585
[epoch17, step2361]: loss 0.355981
[epoch17, step2362]: loss 0.254754
[epoch17, step2363]: loss 0.514080
[epoch17, step2364]: loss 0.324071
[epoch17, step2365]: loss 0.408845
[epoch17, step2366]: loss 0.664764
[epoch17, step2367]: loss 0.453300
[epoch17, step2368]: loss 0.314451
[epoch17, step2369]: loss 0.422950
[epoch17, step2370]: loss 0.455946
[epoch17, step2371]: loss 0.593853
[epoch17, step2372]: loss 0.153822
[epoch17, step2373]: loss 0.442688
[epoch17, step2374]: loss 0.406492
[epoch17, step2375]: loss 0.584054
[epoch17, step2376]: loss 0.548124
[epoch17, step2377]: loss 0.562633
[epoch17, step2378]: loss 0.482430
[epoch17, step2379]: loss 0.435239
[epoch17, step2380]: loss 0.426225
[epoch17, step2381]: loss 0.495437
[epoch17, step2382]: loss 0.626437
[epoch17, step2383]: loss 0.440166
[epoch17, step2384]: loss 0.319538
[epoch17, step2385]: loss 0.419841
[epoch17, step2386]: loss 0.465250
[epoch17, step2387]: loss 0.560535
[epoch17, step2388]: loss 0.512341
[epoch17, step2389]: loss 0.506301
[epoch17, step2390]: loss 0.270942
[epoch17, step2391]: loss 0.388079
[epoch17, step2392]: loss 0.291712
[epoch17, step2393]: loss 0.405771
[epoch17, step2394]: loss 0.497878
[epoch17, step2395]: loss 0.213884
[epoch17, step2396]: loss 0.408272
[epoch17, step2397]: loss 0.647450
[epoch17, step2398]: loss 0.696654
[epoch17, step2399]: loss 0.558510
[epoch17, step2400]: loss 0.520159
[epoch17, step2401]: loss 0.527300
[epoch17, step2402]: loss 0.491574
[epoch17, step2403]: loss 0.579333
[epoch17, step2404]: loss 0.622352
[epoch17, step2405]: loss 0.329937
[epoch17, step2406]: loss 0.112187
[epoch17, step2407]: loss 0.335554
[epoch17, step2408]: loss 0.542871
[epoch17, step2409]: loss 0.375191
[epoch17, step2410]: loss 0.365475
[epoch17, step2411]: loss 0.610177
[epoch17, step2412]: loss 0.499448
[epoch17, step2413]: loss 0.538265
[epoch17, step2414]: loss 0.423731
[epoch17, step2415]: loss 0.651868
[epoch17, step2416]: loss 0.480272
[epoch17, step2417]: loss 0.446660
[epoch17, step2418]: loss 0.552853
[epoch17, step2419]: loss 0.395926
[epoch17, step2420]: loss 0.595026
[epoch17, step2421]: loss 0.686642
[epoch17, step2422]: loss 0.583955
[epoch17, step2423]: loss 0.590999
[epoch17, step2424]: loss 0.442457
[epoch17, step2425]: loss 0.535198
[epoch17, step2426]: loss 0.164339
[epoch17, step2427]: loss 0.586774
[epoch17, step2428]: loss 0.674639
[epoch17, step2429]: loss 0.609212
[epoch17, step2430]: loss 0.573184
[epoch17, step2431]: loss 0.492826
[epoch17, step2432]: loss 0.577553
[epoch17, step2433]: loss 0.468997
[epoch17, step2434]: loss 0.663188
[epoch17, step2435]: loss 0.498372
[epoch17, step2436]: loss 0.523840
[epoch17, step2437]: loss 0.179293
[epoch17, step2438]: loss 0.382888
[epoch17, step2439]: loss 0.501243
[epoch17, step2440]: loss 0.617854
[epoch17, step2441]: loss 0.540819
[epoch17, step2442]: loss 0.563544
[epoch17, step2443]: loss 0.476976
[epoch17, step2444]: loss 0.498951
[epoch17, step2445]: loss 0.235120
[epoch17, step2446]: loss 0.672128
[epoch17, step2447]: loss 0.178416
[epoch17, step2448]: loss 0.427574
[epoch17, step2449]: loss 0.614821
[epoch17, step2450]: loss 0.597120
[epoch17, step2451]: loss 0.473565
[epoch17, step2452]: loss 0.630599
[epoch17, step2453]: loss 0.510073
[epoch17, step2454]: loss 0.462986
[epoch17, step2455]: loss 0.234374
[epoch17, step2456]: loss 0.375073
[epoch17, step2457]: loss 0.570713
[epoch17, step2458]: loss 0.251424
[epoch17, step2459]: loss 0.526928
[epoch17, step2460]: loss 0.318372
[epoch17, step2461]: loss 0.501322
[epoch17, step2462]: loss 0.443678
[epoch17, step2463]: loss 0.573394
[epoch17, step2464]: loss 0.485823
[epoch17, step2465]: loss 0.732651
[epoch17, step2466]: loss 0.377056
[epoch17, step2467]: loss 0.748453
[epoch17, step2468]: loss 0.425445
[epoch17, step2469]: loss 0.658340
[epoch17, step2470]: loss 0.614508
[epoch17, step2471]: loss 0.514783
[epoch17, step2472]: loss 0.539430
[epoch17, step2473]: loss 0.577293
[epoch17, step2474]: loss 0.723670
[epoch17, step2475]: loss 0.139381
[epoch17, step2476]: loss 0.526793
[epoch17, step2477]: loss 0.506020
[epoch17, step2478]: loss 0.490129
[epoch17, step2479]: loss 0.420729
[epoch17, step2480]: loss 0.250037
[epoch17, step2481]: loss 0.460333
[epoch17, step2482]: loss 0.625432
[epoch17, step2483]: loss 0.550619
[epoch17, step2484]: loss 0.343146
[epoch17, step2485]: loss 0.624424
[epoch17, step2486]: loss 0.620118
[epoch17, step2487]: loss 0.359212
[epoch17, step2488]: loss 0.626615
[epoch17, step2489]: loss 0.568470
[epoch17, step2490]: loss 0.348848
[epoch17, step2491]: loss 0.562953
[epoch17, step2492]: loss 0.554994
[epoch17, step2493]: loss 0.298232
[epoch17, step2494]: loss 0.253781
[epoch17, step2495]: loss 0.149789
[epoch17, step2496]: loss 0.594529
[epoch17, step2497]: loss 0.576926
[epoch17, step2498]: loss 0.707503
[epoch17, step2499]: loss 0.446400
[epoch17, step2500]: loss 0.582901
[epoch17, step2501]: loss 0.539057
[epoch17, step2502]: loss 0.387400
[epoch17, step2503]: loss 0.396772
[epoch17, step2504]: loss 0.410744
[epoch17, step2505]: loss 0.255258
[epoch17, step2506]: loss 0.508165
[epoch17, step2507]: loss 0.253143
[epoch17, step2508]: loss 0.468250
[epoch17, step2509]: loss 0.446095
[epoch17, step2510]: loss 0.608239
[epoch17, step2511]: loss 0.596735
[epoch17, step2512]: loss 0.245419
[epoch17, step2513]: loss 0.281603
[epoch17, step2514]: loss 0.446980
[epoch17, step2515]: loss 0.648677
[epoch17, step2516]: loss 0.246154
[epoch17, step2517]: loss 0.686748
[epoch17, step2518]: loss 0.519591
[epoch17, step2519]: loss 0.299381
[epoch17, step2520]: loss 0.450120
[epoch17, step2521]: loss 0.448529
[epoch17, step2522]: loss 0.757299
[epoch17, step2523]: loss 0.626402
[epoch17, step2524]: loss 0.466550
[epoch17, step2525]: loss 0.635493
[epoch17, step2526]: loss 0.568289
[epoch17, step2527]: loss 0.509076
[epoch17, step2528]: loss 0.415493
[epoch17, step2529]: loss 0.548922
[epoch17, step2530]: loss 0.426649
[epoch17, step2531]: loss 0.671730
[epoch17, step2532]: loss 0.465809
[epoch17, step2533]: loss 0.242050
[epoch17, step2534]: loss 0.319726
[epoch17, step2535]: loss 0.657856
[epoch17, step2536]: loss 0.621821
[epoch17, step2537]: loss 0.638073
[epoch17, step2538]: loss 0.085026
[epoch17, step2539]: loss 0.519050
[epoch17, step2540]: loss 0.369344
[epoch17, step2541]: loss 0.248844
[epoch17, step2542]: loss 0.392724
[epoch17, step2543]: loss 0.479149
[epoch17, step2544]: loss 0.378686
[epoch17, step2545]: loss 0.687437
[epoch17, step2546]: loss 0.415871
[epoch17, step2547]: loss 0.233293
[epoch17, step2548]: loss 0.491959
[epoch17, step2549]: loss 0.525869
[epoch17, step2550]: loss 0.449789
[epoch17, step2551]: loss 0.551417
[epoch17, step2552]: loss 0.378580
[epoch17, step2553]: loss 0.437354
[epoch17, step2554]: loss 0.517957
[epoch17, step2555]: loss 0.512748
[epoch17, step2556]: loss 0.431244
[epoch17, step2557]: loss 0.670260
[epoch17, step2558]: loss 0.632662
[epoch17, step2559]: loss 0.382257
[epoch17, step2560]: loss 0.591994
[epoch17, step2561]: loss 0.449735
[epoch17, step2562]: loss 0.373685
[epoch17, step2563]: loss 0.435574
[epoch17, step2564]: loss 0.471798
[epoch17, step2565]: loss 0.371219
[epoch17, step2566]: loss 0.513256
[epoch17, step2567]: loss 0.291736
[epoch17, step2568]: loss 0.480629
[epoch17, step2569]: loss 0.586396
[epoch17, step2570]: loss 0.354562
[epoch17, step2571]: loss 0.375994
[epoch17, step2572]: loss 0.557162
[epoch17, step2573]: loss 0.337438
[epoch17, step2574]: loss 0.456213
[epoch17, step2575]: loss 0.328729
[epoch17, step2576]: loss 0.777452
[epoch17, step2577]: loss 0.512560
[epoch17, step2578]: loss 0.597524
[epoch17, step2579]: loss 0.321951
[epoch17, step2580]: loss 0.334697
[epoch17, step2581]: loss 0.454258
[epoch17, step2582]: loss 0.359787
[epoch17, step2583]: loss 0.606317
[epoch17, step2584]: loss 0.533212
[epoch17, step2585]: loss 0.242551
[epoch17, step2586]: loss 0.419546
[epoch17, step2587]: loss 0.606603
[epoch17, step2588]: loss 0.382757
[epoch17, step2589]: loss 0.373201
[epoch17, step2590]: loss 0.415140
[epoch17, step2591]: loss 0.610302
[epoch17, step2592]: loss 0.366728
[epoch17, step2593]: loss 0.646668
[epoch17, step2594]: loss 0.777089
[epoch17, step2595]: loss 0.583387
[epoch17, step2596]: loss 0.669360
[epoch17, step2597]: loss 0.335804
[epoch17, step2598]: loss 0.396065
[epoch17, step2599]: loss 0.555920
[epoch17, step2600]: loss 0.547985
[epoch17, step2601]: loss 0.727633
[epoch17, step2602]: loss 0.450126
[epoch17, step2603]: loss 0.479652
[epoch17, step2604]: loss 0.488716
[epoch17, step2605]: loss 0.495143
[epoch17, step2606]: loss 0.568311
[epoch17, step2607]: loss 0.473114
[epoch17, step2608]: loss 0.421319
[epoch17, step2609]: loss 0.446912
[epoch17, step2610]: loss 0.324563
[epoch17, step2611]: loss 0.270368
[epoch17, step2612]: loss 0.574131
[epoch17, step2613]: loss 0.581313
[epoch17, step2614]: loss 0.461779
[epoch17, step2615]: loss 0.556293
[epoch17, step2616]: loss 0.558884
[epoch17, step2617]: loss 0.597730
[epoch17, step2618]: loss 0.618772
[epoch17, step2619]: loss 0.647854
[epoch17, step2620]: loss 0.459921
[epoch17, step2621]: loss 0.480501
[epoch17, step2622]: loss 0.516633
[epoch17, step2623]: loss 0.464141
[epoch17, step2624]: loss 0.403560
[epoch17, step2625]: loss 0.406992
[epoch17, step2626]: loss 0.283756
[epoch17, step2627]: loss 0.527320
[epoch17, step2628]: loss 0.685287
[epoch17, step2629]: loss 0.344867
[epoch17, step2630]: loss 0.353574
[epoch17, step2631]: loss 0.401529
[epoch17, step2632]: loss 0.386780
[epoch17, step2633]: loss 0.445109
[epoch17, step2634]: loss 0.351329
[epoch17, step2635]: loss 0.348868
[epoch17, step2636]: loss 0.368691
[epoch17, step2637]: loss 0.423805
[epoch17, step2638]: loss 0.119174
[epoch17, step2639]: loss 0.367112
[epoch17, step2640]: loss 0.340846
[epoch17, step2641]: loss 0.410404
[epoch17, step2642]: loss 0.502095
[epoch17, step2643]: loss 0.510889
[epoch17, step2644]: loss 0.570951
[epoch17, step2645]: loss 0.536160
[epoch17, step2646]: loss 0.434503
[epoch17, step2647]: loss 0.404299
[epoch17, step2648]: loss 0.564108
[epoch17, step2649]: loss 0.541689
[epoch17, step2650]: loss 0.456069
[epoch17, step2651]: loss 0.305854
[epoch17, step2652]: loss 0.420617
[epoch17, step2653]: loss 0.557586
[epoch17, step2654]: loss 0.488637
[epoch17, step2655]: loss 0.553376
[epoch17, step2656]: loss 0.320698
[epoch17, step2657]: loss 0.477486
[epoch17, step2658]: loss 0.595240
[epoch17, step2659]: loss 0.397166
[epoch17, step2660]: loss 0.564704
[epoch17, step2661]: loss 0.563400
[epoch17, step2662]: loss 0.378772
[epoch17, step2663]: loss 0.578805
[epoch17, step2664]: loss 0.592883
[epoch17, step2665]: loss 0.277588
[epoch17, step2666]: loss 0.599847
[epoch17, step2667]: loss 0.440128
[epoch17, step2668]: loss 0.349548
[epoch17, step2669]: loss 0.316843
[epoch17, step2670]: loss 0.487111
[epoch17, step2671]: loss 0.609014
[epoch17, step2672]: loss 0.516091
[epoch17, step2673]: loss 0.361135
[epoch17, step2674]: loss 0.453737
[epoch17, step2675]: loss 0.351608
[epoch17, step2676]: loss 0.558392
[epoch17, step2677]: loss 0.610270
[epoch17, step2678]: loss 0.278290
[epoch17, step2679]: loss 0.559834
[epoch17, step2680]: loss 0.565672
[epoch17, step2681]: loss 0.464903
[epoch17, step2682]: loss 0.421194
[epoch17, step2683]: loss 0.637127
[epoch17, step2684]: loss 0.657212
[epoch17, step2685]: loss 0.633908
[epoch17, step2686]: loss 0.436945
[epoch17, step2687]: loss 0.529632
[epoch17, step2688]: loss 0.463124
[epoch17, step2689]: loss 0.396526
[epoch17, step2690]: loss 0.480775
[epoch17, step2691]: loss 0.466563
[epoch17, step2692]: loss 0.604600
[epoch17, step2693]: loss 0.418653
[epoch17, step2694]: loss 0.425679
[epoch17, step2695]: loss 0.546779
[epoch17, step2696]: loss 0.456396
[epoch17, step2697]: loss 0.353349
[epoch17, step2698]: loss 0.714664
[epoch17, step2699]: loss 0.539675
[epoch17, step2700]: loss 0.578471
[epoch17, step2701]: loss 0.239779
[epoch17, step2702]: loss 0.517433
[epoch17, step2703]: loss 0.238895
[epoch17, step2704]: loss 0.467546
[epoch17, step2705]: loss 0.688288
[epoch17, step2706]: loss 0.505408
[epoch17, step2707]: loss 0.387355
[epoch17, step2708]: loss 0.324656
[epoch17, step2709]: loss 0.493883
[epoch17, step2710]: loss 0.548720
[epoch17, step2711]: loss 0.472856
[epoch17, step2712]: loss 0.587954
[epoch17, step2713]: loss 0.367363
[epoch17, step2714]: loss 0.445180
[epoch17, step2715]: loss 0.563376
[epoch17, step2716]: loss 0.509502
[epoch17, step2717]: loss 0.588843
[epoch17, step2718]: loss 0.508774
[epoch17, step2719]: loss 0.680328
[epoch17, step2720]: loss 0.488850
[epoch17, step2721]: loss 0.523391
[epoch17, step2722]: loss 0.493236
[epoch17, step2723]: loss 0.408423
[epoch17, step2724]: loss 0.474752
[epoch17, step2725]: loss 0.546168
[epoch17, step2726]: loss 0.601715
[epoch17, step2727]: loss 0.452619
[epoch17, step2728]: loss 0.643974
[epoch17, step2729]: loss 0.582397
[epoch17, step2730]: loss 0.395038
[epoch17, step2731]: loss 0.515805
[epoch17, step2732]: loss 0.382005
[epoch17, step2733]: loss 0.285053
[epoch17, step2734]: loss 0.121630
[epoch17, step2735]: loss 0.373944
[epoch17, step2736]: loss 0.378304
[epoch17, step2737]: loss 0.606771
[epoch17, step2738]: loss 0.340085
[epoch17, step2739]: loss 0.399668
[epoch17, step2740]: loss 0.563659
[epoch17, step2741]: loss 0.453901
[epoch17, step2742]: loss 0.250458
[epoch17, step2743]: loss 0.352535
[epoch17, step2744]: loss 0.364752
[epoch17, step2745]: loss 0.589517
[epoch17, step2746]: loss 0.373800
[epoch17, step2747]: loss 0.551809
[epoch17, step2748]: loss 0.195563
[epoch17, step2749]: loss 0.505668
[epoch17, step2750]: loss 0.452358
[epoch17, step2751]: loss 0.645423
[epoch17, step2752]: loss 0.462083
[epoch17, step2753]: loss 0.357995
[epoch17, step2754]: loss 0.499420
[epoch17, step2755]: loss 0.598461
[epoch17, step2756]: loss 0.666624
[epoch17, step2757]: loss 0.352353
[epoch17, step2758]: loss 0.635313
[epoch17, step2759]: loss 0.389353
[epoch17, step2760]: loss 0.273001
[epoch17, step2761]: loss 0.400021
[epoch17, step2762]: loss 0.506699
[epoch17, step2763]: loss 0.255699
[epoch17, step2764]: loss 0.317030
[epoch17, step2765]: loss 0.350941
[epoch17, step2766]: loss 0.682066
[epoch17, step2767]: loss 0.564320
[epoch17, step2768]: loss 0.416433
[epoch17, step2769]: loss 0.373184
[epoch17, step2770]: loss 0.422995
[epoch17, step2771]: loss 0.251741
[epoch17, step2772]: loss 0.604485
[epoch17, step2773]: loss 0.559630
[epoch17, step2774]: loss 0.533348
[epoch17, step2775]: loss 0.441690
[epoch17, step2776]: loss 0.733391
[epoch17, step2777]: loss 0.416732
[epoch17, step2778]: loss 0.353119
[epoch17, step2779]: loss 0.133892
[epoch17, step2780]: loss 0.404191
[epoch17, step2781]: loss 0.453493
[epoch17, step2782]: loss 0.550321
[epoch17, step2783]: loss 0.442771
[epoch17, step2784]: loss 0.340731
[epoch17, step2785]: loss 0.655196
[epoch17, step2786]: loss 0.427504
[epoch17, step2787]: loss 0.384941
[epoch17, step2788]: loss 0.481766
[epoch17, step2789]: loss 0.558790
[epoch17, step2790]: loss 0.414318
[epoch17, step2791]: loss 0.212607
[epoch17, step2792]: loss 0.529250
[epoch17, step2793]: loss 0.323741
[epoch17, step2794]: loss 0.661271
[epoch17, step2795]: loss 0.361078
[epoch17, step2796]: loss 0.468730
[epoch17, step2797]: loss 0.302611
[epoch17, step2798]: loss 0.544360
[epoch17, step2799]: loss 0.356241
[epoch17, step2800]: loss 0.459346
[epoch17, step2801]: loss 0.468807
[epoch17, step2802]: loss 0.442605
[epoch17, step2803]: loss 0.396440
[epoch17, step2804]: loss 0.551184
[epoch17, step2805]: loss 0.552756
[epoch17, step2806]: loss 0.373282
[epoch17, step2807]: loss 0.441859
[epoch17, step2808]: loss 0.372495
[epoch17, step2809]: loss 0.605900
[epoch17, step2810]: loss 0.513384
[epoch17, step2811]: loss 0.486864
[epoch17, step2812]: loss 0.426611
[epoch17, step2813]: loss 0.456273
[epoch17, step2814]: loss 0.424398
[epoch17, step2815]: loss 0.566716
[epoch17, step2816]: loss 0.568440
[epoch17, step2817]: loss 0.315623
[epoch17, step2818]: loss 0.575593
[epoch17, step2819]: loss 0.522768
[epoch17, step2820]: loss 0.175904
[epoch17, step2821]: loss 0.442928
[epoch17, step2822]: loss 0.564598
[epoch17, step2823]: loss 0.673882
[epoch17, step2824]: loss 0.269548
[epoch17, step2825]: loss 0.454079
[epoch17, step2826]: loss 0.663610
[epoch17, step2827]: loss 0.567428
[epoch17, step2828]: loss 0.501449
[epoch17, step2829]: loss 0.512291
[epoch17, step2830]: loss 0.479602
[epoch17, step2831]: loss 0.690220
[epoch17, step2832]: loss 0.365503
[epoch17, step2833]: loss 0.363843
[epoch17, step2834]: loss 0.396104
[epoch17, step2835]: loss 0.309479
[epoch17, step2836]: loss 0.376232
[epoch17, step2837]: loss 0.503340
[epoch17, step2838]: loss 0.417050
[epoch17, step2839]: loss 0.424508
[epoch17, step2840]: loss 0.360628
[epoch17, step2841]: loss 0.346344
[epoch17, step2842]: loss 0.158729
[epoch17, step2843]: loss 0.646134
[epoch17, step2844]: loss 0.520402
[epoch17, step2845]: loss 0.567257
[epoch17, step2846]: loss 0.637027
[epoch17, step2847]: loss 0.650482
[epoch17, step2848]: loss 0.458546
[epoch17, step2849]: loss 0.460834
[epoch17, step2850]: loss 0.441262
[epoch17, step2851]: loss 0.343148
[epoch17, step2852]: loss 0.626660
[epoch17, step2853]: loss 0.489575
[epoch17, step2854]: loss 0.551118
[epoch17, step2855]: loss 0.578583
[epoch17, step2856]: loss 0.310445
[epoch17, step2857]: loss 0.460648
[epoch17, step2858]: loss 0.530455
[epoch17, step2859]: loss 0.188386
[epoch17, step2860]: loss 0.322060
[epoch17, step2861]: loss 0.709869
[epoch17, step2862]: loss 0.483269
[epoch17, step2863]: loss 0.361517
[epoch17, step2864]: loss 0.464273
[epoch17, step2865]: loss 0.594517
[epoch17, step2866]: loss 0.342068
[epoch17, step2867]: loss 0.626638
[epoch17, step2868]: loss 0.314803
[epoch17, step2869]: loss 0.452663
[epoch17, step2870]: loss 0.764081
[epoch17, step2871]: loss 0.580635
[epoch17, step2872]: loss 0.565377
[epoch17, step2873]: loss 0.729235
[epoch17, step2874]: loss 0.444727
[epoch17, step2875]: loss 0.406948
[epoch17, step2876]: loss 0.561719
[epoch17, step2877]: loss 0.676971
[epoch17, step2878]: loss 0.426610
[epoch17, step2879]: loss 0.540293
[epoch17, step2880]: loss 0.448027
[epoch17, step2881]: loss 0.630851
[epoch17, step2882]: loss 0.612392
[epoch17, step2883]: loss 0.683370
[epoch17, step2884]: loss 0.519865
[epoch17, step2885]: loss 0.608358
[epoch17, step2886]: loss 0.286807
[epoch17, step2887]: loss 0.338033
[epoch17, step2888]: loss 0.350001
[epoch17, step2889]: loss 0.436027
[epoch17, step2890]: loss 0.395232
[epoch17, step2891]: loss 0.459923
[epoch17, step2892]: loss 0.527408
[epoch17, step2893]: loss 0.281648
[epoch17, step2894]: loss 0.425453
[epoch17, step2895]: loss 0.695168
[epoch17, step2896]: loss 0.373510
[epoch17, step2897]: loss 0.416053
[epoch17, step2898]: loss 0.379178
[epoch17, step2899]: loss 0.710651
[epoch17, step2900]: loss 0.530379
[epoch17, step2901]: loss 0.434312
[epoch17, step2902]: loss 0.583283
[epoch17, step2903]: loss 0.412777
[epoch17, step2904]: loss 0.532983
[epoch17, step2905]: loss 0.360993
[epoch17, step2906]: loss 0.440539
[epoch17, step2907]: loss 0.469921
[epoch17, step2908]: loss 0.374705
[epoch17, step2909]: loss 0.702747
[epoch17, step2910]: loss 0.545000
[epoch17, step2911]: loss 0.605995
[epoch17, step2912]: loss 0.493440
[epoch17, step2913]: loss 0.527509
[epoch17, step2914]: loss 0.263831
[epoch17, step2915]: loss 0.599279
[epoch17, step2916]: loss 0.639991
[epoch17, step2917]: loss 0.142477
[epoch17, step2918]: loss 0.348233
[epoch17, step2919]: loss 0.399669
[epoch17, step2920]: loss 0.450090
[epoch17, step2921]: loss 0.449759
[epoch17, step2922]: loss 0.551039
[epoch17, step2923]: loss 0.354548
[epoch17, step2924]: loss 0.615602
[epoch17, step2925]: loss 0.586704
[epoch17, step2926]: loss 0.537060
[epoch17, step2927]: loss 0.449683
[epoch17, step2928]: loss 0.391403
[epoch17, step2929]: loss 0.361171
[epoch17, step2930]: loss 0.399929
[epoch17, step2931]: loss 0.582089
[epoch17, step2932]: loss 0.270189
[epoch17, step2933]: loss 0.490368
[epoch17, step2934]: loss 0.439491
[epoch17, step2935]: loss 0.545061
[epoch17, step2936]: loss 0.417290
[epoch17, step2937]: loss 0.444190
[epoch17, step2938]: loss 0.453108
[epoch17, step2939]: loss 0.476989
[epoch17, step2940]: loss 0.410520
[epoch17, step2941]: loss 0.654450
[epoch17, step2942]: loss 0.511258
[epoch17, step2943]: loss 0.658600
[epoch17, step2944]: loss 0.509869
[epoch17, step2945]: loss 0.531586
[epoch17, step2946]: loss 0.420661
[epoch17, step2947]: loss 0.383423
[epoch17, step2948]: loss 0.254248
[epoch17, step2949]: loss 0.326498
[epoch17, step2950]: loss 0.326955
[epoch17, step2951]: loss 0.604393
[epoch17, step2952]: loss 0.662047
[epoch17, step2953]: loss 0.454048
[epoch17, step2954]: loss 0.565374
[epoch17, step2955]: loss 0.379755
[epoch17, step2956]: loss 0.557679
[epoch17, step2957]: loss 0.368889
[epoch17, step2958]: loss 0.592259
[epoch17, step2959]: loss 0.641380
[epoch17, step2960]: loss 0.490337
[epoch17, step2961]: loss 0.500612
[epoch17, step2962]: loss 0.554124
[epoch17, step2963]: loss 0.558945
[epoch17, step2964]: loss 0.398658
[epoch17, step2965]: loss 0.418530
[epoch17, step2966]: loss 0.617987
[epoch17, step2967]: loss 0.438832
[epoch17, step2968]: loss 0.486364
[epoch17, step2969]: loss 0.516439
[epoch17, step2970]: loss 0.393472
[epoch17, step2971]: loss 0.490483
[epoch17, step2972]: loss 0.331381
[epoch17, step2973]: loss 0.397943
[epoch17, step2974]: loss 0.408866
[epoch17, step2975]: loss 0.586775
[epoch17, step2976]: loss 0.635866
[epoch17, step2977]: loss 0.575755
[epoch17, step2978]: loss 0.559580
[epoch17, step2979]: loss 0.280490
[epoch17, step2980]: loss 0.505170
[epoch17, step2981]: loss 0.380408
[epoch17, step2982]: loss 0.481777
[epoch17, step2983]: loss 0.441120
[epoch17, step2984]: loss 0.660154
[epoch17, step2985]: loss 0.439152
[epoch17, step2986]: loss 0.390286
[epoch17, step2987]: loss 0.332419
[epoch17, step2988]: loss 0.360957
[epoch17, step2989]: loss 0.379573
[epoch17, step2990]: loss 0.406192
[epoch17, step2991]: loss 0.620561
[epoch17, step2992]: loss 0.387393
[epoch17, step2993]: loss 0.554935
[epoch17, step2994]: loss 0.483372
[epoch17, step2995]: loss 0.351820
[epoch17, step2996]: loss 0.647855
[epoch17, step2997]: loss 0.544617
[epoch17, step2998]: loss 0.147272
[epoch17, step2999]: loss 0.521356
[epoch17, step3000]: loss 0.406058
[epoch17, step3001]: loss 0.439380
[epoch17, step3002]: loss 0.582030
[epoch17, step3003]: loss 0.698494
[epoch17, step3004]: loss 0.403049
[epoch17, step3005]: loss 0.243468
[epoch17, step3006]: loss 0.571379
[epoch17, step3007]: loss 0.262545
[epoch17, step3008]: loss 0.601674
[epoch17, step3009]: loss 0.421639
[epoch17, step3010]: loss 0.317858
[epoch17, step3011]: loss 0.636051
[epoch17, step3012]: loss 0.381839
[epoch17, step3013]: loss 0.550764
[epoch17, step3014]: loss 0.365165
[epoch17, step3015]: loss 0.486079
[epoch17, step3016]: loss 0.664431
[epoch17, step3017]: loss 0.240312
[epoch17, step3018]: loss 0.366714
[epoch17, step3019]: loss 0.506469
[epoch17, step3020]: loss 0.495565
[epoch17, step3021]: loss 0.502527
[epoch17, step3022]: loss 0.563474
[epoch17, step3023]: loss 0.260732
[epoch17, step3024]: loss 0.439392
[epoch17, step3025]: loss 0.337037
[epoch17, step3026]: loss 0.304411
[epoch17, step3027]: loss 0.468663
[epoch17, step3028]: loss 0.486151
[epoch17, step3029]: loss 0.439447
[epoch17, step3030]: loss 0.405827
[epoch17, step3031]: loss 0.545998
[epoch17, step3032]: loss 0.500158
[epoch17, step3033]: loss 0.399700
[epoch17, step3034]: loss 0.268747
[epoch17, step3035]: loss 0.663740
[epoch17, step3036]: loss 0.523861
[epoch17, step3037]: loss 0.620961
[epoch17, step3038]: loss 0.494927
[epoch17, step3039]: loss 0.580898
[epoch17, step3040]: loss 0.469316
[epoch17, step3041]: loss 0.328128
[epoch17, step3042]: loss 0.555784
[epoch17, step3043]: loss 0.437716
[epoch17, step3044]: loss 0.370619
[epoch17, step3045]: loss 0.518611
[epoch17, step3046]: loss 0.414221
[epoch17, step3047]: loss 0.483166
[epoch17, step3048]: loss 0.600833
[epoch17, step3049]: loss 0.596110
[epoch17, step3050]: loss 0.514606
[epoch17, step3051]: loss 0.557719
[epoch17, step3052]: loss 0.440258
[epoch17, step3053]: loss 0.557753
[epoch17, step3054]: loss 0.246099
[epoch17, step3055]: loss 0.557148
[epoch17, step3056]: loss 0.623879
[epoch17, step3057]: loss 0.538301
[epoch17, step3058]: loss 0.485018
[epoch17, step3059]: loss 0.873674
[epoch17, step3060]: loss 0.375636
[epoch17, step3061]: loss 0.277818
[epoch17, step3062]: loss 0.688643
[epoch17, step3063]: loss 0.343802
[epoch17, step3064]: loss 0.467099
[epoch17, step3065]: loss 0.570074
[epoch17, step3066]: loss 0.494710
[epoch17, step3067]: loss 0.405005
[epoch17, step3068]: loss 0.463252
[epoch17, step3069]: loss 0.243750
[epoch17, step3070]: loss 0.626837
[epoch17, step3071]: loss 0.788822
[epoch17, step3072]: loss 0.327391
[epoch17, step3073]: loss 0.515979
[epoch17, step3074]: loss 0.327458
[epoch17, step3075]: loss 0.541181
[epoch17, step3076]: loss 0.276481

[epoch17]: avg loss 0.276481

[epoch18, step1]: loss 0.544484
[epoch18, step2]: loss 0.494612
[epoch18, step3]: loss 0.466751
[epoch18, step4]: loss 0.786386
[epoch18, step5]: loss 0.437720
[epoch18, step6]: loss 0.693051
[epoch18, step7]: loss 0.359473
[epoch18, step8]: loss 0.503518
[epoch18, step9]: loss 0.335517
[epoch18, step10]: loss 0.495015
[epoch18, step11]: loss 0.571040
[epoch18, step12]: loss 0.439059
[epoch18, step13]: loss 0.442851
[epoch18, step14]: loss 0.578572
[epoch18, step15]: loss 0.392298
[epoch18, step16]: loss 0.409886
[epoch18, step17]: loss 0.521137
[epoch18, step18]: loss 0.689227
[epoch18, step19]: loss 0.468431
[epoch18, step20]: loss 0.559183
[epoch18, step21]: loss 0.600666
[epoch18, step22]: loss 0.515405
[epoch18, step23]: loss 0.452878
[epoch18, step24]: loss 0.519410
[epoch18, step25]: loss 0.538751
[epoch18, step26]: loss 0.619806
[epoch18, step27]: loss 0.468924
[epoch18, step28]: loss 0.201984
[epoch18, step29]: loss 0.511782
[epoch18, step30]: loss 0.365385
[epoch18, step31]: loss 0.635671
[epoch18, step32]: loss 0.455755
[epoch18, step33]: loss 0.606252
[epoch18, step34]: loss 0.464942
[epoch18, step35]: loss 0.536478
[epoch18, step36]: loss 0.258615
[epoch18, step37]: loss 0.522690
[epoch18, step38]: loss 0.364799
[epoch18, step39]: loss 0.172402
[epoch18, step40]: loss 0.410866
[epoch18, step41]: loss 0.520402
[epoch18, step42]: loss 0.324631
[epoch18, step43]: loss 0.476157
[epoch18, step44]: loss 0.426746
[epoch18, step45]: loss 0.731615
[epoch18, step46]: loss 0.392882
[epoch18, step47]: loss 0.594121
[epoch18, step48]: loss 0.401723
[epoch18, step49]: loss 0.428442
[epoch18, step50]: loss 0.412016
[epoch18, step51]: loss 0.373919
[epoch18, step52]: loss 0.395902
[epoch18, step53]: loss 0.531190
[epoch18, step54]: loss 0.665329
[epoch18, step55]: loss 0.326472
[epoch18, step56]: loss 0.529418
[epoch18, step57]: loss 0.558674
[epoch18, step58]: loss 0.567058
[epoch18, step59]: loss 0.342116
[epoch18, step60]: loss 0.460833
[epoch18, step61]: loss 0.664643
[epoch18, step62]: loss 0.479046
[epoch18, step63]: loss 0.424227
[epoch18, step64]: loss 0.419050
[epoch18, step65]: loss 0.485667
[epoch18, step66]: loss 0.396165
[epoch18, step67]: loss 0.402628
[epoch18, step68]: loss 0.497609
[epoch18, step69]: loss 0.587514
[epoch18, step70]: loss 0.569295
[epoch18, step71]: loss 0.410202
[epoch18, step72]: loss 0.578700
[epoch18, step73]: loss 0.459800
[epoch18, step74]: loss 0.494663
[epoch18, step75]: loss 0.267911
[epoch18, step76]: loss 0.141295
[epoch18, step77]: loss 0.534580
[epoch18, step78]: loss 0.457869
[epoch18, step79]: loss 0.208074
[epoch18, step80]: loss 0.561290
[epoch18, step81]: loss 0.424383
[epoch18, step82]: loss 0.648222
[epoch18, step83]: loss 0.107363
[epoch18, step84]: loss 0.462649
[epoch18, step85]: loss 0.536339
[epoch18, step86]: loss 0.502231
[epoch18, step87]: loss 0.295881
[epoch18, step88]: loss 0.636392
[epoch18, step89]: loss 0.717349
[epoch18, step90]: loss 0.453051
[epoch18, step91]: loss 0.406106
[epoch18, step92]: loss 0.647470
[epoch18, step93]: loss 0.479831
[epoch18, step94]: loss 0.546322
[epoch18, step95]: loss 0.694822
[epoch18, step96]: loss 0.416830
[epoch18, step97]: loss 0.580102
[epoch18, step98]: loss 0.608538
[epoch18, step99]: loss 0.449524
[epoch18, step100]: loss 0.359534
[epoch18, step101]: loss 0.578126
[epoch18, step102]: loss 0.454235
[epoch18, step103]: loss 0.570265
[epoch18, step104]: loss 0.341782
[epoch18, step105]: loss 0.382817
[epoch18, step106]: loss 0.655596
[epoch18, step107]: loss 0.343047
[epoch18, step108]: loss 0.238355
[epoch18, step109]: loss 0.594559
[epoch18, step110]: loss 0.520652
[epoch18, step111]: loss 0.354252
[epoch18, step112]: loss 0.519045
[epoch18, step113]: loss 0.547407
[epoch18, step114]: loss 0.567051
[epoch18, step115]: loss 0.434099
[epoch18, step116]: loss 0.662846
[epoch18, step117]: loss 0.596144
[epoch18, step118]: loss 0.565690
[epoch18, step119]: loss 0.610089
[epoch18, step120]: loss 0.583756
[epoch18, step121]: loss 0.513363
[epoch18, step122]: loss 0.377260
[epoch18, step123]: loss 0.616395
[epoch18, step124]: loss 0.275155
[epoch18, step125]: loss 0.431057
[epoch18, step126]: loss 0.373986
[epoch18, step127]: loss 0.611034
[epoch18, step128]: loss 0.510791
[epoch18, step129]: loss 0.482774
[epoch18, step130]: loss 0.580820
[epoch18, step131]: loss 0.516176
[epoch18, step132]: loss 0.631723
[epoch18, step133]: loss 0.488018
[epoch18, step134]: loss 0.521996
[epoch18, step135]: loss 0.591777
[epoch18, step136]: loss 0.494889
[epoch18, step137]: loss 0.385202
[epoch18, step138]: loss 0.262596
[epoch18, step139]: loss 0.422045
[epoch18, step140]: loss 0.455876
[epoch18, step141]: loss 0.307429
[epoch18, step142]: loss 0.426014
[epoch18, step143]: loss 0.441898
[epoch18, step144]: loss 0.444463
[epoch18, step145]: loss 0.651954
[epoch18, step146]: loss 0.495676
[epoch18, step147]: loss 0.426423
[epoch18, step148]: loss 0.414908
[epoch18, step149]: loss 0.571851
[epoch18, step150]: loss 0.634607
[epoch18, step151]: loss 0.525639
[epoch18, step152]: loss 0.342611
[epoch18, step153]: loss 0.253854
[epoch18, step154]: loss 0.634734
[epoch18, step155]: loss 0.497533
[epoch18, step156]: loss 0.624684
[epoch18, step157]: loss 0.581490
[epoch18, step158]: loss 0.589946
[epoch18, step159]: loss 0.368705
[epoch18, step160]: loss 0.342754
[epoch18, step161]: loss 0.416527
[epoch18, step162]: loss 0.341520
[epoch18, step163]: loss 0.551028
[epoch18, step164]: loss 0.489749
[epoch18, step165]: loss 0.568120
[epoch18, step166]: loss 0.498738
[epoch18, step167]: loss 0.652342
[epoch18, step168]: loss 0.431050
[epoch18, step169]: loss 0.499260
[epoch18, step170]: loss 0.417822
[epoch18, step171]: loss 0.429340
[epoch18, step172]: loss 0.568253
[epoch18, step173]: loss 0.470356
[epoch18, step174]: loss 0.550468
[epoch18, step175]: loss 0.363478
[epoch18, step176]: loss 0.293811
[epoch18, step177]: loss 0.370295
[epoch18, step178]: loss 0.437936
[epoch18, step179]: loss 0.592731
[epoch18, step180]: loss 0.515727
[epoch18, step181]: loss 0.448276
[epoch18, step182]: loss 0.612211
[epoch18, step183]: loss 0.583105
[epoch18, step184]: loss 0.473550
[epoch18, step185]: loss 0.523902
[epoch18, step186]: loss 0.595530
[epoch18, step187]: loss 0.263814
[epoch18, step188]: loss 0.674575
[epoch18, step189]: loss 0.318778
[epoch18, step190]: loss 0.261815
[epoch18, step191]: loss 0.402827
[epoch18, step192]: loss 0.238023
[epoch18, step193]: loss 0.408127
[epoch18, step194]: loss 0.332320
[epoch18, step195]: loss 0.299900
[epoch18, step196]: loss 0.557334
[epoch18, step197]: loss 0.272317
[epoch18, step198]: loss 0.411516
[epoch18, step199]: loss 0.322076
[epoch18, step200]: loss 0.600048
[epoch18, step201]: loss 0.473599
[epoch18, step202]: loss 0.320081
[epoch18, step203]: loss 0.459358
[epoch18, step204]: loss 0.115832
[epoch18, step205]: loss 0.483706
[epoch18, step206]: loss 0.453925
[epoch18, step207]: loss 0.589228
[epoch18, step208]: loss 0.331392
[epoch18, step209]: loss 0.716396
[epoch18, step210]: loss 0.533973
[epoch18, step211]: loss 0.349010
[epoch18, step212]: loss 0.415529
[epoch18, step213]: loss 0.461501
[epoch18, step214]: loss 0.521470
[epoch18, step215]: loss 0.427620
[epoch18, step216]: loss 0.260008
[epoch18, step217]: loss 0.427425
[epoch18, step218]: loss 0.721658
[epoch18, step219]: loss 0.468156
[epoch18, step220]: loss 0.645054
[epoch18, step221]: loss 0.506049
[epoch18, step222]: loss 0.571803
[epoch18, step223]: loss 0.358057
[epoch18, step224]: loss 0.288554
[epoch18, step225]: loss 0.483872
[epoch18, step226]: loss 0.481044
[epoch18, step227]: loss 0.528406
[epoch18, step228]: loss 0.537139
[epoch18, step229]: loss 0.508146
[epoch18, step230]: loss 0.333099
[epoch18, step231]: loss 0.682962
[epoch18, step232]: loss 0.376248
[epoch18, step233]: loss 0.460485
[epoch18, step234]: loss 0.543252
[epoch18, step235]: loss 0.568803
[epoch18, step236]: loss 0.417183
[epoch18, step237]: loss 0.575326
[epoch18, step238]: loss 0.477290
[epoch18, step239]: loss 0.503270
[epoch18, step240]: loss 0.368308
[epoch18, step241]: loss 0.362642
[epoch18, step242]: loss 0.740264
[epoch18, step243]: loss 0.396686
[epoch18, step244]: loss 0.366161
[epoch18, step245]: loss 0.539088
[epoch18, step246]: loss 0.608042
[epoch18, step247]: loss 0.547403
[epoch18, step248]: loss 0.224079
[epoch18, step249]: loss 0.393707
[epoch18, step250]: loss 0.415449
[epoch18, step251]: loss 0.346908
[epoch18, step252]: loss 0.656262
[epoch18, step253]: loss 0.365739
[epoch18, step254]: loss 0.409291
[epoch18, step255]: loss 0.391484
[epoch18, step256]: loss 0.541195
[epoch18, step257]: loss 0.573997
[epoch18, step258]: loss 0.502431
[epoch18, step259]: loss 0.377743
[epoch18, step260]: loss 0.613387
[epoch18, step261]: loss 0.599865
[epoch18, step262]: loss 0.254138
[epoch18, step263]: loss 0.520546
[epoch18, step264]: loss 0.527864
[epoch18, step265]: loss 0.461230
[epoch18, step266]: loss 0.420142
[epoch18, step267]: loss 0.458942
[epoch18, step268]: loss 0.267206
[epoch18, step269]: loss 0.434759
[epoch18, step270]: loss 0.438561
[epoch18, step271]: loss 0.088741
[epoch18, step272]: loss 0.406009
[epoch18, step273]: loss 0.391253
[epoch18, step274]: loss 0.472550
[epoch18, step275]: loss 0.305118
[epoch18, step276]: loss 0.500068
[epoch18, step277]: loss 0.343381
[epoch18, step278]: loss 0.427214
[epoch18, step279]: loss 0.406100
[epoch18, step280]: loss 0.549923
[epoch18, step281]: loss 0.504491
[epoch18, step282]: loss 0.591879
[epoch18, step283]: loss 0.430724
[epoch18, step284]: loss 0.425673
[epoch18, step285]: loss 0.447870
[epoch18, step286]: loss 0.603111
[epoch18, step287]: loss 0.500801
[epoch18, step288]: loss 0.479535
[epoch18, step289]: loss 0.529342
[epoch18, step290]: loss 0.527735
[epoch18, step291]: loss 0.246681
[epoch18, step292]: loss 0.546157
[epoch18, step293]: loss 0.424374
[epoch18, step294]: loss 0.458422
[epoch18, step295]: loss 0.562782
[epoch18, step296]: loss 0.440235
[epoch18, step297]: loss 0.382290
[epoch18, step298]: loss 0.491426
[epoch18, step299]: loss 0.508139
[epoch18, step300]: loss 0.572821
[epoch18, step301]: loss 0.427818
[epoch18, step302]: loss 0.456003
[epoch18, step303]: loss 0.320942
[epoch18, step304]: loss 0.434850
[epoch18, step305]: loss 0.303718
[epoch18, step306]: loss 0.383110
[epoch18, step307]: loss 0.528485
[epoch18, step308]: loss 0.354939
[epoch18, step309]: loss 0.565299
[epoch18, step310]: loss 0.502269
[epoch18, step311]: loss 0.503475
[epoch18, step312]: loss 0.452597
[epoch18, step313]: loss 0.569722
[epoch18, step314]: loss 0.510063
[epoch18, step315]: loss 0.606832
[epoch18, step316]: loss 0.419556
[epoch18, step317]: loss 0.398966
[epoch18, step318]: loss 0.496934
[epoch18, step319]: loss 0.554798
[epoch18, step320]: loss 0.602152
[epoch18, step321]: loss 0.584131
[epoch18, step322]: loss 0.357482
[epoch18, step323]: loss 0.259032
[epoch18, step324]: loss 0.598966
[epoch18, step325]: loss 0.683044
[epoch18, step326]: loss 0.217103
[epoch18, step327]: loss 0.472045
[epoch18, step328]: loss 0.427891
[epoch18, step329]: loss 0.301659
[epoch18, step330]: loss 0.601160
[epoch18, step331]: loss 0.564547
[epoch18, step332]: loss 0.329058
[epoch18, step333]: loss 0.486841
[epoch18, step334]: loss 0.597130
[epoch18, step335]: loss 0.421656
[epoch18, step336]: loss 0.525053
[epoch18, step337]: loss 0.172891
[epoch18, step338]: loss 0.372411
[epoch18, step339]: loss 0.408063
[epoch18, step340]: loss 0.544752
[epoch18, step341]: loss 0.472471
[epoch18, step342]: loss 0.434137
[epoch18, step343]: loss 0.687192
[epoch18, step344]: loss 0.290924
[epoch18, step345]: loss 0.331210
[epoch18, step346]: loss 0.444397
[epoch18, step347]: loss 0.699563
[epoch18, step348]: loss 0.382192
[epoch18, step349]: loss 0.516458
[epoch18, step350]: loss 0.469748
[epoch18, step351]: loss 0.633704
[epoch18, step352]: loss 0.725105
[epoch18, step353]: loss 0.317231
[epoch18, step354]: loss 0.527387
[epoch18, step355]: loss 0.414081
[epoch18, step356]: loss 0.512578
[epoch18, step357]: loss 0.598905
[epoch18, step358]: loss 0.266694
[epoch18, step359]: loss 0.465101
[epoch18, step360]: loss 0.601995
[epoch18, step361]: loss 0.626841
[epoch18, step362]: loss 0.546108
[epoch18, step363]: loss 0.654684
[epoch18, step364]: loss 0.457244
[epoch18, step365]: loss 0.310655
[epoch18, step366]: loss 0.528847
[epoch18, step367]: loss 0.643677
[epoch18, step368]: loss 0.406863
[epoch18, step369]: loss 0.224918
[epoch18, step370]: loss 0.503126
[epoch18, step371]: loss 0.420730
[epoch18, step372]: loss 0.580450
[epoch18, step373]: loss 0.514678
[epoch18, step374]: loss 0.478030
[epoch18, step375]: loss 0.558719
[epoch18, step376]: loss 0.281320
[epoch18, step377]: loss 0.406892
[epoch18, step378]: loss 0.505567
[epoch18, step379]: loss 0.319246
[epoch18, step380]: loss 0.558629
[epoch18, step381]: loss 0.452416
[epoch18, step382]: loss 0.557702
[epoch18, step383]: loss 0.504237
[epoch18, step384]: loss 0.657370
[epoch18, step385]: loss 0.431004
[epoch18, step386]: loss 0.391151
[epoch18, step387]: loss 0.515244
[epoch18, step388]: loss 0.428711
[epoch18, step389]: loss 0.385601
[epoch18, step390]: loss 0.516824
[epoch18, step391]: loss 0.664657
[epoch18, step392]: loss 0.392592
[epoch18, step393]: loss 0.340851
[epoch18, step394]: loss 0.682204
[epoch18, step395]: loss 0.395142
[epoch18, step396]: loss 0.458350
[epoch18, step397]: loss 0.515929
[epoch18, step398]: loss 0.650494
[epoch18, step399]: loss 0.489968
[epoch18, step400]: loss 0.503980
[epoch18, step401]: loss 0.524764
[epoch18, step402]: loss 0.582306
[epoch18, step403]: loss 0.543421
[epoch18, step404]: loss 0.426132
[epoch18, step405]: loss 0.474893
[epoch18, step406]: loss 0.547083
[epoch18, step407]: loss 0.381000
[epoch18, step408]: loss 0.331518
[epoch18, step409]: loss 0.522742
[epoch18, step410]: loss 0.662796
[epoch18, step411]: loss 0.374528
[epoch18, step412]: loss 0.392017
[epoch18, step413]: loss 0.687507
[epoch18, step414]: loss 0.414914
[epoch18, step415]: loss 0.545838
[epoch18, step416]: loss 0.371192
[epoch18, step417]: loss 0.544730
[epoch18, step418]: loss 0.457461
[epoch18, step419]: loss 0.612455
[epoch18, step420]: loss 0.642061
[epoch18, step421]: loss 0.415437
[epoch18, step422]: loss 0.621993
[epoch18, step423]: loss 0.660216
[epoch18, step424]: loss 0.536326
[epoch18, step425]: loss 0.439475
[epoch18, step426]: loss 0.638756
[epoch18, step427]: loss 0.460921
[epoch18, step428]: loss 0.494016
[epoch18, step429]: loss 0.436440
[epoch18, step430]: loss 0.439513
[epoch18, step431]: loss 0.413420
[epoch18, step432]: loss 0.421078
[epoch18, step433]: loss 0.510441
[epoch18, step434]: loss 0.539141
[epoch18, step435]: loss 0.576638
[epoch18, step436]: loss 0.312514
[epoch18, step437]: loss 0.552013
[epoch18, step438]: loss 0.622606
[epoch18, step439]: loss 0.319503
[epoch18, step440]: loss 0.427595
[epoch18, step441]: loss 0.599530
[epoch18, step442]: loss 0.356928
[epoch18, step443]: loss 0.439067
[epoch18, step444]: loss 0.164753
[epoch18, step445]: loss 0.351513
[epoch18, step446]: loss 0.594718
[epoch18, step447]: loss 0.182314
[epoch18, step448]: loss 0.680891
[epoch18, step449]: loss 0.450148
[epoch18, step450]: loss 0.396638
[epoch18, step451]: loss 0.315107
[epoch18, step452]: loss 0.298442
[epoch18, step453]: loss 0.249320
[epoch18, step454]: loss 0.450943
[epoch18, step455]: loss 0.332142
[epoch18, step456]: loss 0.567897
[epoch18, step457]: loss 0.598056
[epoch18, step458]: loss 0.481212
[epoch18, step459]: loss 0.547200
[epoch18, step460]: loss 0.287524
[epoch18, step461]: loss 0.498049
[epoch18, step462]: loss 0.575999
[epoch18, step463]: loss 0.435410
[epoch18, step464]: loss 0.283339
[epoch18, step465]: loss 0.500700
[epoch18, step466]: loss 0.483469
[epoch18, step467]: loss 0.285123
[epoch18, step468]: loss 0.381912
[epoch18, step469]: loss 0.541602
[epoch18, step470]: loss 0.564165
[epoch18, step471]: loss 0.613087
[epoch18, step472]: loss 0.584448
[epoch18, step473]: loss 0.436038
[epoch18, step474]: loss 0.480566
[epoch18, step475]: loss 0.494926
[epoch18, step476]: loss 0.547706
[epoch18, step477]: loss 0.227593
[epoch18, step478]: loss 0.617679
[epoch18, step479]: loss 0.353796
[epoch18, step480]: loss 0.725844
[epoch18, step481]: loss 0.507780
[epoch18, step482]: loss 0.402467
[epoch18, step483]: loss 0.233618
[epoch18, step484]: loss 0.710525
[epoch18, step485]: loss 0.389491
[epoch18, step486]: loss 0.503766
[epoch18, step487]: loss 0.511171
[epoch18, step488]: loss 0.490889
[epoch18, step489]: loss 0.610802
[epoch18, step490]: loss 0.451018
[epoch18, step491]: loss 0.535407
[epoch18, step492]: loss 0.656752
[epoch18, step493]: loss 0.598119
[epoch18, step494]: loss 0.348522
[epoch18, step495]: loss 0.404586
[epoch18, step496]: loss 0.519603
[epoch18, step497]: loss 0.459304
[epoch18, step498]: loss 0.442790
[epoch18, step499]: loss 0.459181
[epoch18, step500]: loss 0.417630
[epoch18, step501]: loss 0.600248
[epoch18, step502]: loss 0.680350
[epoch18, step503]: loss 0.402090
[epoch18, step504]: loss 0.541967
[epoch18, step505]: loss 0.333134
[epoch18, step506]: loss 0.260139
[epoch18, step507]: loss 0.528186
[epoch18, step508]: loss 0.355728
[epoch18, step509]: loss 0.585506
[epoch18, step510]: loss 0.213633
[epoch18, step511]: loss 0.212510
[epoch18, step512]: loss 0.360717
[epoch18, step513]: loss 0.290907
[epoch18, step514]: loss 0.258317
[epoch18, step515]: loss 0.111206
[epoch18, step516]: loss 0.557113
[epoch18, step517]: loss 0.441237
[epoch18, step518]: loss 0.602854
[epoch18, step519]: loss 0.525046
[epoch18, step520]: loss 0.258134
[epoch18, step521]: loss 0.578210
[epoch18, step522]: loss 0.469140
[epoch18, step523]: loss 0.571114
[epoch18, step524]: loss 0.444000
[epoch18, step525]: loss 0.473804
[epoch18, step526]: loss 0.500113
[epoch18, step527]: loss 0.283288
[epoch18, step528]: loss 0.151119
[epoch18, step529]: loss 0.360019
[epoch18, step530]: loss 0.580274
[epoch18, step531]: loss 0.403275
[epoch18, step532]: loss 0.368693
[epoch18, step533]: loss 0.382665
[epoch18, step534]: loss 0.445170
[epoch18, step535]: loss 0.344158
[epoch18, step536]: loss 0.374839
[epoch18, step537]: loss 0.490092
[epoch18, step538]: loss 0.464003
[epoch18, step539]: loss 0.415075
[epoch18, step540]: loss 0.595529
[epoch18, step541]: loss 0.506478
[epoch18, step542]: loss 0.551127
[epoch18, step543]: loss 0.480869
[epoch18, step544]: loss 0.777758
[epoch18, step545]: loss 0.506710
[epoch18, step546]: loss 0.757361
[epoch18, step547]: loss 0.417949
[epoch18, step548]: loss 0.545091
[epoch18, step549]: loss 0.238500
[epoch18, step550]: loss 0.596106
[epoch18, step551]: loss 0.528044
[epoch18, step552]: loss 0.436171
[epoch18, step553]: loss 0.486500
[epoch18, step554]: loss 0.692033
[epoch18, step555]: loss 0.457313
[epoch18, step556]: loss 0.507816
[epoch18, step557]: loss 0.725649
[epoch18, step558]: loss 0.486899
[epoch18, step559]: loss 0.464428
[epoch18, step560]: loss 0.292513
[epoch18, step561]: loss 0.373583
[epoch18, step562]: loss 0.527226
[epoch18, step563]: loss 0.116595
[epoch18, step564]: loss 0.416947
[epoch18, step565]: loss 0.519298
[epoch18, step566]: loss 0.339484
[epoch18, step567]: loss 0.616201
[epoch18, step568]: loss 0.629422
[epoch18, step569]: loss 0.502996
[epoch18, step570]: loss 0.551680
[epoch18, step571]: loss 0.501681
[epoch18, step572]: loss 0.302930
[epoch18, step573]: loss 0.607168
[epoch18, step574]: loss 0.665620
[epoch18, step575]: loss 0.492482
[epoch18, step576]: loss 0.476602
[epoch18, step577]: loss 0.428613
[epoch18, step578]: loss 0.539684
[epoch18, step579]: loss 0.426034
[epoch18, step580]: loss 0.548179
[epoch18, step581]: loss 0.486538
[epoch18, step582]: loss 0.508052
[epoch18, step583]: loss 0.481260
[epoch18, step584]: loss 0.430664
[epoch18, step585]: loss 0.428887
[epoch18, step586]: loss 0.251460
[epoch18, step587]: loss 0.574909
[epoch18, step588]: loss 0.290201
[epoch18, step589]: loss 0.522958
[epoch18, step590]: loss 0.403027
[epoch18, step591]: loss 0.581783
[epoch18, step592]: loss 0.548578
[epoch18, step593]: loss 0.604603
[epoch18, step594]: loss 0.548621
[epoch18, step595]: loss 0.641205
[epoch18, step596]: loss 0.246885
[epoch18, step597]: loss 0.407422
[epoch18, step598]: loss 0.450110
[epoch18, step599]: loss 0.578583
[epoch18, step600]: loss 0.593441
[epoch18, step601]: loss 0.421225
[epoch18, step602]: loss 0.457747
[epoch18, step603]: loss 0.428295
[epoch18, step604]: loss 0.424529
[epoch18, step605]: loss 0.411342
[epoch18, step606]: loss 0.480830
[epoch18, step607]: loss 0.534255
[epoch18, step608]: loss 0.358436
[epoch18, step609]: loss 0.500648
[epoch18, step610]: loss 0.459580
[epoch18, step611]: loss 0.411807
[epoch18, step612]: loss 0.199944
[epoch18, step613]: loss 0.604018
[epoch18, step614]: loss 0.635659
[epoch18, step615]: loss 0.229003
[epoch18, step616]: loss 0.233034
[epoch18, step617]: loss 0.226567
[epoch18, step618]: loss 0.525462
[epoch18, step619]: loss 0.274399
[epoch18, step620]: loss 0.504703
[epoch18, step621]: loss 0.476283
[epoch18, step622]: loss 0.586097
[epoch18, step623]: loss 0.440856
[epoch18, step624]: loss 0.362945
[epoch18, step625]: loss 0.504804
[epoch18, step626]: loss 0.698896
[epoch18, step627]: loss 0.535421
[epoch18, step628]: loss 0.428045
[epoch18, step629]: loss 0.380534
[epoch18, step630]: loss 0.565875
[epoch18, step631]: loss 0.529425
[epoch18, step632]: loss 0.222097
[epoch18, step633]: loss 0.101969
[epoch18, step634]: loss 0.565332
[epoch18, step635]: loss 0.677316
[epoch18, step636]: loss 0.290877
[epoch18, step637]: loss 0.252875
[epoch18, step638]: loss 0.352465
[epoch18, step639]: loss 0.599674
[epoch18, step640]: loss 0.497000
[epoch18, step641]: loss 0.490943
[epoch18, step642]: loss 0.402187
[epoch18, step643]: loss 0.613084
[epoch18, step644]: loss 0.589249
[epoch18, step645]: loss 0.416982
[epoch18, step646]: loss 0.272491
[epoch18, step647]: loss 0.612582
[epoch18, step648]: loss 0.586670
[epoch18, step649]: loss 0.495629
[epoch18, step650]: loss 0.579183
[epoch18, step651]: loss 0.512441
[epoch18, step652]: loss 0.322359
[epoch18, step653]: loss 0.453057
[epoch18, step654]: loss 0.510258
[epoch18, step655]: loss 0.483413
[epoch18, step656]: loss 0.626415
[epoch18, step657]: loss 0.442796
[epoch18, step658]: loss 0.475319
[epoch18, step659]: loss 0.520025
[epoch18, step660]: loss 0.530165
[epoch18, step661]: loss 0.260003
[epoch18, step662]: loss 0.348821
[epoch18, step663]: loss 0.500158
[epoch18, step664]: loss 0.457418
[epoch18, step665]: loss 0.504573
[epoch18, step666]: loss 0.421269
[epoch18, step667]: loss 0.661289
[epoch18, step668]: loss 0.534472
[epoch18, step669]: loss 0.364222
[epoch18, step670]: loss 0.517322
[epoch18, step671]: loss 0.462350
[epoch18, step672]: loss 0.216979
[epoch18, step673]: loss 0.590053
[epoch18, step674]: loss 0.479751
[epoch18, step675]: loss 0.164304
[epoch18, step676]: loss 0.348127
[epoch18, step677]: loss 0.632577
[epoch18, step678]: loss 0.627558
[epoch18, step679]: loss 0.617556
[epoch18, step680]: loss 0.378051
[epoch18, step681]: loss 0.443537
[epoch18, step682]: loss 0.619248
[epoch18, step683]: loss 0.449738
[epoch18, step684]: loss 0.511874
[epoch18, step685]: loss 0.354348
[epoch18, step686]: loss 0.674996
[epoch18, step687]: loss 0.465939
[epoch18, step688]: loss 0.545320
[epoch18, step689]: loss 0.445914
[epoch18, step690]: loss 0.290835
[epoch18, step691]: loss 0.636474
[epoch18, step692]: loss 0.247535
[epoch18, step693]: loss 0.321560
[epoch18, step694]: loss 0.537827
[epoch18, step695]: loss 0.281811
[epoch18, step696]: loss 0.565637
[epoch18, step697]: loss 0.610314
[epoch18, step698]: loss 0.239062
[epoch18, step699]: loss 0.544437
[epoch18, step700]: loss 0.316463
[epoch18, step701]: loss 0.658573
[epoch18, step702]: loss 0.376595
[epoch18, step703]: loss 0.450067
[epoch18, step704]: loss 0.282214
[epoch18, step705]: loss 0.404589
[epoch18, step706]: loss 0.451063
[epoch18, step707]: loss 0.401422
[epoch18, step708]: loss 0.611518
[epoch18, step709]: loss 0.395417
[epoch18, step710]: loss 0.701358
[epoch18, step711]: loss 0.395118
[epoch18, step712]: loss 0.472756
[epoch18, step713]: loss 0.452447
[epoch18, step714]: loss 0.409190
[epoch18, step715]: loss 0.667363
[epoch18, step716]: loss 0.484271
[epoch18, step717]: loss 0.361846
[epoch18, step718]: loss 0.503285
[epoch18, step719]: loss 0.342999
[epoch18, step720]: loss 0.223987
[epoch18, step721]: loss 0.494903
[epoch18, step722]: loss 0.536850
[epoch18, step723]: loss 0.359160
[epoch18, step724]: loss 0.437781
[epoch18, step725]: loss 0.700175
[epoch18, step726]: loss 0.337160
[epoch18, step727]: loss 0.254565
[epoch18, step728]: loss 0.335382
[epoch18, step729]: loss 0.635050
[epoch18, step730]: loss 0.564866
[epoch18, step731]: loss 0.393465
[epoch18, step732]: loss 0.600106
[epoch18, step733]: loss 0.463198
[epoch18, step734]: loss 0.365843
[epoch18, step735]: loss 0.528062
[epoch18, step736]: loss 0.584150
[epoch18, step737]: loss 0.593884
[epoch18, step738]: loss 0.601032
[epoch18, step739]: loss 0.450030
[epoch18, step740]: loss 0.390060
[epoch18, step741]: loss 0.404972
[epoch18, step742]: loss 0.570314
[epoch18, step743]: loss 0.607704
[epoch18, step744]: loss 0.663727
[epoch18, step745]: loss 0.434373
[epoch18, step746]: loss 0.252795
[epoch18, step747]: loss 0.705206
[epoch18, step748]: loss 0.714108
[epoch18, step749]: loss 0.422022
[epoch18, step750]: loss 0.303204
[epoch18, step751]: loss 0.496556
[epoch18, step752]: loss 0.467523
[epoch18, step753]: loss 0.406062
[epoch18, step754]: loss 0.531639
[epoch18, step755]: loss 0.291575
[epoch18, step756]: loss 0.449463
[epoch18, step757]: loss 0.240828
[epoch18, step758]: loss 0.360429
[epoch18, step759]: loss 0.465343
[epoch18, step760]: loss 0.495491
[epoch18, step761]: loss 0.397343
[epoch18, step762]: loss 0.411204
[epoch18, step763]: loss 0.634177
[epoch18, step764]: loss 0.163639
[epoch18, step765]: loss 0.729790
[epoch18, step766]: loss 0.335346
[epoch18, step767]: loss 0.369005
[epoch18, step768]: loss 0.326298
[epoch18, step769]: loss 0.675343
[epoch18, step770]: loss 0.644200
[epoch18, step771]: loss 0.342314
[epoch18, step772]: loss 0.627303
[epoch18, step773]: loss 0.355925
[epoch18, step774]: loss 0.472228
[epoch18, step775]: loss 0.345611
[epoch18, step776]: loss 0.530540
[epoch18, step777]: loss 0.426134
[epoch18, step778]: loss 0.381807
[epoch18, step779]: loss 0.574047
[epoch18, step780]: loss 0.430661
[epoch18, step781]: loss 0.613243
[epoch18, step782]: loss 0.532798
[epoch18, step783]: loss 0.561209
[epoch18, step784]: loss 0.532313
[epoch18, step785]: loss 0.467012
[epoch18, step786]: loss 0.537502
[epoch18, step787]: loss 0.747364
[epoch18, step788]: loss 0.417633
[epoch18, step789]: loss 0.596860
[epoch18, step790]: loss 0.641989
[epoch18, step791]: loss 0.729618
[epoch18, step792]: loss 0.395314
[epoch18, step793]: loss 0.621409
[epoch18, step794]: loss 0.533671
[epoch18, step795]: loss 0.314246
[epoch18, step796]: loss 0.627567
[epoch18, step797]: loss 0.432326
[epoch18, step798]: loss 0.306196
[epoch18, step799]: loss 0.564380
[epoch18, step800]: loss 0.578314
[epoch18, step801]: loss 0.524383
[epoch18, step802]: loss 0.455774
[epoch18, step803]: loss 0.434212
[epoch18, step804]: loss 0.387851
[epoch18, step805]: loss 0.378423
[epoch18, step806]: loss 0.363987
[epoch18, step807]: loss 0.540556
[epoch18, step808]: loss 0.254873
[epoch18, step809]: loss 0.418300
[epoch18, step810]: loss 0.628922
[epoch18, step811]: loss 0.221974
[epoch18, step812]: loss 0.483780
[epoch18, step813]: loss 0.573794
[epoch18, step814]: loss 0.397328
[epoch18, step815]: loss 0.448657
[epoch18, step816]: loss 0.415595
[epoch18, step817]: loss 0.394007
[epoch18, step818]: loss 0.515574
[epoch18, step819]: loss 0.612952
[epoch18, step820]: loss 0.557378
[epoch18, step821]: loss 0.546705
[epoch18, step822]: loss 0.268108
[epoch18, step823]: loss 0.398782
[epoch18, step824]: loss 0.366666
[epoch18, step825]: loss 0.488637
[epoch18, step826]: loss 0.686239
[epoch18, step827]: loss 0.510771
[epoch18, step828]: loss 0.381188
[epoch18, step829]: loss 0.649654
[epoch18, step830]: loss 0.506576
[epoch18, step831]: loss 0.354660
[epoch18, step832]: loss 0.489327
[epoch18, step833]: loss 0.389563
[epoch18, step834]: loss 0.608629
[epoch18, step835]: loss 0.616296
[epoch18, step836]: loss 0.317681
[epoch18, step837]: loss 0.528682
[epoch18, step838]: loss 0.392752
[epoch18, step839]: loss 0.274983
[epoch18, step840]: loss 0.355504
[epoch18, step841]: loss 0.347712
[epoch18, step842]: loss 0.230638
[epoch18, step843]: loss 0.407857
[epoch18, step844]: loss 0.407340
[epoch18, step845]: loss 0.329144
[epoch18, step846]: loss 0.698278
[epoch18, step847]: loss 0.327753
[epoch18, step848]: loss 0.323254
[epoch18, step849]: loss 0.602659
[epoch18, step850]: loss 0.367564
[epoch18, step851]: loss 0.212658
[epoch18, step852]: loss 0.400465
[epoch18, step853]: loss 0.496194
[epoch18, step854]: loss 0.330806
[epoch18, step855]: loss 0.497203
[epoch18, step856]: loss 0.472415
[epoch18, step857]: loss 0.473440
[epoch18, step858]: loss 0.483035
[epoch18, step859]: loss 0.623702
[epoch18, step860]: loss 0.352612
[epoch18, step861]: loss 0.483392
[epoch18, step862]: loss 0.285747
[epoch18, step863]: loss 0.585593
[epoch18, step864]: loss 0.353523
[epoch18, step865]: loss 0.238275
[epoch18, step866]: loss 0.458446
[epoch18, step867]: loss 0.623675
[epoch18, step868]: loss 0.469409
[epoch18, step869]: loss 0.669303
[epoch18, step870]: loss 0.407974
[epoch18, step871]: loss 0.490367
[epoch18, step872]: loss 0.278696
[epoch18, step873]: loss 0.339610
[epoch18, step874]: loss 0.413383
[epoch18, step875]: loss 0.442009
[epoch18, step876]: loss 0.172890
[epoch18, step877]: loss 0.443458
[epoch18, step878]: loss 0.530849
[epoch18, step879]: loss 0.509322
[epoch18, step880]: loss 0.352957
[epoch18, step881]: loss 0.406597
[epoch18, step882]: loss 0.363149
[epoch18, step883]: loss 0.426542
[epoch18, step884]: loss 0.579690
[epoch18, step885]: loss 0.549292
[epoch18, step886]: loss 0.471090
[epoch18, step887]: loss 0.360654
[epoch18, step888]: loss 0.717007
[epoch18, step889]: loss 0.358327
[epoch18, step890]: loss 0.627475
[epoch18, step891]: loss 0.295257
[epoch18, step892]: loss 0.578363
[epoch18, step893]: loss 0.381223
[epoch18, step894]: loss 0.528101
[epoch18, step895]: loss 0.485624
[epoch18, step896]: loss 0.722351
[epoch18, step897]: loss 0.630497
[epoch18, step898]: loss 0.492400
[epoch18, step899]: loss 0.311793
[epoch18, step900]: loss 0.536094
[epoch18, step901]: loss 0.507973
[epoch18, step902]: loss 0.424525
[epoch18, step903]: loss 0.523229
[epoch18, step904]: loss 0.392220
[epoch18, step905]: loss 0.553688
[epoch18, step906]: loss 0.536541
[epoch18, step907]: loss 0.633624
[epoch18, step908]: loss 0.344245
[epoch18, step909]: loss 0.340211
[epoch18, step910]: loss 0.511985
[epoch18, step911]: loss 0.589098
[epoch18, step912]: loss 0.586522
[epoch18, step913]: loss 0.492497
[epoch18, step914]: loss 0.505319
[epoch18, step915]: loss 0.451012
[epoch18, step916]: loss 0.539146
[epoch18, step917]: loss 0.564930
[epoch18, step918]: loss 0.524342
[epoch18, step919]: loss 0.419165
[epoch18, step920]: loss 0.620159
[epoch18, step921]: loss 0.495281
[epoch18, step922]: loss 0.338817
[epoch18, step923]: loss 0.674413
[epoch18, step924]: loss 0.609374
[epoch18, step925]: loss 0.196878
[epoch18, step926]: loss 0.696392
[epoch18, step927]: loss 0.603077
[epoch18, step928]: loss 0.522921
[epoch18, step929]: loss 0.480998
[epoch18, step930]: loss 0.596905
[epoch18, step931]: loss 0.381469
[epoch18, step932]: loss 0.595398
[epoch18, step933]: loss 0.523638
[epoch18, step934]: loss 0.502304
[epoch18, step935]: loss 0.451674
[epoch18, step936]: loss 0.473542
[epoch18, step937]: loss 0.418465
[epoch18, step938]: loss 0.701343
[epoch18, step939]: loss 0.413152
[epoch18, step940]: loss 0.520630
[epoch18, step941]: loss 0.139480
[epoch18, step942]: loss 0.485155
[epoch18, step943]: loss 0.585593
[epoch18, step944]: loss 0.427056
[epoch18, step945]: loss 0.515475
[epoch18, step946]: loss 0.402302
[epoch18, step947]: loss 0.630737
[epoch18, step948]: loss 0.533453
[epoch18, step949]: loss 0.425892
[epoch18, step950]: loss 0.702591
[epoch18, step951]: loss 0.504865
[epoch18, step952]: loss 0.187682
[epoch18, step953]: loss 0.506796
[epoch18, step954]: loss 0.780136
[epoch18, step955]: loss 0.576252
[epoch18, step956]: loss 0.510269
[epoch18, step957]: loss 0.381173
[epoch18, step958]: loss 0.447208
[epoch18, step959]: loss 0.526874
[epoch18, step960]: loss 0.594058
[epoch18, step961]: loss 0.470348
[epoch18, step962]: loss 0.640713
[epoch18, step963]: loss 0.697898
[epoch18, step964]: loss 0.388039
[epoch18, step965]: loss 0.469255
[epoch18, step966]: loss 0.472681
[epoch18, step967]: loss 0.621941
[epoch18, step968]: loss 0.556180
[epoch18, step969]: loss 0.577452
[epoch18, step970]: loss 0.320402
[epoch18, step971]: loss 0.336409
[epoch18, step972]: loss 0.595220
[epoch18, step973]: loss 0.339718
[epoch18, step974]: loss 0.604715
[epoch18, step975]: loss 0.498746
[epoch18, step976]: loss 0.496928
[epoch18, step977]: loss 0.702212
[epoch18, step978]: loss 0.557746
[epoch18, step979]: loss 0.398291
[epoch18, step980]: loss 0.113750
[epoch18, step981]: loss 0.609604
[epoch18, step982]: loss 0.259937
[epoch18, step983]: loss 0.294751
[epoch18, step984]: loss 0.508376
[epoch18, step985]: loss 0.293314
[epoch18, step986]: loss 0.453687
[epoch18, step987]: loss 0.446383
[epoch18, step988]: loss 0.345791
[epoch18, step989]: loss 0.553878
[epoch18, step990]: loss 0.460275
[epoch18, step991]: loss 0.489709
[epoch18, step992]: loss 0.544169
[epoch18, step993]: loss 0.498052
[epoch18, step994]: loss 0.554961
[epoch18, step995]: loss 0.589015
[epoch18, step996]: loss 0.506452
[epoch18, step997]: loss 0.616949
[epoch18, step998]: loss 0.547648
[epoch18, step999]: loss 0.599725
[epoch18, step1000]: loss 0.516676
[epoch18, step1001]: loss 0.442104
[epoch18, step1002]: loss 0.528285
[epoch18, step1003]: loss 0.561003
[epoch18, step1004]: loss 0.359008
[epoch18, step1005]: loss 0.638015
[epoch18, step1006]: loss 0.508049
[epoch18, step1007]: loss 0.225892
[epoch18, step1008]: loss 0.342366
[epoch18, step1009]: loss 0.653712
[epoch18, step1010]: loss 0.324534
[epoch18, step1011]: loss 0.319974
[epoch18, step1012]: loss 0.611985
[epoch18, step1013]: loss 0.379421
[epoch18, step1014]: loss 0.400724
[epoch18, step1015]: loss 0.366353
[epoch18, step1016]: loss 0.686440
[epoch18, step1017]: loss 0.479756
[epoch18, step1018]: loss 0.558549
[epoch18, step1019]: loss 0.682483
[epoch18, step1020]: loss 0.354997
[epoch18, step1021]: loss 0.607567
[epoch18, step1022]: loss 0.361367
[epoch18, step1023]: loss 0.371027
[epoch18, step1024]: loss 0.226878
[epoch18, step1025]: loss 0.475208
[epoch18, step1026]: loss 0.492330
[epoch18, step1027]: loss 0.347302
[epoch18, step1028]: loss 0.308895
[epoch18, step1029]: loss 0.088338
[epoch18, step1030]: loss 0.462768
[epoch18, step1031]: loss 0.578303
[epoch18, step1032]: loss 0.711148
[epoch18, step1033]: loss 0.463863
[epoch18, step1034]: loss 0.378126
[epoch18, step1035]: loss 0.438290
[epoch18, step1036]: loss 0.432190
[epoch18, step1037]: loss 0.495293
[epoch18, step1038]: loss 0.434026
[epoch18, step1039]: loss 0.588329
[epoch18, step1040]: loss 0.462721
[epoch18, step1041]: loss 0.693191
[epoch18, step1042]: loss 0.342996
[epoch18, step1043]: loss 0.389655
[epoch18, step1044]: loss 0.444749
[epoch18, step1045]: loss 0.298121
[epoch18, step1046]: loss 0.549193
[epoch18, step1047]: loss 0.491785
[epoch18, step1048]: loss 0.589043
[epoch18, step1049]: loss 0.782150
[epoch18, step1050]: loss 0.460102
[epoch18, step1051]: loss 0.503504
[epoch18, step1052]: loss 0.520373
[epoch18, step1053]: loss 0.584806
[epoch18, step1054]: loss 0.450353
[epoch18, step1055]: loss 0.559568
[epoch18, step1056]: loss 0.458316
[epoch18, step1057]: loss 0.253472
[epoch18, step1058]: loss 0.590132
[epoch18, step1059]: loss 0.518031
[epoch18, step1060]: loss 0.420833
[epoch18, step1061]: loss 0.452502
[epoch18, step1062]: loss 0.277893
[epoch18, step1063]: loss 0.567414
[epoch18, step1064]: loss 0.343573
[epoch18, step1065]: loss 0.383051
[epoch18, step1066]: loss 0.497676
[epoch18, step1067]: loss 0.565746
[epoch18, step1068]: loss 0.716979
[epoch18, step1069]: loss 0.527775
[epoch18, step1070]: loss 0.601677
[epoch18, step1071]: loss 0.418722
[epoch18, step1072]: loss 0.341451
[epoch18, step1073]: loss 0.443379
[epoch18, step1074]: loss 0.221290
[epoch18, step1075]: loss 0.356634
[epoch18, step1076]: loss 0.500126
[epoch18, step1077]: loss 0.264225
[epoch18, step1078]: loss 0.398406
[epoch18, step1079]: loss 0.371881
[epoch18, step1080]: loss 0.695464
[epoch18, step1081]: loss 0.477216
[epoch18, step1082]: loss 0.419244
[epoch18, step1083]: loss 0.315661
[epoch18, step1084]: loss 0.524878
[epoch18, step1085]: loss 0.156516
[epoch18, step1086]: loss 0.494499
[epoch18, step1087]: loss 0.599693
[epoch18, step1088]: loss 0.470834
[epoch18, step1089]: loss 0.696350
[epoch18, step1090]: loss 0.548842
[epoch18, step1091]: loss 0.214916
[epoch18, step1092]: loss 0.169085
[epoch18, step1093]: loss 0.314724
[epoch18, step1094]: loss 0.458932
[epoch18, step1095]: loss 0.590126
[epoch18, step1096]: loss 0.441388
[epoch18, step1097]: loss 0.515664
[epoch18, step1098]: loss 0.523984
[epoch18, step1099]: loss 0.346387
[epoch18, step1100]: loss 0.659886
[epoch18, step1101]: loss 0.383494
[epoch18, step1102]: loss 0.412646
[epoch18, step1103]: loss 0.289447
[epoch18, step1104]: loss 0.474048
[epoch18, step1105]: loss 0.492586
[epoch18, step1106]: loss 0.253361
[epoch18, step1107]: loss 0.508752
[epoch18, step1108]: loss 0.485862
[epoch18, step1109]: loss 0.348482
[epoch18, step1110]: loss 0.479556
[epoch18, step1111]: loss 0.591798
[epoch18, step1112]: loss 0.416629
[epoch18, step1113]: loss 0.683723
[epoch18, step1114]: loss 0.413199
[epoch18, step1115]: loss 0.507991
[epoch18, step1116]: loss 0.176349
[epoch18, step1117]: loss 0.613687
[epoch18, step1118]: loss 0.552834
[epoch18, step1119]: loss 0.572694
[epoch18, step1120]: loss 0.219938
[epoch18, step1121]: loss 0.286720
[epoch18, step1122]: loss 0.604715
[epoch18, step1123]: loss 0.548601
[epoch18, step1124]: loss 0.551320
[epoch18, step1125]: loss 0.569024
[epoch18, step1126]: loss 0.430691
[epoch18, step1127]: loss 0.537891
[epoch18, step1128]: loss 0.544532
[epoch18, step1129]: loss 0.414512
[epoch18, step1130]: loss 0.461044
[epoch18, step1131]: loss 0.593980
[epoch18, step1132]: loss 0.528000
[epoch18, step1133]: loss 0.577662
[epoch18, step1134]: loss 0.498855
[epoch18, step1135]: loss 0.454696
[epoch18, step1136]: loss 0.585842
[epoch18, step1137]: loss 0.416911
[epoch18, step1138]: loss 0.657773
[epoch18, step1139]: loss 0.477463
[epoch18, step1140]: loss 0.374305
[epoch18, step1141]: loss 0.285609
[epoch18, step1142]: loss 0.412371
[epoch18, step1143]: loss 0.384799
[epoch18, step1144]: loss 0.426002
[epoch18, step1145]: loss 0.638645
[epoch18, step1146]: loss 0.242044
[epoch18, step1147]: loss 0.468440
[epoch18, step1148]: loss 0.364031
[epoch18, step1149]: loss 0.472948
[epoch18, step1150]: loss 0.627178
[epoch18, step1151]: loss 0.677606
[epoch18, step1152]: loss 0.138267
[epoch18, step1153]: loss 0.581541
[epoch18, step1154]: loss 0.412189
[epoch18, step1155]: loss 0.329275
[epoch18, step1156]: loss 0.601360
[epoch18, step1157]: loss 0.357302
[epoch18, step1158]: loss 0.321999
[epoch18, step1159]: loss 0.658445
[epoch18, step1160]: loss 0.459634
[epoch18, step1161]: loss 0.409944
[epoch18, step1162]: loss 0.595767
[epoch18, step1163]: loss 0.420278
[epoch18, step1164]: loss 0.522331
[epoch18, step1165]: loss 0.182572
[epoch18, step1166]: loss 0.625273
[epoch18, step1167]: loss 0.664755
[epoch18, step1168]: loss 0.672938
[epoch18, step1169]: loss 0.597314
[epoch18, step1170]: loss 0.498698
[epoch18, step1171]: loss 0.633356
[epoch18, step1172]: loss 0.333340
[epoch18, step1173]: loss 0.259062
[epoch18, step1174]: loss 0.447504
[epoch18, step1175]: loss 0.402996
[epoch18, step1176]: loss 0.551487
[epoch18, step1177]: loss 0.321186
[epoch18, step1178]: loss 0.564076
[epoch18, step1179]: loss 0.370705
[epoch18, step1180]: loss 0.517413
[epoch18, step1181]: loss 0.385166
[epoch18, step1182]: loss 0.528221
[epoch18, step1183]: loss 0.527246
[epoch18, step1184]: loss 0.613078
[epoch18, step1185]: loss 0.385417
[epoch18, step1186]: loss 0.429331
[epoch18, step1187]: loss 0.458340
[epoch18, step1188]: loss 0.546186
[epoch18, step1189]: loss 0.591015
[epoch18, step1190]: loss 0.602025
[epoch18, step1191]: loss 0.393524
[epoch18, step1192]: loss 0.508362
[epoch18, step1193]: loss 0.205646
[epoch18, step1194]: loss 0.462728
[epoch18, step1195]: loss 0.369377
[epoch18, step1196]: loss 0.449256
[epoch18, step1197]: loss 0.286616
[epoch18, step1198]: loss 0.446715
[epoch18, step1199]: loss 0.487095
[epoch18, step1200]: loss 0.114146
[epoch18, step1201]: loss 0.511653
[epoch18, step1202]: loss 0.504129
[epoch18, step1203]: loss 0.406263
[epoch18, step1204]: loss 0.603150
[epoch18, step1205]: loss 0.437138
[epoch18, step1206]: loss 0.456015
[epoch18, step1207]: loss 0.417043
[epoch18, step1208]: loss 0.442883
[epoch18, step1209]: loss 0.493330
[epoch18, step1210]: loss 0.254615
[epoch18, step1211]: loss 0.465317
[epoch18, step1212]: loss 0.515685
[epoch18, step1213]: loss 0.232611
[epoch18, step1214]: loss 0.546033
[epoch18, step1215]: loss 0.473007
[epoch18, step1216]: loss 0.405054
[epoch18, step1217]: loss 0.325436
[epoch18, step1218]: loss 0.309352
[epoch18, step1219]: loss 0.602431
[epoch18, step1220]: loss 0.415196
[epoch18, step1221]: loss 0.332895
[epoch18, step1222]: loss 0.562068
[epoch18, step1223]: loss 0.451286
[epoch18, step1224]: loss 0.479265
[epoch18, step1225]: loss 0.376948
[epoch18, step1226]: loss 0.430623
[epoch18, step1227]: loss 0.421144
[epoch18, step1228]: loss 0.418568
[epoch18, step1229]: loss 0.327369
[epoch18, step1230]: loss 0.339561
[epoch18, step1231]: loss 0.625049
[epoch18, step1232]: loss 0.440128
[epoch18, step1233]: loss 0.500161
[epoch18, step1234]: loss 0.514679
[epoch18, step1235]: loss 0.172808
[epoch18, step1236]: loss 0.372641
[epoch18, step1237]: loss 0.456414
[epoch18, step1238]: loss 0.714244
[epoch18, step1239]: loss 0.509720
[epoch18, step1240]: loss 0.431403
[epoch18, step1241]: loss 0.446793
[epoch18, step1242]: loss 0.541494
[epoch18, step1243]: loss 0.523081
[epoch18, step1244]: loss 0.662751
[epoch18, step1245]: loss 0.376331
[epoch18, step1246]: loss 0.429172
[epoch18, step1247]: loss 0.631739
[epoch18, step1248]: loss 0.342514
[epoch18, step1249]: loss 0.421457
[epoch18, step1250]: loss 0.479138
[epoch18, step1251]: loss 0.438295
[epoch18, step1252]: loss 0.439728
[epoch18, step1253]: loss 0.672217
[epoch18, step1254]: loss 0.420160
[epoch18, step1255]: loss 0.621518
[epoch18, step1256]: loss 0.731006
[epoch18, step1257]: loss 0.480705
[epoch18, step1258]: loss 0.448785
[epoch18, step1259]: loss 0.413626
[epoch18, step1260]: loss 0.241515
[epoch18, step1261]: loss 0.654620
[epoch18, step1262]: loss 0.335754
[epoch18, step1263]: loss 0.333101
[epoch18, step1264]: loss 0.648732
[epoch18, step1265]: loss 0.662804
[epoch18, step1266]: loss 0.439321
[epoch18, step1267]: loss 0.738469
[epoch18, step1268]: loss 0.403327
[epoch18, step1269]: loss 0.295533
[epoch18, step1270]: loss 0.486038
[epoch18, step1271]: loss 0.480259
[epoch18, step1272]: loss 0.690773
[epoch18, step1273]: loss 0.525150
[epoch18, step1274]: loss 0.435914
[epoch18, step1275]: loss 0.471606
[epoch18, step1276]: loss 0.335965
[epoch18, step1277]: loss 0.463737
[epoch18, step1278]: loss 0.536328
[epoch18, step1279]: loss 0.673363
[epoch18, step1280]: loss 0.477402
[epoch18, step1281]: loss 0.593274
[epoch18, step1282]: loss 0.446836
[epoch18, step1283]: loss 0.485707
[epoch18, step1284]: loss 0.285291
[epoch18, step1285]: loss 0.550460
[epoch18, step1286]: loss 0.563356
[epoch18, step1287]: loss 0.370368
[epoch18, step1288]: loss 0.410756
[epoch18, step1289]: loss 0.411513
[epoch18, step1290]: loss 0.543558
[epoch18, step1291]: loss 0.682077
[epoch18, step1292]: loss 0.549732
[epoch18, step1293]: loss 0.530896
[epoch18, step1294]: loss 0.612744
[epoch18, step1295]: loss 0.425392
[epoch18, step1296]: loss 0.384819
[epoch18, step1297]: loss 0.412832
[epoch18, step1298]: loss 0.292674
[epoch18, step1299]: loss 0.593436
[epoch18, step1300]: loss 0.528960
[epoch18, step1301]: loss 0.563958
[epoch18, step1302]: loss 0.249379
[epoch18, step1303]: loss 0.272100
[epoch18, step1304]: loss 0.611198
[epoch18, step1305]: loss 0.333313
[epoch18, step1306]: loss 0.468268
[epoch18, step1307]: loss 0.615268
[epoch18, step1308]: loss 0.492710
[epoch18, step1309]: loss 0.410771
[epoch18, step1310]: loss 0.417160
[epoch18, step1311]: loss 0.495758
[epoch18, step1312]: loss 0.155719
[epoch18, step1313]: loss 0.496572
[epoch18, step1314]: loss 0.523805
[epoch18, step1315]: loss 0.494944
[epoch18, step1316]: loss 0.828225
[epoch18, step1317]: loss 0.405516
[epoch18, step1318]: loss 0.586547
[epoch18, step1319]: loss 0.351017
[epoch18, step1320]: loss 0.539734
[epoch18, step1321]: loss 0.494729
[epoch18, step1322]: loss 0.425201
[epoch18, step1323]: loss 0.516453
[epoch18, step1324]: loss 0.399613
[epoch18, step1325]: loss 0.362518
[epoch18, step1326]: loss 0.725852
[epoch18, step1327]: loss 0.334744
[epoch18, step1328]: loss 0.458068
[epoch18, step1329]: loss 0.428111
[epoch18, step1330]: loss 0.502963
[epoch18, step1331]: loss 0.112121
[epoch18, step1332]: loss 0.472110
[epoch18, step1333]: loss 0.610767
[epoch18, step1334]: loss 0.509578
[epoch18, step1335]: loss 0.524430
[epoch18, step1336]: loss 0.514110
[epoch18, step1337]: loss 0.492483
[epoch18, step1338]: loss 0.334606
[epoch18, step1339]: loss 0.389688
[epoch18, step1340]: loss 0.646363
[epoch18, step1341]: loss 0.549367
[epoch18, step1342]: loss 0.141994
[epoch18, step1343]: loss 0.529767
[epoch18, step1344]: loss 0.293244
[epoch18, step1345]: loss 0.410734
[epoch18, step1346]: loss 0.499693
[epoch18, step1347]: loss 0.448177
[epoch18, step1348]: loss 0.553154
[epoch18, step1349]: loss 0.337848
[epoch18, step1350]: loss 0.642961
[epoch18, step1351]: loss 0.263414
[epoch18, step1352]: loss 0.619659
[epoch18, step1353]: loss 0.720869
[epoch18, step1354]: loss 0.319337
[epoch18, step1355]: loss 0.305374
[epoch18, step1356]: loss 0.462537
[epoch18, step1357]: loss 0.546403
[epoch18, step1358]: loss 0.586596
[epoch18, step1359]: loss 0.440656
[epoch18, step1360]: loss 0.687360
[epoch18, step1361]: loss 0.462978
[epoch18, step1362]: loss 0.447934
[epoch18, step1363]: loss 0.421678
[epoch18, step1364]: loss 0.581637
[epoch18, step1365]: loss 0.409918
[epoch18, step1366]: loss 0.483994
[epoch18, step1367]: loss 0.627779
[epoch18, step1368]: loss 0.712996
[epoch18, step1369]: loss 0.550672
[epoch18, step1370]: loss 0.344543
[epoch18, step1371]: loss 0.273286
[epoch18, step1372]: loss 0.212865
[epoch18, step1373]: loss 0.665479
[epoch18, step1374]: loss 0.543121
[epoch18, step1375]: loss 0.266651
[epoch18, step1376]: loss 0.537279
[epoch18, step1377]: loss 0.619645
[epoch18, step1378]: loss 0.362304
[epoch18, step1379]: loss 0.352562
[epoch18, step1380]: loss 0.704153
[epoch18, step1381]: loss 0.583617
[epoch18, step1382]: loss 0.503865
[epoch18, step1383]: loss 0.498058
[epoch18, step1384]: loss 0.467684
[epoch18, step1385]: loss 0.625578
[epoch18, step1386]: loss 0.487600
[epoch18, step1387]: loss 0.606543
[epoch18, step1388]: loss 0.386142
[epoch18, step1389]: loss 0.515147
[epoch18, step1390]: loss 0.506511
[epoch18, step1391]: loss 0.388784
[epoch18, step1392]: loss 0.418079
[epoch18, step1393]: loss 0.617698
[epoch18, step1394]: loss 0.379605
[epoch18, step1395]: loss 0.589921
[epoch18, step1396]: loss 0.306127
[epoch18, step1397]: loss 0.495806
[epoch18, step1398]: loss 0.589451
[epoch18, step1399]: loss 0.229277
[epoch18, step1400]: loss 0.624863
[epoch18, step1401]: loss 0.359495
[epoch18, step1402]: loss 0.320144
[epoch18, step1403]: loss 0.366019
[epoch18, step1404]: loss 0.497830
[epoch18, step1405]: loss 0.375394
[epoch18, step1406]: loss 0.561701
[epoch18, step1407]: loss 0.542784
[epoch18, step1408]: loss 0.313359
[epoch18, step1409]: loss 0.556345
[epoch18, step1410]: loss 0.712247
[epoch18, step1411]: loss 0.411844
[epoch18, step1412]: loss 0.498707
[epoch18, step1413]: loss 0.480679
[epoch18, step1414]: loss 0.499197
[epoch18, step1415]: loss 0.592962
[epoch18, step1416]: loss 0.222556
[epoch18, step1417]: loss 0.479527
[epoch18, step1418]: loss 0.311110
[epoch18, step1419]: loss 0.216817
[epoch18, step1420]: loss 0.481921
[epoch18, step1421]: loss 0.360544
[epoch18, step1422]: loss 0.516611
[epoch18, step1423]: loss 0.526712
[epoch18, step1424]: loss 0.239350
[epoch18, step1425]: loss 0.433051
[epoch18, step1426]: loss 0.231571
[epoch18, step1427]: loss 0.549586
[epoch18, step1428]: loss 0.528400
[epoch18, step1429]: loss 0.373889
[epoch18, step1430]: loss 0.485356
[epoch18, step1431]: loss 0.451860
[epoch18, step1432]: loss 0.446733
[epoch18, step1433]: loss 0.448524
[epoch18, step1434]: loss 0.417716
[epoch18, step1435]: loss 0.404905
[epoch18, step1436]: loss 0.592640
[epoch18, step1437]: loss 0.652505
[epoch18, step1438]: loss 0.406300
[epoch18, step1439]: loss 0.544957
[epoch18, step1440]: loss 0.469653
[epoch18, step1441]: loss 0.398087
[epoch18, step1442]: loss 0.652130
[epoch18, step1443]: loss 0.639822
[epoch18, step1444]: loss 0.510614
[epoch18, step1445]: loss 0.404066
[epoch18, step1446]: loss 0.316380
[epoch18, step1447]: loss 0.560268
[epoch18, step1448]: loss 0.692724
[epoch18, step1449]: loss 0.523176
[epoch18, step1450]: loss 0.571064
[epoch18, step1451]: loss 0.525648
[epoch18, step1452]: loss 0.327910
[epoch18, step1453]: loss 0.471202
[epoch18, step1454]: loss 0.477031
[epoch18, step1455]: loss 0.358237
[epoch18, step1456]: loss 0.573973
[epoch18, step1457]: loss 0.451550
[epoch18, step1458]: loss 0.419510
[epoch18, step1459]: loss 0.446153
[epoch18, step1460]: loss 0.403091
[epoch18, step1461]: loss 0.399112
[epoch18, step1462]: loss 0.570747
[epoch18, step1463]: loss 0.520577
[epoch18, step1464]: loss 0.622896
[epoch18, step1465]: loss 0.552093
[epoch18, step1466]: loss 0.262184
[epoch18, step1467]: loss 0.418732
[epoch18, step1468]: loss 0.576109
[epoch18, step1469]: loss 0.520726
[epoch18, step1470]: loss 0.408452
[epoch18, step1471]: loss 0.545187
[epoch18, step1472]: loss 0.487215
[epoch18, step1473]: loss 0.414204
[epoch18, step1474]: loss 0.531026
[epoch18, step1475]: loss 0.738267
[epoch18, step1476]: loss 0.459725
[epoch18, step1477]: loss 0.536283
[epoch18, step1478]: loss 0.346615
[epoch18, step1479]: loss 0.526985
[epoch18, step1480]: loss 0.544581
[epoch18, step1481]: loss 0.290816
[epoch18, step1482]: loss 0.420226
[epoch18, step1483]: loss 0.391771
[epoch18, step1484]: loss 0.427942
[epoch18, step1485]: loss 0.519359
[epoch18, step1486]: loss 0.561956
[epoch18, step1487]: loss 0.577520
[epoch18, step1488]: loss 0.701948
[epoch18, step1489]: loss 0.444472
[epoch18, step1490]: loss 0.246118
[epoch18, step1491]: loss 0.587714
[epoch18, step1492]: loss 0.423736
[epoch18, step1493]: loss 0.438737
[epoch18, step1494]: loss 0.436745
[epoch18, step1495]: loss 0.523269
[epoch18, step1496]: loss 0.390957
[epoch18, step1497]: loss 0.332390
[epoch18, step1498]: loss 0.344576
[epoch18, step1499]: loss 0.645906
[epoch18, step1500]: loss 0.525148
[epoch18, step1501]: loss 0.675703
[epoch18, step1502]: loss 0.592005
[epoch18, step1503]: loss 0.470692
[epoch18, step1504]: loss 0.597556
[epoch18, step1505]: loss 0.378027
[epoch18, step1506]: loss 0.374089
[epoch18, step1507]: loss 0.541075
[epoch18, step1508]: loss 0.191517
[epoch18, step1509]: loss 0.434436
[epoch18, step1510]: loss 0.662595
[epoch18, step1511]: loss 0.484714
[epoch18, step1512]: loss 0.441292
[epoch18, step1513]: loss 0.510660
[epoch18, step1514]: loss 0.557074
[epoch18, step1515]: loss 0.385097
[epoch18, step1516]: loss 0.467468
[epoch18, step1517]: loss 0.529623
[epoch18, step1518]: loss 0.493256
[epoch18, step1519]: loss 0.551329
[epoch18, step1520]: loss 0.490562
[epoch18, step1521]: loss 0.387148
[epoch18, step1522]: loss 0.349926
[epoch18, step1523]: loss 0.450080
[epoch18, step1524]: loss 0.359776
[epoch18, step1525]: loss 0.476613
[epoch18, step1526]: loss 0.638194
[epoch18, step1527]: loss 0.247149
[epoch18, step1528]: loss 0.381137
[epoch18, step1529]: loss 0.429598
[epoch18, step1530]: loss 0.316472
[epoch18, step1531]: loss 0.104317
[epoch18, step1532]: loss 0.380725
[epoch18, step1533]: loss 0.569754
[epoch18, step1534]: loss 0.326360
[epoch18, step1535]: loss 0.615253
[epoch18, step1536]: loss 0.490601
[epoch18, step1537]: loss 0.287543
[epoch18, step1538]: loss 0.447107
[epoch18, step1539]: loss 0.584965
[epoch18, step1540]: loss 0.449467
[epoch18, step1541]: loss 0.421936
[epoch18, step1542]: loss 0.422998
[epoch18, step1543]: loss 0.566675
[epoch18, step1544]: loss 0.580984
[epoch18, step1545]: loss 0.578781
[epoch18, step1546]: loss 0.532357
[epoch18, step1547]: loss 0.530897
[epoch18, step1548]: loss 0.629518
[epoch18, step1549]: loss 0.518072
[epoch18, step1550]: loss 0.409972
[epoch18, step1551]: loss 0.345674
[epoch18, step1552]: loss 0.360518
[epoch18, step1553]: loss 0.543701
[epoch18, step1554]: loss 0.332285
[epoch18, step1555]: loss 0.437811
[epoch18, step1556]: loss 0.376293
[epoch18, step1557]: loss 0.415237
[epoch18, step1558]: loss 0.370670
[epoch18, step1559]: loss 0.371905
[epoch18, step1560]: loss 0.448424
[epoch18, step1561]: loss 0.565686
[epoch18, step1562]: loss 0.427628
[epoch18, step1563]: loss 0.372374
[epoch18, step1564]: loss 0.532627
[epoch18, step1565]: loss 0.262177
[epoch18, step1566]: loss 0.208756
[epoch18, step1567]: loss 0.457418
[epoch18, step1568]: loss 0.292961
[epoch18, step1569]: loss 0.307231
[epoch18, step1570]: loss 0.649172
[epoch18, step1571]: loss 0.606356
[epoch18, step1572]: loss 0.568913
[epoch18, step1573]: loss 0.443925
[epoch18, step1574]: loss 0.521966
[epoch18, step1575]: loss 0.233257
[epoch18, step1576]: loss 0.233640
[epoch18, step1577]: loss 0.512883
[epoch18, step1578]: loss 0.431984
[epoch18, step1579]: loss 0.487153
[epoch18, step1580]: loss 0.446504
[epoch18, step1581]: loss 0.585638
[epoch18, step1582]: loss 0.409327
[epoch18, step1583]: loss 0.503420
[epoch18, step1584]: loss 0.445702
[epoch18, step1585]: loss 0.608437
[epoch18, step1586]: loss 0.469034
[epoch18, step1587]: loss 0.409072
[epoch18, step1588]: loss 0.378589
[epoch18, step1589]: loss 0.331900
[epoch18, step1590]: loss 0.147540
[epoch18, step1591]: loss 0.379776
[epoch18, step1592]: loss 0.505453
[epoch18, step1593]: loss 0.432299
[epoch18, step1594]: loss 0.424533
[epoch18, step1595]: loss 0.560182
[epoch18, step1596]: loss 0.482078
[epoch18, step1597]: loss 0.420486
[epoch18, step1598]: loss 0.290434
[epoch18, step1599]: loss 0.393453
[epoch18, step1600]: loss 0.290179
[epoch18, step1601]: loss 0.782283
[epoch18, step1602]: loss 0.566634
[epoch18, step1603]: loss 0.443896
[epoch18, step1604]: loss 0.383969
[epoch18, step1605]: loss 0.352070
[epoch18, step1606]: loss 0.409495
[epoch18, step1607]: loss 0.502067
[epoch18, step1608]: loss 0.514588
[epoch18, step1609]: loss 0.206763
[epoch18, step1610]: loss 0.227663
[epoch18, step1611]: loss 0.421452
[epoch18, step1612]: loss 0.601971
[epoch18, step1613]: loss 0.478820
[epoch18, step1614]: loss 0.537506
[epoch18, step1615]: loss 0.321187
[epoch18, step1616]: loss 0.629293
[epoch18, step1617]: loss 0.327687
[epoch18, step1618]: loss 0.573922
[epoch18, step1619]: loss 0.444612
[epoch18, step1620]: loss 0.290241
[epoch18, step1621]: loss 0.570639
[epoch18, step1622]: loss 0.430020
[epoch18, step1623]: loss 0.305316
[epoch18, step1624]: loss 0.464455
[epoch18, step1625]: loss 0.713724
[epoch18, step1626]: loss 0.529613
[epoch18, step1627]: loss 0.617829
[epoch18, step1628]: loss 0.366612
[epoch18, step1629]: loss 0.688180
[epoch18, step1630]: loss 0.608201
[epoch18, step1631]: loss 0.595924
[epoch18, step1632]: loss 0.350557
[epoch18, step1633]: loss 0.537452
[epoch18, step1634]: loss 0.534063
[epoch18, step1635]: loss 0.487188
[epoch18, step1636]: loss 0.522795
[epoch18, step1637]: loss 0.360076
[epoch18, step1638]: loss 0.420683
[epoch18, step1639]: loss 0.375697
[epoch18, step1640]: loss 0.494612
[epoch18, step1641]: loss 0.359746
[epoch18, step1642]: loss 0.520751
[epoch18, step1643]: loss 0.545129
[epoch18, step1644]: loss 0.434503
[epoch18, step1645]: loss 0.341139
[epoch18, step1646]: loss 0.258279
[epoch18, step1647]: loss 0.796205
[epoch18, step1648]: loss 0.343228
[epoch18, step1649]: loss 0.373767
[epoch18, step1650]: loss 0.507890
[epoch18, step1651]: loss 0.566780
[epoch18, step1652]: loss 0.400706
[epoch18, step1653]: loss 0.416008
[epoch18, step1654]: loss 0.600380
[epoch18, step1655]: loss 0.649276
[epoch18, step1656]: loss 0.429834
[epoch18, step1657]: loss 0.543694
[epoch18, step1658]: loss 0.257538
[epoch18, step1659]: loss 0.434497
[epoch18, step1660]: loss 0.319153
[epoch18, step1661]: loss 0.490738
[epoch18, step1662]: loss 0.581131
[epoch18, step1663]: loss 0.481812
[epoch18, step1664]: loss 0.420072
[epoch18, step1665]: loss 0.751498
[epoch18, step1666]: loss 0.374548
[epoch18, step1667]: loss 0.612882
[epoch18, step1668]: loss 0.405202
[epoch18, step1669]: loss 0.536572
[epoch18, step1670]: loss 0.408965
[epoch18, step1671]: loss 0.311817
[epoch18, step1672]: loss 0.453010
[epoch18, step1673]: loss 0.356408
[epoch18, step1674]: loss 0.500899
[epoch18, step1675]: loss 0.348052
[epoch18, step1676]: loss 0.458147
[epoch18, step1677]: loss 0.417081
[epoch18, step1678]: loss 0.443004
[epoch18, step1679]: loss 0.504167
[epoch18, step1680]: loss 0.495283
[epoch18, step1681]: loss 0.254524
[epoch18, step1682]: loss 0.451788
[epoch18, step1683]: loss 0.605993
[epoch18, step1684]: loss 0.487267
[epoch18, step1685]: loss 0.619434
[epoch18, step1686]: loss 0.441797
[epoch18, step1687]: loss 0.565870
[epoch18, step1688]: loss 0.164352
[epoch18, step1689]: loss 0.346270
[epoch18, step1690]: loss 0.380058
[epoch18, step1691]: loss 0.585054
[epoch18, step1692]: loss 0.323215
[epoch18, step1693]: loss 0.439887
[epoch18, step1694]: loss 0.524447
[epoch18, step1695]: loss 0.375889
[epoch18, step1696]: loss 0.547106
[epoch18, step1697]: loss 0.450459
[epoch18, step1698]: loss 0.241644
[epoch18, step1699]: loss 0.473074
[epoch18, step1700]: loss 0.405353
[epoch18, step1701]: loss 0.458160
[epoch18, step1702]: loss 0.235238
[epoch18, step1703]: loss 0.705060
[epoch18, step1704]: loss 0.675746
[epoch18, step1705]: loss 0.412595
[epoch18, step1706]: loss 0.619999
[epoch18, step1707]: loss 0.749677
[epoch18, step1708]: loss 0.407523
[epoch18, step1709]: loss 0.449394
[epoch18, step1710]: loss 0.350558
[epoch18, step1711]: loss 0.552806
[epoch18, step1712]: loss 0.636663
[epoch18, step1713]: loss 0.402265
[epoch18, step1714]: loss 0.470960
[epoch18, step1715]: loss 0.453011
[epoch18, step1716]: loss 0.487269
[epoch18, step1717]: loss 0.370663
[epoch18, step1718]: loss 0.563488
[epoch18, step1719]: loss 0.415845
[epoch18, step1720]: loss 0.514406
[epoch18, step1721]: loss 0.533992
[epoch18, step1722]: loss 0.374215
[epoch18, step1723]: loss 0.564605
[epoch18, step1724]: loss 0.419266
[epoch18, step1725]: loss 0.422009
[epoch18, step1726]: loss 0.338075
[epoch18, step1727]: loss 0.449214
[epoch18, step1728]: loss 0.547761
[epoch18, step1729]: loss 0.614731
[epoch18, step1730]: loss 0.421692
[epoch18, step1731]: loss 0.591762
[epoch18, step1732]: loss 0.710227
[epoch18, step1733]: loss 0.593129
[epoch18, step1734]: loss 0.619620
[epoch18, step1735]: loss 0.386843
[epoch18, step1736]: loss 0.307020
[epoch18, step1737]: loss 0.416093
[epoch18, step1738]: loss 0.403100
[epoch18, step1739]: loss 0.296878
[epoch18, step1740]: loss 0.372129
[epoch18, step1741]: loss 0.351925
[epoch18, step1742]: loss 0.296018
[epoch18, step1743]: loss 0.519245
[epoch18, step1744]: loss 0.345136
[epoch18, step1745]: loss 0.422825
[epoch18, step1746]: loss 0.563521
[epoch18, step1747]: loss 0.498465
[epoch18, step1748]: loss 0.393329
[epoch18, step1749]: loss 0.681270
[epoch18, step1750]: loss 0.419657
[epoch18, step1751]: loss 0.727492
[epoch18, step1752]: loss 0.625491
[epoch18, step1753]: loss 0.506877
[epoch18, step1754]: loss 0.464656
[epoch18, step1755]: loss 0.346725
[epoch18, step1756]: loss 0.474360
[epoch18, step1757]: loss 0.530582
[epoch18, step1758]: loss 0.451178
[epoch18, step1759]: loss 0.342820
[epoch18, step1760]: loss 0.451328
[epoch18, step1761]: loss 0.472912
[epoch18, step1762]: loss 0.618966
[epoch18, step1763]: loss 0.426407
[epoch18, step1764]: loss 0.450978
[epoch18, step1765]: loss 0.451282
[epoch18, step1766]: loss 0.236978
[epoch18, step1767]: loss 0.501240
[epoch18, step1768]: loss 0.382703
[epoch18, step1769]: loss 0.257059
[epoch18, step1770]: loss 0.369987
[epoch18, step1771]: loss 0.540877
[epoch18, step1772]: loss 0.514462
[epoch18, step1773]: loss 0.365957
[epoch18, step1774]: loss 0.471473
[epoch18, step1775]: loss 0.400689
[epoch18, step1776]: loss 0.307595
[epoch18, step1777]: loss 0.423869
[epoch18, step1778]: loss 0.339253
[epoch18, step1779]: loss 0.469707
[epoch18, step1780]: loss 0.416473
[epoch18, step1781]: loss 0.671644
[epoch18, step1782]: loss 0.473670
[epoch18, step1783]: loss 0.389640
[epoch18, step1784]: loss 0.315956
[epoch18, step1785]: loss 0.698550
[epoch18, step1786]: loss 0.373254
[epoch18, step1787]: loss 0.563164
[epoch18, step1788]: loss 0.621100
[epoch18, step1789]: loss 0.477170
[epoch18, step1790]: loss 0.663799
[epoch18, step1791]: loss 0.720898
[epoch18, step1792]: loss 0.735754
[epoch18, step1793]: loss 0.439942
[epoch18, step1794]: loss 0.448534
[epoch18, step1795]: loss 0.564026
[epoch18, step1796]: loss 0.481429
[epoch18, step1797]: loss 0.232682
[epoch18, step1798]: loss 0.324378
[epoch18, step1799]: loss 0.498858
[epoch18, step1800]: loss 0.295731
[epoch18, step1801]: loss 0.360148
[epoch18, step1802]: loss 0.551266
[epoch18, step1803]: loss 0.494609
[epoch18, step1804]: loss 0.741708
[epoch18, step1805]: loss 0.438755
[epoch18, step1806]: loss 0.374961
[epoch18, step1807]: loss 0.461688
[epoch18, step1808]: loss 0.288701
[epoch18, step1809]: loss 0.352784
[epoch18, step1810]: loss 0.518712
[epoch18, step1811]: loss 0.427842
[epoch18, step1812]: loss 0.199929
[epoch18, step1813]: loss 0.556899
[epoch18, step1814]: loss 0.555502
[epoch18, step1815]: loss 0.543099
[epoch18, step1816]: loss 0.500368
[epoch18, step1817]: loss 0.445271
[epoch18, step1818]: loss 0.525063
[epoch18, step1819]: loss 0.138082
[epoch18, step1820]: loss 0.507617
[epoch18, step1821]: loss 0.631964
[epoch18, step1822]: loss 0.559042
[epoch18, step1823]: loss 0.699263
[epoch18, step1824]: loss 0.594097
[epoch18, step1825]: loss 0.558003
[epoch18, step1826]: loss 0.252167
[epoch18, step1827]: loss 0.457261
[epoch18, step1828]: loss 0.479772
[epoch18, step1829]: loss 0.548553
[epoch18, step1830]: loss 0.522736
[epoch18, step1831]: loss 0.327895
[epoch18, step1832]: loss 0.426729
[epoch18, step1833]: loss 0.454059
[epoch18, step1834]: loss 0.505490
[epoch18, step1835]: loss 0.355501
[epoch18, step1836]: loss 0.419767
[epoch18, step1837]: loss 0.671063
[epoch18, step1838]: loss 0.699752
[epoch18, step1839]: loss 0.527328
[epoch18, step1840]: loss 0.589213
[epoch18, step1841]: loss 0.665041
[epoch18, step1842]: loss 0.273004
[epoch18, step1843]: loss 0.418433
[epoch18, step1844]: loss 0.511812
[epoch18, step1845]: loss 0.646674
[epoch18, step1846]: loss 0.393109
[epoch18, step1847]: loss 0.386445
[epoch18, step1848]: loss 0.509655
[epoch18, step1849]: loss 0.390283
[epoch18, step1850]: loss 0.282152
[epoch18, step1851]: loss 0.433814
[epoch18, step1852]: loss 0.541160
[epoch18, step1853]: loss 0.447626
[epoch18, step1854]: loss 0.248706
[epoch18, step1855]: loss 0.295873
[epoch18, step1856]: loss 0.469611
[epoch18, step1857]: loss 0.585285
[epoch18, step1858]: loss 0.273065
[epoch18, step1859]: loss 0.587281
[epoch18, step1860]: loss 0.478515
[epoch18, step1861]: loss 0.368339
[epoch18, step1862]: loss 0.603685
[epoch18, step1863]: loss 0.312055
[epoch18, step1864]: loss 0.555205
[epoch18, step1865]: loss 0.541522
[epoch18, step1866]: loss 0.596080
[epoch18, step1867]: loss 0.497713
[epoch18, step1868]: loss 0.293091
[epoch18, step1869]: loss 0.313351
[epoch18, step1870]: loss 0.649490
[epoch18, step1871]: loss 0.493588
[epoch18, step1872]: loss 0.649690
[epoch18, step1873]: loss 0.656424
[epoch18, step1874]: loss 0.476260
[epoch18, step1875]: loss 0.711317
[epoch18, step1876]: loss 0.508673
[epoch18, step1877]: loss 0.659034
[epoch18, step1878]: loss 0.469283
[epoch18, step1879]: loss 0.562526
[epoch18, step1880]: loss 0.279547
[epoch18, step1881]: loss 0.495461
[epoch18, step1882]: loss 0.474538
[epoch18, step1883]: loss 0.771012
[epoch18, step1884]: loss 0.497557
[epoch18, step1885]: loss 0.177851
[epoch18, step1886]: loss 0.383710
[epoch18, step1887]: loss 0.562030
[epoch18, step1888]: loss 0.567934
[epoch18, step1889]: loss 0.568542
[epoch18, step1890]: loss 0.430407
[epoch18, step1891]: loss 0.384888
[epoch18, step1892]: loss 0.396015
[epoch18, step1893]: loss 0.684577
[epoch18, step1894]: loss 0.501080
[epoch18, step1895]: loss 0.488272
[epoch18, step1896]: loss 0.482553
[epoch18, step1897]: loss 0.332004
[epoch18, step1898]: loss 0.598828
[epoch18, step1899]: loss 0.520945
[epoch18, step1900]: loss 0.384140
[epoch18, step1901]: loss 0.299346
[epoch18, step1902]: loss 0.514245
[epoch18, step1903]: loss 0.499674
[epoch18, step1904]: loss 0.401150
[epoch18, step1905]: loss 0.520828
[epoch18, step1906]: loss 0.411481
[epoch18, step1907]: loss 0.408821
[epoch18, step1908]: loss 0.445275
[epoch18, step1909]: loss 0.420659
[epoch18, step1910]: loss 0.269109
[epoch18, step1911]: loss 0.536685
[epoch18, step1912]: loss 0.515798
[epoch18, step1913]: loss 0.478652
[epoch18, step1914]: loss 0.125395
[epoch18, step1915]: loss 0.491688
[epoch18, step1916]: loss 0.439822
[epoch18, step1917]: loss 0.522927
[epoch18, step1918]: loss 0.398844
[epoch18, step1919]: loss 0.571560
[epoch18, step1920]: loss 0.367789
[epoch18, step1921]: loss 0.296364
[epoch18, step1922]: loss 0.416046
[epoch18, step1923]: loss 0.562584
[epoch18, step1924]: loss 0.664947
[epoch18, step1925]: loss 0.389318
[epoch18, step1926]: loss 0.476359
[epoch18, step1927]: loss 0.415682
[epoch18, step1928]: loss 0.347452
[epoch18, step1929]: loss 0.532302
[epoch18, step1930]: loss 0.350230
[epoch18, step1931]: loss 0.474825
[epoch18, step1932]: loss 0.384334
[epoch18, step1933]: loss 0.454336
[epoch18, step1934]: loss 0.624469
[epoch18, step1935]: loss 0.558990
[epoch18, step1936]: loss 0.259238
[epoch18, step1937]: loss 0.589003
[epoch18, step1938]: loss 0.346129
[epoch18, step1939]: loss 0.467411
[epoch18, step1940]: loss 0.132298
[epoch18, step1941]: loss 0.536722
[epoch18, step1942]: loss 0.392136
[epoch18, step1943]: loss 0.532166
[epoch18, step1944]: loss 0.651242
[epoch18, step1945]: loss 0.344184
[epoch18, step1946]: loss 0.413156
[epoch18, step1947]: loss 0.435028
[epoch18, step1948]: loss 0.562243
[epoch18, step1949]: loss 0.619509
[epoch18, step1950]: loss 0.558165
[epoch18, step1951]: loss 0.496054
[epoch18, step1952]: loss 0.551865
[epoch18, step1953]: loss 0.625658
[epoch18, step1954]: loss 0.507004
[epoch18, step1955]: loss 0.494455
[epoch18, step1956]: loss 0.521788
[epoch18, step1957]: loss 0.347261
[epoch18, step1958]: loss 0.735233
[epoch18, step1959]: loss 0.433826
[epoch18, step1960]: loss 0.571079
[epoch18, step1961]: loss 0.360190
[epoch18, step1962]: loss 0.329375
[epoch18, step1963]: loss 0.553141
[epoch18, step1964]: loss 0.434531
[epoch18, step1965]: loss 0.545131
[epoch18, step1966]: loss 0.702679
[epoch18, step1967]: loss 0.331699
[epoch18, step1968]: loss 0.560898
[epoch18, step1969]: loss 0.403673
[epoch18, step1970]: loss 0.508522
[epoch18, step1971]: loss 0.325681
[epoch18, step1972]: loss 0.429042
[epoch18, step1973]: loss 0.353937
[epoch18, step1974]: loss 0.518949
[epoch18, step1975]: loss 0.404674
[epoch18, step1976]: loss 0.545336
[epoch18, step1977]: loss 0.321869
[epoch18, step1978]: loss 0.382755
[epoch18, step1979]: loss 0.734029
[epoch18, step1980]: loss 0.566181
[epoch18, step1981]: loss 0.528831
[epoch18, step1982]: loss 0.555861
[epoch18, step1983]: loss 0.666348
[epoch18, step1984]: loss 0.429051
[epoch18, step1985]: loss 0.596342
[epoch18, step1986]: loss 0.330354
[epoch18, step1987]: loss 0.442397
[epoch18, step1988]: loss 0.615963
[epoch18, step1989]: loss 0.541943
[epoch18, step1990]: loss 0.634009
[epoch18, step1991]: loss 0.480568
[epoch18, step1992]: loss 0.632104
[epoch18, step1993]: loss 0.461150
[epoch18, step1994]: loss 0.329668
[epoch18, step1995]: loss 0.374964
[epoch18, step1996]: loss 0.218485
[epoch18, step1997]: loss 0.338730
[epoch18, step1998]: loss 0.462643
[epoch18, step1999]: loss 0.348147
[epoch18, step2000]: loss 0.565907
[epoch18, step2001]: loss 0.334710
[epoch18, step2002]: loss 0.485598
[epoch18, step2003]: loss 0.529925
[epoch18, step2004]: loss 0.489609
[epoch18, step2005]: loss 0.459840
[epoch18, step2006]: loss 0.617176
[epoch18, step2007]: loss 0.353993
[epoch18, step2008]: loss 0.580534
[epoch18, step2009]: loss 0.607270
[epoch18, step2010]: loss 0.393549
[epoch18, step2011]: loss 0.508919
[epoch18, step2012]: loss 0.590236
[epoch18, step2013]: loss 0.111532
[epoch18, step2014]: loss 0.289476
[epoch18, step2015]: loss 0.301009
[epoch18, step2016]: loss 0.607612
[epoch18, step2017]: loss 0.516800
[epoch18, step2018]: loss 0.248483
[epoch18, step2019]: loss 0.447421
[epoch18, step2020]: loss 0.344719
[epoch18, step2021]: loss 0.635482
[epoch18, step2022]: loss 0.504032
[epoch18, step2023]: loss 0.504080
[epoch18, step2024]: loss 0.421252
[epoch18, step2025]: loss 0.519642
[epoch18, step2026]: loss 0.486817
[epoch18, step2027]: loss 0.617231
[epoch18, step2028]: loss 0.716266
[epoch18, step2029]: loss 0.154891
[epoch18, step2030]: loss 0.288058
[epoch18, step2031]: loss 0.536679
[epoch18, step2032]: loss 0.381093
[epoch18, step2033]: loss 0.552600
[epoch18, step2034]: loss 0.467377
[epoch18, step2035]: loss 0.470048
[epoch18, step2036]: loss 0.491277
[epoch18, step2037]: loss 0.319099
[epoch18, step2038]: loss 0.362681
[epoch18, step2039]: loss 0.588230
[epoch18, step2040]: loss 0.413894
[epoch18, step2041]: loss 0.502624
[epoch18, step2042]: loss 0.392695
[epoch18, step2043]: loss 0.491853
[epoch18, step2044]: loss 0.372584
[epoch18, step2045]: loss 0.209474
[epoch18, step2046]: loss 0.409879
[epoch18, step2047]: loss 0.496835
[epoch18, step2048]: loss 0.276432
[epoch18, step2049]: loss 0.525858
[epoch18, step2050]: loss 0.206289
[epoch18, step2051]: loss 0.227167
[epoch18, step2052]: loss 0.619303
[epoch18, step2053]: loss 0.445146
[epoch18, step2054]: loss 0.647947
[epoch18, step2055]: loss 0.346445
[epoch18, step2056]: loss 0.472997
[epoch18, step2057]: loss 0.556839
[epoch18, step2058]: loss 0.681519
[epoch18, step2059]: loss 0.542728
[epoch18, step2060]: loss 0.520433
[epoch18, step2061]: loss 0.395753
[epoch18, step2062]: loss 0.524409
[epoch18, step2063]: loss 0.478295
[epoch18, step2064]: loss 0.617649
[epoch18, step2065]: loss 0.517242
[epoch18, step2066]: loss 0.674892
[epoch18, step2067]: loss 0.373899
[epoch18, step2068]: loss 0.477271
[epoch18, step2069]: loss 0.326459
[epoch18, step2070]: loss 0.334680
[epoch18, step2071]: loss 0.501389
[epoch18, step2072]: loss 0.417404
[epoch18, step2073]: loss 0.633405
[epoch18, step2074]: loss 0.347020
[epoch18, step2075]: loss 0.373127
[epoch18, step2076]: loss 0.457475
[epoch18, step2077]: loss 0.343361
[epoch18, step2078]: loss 0.440568
[epoch18, step2079]: loss 0.450293
[epoch18, step2080]: loss 0.380167
[epoch18, step2081]: loss 0.189433
[epoch18, step2082]: loss 0.543647
[epoch18, step2083]: loss 0.401109
[epoch18, step2084]: loss 0.514939
[epoch18, step2085]: loss 0.374791
[epoch18, step2086]: loss 0.560340
[epoch18, step2087]: loss 0.473134
[epoch18, step2088]: loss 0.512319
[epoch18, step2089]: loss 0.565302
[epoch18, step2090]: loss 0.536115
[epoch18, step2091]: loss 0.316792
[epoch18, step2092]: loss 0.447060
[epoch18, step2093]: loss 0.430109
[epoch18, step2094]: loss 0.478717
[epoch18, step2095]: loss 0.633798
[epoch18, step2096]: loss 0.359584
[epoch18, step2097]: loss 0.537165
[epoch18, step2098]: loss 0.583184
[epoch18, step2099]: loss 0.398450
[epoch18, step2100]: loss 0.655681
[epoch18, step2101]: loss 0.776185
[epoch18, step2102]: loss 0.319725
[epoch18, step2103]: loss 0.503146
[epoch18, step2104]: loss 0.455415
[epoch18, step2105]: loss 0.391678
[epoch18, step2106]: loss 0.289358
[epoch18, step2107]: loss 0.477275
[epoch18, step2108]: loss 0.551041
[epoch18, step2109]: loss 0.579224
[epoch18, step2110]: loss 0.616153
[epoch18, step2111]: loss 0.409287
[epoch18, step2112]: loss 0.283983
[epoch18, step2113]: loss 0.538116
[epoch18, step2114]: loss 0.647770
[epoch18, step2115]: loss 0.342075
[epoch18, step2116]: loss 0.417079
[epoch18, step2117]: loss 0.452915
[epoch18, step2118]: loss 0.587587
[epoch18, step2119]: loss 0.428928
[epoch18, step2120]: loss 0.388778
[epoch18, step2121]: loss 0.270565
[epoch18, step2122]: loss 0.520800
[epoch18, step2123]: loss 0.527563
[epoch18, step2124]: loss 0.700298
[epoch18, step2125]: loss 0.547660
[epoch18, step2126]: loss 0.368323
[epoch18, step2127]: loss 0.192619
[epoch18, step2128]: loss 0.616352
[epoch18, step2129]: loss 0.482859
[epoch18, step2130]: loss 0.359170
[epoch18, step2131]: loss 0.372608
[epoch18, step2132]: loss 0.359669
[epoch18, step2133]: loss 0.458337
[epoch18, step2134]: loss 0.525062
[epoch18, step2135]: loss 0.601222
[epoch18, step2136]: loss 0.495220
[epoch18, step2137]: loss 0.355999
[epoch18, step2138]: loss 0.526109
[epoch18, step2139]: loss 0.281190
[epoch18, step2140]: loss 0.515259
[epoch18, step2141]: loss 0.536123
[epoch18, step2142]: loss 0.223788
[epoch18, step2143]: loss 0.421653
[epoch18, step2144]: loss 0.348736
[epoch18, step2145]: loss 0.370847
[epoch18, step2146]: loss 0.466367
[epoch18, step2147]: loss 0.521604
[epoch18, step2148]: loss 0.618369
[epoch18, step2149]: loss 0.553068
[epoch18, step2150]: loss 0.432630
[epoch18, step2151]: loss 0.575594
[epoch18, step2152]: loss 0.505343
[epoch18, step2153]: loss 0.636661
[epoch18, step2154]: loss 0.655687
[epoch18, step2155]: loss 0.528718
[epoch18, step2156]: loss 0.368756
[epoch18, step2157]: loss 0.415038
[epoch18, step2158]: loss 0.438841
[epoch18, step2159]: loss 0.483488
[epoch18, step2160]: loss 0.310532
[epoch18, step2161]: loss 0.367193
[epoch18, step2162]: loss 0.373669
[epoch18, step2163]: loss 0.637427
[epoch18, step2164]: loss 0.723207
[epoch18, step2165]: loss 0.461833
[epoch18, step2166]: loss 0.488746
[epoch18, step2167]: loss 0.402728
[epoch18, step2168]: loss 0.513410
[epoch18, step2169]: loss 0.273122
[epoch18, step2170]: loss 0.383271
[epoch18, step2171]: loss 0.529164
[epoch18, step2172]: loss 0.542958
[epoch18, step2173]: loss 0.512987
[epoch18, step2174]: loss 0.585335
[epoch18, step2175]: loss 0.469255
[epoch18, step2176]: loss 0.622923
[epoch18, step2177]: loss 0.498427
[epoch18, step2178]: loss 0.454320
[epoch18, step2179]: loss 0.601531
[epoch18, step2180]: loss 0.504663
[epoch18, step2181]: loss 0.550008
[epoch18, step2182]: loss 0.607512
[epoch18, step2183]: loss 0.368540
[epoch18, step2184]: loss 0.526311
[epoch18, step2185]: loss 0.603902
[epoch18, step2186]: loss 0.340288
[epoch18, step2187]: loss 0.414997
[epoch18, step2188]: loss 0.392190
[epoch18, step2189]: loss 0.594579
[epoch18, step2190]: loss 0.466028
[epoch18, step2191]: loss 0.398986
[epoch18, step2192]: loss 0.614444
[epoch18, step2193]: loss 0.405171
[epoch18, step2194]: loss 0.543752
[epoch18, step2195]: loss 0.508225
[epoch18, step2196]: loss 0.402177
[epoch18, step2197]: loss 0.501722
[epoch18, step2198]: loss 0.432296
[epoch18, step2199]: loss 0.300751
[epoch18, step2200]: loss 0.408137
[epoch18, step2201]: loss 0.442661
[epoch18, step2202]: loss 0.585626
[epoch18, step2203]: loss 0.455408
[epoch18, step2204]: loss 0.503784
[epoch18, step2205]: loss 0.431654
[epoch18, step2206]: loss 0.364162
[epoch18, step2207]: loss 0.359527
[epoch18, step2208]: loss 0.436982
[epoch18, step2209]: loss 0.571748
[epoch18, step2210]: loss 0.470255
[epoch18, step2211]: loss 0.356766
[epoch18, step2212]: loss 0.436776
[epoch18, step2213]: loss 0.458244
[epoch18, step2214]: loss 0.563882
[epoch18, step2215]: loss 0.600163
[epoch18, step2216]: loss 0.517983
[epoch18, step2217]: loss 0.377403
[epoch18, step2218]: loss 0.490790
[epoch18, step2219]: loss 0.534583
[epoch18, step2220]: loss 0.209667
[epoch18, step2221]: loss 0.401489
[epoch18, step2222]: loss 0.602176
[epoch18, step2223]: loss 0.416532
[epoch18, step2224]: loss 0.320488
[epoch18, step2225]: loss 0.361934
[epoch18, step2226]: loss 0.597973
[epoch18, step2227]: loss 0.274229
[epoch18, step2228]: loss 0.252919
[epoch18, step2229]: loss 0.540910
[epoch18, step2230]: loss 0.379673
[epoch18, step2231]: loss 0.642415
[epoch18, step2232]: loss 0.423199
[epoch18, step2233]: loss 0.512010
[epoch18, step2234]: loss 0.563887
[epoch18, step2235]: loss 0.442899
[epoch18, step2236]: loss 0.414735
[epoch18, step2237]: loss 0.436616
[epoch18, step2238]: loss 0.465735
[epoch18, step2239]: loss 0.538086
[epoch18, step2240]: loss 0.565738
[epoch18, step2241]: loss 0.614819
[epoch18, step2242]: loss 0.562293
[epoch18, step2243]: loss 0.472043
[epoch18, step2244]: loss 0.534006
[epoch18, step2245]: loss 0.480500
[epoch18, step2246]: loss 0.532498
[epoch18, step2247]: loss 0.318092
[epoch18, step2248]: loss 0.487382
[epoch18, step2249]: loss 0.540753
[epoch18, step2250]: loss 0.552101
[epoch18, step2251]: loss 0.559004
[epoch18, step2252]: loss 0.583925
[epoch18, step2253]: loss 0.764658
[epoch18, step2254]: loss 0.633423
[epoch18, step2255]: loss 0.541787
[epoch18, step2256]: loss 0.363244
[epoch18, step2257]: loss 0.634871
[epoch18, step2258]: loss 0.637610
[epoch18, step2259]: loss 0.324987
[epoch18, step2260]: loss 0.459221
[epoch18, step2261]: loss 0.203495
[epoch18, step2262]: loss 0.223069
[epoch18, step2263]: loss 0.459006
[epoch18, step2264]: loss 0.424954
[epoch18, step2265]: loss 0.416275
[epoch18, step2266]: loss 0.517409
[epoch18, step2267]: loss 0.458352
[epoch18, step2268]: loss 0.489645
[epoch18, step2269]: loss 0.674329
[epoch18, step2270]: loss 0.446430
[epoch18, step2271]: loss 0.576581
[epoch18, step2272]: loss 0.562044
[epoch18, step2273]: loss 0.591802
[epoch18, step2274]: loss 0.363290
[epoch18, step2275]: loss 0.391259
[epoch18, step2276]: loss 0.417794
[epoch18, step2277]: loss 0.683042
[epoch18, step2278]: loss 0.607055
[epoch18, step2279]: loss 0.220643
[epoch18, step2280]: loss 0.610580
[epoch18, step2281]: loss 0.399490
[epoch18, step2282]: loss 0.576045
[epoch18, step2283]: loss 0.510763
[epoch18, step2284]: loss 0.536243
[epoch18, step2285]: loss 0.488025
[epoch18, step2286]: loss 0.409389
[epoch18, step2287]: loss 0.573491
[epoch18, step2288]: loss 0.241322
[epoch18, step2289]: loss 0.600248
[epoch18, step2290]: loss 0.263166
[epoch18, step2291]: loss 0.120090
[epoch18, step2292]: loss 0.420731
[epoch18, step2293]: loss 0.257376
[epoch18, step2294]: loss 0.504111
[epoch18, step2295]: loss 0.437093
[epoch18, step2296]: loss 0.437482
[epoch18, step2297]: loss 0.542167
[epoch18, step2298]: loss 0.419984
[epoch18, step2299]: loss 0.571158
[epoch18, step2300]: loss 0.542663
[epoch18, step2301]: loss 0.518173
[epoch18, step2302]: loss 0.454131
[epoch18, step2303]: loss 0.601110
[epoch18, step2304]: loss 0.378764
[epoch18, step2305]: loss 0.667281
[epoch18, step2306]: loss 0.582396
[epoch18, step2307]: loss 0.572815
[epoch18, step2308]: loss 0.533371
[epoch18, step2309]: loss 0.156028
[epoch18, step2310]: loss 0.467537
[epoch18, step2311]: loss 0.394082
[epoch18, step2312]: loss 0.491092
[epoch18, step2313]: loss 0.656035
[epoch18, step2314]: loss 0.308995
[epoch18, step2315]: loss 0.605537
[epoch18, step2316]: loss 0.542265
[epoch18, step2317]: loss 0.581901
[epoch18, step2318]: loss 0.545578
[epoch18, step2319]: loss 0.786089
[epoch18, step2320]: loss 0.592693
[epoch18, step2321]: loss 0.378878
[epoch18, step2322]: loss 0.590648
[epoch18, step2323]: loss 0.496247
[epoch18, step2324]: loss 0.635016
[epoch18, step2325]: loss 0.429139
[epoch18, step2326]: loss 0.268976
[epoch18, step2327]: loss 0.514309
[epoch18, step2328]: loss 0.530856
[epoch18, step2329]: loss 0.549346
[epoch18, step2330]: loss 0.093206
[epoch18, step2331]: loss 0.541402
[epoch18, step2332]: loss 0.616816
[epoch18, step2333]: loss 0.664733
[epoch18, step2334]: loss 0.428437
[epoch18, step2335]: loss 0.412202
[epoch18, step2336]: loss 0.607933
[epoch18, step2337]: loss 0.450647
[epoch18, step2338]: loss 0.519729
[epoch18, step2339]: loss 0.396434
[epoch18, step2340]: loss 0.605604
[epoch18, step2341]: loss 0.517889
[epoch18, step2342]: loss 0.654548
[epoch18, step2343]: loss 0.443052
[epoch18, step2344]: loss 0.601588
[epoch18, step2345]: loss 0.323165
[epoch18, step2346]: loss 0.278131
[epoch18, step2347]: loss 0.582895
[epoch18, step2348]: loss 0.514614
[epoch18, step2349]: loss 0.244161
[epoch18, step2350]: loss 0.725674
[epoch18, step2351]: loss 0.363885
[epoch18, step2352]: loss 0.496180
[epoch18, step2353]: loss 0.497448
[epoch18, step2354]: loss 0.418934
[epoch18, step2355]: loss 0.504827
[epoch18, step2356]: loss 0.502772
[epoch18, step2357]: loss 0.290011
[epoch18, step2358]: loss 0.417335
[epoch18, step2359]: loss 0.384653
[epoch18, step2360]: loss 0.353964
[epoch18, step2361]: loss 0.557754
[epoch18, step2362]: loss 0.496723
[epoch18, step2363]: loss 0.468073
[epoch18, step2364]: loss 0.328178
[epoch18, step2365]: loss 0.504616
[epoch18, step2366]: loss 0.537794
[epoch18, step2367]: loss 0.644864
[epoch18, step2368]: loss 0.389564
[epoch18, step2369]: loss 0.463401
[epoch18, step2370]: loss 0.329086
[epoch18, step2371]: loss 0.617029
[epoch18, step2372]: loss 0.564762
[epoch18, step2373]: loss 0.247083
[epoch18, step2374]: loss 0.120308
[epoch18, step2375]: loss 0.681266
[epoch18, step2376]: loss 0.342796
[epoch18, step2377]: loss 0.446030
[epoch18, step2378]: loss 0.392354
[epoch18, step2379]: loss 0.427828
[epoch18, step2380]: loss 0.448661
[epoch18, step2381]: loss 0.439051
[epoch18, step2382]: loss 0.450592
[epoch18, step2383]: loss 0.377119
[epoch18, step2384]: loss 0.421675
[epoch18, step2385]: loss 0.532380
[epoch18, step2386]: loss 0.460712
[epoch18, step2387]: loss 0.309872
[epoch18, step2388]: loss 0.417693
[epoch18, step2389]: loss 0.148503
[epoch18, step2390]: loss 0.362825
[epoch18, step2391]: loss 0.336617
[epoch18, step2392]: loss 0.724199
[epoch18, step2393]: loss 0.552904
[epoch18, step2394]: loss 0.446396
[epoch18, step2395]: loss 0.327625
[epoch18, step2396]: loss 0.551704
[epoch18, step2397]: loss 0.318223
[epoch18, step2398]: loss 0.158248
[epoch18, step2399]: loss 0.276798
[epoch18, step2400]: loss 0.490932
[epoch18, step2401]: loss 0.512270
[epoch18, step2402]: loss 0.428289
[epoch18, step2403]: loss 0.300432
[epoch18, step2404]: loss 0.279606
[epoch18, step2405]: loss 0.492184
[epoch18, step2406]: loss 0.126388
[epoch18, step2407]: loss 0.564187
[epoch18, step2408]: loss 0.647375
[epoch18, step2409]: loss 0.325388
[epoch18, step2410]: loss 0.559265
[epoch18, step2411]: loss 0.366139
[epoch18, step2412]: loss 0.358081
[epoch18, step2413]: loss 0.365999
[epoch18, step2414]: loss 0.484749
[epoch18, step2415]: loss 0.623526
[epoch18, step2416]: loss 0.567058
[epoch18, step2417]: loss 0.262995
[epoch18, step2418]: loss 0.255314
[epoch18, step2419]: loss 0.429902
[epoch18, step2420]: loss 0.373861
[epoch18, step2421]: loss 0.444488
[epoch18, step2422]: loss 0.328988
[epoch18, step2423]: loss 0.656073
[epoch18, step2424]: loss 0.463428
[epoch18, step2425]: loss 0.440855
[epoch18, step2426]: loss 0.456986
[epoch18, step2427]: loss 0.664990
[epoch18, step2428]: loss 0.515500
[epoch18, step2429]: loss 0.285604
[epoch18, step2430]: loss 0.401850
[epoch18, step2431]: loss 0.393273
[epoch18, step2432]: loss 0.641678
[epoch18, step2433]: loss 0.519782
[epoch18, step2434]: loss 0.384109
[epoch18, step2435]: loss 0.359483
[epoch18, step2436]: loss 0.113007
[epoch18, step2437]: loss 0.436037
[epoch18, step2438]: loss 0.609670
[epoch18, step2439]: loss 0.125639
[epoch18, step2440]: loss 0.406743
[epoch18, step2441]: loss 0.450559
[epoch18, step2442]: loss 0.371275
[epoch18, step2443]: loss 0.517741
[epoch18, step2444]: loss 0.504498
[epoch18, step2445]: loss 0.756701
[epoch18, step2446]: loss 0.312323
[epoch18, step2447]: loss 0.578616
[epoch18, step2448]: loss 0.187503
[epoch18, step2449]: loss 0.581024
[epoch18, step2450]: loss 0.527914
[epoch18, step2451]: loss 0.538807
[epoch18, step2452]: loss 0.427423
[epoch18, step2453]: loss 0.519996
[epoch18, step2454]: loss 0.515143
[epoch18, step2455]: loss 0.350513
[epoch18, step2456]: loss 0.592325
[epoch18, step2457]: loss 0.674157
[epoch18, step2458]: loss 0.320265
[epoch18, step2459]: loss 0.566186
[epoch18, step2460]: loss 0.541803
[epoch18, step2461]: loss 0.295273
[epoch18, step2462]: loss 0.813617
[epoch18, step2463]: loss 0.452695
[epoch18, step2464]: loss 0.526487
[epoch18, step2465]: loss 0.457982
[epoch18, step2466]: loss 0.481795
[epoch18, step2467]: loss 0.421206
[epoch18, step2468]: loss 0.442609
[epoch18, step2469]: loss 0.504245
[epoch18, step2470]: loss 0.261176
[epoch18, step2471]: loss 0.279554
[epoch18, step2472]: loss 0.411213
[epoch18, step2473]: loss 0.613133
[epoch18, step2474]: loss 0.312951
[epoch18, step2475]: loss 0.690302
[epoch18, step2476]: loss 0.567881
[epoch18, step2477]: loss 0.322551
[epoch18, step2478]: loss 0.454835
[epoch18, step2479]: loss 0.321607
[epoch18, step2480]: loss 0.680188
[epoch18, step2481]: loss 0.424326
[epoch18, step2482]: loss 0.426359
[epoch18, step2483]: loss 0.280613
[epoch18, step2484]: loss 0.146208
[epoch18, step2485]: loss 0.557249
[epoch18, step2486]: loss 0.517510
[epoch18, step2487]: loss 0.264878
[epoch18, step2488]: loss 0.665702
[epoch18, step2489]: loss 0.382421
[epoch18, step2490]: loss 0.435438
[epoch18, step2491]: loss 0.456609
[epoch18, step2492]: loss 0.516682
[epoch18, step2493]: loss 0.371907
[epoch18, step2494]: loss 0.453221
[epoch18, step2495]: loss 0.647390
[epoch18, step2496]: loss 0.310071
[epoch18, step2497]: loss 0.523418
[epoch18, step2498]: loss 0.373347
[epoch18, step2499]: loss 0.371467
[epoch18, step2500]: loss 0.388800
[epoch18, step2501]: loss 0.403464
[epoch18, step2502]: loss 0.392501
[epoch18, step2503]: loss 0.468326
[epoch18, step2504]: loss 0.593157
[epoch18, step2505]: loss 0.576237
[epoch18, step2506]: loss 0.516332
[epoch18, step2507]: loss 0.315112
[epoch18, step2508]: loss 0.628406
[epoch18, step2509]: loss 0.557985
[epoch18, step2510]: loss 0.402839
[epoch18, step2511]: loss 0.441396
[epoch18, step2512]: loss 0.451827
[epoch18, step2513]: loss 0.306542
[epoch18, step2514]: loss 0.397759
[epoch18, step2515]: loss 0.461318
[epoch18, step2516]: loss 0.373596
[epoch18, step2517]: loss 0.564032
[epoch18, step2518]: loss 0.316340
[epoch18, step2519]: loss 0.482236
[epoch18, step2520]: loss 0.575185
[epoch18, step2521]: loss 0.501064
[epoch18, step2522]: loss 0.549577
[epoch18, step2523]: loss 0.368095
[epoch18, step2524]: loss 0.367580
[epoch18, step2525]: loss 0.619802
[epoch18, step2526]: loss 0.525622
[epoch18, step2527]: loss 0.443559
[epoch18, step2528]: loss 0.386972
[epoch18, step2529]: loss 0.535348
[epoch18, step2530]: loss 0.533687
[epoch18, step2531]: loss 0.381513
[epoch18, step2532]: loss 0.477639
[epoch18, step2533]: loss 0.447096
[epoch18, step2534]: loss 0.557622
[epoch18, step2535]: loss 0.474737
[epoch18, step2536]: loss 0.617009
[epoch18, step2537]: loss 0.522016
[epoch18, step2538]: loss 0.350878
[epoch18, step2539]: loss 0.430466
[epoch18, step2540]: loss 0.513187
[epoch18, step2541]: loss 0.346950
[epoch18, step2542]: loss 0.542797
[epoch18, step2543]: loss 0.405989
[epoch18, step2544]: loss 0.229822
[epoch18, step2545]: loss 0.475201
[epoch18, step2546]: loss 0.589057
[epoch18, step2547]: loss 0.448825
[epoch18, step2548]: loss 0.516885
[epoch18, step2549]: loss 0.230662
[epoch18, step2550]: loss 0.579006
[epoch18, step2551]: loss 0.462777
[epoch18, step2552]: loss 0.311436
[epoch18, step2553]: loss 0.463835
[epoch18, step2554]: loss 0.500797
[epoch18, step2555]: loss 0.241095
[epoch18, step2556]: loss 0.364313
[epoch18, step2557]: loss 0.609964
[epoch18, step2558]: loss 0.501564
[epoch18, step2559]: loss 0.338606
[epoch18, step2560]: loss 0.370415
[epoch18, step2561]: loss 0.478144
[epoch18, step2562]: loss 0.321480
[epoch18, step2563]: loss 0.280908
[epoch18, step2564]: loss 0.560516
[epoch18, step2565]: loss 0.520505
[epoch18, step2566]: loss 0.264438
[epoch18, step2567]: loss 0.512033
[epoch18, step2568]: loss 0.582498
[epoch18, step2569]: loss 0.578118
[epoch18, step2570]: loss 0.577770
[epoch18, step2571]: loss 0.380357
[epoch18, step2572]: loss 0.176692
[epoch18, step2573]: loss 0.489466
[epoch18, step2574]: loss 0.263380
[epoch18, step2575]: loss 0.211124
[epoch18, step2576]: loss 0.450076
[epoch18, step2577]: loss 0.240481
[epoch18, step2578]: loss 0.553732
[epoch18, step2579]: loss 0.331868
[epoch18, step2580]: loss 0.335972
[epoch18, step2581]: loss 0.403182
[epoch18, step2582]: loss 0.326671
[epoch18, step2583]: loss 0.467481
[epoch18, step2584]: loss 0.557271
[epoch18, step2585]: loss 0.669988
[epoch18, step2586]: loss 0.480742
[epoch18, step2587]: loss 0.265227
[epoch18, step2588]: loss 0.276514
[epoch18, step2589]: loss 0.359571
[epoch18, step2590]: loss 0.476530
[epoch18, step2591]: loss 0.511924
[epoch18, step2592]: loss 0.471856
[epoch18, step2593]: loss 0.557864
[epoch18, step2594]: loss 0.489404
[epoch18, step2595]: loss 0.131073
[epoch18, step2596]: loss 0.501172
[epoch18, step2597]: loss 0.524437
[epoch18, step2598]: loss 0.660136
[epoch18, step2599]: loss 0.387217
[epoch18, step2600]: loss 0.472136
[epoch18, step2601]: loss 0.238480
[epoch18, step2602]: loss 0.488426
[epoch18, step2603]: loss 0.455908
[epoch18, step2604]: loss 0.408990
[epoch18, step2605]: loss 0.536473
[epoch18, step2606]: loss 0.563400
[epoch18, step2607]: loss 0.617891
[epoch18, step2608]: loss 0.478446
[epoch18, step2609]: loss 0.389318
[epoch18, step2610]: loss 0.565628
[epoch18, step2611]: loss 0.233489
[epoch18, step2612]: loss 0.509281
[epoch18, step2613]: loss 0.443062
[epoch18, step2614]: loss 0.466800
[epoch18, step2615]: loss 0.123806
[epoch18, step2616]: loss 0.531696
[epoch18, step2617]: loss 0.584313
[epoch18, step2618]: loss 0.392619
[epoch18, step2619]: loss 0.427776
[epoch18, step2620]: loss 0.642128
[epoch18, step2621]: loss 0.554194
[epoch18, step2622]: loss 0.406168
[epoch18, step2623]: loss 0.480140
[epoch18, step2624]: loss 0.453134
[epoch18, step2625]: loss 0.684323
[epoch18, step2626]: loss 0.565310
[epoch18, step2627]: loss 0.464100
[epoch18, step2628]: loss 0.558292
[epoch18, step2629]: loss 0.599821
[epoch18, step2630]: loss 0.555476
[epoch18, step2631]: loss 0.543752
[epoch18, step2632]: loss 0.468534
[epoch18, step2633]: loss 0.671677
[epoch18, step2634]: loss 0.498235
[epoch18, step2635]: loss 0.496684
[epoch18, step2636]: loss 0.238402
[epoch18, step2637]: loss 0.426609
[epoch18, step2638]: loss 0.722949
[epoch18, step2639]: loss 0.642266
[epoch18, step2640]: loss 0.315199
[epoch18, step2641]: loss 0.450748
[epoch18, step2642]: loss 0.422092
[epoch18, step2643]: loss 0.360924
[epoch18, step2644]: loss 0.203773
[epoch18, step2645]: loss 0.439940
[epoch18, step2646]: loss 0.287221
[epoch18, step2647]: loss 0.340223
[epoch18, step2648]: loss 0.307258
[epoch18, step2649]: loss 0.534354
[epoch18, step2650]: loss 0.536989
[epoch18, step2651]: loss 0.507879
[epoch18, step2652]: loss 0.430716
[epoch18, step2653]: loss 0.589049
[epoch18, step2654]: loss 0.390972
[epoch18, step2655]: loss 0.138448
[epoch18, step2656]: loss 0.554532
[epoch18, step2657]: loss 0.426863
[epoch18, step2658]: loss 0.657291
[epoch18, step2659]: loss 0.327466
[epoch18, step2660]: loss 0.278949
[epoch18, step2661]: loss 0.509849
[epoch18, step2662]: loss 0.526450
[epoch18, step2663]: loss 0.321287
[epoch18, step2664]: loss 0.254950
[epoch18, step2665]: loss 0.544890
[epoch18, step2666]: loss 0.423881
[epoch18, step2667]: loss 0.481659
[epoch18, step2668]: loss 0.576632
[epoch18, step2669]: loss 0.604204
[epoch18, step2670]: loss 0.464529
[epoch18, step2671]: loss 0.415189
[epoch18, step2672]: loss 0.450514
[epoch18, step2673]: loss 0.534758
[epoch18, step2674]: loss 0.411090
[epoch18, step2675]: loss 0.511476
[epoch18, step2676]: loss 0.329182
[epoch18, step2677]: loss 0.378266
[epoch18, step2678]: loss 0.636821
[epoch18, step2679]: loss 0.466431
[epoch18, step2680]: loss 0.459116
[epoch18, step2681]: loss 0.463327
[epoch18, step2682]: loss 0.258689
[epoch18, step2683]: loss 0.633666
[epoch18, step2684]: loss 0.611357
[epoch18, step2685]: loss 0.233015
[epoch18, step2686]: loss 0.347809
[epoch18, step2687]: loss 0.568366
[epoch18, step2688]: loss 0.499933
[epoch18, step2689]: loss 0.383379
[epoch18, step2690]: loss 0.435387
[epoch18, step2691]: loss 0.633034
[epoch18, step2692]: loss 0.511336
[epoch18, step2693]: loss 0.569376
[epoch18, step2694]: loss 0.655085
[epoch18, step2695]: loss 0.437381
[epoch18, step2696]: loss 0.273880
[epoch18, step2697]: loss 0.613188
[epoch18, step2698]: loss 0.566537
[epoch18, step2699]: loss 0.265911
[epoch18, step2700]: loss 0.448569
[epoch18, step2701]: loss 0.495923
[epoch18, step2702]: loss 0.628243
[epoch18, step2703]: loss 0.553121
[epoch18, step2704]: loss 0.586780
[epoch18, step2705]: loss 0.435260
[epoch18, step2706]: loss 0.458293
[epoch18, step2707]: loss 0.595080
[epoch18, step2708]: loss 0.514224
[epoch18, step2709]: loss 0.436016
[epoch18, step2710]: loss 0.444922
[epoch18, step2711]: loss 0.711551
[epoch18, step2712]: loss 0.569429
[epoch18, step2713]: loss 0.462630
[epoch18, step2714]: loss 0.799388
[epoch18, step2715]: loss 0.650356
[epoch18, step2716]: loss 0.448371
[epoch18, step2717]: loss 0.512245
[epoch18, step2718]: loss 0.556507
[epoch18, step2719]: loss 0.336982
[epoch18, step2720]: loss 0.641112
[epoch18, step2721]: loss 0.599835
[epoch18, step2722]: loss 0.453976
[epoch18, step2723]: loss 0.463302
[epoch18, step2724]: loss 0.516023
[epoch18, step2725]: loss 0.566794
[epoch18, step2726]: loss 0.571365
[epoch18, step2727]: loss 0.471893
[epoch18, step2728]: loss 0.408031
[epoch18, step2729]: loss 0.439883
[epoch18, step2730]: loss 0.531840
[epoch18, step2731]: loss 0.575863
[epoch18, step2732]: loss 0.529781
[epoch18, step2733]: loss 0.233021
[epoch18, step2734]: loss 0.680687
[epoch18, step2735]: loss 0.491803
[epoch18, step2736]: loss 0.555920
[epoch18, step2737]: loss 0.363725
[epoch18, step2738]: loss 0.442398
[epoch18, step2739]: loss 0.405316
[epoch18, step2740]: loss 0.450322
[epoch18, step2741]: loss 0.524206
[epoch18, step2742]: loss 0.497957
[epoch18, step2743]: loss 0.320673
[epoch18, step2744]: loss 0.562776
[epoch18, step2745]: loss 0.480904
[epoch18, step2746]: loss 0.642245
[epoch18, step2747]: loss 0.502362
[epoch18, step2748]: loss 0.331276
[epoch18, step2749]: loss 0.435605
[epoch18, step2750]: loss 0.454696
[epoch18, step2751]: loss 0.647297
[epoch18, step2752]: loss 0.559956
[epoch18, step2753]: loss 0.517181
[epoch18, step2754]: loss 0.414922
[epoch18, step2755]: loss 0.404570
[epoch18, step2756]: loss 0.369209
[epoch18, step2757]: loss 0.386005
[epoch18, step2758]: loss 0.278087
[epoch18, step2759]: loss 0.238063
[epoch18, step2760]: loss 0.593198
[epoch18, step2761]: loss 0.718784
[epoch18, step2762]: loss 0.639015
[epoch18, step2763]: loss 0.644847
[epoch18, step2764]: loss 0.529475
[epoch18, step2765]: loss 0.338525
[epoch18, step2766]: loss 0.503910
[epoch18, step2767]: loss 0.325964
[epoch18, step2768]: loss 0.742050
[epoch18, step2769]: loss 0.588034
[epoch18, step2770]: loss 0.646348
[epoch18, step2771]: loss 0.603788
[epoch18, step2772]: loss 0.423661
[epoch18, step2773]: loss 0.604513
[epoch18, step2774]: loss 0.348360
[epoch18, step2775]: loss 0.502696
[epoch18, step2776]: loss 0.660099
[epoch18, step2777]: loss 0.415032
[epoch18, step2778]: loss 0.329530
[epoch18, step2779]: loss 0.413138
[epoch18, step2780]: loss 0.226773
[epoch18, step2781]: loss 0.566459
[epoch18, step2782]: loss 0.542151
[epoch18, step2783]: loss 0.450469
[epoch18, step2784]: loss 0.236272
[epoch18, step2785]: loss 0.426156
[epoch18, step2786]: loss 0.311107
[epoch18, step2787]: loss 0.449146
[epoch18, step2788]: loss 0.438132
[epoch18, step2789]: loss 0.428917
[epoch18, step2790]: loss 0.489662
[epoch18, step2791]: loss 0.352427
[epoch18, step2792]: loss 0.491634
[epoch18, step2793]: loss 0.388729
[epoch18, step2794]: loss 0.617347
[epoch18, step2795]: loss 0.618562
[epoch18, step2796]: loss 0.712837
[epoch18, step2797]: loss 0.348474
[epoch18, step2798]: loss 0.315238
[epoch18, step2799]: loss 0.478768
[epoch18, step2800]: loss 0.542311
[epoch18, step2801]: loss 0.443135
[epoch18, step2802]: loss 0.465960
[epoch18, step2803]: loss 0.467704
[epoch18, step2804]: loss 0.662443
[epoch18, step2805]: loss 0.464737
[epoch18, step2806]: loss 0.558102
[epoch18, step2807]: loss 0.515876
[epoch18, step2808]: loss 0.483390
[epoch18, step2809]: loss 0.569685
[epoch18, step2810]: loss 0.469923
[epoch18, step2811]: loss 0.404154
[epoch18, step2812]: loss 0.236043
[epoch18, step2813]: loss 0.572840
[epoch18, step2814]: loss 0.437801
[epoch18, step2815]: loss 0.323720
[epoch18, step2816]: loss 0.457957
[epoch18, step2817]: loss 0.398392
[epoch18, step2818]: loss 0.324165
[epoch18, step2819]: loss 0.438931
[epoch18, step2820]: loss 0.393847
[epoch18, step2821]: loss 0.299820
[epoch18, step2822]: loss 0.233109
[epoch18, step2823]: loss 0.440001
[epoch18, step2824]: loss 0.646668
[epoch18, step2825]: loss 0.420178
[epoch18, step2826]: loss 0.351045
[epoch18, step2827]: loss 0.476364
[epoch18, step2828]: loss 0.372282
[epoch18, step2829]: loss 0.374644
[epoch18, step2830]: loss 0.588720
[epoch18, step2831]: loss 0.549915
[epoch18, step2832]: loss 0.527915
[epoch18, step2833]: loss 0.546910
[epoch18, step2834]: loss 0.311631
[epoch18, step2835]: loss 0.510769
[epoch18, step2836]: loss 0.501855
[epoch18, step2837]: loss 0.505748
[epoch18, step2838]: loss 0.238165
[epoch18, step2839]: loss 0.532570
[epoch18, step2840]: loss 0.540886
[epoch18, step2841]: loss 0.487761
[epoch18, step2842]: loss 0.567170
[epoch18, step2843]: loss 0.559466
[epoch18, step2844]: loss 0.363789
[epoch18, step2845]: loss 0.446674
[epoch18, step2846]: loss 0.582586
[epoch18, step2847]: loss 0.669234
[epoch18, step2848]: loss 0.390525
[epoch18, step2849]: loss 0.663210
[epoch18, step2850]: loss 0.569176
[epoch18, step2851]: loss 0.492560
[epoch18, step2852]: loss 0.621197
[epoch18, step2853]: loss 0.237466
[epoch18, step2854]: loss 0.561453
[epoch18, step2855]: loss 0.438780
[epoch18, step2856]: loss 0.324174
[epoch18, step2857]: loss 0.626302
[epoch18, step2858]: loss 0.392289
[epoch18, step2859]: loss 0.371810
[epoch18, step2860]: loss 0.589643
[epoch18, step2861]: loss 0.575180
[epoch18, step2862]: loss 0.324239
[epoch18, step2863]: loss 0.438566
[epoch18, step2864]: loss 0.669128
[epoch18, step2865]: loss 0.569933
[epoch18, step2866]: loss 0.346580
[epoch18, step2867]: loss 0.613334
[epoch18, step2868]: loss 0.335354
[epoch18, step2869]: loss 0.515947
[epoch18, step2870]: loss 0.680034
[epoch18, step2871]: loss 0.500887
[epoch18, step2872]: loss 0.445109
[epoch18, step2873]: loss 0.624031
[epoch18, step2874]: loss 0.355177
[epoch18, step2875]: loss 0.593365
[epoch18, step2876]: loss 0.261939
[epoch18, step2877]: loss 0.178752
[epoch18, step2878]: loss 0.339892
[epoch18, step2879]: loss 0.620688
[epoch18, step2880]: loss 0.218712
[epoch18, step2881]: loss 0.440997
[epoch18, step2882]: loss 0.192901
[epoch18, step2883]: loss 0.364193
[epoch18, step2884]: loss 0.458632
[epoch18, step2885]: loss 0.346599
[epoch18, step2886]: loss 0.430380
[epoch18, step2887]: loss 0.376595
[epoch18, step2888]: loss 0.246029
[epoch18, step2889]: loss 0.496666
[epoch18, step2890]: loss 0.503890
[epoch18, step2891]: loss 0.405205
[epoch18, step2892]: loss 0.430170
[epoch18, step2893]: loss 0.459723
[epoch18, step2894]: loss 0.434290
[epoch18, step2895]: loss 0.543495
[epoch18, step2896]: loss 0.616129
[epoch18, step2897]: loss 0.230835
[epoch18, step2898]: loss 0.688354
[epoch18, step2899]: loss 0.411055
[epoch18, step2900]: loss 0.218358
[epoch18, step2901]: loss 0.406124
[epoch18, step2902]: loss 0.234639
[epoch18, step2903]: loss 0.445842
[epoch18, step2904]: loss 0.476166
[epoch18, step2905]: loss 0.507105
[epoch18, step2906]: loss 0.254250
[epoch18, step2907]: loss 0.629101
[epoch18, step2908]: loss 0.540829
[epoch18, step2909]: loss 0.488446
[epoch18, step2910]: loss 0.318739
[epoch18, step2911]: loss 0.504665
[epoch18, step2912]: loss 0.727388
[epoch18, step2913]: loss 0.428033
[epoch18, step2914]: loss 0.341078
[epoch18, step2915]: loss 0.466207
[epoch18, step2916]: loss 0.493075
[epoch18, step2917]: loss 0.583861
[epoch18, step2918]: loss 0.330839
[epoch18, step2919]: loss 0.433167
[epoch18, step2920]: loss 0.436625
[epoch18, step2921]: loss 0.652001
[epoch18, step2922]: loss 0.472760
[epoch18, step2923]: loss 0.522431
[epoch18, step2924]: loss 0.487963
[epoch18, step2925]: loss 0.450132
[epoch18, step2926]: loss 0.488506
[epoch18, step2927]: loss 0.446587
[epoch18, step2928]: loss 0.212414
[epoch18, step2929]: loss 0.581076
[epoch18, step2930]: loss 0.716437
[epoch18, step2931]: loss 0.231576
[epoch18, step2932]: loss 0.256811
[epoch18, step2933]: loss 0.480104
[epoch18, step2934]: loss 0.544779
[epoch18, step2935]: loss 0.441363
[epoch18, step2936]: loss 0.527958
[epoch18, step2937]: loss 0.550879
[epoch18, step2938]: loss 0.393737
[epoch18, step2939]: loss 0.472880
[epoch18, step2940]: loss 0.548338
[epoch18, step2941]: loss 0.405582
[epoch18, step2942]: loss 0.341702
[epoch18, step2943]: loss 0.500589
[epoch18, step2944]: loss 0.376428
[epoch18, step2945]: loss 0.661033
[epoch18, step2946]: loss 0.491478
[epoch18, step2947]: loss 0.605949
[epoch18, step2948]: loss 0.534183
[epoch18, step2949]: loss 0.153805
[epoch18, step2950]: loss 0.337692
[epoch18, step2951]: loss 0.504871
[epoch18, step2952]: loss 0.520585
[epoch18, step2953]: loss 0.638275
[epoch18, step2954]: loss 0.378564
[epoch18, step2955]: loss 0.735620
[epoch18, step2956]: loss 0.467009
[epoch18, step2957]: loss 0.385131
[epoch18, step2958]: loss 0.626068
[epoch18, step2959]: loss 0.554712
[epoch18, step2960]: loss 0.345559
[epoch18, step2961]: loss 0.521604
[epoch18, step2962]: loss 0.439112
[epoch18, step2963]: loss 0.326046
[epoch18, step2964]: loss 0.368809
[epoch18, step2965]: loss 0.228549
[epoch18, step2966]: loss 0.504960
[epoch18, step2967]: loss 0.234240
[epoch18, step2968]: loss 0.251548
[epoch18, step2969]: loss 0.455860
[epoch18, step2970]: loss 0.635846
[epoch18, step2971]: loss 0.574337
[epoch18, step2972]: loss 0.279665
[epoch18, step2973]: loss 0.300340
[epoch18, step2974]: loss 0.508499
[epoch18, step2975]: loss 0.364413
[epoch18, step2976]: loss 0.547020
[epoch18, step2977]: loss 0.314552
[epoch18, step2978]: loss 0.384421
[epoch18, step2979]: loss 0.489661
[epoch18, step2980]: loss 0.506289
[epoch18, step2981]: loss 0.391788
[epoch18, step2982]: loss 0.260067
[epoch18, step2983]: loss 0.649882
[epoch18, step2984]: loss 0.454907
[epoch18, step2985]: loss 0.610793
[epoch18, step2986]: loss 0.201130
[epoch18, step2987]: loss 0.656875
[epoch18, step2988]: loss 0.398005
[epoch18, step2989]: loss 0.663968
[epoch18, step2990]: loss 0.566396
[epoch18, step2991]: loss 0.478036
[epoch18, step2992]: loss 0.414427
[epoch18, step2993]: loss 0.544416
[epoch18, step2994]: loss 0.463879
[epoch18, step2995]: loss 0.514851
[epoch18, step2996]: loss 0.621431
[epoch18, step2997]: loss 0.350359
[epoch18, step2998]: loss 0.455882
[epoch18, step2999]: loss 0.522393
[epoch18, step3000]: loss 0.637154
[epoch18, step3001]: loss 0.425394
[epoch18, step3002]: loss 0.612354
[epoch18, step3003]: loss 0.288757
[epoch18, step3004]: loss 0.545575
[epoch18, step3005]: loss 0.214903
[epoch18, step3006]: loss 0.651508
[epoch18, step3007]: loss 0.742412
[epoch18, step3008]: loss 0.503089
[epoch18, step3009]: loss 0.437194
[epoch18, step3010]: loss 0.342880
[epoch18, step3011]: loss 0.461521
[epoch18, step3012]: loss 0.536154
[epoch18, step3013]: loss 0.307908
[epoch18, step3014]: loss 0.412652
[epoch18, step3015]: loss 0.493083
[epoch18, step3016]: loss 0.329599
[epoch18, step3017]: loss 0.561289
[epoch18, step3018]: loss 0.442309
[epoch18, step3019]: loss 0.401268
[epoch18, step3020]: loss 0.556993
[epoch18, step3021]: loss 0.554949
[epoch18, step3022]: loss 0.282683
[epoch18, step3023]: loss 0.374150
[epoch18, step3024]: loss 0.430177
[epoch18, step3025]: loss 0.654104
[epoch18, step3026]: loss 0.324757
[epoch18, step3027]: loss 0.514218
[epoch18, step3028]: loss 0.552461
[epoch18, step3029]: loss 0.496375
[epoch18, step3030]: loss 0.697115
[epoch18, step3031]: loss 0.515563
[epoch18, step3032]: loss 0.532252
[epoch18, step3033]: loss 0.344786
[epoch18, step3034]: loss 0.293482
[epoch18, step3035]: loss 0.504295
[epoch18, step3036]: loss 0.252625
[epoch18, step3037]: loss 0.423852
[epoch18, step3038]: loss 0.315957
[epoch18, step3039]: loss 0.521661
[epoch18, step3040]: loss 0.598154
[epoch18, step3041]: loss 0.384301
[epoch18, step3042]: loss 0.649879
[epoch18, step3043]: loss 0.542299
[epoch18, step3044]: loss 0.464342
[epoch18, step3045]: loss 0.407037
[epoch18, step3046]: loss 0.374698
[epoch18, step3047]: loss 0.462707
[epoch18, step3048]: loss 0.384349
[epoch18, step3049]: loss 0.345765
[epoch18, step3050]: loss 0.251440
[epoch18, step3051]: loss 0.282691
[epoch18, step3052]: loss 0.506003
[epoch18, step3053]: loss 0.274668
[epoch18, step3054]: loss 0.521642
[epoch18, step3055]: loss 0.551627
[epoch18, step3056]: loss 0.408009
[epoch18, step3057]: loss 0.470317
[epoch18, step3058]: loss 0.388018
[epoch18, step3059]: loss 0.263244
[epoch18, step3060]: loss 0.588817
[epoch18, step3061]: loss 0.510303
[epoch18, step3062]: loss 0.479621
[epoch18, step3063]: loss 0.287352
[epoch18, step3064]: loss 0.303197
[epoch18, step3065]: loss 0.464697
[epoch18, step3066]: loss 0.489501
[epoch18, step3067]: loss 0.558966
[epoch18, step3068]: loss 0.462764
[epoch18, step3069]: loss 0.449783
[epoch18, step3070]: loss 0.638131
[epoch18, step3071]: loss 0.427928
[epoch18, step3072]: loss 0.362924
[epoch18, step3073]: loss 0.513088
[epoch18, step3074]: loss 0.448568
[epoch18, step3075]: loss 0.538392
[epoch18, step3076]: loss 0.470300

[epoch18]: avg loss 0.470300

[epoch19, step1]: loss 0.729193
[epoch19, step2]: loss 0.605915
[epoch19, step3]: loss 0.548938
[epoch19, step4]: loss 0.707213
[epoch19, step5]: loss 0.582495
[epoch19, step6]: loss 0.442403
[epoch19, step7]: loss 0.144224
[epoch19, step8]: loss 0.510448
[epoch19, step9]: loss 0.475474
[epoch19, step10]: loss 0.399019
[epoch19, step11]: loss 0.271533
[epoch19, step12]: loss 0.424326
[epoch19, step13]: loss 0.487615
[epoch19, step14]: loss 0.537145
[epoch19, step15]: loss 0.620071
[epoch19, step16]: loss 0.642932
[epoch19, step17]: loss 0.442911
[epoch19, step18]: loss 0.368307
[epoch19, step19]: loss 0.455393
[epoch19, step20]: loss 0.425954
[epoch19, step21]: loss 0.634057
[epoch19, step22]: loss 0.580990
[epoch19, step23]: loss 0.317919
[epoch19, step24]: loss 0.420918
[epoch19, step25]: loss 0.507150
[epoch19, step26]: loss 0.466597
[epoch19, step27]: loss 0.439083
[epoch19, step28]: loss 0.614422
[epoch19, step29]: loss 0.152794
[epoch19, step30]: loss 0.524616
[epoch19, step31]: loss 0.454922
[epoch19, step32]: loss 0.329068
[epoch19, step33]: loss 0.553054
[epoch19, step34]: loss 0.509800
[epoch19, step35]: loss 0.506580
[epoch19, step36]: loss 0.309597
[epoch19, step37]: loss 0.428556
[epoch19, step38]: loss 0.331526
[epoch19, step39]: loss 0.646577
[epoch19, step40]: loss 0.132692
[epoch19, step41]: loss 0.233887
[epoch19, step42]: loss 0.671246
[epoch19, step43]: loss 0.406651
[epoch19, step44]: loss 0.531829
[epoch19, step45]: loss 0.500288
[epoch19, step46]: loss 0.510542
[epoch19, step47]: loss 0.492768
[epoch19, step48]: loss 0.412587
[epoch19, step49]: loss 0.504777
[epoch19, step50]: loss 0.337684
[epoch19, step51]: loss 0.499810
[epoch19, step52]: loss 0.221455
[epoch19, step53]: loss 0.365947
[epoch19, step54]: loss 0.511769
[epoch19, step55]: loss 0.377466
[epoch19, step56]: loss 0.419692
[epoch19, step57]: loss 0.402117
[epoch19, step58]: loss 0.388246
[epoch19, step59]: loss 0.358739
[epoch19, step60]: loss 0.628141
[epoch19, step61]: loss 0.531689
[epoch19, step62]: loss 0.683882
[epoch19, step63]: loss 0.370372
[epoch19, step64]: loss 0.419675
[epoch19, step65]: loss 0.369544
[epoch19, step66]: loss 0.626950
[epoch19, step67]: loss 0.697037
[epoch19, step68]: loss 0.552005
[epoch19, step69]: loss 0.524727
[epoch19, step70]: loss 0.550514
[epoch19, step71]: loss 0.344336
[epoch19, step72]: loss 0.453997
[epoch19, step73]: loss 0.336667
[epoch19, step74]: loss 0.683878
[epoch19, step75]: loss 0.360686
[epoch19, step76]: loss 0.377803
[epoch19, step77]: loss 0.744722
[epoch19, step78]: loss 0.531308
[epoch19, step79]: loss 0.411164
[epoch19, step80]: loss 0.363842
[epoch19, step81]: loss 0.577239
[epoch19, step82]: loss 0.563681
[epoch19, step83]: loss 0.464886
[epoch19, step84]: loss 0.216377
[epoch19, step85]: loss 0.538730
[epoch19, step86]: loss 0.409919
[epoch19, step87]: loss 0.524162
[epoch19, step88]: loss 0.447179
[epoch19, step89]: loss 0.388327
[epoch19, step90]: loss 0.579711
[epoch19, step91]: loss 0.648888
[epoch19, step92]: loss 0.462114
[epoch19, step93]: loss 0.290666
[epoch19, step94]: loss 0.282343
[epoch19, step95]: loss 0.499146
[epoch19, step96]: loss 0.553049
[epoch19, step97]: loss 0.525244
[epoch19, step98]: loss 0.474979
[epoch19, step99]: loss 0.574679
[epoch19, step100]: loss 0.536024
[epoch19, step101]: loss 0.344013
[epoch19, step102]: loss 0.545785
[epoch19, step103]: loss 0.289909
[epoch19, step104]: loss 0.382217
[epoch19, step105]: loss 0.592826
[epoch19, step106]: loss 0.429757
[epoch19, step107]: loss 0.620829
[epoch19, step108]: loss 0.505270
[epoch19, step109]: loss 0.609127
[epoch19, step110]: loss 0.282532
[epoch19, step111]: loss 0.543984
[epoch19, step112]: loss 0.473458
[epoch19, step113]: loss 0.407191
[epoch19, step114]: loss 0.531477
[epoch19, step115]: loss 0.465950
[epoch19, step116]: loss 0.544189
[epoch19, step117]: loss 0.553629
[epoch19, step118]: loss 0.239116
[epoch19, step119]: loss 0.686409
[epoch19, step120]: loss 0.420055
[epoch19, step121]: loss 0.382115
[epoch19, step122]: loss 0.542746
[epoch19, step123]: loss 0.460302
[epoch19, step124]: loss 0.436103
[epoch19, step125]: loss 0.511149
[epoch19, step126]: loss 0.480453
[epoch19, step127]: loss 0.292166
[epoch19, step128]: loss 0.599473
[epoch19, step129]: loss 0.457557
[epoch19, step130]: loss 0.353540
[epoch19, step131]: loss 0.469988
[epoch19, step132]: loss 0.556601
[epoch19, step133]: loss 0.399377
[epoch19, step134]: loss 0.603193
[epoch19, step135]: loss 0.380518
[epoch19, step136]: loss 0.240361
[epoch19, step137]: loss 0.452431
[epoch19, step138]: loss 0.293460
[epoch19, step139]: loss 0.605669
[epoch19, step140]: loss 0.382722
[epoch19, step141]: loss 0.304508
[epoch19, step142]: loss 0.354362
[epoch19, step143]: loss 0.656040
[epoch19, step144]: loss 0.594547
[epoch19, step145]: loss 0.423225
[epoch19, step146]: loss 0.432600
[epoch19, step147]: loss 0.544152
[epoch19, step148]: loss 0.490330
[epoch19, step149]: loss 0.229424
[epoch19, step150]: loss 0.531869
[epoch19, step151]: loss 0.341004
[epoch19, step152]: loss 0.570598
[epoch19, step153]: loss 0.507654
[epoch19, step154]: loss 0.344190
[epoch19, step155]: loss 0.236289
[epoch19, step156]: loss 0.540530
[epoch19, step157]: loss 0.345086
[epoch19, step158]: loss 0.438125
[epoch19, step159]: loss 0.386444
[epoch19, step160]: loss 0.434712
[epoch19, step161]: loss 0.547200
[epoch19, step162]: loss 0.605548
[epoch19, step163]: loss 0.537966
[epoch19, step164]: loss 0.572692
[epoch19, step165]: loss 0.752986
[epoch19, step166]: loss 0.389025
[epoch19, step167]: loss 0.428431
[epoch19, step168]: loss 0.507987
[epoch19, step169]: loss 0.514040
[epoch19, step170]: loss 0.494279
[epoch19, step171]: loss 0.461090
[epoch19, step172]: loss 0.583322
[epoch19, step173]: loss 0.527328
[epoch19, step174]: loss 0.553805
[epoch19, step175]: loss 0.460914
[epoch19, step176]: loss 0.407377
[epoch19, step177]: loss 0.462155
[epoch19, step178]: loss 0.296855
[epoch19, step179]: loss 0.579624
[epoch19, step180]: loss 0.577263
[epoch19, step181]: loss 0.521528
[epoch19, step182]: loss 0.336652
[epoch19, step183]: loss 0.553664
[epoch19, step184]: loss 0.373214
[epoch19, step185]: loss 0.555670
[epoch19, step186]: loss 0.320278
[epoch19, step187]: loss 0.198222
[epoch19, step188]: loss 0.477739
[epoch19, step189]: loss 0.512787
[epoch19, step190]: loss 0.340279
[epoch19, step191]: loss 0.327335
[epoch19, step192]: loss 0.473435
[epoch19, step193]: loss 0.540119
[epoch19, step194]: loss 0.248497
[epoch19, step195]: loss 0.417769
[epoch19, step196]: loss 0.374308
[epoch19, step197]: loss 0.444584
[epoch19, step198]: loss 0.266427
[epoch19, step199]: loss 0.493842
[epoch19, step200]: loss 0.259904
[epoch19, step201]: loss 0.499454
[epoch19, step202]: loss 0.366006
[epoch19, step203]: loss 0.522060
[epoch19, step204]: loss 0.475558
[epoch19, step205]: loss 0.371091
[epoch19, step206]: loss 0.546518
[epoch19, step207]: loss 0.523061
[epoch19, step208]: loss 0.550606
[epoch19, step209]: loss 0.341227
[epoch19, step210]: loss 0.368280
[epoch19, step211]: loss 0.623000
[epoch19, step212]: loss 0.357224
[epoch19, step213]: loss 0.347774
[epoch19, step214]: loss 0.420286
[epoch19, step215]: loss 0.510632
[epoch19, step216]: loss 0.343524
[epoch19, step217]: loss 0.618038
[epoch19, step218]: loss 0.346652
[epoch19, step219]: loss 0.676929
[epoch19, step220]: loss 0.651703
[epoch19, step221]: loss 0.465367
[epoch19, step222]: loss 0.368770
[epoch19, step223]: loss 0.249469
[epoch19, step224]: loss 0.435194
[epoch19, step225]: loss 0.509791
[epoch19, step226]: loss 0.717782
[epoch19, step227]: loss 0.546868
[epoch19, step228]: loss 0.354482
[epoch19, step229]: loss 0.402762
[epoch19, step230]: loss 0.256189
[epoch19, step231]: loss 0.534942
[epoch19, step232]: loss 0.528490
[epoch19, step233]: loss 0.470246
[epoch19, step234]: loss 0.505728
[epoch19, step235]: loss 0.470519
[epoch19, step236]: loss 0.645199
[epoch19, step237]: loss 0.511328
[epoch19, step238]: loss 0.492962
[epoch19, step239]: loss 0.581036
[epoch19, step240]: loss 0.572969
[epoch19, step241]: loss 0.419275
[epoch19, step242]: loss 0.408469
[epoch19, step243]: loss 0.378501
[epoch19, step244]: loss 0.541447
[epoch19, step245]: loss 0.652168
[epoch19, step246]: loss 0.738507
[epoch19, step247]: loss 0.544632
[epoch19, step248]: loss 0.283721
[epoch19, step249]: loss 0.536116
[epoch19, step250]: loss 0.317844
[epoch19, step251]: loss 0.626878
[epoch19, step252]: loss 0.161735
[epoch19, step253]: loss 0.631027
[epoch19, step254]: loss 0.433879
[epoch19, step255]: loss 0.623777
[epoch19, step256]: loss 0.594094
[epoch19, step257]: loss 0.409939
[epoch19, step258]: loss 0.455721
[epoch19, step259]: loss 0.566763
[epoch19, step260]: loss 0.481193
[epoch19, step261]: loss 0.526366
[epoch19, step262]: loss 0.452134
[epoch19, step263]: loss 0.512703
[epoch19, step264]: loss 0.510831
[epoch19, step265]: loss 0.304849
[epoch19, step266]: loss 0.375450
[epoch19, step267]: loss 0.554114
[epoch19, step268]: loss 0.569359
[epoch19, step269]: loss 0.454615
[epoch19, step270]: loss 0.476374
[epoch19, step271]: loss 0.606543
[epoch19, step272]: loss 0.594857
[epoch19, step273]: loss 0.469755
[epoch19, step274]: loss 0.486129
[epoch19, step275]: loss 0.525720
[epoch19, step276]: loss 0.475707
[epoch19, step277]: loss 0.477728
[epoch19, step278]: loss 0.530724
[epoch19, step279]: loss 0.388763
[epoch19, step280]: loss 0.384705
[epoch19, step281]: loss 0.394989
[epoch19, step282]: loss 0.548842
[epoch19, step283]: loss 0.423883
[epoch19, step284]: loss 0.448756
[epoch19, step285]: loss 0.133496
[epoch19, step286]: loss 0.642567
[epoch19, step287]: loss 0.317337
[epoch19, step288]: loss 0.521038
[epoch19, step289]: loss 0.559782
[epoch19, step290]: loss 0.471071
[epoch19, step291]: loss 0.413077
[epoch19, step292]: loss 0.516280
[epoch19, step293]: loss 0.407076
[epoch19, step294]: loss 0.421540
[epoch19, step295]: loss 0.617499
[epoch19, step296]: loss 0.438111
[epoch19, step297]: loss 0.395450
[epoch19, step298]: loss 0.634600
[epoch19, step299]: loss 0.526039
[epoch19, step300]: loss 0.484221
[epoch19, step301]: loss 0.471887
[epoch19, step302]: loss 0.448057
[epoch19, step303]: loss 0.316572
[epoch19, step304]: loss 0.557080
[epoch19, step305]: loss 0.510842
[epoch19, step306]: loss 0.544523
[epoch19, step307]: loss 0.407085
[epoch19, step308]: loss 0.322484
[epoch19, step309]: loss 0.445861
[epoch19, step310]: loss 0.284262
[epoch19, step311]: loss 0.418625
[epoch19, step312]: loss 0.436416
[epoch19, step313]: loss 0.410954
[epoch19, step314]: loss 0.462384
[epoch19, step315]: loss 0.549806
[epoch19, step316]: loss 0.571780
[epoch19, step317]: loss 0.388354
[epoch19, step318]: loss 0.473461
[epoch19, step319]: loss 0.486616
[epoch19, step320]: loss 0.609240
[epoch19, step321]: loss 0.363905
[epoch19, step322]: loss 0.466231
[epoch19, step323]: loss 0.297095
[epoch19, step324]: loss 0.554385
[epoch19, step325]: loss 0.582014
[epoch19, step326]: loss 0.649313
[epoch19, step327]: loss 0.490368
[epoch19, step328]: loss 0.297771
[epoch19, step329]: loss 0.602007
[epoch19, step330]: loss 0.610728
[epoch19, step331]: loss 0.390977
[epoch19, step332]: loss 0.253037
[epoch19, step333]: loss 0.422375
[epoch19, step334]: loss 0.435822
[epoch19, step335]: loss 0.622448
[epoch19, step336]: loss 0.440402
[epoch19, step337]: loss 0.261768
[epoch19, step338]: loss 0.411982
[epoch19, step339]: loss 0.489730
[epoch19, step340]: loss 0.466764
[epoch19, step341]: loss 0.571932
[epoch19, step342]: loss 0.677483
[epoch19, step343]: loss 0.344105
[epoch19, step344]: loss 0.533167
[epoch19, step345]: loss 0.499869
[epoch19, step346]: loss 0.253359
[epoch19, step347]: loss 0.279514
[epoch19, step348]: loss 0.479028
[epoch19, step349]: loss 0.488962
[epoch19, step350]: loss 0.553418
[epoch19, step351]: loss 0.551808
[epoch19, step352]: loss 0.547409
[epoch19, step353]: loss 0.255007
[epoch19, step354]: loss 0.460015
[epoch19, step355]: loss 0.516008
[epoch19, step356]: loss 0.468074
[epoch19, step357]: loss 0.341457
[epoch19, step358]: loss 0.553528
[epoch19, step359]: loss 0.621389
[epoch19, step360]: loss 0.579854
[epoch19, step361]: loss 0.234632
[epoch19, step362]: loss 0.475678
[epoch19, step363]: loss 0.391220
[epoch19, step364]: loss 0.462142
[epoch19, step365]: loss 0.381842
[epoch19, step366]: loss 0.426312
[epoch19, step367]: loss 0.326954
[epoch19, step368]: loss 0.266625
[epoch19, step369]: loss 0.582676
[epoch19, step370]: loss 0.442823
[epoch19, step371]: loss 0.557648
[epoch19, step372]: loss 0.527906
[epoch19, step373]: loss 0.426817
[epoch19, step374]: loss 0.349801
[epoch19, step375]: loss 0.356895
[epoch19, step376]: loss 0.417745
[epoch19, step377]: loss 0.312789
[epoch19, step378]: loss 0.319439
[epoch19, step379]: loss 0.539509
[epoch19, step380]: loss 0.355288
[epoch19, step381]: loss 0.506925
[epoch19, step382]: loss 0.731302
[epoch19, step383]: loss 0.303795
[epoch19, step384]: loss 0.337903
[epoch19, step385]: loss 0.496963
[epoch19, step386]: loss 0.558602
[epoch19, step387]: loss 0.442610
[epoch19, step388]: loss 0.284918
[epoch19, step389]: loss 0.416257
[epoch19, step390]: loss 0.533615
[epoch19, step391]: loss 0.641413
[epoch19, step392]: loss 0.701435
[epoch19, step393]: loss 0.523957
[epoch19, step394]: loss 0.444419
[epoch19, step395]: loss 0.460832
[epoch19, step396]: loss 0.180212
[epoch19, step397]: loss 0.586669
[epoch19, step398]: loss 0.520027
[epoch19, step399]: loss 0.396612
[epoch19, step400]: loss 0.356645
[epoch19, step401]: loss 0.410997
[epoch19, step402]: loss 0.550145
[epoch19, step403]: loss 0.302973
[epoch19, step404]: loss 0.461960
[epoch19, step405]: loss 0.107069
[epoch19, step406]: loss 0.212813
[epoch19, step407]: loss 0.566319
[epoch19, step408]: loss 0.501509
[epoch19, step409]: loss 0.577937
[epoch19, step410]: loss 0.650020
[epoch19, step411]: loss 0.414265
[epoch19, step412]: loss 0.412208
[epoch19, step413]: loss 0.242639
[epoch19, step414]: loss 0.453537
[epoch19, step415]: loss 0.438067
[epoch19, step416]: loss 0.552335
[epoch19, step417]: loss 0.256557
[epoch19, step418]: loss 0.645681
[epoch19, step419]: loss 0.596169
[epoch19, step420]: loss 0.397472
[epoch19, step421]: loss 0.408196
[epoch19, step422]: loss 0.736013
[epoch19, step423]: loss 0.368537
[epoch19, step424]: loss 0.263173
[epoch19, step425]: loss 0.425033
[epoch19, step426]: loss 0.431485
[epoch19, step427]: loss 0.398928
[epoch19, step428]: loss 0.380263
[epoch19, step429]: loss 0.372948
[epoch19, step430]: loss 0.538738
[epoch19, step431]: loss 0.363081
[epoch19, step432]: loss 0.168621
[epoch19, step433]: loss 0.311827
[epoch19, step434]: loss 0.239020
[epoch19, step435]: loss 0.463174
[epoch19, step436]: loss 0.389941
[epoch19, step437]: loss 0.531222
[epoch19, step438]: loss 0.259052
[epoch19, step439]: loss 0.468633
[epoch19, step440]: loss 0.376740
[epoch19, step441]: loss 0.509806
[epoch19, step442]: loss 0.207116
[epoch19, step443]: loss 0.561467
[epoch19, step444]: loss 0.322572
[epoch19, step445]: loss 0.373191
[epoch19, step446]: loss 0.459432
[epoch19, step447]: loss 0.437055
[epoch19, step448]: loss 0.442041
[epoch19, step449]: loss 0.456200
[epoch19, step450]: loss 0.426504
[epoch19, step451]: loss 0.205133
[epoch19, step452]: loss 0.531903
[epoch19, step453]: loss 0.505609
[epoch19, step454]: loss 0.329347
[epoch19, step455]: loss 0.482765
[epoch19, step456]: loss 0.276762
[epoch19, step457]: loss 0.475006
[epoch19, step458]: loss 0.138571
[epoch19, step459]: loss 0.414202
[epoch19, step460]: loss 0.748799
[epoch19, step461]: loss 0.586534
[epoch19, step462]: loss 0.641669
[epoch19, step463]: loss 0.477698
[epoch19, step464]: loss 0.408817
[epoch19, step465]: loss 0.487856
[epoch19, step466]: loss 0.284619
[epoch19, step467]: loss 0.371001
[epoch19, step468]: loss 0.584381
[epoch19, step469]: loss 0.559976
[epoch19, step470]: loss 0.396084
[epoch19, step471]: loss 0.605018
[epoch19, step472]: loss 0.260129
[epoch19, step473]: loss 0.388060
[epoch19, step474]: loss 0.341217
[epoch19, step475]: loss 0.535724
[epoch19, step476]: loss 0.576133
[epoch19, step477]: loss 0.651271
[epoch19, step478]: loss 0.480272
[epoch19, step479]: loss 0.318741
[epoch19, step480]: loss 0.415185
[epoch19, step481]: loss 0.580883
[epoch19, step482]: loss 0.621056
[epoch19, step483]: loss 0.369520
[epoch19, step484]: loss 0.369439
[epoch19, step485]: loss 0.454676
[epoch19, step486]: loss 0.717917
[epoch19, step487]: loss 0.575753
[epoch19, step488]: loss 0.506375
[epoch19, step489]: loss 0.560196
[epoch19, step490]: loss 0.625905
[epoch19, step491]: loss 0.347668
[epoch19, step492]: loss 0.470564
[epoch19, step493]: loss 0.474266
[epoch19, step494]: loss 0.487854
[epoch19, step495]: loss 0.440130
[epoch19, step496]: loss 0.455300
[epoch19, step497]: loss 0.514500
[epoch19, step498]: loss 0.355319
[epoch19, step499]: loss 0.405800
[epoch19, step500]: loss 0.490277
[epoch19, step501]: loss 0.575093
[epoch19, step502]: loss 0.485885
[epoch19, step503]: loss 0.113787
[epoch19, step504]: loss 0.551314
[epoch19, step505]: loss 0.289590
[epoch19, step506]: loss 0.534201
[epoch19, step507]: loss 0.538330
[epoch19, step508]: loss 0.660648
[epoch19, step509]: loss 0.293716
[epoch19, step510]: loss 0.390888
[epoch19, step511]: loss 0.446560
[epoch19, step512]: loss 0.478539
[epoch19, step513]: loss 0.417732
[epoch19, step514]: loss 0.600742
[epoch19, step515]: loss 0.508376
[epoch19, step516]: loss 0.546505
[epoch19, step517]: loss 0.626486
[epoch19, step518]: loss 0.404444
[epoch19, step519]: loss 0.361174
[epoch19, step520]: loss 0.575022
[epoch19, step521]: loss 0.402458
[epoch19, step522]: loss 0.605568
[epoch19, step523]: loss 0.467397
[epoch19, step524]: loss 0.471357
[epoch19, step525]: loss 0.351201
[epoch19, step526]: loss 0.346367
[epoch19, step527]: loss 0.214860
[epoch19, step528]: loss 0.682876
[epoch19, step529]: loss 0.683356
[epoch19, step530]: loss 0.332119
[epoch19, step531]: loss 0.388285
[epoch19, step532]: loss 0.480932
[epoch19, step533]: loss 0.481952
[epoch19, step534]: loss 0.672248
[epoch19, step535]: loss 0.509718
[epoch19, step536]: loss 0.348697
[epoch19, step537]: loss 0.384524
[epoch19, step538]: loss 0.460837
[epoch19, step539]: loss 0.453334
[epoch19, step540]: loss 0.620577
[epoch19, step541]: loss 0.619795
[epoch19, step542]: loss 0.669889
[epoch19, step543]: loss 0.598021
[epoch19, step544]: loss 0.487831
[epoch19, step545]: loss 0.515491
[epoch19, step546]: loss 0.527792
[epoch19, step547]: loss 0.523027
[epoch19, step548]: loss 0.576405
[epoch19, step549]: loss 0.268364
[epoch19, step550]: loss 0.524410
[epoch19, step551]: loss 0.542716
[epoch19, step552]: loss 0.419634
[epoch19, step553]: loss 0.293922
[epoch19, step554]: loss 0.354921
[epoch19, step555]: loss 0.520071
[epoch19, step556]: loss 0.417677
[epoch19, step557]: loss 0.341448
[epoch19, step558]: loss 0.414541
[epoch19, step559]: loss 0.377288
[epoch19, step560]: loss 0.320437
[epoch19, step561]: loss 0.537509
[epoch19, step562]: loss 0.482900
[epoch19, step563]: loss 0.558007
[epoch19, step564]: loss 0.477437
[epoch19, step565]: loss 0.429220
[epoch19, step566]: loss 0.434883
[epoch19, step567]: loss 0.477186
[epoch19, step568]: loss 0.488510
[epoch19, step569]: loss 0.591137
[epoch19, step570]: loss 0.380792
[epoch19, step571]: loss 0.260232
[epoch19, step572]: loss 0.442050
[epoch19, step573]: loss 0.405626
[epoch19, step574]: loss 0.365219
[epoch19, step575]: loss 0.654209
[epoch19, step576]: loss 0.324728
[epoch19, step577]: loss 0.534019
[epoch19, step578]: loss 0.492914
[epoch19, step579]: loss 0.651601
[epoch19, step580]: loss 0.397991
[epoch19, step581]: loss 0.323824
[epoch19, step582]: loss 0.690928
[epoch19, step583]: loss 0.377274
[epoch19, step584]: loss 0.535594
[epoch19, step585]: loss 0.649859
[epoch19, step586]: loss 0.324997
[epoch19, step587]: loss 0.632767
[epoch19, step588]: loss 0.372210
[epoch19, step589]: loss 0.274458
[epoch19, step590]: loss 0.434405
[epoch19, step591]: loss 0.391363
[epoch19, step592]: loss 0.524364
[epoch19, step593]: loss 0.592485
[epoch19, step594]: loss 0.317953
[epoch19, step595]: loss 0.472365
[epoch19, step596]: loss 0.459258
[epoch19, step597]: loss 0.370574
[epoch19, step598]: loss 0.480502
[epoch19, step599]: loss 0.599382
[epoch19, step600]: loss 0.523417
[epoch19, step601]: loss 0.722070
[epoch19, step602]: loss 0.441222
[epoch19, step603]: loss 0.420198
[epoch19, step604]: loss 0.469437
[epoch19, step605]: loss 0.471727
[epoch19, step606]: loss 0.432635
[epoch19, step607]: loss 0.608227
[epoch19, step608]: loss 0.586384
[epoch19, step609]: loss 0.339033
[epoch19, step610]: loss 0.543313
[epoch19, step611]: loss 0.383908
[epoch19, step612]: loss 0.477188
[epoch19, step613]: loss 0.564509
[epoch19, step614]: loss 0.667580
[epoch19, step615]: loss 0.564610
[epoch19, step616]: loss 0.411514
[epoch19, step617]: loss 0.695332
[epoch19, step618]: loss 0.407321
[epoch19, step619]: loss 0.561290
[epoch19, step620]: loss 0.234757
[epoch19, step621]: loss 0.310456
[epoch19, step622]: loss 0.516030
[epoch19, step623]: loss 0.277843
[epoch19, step624]: loss 0.700542
[epoch19, step625]: loss 0.542010
[epoch19, step626]: loss 0.327584
[epoch19, step627]: loss 0.426852
[epoch19, step628]: loss 0.416912
[epoch19, step629]: loss 0.492854
[epoch19, step630]: loss 0.617974
[epoch19, step631]: loss 0.537393
[epoch19, step632]: loss 0.579162
[epoch19, step633]: loss 0.517850
[epoch19, step634]: loss 0.505402
[epoch19, step635]: loss 0.473708
[epoch19, step636]: loss 0.551299
[epoch19, step637]: loss 0.502915
[epoch19, step638]: loss 0.263846
[epoch19, step639]: loss 0.310041
[epoch19, step640]: loss 0.440144
[epoch19, step641]: loss 0.495316
[epoch19, step642]: loss 0.394428
[epoch19, step643]: loss 0.549816
[epoch19, step644]: loss 0.365542
[epoch19, step645]: loss 0.284892
[epoch19, step646]: loss 0.546336
[epoch19, step647]: loss 0.249537
[epoch19, step648]: loss 0.309100
[epoch19, step649]: loss 0.383870
[epoch19, step650]: loss 0.432632
[epoch19, step651]: loss 0.491233
[epoch19, step652]: loss 0.571841
[epoch19, step653]: loss 0.355444
[epoch19, step654]: loss 0.301983
[epoch19, step655]: loss 0.484220
[epoch19, step656]: loss 0.548513
[epoch19, step657]: loss 0.357022
[epoch19, step658]: loss 0.683553
[epoch19, step659]: loss 0.336625
[epoch19, step660]: loss 0.167725
[epoch19, step661]: loss 0.537266
[epoch19, step662]: loss 0.408548
[epoch19, step663]: loss 0.623583
[epoch19, step664]: loss 0.432213
[epoch19, step665]: loss 0.551394
[epoch19, step666]: loss 0.594554
[epoch19, step667]: loss 0.280060
[epoch19, step668]: loss 0.615442
[epoch19, step669]: loss 0.451042
[epoch19, step670]: loss 0.361663
[epoch19, step671]: loss 0.261051
[epoch19, step672]: loss 0.638497
[epoch19, step673]: loss 0.568813
[epoch19, step674]: loss 0.486568
[epoch19, step675]: loss 0.527780
[epoch19, step676]: loss 0.514786
[epoch19, step677]: loss 0.364509
[epoch19, step678]: loss 0.375902
[epoch19, step679]: loss 0.337230
[epoch19, step680]: loss 0.469189
[epoch19, step681]: loss 0.491404
[epoch19, step682]: loss 0.889596
[epoch19, step683]: loss 0.485286
[epoch19, step684]: loss 0.422811
[epoch19, step685]: loss 0.585125
[epoch19, step686]: loss 0.489923
[epoch19, step687]: loss 0.265757
[epoch19, step688]: loss 0.441297
[epoch19, step689]: loss 0.610664
[epoch19, step690]: loss 0.309101
[epoch19, step691]: loss 0.357628
[epoch19, step692]: loss 0.448355
[epoch19, step693]: loss 0.411868
[epoch19, step694]: loss 0.318065
[epoch19, step695]: loss 0.153125
[epoch19, step696]: loss 0.374573
[epoch19, step697]: loss 0.647599
[epoch19, step698]: loss 0.426742
[epoch19, step699]: loss 0.519741
[epoch19, step700]: loss 0.606840
[epoch19, step701]: loss 0.571339
[epoch19, step702]: loss 0.518957
[epoch19, step703]: loss 0.438520
[epoch19, step704]: loss 0.425935
[epoch19, step705]: loss 0.556257
[epoch19, step706]: loss 0.580892
[epoch19, step707]: loss 0.502569
[epoch19, step708]: loss 0.502022
[epoch19, step709]: loss 0.547012
[epoch19, step710]: loss 0.523536
[epoch19, step711]: loss 0.512878
[epoch19, step712]: loss 0.520329
[epoch19, step713]: loss 0.569815
[epoch19, step714]: loss 0.462879
[epoch19, step715]: loss 0.466927
[epoch19, step716]: loss 0.310235
[epoch19, step717]: loss 0.268504
[epoch19, step718]: loss 0.471916
[epoch19, step719]: loss 0.728839
[epoch19, step720]: loss 0.503401
[epoch19, step721]: loss 0.456479
[epoch19, step722]: loss 0.552947
[epoch19, step723]: loss 0.556178
[epoch19, step724]: loss 0.449756
[epoch19, step725]: loss 0.247037
[epoch19, step726]: loss 0.324800
[epoch19, step727]: loss 0.406593
[epoch19, step728]: loss 0.624236
[epoch19, step729]: loss 0.512830
[epoch19, step730]: loss 0.454499
[epoch19, step731]: loss 0.344301
[epoch19, step732]: loss 0.710610
[epoch19, step733]: loss 0.462684
[epoch19, step734]: loss 0.261421
[epoch19, step735]: loss 0.437273
[epoch19, step736]: loss 0.309391
[epoch19, step737]: loss 0.346237
[epoch19, step738]: loss 0.648029
[epoch19, step739]: loss 0.449569
[epoch19, step740]: loss 0.600534
[epoch19, step741]: loss 0.435719
[epoch19, step742]: loss 0.459454
[epoch19, step743]: loss 0.595971
[epoch19, step744]: loss 0.152703
[epoch19, step745]: loss 0.491978
[epoch19, step746]: loss 0.387863
[epoch19, step747]: loss 0.429632
[epoch19, step748]: loss 0.451032
[epoch19, step749]: loss 0.431393
[epoch19, step750]: loss 0.260508
[epoch19, step751]: loss 0.449768
[epoch19, step752]: loss 0.594088
[epoch19, step753]: loss 0.486712
[epoch19, step754]: loss 0.257493
[epoch19, step755]: loss 0.488618
[epoch19, step756]: loss 0.138978
[epoch19, step757]: loss 0.455626
[epoch19, step758]: loss 0.537166
[epoch19, step759]: loss 0.435567
[epoch19, step760]: loss 0.352868
[epoch19, step761]: loss 0.521362
[epoch19, step762]: loss 0.612750
[epoch19, step763]: loss 0.448525
[epoch19, step764]: loss 0.575245
[epoch19, step765]: loss 0.587142
[epoch19, step766]: loss 0.442004
[epoch19, step767]: loss 0.386709
[epoch19, step768]: loss 0.581870
[epoch19, step769]: loss 0.420583
[epoch19, step770]: loss 0.541424
[epoch19, step771]: loss 0.396655
[epoch19, step772]: loss 0.496615
[epoch19, step773]: loss 0.554603
[epoch19, step774]: loss 0.611152
[epoch19, step775]: loss 0.376843
[epoch19, step776]: loss 0.377427
[epoch19, step777]: loss 0.333499
[epoch19, step778]: loss 0.296412
[epoch19, step779]: loss 0.366532
[epoch19, step780]: loss 0.346948
[epoch19, step781]: loss 0.500179
[epoch19, step782]: loss 0.625037
[epoch19, step783]: loss 0.675275
[epoch19, step784]: loss 0.414885
[epoch19, step785]: loss 0.582205
[epoch19, step786]: loss 0.477807
[epoch19, step787]: loss 0.560603
[epoch19, step788]: loss 0.449878
[epoch19, step789]: loss 0.726458
[epoch19, step790]: loss 0.520375
[epoch19, step791]: loss 0.531851
[epoch19, step792]: loss 0.466339
[epoch19, step793]: loss 0.380665
[epoch19, step794]: loss 0.492029
[epoch19, step795]: loss 0.529905
[epoch19, step796]: loss 0.408785
[epoch19, step797]: loss 0.368306
[epoch19, step798]: loss 0.159843
[epoch19, step799]: loss 0.504733
[epoch19, step800]: loss 0.260835
[epoch19, step801]: loss 0.546482
[epoch19, step802]: loss 0.350049
[epoch19, step803]: loss 0.451259
[epoch19, step804]: loss 0.220096
[epoch19, step805]: loss 0.519364
[epoch19, step806]: loss 0.462998
[epoch19, step807]: loss 0.257810
[epoch19, step808]: loss 0.607463
[epoch19, step809]: loss 0.244749
[epoch19, step810]: loss 0.575869
[epoch19, step811]: loss 0.438123
[epoch19, step812]: loss 0.164175
[epoch19, step813]: loss 0.482628
[epoch19, step814]: loss 0.596209
[epoch19, step815]: loss 0.448636
[epoch19, step816]: loss 0.253222
[epoch19, step817]: loss 0.399382
[epoch19, step818]: loss 0.549791
[epoch19, step819]: loss 0.289698
[epoch19, step820]: loss 0.528888
[epoch19, step821]: loss 0.406910
[epoch19, step822]: loss 0.361819
[epoch19, step823]: loss 0.297809
[epoch19, step824]: loss 0.524512
[epoch19, step825]: loss 0.489263
[epoch19, step826]: loss 0.473867
[epoch19, step827]: loss 0.462394
[epoch19, step828]: loss 0.406768
[epoch19, step829]: loss 0.519076
[epoch19, step830]: loss 0.597093
[epoch19, step831]: loss 0.681081
[epoch19, step832]: loss 0.468415
[epoch19, step833]: loss 0.591541
[epoch19, step834]: loss 0.552609
[epoch19, step835]: loss 0.265244
[epoch19, step836]: loss 0.573491
[epoch19, step837]: loss 0.305954
[epoch19, step838]: loss 0.160430
[epoch19, step839]: loss 0.588349
[epoch19, step840]: loss 0.524250
[epoch19, step841]: loss 0.629879
[epoch19, step842]: loss 0.516221
[epoch19, step843]: loss 0.451972
[epoch19, step844]: loss 0.653477
[epoch19, step845]: loss 0.610213
[epoch19, step846]: loss 0.516174
[epoch19, step847]: loss 0.521881
[epoch19, step848]: loss 0.573391
[epoch19, step849]: loss 0.439221
[epoch19, step850]: loss 0.501508
[epoch19, step851]: loss 0.607508
[epoch19, step852]: loss 0.689571
[epoch19, step853]: loss 0.523308
[epoch19, step854]: loss 0.576728
[epoch19, step855]: loss 0.604845
[epoch19, step856]: loss 0.304416
[epoch19, step857]: loss 0.321084
[epoch19, step858]: loss 0.398664
[epoch19, step859]: loss 0.531726
[epoch19, step860]: loss 0.437620
[epoch19, step861]: loss 0.567592
[epoch19, step862]: loss 0.563287
[epoch19, step863]: loss 0.547615
[epoch19, step864]: loss 0.362556
[epoch19, step865]: loss 0.583119
[epoch19, step866]: loss 0.489742
[epoch19, step867]: loss 0.483429
[epoch19, step868]: loss 0.453949
[epoch19, step869]: loss 0.434639
[epoch19, step870]: loss 0.667938
[epoch19, step871]: loss 0.494691
[epoch19, step872]: loss 0.337921
[epoch19, step873]: loss 0.481541
[epoch19, step874]: loss 0.332620
[epoch19, step875]: loss 0.499995
[epoch19, step876]: loss 0.463965
[epoch19, step877]: loss 0.467022
[epoch19, step878]: loss 0.493014
[epoch19, step879]: loss 0.629827
[epoch19, step880]: loss 0.551899
[epoch19, step881]: loss 0.664613
[epoch19, step882]: loss 0.526733
[epoch19, step883]: loss 0.431117
[epoch19, step884]: loss 0.259837
[epoch19, step885]: loss 0.282757
[epoch19, step886]: loss 0.404358
[epoch19, step887]: loss 0.455203
[epoch19, step888]: loss 0.603863
[epoch19, step889]: loss 0.472669
[epoch19, step890]: loss 0.411733
[epoch19, step891]: loss 0.177998
[epoch19, step892]: loss 0.583849
[epoch19, step893]: loss 0.414405
[epoch19, step894]: loss 0.430761
[epoch19, step895]: loss 0.267113
[epoch19, step896]: loss 0.548676
[epoch19, step897]: loss 0.563869
[epoch19, step898]: loss 0.401870
[epoch19, step899]: loss 0.301601
[epoch19, step900]: loss 0.502278
[epoch19, step901]: loss 0.459294
[epoch19, step902]: loss 0.352918
[epoch19, step903]: loss 0.461922
[epoch19, step904]: loss 0.465794
[epoch19, step905]: loss 0.452459
[epoch19, step906]: loss 0.662855
[epoch19, step907]: loss 0.436609
[epoch19, step908]: loss 0.543470
[epoch19, step909]: loss 0.464209
[epoch19, step910]: loss 0.443346
[epoch19, step911]: loss 0.445954
[epoch19, step912]: loss 0.312556
[epoch19, step913]: loss 0.443551
[epoch19, step914]: loss 0.339510
[epoch19, step915]: loss 0.633870
[epoch19, step916]: loss 0.349535
[epoch19, step917]: loss 0.683729
[epoch19, step918]: loss 0.458601
[epoch19, step919]: loss 0.485217
[epoch19, step920]: loss 0.367547
[epoch19, step921]: loss 0.518263
[epoch19, step922]: loss 0.262083
[epoch19, step923]: loss 0.552866
[epoch19, step924]: loss 0.478975
[epoch19, step925]: loss 0.428143
[epoch19, step926]: loss 0.511049
[epoch19, step927]: loss 0.329400
[epoch19, step928]: loss 0.510416
[epoch19, step929]: loss 0.368277
[epoch19, step930]: loss 0.347994
[epoch19, step931]: loss 0.427233
[epoch19, step932]: loss 0.627557
[epoch19, step933]: loss 0.552875
[epoch19, step934]: loss 0.498596
[epoch19, step935]: loss 0.327525
[epoch19, step936]: loss 0.470831
[epoch19, step937]: loss 0.389930
[epoch19, step938]: loss 0.447987
[epoch19, step939]: loss 0.663045
[epoch19, step940]: loss 0.422839
[epoch19, step941]: loss 0.386710
[epoch19, step942]: loss 0.470160
[epoch19, step943]: loss 0.469944
[epoch19, step944]: loss 0.463912
[epoch19, step945]: loss 0.447662
[epoch19, step946]: loss 0.523245
[epoch19, step947]: loss 0.508027
[epoch19, step948]: loss 0.666241
[epoch19, step949]: loss 0.459438
[epoch19, step950]: loss 0.552094
[epoch19, step951]: loss 0.666163
[epoch19, step952]: loss 0.643244
[epoch19, step953]: loss 0.588132
[epoch19, step954]: loss 0.566868
[epoch19, step955]: loss 0.336919
[epoch19, step956]: loss 0.517795
[epoch19, step957]: loss 0.363970
[epoch19, step958]: loss 0.444094
[epoch19, step959]: loss 0.196865
[epoch19, step960]: loss 0.454057
[epoch19, step961]: loss 0.362833
[epoch19, step962]: loss 0.425827
[epoch19, step963]: loss 0.351447
[epoch19, step964]: loss 0.224167
[epoch19, step965]: loss 0.519002
[epoch19, step966]: loss 0.411485
[epoch19, step967]: loss 0.509996
[epoch19, step968]: loss 0.307240
[epoch19, step969]: loss 0.635795
[epoch19, step970]: loss 0.347033
[epoch19, step971]: loss 0.501314
[epoch19, step972]: loss 0.416054
[epoch19, step973]: loss 0.250720
[epoch19, step974]: loss 0.552689
[epoch19, step975]: loss 0.376207
[epoch19, step976]: loss 0.510861
[epoch19, step977]: loss 0.455919
[epoch19, step978]: loss 0.264544
[epoch19, step979]: loss 0.522656
[epoch19, step980]: loss 0.413241
[epoch19, step981]: loss 0.374711
[epoch19, step982]: loss 0.363975
[epoch19, step983]: loss 0.541873
[epoch19, step984]: loss 0.219684
[epoch19, step985]: loss 0.369062
[epoch19, step986]: loss 0.426718
[epoch19, step987]: loss 0.236919
[epoch19, step988]: loss 0.255844
[epoch19, step989]: loss 0.479695
[epoch19, step990]: loss 0.538310
[epoch19, step991]: loss 0.189790
[epoch19, step992]: loss 0.470360
[epoch19, step993]: loss 0.507331
[epoch19, step994]: loss 0.575160
[epoch19, step995]: loss 0.653225
[epoch19, step996]: loss 0.445816
[epoch19, step997]: loss 0.510652
[epoch19, step998]: loss 0.540907
[epoch19, step999]: loss 0.347428
[epoch19, step1000]: loss 0.461869
[epoch19, step1001]: loss 0.591650
[epoch19, step1002]: loss 0.607205
[epoch19, step1003]: loss 0.476969
[epoch19, step1004]: loss 0.565167
[epoch19, step1005]: loss 0.642054
[epoch19, step1006]: loss 0.655173
[epoch19, step1007]: loss 0.502066
[epoch19, step1008]: loss 0.573688
[epoch19, step1009]: loss 0.398977
[epoch19, step1010]: loss 0.453947
[epoch19, step1011]: loss 0.662918
[epoch19, step1012]: loss 0.391356
[epoch19, step1013]: loss 0.621262
[epoch19, step1014]: loss 0.564330
[epoch19, step1015]: loss 0.421588
[epoch19, step1016]: loss 0.608043
[epoch19, step1017]: loss 0.386616
[epoch19, step1018]: loss 0.369241
[epoch19, step1019]: loss 0.117087
[epoch19, step1020]: loss 0.516931
[epoch19, step1021]: loss 0.689908
[epoch19, step1022]: loss 0.578546
[epoch19, step1023]: loss 0.466524
[epoch19, step1024]: loss 0.529811
[epoch19, step1025]: loss 0.529538
[epoch19, step1026]: loss 0.712373
[epoch19, step1027]: loss 0.288203
[epoch19, step1028]: loss 0.518226
[epoch19, step1029]: loss 0.428902
[epoch19, step1030]: loss 0.637904
[epoch19, step1031]: loss 0.414023
[epoch19, step1032]: loss 0.327732
[epoch19, step1033]: loss 0.422218
[epoch19, step1034]: loss 0.478866
[epoch19, step1035]: loss 0.261811
[epoch19, step1036]: loss 0.249061
[epoch19, step1037]: loss 0.270793
[epoch19, step1038]: loss 0.292704
[epoch19, step1039]: loss 0.623982
[epoch19, step1040]: loss 0.475974
[epoch19, step1041]: loss 0.579898
[epoch19, step1042]: loss 0.352823
[epoch19, step1043]: loss 0.218890
[epoch19, step1044]: loss 0.362333
[epoch19, step1045]: loss 0.377233
[epoch19, step1046]: loss 0.325121
[epoch19, step1047]: loss 0.550768
[epoch19, step1048]: loss 0.478670
[epoch19, step1049]: loss 0.651297
[epoch19, step1050]: loss 0.618326
[epoch19, step1051]: loss 0.644413
[epoch19, step1052]: loss 0.656653
[epoch19, step1053]: loss 0.290839
[epoch19, step1054]: loss 0.577003
[epoch19, step1055]: loss 0.450934
[epoch19, step1056]: loss 0.351622
[epoch19, step1057]: loss 0.522823
[epoch19, step1058]: loss 0.416894
[epoch19, step1059]: loss 0.465546
[epoch19, step1060]: loss 0.302452
[epoch19, step1061]: loss 0.269497
[epoch19, step1062]: loss 0.502354
[epoch19, step1063]: loss 0.496853
[epoch19, step1064]: loss 0.520819
[epoch19, step1065]: loss 0.323857
[epoch19, step1066]: loss 0.519993
[epoch19, step1067]: loss 0.635193
[epoch19, step1068]: loss 0.152935
[epoch19, step1069]: loss 0.575536
[epoch19, step1070]: loss 0.584274
[epoch19, step1071]: loss 0.383761
[epoch19, step1072]: loss 0.166221
[epoch19, step1073]: loss 0.602208
[epoch19, step1074]: loss 0.625310
[epoch19, step1075]: loss 0.604553
[epoch19, step1076]: loss 0.213676
[epoch19, step1077]: loss 0.488937
[epoch19, step1078]: loss 0.266417
[epoch19, step1079]: loss 0.229063
[epoch19, step1080]: loss 0.695435
[epoch19, step1081]: loss 0.530011
[epoch19, step1082]: loss 0.516501
[epoch19, step1083]: loss 0.436513
[epoch19, step1084]: loss 0.450982
[epoch19, step1085]: loss 0.581047
[epoch19, step1086]: loss 0.199081
[epoch19, step1087]: loss 0.416458
[epoch19, step1088]: loss 0.123602
[epoch19, step1089]: loss 0.567879
[epoch19, step1090]: loss 0.610633
[epoch19, step1091]: loss 0.396393
[epoch19, step1092]: loss 0.603784
[epoch19, step1093]: loss 0.208477
[epoch19, step1094]: loss 0.335818
[epoch19, step1095]: loss 0.477740
[epoch19, step1096]: loss 0.519858
[epoch19, step1097]: loss 0.467898
[epoch19, step1098]: loss 0.442966
[epoch19, step1099]: loss 0.395132
[epoch19, step1100]: loss 0.384426
[epoch19, step1101]: loss 0.546506
[epoch19, step1102]: loss 0.282156
[epoch19, step1103]: loss 0.515859
[epoch19, step1104]: loss 0.336381
[epoch19, step1105]: loss 0.622037
[epoch19, step1106]: loss 0.215082
[epoch19, step1107]: loss 0.299696
[epoch19, step1108]: loss 0.341797
[epoch19, step1109]: loss 0.476215
[epoch19, step1110]: loss 0.425732
[epoch19, step1111]: loss 0.512489
[epoch19, step1112]: loss 0.530391
[epoch19, step1113]: loss 0.527733
[epoch19, step1114]: loss 0.625433
[epoch19, step1115]: loss 0.633172
[epoch19, step1116]: loss 0.352996
[epoch19, step1117]: loss 0.465616
[epoch19, step1118]: loss 0.502760
[epoch19, step1119]: loss 0.541522
[epoch19, step1120]: loss 0.611798
[epoch19, step1121]: loss 0.304633
[epoch19, step1122]: loss 0.426999
[epoch19, step1123]: loss 0.506000
[epoch19, step1124]: loss 0.569762
[epoch19, step1125]: loss 0.556590
[epoch19, step1126]: loss 0.379416
[epoch19, step1127]: loss 0.453406
[epoch19, step1128]: loss 0.552478
[epoch19, step1129]: loss 0.546231
[epoch19, step1130]: loss 0.431900
[epoch19, step1131]: loss 0.584677
[epoch19, step1132]: loss 0.469462
[epoch19, step1133]: loss 0.476363
[epoch19, step1134]: loss 0.325507
[epoch19, step1135]: loss 0.442614
[epoch19, step1136]: loss 0.572045
[epoch19, step1137]: loss 0.443260
[epoch19, step1138]: loss 0.567776
[epoch19, step1139]: loss 0.542228
[epoch19, step1140]: loss 0.311157
[epoch19, step1141]: loss 0.435492
[epoch19, step1142]: loss 0.293917
[epoch19, step1143]: loss 0.443179
[epoch19, step1144]: loss 0.399717
[epoch19, step1145]: loss 0.264987
[epoch19, step1146]: loss 0.563808
[epoch19, step1147]: loss 0.367438
[epoch19, step1148]: loss 0.218515
[epoch19, step1149]: loss 0.550198
[epoch19, step1150]: loss 0.434082
[epoch19, step1151]: loss 0.586108
[epoch19, step1152]: loss 0.231502
[epoch19, step1153]: loss 0.521138
[epoch19, step1154]: loss 0.544111
[epoch19, step1155]: loss 0.433814
[epoch19, step1156]: loss 0.541138
[epoch19, step1157]: loss 0.488347
[epoch19, step1158]: loss 0.565884
[epoch19, step1159]: loss 0.680152
[epoch19, step1160]: loss 0.323643
[epoch19, step1161]: loss 0.295766
[epoch19, step1162]: loss 0.471926
[epoch19, step1163]: loss 0.561462
[epoch19, step1164]: loss 0.429933
[epoch19, step1165]: loss 0.317430
[epoch19, step1166]: loss 0.675578
[epoch19, step1167]: loss 0.470364
[epoch19, step1168]: loss 0.399841
[epoch19, step1169]: loss 0.442204
[epoch19, step1170]: loss 0.301369
[epoch19, step1171]: loss 0.605834
[epoch19, step1172]: loss 0.662051
[epoch19, step1173]: loss 0.232147
[epoch19, step1174]: loss 0.398647
[epoch19, step1175]: loss 0.423659
[epoch19, step1176]: loss 0.509686
[epoch19, step1177]: loss 0.440176
[epoch19, step1178]: loss 0.438811
[epoch19, step1179]: loss 0.771508
[epoch19, step1180]: loss 0.342061
[epoch19, step1181]: loss 0.599505
[epoch19, step1182]: loss 0.457554
[epoch19, step1183]: loss 0.491160
[epoch19, step1184]: loss 0.478932
[epoch19, step1185]: loss 0.651623
[epoch19, step1186]: loss 0.501724
[epoch19, step1187]: loss 0.590868
[epoch19, step1188]: loss 0.546219
[epoch19, step1189]: loss 0.464607
[epoch19, step1190]: loss 0.434878
[epoch19, step1191]: loss 0.450059
[epoch19, step1192]: loss 0.471331
[epoch19, step1193]: loss 0.393322
[epoch19, step1194]: loss 0.471041
[epoch19, step1195]: loss 0.265096
[epoch19, step1196]: loss 0.392347
[epoch19, step1197]: loss 0.586813
[epoch19, step1198]: loss 0.403706
[epoch19, step1199]: loss 0.500126
[epoch19, step1200]: loss 0.209108
[epoch19, step1201]: loss 0.317298
[epoch19, step1202]: loss 0.475266
[epoch19, step1203]: loss 0.304737
[epoch19, step1204]: loss 0.369479
[epoch19, step1205]: loss 0.205674
[epoch19, step1206]: loss 0.635494
[epoch19, step1207]: loss 0.558557
[epoch19, step1208]: loss 0.607019
[epoch19, step1209]: loss 0.186592
[epoch19, step1210]: loss 0.336628
[epoch19, step1211]: loss 0.499106
[epoch19, step1212]: loss 0.593719
[epoch19, step1213]: loss 0.662389
[epoch19, step1214]: loss 0.369160
[epoch19, step1215]: loss 0.584457
[epoch19, step1216]: loss 0.379383
[epoch19, step1217]: loss 0.180911
[epoch19, step1218]: loss 0.310252
[epoch19, step1219]: loss 0.596874
[epoch19, step1220]: loss 0.581544
[epoch19, step1221]: loss 0.249002
[epoch19, step1222]: loss 0.259576
[epoch19, step1223]: loss 0.417816
[epoch19, step1224]: loss 0.252981
[epoch19, step1225]: loss 0.504742
[epoch19, step1226]: loss 0.290794
[epoch19, step1227]: loss 0.335450
[epoch19, step1228]: loss 0.319609
[epoch19, step1229]: loss 0.623439
[epoch19, step1230]: loss 0.493285
[epoch19, step1231]: loss 0.457587
[epoch19, step1232]: loss 0.624431
[epoch19, step1233]: loss 0.477868
[epoch19, step1234]: loss 0.269781
[epoch19, step1235]: loss 0.443031
[epoch19, step1236]: loss 0.545664
[epoch19, step1237]: loss 0.418720
[epoch19, step1238]: loss 0.575653
[epoch19, step1239]: loss 0.493746
[epoch19, step1240]: loss 0.333239
[epoch19, step1241]: loss 0.483102
[epoch19, step1242]: loss 0.400844
[epoch19, step1243]: loss 0.583566
[epoch19, step1244]: loss 0.610776
[epoch19, step1245]: loss 0.631696
[epoch19, step1246]: loss 0.509761
[epoch19, step1247]: loss 0.565099
[epoch19, step1248]: loss 0.375025
[epoch19, step1249]: loss 0.607149
[epoch19, step1250]: loss 0.712032
[epoch19, step1251]: loss 0.383426
[epoch19, step1252]: loss 0.604009
[epoch19, step1253]: loss 0.641928
[epoch19, step1254]: loss 0.525354
[epoch19, step1255]: loss 0.545651
[epoch19, step1256]: loss 0.257952
[epoch19, step1257]: loss 0.420609
[epoch19, step1258]: loss 0.737271
[epoch19, step1259]: loss 0.293546
[epoch19, step1260]: loss 0.465261
[epoch19, step1261]: loss 0.453531
[epoch19, step1262]: loss 0.486021
[epoch19, step1263]: loss 0.371578
[epoch19, step1264]: loss 0.428733
[epoch19, step1265]: loss 0.474845
[epoch19, step1266]: loss 0.437250
[epoch19, step1267]: loss 0.570935
[epoch19, step1268]: loss 0.576950
[epoch19, step1269]: loss 0.564341
[epoch19, step1270]: loss 0.481021
[epoch19, step1271]: loss 0.393651
[epoch19, step1272]: loss 0.554988
[epoch19, step1273]: loss 0.323205
[epoch19, step1274]: loss 0.490885
[epoch19, step1275]: loss 0.429240
[epoch19, step1276]: loss 0.426877
[epoch19, step1277]: loss 0.420668
[epoch19, step1278]: loss 0.271349
[epoch19, step1279]: loss 0.372486
[epoch19, step1280]: loss 0.341292
[epoch19, step1281]: loss 0.409821
[epoch19, step1282]: loss 0.521099
[epoch19, step1283]: loss 0.511269
[epoch19, step1284]: loss 0.431216
[epoch19, step1285]: loss 0.523701
[epoch19, step1286]: loss 0.635679
[epoch19, step1287]: loss 0.349907
[epoch19, step1288]: loss 0.735766
[epoch19, step1289]: loss 0.160215
[epoch19, step1290]: loss 0.512925
[epoch19, step1291]: loss 0.422441
[epoch19, step1292]: loss 0.412023
[epoch19, step1293]: loss 0.428025
[epoch19, step1294]: loss 0.623945
[epoch19, step1295]: loss 0.377850
[epoch19, step1296]: loss 0.363540
[epoch19, step1297]: loss 0.520209
[epoch19, step1298]: loss 0.370128
[epoch19, step1299]: loss 0.572955
[epoch19, step1300]: loss 0.543574
[epoch19, step1301]: loss 0.412147
[epoch19, step1302]: loss 0.413750
[epoch19, step1303]: loss 0.546802
[epoch19, step1304]: loss 0.488115
[epoch19, step1305]: loss 0.189655
[epoch19, step1306]: loss 0.522393
[epoch19, step1307]: loss 0.503580
[epoch19, step1308]: loss 0.732666
[epoch19, step1309]: loss 0.398515
[epoch19, step1310]: loss 0.496842
[epoch19, step1311]: loss 0.256089
[epoch19, step1312]: loss 0.343936
[epoch19, step1313]: loss 0.327529
[epoch19, step1314]: loss 0.550269
[epoch19, step1315]: loss 0.664466
[epoch19, step1316]: loss 0.630677
[epoch19, step1317]: loss 0.325010
[epoch19, step1318]: loss 0.238982
[epoch19, step1319]: loss 0.397037
[epoch19, step1320]: loss 0.546588
[epoch19, step1321]: loss 0.482815
[epoch19, step1322]: loss 0.266665
[epoch19, step1323]: loss 0.590012
[epoch19, step1324]: loss 0.570716
[epoch19, step1325]: loss 0.614653
[epoch19, step1326]: loss 0.387146
[epoch19, step1327]: loss 0.312149
[epoch19, step1328]: loss 0.643476
[epoch19, step1329]: loss 0.598408
[epoch19, step1330]: loss 0.560806
[epoch19, step1331]: loss 0.415943
[epoch19, step1332]: loss 0.565340
[epoch19, step1333]: loss 0.719139
[epoch19, step1334]: loss 0.254194
[epoch19, step1335]: loss 0.379179
[epoch19, step1336]: loss 0.586278
[epoch19, step1337]: loss 0.605599
[epoch19, step1338]: loss 0.200037
[epoch19, step1339]: loss 0.533436
[epoch19, step1340]: loss 0.484047
[epoch19, step1341]: loss 0.412251
[epoch19, step1342]: loss 0.622771
[epoch19, step1343]: loss 0.292805
[epoch19, step1344]: loss 0.454247
[epoch19, step1345]: loss 0.340883
[epoch19, step1346]: loss 0.614865
[epoch19, step1347]: loss 0.437049
[epoch19, step1348]: loss 0.531123
[epoch19, step1349]: loss 0.557782
[epoch19, step1350]: loss 0.423221
[epoch19, step1351]: loss 0.343006
[epoch19, step1352]: loss 0.520370
[epoch19, step1353]: loss 0.509433
[epoch19, step1354]: loss 0.587545
[epoch19, step1355]: loss 0.522842
[epoch19, step1356]: loss 0.420059
[epoch19, step1357]: loss 0.686595
[epoch19, step1358]: loss 0.439885
[epoch19, step1359]: loss 0.651625
[epoch19, step1360]: loss 0.419996
[epoch19, step1361]: loss 0.541090
[epoch19, step1362]: loss 0.350135
[epoch19, step1363]: loss 0.487633
[epoch19, step1364]: loss 0.383787
[epoch19, step1365]: loss 0.650755
[epoch19, step1366]: loss 0.424230
[epoch19, step1367]: loss 0.603568
[epoch19, step1368]: loss 0.446632
[epoch19, step1369]: loss 0.417255
[epoch19, step1370]: loss 0.440787
[epoch19, step1371]: loss 0.519619
[epoch19, step1372]: loss 0.532096
[epoch19, step1373]: loss 0.539713
[epoch19, step1374]: loss 0.669961
[epoch19, step1375]: loss 0.200846
[epoch19, step1376]: loss 0.659863
[epoch19, step1377]: loss 0.373533
[epoch19, step1378]: loss 0.216704
[epoch19, step1379]: loss 0.363246
[epoch19, step1380]: loss 0.331370
[epoch19, step1381]: loss 0.402521
[epoch19, step1382]: loss 0.336331
[epoch19, step1383]: loss 0.313671
[epoch19, step1384]: loss 0.534662
[epoch19, step1385]: loss 0.336491
[epoch19, step1386]: loss 0.634965
[epoch19, step1387]: loss 0.355041
[epoch19, step1388]: loss 0.269991
[epoch19, step1389]: loss 0.355613
[epoch19, step1390]: loss 0.332311
[epoch19, step1391]: loss 0.616786
[epoch19, step1392]: loss 0.468031
[epoch19, step1393]: loss 0.471573
[epoch19, step1394]: loss 0.538735
[epoch19, step1395]: loss 0.517525
[epoch19, step1396]: loss 0.111943
[epoch19, step1397]: loss 0.235065
[epoch19, step1398]: loss 0.470876
[epoch19, step1399]: loss 0.510956
[epoch19, step1400]: loss 0.697436
[epoch19, step1401]: loss 0.516613
[epoch19, step1402]: loss 0.585258
[epoch19, step1403]: loss 0.286468
[epoch19, step1404]: loss 0.647000
[epoch19, step1405]: loss 0.437017
[epoch19, step1406]: loss 0.481413
[epoch19, step1407]: loss 0.350314
[epoch19, step1408]: loss 0.742008
[epoch19, step1409]: loss 0.505376
[epoch19, step1410]: loss 0.464011
[epoch19, step1411]: loss 0.258474
[epoch19, step1412]: loss 0.477831
[epoch19, step1413]: loss 0.537820
[epoch19, step1414]: loss 0.395011
[epoch19, step1415]: loss 0.465094
[epoch19, step1416]: loss 0.401542
[epoch19, step1417]: loss 0.350893
[epoch19, step1418]: loss 0.518042
[epoch19, step1419]: loss 0.565639
[epoch19, step1420]: loss 0.517556
[epoch19, step1421]: loss 0.315238
[epoch19, step1422]: loss 0.365256
[epoch19, step1423]: loss 0.309929
[epoch19, step1424]: loss 0.656601
[epoch19, step1425]: loss 0.539791
[epoch19, step1426]: loss 0.616192
[epoch19, step1427]: loss 0.484161
[epoch19, step1428]: loss 0.679669
[epoch19, step1429]: loss 0.573900
[epoch19, step1430]: loss 0.660451
[epoch19, step1431]: loss 0.372654
[epoch19, step1432]: loss 0.358012
[epoch19, step1433]: loss 0.520493
[epoch19, step1434]: loss 0.125219
[epoch19, step1435]: loss 0.486874
[epoch19, step1436]: loss 0.522224
[epoch19, step1437]: loss 0.346903
[epoch19, step1438]: loss 0.640903
[epoch19, step1439]: loss 0.271185
[epoch19, step1440]: loss 0.258457
[epoch19, step1441]: loss 0.361119
[epoch19, step1442]: loss 0.404234
[epoch19, step1443]: loss 0.448378
[epoch19, step1444]: loss 0.290229
[epoch19, step1445]: loss 0.261193
[epoch19, step1446]: loss 0.467440
[epoch19, step1447]: loss 0.247675
[epoch19, step1448]: loss 0.618546
[epoch19, step1449]: loss 0.534580
[epoch19, step1450]: loss 0.590590
[epoch19, step1451]: loss 0.688692
[epoch19, step1452]: loss 0.354490
[epoch19, step1453]: loss 0.697323
[epoch19, step1454]: loss 0.626085
[epoch19, step1455]: loss 0.361381
[epoch19, step1456]: loss 0.457834
[epoch19, step1457]: loss 0.282381
[epoch19, step1458]: loss 0.392502
[epoch19, step1459]: loss 0.514745
[epoch19, step1460]: loss 0.695443
[epoch19, step1461]: loss 0.479639
[epoch19, step1462]: loss 0.424158
[epoch19, step1463]: loss 0.497283
[epoch19, step1464]: loss 0.485224
[epoch19, step1465]: loss 0.267341
[epoch19, step1466]: loss 0.467894
[epoch19, step1467]: loss 0.403191
[epoch19, step1468]: loss 0.424537
[epoch19, step1469]: loss 0.601838
[epoch19, step1470]: loss 0.419569
[epoch19, step1471]: loss 0.520026
[epoch19, step1472]: loss 0.524414
[epoch19, step1473]: loss 0.219890
[epoch19, step1474]: loss 0.575499
[epoch19, step1475]: loss 0.518054
[epoch19, step1476]: loss 0.434413
[epoch19, step1477]: loss 0.535804
[epoch19, step1478]: loss 0.251695
[epoch19, step1479]: loss 0.553869
[epoch19, step1480]: loss 0.476946
[epoch19, step1481]: loss 0.599257
[epoch19, step1482]: loss 0.427946
[epoch19, step1483]: loss 0.452582
[epoch19, step1484]: loss 0.514302
[epoch19, step1485]: loss 0.670929
[epoch19, step1486]: loss 0.391056
[epoch19, step1487]: loss 0.539797
[epoch19, step1488]: loss 0.335149
[epoch19, step1489]: loss 0.332898
[epoch19, step1490]: loss 0.435251
[epoch19, step1491]: loss 0.120568
[epoch19, step1492]: loss 0.602199
[epoch19, step1493]: loss 0.212043
[epoch19, step1494]: loss 0.464743
[epoch19, step1495]: loss 0.585892
[epoch19, step1496]: loss 0.547343
[epoch19, step1497]: loss 0.676115
[epoch19, step1498]: loss 0.673041
[epoch19, step1499]: loss 0.548655
[epoch19, step1500]: loss 0.440904
[epoch19, step1501]: loss 0.232637
[epoch19, step1502]: loss 0.367097
[epoch19, step1503]: loss 0.487502
[epoch19, step1504]: loss 0.762130
[epoch19, step1505]: loss 0.339212
[epoch19, step1506]: loss 0.436354
[epoch19, step1507]: loss 0.737659
[epoch19, step1508]: loss 0.328838
[epoch19, step1509]: loss 0.599076
[epoch19, step1510]: loss 0.527870
[epoch19, step1511]: loss 0.557375
[epoch19, step1512]: loss 0.394697
[epoch19, step1513]: loss 0.272906
[epoch19, step1514]: loss 0.371263
[epoch19, step1515]: loss 0.582359
[epoch19, step1516]: loss 0.525817
[epoch19, step1517]: loss 0.624239
[epoch19, step1518]: loss 0.603061
[epoch19, step1519]: loss 0.505861
[epoch19, step1520]: loss 0.429611
[epoch19, step1521]: loss 0.295762
[epoch19, step1522]: loss 0.528113
[epoch19, step1523]: loss 0.669112
[epoch19, step1524]: loss 0.484407
[epoch19, step1525]: loss 0.389327
[epoch19, step1526]: loss 0.442832
[epoch19, step1527]: loss 0.441995
[epoch19, step1528]: loss 0.443558
[epoch19, step1529]: loss 0.679504
[epoch19, step1530]: loss 0.532843
[epoch19, step1531]: loss 0.464379
[epoch19, step1532]: loss 0.331453
[epoch19, step1533]: loss 0.574483
[epoch19, step1534]: loss 0.481002
[epoch19, step1535]: loss 0.613347
[epoch19, step1536]: loss 0.640274
[epoch19, step1537]: loss 0.543651
[epoch19, step1538]: loss 0.336941
[epoch19, step1539]: loss 0.545056
[epoch19, step1540]: loss 0.674783
[epoch19, step1541]: loss 0.642365
[epoch19, step1542]: loss 0.563208
[epoch19, step1543]: loss 0.426928
[epoch19, step1544]: loss 0.505794
[epoch19, step1545]: loss 0.613947
[epoch19, step1546]: loss 0.537463
[epoch19, step1547]: loss 0.375719
[epoch19, step1548]: loss 0.425177
[epoch19, step1549]: loss 0.477474
[epoch19, step1550]: loss 0.523235
[epoch19, step1551]: loss 0.305784
[epoch19, step1552]: loss 0.455728
[epoch19, step1553]: loss 0.466975
[epoch19, step1554]: loss 0.344655
[epoch19, step1555]: loss 0.648506
[epoch19, step1556]: loss 0.347137
[epoch19, step1557]: loss 0.493608
[epoch19, step1558]: loss 0.463653
[epoch19, step1559]: loss 0.431334
[epoch19, step1560]: loss 0.541019
[epoch19, step1561]: loss 0.308703
[epoch19, step1562]: loss 0.382449
[epoch19, step1563]: loss 0.224223
[epoch19, step1564]: loss 0.407898
[epoch19, step1565]: loss 0.503276
[epoch19, step1566]: loss 0.582097
[epoch19, step1567]: loss 0.694704
[epoch19, step1568]: loss 0.406653
[epoch19, step1569]: loss 0.195482
[epoch19, step1570]: loss 0.533323
[epoch19, step1571]: loss 0.505865
[epoch19, step1572]: loss 0.601051
[epoch19, step1573]: loss 0.456816
[epoch19, step1574]: loss 0.579141
[epoch19, step1575]: loss 0.376437
[epoch19, step1576]: loss 0.610313
[epoch19, step1577]: loss 0.531284
[epoch19, step1578]: loss 0.427714
[epoch19, step1579]: loss 0.633328
[epoch19, step1580]: loss 0.417646
[epoch19, step1581]: loss 0.549730
[epoch19, step1582]: loss 0.275591
[epoch19, step1583]: loss 0.557975
[epoch19, step1584]: loss 0.395022
[epoch19, step1585]: loss 0.555808
[epoch19, step1586]: loss 0.521203
[epoch19, step1587]: loss 0.529505
[epoch19, step1588]: loss 0.313882
[epoch19, step1589]: loss 0.443243
[epoch19, step1590]: loss 0.358178
[epoch19, step1591]: loss 0.466340
[epoch19, step1592]: loss 0.310326
[epoch19, step1593]: loss 0.392316
[epoch19, step1594]: loss 0.576971
[epoch19, step1595]: loss 0.485015
[epoch19, step1596]: loss 0.576197
[epoch19, step1597]: loss 0.542420
[epoch19, step1598]: loss 0.602124
[epoch19, step1599]: loss 0.562879
[epoch19, step1600]: loss 0.576753
[epoch19, step1601]: loss 0.473497
[epoch19, step1602]: loss 0.522919
[epoch19, step1603]: loss 0.509480
[epoch19, step1604]: loss 0.506123
[epoch19, step1605]: loss 0.254842
[epoch19, step1606]: loss 0.720967
[epoch19, step1607]: loss 0.335127
[epoch19, step1608]: loss 0.565590
[epoch19, step1609]: loss 0.457608
[epoch19, step1610]: loss 0.465886
[epoch19, step1611]: loss 0.357584
[epoch19, step1612]: loss 0.521568
[epoch19, step1613]: loss 0.556745
[epoch19, step1614]: loss 0.472086
[epoch19, step1615]: loss 0.303072
[epoch19, step1616]: loss 0.440110
[epoch19, step1617]: loss 0.338373
[epoch19, step1618]: loss 0.398042
[epoch19, step1619]: loss 0.797222
[epoch19, step1620]: loss 0.336047
[epoch19, step1621]: loss 0.493010
[epoch19, step1622]: loss 0.246429
[epoch19, step1623]: loss 0.427240
[epoch19, step1624]: loss 0.547042
[epoch19, step1625]: loss 0.585437
[epoch19, step1626]: loss 0.551588
[epoch19, step1627]: loss 0.416960
[epoch19, step1628]: loss 0.506840
[epoch19, step1629]: loss 0.470652
[epoch19, step1630]: loss 0.380039
[epoch19, step1631]: loss 0.295171
[epoch19, step1632]: loss 0.511107
[epoch19, step1633]: loss 0.297722
[epoch19, step1634]: loss 0.472341
[epoch19, step1635]: loss 0.348326
[epoch19, step1636]: loss 0.306084
[epoch19, step1637]: loss 0.442319
[epoch19, step1638]: loss 0.353058
[epoch19, step1639]: loss 0.465360
[epoch19, step1640]: loss 0.203223
[epoch19, step1641]: loss 0.549281
[epoch19, step1642]: loss 0.658184
[epoch19, step1643]: loss 0.543464
[epoch19, step1644]: loss 0.233954
[epoch19, step1645]: loss 0.541473
[epoch19, step1646]: loss 0.414648
[epoch19, step1647]: loss 0.366409
[epoch19, step1648]: loss 0.388695
[epoch19, step1649]: loss 0.324280
[epoch19, step1650]: loss 0.490641
[epoch19, step1651]: loss 0.570892
[epoch19, step1652]: loss 0.323542
[epoch19, step1653]: loss 0.343274
[epoch19, step1654]: loss 0.322733
[epoch19, step1655]: loss 0.396987
[epoch19, step1656]: loss 0.635432
[epoch19, step1657]: loss 0.514380
[epoch19, step1658]: loss 0.522615
[epoch19, step1659]: loss 0.305818
[epoch19, step1660]: loss 0.708241
[epoch19, step1661]: loss 0.370978
[epoch19, step1662]: loss 0.449831
[epoch19, step1663]: loss 0.407738
[epoch19, step1664]: loss 0.534699
[epoch19, step1665]: loss 0.642447
[epoch19, step1666]: loss 0.500786
[epoch19, step1667]: loss 0.544141
[epoch19, step1668]: loss 0.555755
[epoch19, step1669]: loss 0.465655
[epoch19, step1670]: loss 0.452362
[epoch19, step1671]: loss 0.221754
[epoch19, step1672]: loss 0.434927
[epoch19, step1673]: loss 0.681467
[epoch19, step1674]: loss 0.653666
[epoch19, step1675]: loss 0.476451
[epoch19, step1676]: loss 0.505394
[epoch19, step1677]: loss 0.585944
[epoch19, step1678]: loss 0.234901
[epoch19, step1679]: loss 0.541838
[epoch19, step1680]: loss 0.505937
[epoch19, step1681]: loss 0.573759
[epoch19, step1682]: loss 0.582876
[epoch19, step1683]: loss 0.545515
[epoch19, step1684]: loss 0.560974
[epoch19, step1685]: loss 0.524208
[epoch19, step1686]: loss 0.553908
[epoch19, step1687]: loss 0.435499
[epoch19, step1688]: loss 0.584322
[epoch19, step1689]: loss 0.404042
[epoch19, step1690]: loss 0.312661
[epoch19, step1691]: loss 0.606624
[epoch19, step1692]: loss 0.533527
[epoch19, step1693]: loss 0.526861
[epoch19, step1694]: loss 0.647480
[epoch19, step1695]: loss 0.480648
[epoch19, step1696]: loss 0.682399
[epoch19, step1697]: loss 0.443776
[epoch19, step1698]: loss 0.541438
[epoch19, step1699]: loss 0.582753
[epoch19, step1700]: loss 0.440679
[epoch19, step1701]: loss 0.241036
[epoch19, step1702]: loss 0.427575
[epoch19, step1703]: loss 0.529576
[epoch19, step1704]: loss 0.418655
[epoch19, step1705]: loss 0.448235
[epoch19, step1706]: loss 0.322445
[epoch19, step1707]: loss 0.414585
[epoch19, step1708]: loss 0.554128
[epoch19, step1709]: loss 0.265488
[epoch19, step1710]: loss 0.368960
[epoch19, step1711]: loss 0.446354
[epoch19, step1712]: loss 0.319282
[epoch19, step1713]: loss 0.616757
[epoch19, step1714]: loss 0.439159
[epoch19, step1715]: loss 0.639590
[epoch19, step1716]: loss 0.504416
[epoch19, step1717]: loss 0.742000
[epoch19, step1718]: loss 0.423793
[epoch19, step1719]: loss 0.461114
[epoch19, step1720]: loss 0.290089
[epoch19, step1721]: loss 0.598324
[epoch19, step1722]: loss 0.465583
[epoch19, step1723]: loss 0.362454
[epoch19, step1724]: loss 0.381284
[epoch19, step1725]: loss 0.722212
[epoch19, step1726]: loss 0.347237
[epoch19, step1727]: loss 0.391324
[epoch19, step1728]: loss 0.369321
[epoch19, step1729]: loss 0.274699
[epoch19, step1730]: loss 0.345947
[epoch19, step1731]: loss 0.428829
[epoch19, step1732]: loss 0.442045
[epoch19, step1733]: loss 0.434240
[epoch19, step1734]: loss 0.646457
[epoch19, step1735]: loss 0.448922
[epoch19, step1736]: loss 0.248517
[epoch19, step1737]: loss 0.481753
[epoch19, step1738]: loss 0.250468
[epoch19, step1739]: loss 0.320235
[epoch19, step1740]: loss 0.408610
[epoch19, step1741]: loss 0.672252
[epoch19, step1742]: loss 0.628746
[epoch19, step1743]: loss 0.411773
[epoch19, step1744]: loss 0.441509
[epoch19, step1745]: loss 0.507174
[epoch19, step1746]: loss 0.366584
[epoch19, step1747]: loss 0.509680
[epoch19, step1748]: loss 0.548360
[epoch19, step1749]: loss 0.598514
[epoch19, step1750]: loss 0.461440
[epoch19, step1751]: loss 0.343499
[epoch19, step1752]: loss 0.351411
[epoch19, step1753]: loss 0.549127
[epoch19, step1754]: loss 0.625687
[epoch19, step1755]: loss 0.516752
[epoch19, step1756]: loss 0.606902
[epoch19, step1757]: loss 0.569624
[epoch19, step1758]: loss 0.563736
[epoch19, step1759]: loss 0.571360
[epoch19, step1760]: loss 0.622346
[epoch19, step1761]: loss 0.508708
[epoch19, step1762]: loss 0.459701
[epoch19, step1763]: loss 0.501235
[epoch19, step1764]: loss 0.386835
[epoch19, step1765]: loss 0.492291
[epoch19, step1766]: loss 0.564845
[epoch19, step1767]: loss 0.417684
[epoch19, step1768]: loss 0.561333
[epoch19, step1769]: loss 0.340013
[epoch19, step1770]: loss 0.558132
[epoch19, step1771]: loss 0.553444
[epoch19, step1772]: loss 0.492997
[epoch19, step1773]: loss 0.529935
[epoch19, step1774]: loss 0.479699
[epoch19, step1775]: loss 0.534444
[epoch19, step1776]: loss 0.403105
[epoch19, step1777]: loss 0.383711
[epoch19, step1778]: loss 0.651322
[epoch19, step1779]: loss 0.529947
[epoch19, step1780]: loss 0.595150
[epoch19, step1781]: loss 0.429180
[epoch19, step1782]: loss 0.559494
[epoch19, step1783]: loss 0.516488
[epoch19, step1784]: loss 0.410574
[epoch19, step1785]: loss 0.608954
[epoch19, step1786]: loss 0.437454
[epoch19, step1787]: loss 0.608644
[epoch19, step1788]: loss 0.718138
[epoch19, step1789]: loss 0.398288
[epoch19, step1790]: loss 0.466889
[epoch19, step1791]: loss 0.617719
[epoch19, step1792]: loss 0.231422
[epoch19, step1793]: loss 0.702752
[epoch19, step1794]: loss 0.496369
[epoch19, step1795]: loss 0.413448
[epoch19, step1796]: loss 0.364538
[epoch19, step1797]: loss 0.437791
[epoch19, step1798]: loss 0.267157
[epoch19, step1799]: loss 0.434038
[epoch19, step1800]: loss 0.777177
[epoch19, step1801]: loss 0.564487
[epoch19, step1802]: loss 0.465185
[epoch19, step1803]: loss 0.541720
[epoch19, step1804]: loss 0.439104
[epoch19, step1805]: loss 0.587481
[epoch19, step1806]: loss 0.577394
[epoch19, step1807]: loss 0.735215
[epoch19, step1808]: loss 0.430792
[epoch19, step1809]: loss 0.284242
[epoch19, step1810]: loss 0.549961
[epoch19, step1811]: loss 0.308879
[epoch19, step1812]: loss 0.489322
[epoch19, step1813]: loss 0.251506
[epoch19, step1814]: loss 0.562550
[epoch19, step1815]: loss 0.274651
[epoch19, step1816]: loss 0.486298
[epoch19, step1817]: loss 0.372970
[epoch19, step1818]: loss 0.350730
[epoch19, step1819]: loss 0.244497
[epoch19, step1820]: loss 0.521063
[epoch19, step1821]: loss 0.524732
[epoch19, step1822]: loss 0.675736
[epoch19, step1823]: loss 0.647145
[epoch19, step1824]: loss 0.444783
[epoch19, step1825]: loss 0.436042
[epoch19, step1826]: loss 0.395463
[epoch19, step1827]: loss 0.362567
[epoch19, step1828]: loss 0.341651
[epoch19, step1829]: loss 0.277288
[epoch19, step1830]: loss 0.361991
[epoch19, step1831]: loss 0.488251
[epoch19, step1832]: loss 0.624814
[epoch19, step1833]: loss 0.326837
[epoch19, step1834]: loss 0.554135
[epoch19, step1835]: loss 0.398269
[epoch19, step1836]: loss 0.501009
[epoch19, step1837]: loss 0.376540
[epoch19, step1838]: loss 0.612420
[epoch19, step1839]: loss 0.329594
[epoch19, step1840]: loss 0.480370
[epoch19, step1841]: loss 0.326421
[epoch19, step1842]: loss 0.771701
[epoch19, step1843]: loss 0.430425
[epoch19, step1844]: loss 0.343224
[epoch19, step1845]: loss 0.236019
[epoch19, step1846]: loss 0.462604
[epoch19, step1847]: loss 0.388337
[epoch19, step1848]: loss 0.571285
[epoch19, step1849]: loss 0.415474
[epoch19, step1850]: loss 0.308514
[epoch19, step1851]: loss 0.473502
[epoch19, step1852]: loss 0.350146
[epoch19, step1853]: loss 0.400788
[epoch19, step1854]: loss 0.532542
[epoch19, step1855]: loss 0.456934
[epoch19, step1856]: loss 0.458743
[epoch19, step1857]: loss 0.284609
[epoch19, step1858]: loss 0.417360
[epoch19, step1859]: loss 0.409303
[epoch19, step1860]: loss 0.361934
[epoch19, step1861]: loss 0.610913
[epoch19, step1862]: loss 0.387763
[epoch19, step1863]: loss 0.534530
[epoch19, step1864]: loss 0.375421
[epoch19, step1865]: loss 0.531852
[epoch19, step1866]: loss 0.519526
[epoch19, step1867]: loss 0.720459
[epoch19, step1868]: loss 0.152054
[epoch19, step1869]: loss 0.350056
[epoch19, step1870]: loss 0.292881
[epoch19, step1871]: loss 0.434725
[epoch19, step1872]: loss 0.287686
[epoch19, step1873]: loss 0.254079
[epoch19, step1874]: loss 0.779395
[epoch19, step1875]: loss 0.339536
[epoch19, step1876]: loss 0.575823
[epoch19, step1877]: loss 0.094774
[epoch19, step1878]: loss 0.372278
[epoch19, step1879]: loss 0.575646
[epoch19, step1880]: loss 0.535444
[epoch19, step1881]: loss 0.301901
[epoch19, step1882]: loss 0.516201
[epoch19, step1883]: loss 0.269959
[epoch19, step1884]: loss 0.703603
[epoch19, step1885]: loss 0.708605
[epoch19, step1886]: loss 0.597669
[epoch19, step1887]: loss 0.420640
[epoch19, step1888]: loss 0.426843
[epoch19, step1889]: loss 0.473793
[epoch19, step1890]: loss 0.519738
[epoch19, step1891]: loss 0.465706
[epoch19, step1892]: loss 0.474005
[epoch19, step1893]: loss 0.599533
[epoch19, step1894]: loss 0.568090
[epoch19, step1895]: loss 0.479429
[epoch19, step1896]: loss 0.625176
[epoch19, step1897]: loss 0.365593
[epoch19, step1898]: loss 0.604946
[epoch19, step1899]: loss 0.329483
[epoch19, step1900]: loss 0.503318
[epoch19, step1901]: loss 0.369573
[epoch19, step1902]: loss 0.697948
[epoch19, step1903]: loss 0.572817
[epoch19, step1904]: loss 0.530345
[epoch19, step1905]: loss 0.339512
[epoch19, step1906]: loss 0.393053
[epoch19, step1907]: loss 0.562412
[epoch19, step1908]: loss 0.500493
[epoch19, step1909]: loss 0.629886
[epoch19, step1910]: loss 0.458411
[epoch19, step1911]: loss 0.467006
[epoch19, step1912]: loss 0.385230
[epoch19, step1913]: loss 0.401432
[epoch19, step1914]: loss 0.459279
[epoch19, step1915]: loss 0.558774
[epoch19, step1916]: loss 0.334366
[epoch19, step1917]: loss 0.582019
[epoch19, step1918]: loss 0.519396
[epoch19, step1919]: loss 0.389234
[epoch19, step1920]: loss 0.544555
[epoch19, step1921]: loss 0.375445
[epoch19, step1922]: loss 0.606623
[epoch19, step1923]: loss 0.323887
[epoch19, step1924]: loss 0.613077
[epoch19, step1925]: loss 0.528432
[epoch19, step1926]: loss 0.358561
[epoch19, step1927]: loss 0.456203
[epoch19, step1928]: loss 0.348977
[epoch19, step1929]: loss 0.350194
[epoch19, step1930]: loss 0.379019
[epoch19, step1931]: loss 0.332497
[epoch19, step1932]: loss 0.492027
[epoch19, step1933]: loss 0.297969
[epoch19, step1934]: loss 0.522328
[epoch19, step1935]: loss 0.152760
[epoch19, step1936]: loss 0.423289
[epoch19, step1937]: loss 0.540226
[epoch19, step1938]: loss 0.410073
[epoch19, step1939]: loss 0.490766
[epoch19, step1940]: loss 0.456871
[epoch19, step1941]: loss 0.461019
[epoch19, step1942]: loss 0.571940
[epoch19, step1943]: loss 0.624975
[epoch19, step1944]: loss 0.789964
[epoch19, step1945]: loss 0.194241
[epoch19, step1946]: loss 0.389722
[epoch19, step1947]: loss 0.335103
[epoch19, step1948]: loss 0.554652
[epoch19, step1949]: loss 0.417527
[epoch19, step1950]: loss 0.627510
[epoch19, step1951]: loss 0.604467
[epoch19, step1952]: loss 0.496346
[epoch19, step1953]: loss 0.452880
[epoch19, step1954]: loss 0.452614
[epoch19, step1955]: loss 0.401626
[epoch19, step1956]: loss 0.457797
[epoch19, step1957]: loss 0.327432
[epoch19, step1958]: loss 0.474858
[epoch19, step1959]: loss 0.450583
[epoch19, step1960]: loss 0.408207
[epoch19, step1961]: loss 0.571032
[epoch19, step1962]: loss 0.584619
[epoch19, step1963]: loss 0.609435
[epoch19, step1964]: loss 0.577696
[epoch19, step1965]: loss 0.623969
[epoch19, step1966]: loss 0.364879
[epoch19, step1967]: loss 0.440680
[epoch19, step1968]: loss 0.559918
[epoch19, step1969]: loss 0.472221
[epoch19, step1970]: loss 0.587277
[epoch19, step1971]: loss 0.566887
[epoch19, step1972]: loss 0.513891
[epoch19, step1973]: loss 0.576039
[epoch19, step1974]: loss 0.594838
[epoch19, step1975]: loss 0.414496
[epoch19, step1976]: loss 0.463067
[epoch19, step1977]: loss 0.595454
[epoch19, step1978]: loss 0.440204
[epoch19, step1979]: loss 0.428562
[epoch19, step1980]: loss 0.252413
[epoch19, step1981]: loss 0.638837
[epoch19, step1982]: loss 0.395309
[epoch19, step1983]: loss 0.272030
[epoch19, step1984]: loss 0.597030
[epoch19, step1985]: loss 0.674904
[epoch19, step1986]: loss 0.587904
[epoch19, step1987]: loss 0.216259
[epoch19, step1988]: loss 0.405872
[epoch19, step1989]: loss 0.479726
[epoch19, step1990]: loss 0.280483
[epoch19, step1991]: loss 0.409505
[epoch19, step1992]: loss 0.232729
[epoch19, step1993]: loss 0.409247
[epoch19, step1994]: loss 0.429596
[epoch19, step1995]: loss 0.357647
[epoch19, step1996]: loss 0.673597
[epoch19, step1997]: loss 0.517250
[epoch19, step1998]: loss 0.440681
[epoch19, step1999]: loss 0.708992
[epoch19, step2000]: loss 0.678588
[epoch19, step2001]: loss 0.461826
[epoch19, step2002]: loss 0.299912
[epoch19, step2003]: loss 0.408467
[epoch19, step2004]: loss 0.385602
[epoch19, step2005]: loss 0.687363
[epoch19, step2006]: loss 0.501802
[epoch19, step2007]: loss 0.489701
[epoch19, step2008]: loss 0.521671
[epoch19, step2009]: loss 0.520097
[epoch19, step2010]: loss 0.614176
[epoch19, step2011]: loss 0.436410
[epoch19, step2012]: loss 0.232558
[epoch19, step2013]: loss 0.545036
[epoch19, step2014]: loss 0.212472
[epoch19, step2015]: loss 0.345179
[epoch19, step2016]: loss 0.782115
[epoch19, step2017]: loss 0.276791
[epoch19, step2018]: loss 0.448524
[epoch19, step2019]: loss 0.421562
[epoch19, step2020]: loss 0.242737
[epoch19, step2021]: loss 0.666763
[epoch19, step2022]: loss 0.248113
[epoch19, step2023]: loss 0.647232
[epoch19, step2024]: loss 0.362395
[epoch19, step2025]: loss 0.439081
[epoch19, step2026]: loss 0.581147
[epoch19, step2027]: loss 0.592299
[epoch19, step2028]: loss 0.626116
[epoch19, step2029]: loss 0.403349
[epoch19, step2030]: loss 0.394385
[epoch19, step2031]: loss 0.555931
[epoch19, step2032]: loss 0.431524
[epoch19, step2033]: loss 0.524048
[epoch19, step2034]: loss 0.586078
[epoch19, step2035]: loss 0.743629
[epoch19, step2036]: loss 0.555631
[epoch19, step2037]: loss 0.319879
[epoch19, step2038]: loss 0.270449
[epoch19, step2039]: loss 0.484187
[epoch19, step2040]: loss 0.530302
[epoch19, step2041]: loss 0.406726
[epoch19, step2042]: loss 0.386381
[epoch19, step2043]: loss 0.435973
[epoch19, step2044]: loss 0.537221
[epoch19, step2045]: loss 0.281657
[epoch19, step2046]: loss 0.395606
[epoch19, step2047]: loss 0.355442
[epoch19, step2048]: loss 0.372397
[epoch19, step2049]: loss 0.620967
[epoch19, step2050]: loss 0.482697
[epoch19, step2051]: loss 0.541672
[epoch19, step2052]: loss 0.334824
[epoch19, step2053]: loss 0.382354
[epoch19, step2054]: loss 0.453879
[epoch19, step2055]: loss 0.633881
[epoch19, step2056]: loss 0.407880
[epoch19, step2057]: loss 0.494522
[epoch19, step2058]: loss 0.242274
[epoch19, step2059]: loss 0.488157
[epoch19, step2060]: loss 0.286570
[epoch19, step2061]: loss 0.345703
[epoch19, step2062]: loss 0.499792
[epoch19, step2063]: loss 0.458890
[epoch19, step2064]: loss 0.370007
[epoch19, step2065]: loss 0.563173
[epoch19, step2066]: loss 0.526831
[epoch19, step2067]: loss 0.547661
[epoch19, step2068]: loss 0.590565
[epoch19, step2069]: loss 0.480217
[epoch19, step2070]: loss 0.531571
[epoch19, step2071]: loss 0.637754
[epoch19, step2072]: loss 0.357375
[epoch19, step2073]: loss 0.466176
[epoch19, step2074]: loss 0.435612
[epoch19, step2075]: loss 0.246536
[epoch19, step2076]: loss 0.208256
[epoch19, step2077]: loss 0.492327
[epoch19, step2078]: loss 0.419902
[epoch19, step2079]: loss 0.332793
[epoch19, step2080]: loss 0.411222
[epoch19, step2081]: loss 0.545129
[epoch19, step2082]: loss 0.540677
[epoch19, step2083]: loss 0.450411
[epoch19, step2084]: loss 0.379323
[epoch19, step2085]: loss 0.481757
[epoch19, step2086]: loss 0.580275
[epoch19, step2087]: loss 0.428153
[epoch19, step2088]: loss 0.422180
[epoch19, step2089]: loss 0.273229
[epoch19, step2090]: loss 0.413200
[epoch19, step2091]: loss 0.157354
[epoch19, step2092]: loss 0.374740
[epoch19, step2093]: loss 0.477340
[epoch19, step2094]: loss 0.470296
[epoch19, step2095]: loss 0.398648
[epoch19, step2096]: loss 0.417936
[epoch19, step2097]: loss 0.563090
[epoch19, step2098]: loss 0.629233
[epoch19, step2099]: loss 0.464355
[epoch19, step2100]: loss 0.596760
[epoch19, step2101]: loss 0.491223
[epoch19, step2102]: loss 0.565294
[epoch19, step2103]: loss 0.457205
[epoch19, step2104]: loss 0.359219
[epoch19, step2105]: loss 0.341396
[epoch19, step2106]: loss 0.362165
[epoch19, step2107]: loss 0.474657
[epoch19, step2108]: loss 0.769292
[epoch19, step2109]: loss 0.466943
[epoch19, step2110]: loss 0.517969
[epoch19, step2111]: loss 0.460198
[epoch19, step2112]: loss 0.476236
[epoch19, step2113]: loss 0.506703
[epoch19, step2114]: loss 0.604006
[epoch19, step2115]: loss 0.512371
[epoch19, step2116]: loss 0.356569
[epoch19, step2117]: loss 0.409254
[epoch19, step2118]: loss 0.542616
[epoch19, step2119]: loss 0.542630
[epoch19, step2120]: loss 0.459146
[epoch19, step2121]: loss 0.650033
[epoch19, step2122]: loss 0.514998
[epoch19, step2123]: loss 0.407719
[epoch19, step2124]: loss 0.552754
[epoch19, step2125]: loss 0.196068
[epoch19, step2126]: loss 0.667240
[epoch19, step2127]: loss 0.165877
[epoch19, step2128]: loss 0.262775
[epoch19, step2129]: loss 0.367263
[epoch19, step2130]: loss 0.311162
[epoch19, step2131]: loss 0.711565
[epoch19, step2132]: loss 0.597973
[epoch19, step2133]: loss 0.495968
[epoch19, step2134]: loss 0.421819
[epoch19, step2135]: loss 0.486035
[epoch19, step2136]: loss 0.512234
[epoch19, step2137]: loss 0.386988
[epoch19, step2138]: loss 0.432140
[epoch19, step2139]: loss 0.399883
[epoch19, step2140]: loss 0.423515
[epoch19, step2141]: loss 0.476348
[epoch19, step2142]: loss 0.449149
[epoch19, step2143]: loss 0.367481
[epoch19, step2144]: loss 0.475392
[epoch19, step2145]: loss 0.652886
[epoch19, step2146]: loss 0.326175
[epoch19, step2147]: loss 0.504849
[epoch19, step2148]: loss 0.427961
[epoch19, step2149]: loss 0.644898
[epoch19, step2150]: loss 0.472516
[epoch19, step2151]: loss 0.528212
[epoch19, step2152]: loss 0.558322
[epoch19, step2153]: loss 0.612114
[epoch19, step2154]: loss 0.485545
[epoch19, step2155]: loss 0.565644
[epoch19, step2156]: loss 0.363620
[epoch19, step2157]: loss 0.567177
[epoch19, step2158]: loss 0.430413
[epoch19, step2159]: loss 0.473317
[epoch19, step2160]: loss 0.521882
[epoch19, step2161]: loss 0.426777
[epoch19, step2162]: loss 0.533285
[epoch19, step2163]: loss 0.732116
[epoch19, step2164]: loss 0.325440
[epoch19, step2165]: loss 0.443996
[epoch19, step2166]: loss 0.360563
[epoch19, step2167]: loss 0.635671
[epoch19, step2168]: loss 0.408961
[epoch19, step2169]: loss 0.487987
[epoch19, step2170]: loss 0.519525
[epoch19, step2171]: loss 0.354994
[epoch19, step2172]: loss 0.582020
[epoch19, step2173]: loss 0.503662
[epoch19, step2174]: loss 0.620241
[epoch19, step2175]: loss 0.136563
[epoch19, step2176]: loss 0.328696
[epoch19, step2177]: loss 0.580732
[epoch19, step2178]: loss 0.428249
[epoch19, step2179]: loss 0.354421
[epoch19, step2180]: loss 0.228411
[epoch19, step2181]: loss 0.536255
[epoch19, step2182]: loss 0.314429
[epoch19, step2183]: loss 0.436477
[epoch19, step2184]: loss 0.300229
[epoch19, step2185]: loss 0.587432
[epoch19, step2186]: loss 0.481712
[epoch19, step2187]: loss 0.373585
[epoch19, step2188]: loss 0.313302
[epoch19, step2189]: loss 0.328205
[epoch19, step2190]: loss 0.379782
[epoch19, step2191]: loss 0.450639
[epoch19, step2192]: loss 0.638885
[epoch19, step2193]: loss 0.686916
[epoch19, step2194]: loss 0.298175
[epoch19, step2195]: loss 0.612974
[epoch19, step2196]: loss 0.380647
[epoch19, step2197]: loss 0.430637
[epoch19, step2198]: loss 0.510433
[epoch19, step2199]: loss 0.695734
[epoch19, step2200]: loss 0.433422
[epoch19, step2201]: loss 0.267266
[epoch19, step2202]: loss 0.512060
[epoch19, step2203]: loss 0.466873
[epoch19, step2204]: loss 0.308355
[epoch19, step2205]: loss 0.666401
[epoch19, step2206]: loss 0.588899
[epoch19, step2207]: loss 0.521635
[epoch19, step2208]: loss 0.536645
[epoch19, step2209]: loss 0.414978
[epoch19, step2210]: loss 0.639083
[epoch19, step2211]: loss 0.465000
[epoch19, step2212]: loss 0.600820
[epoch19, step2213]: loss 0.378013
[epoch19, step2214]: loss 0.470584
[epoch19, step2215]: loss 0.325883
[epoch19, step2216]: loss 0.329854
[epoch19, step2217]: loss 0.454218
[epoch19, step2218]: loss 0.298664
[epoch19, step2219]: loss 0.513561
[epoch19, step2220]: loss 0.373232
[epoch19, step2221]: loss 0.682385
[epoch19, step2222]: loss 0.428175
[epoch19, step2223]: loss 0.669995
[epoch19, step2224]: loss 0.574695
[epoch19, step2225]: loss 0.525608
[epoch19, step2226]: loss 0.340586
[epoch19, step2227]: loss 0.236527
[epoch19, step2228]: loss 0.529263
[epoch19, step2229]: loss 0.307397
[epoch19, step2230]: loss 0.285837
[epoch19, step2231]: loss 0.495227
[epoch19, step2232]: loss 0.537809
[epoch19, step2233]: loss 0.519499
[epoch19, step2234]: loss 0.398041
[epoch19, step2235]: loss 0.515682
[epoch19, step2236]: loss 0.604604
[epoch19, step2237]: loss 0.351835
[epoch19, step2238]: loss 0.391197
[epoch19, step2239]: loss 0.567759
[epoch19, step2240]: loss 0.437182
[epoch19, step2241]: loss 0.504527
[epoch19, step2242]: loss 0.737255
[epoch19, step2243]: loss 0.433214
[epoch19, step2244]: loss 0.669793
[epoch19, step2245]: loss 0.389139
[epoch19, step2246]: loss 0.593682
[epoch19, step2247]: loss 0.492854
[epoch19, step2248]: loss 0.527237
[epoch19, step2249]: loss 0.331918
[epoch19, step2250]: loss 0.235267
[epoch19, step2251]: loss 0.266548
[epoch19, step2252]: loss 0.481691
[epoch19, step2253]: loss 0.464860
[epoch19, step2254]: loss 0.432140
[epoch19, step2255]: loss 0.241563
[epoch19, step2256]: loss 0.558251
[epoch19, step2257]: loss 0.425631
[epoch19, step2258]: loss 0.608432
[epoch19, step2259]: loss 0.440151
[epoch19, step2260]: loss 0.484064
[epoch19, step2261]: loss 0.433650
[epoch19, step2262]: loss 0.483987
[epoch19, step2263]: loss 0.292516
[epoch19, step2264]: loss 0.407252
[epoch19, step2265]: loss 0.448679
[epoch19, step2266]: loss 0.366641
[epoch19, step2267]: loss 0.479085
[epoch19, step2268]: loss 0.648682
[epoch19, step2269]: loss 0.426529
[epoch19, step2270]: loss 0.488060
[epoch19, step2271]: loss 0.428745
[epoch19, step2272]: loss 0.476234
[epoch19, step2273]: loss 0.517552
[epoch19, step2274]: loss 0.627352
[epoch19, step2275]: loss 0.499059
[epoch19, step2276]: loss 0.578639
[epoch19, step2277]: loss 0.317759
[epoch19, step2278]: loss 0.405258
[epoch19, step2279]: loss 0.410396
[epoch19, step2280]: loss 0.514823
[epoch19, step2281]: loss 0.268864
[epoch19, step2282]: loss 0.424882
[epoch19, step2283]: loss 0.351515
[epoch19, step2284]: loss 0.382594
[epoch19, step2285]: loss 0.544577
[epoch19, step2286]: loss 0.355791
[epoch19, step2287]: loss 0.435248
[epoch19, step2288]: loss 0.284666
[epoch19, step2289]: loss 0.441249
[epoch19, step2290]: loss 0.410963
[epoch19, step2291]: loss 0.536135
[epoch19, step2292]: loss 0.531749
[epoch19, step2293]: loss 0.517553
[epoch19, step2294]: loss 0.250157
[epoch19, step2295]: loss 0.267542
[epoch19, step2296]: loss 0.631488
[epoch19, step2297]: loss 0.438179
[epoch19, step2298]: loss 0.482397
[epoch19, step2299]: loss 0.425466
[epoch19, step2300]: loss 0.648636
[epoch19, step2301]: loss 0.307041
[epoch19, step2302]: loss 0.487039
[epoch19, step2303]: loss 0.344603
[epoch19, step2304]: loss 0.601258
[epoch19, step2305]: loss 0.650926
[epoch19, step2306]: loss 0.414961
[epoch19, step2307]: loss 0.535069
[epoch19, step2308]: loss 0.488077
[epoch19, step2309]: loss 0.634190
[epoch19, step2310]: loss 0.584709
[epoch19, step2311]: loss 0.419317
[epoch19, step2312]: loss 0.399409
[epoch19, step2313]: loss 0.607210
[epoch19, step2314]: loss 0.506033
[epoch19, step2315]: loss 0.529030
[epoch19, step2316]: loss 0.361567
[epoch19, step2317]: loss 0.276423
[epoch19, step2318]: loss 0.275487
[epoch19, step2319]: loss 0.587584
[epoch19, step2320]: loss 0.409519
[epoch19, step2321]: loss 0.531785
[epoch19, step2322]: loss 0.577825
[epoch19, step2323]: loss 0.297655
[epoch19, step2324]: loss 0.442111
[epoch19, step2325]: loss 0.389952
[epoch19, step2326]: loss 0.462313
[epoch19, step2327]: loss 0.415959
[epoch19, step2328]: loss 0.645568
[epoch19, step2329]: loss 0.416257
[epoch19, step2330]: loss 0.594934
[epoch19, step2331]: loss 0.575764
[epoch19, step2332]: loss 0.557351
[epoch19, step2333]: loss 0.394291
[epoch19, step2334]: loss 0.343457
[epoch19, step2335]: loss 0.336702
[epoch19, step2336]: loss 0.465257
[epoch19, step2337]: loss 0.612806
[epoch19, step2338]: loss 0.270278
[epoch19, step2339]: loss 0.654534
[epoch19, step2340]: loss 0.399115
[epoch19, step2341]: loss 0.418984
[epoch19, step2342]: loss 0.423487
[epoch19, step2343]: loss 0.521320
[epoch19, step2344]: loss 0.440784
[epoch19, step2345]: loss 0.488009
[epoch19, step2346]: loss 0.460601
[epoch19, step2347]: loss 0.409916
[epoch19, step2348]: loss 0.506657
[epoch19, step2349]: loss 0.475221
[epoch19, step2350]: loss 0.172947
[epoch19, step2351]: loss 0.346712
[epoch19, step2352]: loss 0.521562
[epoch19, step2353]: loss 0.474619
[epoch19, step2354]: loss 0.611177
[epoch19, step2355]: loss 0.491394
[epoch19, step2356]: loss 0.309364
[epoch19, step2357]: loss 0.419865
[epoch19, step2358]: loss 0.726460
[epoch19, step2359]: loss 0.613768
[epoch19, step2360]: loss 0.318828
[epoch19, step2361]: loss 0.678556
[epoch19, step2362]: loss 0.343134
[epoch19, step2363]: loss 0.539702
[epoch19, step2364]: loss 0.558812
[epoch19, step2365]: loss 0.558465
[epoch19, step2366]: loss 0.426191
[epoch19, step2367]: loss 0.311591
[epoch19, step2368]: loss 0.436845
[epoch19, step2369]: loss 0.288729
[epoch19, step2370]: loss 0.518665
[epoch19, step2371]: loss 0.632807
[epoch19, step2372]: loss 0.526108
[epoch19, step2373]: loss 0.367444
[epoch19, step2374]: loss 0.509658
[epoch19, step2375]: loss 0.489159
[epoch19, step2376]: loss 0.419150
[epoch19, step2377]: loss 0.488976
[epoch19, step2378]: loss 0.492078
[epoch19, step2379]: loss 0.292987
[epoch19, step2380]: loss 0.554699
[epoch19, step2381]: loss 0.395055
[epoch19, step2382]: loss 0.533747
[epoch19, step2383]: loss 0.507738
[epoch19, step2384]: loss 0.480221
[epoch19, step2385]: loss 0.558720
[epoch19, step2386]: loss 0.516600
[epoch19, step2387]: loss 0.349968
[epoch19, step2388]: loss 0.611037
[epoch19, step2389]: loss 0.566434
[epoch19, step2390]: loss 0.666156
[epoch19, step2391]: loss 0.252188
[epoch19, step2392]: loss 0.503185
[epoch19, step2393]: loss 0.536079
[epoch19, step2394]: loss 0.469859
[epoch19, step2395]: loss 0.382823
[epoch19, step2396]: loss 0.411077
[epoch19, step2397]: loss 0.443576
[epoch19, step2398]: loss 0.113971
[epoch19, step2399]: loss 0.400425
[epoch19, step2400]: loss 0.505914
[epoch19, step2401]: loss 0.631221
[epoch19, step2402]: loss 0.453638
[epoch19, step2403]: loss 0.339975
[epoch19, step2404]: loss 0.367187
[epoch19, step2405]: loss 0.536672
[epoch19, step2406]: loss 0.343592
[epoch19, step2407]: loss 0.487118
[epoch19, step2408]: loss 0.396372
[epoch19, step2409]: loss 0.521407
[epoch19, step2410]: loss 0.230658
[epoch19, step2411]: loss 0.503865
[epoch19, step2412]: loss 0.414165
[epoch19, step2413]: loss 0.533674
[epoch19, step2414]: loss 0.489405
[epoch19, step2415]: loss 0.639328
[epoch19, step2416]: loss 0.466292
[epoch19, step2417]: loss 0.274469
[epoch19, step2418]: loss 0.587846
[epoch19, step2419]: loss 0.549523
[epoch19, step2420]: loss 0.540943
[epoch19, step2421]: loss 0.414645
[epoch19, step2422]: loss 0.394870
[epoch19, step2423]: loss 0.387893
[epoch19, step2424]: loss 0.406517
[epoch19, step2425]: loss 0.336195
[epoch19, step2426]: loss 0.440616
[epoch19, step2427]: loss 0.565431
[epoch19, step2428]: loss 0.564065
[epoch19, step2429]: loss 0.532475
[epoch19, step2430]: loss 0.501673
[epoch19, step2431]: loss 0.242872
[epoch19, step2432]: loss 0.363037
[epoch19, step2433]: loss 0.277448
[epoch19, step2434]: loss 0.438670
[epoch19, step2435]: loss 0.349422
[epoch19, step2436]: loss 0.480666
[epoch19, step2437]: loss 0.514721
[epoch19, step2438]: loss 0.425522
[epoch19, step2439]: loss 0.370526
[epoch19, step2440]: loss 0.518975
[epoch19, step2441]: loss 0.450114
[epoch19, step2442]: loss 0.600408
[epoch19, step2443]: loss 0.537534
[epoch19, step2444]: loss 0.273402
[epoch19, step2445]: loss 0.234295
[epoch19, step2446]: loss 0.431385
[epoch19, step2447]: loss 0.768011
[epoch19, step2448]: loss 0.720127
[epoch19, step2449]: loss 0.250694
[epoch19, step2450]: loss 0.443379
[epoch19, step2451]: loss 0.458204
[epoch19, step2452]: loss 0.489931
[epoch19, step2453]: loss 0.500595
[epoch19, step2454]: loss 0.474528
[epoch19, step2455]: loss 0.234674
[epoch19, step2456]: loss 0.488370
[epoch19, step2457]: loss 0.466191
[epoch19, step2458]: loss 0.240065
[epoch19, step2459]: loss 0.621190
[epoch19, step2460]: loss 0.513660
[epoch19, step2461]: loss 0.423683
[epoch19, step2462]: loss 0.562690
[epoch19, step2463]: loss 0.654559
[epoch19, step2464]: loss 0.482049
[epoch19, step2465]: loss 0.365998
[epoch19, step2466]: loss 0.506863
[epoch19, step2467]: loss 0.421834
[epoch19, step2468]: loss 0.464376
[epoch19, step2469]: loss 0.443671
[epoch19, step2470]: loss 0.338824
[epoch19, step2471]: loss 0.580652
[epoch19, step2472]: loss 0.359999
[epoch19, step2473]: loss 0.405113
[epoch19, step2474]: loss 0.584996
[epoch19, step2475]: loss 0.304663
[epoch19, step2476]: loss 0.441352
[epoch19, step2477]: loss 0.601234
[epoch19, step2478]: loss 0.643581
[epoch19, step2479]: loss 0.543046
[epoch19, step2480]: loss 0.503975
[epoch19, step2481]: loss 0.609634
[epoch19, step2482]: loss 0.428283
[epoch19, step2483]: loss 0.479594
[epoch19, step2484]: loss 0.528212
[epoch19, step2485]: loss 0.450632
[epoch19, step2486]: loss 0.363783
[epoch19, step2487]: loss 0.376180
[epoch19, step2488]: loss 0.601098
[epoch19, step2489]: loss 0.538387
[epoch19, step2490]: loss 0.510973
[epoch19, step2491]: loss 0.677010
[epoch19, step2492]: loss 0.462788
[epoch19, step2493]: loss 0.537672
[epoch19, step2494]: loss 0.395976
[epoch19, step2495]: loss 0.533606
[epoch19, step2496]: loss 0.592040
[epoch19, step2497]: loss 0.447844
[epoch19, step2498]: loss 0.432822
[epoch19, step2499]: loss 0.633399
[epoch19, step2500]: loss 0.240648
[epoch19, step2501]: loss 0.387432
[epoch19, step2502]: loss 0.587607
[epoch19, step2503]: loss 0.541424
[epoch19, step2504]: loss 0.356407
[epoch19, step2505]: loss 0.420684
[epoch19, step2506]: loss 0.561424
[epoch19, step2507]: loss 0.496855
[epoch19, step2508]: loss 0.397811
[epoch19, step2509]: loss 0.409229
[epoch19, step2510]: loss 0.552278
[epoch19, step2511]: loss 0.497203
[epoch19, step2512]: loss 0.466086
[epoch19, step2513]: loss 0.331086
[epoch19, step2514]: loss 0.555855
[epoch19, step2515]: loss 0.435447
[epoch19, step2516]: loss 0.212829
[epoch19, step2517]: loss 0.609815
[epoch19, step2518]: loss 0.345702
[epoch19, step2519]: loss 0.394825
[epoch19, step2520]: loss 0.484961
[epoch19, step2521]: loss 0.590529
[epoch19, step2522]: loss 0.532659
[epoch19, step2523]: loss 0.315195
[epoch19, step2524]: loss 0.488302
[epoch19, step2525]: loss 0.602249
[epoch19, step2526]: loss 0.339707
[epoch19, step2527]: loss 0.671447
[epoch19, step2528]: loss 0.441574
[epoch19, step2529]: loss 0.479382
[epoch19, step2530]: loss 0.368823
[epoch19, step2531]: loss 0.265108
[epoch19, step2532]: loss 0.467565
[epoch19, step2533]: loss 0.543241
[epoch19, step2534]: loss 0.387120
[epoch19, step2535]: loss 0.531969
[epoch19, step2536]: loss 0.484150
[epoch19, step2537]: loss 0.646627
[epoch19, step2538]: loss 0.431325
[epoch19, step2539]: loss 0.446292
[epoch19, step2540]: loss 0.499489
[epoch19, step2541]: loss 0.568783
[epoch19, step2542]: loss 0.242405
[epoch19, step2543]: loss 0.526256
[epoch19, step2544]: loss 0.450173
[epoch19, step2545]: loss 0.755972
[epoch19, step2546]: loss 0.418474
[epoch19, step2547]: loss 0.458744
[epoch19, step2548]: loss 0.482477
[epoch19, step2549]: loss 0.172748
[epoch19, step2550]: loss 0.319918
[epoch19, step2551]: loss 0.437299
[epoch19, step2552]: loss 0.464582
[epoch19, step2553]: loss 0.777706
[epoch19, step2554]: loss 0.346739
[epoch19, step2555]: loss 0.546719
[epoch19, step2556]: loss 0.637853
[epoch19, step2557]: loss 0.346537
[epoch19, step2558]: loss 0.589371
[epoch19, step2559]: loss 0.421663
[epoch19, step2560]: loss 0.291491
[epoch19, step2561]: loss 0.505510
[epoch19, step2562]: loss 0.334693
[epoch19, step2563]: loss 0.659101
[epoch19, step2564]: loss 0.455053
[epoch19, step2565]: loss 0.597830
[epoch19, step2566]: loss 0.326796
[epoch19, step2567]: loss 0.660005
[epoch19, step2568]: loss 0.358288
[epoch19, step2569]: loss 0.348525
[epoch19, step2570]: loss 0.549635
[epoch19, step2571]: loss 0.422989
[epoch19, step2572]: loss 0.346703
[epoch19, step2573]: loss 0.599010
[epoch19, step2574]: loss 0.505879
[epoch19, step2575]: loss 0.503495
[epoch19, step2576]: loss 0.453573
[epoch19, step2577]: loss 0.472402
[epoch19, step2578]: loss 0.363788
[epoch19, step2579]: loss 0.467600
[epoch19, step2580]: loss 0.366714
[epoch19, step2581]: loss 0.285474
[epoch19, step2582]: loss 0.644707
[epoch19, step2583]: loss 0.504741
[epoch19, step2584]: loss 0.537465
[epoch19, step2585]: loss 0.327880
[epoch19, step2586]: loss 0.598724
[epoch19, step2587]: loss 0.617309
[epoch19, step2588]: loss 0.460841
[epoch19, step2589]: loss 0.291350
[epoch19, step2590]: loss 0.516542
[epoch19, step2591]: loss 0.544623
[epoch19, step2592]: loss 0.483312
[epoch19, step2593]: loss 0.422647
[epoch19, step2594]: loss 0.336271
[epoch19, step2595]: loss 0.459058
[epoch19, step2596]: loss 0.279425
[epoch19, step2597]: loss 0.436751
[epoch19, step2598]: loss 0.431802
[epoch19, step2599]: loss 0.549811
[epoch19, step2600]: loss 0.333094
[epoch19, step2601]: loss 0.553444
[epoch19, step2602]: loss 0.417000
[epoch19, step2603]: loss 0.521388
[epoch19, step2604]: loss 0.271587
[epoch19, step2605]: loss 0.517381
[epoch19, step2606]: loss 0.445254
[epoch19, step2607]: loss 0.575397
[epoch19, step2608]: loss 0.512717
[epoch19, step2609]: loss 0.434552
[epoch19, step2610]: loss 0.534171
[epoch19, step2611]: loss 0.461800
[epoch19, step2612]: loss 0.407306
[epoch19, step2613]: loss 0.309199
[epoch19, step2614]: loss 0.350168
[epoch19, step2615]: loss 0.465988
[epoch19, step2616]: loss 0.361075
[epoch19, step2617]: loss 0.332488
[epoch19, step2618]: loss 0.421321
[epoch19, step2619]: loss 0.448075
[epoch19, step2620]: loss 0.263989
[epoch19, step2621]: loss 0.510295
[epoch19, step2622]: loss 0.542000
[epoch19, step2623]: loss 0.422379
[epoch19, step2624]: loss 0.559860
[epoch19, step2625]: loss 0.534356
[epoch19, step2626]: loss 0.592726
[epoch19, step2627]: loss 0.489211
[epoch19, step2628]: loss 0.583780
[epoch19, step2629]: loss 0.363331
[epoch19, step2630]: loss 0.262377
[epoch19, step2631]: loss 0.431547
[epoch19, step2632]: loss 0.580762
[epoch19, step2633]: loss 0.247811
[epoch19, step2634]: loss 0.311510
[epoch19, step2635]: loss 0.350459
[epoch19, step2636]: loss 0.444476
[epoch19, step2637]: loss 0.498930
[epoch19, step2638]: loss 0.524302
[epoch19, step2639]: loss 0.300344
[epoch19, step2640]: loss 0.524926
[epoch19, step2641]: loss 0.493214
[epoch19, step2642]: loss 0.543078
[epoch19, step2643]: loss 0.335350
[epoch19, step2644]: loss 0.488379
[epoch19, step2645]: loss 0.438239
[epoch19, step2646]: loss 0.552889
[epoch19, step2647]: loss 0.513019
[epoch19, step2648]: loss 0.572285
[epoch19, step2649]: loss 0.669962
[epoch19, step2650]: loss 0.600606
[epoch19, step2651]: loss 0.523742
[epoch19, step2652]: loss 0.426651
[epoch19, step2653]: loss 0.317393
[epoch19, step2654]: loss 0.524434
[epoch19, step2655]: loss 0.354418
[epoch19, step2656]: loss 0.386806
[epoch19, step2657]: loss 0.403473
[epoch19, step2658]: loss 0.390334
[epoch19, step2659]: loss 0.431642
[epoch19, step2660]: loss 0.283748
[epoch19, step2661]: loss 0.474730
[epoch19, step2662]: loss 0.546218
[epoch19, step2663]: loss 0.240409
[epoch19, step2664]: loss 0.343773
[epoch19, step2665]: loss 0.649281
[epoch19, step2666]: loss 0.561572
[epoch19, step2667]: loss 0.478256
[epoch19, step2668]: loss 0.323489
[epoch19, step2669]: loss 0.650587
[epoch19, step2670]: loss 0.399573
[epoch19, step2671]: loss 0.436024
[epoch19, step2672]: loss 0.673791
[epoch19, step2673]: loss 0.430809
[epoch19, step2674]: loss 0.351286
[epoch19, step2675]: loss 0.571580
[epoch19, step2676]: loss 0.864986
[epoch19, step2677]: loss 0.579642
[epoch19, step2678]: loss 0.404671
[epoch19, step2679]: loss 0.425856
[epoch19, step2680]: loss 0.553703
[epoch19, step2681]: loss 0.344712
[epoch19, step2682]: loss 0.346684
[epoch19, step2683]: loss 0.443027
[epoch19, step2684]: loss 0.427092
[epoch19, step2685]: loss 0.618487
[epoch19, step2686]: loss 0.436819
[epoch19, step2687]: loss 0.542667
[epoch19, step2688]: loss 0.473589
[epoch19, step2689]: loss 0.479160
[epoch19, step2690]: loss 0.571102
[epoch19, step2691]: loss 0.485151
[epoch19, step2692]: loss 0.563972
[epoch19, step2693]: loss 0.408936
[epoch19, step2694]: loss 0.419963
[epoch19, step2695]: loss 0.336897
[epoch19, step2696]: loss 0.612030
[epoch19, step2697]: loss 0.623613
[epoch19, step2698]: loss 0.651393
[epoch19, step2699]: loss 0.437929
[epoch19, step2700]: loss 0.536661
[epoch19, step2701]: loss 0.452774
[epoch19, step2702]: loss 0.581566
[epoch19, step2703]: loss 0.620117
[epoch19, step2704]: loss 0.705090
[epoch19, step2705]: loss 0.337971
[epoch19, step2706]: loss 0.655089
[epoch19, step2707]: loss 0.259497
[epoch19, step2708]: loss 0.508936
[epoch19, step2709]: loss 0.549933
[epoch19, step2710]: loss 0.536881
[epoch19, step2711]: loss 0.399159
[epoch19, step2712]: loss 0.422102
[epoch19, step2713]: loss 0.514175
[epoch19, step2714]: loss 0.582493
[epoch19, step2715]: loss 0.277873
[epoch19, step2716]: loss 0.425535
[epoch19, step2717]: loss 0.546366
[epoch19, step2718]: loss 0.446105
[epoch19, step2719]: loss 0.589871
[epoch19, step2720]: loss 0.369829
[epoch19, step2721]: loss 0.441548
[epoch19, step2722]: loss 0.409267
[epoch19, step2723]: loss 0.345160
[epoch19, step2724]: loss 0.433734
[epoch19, step2725]: loss 0.283849
[epoch19, step2726]: loss 0.467785
[epoch19, step2727]: loss 0.436481
[epoch19, step2728]: loss 0.611220
[epoch19, step2729]: loss 0.551513
[epoch19, step2730]: loss 0.622589
[epoch19, step2731]: loss 0.487148
[epoch19, step2732]: loss 0.546033
[epoch19, step2733]: loss 0.352678
[epoch19, step2734]: loss 0.324321
[epoch19, step2735]: loss 0.377252
[epoch19, step2736]: loss 0.591266
[epoch19, step2737]: loss 0.424443
[epoch19, step2738]: loss 0.618674
[epoch19, step2739]: loss 0.162018
[epoch19, step2740]: loss 0.417360
[epoch19, step2741]: loss 0.401542
[epoch19, step2742]: loss 0.328411
[epoch19, step2743]: loss 0.546712
[epoch19, step2744]: loss 0.356355
[epoch19, step2745]: loss 0.406977
[epoch19, step2746]: loss 0.403680
[epoch19, step2747]: loss 0.568651
[epoch19, step2748]: loss 0.395616
[epoch19, step2749]: loss 0.395582
[epoch19, step2750]: loss 0.457013
[epoch19, step2751]: loss 0.496235
[epoch19, step2752]: loss 0.404259
[epoch19, step2753]: loss 0.634356
[epoch19, step2754]: loss 0.355853
[epoch19, step2755]: loss 0.550397
[epoch19, step2756]: loss 0.624648
[epoch19, step2757]: loss 0.501852
[epoch19, step2758]: loss 0.535665
[epoch19, step2759]: loss 0.552956
[epoch19, step2760]: loss 0.267171
[epoch19, step2761]: loss 0.204750
[epoch19, step2762]: loss 0.594764
[epoch19, step2763]: loss 0.549882
[epoch19, step2764]: loss 0.399938
[epoch19, step2765]: loss 0.433373
[epoch19, step2766]: loss 0.505186
[epoch19, step2767]: loss 0.638576
[epoch19, step2768]: loss 0.282273
[epoch19, step2769]: loss 0.487957
[epoch19, step2770]: loss 0.422432
[epoch19, step2771]: loss 0.540267
[epoch19, step2772]: loss 0.641634
[epoch19, step2773]: loss 0.488583
[epoch19, step2774]: loss 0.266552
[epoch19, step2775]: loss 0.436331
[epoch19, step2776]: loss 0.476271
[epoch19, step2777]: loss 0.433101
[epoch19, step2778]: loss 0.467895
[epoch19, step2779]: loss 0.349659
[epoch19, step2780]: loss 0.351781
[epoch19, step2781]: loss 0.523482
[epoch19, step2782]: loss 0.543105
[epoch19, step2783]: loss 0.238789
[epoch19, step2784]: loss 0.426725
[epoch19, step2785]: loss 0.403051
[epoch19, step2786]: loss 0.512302
[epoch19, step2787]: loss 0.446160
[epoch19, step2788]: loss 0.367210
[epoch19, step2789]: loss 0.222025
[epoch19, step2790]: loss 0.610963
[epoch19, step2791]: loss 0.311992
[epoch19, step2792]: loss 0.454549
[epoch19, step2793]: loss 0.493811
[epoch19, step2794]: loss 0.543444
[epoch19, step2795]: loss 0.464036
[epoch19, step2796]: loss 0.547479
[epoch19, step2797]: loss 0.469572
[epoch19, step2798]: loss 0.731187
[epoch19, step2799]: loss 0.275978
[epoch19, step2800]: loss 0.531267
[epoch19, step2801]: loss 0.449298
[epoch19, step2802]: loss 0.480747
[epoch19, step2803]: loss 0.353038
[epoch19, step2804]: loss 0.452009
[epoch19, step2805]: loss 0.569002
[epoch19, step2806]: loss 0.315380
[epoch19, step2807]: loss 0.456621
[epoch19, step2808]: loss 0.555744
[epoch19, step2809]: loss 0.413081
[epoch19, step2810]: loss 0.616029
[epoch19, step2811]: loss 0.464310
[epoch19, step2812]: loss 0.454518
[epoch19, step2813]: loss 0.547213
[epoch19, step2814]: loss 0.424230
[epoch19, step2815]: loss 0.566640
[epoch19, step2816]: loss 0.457477
[epoch19, step2817]: loss 0.471358
[epoch19, step2818]: loss 0.547606
[epoch19, step2819]: loss 0.601533
[epoch19, step2820]: loss 0.625777
[epoch19, step2821]: loss 0.498060
[epoch19, step2822]: loss 0.616383
[epoch19, step2823]: loss 0.569596
[epoch19, step2824]: loss 0.559115
[epoch19, step2825]: loss 0.550235
[epoch19, step2826]: loss 0.268220
[epoch19, step2827]: loss 0.540599
[epoch19, step2828]: loss 0.698157
[epoch19, step2829]: loss 0.469737
[epoch19, step2830]: loss 0.694833
[epoch19, step2831]: loss 0.253664
[epoch19, step2832]: loss 0.409179
[epoch19, step2833]: loss 0.490070
[epoch19, step2834]: loss 0.462645
[epoch19, step2835]: loss 0.325105
[epoch19, step2836]: loss 0.353549
[epoch19, step2837]: loss 0.501974
[epoch19, step2838]: loss 0.717067
[epoch19, step2839]: loss 0.525447
[epoch19, step2840]: loss 0.428308
[epoch19, step2841]: loss 0.649915
[epoch19, step2842]: loss 0.634241
[epoch19, step2843]: loss 0.565170
[epoch19, step2844]: loss 0.325946
[epoch19, step2845]: loss 0.463528
[epoch19, step2846]: loss 0.409874
[epoch19, step2847]: loss 0.662480
[epoch19, step2848]: loss 0.392745
[epoch19, step2849]: loss 0.368091
[epoch19, step2850]: loss 0.289408
[epoch19, step2851]: loss 0.241998
[epoch19, step2852]: loss 0.465255
[epoch19, step2853]: loss 0.335693
[epoch19, step2854]: loss 0.434329
[epoch19, step2855]: loss 0.633376
[epoch19, step2856]: loss 0.332830
[epoch19, step2857]: loss 0.583746
[epoch19, step2858]: loss 0.633599
[epoch19, step2859]: loss 0.253883
[epoch19, step2860]: loss 0.511486
[epoch19, step2861]: loss 0.437688
[epoch19, step2862]: loss 0.595759
[epoch19, step2863]: loss 0.391626
[epoch19, step2864]: loss 0.403092
[epoch19, step2865]: loss 0.494219
[epoch19, step2866]: loss 0.334140
[epoch19, step2867]: loss 0.515842
[epoch19, step2868]: loss 0.546647
[epoch19, step2869]: loss 0.540095
[epoch19, step2870]: loss 0.419566
[epoch19, step2871]: loss 0.536871
[epoch19, step2872]: loss 0.507744
[epoch19, step2873]: loss 0.543306
[epoch19, step2874]: loss 0.358741
[epoch19, step2875]: loss 0.507773
[epoch19, step2876]: loss 0.453648
[epoch19, step2877]: loss 0.515984
[epoch19, step2878]: loss 0.482851
[epoch19, step2879]: loss 0.253113
[epoch19, step2880]: loss 0.274283
[epoch19, step2881]: loss 0.386399
[epoch19, step2882]: loss 0.547799
[epoch19, step2883]: loss 0.251300
[epoch19, step2884]: loss 0.479846
[epoch19, step2885]: loss 0.487574
[epoch19, step2886]: loss 0.581793
[epoch19, step2887]: loss 0.347594
[epoch19, step2888]: loss 0.502756
[epoch19, step2889]: loss 0.158212
[epoch19, step2890]: loss 0.418176
[epoch19, step2891]: loss 0.571064
[epoch19, step2892]: loss 0.674945
[epoch19, step2893]: loss 0.652097
[epoch19, step2894]: loss 0.319754
[epoch19, step2895]: loss 0.610587
[epoch19, step2896]: loss 0.373652
[epoch19, step2897]: loss 0.645350
[epoch19, step2898]: loss 0.526130
[epoch19, step2899]: loss 0.478420
[epoch19, step2900]: loss 0.615648
[epoch19, step2901]: loss 0.500321
[epoch19, step2902]: loss 0.574110
[epoch19, step2903]: loss 0.466771
[epoch19, step2904]: loss 0.339099
[epoch19, step2905]: loss 0.512090
[epoch19, step2906]: loss 0.336790
[epoch19, step2907]: loss 0.426504
[epoch19, step2908]: loss 0.294990
[epoch19, step2909]: loss 0.253679
[epoch19, step2910]: loss 0.485304
[epoch19, step2911]: loss 0.407494
[epoch19, step2912]: loss 0.376880
[epoch19, step2913]: loss 0.339051
[epoch19, step2914]: loss 0.429185
[epoch19, step2915]: loss 0.400034
[epoch19, step2916]: loss 0.587986
[epoch19, step2917]: loss 0.407033
[epoch19, step2918]: loss 0.486018
[epoch19, step2919]: loss 0.474800
[epoch19, step2920]: loss 0.546888
[epoch19, step2921]: loss 0.616909
[epoch19, step2922]: loss 0.567550
[epoch19, step2923]: loss 0.567094
[epoch19, step2924]: loss 0.534095
[epoch19, step2925]: loss 0.242638
[epoch19, step2926]: loss 0.368439
[epoch19, step2927]: loss 0.607708
[epoch19, step2928]: loss 0.457232
[epoch19, step2929]: loss 0.362023
[epoch19, step2930]: loss 0.274354
[epoch19, step2931]: loss 0.468039
[epoch19, step2932]: loss 0.662550
[epoch19, step2933]: loss 0.540341
[epoch19, step2934]: loss 0.564040
[epoch19, step2935]: loss 0.548901
[epoch19, step2936]: loss 0.246556
[epoch19, step2937]: loss 0.542294
[epoch19, step2938]: loss 0.568910
[epoch19, step2939]: loss 0.613918
[epoch19, step2940]: loss 0.409843
[epoch19, step2941]: loss 0.523262
[epoch19, step2942]: loss 0.288300
[epoch19, step2943]: loss 0.535830
[epoch19, step2944]: loss 0.497628
[epoch19, step2945]: loss 0.430302
[epoch19, step2946]: loss 0.442324
[epoch19, step2947]: loss 0.509759
[epoch19, step2948]: loss 0.466323
[epoch19, step2949]: loss 0.375313
[epoch19, step2950]: loss 0.552677
[epoch19, step2951]: loss 0.480638
[epoch19, step2952]: loss 0.320155
[epoch19, step2953]: loss 0.414280
[epoch19, step2954]: loss 0.668998
[epoch19, step2955]: loss 0.364209
[epoch19, step2956]: loss 0.618387
[epoch19, step2957]: loss 0.469190
[epoch19, step2958]: loss 0.255331
[epoch19, step2959]: loss 0.572942
[epoch19, step2960]: loss 0.684125
[epoch19, step2961]: loss 0.523344
[epoch19, step2962]: loss 0.371745
[epoch19, step2963]: loss 0.610835
[epoch19, step2964]: loss 0.262296
[epoch19, step2965]: loss 0.468290
[epoch19, step2966]: loss 0.438922
[epoch19, step2967]: loss 0.456631
[epoch19, step2968]: loss 0.483944
[epoch19, step2969]: loss 0.481231
[epoch19, step2970]: loss 0.421087
[epoch19, step2971]: loss 0.535738
[epoch19, step2972]: loss 0.624413
[epoch19, step2973]: loss 0.333700
[epoch19, step2974]: loss 0.606012
[epoch19, step2975]: loss 0.138948
[epoch19, step2976]: loss 0.450135
[epoch19, step2977]: loss 0.463264
[epoch19, step2978]: loss 0.214271
[epoch19, step2979]: loss 0.549232
[epoch19, step2980]: loss 0.256814
[epoch19, step2981]: loss 0.451083
[epoch19, step2982]: loss 0.611444
[epoch19, step2983]: loss 0.488284
[epoch19, step2984]: loss 0.528778
[epoch19, step2985]: loss 0.436267
[epoch19, step2986]: loss 0.525173
[epoch19, step2987]: loss 0.544833
[epoch19, step2988]: loss 0.500117
[epoch19, step2989]: loss 0.623294
[epoch19, step2990]: loss 0.490169
[epoch19, step2991]: loss 0.380448
[epoch19, step2992]: loss 0.409637
[epoch19, step2993]: loss 0.312649
[epoch19, step2994]: loss 0.641031
[epoch19, step2995]: loss 0.596755
[epoch19, step2996]: loss 0.461625
[epoch19, step2997]: loss 0.344747
[epoch19, step2998]: loss 0.537371
[epoch19, step2999]: loss 0.461059
[epoch19, step3000]: loss 0.610502
[epoch19, step3001]: loss 0.605527
[epoch19, step3002]: loss 0.615326
[epoch19, step3003]: loss 0.338426
[epoch19, step3004]: loss 0.277681
[epoch19, step3005]: loss 0.552940
[epoch19, step3006]: loss 0.395361
[epoch19, step3007]: loss 0.421385
[epoch19, step3008]: loss 0.414878
[epoch19, step3009]: loss 0.447197
[epoch19, step3010]: loss 0.650946
[epoch19, step3011]: loss 0.686983
[epoch19, step3012]: loss 0.647155
[epoch19, step3013]: loss 0.200611
[epoch19, step3014]: loss 0.559208
[epoch19, step3015]: loss 0.679080
[epoch19, step3016]: loss 0.316102
[epoch19, step3017]: loss 0.577922
[epoch19, step3018]: loss 0.497096
[epoch19, step3019]: loss 0.525433
[epoch19, step3020]: loss 0.326123
[epoch19, step3021]: loss 0.395133
[epoch19, step3022]: loss 0.519679
[epoch19, step3023]: loss 0.360767
[epoch19, step3024]: loss 0.438754
[epoch19, step3025]: loss 0.597032
[epoch19, step3026]: loss 0.304192
[epoch19, step3027]: loss 0.633606
[epoch19, step3028]: loss 0.667144
[epoch19, step3029]: loss 0.519041
[epoch19, step3030]: loss 0.278906
[epoch19, step3031]: loss 0.235804
[epoch19, step3032]: loss 0.412870
[epoch19, step3033]: loss 0.360994
[epoch19, step3034]: loss 0.671208
[epoch19, step3035]: loss 0.439600
[epoch19, step3036]: loss 0.582573
[epoch19, step3037]: loss 0.562982
[epoch19, step3038]: loss 0.491530
[epoch19, step3039]: loss 0.747311
[epoch19, step3040]: loss 0.378794
[epoch19, step3041]: loss 0.674546
[epoch19, step3042]: loss 0.640050
[epoch19, step3043]: loss 0.667784
[epoch19, step3044]: loss 0.380475
[epoch19, step3045]: loss 0.337552
[epoch19, step3046]: loss 0.411051
[epoch19, step3047]: loss 0.463480
[epoch19, step3048]: loss 0.522626
[epoch19, step3049]: loss 0.452393
[epoch19, step3050]: loss 0.430340
[epoch19, step3051]: loss 0.452034
[epoch19, step3052]: loss 0.430352
[epoch19, step3053]: loss 0.477139
[epoch19, step3054]: loss 0.283560
[epoch19, step3055]: loss 0.264746
[epoch19, step3056]: loss 0.274684
[epoch19, step3057]: loss 0.385714
[epoch19, step3058]: loss 0.500708
[epoch19, step3059]: loss 0.508256
[epoch19, step3060]: loss 0.367266
[epoch19, step3061]: loss 0.376404
[epoch19, step3062]: loss 0.503996
[epoch19, step3063]: loss 0.504284
[epoch19, step3064]: loss 0.572019
[epoch19, step3065]: loss 0.529788
[epoch19, step3066]: loss 0.260263
[epoch19, step3067]: loss 0.555605
[epoch19, step3068]: loss 0.287561
[epoch19, step3069]: loss 0.333059
[epoch19, step3070]: loss 0.583577
[epoch19, step3071]: loss 0.118457
[epoch19, step3072]: loss 0.463044
[epoch19, step3073]: loss 0.502395
[epoch19, step3074]: loss 0.464933
[epoch19, step3075]: loss 0.302735
[epoch19, step3076]: loss 0.276091

[epoch19]: avg loss 0.276091

[epoch20, step1]: loss 0.645002
[epoch20, step2]: loss 0.402639
[epoch20, step3]: loss 0.237890
[epoch20, step4]: loss 0.369359
[epoch20, step5]: loss 0.580660
[epoch20, step6]: loss 0.481067
[epoch20, step7]: loss 0.304821
[epoch20, step8]: loss 0.592217
[epoch20, step9]: loss 0.416575
[epoch20, step10]: loss 0.352484
[epoch20, step11]: loss 0.623870
[epoch20, step12]: loss 0.480101
[epoch20, step13]: loss 0.425514
[epoch20, step14]: loss 0.362685
[epoch20, step15]: loss 0.574030
[epoch20, step16]: loss 0.572467
[epoch20, step17]: loss 0.435025
[epoch20, step18]: loss 0.486386
[epoch20, step19]: loss 0.552412
[epoch20, step20]: loss 0.590058
[epoch20, step21]: loss 0.366214
[epoch20, step22]: loss 0.594848
[epoch20, step23]: loss 0.627145
[epoch20, step24]: loss 0.545631
[epoch20, step25]: loss 0.411962
[epoch20, step26]: loss 0.507601
[epoch20, step27]: loss 0.526469
[epoch20, step28]: loss 0.291095
[epoch20, step29]: loss 0.474442
[epoch20, step30]: loss 0.419151
[epoch20, step31]: loss 0.445695
[epoch20, step32]: loss 0.432731
[epoch20, step33]: loss 0.382031
[epoch20, step34]: loss 0.309392
[epoch20, step35]: loss 0.647589
[epoch20, step36]: loss 0.558474
[epoch20, step37]: loss 0.437967
[epoch20, step38]: loss 0.299984
[epoch20, step39]: loss 0.369257
[epoch20, step40]: loss 0.536060
[epoch20, step41]: loss 0.246642
[epoch20, step42]: loss 0.354950
[epoch20, step43]: loss 0.593690
[epoch20, step44]: loss 0.437850
[epoch20, step45]: loss 0.447639
[epoch20, step46]: loss 0.343791
[epoch20, step47]: loss 0.429980
[epoch20, step48]: loss 0.514732
[epoch20, step49]: loss 0.316817
[epoch20, step50]: loss 0.296287
[epoch20, step51]: loss 0.405801
[epoch20, step52]: loss 0.492596
[epoch20, step53]: loss 0.438793
[epoch20, step54]: loss 0.370001
[epoch20, step55]: loss 0.484618
[epoch20, step56]: loss 0.676006
[epoch20, step57]: loss 0.471796
[epoch20, step58]: loss 0.432175
[epoch20, step59]: loss 0.456270
[epoch20, step60]: loss 0.393158
[epoch20, step61]: loss 0.232845
[epoch20, step62]: loss 0.402272
[epoch20, step63]: loss 0.579274
[epoch20, step64]: loss 0.555916
[epoch20, step65]: loss 0.470562
[epoch20, step66]: loss 0.389497
[epoch20, step67]: loss 0.444323
[epoch20, step68]: loss 0.419433
[epoch20, step69]: loss 0.265223
[epoch20, step70]: loss 0.655269
[epoch20, step71]: loss 0.494252
[epoch20, step72]: loss 0.396630
[epoch20, step73]: loss 0.448450
[epoch20, step74]: loss 0.388553
[epoch20, step75]: loss 0.410970
[epoch20, step76]: loss 0.344076
[epoch20, step77]: loss 0.309571
[epoch20, step78]: loss 0.492917
[epoch20, step79]: loss 0.454893
[epoch20, step80]: loss 0.311421
[epoch20, step81]: loss 0.446878
[epoch20, step82]: loss 0.501559
[epoch20, step83]: loss 0.309461
[epoch20, step84]: loss 0.516781
[epoch20, step85]: loss 0.511135
[epoch20, step86]: loss 0.367013
[epoch20, step87]: loss 0.417357
[epoch20, step88]: loss 0.291368
[epoch20, step89]: loss 0.361581
[epoch20, step90]: loss 0.463530
[epoch20, step91]: loss 0.239841
[epoch20, step92]: loss 0.506529
[epoch20, step93]: loss 0.532969
[epoch20, step94]: loss 0.547540
[epoch20, step95]: loss 0.578446
[epoch20, step96]: loss 0.493810
[epoch20, step97]: loss 0.533977
[epoch20, step98]: loss 0.468698
[epoch20, step99]: loss 0.406466
[epoch20, step100]: loss 0.282323
[epoch20, step101]: loss 0.379849
[epoch20, step102]: loss 0.534998
[epoch20, step103]: loss 0.437053
[epoch20, step104]: loss 0.472045
[epoch20, step105]: loss 0.401356
[epoch20, step106]: loss 0.576477
[epoch20, step107]: loss 0.419876
[epoch20, step108]: loss 0.597361
[epoch20, step109]: loss 0.358844
[epoch20, step110]: loss 0.510844
[epoch20, step111]: loss 0.348564
[epoch20, step112]: loss 0.352717
[epoch20, step113]: loss 0.429136
[epoch20, step114]: loss 0.588258
[epoch20, step115]: loss 0.309196
[epoch20, step116]: loss 0.408248
[epoch20, step117]: loss 0.302422
[epoch20, step118]: loss 0.590883
[epoch20, step119]: loss 0.481863
[epoch20, step120]: loss 0.576915
[epoch20, step121]: loss 0.484902
[epoch20, step122]: loss 0.507033
[epoch20, step123]: loss 0.375266
[epoch20, step124]: loss 0.634824
[epoch20, step125]: loss 0.591942
[epoch20, step126]: loss 0.604361
[epoch20, step127]: loss 0.241274
[epoch20, step128]: loss 0.459940
[epoch20, step129]: loss 0.361332
[epoch20, step130]: loss 0.687694
[epoch20, step131]: loss 0.522165
[epoch20, step132]: loss 0.231586
[epoch20, step133]: loss 0.488819
[epoch20, step134]: loss 0.381715
[epoch20, step135]: loss 0.507433
[epoch20, step136]: loss 0.566102
[epoch20, step137]: loss 0.592922
[epoch20, step138]: loss 0.458209
[epoch20, step139]: loss 0.541443
[epoch20, step140]: loss 0.418365
[epoch20, step141]: loss 0.250222
[epoch20, step142]: loss 0.404607
[epoch20, step143]: loss 0.543269
[epoch20, step144]: loss 0.605927
[epoch20, step145]: loss 0.243821
[epoch20, step146]: loss 0.468940
[epoch20, step147]: loss 0.480805
[epoch20, step148]: loss 0.365727
[epoch20, step149]: loss 0.130701
[epoch20, step150]: loss 0.506502
[epoch20, step151]: loss 0.343084
[epoch20, step152]: loss 0.450638
[epoch20, step153]: loss 0.426255
[epoch20, step154]: loss 0.355587
[epoch20, step155]: loss 0.263453
[epoch20, step156]: loss 0.423409
[epoch20, step157]: loss 0.627543
[epoch20, step158]: loss 0.557167
[epoch20, step159]: loss 0.448647
[epoch20, step160]: loss 0.289770
[epoch20, step161]: loss 0.557994
[epoch20, step162]: loss 0.614706
[epoch20, step163]: loss 0.535540
[epoch20, step164]: loss 0.673273
[epoch20, step165]: loss 0.419550
[epoch20, step166]: loss 0.514151
[epoch20, step167]: loss 0.468867
[epoch20, step168]: loss 0.489689
[epoch20, step169]: loss 0.686045
[epoch20, step170]: loss 0.458001
[epoch20, step171]: loss 0.516285
[epoch20, step172]: loss 0.212441
[epoch20, step173]: loss 0.354660
[epoch20, step174]: loss 0.423800
[epoch20, step175]: loss 0.500733
[epoch20, step176]: loss 0.559490
[epoch20, step177]: loss 0.287686
[epoch20, step178]: loss 0.624590
[epoch20, step179]: loss 0.482166
[epoch20, step180]: loss 0.442643
[epoch20, step181]: loss 0.532701
[epoch20, step182]: loss 0.379396
[epoch20, step183]: loss 0.453020
[epoch20, step184]: loss 0.532668
[epoch20, step185]: loss 0.400972
[epoch20, step186]: loss 0.398914
[epoch20, step187]: loss 0.323460
[epoch20, step188]: loss 0.217235
[epoch20, step189]: loss 0.349329
[epoch20, step190]: loss 0.224171
[epoch20, step191]: loss 0.722130
[epoch20, step192]: loss 0.366646
[epoch20, step193]: loss 0.582377
[epoch20, step194]: loss 0.438123
[epoch20, step195]: loss 0.532708
[epoch20, step196]: loss 0.537769
[epoch20, step197]: loss 0.382675
[epoch20, step198]: loss 0.360695
[epoch20, step199]: loss 0.518680
[epoch20, step200]: loss 0.566111
[epoch20, step201]: loss 0.316955
[epoch20, step202]: loss 0.587682
[epoch20, step203]: loss 0.651478
[epoch20, step204]: loss 0.179987
[epoch20, step205]: loss 0.308350
[epoch20, step206]: loss 0.411956
[epoch20, step207]: loss 0.344928
[epoch20, step208]: loss 0.630157
[epoch20, step209]: loss 0.469206
[epoch20, step210]: loss 0.410213
[epoch20, step211]: loss 0.331842
[epoch20, step212]: loss 0.235664
[epoch20, step213]: loss 0.159859
[epoch20, step214]: loss 0.415625
[epoch20, step215]: loss 0.422879
[epoch20, step216]: loss 0.466782
[epoch20, step217]: loss 0.374753
[epoch20, step218]: loss 0.600737
[epoch20, step219]: loss 0.277553
[epoch20, step220]: loss 0.658638
[epoch20, step221]: loss 0.409294
[epoch20, step222]: loss 0.461834
[epoch20, step223]: loss 0.479906
[epoch20, step224]: loss 0.232405
[epoch20, step225]: loss 0.235771
[epoch20, step226]: loss 0.576441
[epoch20, step227]: loss 0.615847
[epoch20, step228]: loss 0.554461
[epoch20, step229]: loss 0.604699
[epoch20, step230]: loss 0.420011
[epoch20, step231]: loss 0.419789
[epoch20, step232]: loss 0.351057
[epoch20, step233]: loss 0.487359
[epoch20, step234]: loss 0.507870
[epoch20, step235]: loss 0.503135
[epoch20, step236]: loss 0.666724
[epoch20, step237]: loss 0.525023
[epoch20, step238]: loss 0.574368
[epoch20, step239]: loss 0.538740
[epoch20, step240]: loss 0.273920
[epoch20, step241]: loss 0.269184
[epoch20, step242]: loss 0.432586
[epoch20, step243]: loss 0.464333
[epoch20, step244]: loss 0.433429
[epoch20, step245]: loss 0.431732
[epoch20, step246]: loss 0.256777
[epoch20, step247]: loss 0.605763
[epoch20, step248]: loss 0.671791
[epoch20, step249]: loss 0.465319
[epoch20, step250]: loss 0.496984
[epoch20, step251]: loss 0.428173
[epoch20, step252]: loss 0.449678
[epoch20, step253]: loss 0.608066
[epoch20, step254]: loss 0.606938
[epoch20, step255]: loss 0.507150
[epoch20, step256]: loss 0.456543
[epoch20, step257]: loss 0.439028
[epoch20, step258]: loss 0.478749
[epoch20, step259]: loss 0.561195
[epoch20, step260]: loss 0.438522
[epoch20, step261]: loss 0.606126
[epoch20, step262]: loss 0.520740
[epoch20, step263]: loss 0.412852
[epoch20, step264]: loss 0.132634
[epoch20, step265]: loss 0.635134
[epoch20, step266]: loss 0.663088
[epoch20, step267]: loss 0.335617
[epoch20, step268]: loss 0.568413
[epoch20, step269]: loss 0.670501
[epoch20, step270]: loss 0.343318
[epoch20, step271]: loss 0.261969
[epoch20, step272]: loss 0.584248
[epoch20, step273]: loss 0.478724
[epoch20, step274]: loss 0.578645
[epoch20, step275]: loss 0.500545
[epoch20, step276]: loss 0.412813
[epoch20, step277]: loss 0.473050
[epoch20, step278]: loss 0.293934
[epoch20, step279]: loss 0.565900
[epoch20, step280]: loss 0.480700
[epoch20, step281]: loss 0.542923
[epoch20, step282]: loss 0.489040
[epoch20, step283]: loss 0.482772
[epoch20, step284]: loss 0.456186
[epoch20, step285]: loss 0.486315
[epoch20, step286]: loss 0.419408
[epoch20, step287]: loss 0.467785
[epoch20, step288]: loss 0.356124
[epoch20, step289]: loss 0.456027
[epoch20, step290]: loss 0.254025
[epoch20, step291]: loss 0.481422
[epoch20, step292]: loss 0.400990
[epoch20, step293]: loss 0.413398
[epoch20, step294]: loss 0.587700
[epoch20, step295]: loss 0.527152
[epoch20, step296]: loss 0.557780
[epoch20, step297]: loss 0.525521
[epoch20, step298]: loss 0.673622
[epoch20, step299]: loss 0.641570
[epoch20, step300]: loss 0.657075
[epoch20, step301]: loss 0.475951
[epoch20, step302]: loss 0.666784
[epoch20, step303]: loss 0.577337
[epoch20, step304]: loss 0.534925
[epoch20, step305]: loss 0.255065
[epoch20, step306]: loss 0.569766
[epoch20, step307]: loss 0.553268
[epoch20, step308]: loss 0.387594
[epoch20, step309]: loss 0.393113
[epoch20, step310]: loss 0.504366
[epoch20, step311]: loss 0.414909
[epoch20, step312]: loss 0.348282
[epoch20, step313]: loss 0.484286
[epoch20, step314]: loss 0.540549
[epoch20, step315]: loss 0.424311
[epoch20, step316]: loss 0.482490
[epoch20, step317]: loss 0.553691
[epoch20, step318]: loss 0.531302
[epoch20, step319]: loss 0.454197
[epoch20, step320]: loss 0.551116
[epoch20, step321]: loss 0.326535
[epoch20, step322]: loss 0.150517
[epoch20, step323]: loss 0.554029
[epoch20, step324]: loss 0.521990
[epoch20, step325]: loss 0.428270
[epoch20, step326]: loss 0.546381
[epoch20, step327]: loss 0.602004
[epoch20, step328]: loss 0.423247
[epoch20, step329]: loss 0.519584
[epoch20, step330]: loss 0.308922
[epoch20, step331]: loss 0.446206
[epoch20, step332]: loss 0.349644
[epoch20, step333]: loss 0.574675
[epoch20, step334]: loss 0.375397
[epoch20, step335]: loss 0.415046
[epoch20, step336]: loss 0.398134
[epoch20, step337]: loss 0.491411
[epoch20, step338]: loss 0.335029
[epoch20, step339]: loss 0.420690
[epoch20, step340]: loss 0.533940
[epoch20, step341]: loss 0.340930
[epoch20, step342]: loss 0.528400
[epoch20, step343]: loss 0.491800
[epoch20, step344]: loss 0.423814
[epoch20, step345]: loss 0.311238
[epoch20, step346]: loss 0.545608
[epoch20, step347]: loss 0.679347
[epoch20, step348]: loss 0.579389
[epoch20, step349]: loss 0.332878
[epoch20, step350]: loss 0.590031
[epoch20, step351]: loss 0.502198
[epoch20, step352]: loss 0.402457
[epoch20, step353]: loss 0.660909
[epoch20, step354]: loss 0.518481
[epoch20, step355]: loss 0.247018
[epoch20, step356]: loss 0.435099
[epoch20, step357]: loss 0.497060
[epoch20, step358]: loss 0.590804
[epoch20, step359]: loss 0.579702
[epoch20, step360]: loss 0.535614
[epoch20, step361]: loss 0.416791
[epoch20, step362]: loss 0.245857
[epoch20, step363]: loss 0.109170
[epoch20, step364]: loss 0.209764
[epoch20, step365]: loss 0.493960
[epoch20, step366]: loss 0.322969
[epoch20, step367]: loss 0.368313
[epoch20, step368]: loss 0.650072
[epoch20, step369]: loss 0.630678
[epoch20, step370]: loss 0.426280
[epoch20, step371]: loss 0.517355
[epoch20, step372]: loss 0.559949
[epoch20, step373]: loss 0.401317
[epoch20, step374]: loss 0.566809
[epoch20, step375]: loss 0.573175
[epoch20, step376]: loss 0.533967
[epoch20, step377]: loss 0.659972
[epoch20, step378]: loss 0.489278
[epoch20, step379]: loss 0.434627
[epoch20, step380]: loss 0.574441
[epoch20, step381]: loss 0.418328
[epoch20, step382]: loss 0.542516
[epoch20, step383]: loss 0.207379
[epoch20, step384]: loss 0.606779
[epoch20, step385]: loss 0.472654
[epoch20, step386]: loss 0.318729
[epoch20, step387]: loss 0.662405
[epoch20, step388]: loss 0.523287
[epoch20, step389]: loss 0.588724
[epoch20, step390]: loss 0.411919
[epoch20, step391]: loss 0.359078
[epoch20, step392]: loss 0.165823
[epoch20, step393]: loss 0.510697
[epoch20, step394]: loss 0.595342
[epoch20, step395]: loss 0.550811
[epoch20, step396]: loss 0.446857
[epoch20, step397]: loss 0.367213
[epoch20, step398]: loss 0.528340
[epoch20, step399]: loss 0.472191
[epoch20, step400]: loss 0.622473
[epoch20, step401]: loss 0.490121
[epoch20, step402]: loss 0.438381
[epoch20, step403]: loss 0.568987
[epoch20, step404]: loss 0.445145
[epoch20, step405]: loss 0.569997
[epoch20, step406]: loss 0.403304
[epoch20, step407]: loss 0.369842
[epoch20, step408]: loss 0.599335
[epoch20, step409]: loss 0.339115
[epoch20, step410]: loss 0.466206
[epoch20, step411]: loss 0.343235
[epoch20, step412]: loss 0.521782
[epoch20, step413]: loss 0.261743
[epoch20, step414]: loss 0.599704
[epoch20, step415]: loss 0.258695
[epoch20, step416]: loss 0.449365
[epoch20, step417]: loss 0.698572
[epoch20, step418]: loss 0.198139
[epoch20, step419]: loss 0.370393
[epoch20, step420]: loss 0.600946
[epoch20, step421]: loss 0.534967
[epoch20, step422]: loss 0.543788
[epoch20, step423]: loss 0.251291
[epoch20, step424]: loss 0.656093
[epoch20, step425]: loss 0.537422
[epoch20, step426]: loss 0.369573
[epoch20, step427]: loss 0.507234
[epoch20, step428]: loss 0.584956
[epoch20, step429]: loss 0.444045
[epoch20, step430]: loss 0.267772
[epoch20, step431]: loss 0.449790
[epoch20, step432]: loss 0.215086
[epoch20, step433]: loss 0.459436
[epoch20, step434]: loss 0.421708
[epoch20, step435]: loss 0.492595
[epoch20, step436]: loss 0.353248
[epoch20, step437]: loss 0.451556
[epoch20, step438]: loss 0.562379
[epoch20, step439]: loss 0.337349
[epoch20, step440]: loss 0.572651
[epoch20, step441]: loss 0.640418
[epoch20, step442]: loss 0.488981
[epoch20, step443]: loss 0.329724
[epoch20, step444]: loss 0.344421
[epoch20, step445]: loss 0.567306
[epoch20, step446]: loss 0.473933
[epoch20, step447]: loss 0.450443
[epoch20, step448]: loss 0.673802
[epoch20, step449]: loss 0.394712
[epoch20, step450]: loss 0.661238
[epoch20, step451]: loss 0.482731
[epoch20, step452]: loss 0.400860
[epoch20, step453]: loss 0.515985
[epoch20, step454]: loss 0.679505
[epoch20, step455]: loss 0.458781
[epoch20, step456]: loss 0.166523
[epoch20, step457]: loss 0.638965
[epoch20, step458]: loss 0.681909
[epoch20, step459]: loss 0.515806
[epoch20, step460]: loss 0.325935
[epoch20, step461]: loss 0.358994
[epoch20, step462]: loss 0.626712
[epoch20, step463]: loss 0.525965
[epoch20, step464]: loss 0.615388
[epoch20, step465]: loss 0.565894
[epoch20, step466]: loss 0.451516
[epoch20, step467]: loss 0.581775
[epoch20, step468]: loss 0.550317
[epoch20, step469]: loss 0.529261
[epoch20, step470]: loss 0.268248
[epoch20, step471]: loss 0.382075
[epoch20, step472]: loss 0.243089
[epoch20, step473]: loss 0.544142
[epoch20, step474]: loss 0.360351
[epoch20, step475]: loss 0.550394
[epoch20, step476]: loss 0.379379
[epoch20, step477]: loss 0.363922
[epoch20, step478]: loss 0.372718
[epoch20, step479]: loss 0.502081
[epoch20, step480]: loss 0.571918
[epoch20, step481]: loss 0.593417
[epoch20, step482]: loss 0.366961
[epoch20, step483]: loss 0.514860
[epoch20, step484]: loss 0.467946
[epoch20, step485]: loss 0.481676
[epoch20, step486]: loss 0.580494
[epoch20, step487]: loss 0.488857
[epoch20, step488]: loss 0.343288
[epoch20, step489]: loss 0.311179
[epoch20, step490]: loss 0.417400
[epoch20, step491]: loss 0.654216
[epoch20, step492]: loss 0.394023
[epoch20, step493]: loss 0.441459
[epoch20, step494]: loss 0.520620
[epoch20, step495]: loss 0.350864
[epoch20, step496]: loss 0.358277
[epoch20, step497]: loss 0.448542
[epoch20, step498]: loss 0.485192
[epoch20, step499]: loss 0.478479
[epoch20, step500]: loss 0.185204
[epoch20, step501]: loss 0.549969
[epoch20, step502]: loss 0.646438
[epoch20, step503]: loss 0.253475
[epoch20, step504]: loss 0.498780
[epoch20, step505]: loss 0.345937
[epoch20, step506]: loss 0.563835
[epoch20, step507]: loss 0.489583
[epoch20, step508]: loss 0.546257
[epoch20, step509]: loss 0.314789
[epoch20, step510]: loss 0.558534
[epoch20, step511]: loss 0.426041
[epoch20, step512]: loss 0.698375
[epoch20, step513]: loss 0.544684
[epoch20, step514]: loss 0.317804
[epoch20, step515]: loss 0.627594
[epoch20, step516]: loss 0.473554
[epoch20, step517]: loss 0.499549
[epoch20, step518]: loss 0.546483
[epoch20, step519]: loss 0.399351
[epoch20, step520]: loss 0.537673
[epoch20, step521]: loss 0.452941
[epoch20, step522]: loss 0.115785
[epoch20, step523]: loss 0.574642
[epoch20, step524]: loss 0.402620
[epoch20, step525]: loss 0.735350
[epoch20, step526]: loss 0.355073
[epoch20, step527]: loss 0.280736
[epoch20, step528]: loss 0.412948
[epoch20, step529]: loss 0.475521
[epoch20, step530]: loss 0.659985
[epoch20, step531]: loss 0.496500
[epoch20, step532]: loss 0.503099
[epoch20, step533]: loss 0.383403
[epoch20, step534]: loss 0.588872
[epoch20, step535]: loss 0.396757
[epoch20, step536]: loss 0.271756
[epoch20, step537]: loss 0.424172
[epoch20, step538]: loss 0.504780
[epoch20, step539]: loss 0.331376
[epoch20, step540]: loss 0.312684
[epoch20, step541]: loss 0.612749
[epoch20, step542]: loss 0.529412
[epoch20, step543]: loss 0.433401
[epoch20, step544]: loss 0.723238
[epoch20, step545]: loss 0.459541
[epoch20, step546]: loss 0.498528
[epoch20, step547]: loss 0.581308
[epoch20, step548]: loss 0.410762
[epoch20, step549]: loss 0.516526
[epoch20, step550]: loss 0.439298
[epoch20, step551]: loss 0.502443
[epoch20, step552]: loss 0.412828
[epoch20, step553]: loss 0.351632
[epoch20, step554]: loss 0.245112
[epoch20, step555]: loss 0.472115
[epoch20, step556]: loss 0.483984
[epoch20, step557]: loss 0.385413
[epoch20, step558]: loss 0.396933
[epoch20, step559]: loss 0.581360
[epoch20, step560]: loss 0.449617
[epoch20, step561]: loss 0.459904
[epoch20, step562]: loss 0.380669
[epoch20, step563]: loss 0.502761
[epoch20, step564]: loss 0.213168
[epoch20, step565]: loss 0.597445
[epoch20, step566]: loss 0.459338
[epoch20, step567]: loss 0.284775
[epoch20, step568]: loss 0.340024
[epoch20, step569]: loss 0.434356
[epoch20, step570]: loss 0.398570
[epoch20, step571]: loss 0.445239
[epoch20, step572]: loss 0.306145
[epoch20, step573]: loss 0.281991
[epoch20, step574]: loss 0.504382
[epoch20, step575]: loss 0.340470
[epoch20, step576]: loss 0.674186
[epoch20, step577]: loss 0.365495
[epoch20, step578]: loss 0.327407
[epoch20, step579]: loss 0.432139
[epoch20, step580]: loss 0.567640
[epoch20, step581]: loss 0.476362
[epoch20, step582]: loss 0.586058
[epoch20, step583]: loss 0.405745
[epoch20, step584]: loss 0.486469
[epoch20, step585]: loss 0.299763
[epoch20, step586]: loss 0.519639
[epoch20, step587]: loss 0.431117
[epoch20, step588]: loss 0.636084
[epoch20, step589]: loss 0.564443
[epoch20, step590]: loss 0.544451
[epoch20, step591]: loss 0.340998
[epoch20, step592]: loss 0.630363
[epoch20, step593]: loss 0.560978
[epoch20, step594]: loss 0.570009
[epoch20, step595]: loss 0.477088
[epoch20, step596]: loss 0.623275
[epoch20, step597]: loss 0.293500
[epoch20, step598]: loss 0.566457
[epoch20, step599]: loss 0.406922
[epoch20, step600]: loss 0.417827
[epoch20, step601]: loss 0.361099
[epoch20, step602]: loss 0.577016
[epoch20, step603]: loss 0.272877
[epoch20, step604]: loss 0.376326
[epoch20, step605]: loss 0.259135
[epoch20, step606]: loss 0.356229
[epoch20, step607]: loss 0.391683
[epoch20, step608]: loss 0.439986
[epoch20, step609]: loss 0.283229
[epoch20, step610]: loss 0.284378
[epoch20, step611]: loss 0.092642
[epoch20, step612]: loss 0.343529
[epoch20, step613]: loss 0.520423
[epoch20, step614]: loss 0.284468
[epoch20, step615]: loss 0.590617
[epoch20, step616]: loss 0.316203
[epoch20, step617]: loss 0.384103
[epoch20, step618]: loss 0.460032
[epoch20, step619]: loss 0.548036
[epoch20, step620]: loss 0.623055
[epoch20, step621]: loss 0.592125
[epoch20, step622]: loss 0.532094
[epoch20, step623]: loss 0.560405
[epoch20, step624]: loss 0.442013
[epoch20, step625]: loss 0.432448
[epoch20, step626]: loss 0.484773
[epoch20, step627]: loss 0.444800
[epoch20, step628]: loss 0.262465
[epoch20, step629]: loss 0.540867
[epoch20, step630]: loss 0.720168
[epoch20, step631]: loss 0.261070
[epoch20, step632]: loss 0.496981
[epoch20, step633]: loss 0.475466
[epoch20, step634]: loss 0.501482
[epoch20, step635]: loss 0.392621
[epoch20, step636]: loss 0.442187
[epoch20, step637]: loss 0.629305
[epoch20, step638]: loss 0.429242
[epoch20, step639]: loss 0.384631
[epoch20, step640]: loss 0.324010
[epoch20, step641]: loss 0.618031
[epoch20, step642]: loss 0.554595
[epoch20, step643]: loss 0.666070
[epoch20, step644]: loss 0.512310
[epoch20, step645]: loss 0.801593
[epoch20, step646]: loss 0.499561
[epoch20, step647]: loss 0.367592
[epoch20, step648]: loss 0.315523
[epoch20, step649]: loss 0.513217
[epoch20, step650]: loss 0.448435
[epoch20, step651]: loss 0.464388
[epoch20, step652]: loss 0.515138
[epoch20, step653]: loss 0.497346
[epoch20, step654]: loss 0.392714
[epoch20, step655]: loss 0.550559
[epoch20, step656]: loss 0.409384
[epoch20, step657]: loss 0.629533
[epoch20, step658]: loss 0.374297
[epoch20, step659]: loss 0.461899
[epoch20, step660]: loss 0.336030
[epoch20, step661]: loss 0.545414
[epoch20, step662]: loss 0.695512
[epoch20, step663]: loss 0.281589
[epoch20, step664]: loss 0.666821
[epoch20, step665]: loss 0.408182
[epoch20, step666]: loss 0.576422
[epoch20, step667]: loss 0.494368
[epoch20, step668]: loss 0.413098
[epoch20, step669]: loss 0.531898
[epoch20, step670]: loss 0.221598
[epoch20, step671]: loss 0.335359
[epoch20, step672]: loss 0.342127
[epoch20, step673]: loss 0.659621
[epoch20, step674]: loss 0.390735
[epoch20, step675]: loss 0.356346
[epoch20, step676]: loss 0.488927
[epoch20, step677]: loss 0.497809
[epoch20, step678]: loss 0.537234
[epoch20, step679]: loss 0.466850
[epoch20, step680]: loss 0.327716
[epoch20, step681]: loss 0.517171
[epoch20, step682]: loss 0.776648
[epoch20, step683]: loss 0.285082
[epoch20, step684]: loss 0.356047
[epoch20, step685]: loss 0.414946
[epoch20, step686]: loss 0.753509
[epoch20, step687]: loss 0.490631
[epoch20, step688]: loss 0.398502
[epoch20, step689]: loss 0.379583
[epoch20, step690]: loss 0.261561
[epoch20, step691]: loss 0.513348
[epoch20, step692]: loss 0.598923
[epoch20, step693]: loss 0.553210
[epoch20, step694]: loss 0.476077
[epoch20, step695]: loss 0.382602
[epoch20, step696]: loss 0.244147
[epoch20, step697]: loss 0.358178
[epoch20, step698]: loss 0.505512
[epoch20, step699]: loss 0.504712
[epoch20, step700]: loss 0.456112
[epoch20, step701]: loss 0.523730
[epoch20, step702]: loss 0.486935
[epoch20, step703]: loss 0.501016
[epoch20, step704]: loss 0.351000
[epoch20, step705]: loss 0.549927
[epoch20, step706]: loss 0.233107
[epoch20, step707]: loss 0.382266
[epoch20, step708]: loss 0.547514
[epoch20, step709]: loss 0.645632
[epoch20, step710]: loss 0.451415
[epoch20, step711]: loss 0.400241
[epoch20, step712]: loss 0.574601
[epoch20, step713]: loss 0.145106
[epoch20, step714]: loss 0.562687
[epoch20, step715]: loss 0.574234
[epoch20, step716]: loss 0.357082
[epoch20, step717]: loss 0.453891
[epoch20, step718]: loss 0.524532
[epoch20, step719]: loss 0.532880
[epoch20, step720]: loss 0.274556
[epoch20, step721]: loss 0.455478
[epoch20, step722]: loss 0.523012
[epoch20, step723]: loss 0.510607
[epoch20, step724]: loss 0.614056
[epoch20, step725]: loss 0.341966
[epoch20, step726]: loss 0.492623
[epoch20, step727]: loss 0.448506
[epoch20, step728]: loss 0.498025
[epoch20, step729]: loss 0.584723
[epoch20, step730]: loss 0.374461
[epoch20, step731]: loss 0.589609
[epoch20, step732]: loss 0.643101
[epoch20, step733]: loss 0.637228
[epoch20, step734]: loss 0.340982
[epoch20, step735]: loss 0.509020
[epoch20, step736]: loss 0.515677
[epoch20, step737]: loss 0.592204
[epoch20, step738]: loss 0.374667
[epoch20, step739]: loss 0.489286
[epoch20, step740]: loss 0.539426
[epoch20, step741]: loss 0.671644
[epoch20, step742]: loss 0.622807
[epoch20, step743]: loss 0.489999
[epoch20, step744]: loss 0.433100
[epoch20, step745]: loss 0.377298
[epoch20, step746]: loss 0.498114
[epoch20, step747]: loss 0.368511
[epoch20, step748]: loss 0.489401
[epoch20, step749]: loss 0.561836
[epoch20, step750]: loss 0.674923
[epoch20, step751]: loss 0.448021
[epoch20, step752]: loss 0.676102
[epoch20, step753]: loss 0.276004
[epoch20, step754]: loss 0.533895
[epoch20, step755]: loss 0.360599
[epoch20, step756]: loss 0.501885
[epoch20, step757]: loss 0.602421
[epoch20, step758]: loss 0.358736
[epoch20, step759]: loss 0.557823
[epoch20, step760]: loss 0.622848
[epoch20, step761]: loss 0.503006
[epoch20, step762]: loss 0.391483
[epoch20, step763]: loss 0.616907
[epoch20, step764]: loss 0.322877
[epoch20, step765]: loss 0.431371
[epoch20, step766]: loss 0.733768
[epoch20, step767]: loss 0.595809
[epoch20, step768]: loss 0.477290
[epoch20, step769]: loss 0.478695
[epoch20, step770]: loss 0.467761
[epoch20, step771]: loss 0.683370
[epoch20, step772]: loss 0.322334
[epoch20, step773]: loss 0.524915
[epoch20, step774]: loss 0.406966
[epoch20, step775]: loss 0.581248
[epoch20, step776]: loss 0.460096
[epoch20, step777]: loss 0.416947
[epoch20, step778]: loss 0.379483
[epoch20, step779]: loss 0.500149
[epoch20, step780]: loss 0.441589
[epoch20, step781]: loss 0.475671
[epoch20, step782]: loss 0.479340
[epoch20, step783]: loss 0.338786
[epoch20, step784]: loss 0.472897
[epoch20, step785]: loss 0.252854
[epoch20, step786]: loss 0.544767
[epoch20, step787]: loss 0.533595
[epoch20, step788]: loss 0.629052
[epoch20, step789]: loss 0.108338
[epoch20, step790]: loss 0.418441
[epoch20, step791]: loss 0.259018
[epoch20, step792]: loss 0.395735
[epoch20, step793]: loss 0.380848
[epoch20, step794]: loss 0.329257
[epoch20, step795]: loss 0.504966
[epoch20, step796]: loss 0.531993
[epoch20, step797]: loss 0.471408
[epoch20, step798]: loss 0.543343
[epoch20, step799]: loss 0.509050
[epoch20, step800]: loss 0.291651
[epoch20, step801]: loss 0.595275
[epoch20, step802]: loss 0.526123
[epoch20, step803]: loss 0.352281
[epoch20, step804]: loss 0.512164
[epoch20, step805]: loss 0.418833
[epoch20, step806]: loss 0.705657
[epoch20, step807]: loss 0.269315
[epoch20, step808]: loss 0.510229
[epoch20, step809]: loss 0.625964
[epoch20, step810]: loss 0.436322
[epoch20, step811]: loss 0.379190
[epoch20, step812]: loss 0.498944
[epoch20, step813]: loss 0.323697
[epoch20, step814]: loss 0.592842
[epoch20, step815]: loss 0.412638
[epoch20, step816]: loss 0.256254
[epoch20, step817]: loss 0.362407
[epoch20, step818]: loss 0.342005
[epoch20, step819]: loss 0.505807
[epoch20, step820]: loss 0.374745
[epoch20, step821]: loss 0.698358
[epoch20, step822]: loss 0.421322
[epoch20, step823]: loss 0.437493
[epoch20, step824]: loss 0.374454
[epoch20, step825]: loss 0.590514
[epoch20, step826]: loss 0.509658
[epoch20, step827]: loss 0.196340
[epoch20, step828]: loss 0.564798
[epoch20, step829]: loss 0.487364
[epoch20, step830]: loss 0.256670
[epoch20, step831]: loss 0.435436
[epoch20, step832]: loss 0.378547
[epoch20, step833]: loss 0.335103
[epoch20, step834]: loss 0.546768
[epoch20, step835]: loss 0.270873
[epoch20, step836]: loss 0.428335
[epoch20, step837]: loss 0.264228
[epoch20, step838]: loss 0.536003
[epoch20, step839]: loss 0.567729
[epoch20, step840]: loss 0.340519
[epoch20, step841]: loss 0.468813
[epoch20, step842]: loss 0.443644
[epoch20, step843]: loss 0.325224
[epoch20, step844]: loss 0.535608
[epoch20, step845]: loss 0.637221
[epoch20, step846]: loss 0.541084
[epoch20, step847]: loss 0.546785
[epoch20, step848]: loss 0.416268
[epoch20, step849]: loss 0.255301
[epoch20, step850]: loss 0.228669
[epoch20, step851]: loss 0.482178
[epoch20, step852]: loss 0.531869
[epoch20, step853]: loss 0.600045
[epoch20, step854]: loss 0.135122
[epoch20, step855]: loss 0.343994
[epoch20, step856]: loss 0.417387
[epoch20, step857]: loss 0.430790
[epoch20, step858]: loss 0.446268
[epoch20, step859]: loss 0.379394
[epoch20, step860]: loss 0.597434
[epoch20, step861]: loss 0.423646
[epoch20, step862]: loss 0.286843
[epoch20, step863]: loss 0.592547
[epoch20, step864]: loss 0.583774
[epoch20, step865]: loss 0.509454
[epoch20, step866]: loss 0.476313
[epoch20, step867]: loss 0.492419
[epoch20, step868]: loss 0.354860
[epoch20, step869]: loss 0.587678
[epoch20, step870]: loss 0.411927
[epoch20, step871]: loss 0.335175
[epoch20, step872]: loss 0.419247
[epoch20, step873]: loss 0.594479
[epoch20, step874]: loss 0.490932
[epoch20, step875]: loss 0.423129
[epoch20, step876]: loss 0.412998
[epoch20, step877]: loss 0.574862
[epoch20, step878]: loss 0.490028
[epoch20, step879]: loss 0.583010
[epoch20, step880]: loss 0.491220
[epoch20, step881]: loss 0.363302
[epoch20, step882]: loss 0.359475
[epoch20, step883]: loss 0.527914
[epoch20, step884]: loss 0.424316
[epoch20, step885]: loss 0.409227
[epoch20, step886]: loss 0.335821
[epoch20, step887]: loss 0.158628
[epoch20, step888]: loss 0.398210
[epoch20, step889]: loss 0.502490
[epoch20, step890]: loss 0.234110
[epoch20, step891]: loss 0.426995
[epoch20, step892]: loss 0.485320
[epoch20, step893]: loss 0.506586
[epoch20, step894]: loss 0.448224
[epoch20, step895]: loss 0.337072
[epoch20, step896]: loss 0.499529
[epoch20, step897]: loss 0.536384
[epoch20, step898]: loss 0.518073
[epoch20, step899]: loss 0.463001
[epoch20, step900]: loss 0.225542
[epoch20, step901]: loss 0.497218
[epoch20, step902]: loss 0.539947
[epoch20, step903]: loss 0.325398
[epoch20, step904]: loss 0.629374
[epoch20, step905]: loss 0.636706
[epoch20, step906]: loss 0.462060
[epoch20, step907]: loss 0.445013
[epoch20, step908]: loss 0.454696
[epoch20, step909]: loss 0.591086
[epoch20, step910]: loss 0.420605
[epoch20, step911]: loss 0.486938
[epoch20, step912]: loss 0.406723
[epoch20, step913]: loss 0.425273
[epoch20, step914]: loss 0.340194
[epoch20, step915]: loss 0.634178
[epoch20, step916]: loss 0.466071
[epoch20, step917]: loss 0.417851
[epoch20, step918]: loss 0.370390
[epoch20, step919]: loss 0.311929
[epoch20, step920]: loss 0.535392
[epoch20, step921]: loss 0.530437
[epoch20, step922]: loss 0.435382
[epoch20, step923]: loss 0.551543
[epoch20, step924]: loss 0.458337
[epoch20, step925]: loss 0.621211
[epoch20, step926]: loss 0.340363
[epoch20, step927]: loss 0.548422
[epoch20, step928]: loss 0.418582
[epoch20, step929]: loss 0.445740
[epoch20, step930]: loss 0.438356
[epoch20, step931]: loss 0.479846
[epoch20, step932]: loss 0.400694
[epoch20, step933]: loss 0.602090
[epoch20, step934]: loss 0.327428
[epoch20, step935]: loss 0.483752
[epoch20, step936]: loss 0.419420
[epoch20, step937]: loss 0.132021
[epoch20, step938]: loss 0.533366
[epoch20, step939]: loss 0.531130
[epoch20, step940]: loss 0.591392
[epoch20, step941]: loss 0.455418
[epoch20, step942]: loss 0.570664
[epoch20, step943]: loss 0.362267
[epoch20, step944]: loss 0.460406
[epoch20, step945]: loss 0.648807
[epoch20, step946]: loss 0.540268
[epoch20, step947]: loss 0.311535
[epoch20, step948]: loss 0.342863
[epoch20, step949]: loss 0.376693
[epoch20, step950]: loss 0.399631
[epoch20, step951]: loss 0.341392
[epoch20, step952]: loss 0.646799
[epoch20, step953]: loss 0.453016
[epoch20, step954]: loss 0.271065
[epoch20, step955]: loss 0.462072
[epoch20, step956]: loss 0.685796
[epoch20, step957]: loss 0.419543
[epoch20, step958]: loss 0.324059
[epoch20, step959]: loss 0.623156
[epoch20, step960]: loss 0.716198
[epoch20, step961]: loss 0.564417
[epoch20, step962]: loss 0.512241
[epoch20, step963]: loss 0.528462
[epoch20, step964]: loss 0.543232
[epoch20, step965]: loss 0.564249
[epoch20, step966]: loss 0.433290
[epoch20, step967]: loss 0.590325
[epoch20, step968]: loss 0.655896
[epoch20, step969]: loss 0.422486
[epoch20, step970]: loss 0.343050
[epoch20, step971]: loss 0.332007
[epoch20, step972]: loss 0.645084
[epoch20, step973]: loss 0.606493
[epoch20, step974]: loss 0.552448
[epoch20, step975]: loss 0.431421
[epoch20, step976]: loss 0.512050
[epoch20, step977]: loss 0.357946
[epoch20, step978]: loss 0.540767
[epoch20, step979]: loss 0.460318
[epoch20, step980]: loss 0.409986
[epoch20, step981]: loss 0.670696
[epoch20, step982]: loss 0.494148
[epoch20, step983]: loss 0.524974
[epoch20, step984]: loss 0.557820
[epoch20, step985]: loss 0.510936
[epoch20, step986]: loss 0.575969
[epoch20, step987]: loss 0.590523
[epoch20, step988]: loss 0.314771
[epoch20, step989]: loss 0.246683
[epoch20, step990]: loss 0.303677
[epoch20, step991]: loss 0.669169
[epoch20, step992]: loss 0.456474
[epoch20, step993]: loss 0.399048
[epoch20, step994]: loss 0.349239
[epoch20, step995]: loss 0.479507
[epoch20, step996]: loss 0.604634
[epoch20, step997]: loss 0.642718
[epoch20, step998]: loss 0.406471
[epoch20, step999]: loss 0.402586
[epoch20, step1000]: loss 0.443060
[epoch20, step1001]: loss 0.400975
[epoch20, step1002]: loss 0.461380
[epoch20, step1003]: loss 0.565342
[epoch20, step1004]: loss 0.485326
[epoch20, step1005]: loss 0.397070
[epoch20, step1006]: loss 0.408125
[epoch20, step1007]: loss 0.470733
[epoch20, step1008]: loss 0.523830
[epoch20, step1009]: loss 0.480128
[epoch20, step1010]: loss 0.472751
[epoch20, step1011]: loss 0.501364
[epoch20, step1012]: loss 0.361042
[epoch20, step1013]: loss 0.362000
[epoch20, step1014]: loss 0.630632
[epoch20, step1015]: loss 0.320416
[epoch20, step1016]: loss 0.547436
[epoch20, step1017]: loss 0.637875
[epoch20, step1018]: loss 0.421646
[epoch20, step1019]: loss 0.314822
[epoch20, step1020]: loss 0.357481
[epoch20, step1021]: loss 0.717129
[epoch20, step1022]: loss 0.402072
[epoch20, step1023]: loss 0.496001
[epoch20, step1024]: loss 0.502323
[epoch20, step1025]: loss 0.486562
[epoch20, step1026]: loss 0.407639
[epoch20, step1027]: loss 0.519316
[epoch20, step1028]: loss 0.545748
[epoch20, step1029]: loss 0.588792
[epoch20, step1030]: loss 0.608062
[epoch20, step1031]: loss 0.470727
[epoch20, step1032]: loss 0.254948
[epoch20, step1033]: loss 0.646422
[epoch20, step1034]: loss 0.410911
[epoch20, step1035]: loss 0.408716
[epoch20, step1036]: loss 0.620141
[epoch20, step1037]: loss 0.331760
[epoch20, step1038]: loss 0.424931
[epoch20, step1039]: loss 0.538863
[epoch20, step1040]: loss 0.484817
[epoch20, step1041]: loss 0.664191
[epoch20, step1042]: loss 0.525726
[epoch20, step1043]: loss 0.338642
[epoch20, step1044]: loss 0.542430
[epoch20, step1045]: loss 0.420396
[epoch20, step1046]: loss 0.524486
[epoch20, step1047]: loss 0.483340
[epoch20, step1048]: loss 0.647754
[epoch20, step1049]: loss 0.357302
[epoch20, step1050]: loss 0.283632
[epoch20, step1051]: loss 0.508946
[epoch20, step1052]: loss 0.336371
[epoch20, step1053]: loss 0.349736
[epoch20, step1054]: loss 0.614046
[epoch20, step1055]: loss 0.711817
[epoch20, step1056]: loss 0.340976
[epoch20, step1057]: loss 0.458845
[epoch20, step1058]: loss 0.422398
[epoch20, step1059]: loss 0.632184
[epoch20, step1060]: loss 0.453812
[epoch20, step1061]: loss 0.314284
[epoch20, step1062]: loss 0.749161
[epoch20, step1063]: loss 0.425099
[epoch20, step1064]: loss 0.514473
[epoch20, step1065]: loss 0.527176
[epoch20, step1066]: loss 0.489397
[epoch20, step1067]: loss 0.129857
[epoch20, step1068]: loss 0.400469
[epoch20, step1069]: loss 0.472561
[epoch20, step1070]: loss 0.491358
[epoch20, step1071]: loss 0.382370
[epoch20, step1072]: loss 0.518207
[epoch20, step1073]: loss 0.363050
[epoch20, step1074]: loss 0.451269
[epoch20, step1075]: loss 0.416458
[epoch20, step1076]: loss 0.457331
[epoch20, step1077]: loss 0.414867
[epoch20, step1078]: loss 0.499988
[epoch20, step1079]: loss 0.504069
[epoch20, step1080]: loss 0.476392
[epoch20, step1081]: loss 0.400713
[epoch20, step1082]: loss 0.547297
[epoch20, step1083]: loss 0.571745
[epoch20, step1084]: loss 0.482725
[epoch20, step1085]: loss 0.391017
[epoch20, step1086]: loss 0.627027
[epoch20, step1087]: loss 0.267608
[epoch20, step1088]: loss 0.523818
[epoch20, step1089]: loss 0.627304
[epoch20, step1090]: loss 0.558689
[epoch20, step1091]: loss 0.519896
[epoch20, step1092]: loss 0.492791
[epoch20, step1093]: loss 0.464761
[epoch20, step1094]: loss 0.348249
[epoch20, step1095]: loss 0.439045
[epoch20, step1096]: loss 0.457385
[epoch20, step1097]: loss 0.656226
[epoch20, step1098]: loss 0.404566
[epoch20, step1099]: loss 0.442194
[epoch20, step1100]: loss 0.262931
[epoch20, step1101]: loss 0.423732
[epoch20, step1102]: loss 0.606560
[epoch20, step1103]: loss 0.504970
[epoch20, step1104]: loss 0.311761
[epoch20, step1105]: loss 0.445046
[epoch20, step1106]: loss 0.368439
[epoch20, step1107]: loss 0.719071
[epoch20, step1108]: loss 0.107561
[epoch20, step1109]: loss 0.382501
[epoch20, step1110]: loss 0.460295
[epoch20, step1111]: loss 0.525849
[epoch20, step1112]: loss 0.249909
[epoch20, step1113]: loss 0.701189
[epoch20, step1114]: loss 0.480702
[epoch20, step1115]: loss 0.358264
[epoch20, step1116]: loss 0.504364
[epoch20, step1117]: loss 0.387250
[epoch20, step1118]: loss 0.261699
[epoch20, step1119]: loss 0.367308
[epoch20, step1120]: loss 0.759362
[epoch20, step1121]: loss 0.545256
[epoch20, step1122]: loss 0.527228
[epoch20, step1123]: loss 0.449171
[epoch20, step1124]: loss 0.136047
[epoch20, step1125]: loss 0.237105
[epoch20, step1126]: loss 0.454751
[epoch20, step1127]: loss 0.565161
[epoch20, step1128]: loss 0.220342
[epoch20, step1129]: loss 0.640268
[epoch20, step1130]: loss 0.218687
[epoch20, step1131]: loss 0.551454
[epoch20, step1132]: loss 0.580000
[epoch20, step1133]: loss 0.520900
[epoch20, step1134]: loss 0.347916
[epoch20, step1135]: loss 0.518029
[epoch20, step1136]: loss 0.641033
[epoch20, step1137]: loss 0.391298
[epoch20, step1138]: loss 0.624536
[epoch20, step1139]: loss 0.531000
[epoch20, step1140]: loss 0.548443
[epoch20, step1141]: loss 0.520637
[epoch20, step1142]: loss 0.470845
[epoch20, step1143]: loss 0.446339
[epoch20, step1144]: loss 0.582580
[epoch20, step1145]: loss 0.362457
[epoch20, step1146]: loss 0.742495
[epoch20, step1147]: loss 0.237817
[epoch20, step1148]: loss 0.437042
[epoch20, step1149]: loss 0.566007
[epoch20, step1150]: loss 0.491120
[epoch20, step1151]: loss 0.601100
[epoch20, step1152]: loss 0.347908
[epoch20, step1153]: loss 0.424970
[epoch20, step1154]: loss 0.263437
[epoch20, step1155]: loss 0.691925
[epoch20, step1156]: loss 0.433213
[epoch20, step1157]: loss 0.340750
[epoch20, step1158]: loss 0.330461
[epoch20, step1159]: loss 0.397314
[epoch20, step1160]: loss 0.346328
[epoch20, step1161]: loss 0.535075
[epoch20, step1162]: loss 0.368117
[epoch20, step1163]: loss 0.440158
[epoch20, step1164]: loss 0.413060
[epoch20, step1165]: loss 0.524677
[epoch20, step1166]: loss 0.622532
[epoch20, step1167]: loss 0.335937
[epoch20, step1168]: loss 0.407445
[epoch20, step1169]: loss 0.669720
[epoch20, step1170]: loss 0.418722
[epoch20, step1171]: loss 0.585467
[epoch20, step1172]: loss 0.212835
[epoch20, step1173]: loss 0.385024
[epoch20, step1174]: loss 0.474184
[epoch20, step1175]: loss 0.428490
[epoch20, step1176]: loss 0.371394
[epoch20, step1177]: loss 0.530713
[epoch20, step1178]: loss 0.443583
[epoch20, step1179]: loss 0.253229
[epoch20, step1180]: loss 0.346714
[epoch20, step1181]: loss 0.447819
[epoch20, step1182]: loss 0.486081
[epoch20, step1183]: loss 0.353308
[epoch20, step1184]: loss 0.450358
[epoch20, step1185]: loss 0.577247
[epoch20, step1186]: loss 0.500883
[epoch20, step1187]: loss 0.443727
[epoch20, step1188]: loss 0.559346
[epoch20, step1189]: loss 0.343441
[epoch20, step1190]: loss 0.511621
[epoch20, step1191]: loss 0.593038
[epoch20, step1192]: loss 0.384927
[epoch20, step1193]: loss 0.535736
[epoch20, step1194]: loss 0.560025
[epoch20, step1195]: loss 0.380588
[epoch20, step1196]: loss 0.444648
[epoch20, step1197]: loss 0.456776
[epoch20, step1198]: loss 0.274067
[epoch20, step1199]: loss 0.593574
[epoch20, step1200]: loss 0.334615
[epoch20, step1201]: loss 0.496797
[epoch20, step1202]: loss 0.232523
[epoch20, step1203]: loss 0.416364
[epoch20, step1204]: loss 0.641538
[epoch20, step1205]: loss 0.265672
[epoch20, step1206]: loss 0.490318
[epoch20, step1207]: loss 0.355465
[epoch20, step1208]: loss 0.394324
[epoch20, step1209]: loss 0.491866
[epoch20, step1210]: loss 0.446175
[epoch20, step1211]: loss 0.639691
[epoch20, step1212]: loss 0.357625
[epoch20, step1213]: loss 0.400365
[epoch20, step1214]: loss 0.739151
[epoch20, step1215]: loss 0.532021
[epoch20, step1216]: loss 0.502659
[epoch20, step1217]: loss 0.524666
[epoch20, step1218]: loss 0.361466
[epoch20, step1219]: loss 0.329627
[epoch20, step1220]: loss 0.609226
[epoch20, step1221]: loss 0.371864
[epoch20, step1222]: loss 0.421111
[epoch20, step1223]: loss 0.473484
[epoch20, step1224]: loss 0.492776
[epoch20, step1225]: loss 0.525062
[epoch20, step1226]: loss 0.520535
[epoch20, step1227]: loss 0.511389
[epoch20, step1228]: loss 0.505407
[epoch20, step1229]: loss 0.512136
[epoch20, step1230]: loss 0.413577
[epoch20, step1231]: loss 0.290985
[epoch20, step1232]: loss 0.363233
[epoch20, step1233]: loss 0.579045
[epoch20, step1234]: loss 0.561534
[epoch20, step1235]: loss 0.365458
[epoch20, step1236]: loss 0.173912
[epoch20, step1237]: loss 0.522910
[epoch20, step1238]: loss 0.377313
[epoch20, step1239]: loss 0.532479
[epoch20, step1240]: loss 0.634188
[epoch20, step1241]: loss 0.577340
[epoch20, step1242]: loss 0.568150
[epoch20, step1243]: loss 0.675816
[epoch20, step1244]: loss 0.422251
[epoch20, step1245]: loss 0.512902
[epoch20, step1246]: loss 0.358483
[epoch20, step1247]: loss 0.317050
[epoch20, step1248]: loss 0.453974
[epoch20, step1249]: loss 0.382168
[epoch20, step1250]: loss 0.392488
[epoch20, step1251]: loss 0.687577
[epoch20, step1252]: loss 0.365177
[epoch20, step1253]: loss 0.511001
[epoch20, step1254]: loss 0.539910
[epoch20, step1255]: loss 0.452071
[epoch20, step1256]: loss 0.517234
[epoch20, step1257]: loss 0.455861
[epoch20, step1258]: loss 0.504520
[epoch20, step1259]: loss 0.567055
[epoch20, step1260]: loss 0.412990
[epoch20, step1261]: loss 0.494867
[epoch20, step1262]: loss 0.541383
[epoch20, step1263]: loss 0.509358
[epoch20, step1264]: loss 0.585025
[epoch20, step1265]: loss 0.563042
[epoch20, step1266]: loss 0.255803
[epoch20, step1267]: loss 0.466936
[epoch20, step1268]: loss 0.426371
[epoch20, step1269]: loss 0.451973
[epoch20, step1270]: loss 0.424778
[epoch20, step1271]: loss 0.368508
[epoch20, step1272]: loss 0.394608
[epoch20, step1273]: loss 0.441968
[epoch20, step1274]: loss 0.604747
[epoch20, step1275]: loss 0.431922
[epoch20, step1276]: loss 0.417413
[epoch20, step1277]: loss 0.289008
[epoch20, step1278]: loss 0.385550
[epoch20, step1279]: loss 0.600355
[epoch20, step1280]: loss 0.379120
[epoch20, step1281]: loss 0.299557
[epoch20, step1282]: loss 0.603715
[epoch20, step1283]: loss 0.263511
[epoch20, step1284]: loss 0.392667
[epoch20, step1285]: loss 0.611360
[epoch20, step1286]: loss 0.488842
[epoch20, step1287]: loss 0.682380
[epoch20, step1288]: loss 0.693314
[epoch20, step1289]: loss 0.257797
[epoch20, step1290]: loss 0.466105
[epoch20, step1291]: loss 0.678962
[epoch20, step1292]: loss 0.132580
[epoch20, step1293]: loss 0.366027
[epoch20, step1294]: loss 0.497416
[epoch20, step1295]: loss 0.420633
[epoch20, step1296]: loss 0.408347
[epoch20, step1297]: loss 0.423663
[epoch20, step1298]: loss 0.356060
[epoch20, step1299]: loss 0.510534
[epoch20, step1300]: loss 0.411368
[epoch20, step1301]: loss 0.513648
[epoch20, step1302]: loss 0.400772
[epoch20, step1303]: loss 0.703014
[epoch20, step1304]: loss 0.505660
[epoch20, step1305]: loss 0.378206
[epoch20, step1306]: loss 0.611630
[epoch20, step1307]: loss 0.441618
[epoch20, step1308]: loss 0.399729
[epoch20, step1309]: loss 0.356282
[epoch20, step1310]: loss 0.586832
[epoch20, step1311]: loss 0.441203
[epoch20, step1312]: loss 0.347200
[epoch20, step1313]: loss 0.487770
[epoch20, step1314]: loss 0.481073
[epoch20, step1315]: loss 0.556942
[epoch20, step1316]: loss 0.452832
[epoch20, step1317]: loss 0.401495
[epoch20, step1318]: loss 0.611200
[epoch20, step1319]: loss 0.567620
[epoch20, step1320]: loss 0.665220
[epoch20, step1321]: loss 0.503160
[epoch20, step1322]: loss 0.570963
[epoch20, step1323]: loss 0.415888
[epoch20, step1324]: loss 0.434027
[epoch20, step1325]: loss 0.492474
[epoch20, step1326]: loss 0.457949
[epoch20, step1327]: loss 0.437071
[epoch20, step1328]: loss 0.441828
[epoch20, step1329]: loss 0.628184
[epoch20, step1330]: loss 0.577824
[epoch20, step1331]: loss 0.428433
[epoch20, step1332]: loss 0.170460
[epoch20, step1333]: loss 0.344765
[epoch20, step1334]: loss 0.405679
[epoch20, step1335]: loss 0.588193
[epoch20, step1336]: loss 0.419453
[epoch20, step1337]: loss 0.498342
[epoch20, step1338]: loss 0.572183
[epoch20, step1339]: loss 0.395846
[epoch20, step1340]: loss 0.334763
[epoch20, step1341]: loss 0.548275
[epoch20, step1342]: loss 0.548611
[epoch20, step1343]: loss 0.419879
[epoch20, step1344]: loss 0.492715
[epoch20, step1345]: loss 0.319553
[epoch20, step1346]: loss 0.512532
[epoch20, step1347]: loss 0.531073
[epoch20, step1348]: loss 0.390920
[epoch20, step1349]: loss 0.614599
[epoch20, step1350]: loss 0.482650
[epoch20, step1351]: loss 0.604927
[epoch20, step1352]: loss 0.264920
[epoch20, step1353]: loss 0.548497
[epoch20, step1354]: loss 0.796432
[epoch20, step1355]: loss 0.488538
[epoch20, step1356]: loss 0.355527
[epoch20, step1357]: loss 0.479919
[epoch20, step1358]: loss 0.605488
[epoch20, step1359]: loss 0.662314
[epoch20, step1360]: loss 0.382121
[epoch20, step1361]: loss 0.347915
[epoch20, step1362]: loss 0.685220
[epoch20, step1363]: loss 0.302290
[epoch20, step1364]: loss 0.500467
[epoch20, step1365]: loss 0.368382
[epoch20, step1366]: loss 0.497757
[epoch20, step1367]: loss 0.318910
[epoch20, step1368]: loss 0.449675
[epoch20, step1369]: loss 0.381619
[epoch20, step1370]: loss 0.393963
[epoch20, step1371]: loss 0.430449
[epoch20, step1372]: loss 0.468710
[epoch20, step1373]: loss 0.300849
[epoch20, step1374]: loss 0.319090
[epoch20, step1375]: loss 0.324471
[epoch20, step1376]: loss 0.517363
[epoch20, step1377]: loss 0.657366
[epoch20, step1378]: loss 0.578263
[epoch20, step1379]: loss 0.524400
[epoch20, step1380]: loss 0.519085
[epoch20, step1381]: loss 0.483805
[epoch20, step1382]: loss 0.465205
[epoch20, step1383]: loss 0.110785
[epoch20, step1384]: loss 0.187595
[epoch20, step1385]: loss 0.538841
[epoch20, step1386]: loss 0.556558
[epoch20, step1387]: loss 0.414284
[epoch20, step1388]: loss 0.535327
[epoch20, step1389]: loss 0.466771
[epoch20, step1390]: loss 0.712909
[epoch20, step1391]: loss 0.468768
[epoch20, step1392]: loss 0.684970
[epoch20, step1393]: loss 0.439579
[epoch20, step1394]: loss 0.584301
[epoch20, step1395]: loss 0.220584
[epoch20, step1396]: loss 0.548856
[epoch20, step1397]: loss 0.589120
[epoch20, step1398]: loss 0.506192
[epoch20, step1399]: loss 0.548989
[epoch20, step1400]: loss 0.420020
[epoch20, step1401]: loss 0.503222
[epoch20, step1402]: loss 0.414173
[epoch20, step1403]: loss 0.469150
[epoch20, step1404]: loss 0.328626
[epoch20, step1405]: loss 0.520949
[epoch20, step1406]: loss 0.630949
[epoch20, step1407]: loss 0.497798
[epoch20, step1408]: loss 0.428214
[epoch20, step1409]: loss 0.531795
[epoch20, step1410]: loss 0.597436
[epoch20, step1411]: loss 0.574803
[epoch20, step1412]: loss 0.383068
[epoch20, step1413]: loss 0.473932
[epoch20, step1414]: loss 0.260368
[epoch20, step1415]: loss 0.391967
[epoch20, step1416]: loss 0.489095
[epoch20, step1417]: loss 0.500978
[epoch20, step1418]: loss 0.676669
[epoch20, step1419]: loss 0.477338
[epoch20, step1420]: loss 0.408001
[epoch20, step1421]: loss 0.282836
[epoch20, step1422]: loss 0.371341
[epoch20, step1423]: loss 0.609688
[epoch20, step1424]: loss 0.425688
[epoch20, step1425]: loss 0.350772
[epoch20, step1426]: loss 0.581101
[epoch20, step1427]: loss 0.499572
[epoch20, step1428]: loss 0.433800
[epoch20, step1429]: loss 0.293256
[epoch20, step1430]: loss 0.489047
[epoch20, step1431]: loss 0.449540
[epoch20, step1432]: loss 0.534673
[epoch20, step1433]: loss 0.244246
[epoch20, step1434]: loss 0.630565
[epoch20, step1435]: loss 0.430392
[epoch20, step1436]: loss 0.487749
[epoch20, step1437]: loss 0.533671
[epoch20, step1438]: loss 0.337665
[epoch20, step1439]: loss 0.422888
[epoch20, step1440]: loss 0.449193
[epoch20, step1441]: loss 0.250466
[epoch20, step1442]: loss 0.443014
[epoch20, step1443]: loss 0.316439
[epoch20, step1444]: loss 0.241396
[epoch20, step1445]: loss 0.465338
[epoch20, step1446]: loss 0.406896
[epoch20, step1447]: loss 0.441825
[epoch20, step1448]: loss 0.543000
[epoch20, step1449]: loss 0.404753
[epoch20, step1450]: loss 0.414735
[epoch20, step1451]: loss 0.210461
[epoch20, step1452]: loss 0.345238
[epoch20, step1453]: loss 0.384505
[epoch20, step1454]: loss 0.587663
[epoch20, step1455]: loss 0.424466
[epoch20, step1456]: loss 0.481952
[epoch20, step1457]: loss 0.440071
[epoch20, step1458]: loss 0.425348
[epoch20, step1459]: loss 0.458220
[epoch20, step1460]: loss 0.460339
[epoch20, step1461]: loss 0.438706
[epoch20, step1462]: loss 0.398460
[epoch20, step1463]: loss 0.575292
[epoch20, step1464]: loss 0.552770
[epoch20, step1465]: loss 0.698222
[epoch20, step1466]: loss 0.565947
[epoch20, step1467]: loss 0.416424
[epoch20, step1468]: loss 0.446764
[epoch20, step1469]: loss 0.255692
[epoch20, step1470]: loss 0.222999
[epoch20, step1471]: loss 0.328765
[epoch20, step1472]: loss 0.407142
[epoch20, step1473]: loss 0.369044
[epoch20, step1474]: loss 0.551118
[epoch20, step1475]: loss 0.393984
[epoch20, step1476]: loss 0.491015
[epoch20, step1477]: loss 0.390237
[epoch20, step1478]: loss 0.143630
[epoch20, step1479]: loss 0.449495
[epoch20, step1480]: loss 0.615286
[epoch20, step1481]: loss 0.464619
[epoch20, step1482]: loss 0.581067
[epoch20, step1483]: loss 0.426590
[epoch20, step1484]: loss 0.529406
[epoch20, step1485]: loss 0.505199
[epoch20, step1486]: loss 0.384287
[epoch20, step1487]: loss 0.344747
[epoch20, step1488]: loss 0.291651
[epoch20, step1489]: loss 0.452014
[epoch20, step1490]: loss 0.516039
[epoch20, step1491]: loss 0.406615
[epoch20, step1492]: loss 0.495252
[epoch20, step1493]: loss 0.456406
[epoch20, step1494]: loss 0.459353
[epoch20, step1495]: loss 0.332951
[epoch20, step1496]: loss 0.358652
[epoch20, step1497]: loss 0.335533
[epoch20, step1498]: loss 0.393320
[epoch20, step1499]: loss 0.496169
[epoch20, step1500]: loss 0.261374
[epoch20, step1501]: loss 0.463011
[epoch20, step1502]: loss 0.360862
[epoch20, step1503]: loss 0.460142
[epoch20, step1504]: loss 0.482377
[epoch20, step1505]: loss 0.468089
[epoch20, step1506]: loss 0.544289
[epoch20, step1507]: loss 0.469112
[epoch20, step1508]: loss 0.579131
[epoch20, step1509]: loss 0.568043
[epoch20, step1510]: loss 0.374554
[epoch20, step1511]: loss 0.622531
[epoch20, step1512]: loss 0.582967
[epoch20, step1513]: loss 0.551340
[epoch20, step1514]: loss 0.441211
[epoch20, step1515]: loss 0.563060
[epoch20, step1516]: loss 0.608596
[epoch20, step1517]: loss 0.534692
[epoch20, step1518]: loss 0.399433
[epoch20, step1519]: loss 0.406962
[epoch20, step1520]: loss 0.600364
[epoch20, step1521]: loss 0.561360
[epoch20, step1522]: loss 0.635165
[epoch20, step1523]: loss 0.375164
[epoch20, step1524]: loss 0.646431
[epoch20, step1525]: loss 0.470784
[epoch20, step1526]: loss 0.650416
[epoch20, step1527]: loss 0.608981
[epoch20, step1528]: loss 0.436025
[epoch20, step1529]: loss 0.381231
[epoch20, step1530]: loss 0.665395
[epoch20, step1531]: loss 0.609691
[epoch20, step1532]: loss 0.351837
[epoch20, step1533]: loss 0.424398
[epoch20, step1534]: loss 0.514724
[epoch20, step1535]: loss 0.197792
[epoch20, step1536]: loss 0.164007
[epoch20, step1537]: loss 0.462369
[epoch20, step1538]: loss 0.502210
[epoch20, step1539]: loss 0.641439
[epoch20, step1540]: loss 0.435925
[epoch20, step1541]: loss 0.489954
[epoch20, step1542]: loss 0.470523
[epoch20, step1543]: loss 0.429296
[epoch20, step1544]: loss 0.376741
[epoch20, step1545]: loss 0.242702
[epoch20, step1546]: loss 0.731870
[epoch20, step1547]: loss 0.606014
[epoch20, step1548]: loss 0.523793
[epoch20, step1549]: loss 0.354855
[epoch20, step1550]: loss 0.322645
[epoch20, step1551]: loss 0.244163
[epoch20, step1552]: loss 0.423697
[epoch20, step1553]: loss 0.556761
[epoch20, step1554]: loss 0.501849
[epoch20, step1555]: loss 0.567311
[epoch20, step1556]: loss 0.412763
[epoch20, step1557]: loss 0.598619
[epoch20, step1558]: loss 0.250992
[epoch20, step1559]: loss 0.605376
[epoch20, step1560]: loss 0.454654
[epoch20, step1561]: loss 0.535845
[epoch20, step1562]: loss 0.153509
[epoch20, step1563]: loss 0.600970
[epoch20, step1564]: loss 0.362437
[epoch20, step1565]: loss 0.461287
[epoch20, step1566]: loss 0.631459
[epoch20, step1567]: loss 0.327770
[epoch20, step1568]: loss 0.608112
[epoch20, step1569]: loss 0.592407
[epoch20, step1570]: loss 0.417574
[epoch20, step1571]: loss 0.576582
[epoch20, step1572]: loss 0.330509
[epoch20, step1573]: loss 0.362788
[epoch20, step1574]: loss 0.400870
[epoch20, step1575]: loss 0.454790
[epoch20, step1576]: loss 0.598356
[epoch20, step1577]: loss 0.648526
[epoch20, step1578]: loss 0.354852
[epoch20, step1579]: loss 0.433290
[epoch20, step1580]: loss 0.539308
[epoch20, step1581]: loss 0.733305
[epoch20, step1582]: loss 0.551947
[epoch20, step1583]: loss 0.478518
[epoch20, step1584]: loss 0.516512
[epoch20, step1585]: loss 0.254245
[epoch20, step1586]: loss 0.516649
[epoch20, step1587]: loss 0.256869
[epoch20, step1588]: loss 0.391400
[epoch20, step1589]: loss 0.651397
[epoch20, step1590]: loss 0.569879
[epoch20, step1591]: loss 0.468451
[epoch20, step1592]: loss 0.544667
[epoch20, step1593]: loss 0.347950
[epoch20, step1594]: loss 0.257538
[epoch20, step1595]: loss 0.450217
[epoch20, step1596]: loss 0.516101
[epoch20, step1597]: loss 0.630696
[epoch20, step1598]: loss 0.310330
[epoch20, step1599]: loss 0.464764
[epoch20, step1600]: loss 0.534137
[epoch20, step1601]: loss 0.628049
[epoch20, step1602]: loss 0.537319
[epoch20, step1603]: loss 0.625170
[epoch20, step1604]: loss 0.312226
[epoch20, step1605]: loss 0.264350
[epoch20, step1606]: loss 0.377094
[epoch20, step1607]: loss 0.504999
[epoch20, step1608]: loss 0.492607
[epoch20, step1609]: loss 0.413882
[epoch20, step1610]: loss 0.586875
[epoch20, step1611]: loss 0.553922
[epoch20, step1612]: loss 0.492527
[epoch20, step1613]: loss 0.429205
[epoch20, step1614]: loss 0.578513
[epoch20, step1615]: loss 0.391655
[epoch20, step1616]: loss 0.390891
[epoch20, step1617]: loss 0.473781
[epoch20, step1618]: loss 0.379046
[epoch20, step1619]: loss 0.575588
[epoch20, step1620]: loss 0.583924
[epoch20, step1621]: loss 0.475285
[epoch20, step1622]: loss 0.240494
[epoch20, step1623]: loss 0.600622
[epoch20, step1624]: loss 0.587708
[epoch20, step1625]: loss 0.479102
[epoch20, step1626]: loss 0.489477
[epoch20, step1627]: loss 0.492961
[epoch20, step1628]: loss 0.454155
[epoch20, step1629]: loss 0.361356
[epoch20, step1630]: loss 0.455644
[epoch20, step1631]: loss 0.406891
[epoch20, step1632]: loss 0.440505
[epoch20, step1633]: loss 0.579090
[epoch20, step1634]: loss 0.592629
[epoch20, step1635]: loss 0.365877
[epoch20, step1636]: loss 0.630554
[epoch20, step1637]: loss 0.403686
[epoch20, step1638]: loss 0.643396
[epoch20, step1639]: loss 0.439644
[epoch20, step1640]: loss 0.329176
[epoch20, step1641]: loss 0.427926
[epoch20, step1642]: loss 0.494807
[epoch20, step1643]: loss 0.438121
[epoch20, step1644]: loss 0.707456
[epoch20, step1645]: loss 0.570354
[epoch20, step1646]: loss 0.346809
[epoch20, step1647]: loss 0.681957
[epoch20, step1648]: loss 0.501900
[epoch20, step1649]: loss 0.485927
[epoch20, step1650]: loss 0.388355
[epoch20, step1651]: loss 0.451418
[epoch20, step1652]: loss 0.445409
[epoch20, step1653]: loss 0.307216
[epoch20, step1654]: loss 0.576490
[epoch20, step1655]: loss 0.360131
[epoch20, step1656]: loss 0.426866
[epoch20, step1657]: loss 0.391090
[epoch20, step1658]: loss 0.481023
[epoch20, step1659]: loss 0.722781
[epoch20, step1660]: loss 0.630333
[epoch20, step1661]: loss 0.122856
[epoch20, step1662]: loss 0.310642
[epoch20, step1663]: loss 0.628785
[epoch20, step1664]: loss 0.682220
[epoch20, step1665]: loss 0.497412
[epoch20, step1666]: loss 0.528031
[epoch20, step1667]: loss 0.745223
[epoch20, step1668]: loss 0.463018
[epoch20, step1669]: loss 0.686203
[epoch20, step1670]: loss 0.350470
[epoch20, step1671]: loss 0.632336
[epoch20, step1672]: loss 0.505138
[epoch20, step1673]: loss 0.345945
[epoch20, step1674]: loss 0.431188
[epoch20, step1675]: loss 0.397317
[epoch20, step1676]: loss 0.410929
[epoch20, step1677]: loss 0.495404
[epoch20, step1678]: loss 0.438286
[epoch20, step1679]: loss 0.485576
[epoch20, step1680]: loss 0.711208
[epoch20, step1681]: loss 0.411593
[epoch20, step1682]: loss 0.323172
[epoch20, step1683]: loss 0.551746
[epoch20, step1684]: loss 0.268055
[epoch20, step1685]: loss 0.410608
[epoch20, step1686]: loss 0.230929
[epoch20, step1687]: loss 0.582721
[epoch20, step1688]: loss 0.499833
[epoch20, step1689]: loss 0.432401
[epoch20, step1690]: loss 0.220104
[epoch20, step1691]: loss 0.433548
[epoch20, step1692]: loss 0.419561
[epoch20, step1693]: loss 0.488881
[epoch20, step1694]: loss 0.224797
[epoch20, step1695]: loss 0.293446
[epoch20, step1696]: loss 0.146463
[epoch20, step1697]: loss 0.612763
[epoch20, step1698]: loss 0.520376
[epoch20, step1699]: loss 0.350532
[epoch20, step1700]: loss 0.615962
[epoch20, step1701]: loss 0.453043
[epoch20, step1702]: loss 0.294729
[epoch20, step1703]: loss 0.697437
[epoch20, step1704]: loss 0.353219
[epoch20, step1705]: loss 0.456344
[epoch20, step1706]: loss 0.576510
[epoch20, step1707]: loss 0.319876
[epoch20, step1708]: loss 0.357400
[epoch20, step1709]: loss 0.459635
[epoch20, step1710]: loss 0.546004
[epoch20, step1711]: loss 0.545511
[epoch20, step1712]: loss 0.529292
[epoch20, step1713]: loss 0.412124
[epoch20, step1714]: loss 0.398345
[epoch20, step1715]: loss 0.238635
[epoch20, step1716]: loss 0.544224
[epoch20, step1717]: loss 0.571789
[epoch20, step1718]: loss 0.589567
[epoch20, step1719]: loss 0.446844
[epoch20, step1720]: loss 0.682140
[epoch20, step1721]: loss 0.123128
[epoch20, step1722]: loss 0.144496
[epoch20, step1723]: loss 0.584357
[epoch20, step1724]: loss 0.304074
[epoch20, step1725]: loss 0.577265
[epoch20, step1726]: loss 0.381272
[epoch20, step1727]: loss 0.541433
[epoch20, step1728]: loss 0.632364
[epoch20, step1729]: loss 0.693281
[epoch20, step1730]: loss 0.483225
[epoch20, step1731]: loss 0.424120
[epoch20, step1732]: loss 0.661756
[epoch20, step1733]: loss 0.393446
[epoch20, step1734]: loss 0.447779
[epoch20, step1735]: loss 0.514988
[epoch20, step1736]: loss 0.528331
[epoch20, step1737]: loss 0.448033
[epoch20, step1738]: loss 0.553664
[epoch20, step1739]: loss 0.627133
[epoch20, step1740]: loss 0.480906
[epoch20, step1741]: loss 0.422089
[epoch20, step1742]: loss 0.453933
[epoch20, step1743]: loss 0.634939
[epoch20, step1744]: loss 0.527227
[epoch20, step1745]: loss 0.479781
[epoch20, step1746]: loss 0.433655
[epoch20, step1747]: loss 0.540480
[epoch20, step1748]: loss 0.545365
[epoch20, step1749]: loss 0.464633
[epoch20, step1750]: loss 0.366787
[epoch20, step1751]: loss 0.432471
[epoch20, step1752]: loss 0.499234
[epoch20, step1753]: loss 0.496739
[epoch20, step1754]: loss 0.474730
[epoch20, step1755]: loss 0.396273
[epoch20, step1756]: loss 0.709299
[epoch20, step1757]: loss 0.736375
[epoch20, step1758]: loss 0.480511
[epoch20, step1759]: loss 0.632507
[epoch20, step1760]: loss 0.401818
[epoch20, step1761]: loss 0.563757
[epoch20, step1762]: loss 0.471536
[epoch20, step1763]: loss 0.568273
[epoch20, step1764]: loss 0.556113
[epoch20, step1765]: loss 0.631496
[epoch20, step1766]: loss 0.540582
[epoch20, step1767]: loss 0.421225
[epoch20, step1768]: loss 0.220296
[epoch20, step1769]: loss 0.455710
[epoch20, step1770]: loss 0.451141
[epoch20, step1771]: loss 0.513898
[epoch20, step1772]: loss 0.457439
[epoch20, step1773]: loss 0.278907
[epoch20, step1774]: loss 0.367320
[epoch20, step1775]: loss 0.329753
[epoch20, step1776]: loss 0.469639
[epoch20, step1777]: loss 0.542956
[epoch20, step1778]: loss 0.438555
[epoch20, step1779]: loss 0.554693
[epoch20, step1780]: loss 0.539989
[epoch20, step1781]: loss 0.461733
[epoch20, step1782]: loss 0.476640
[epoch20, step1783]: loss 0.386549
[epoch20, step1784]: loss 0.580879
[epoch20, step1785]: loss 0.220052
[epoch20, step1786]: loss 0.413099
[epoch20, step1787]: loss 0.547769
[epoch20, step1788]: loss 0.531620
[epoch20, step1789]: loss 0.454711
[epoch20, step1790]: loss 0.344264
[epoch20, step1791]: loss 0.462503
[epoch20, step1792]: loss 0.519641
[epoch20, step1793]: loss 0.450640
[epoch20, step1794]: loss 0.427057
[epoch20, step1795]: loss 0.668309
[epoch20, step1796]: loss 0.415008
[epoch20, step1797]: loss 0.444019
[epoch20, step1798]: loss 0.224480
[epoch20, step1799]: loss 0.249879
[epoch20, step1800]: loss 0.537308
[epoch20, step1801]: loss 0.505356
[epoch20, step1802]: loss 0.631966
[epoch20, step1803]: loss 0.307428
[epoch20, step1804]: loss 0.420424
[epoch20, step1805]: loss 0.581228
[epoch20, step1806]: loss 0.440096
[epoch20, step1807]: loss 0.657266
[epoch20, step1808]: loss 0.431303
[epoch20, step1809]: loss 0.320139
[epoch20, step1810]: loss 0.615103
[epoch20, step1811]: loss 0.361636
[epoch20, step1812]: loss 0.515540
[epoch20, step1813]: loss 0.341953
[epoch20, step1814]: loss 0.394580
[epoch20, step1815]: loss 0.551111
[epoch20, step1816]: loss 0.386120
[epoch20, step1817]: loss 0.482487
[epoch20, step1818]: loss 0.611562
[epoch20, step1819]: loss 0.536336
[epoch20, step1820]: loss 0.598280
[epoch20, step1821]: loss 0.332347
[epoch20, step1822]: loss 0.509484
[epoch20, step1823]: loss 0.332194
[epoch20, step1824]: loss 0.417945
[epoch20, step1825]: loss 0.545050
[epoch20, step1826]: loss 0.298484
[epoch20, step1827]: loss 0.507533
[epoch20, step1828]: loss 0.510439
[epoch20, step1829]: loss 0.498416
[epoch20, step1830]: loss 0.702644
[epoch20, step1831]: loss 0.423810
[epoch20, step1832]: loss 0.237944
[epoch20, step1833]: loss 0.412431
[epoch20, step1834]: loss 0.428410
[epoch20, step1835]: loss 0.417552
[epoch20, step1836]: loss 0.404783
[epoch20, step1837]: loss 0.488513
[epoch20, step1838]: loss 0.470647
[epoch20, step1839]: loss 0.495494
[epoch20, step1840]: loss 0.659867
[epoch20, step1841]: loss 0.323234
[epoch20, step1842]: loss 0.581633
[epoch20, step1843]: loss 0.465513
[epoch20, step1844]: loss 0.297018
[epoch20, step1845]: loss 0.547534
[epoch20, step1846]: loss 0.715590
[epoch20, step1847]: loss 0.422699
[epoch20, step1848]: loss 0.372214
[epoch20, step1849]: loss 0.437022
[epoch20, step1850]: loss 0.350429
[epoch20, step1851]: loss 0.651267
[epoch20, step1852]: loss 0.506587
[epoch20, step1853]: loss 0.686529
[epoch20, step1854]: loss 0.429825
[epoch20, step1855]: loss 0.257947
[epoch20, step1856]: loss 0.520336
[epoch20, step1857]: loss 0.437057
[epoch20, step1858]: loss 0.501779
[epoch20, step1859]: loss 0.219949
[epoch20, step1860]: loss 0.647847
[epoch20, step1861]: loss 0.398989
[epoch20, step1862]: loss 0.628820
[epoch20, step1863]: loss 0.501601
[epoch20, step1864]: loss 0.413653
[epoch20, step1865]: loss 0.477195
[epoch20, step1866]: loss 0.691101
[epoch20, step1867]: loss 0.580183
[epoch20, step1868]: loss 0.398224
[epoch20, step1869]: loss 0.564993
[epoch20, step1870]: loss 0.505091
[epoch20, step1871]: loss 0.509929
[epoch20, step1872]: loss 0.545531
[epoch20, step1873]: loss 0.417219
[epoch20, step1874]: loss 0.543694
[epoch20, step1875]: loss 0.441151
[epoch20, step1876]: loss 0.465046
[epoch20, step1877]: loss 0.528821
[epoch20, step1878]: loss 0.527223
[epoch20, step1879]: loss 0.655619
[epoch20, step1880]: loss 0.519790
[epoch20, step1881]: loss 0.634008
[epoch20, step1882]: loss 0.232330
[epoch20, step1883]: loss 0.571412
[epoch20, step1884]: loss 0.565656
[epoch20, step1885]: loss 0.542967
[epoch20, step1886]: loss 0.651463
[epoch20, step1887]: loss 0.436054
[epoch20, step1888]: loss 0.425358
[epoch20, step1889]: loss 0.532371
[epoch20, step1890]: loss 0.490777
[epoch20, step1891]: loss 0.328037
[epoch20, step1892]: loss 0.323554
[epoch20, step1893]: loss 0.140182
[epoch20, step1894]: loss 0.517440
[epoch20, step1895]: loss 0.427264
[epoch20, step1896]: loss 0.536962
[epoch20, step1897]: loss 0.353893
[epoch20, step1898]: loss 0.318794
[epoch20, step1899]: loss 0.635251
[epoch20, step1900]: loss 0.262664
[epoch20, step1901]: loss 0.095335
[epoch20, step1902]: loss 0.562857
[epoch20, step1903]: loss 0.467439
[epoch20, step1904]: loss 0.337105
[epoch20, step1905]: loss 0.537611
[epoch20, step1906]: loss 0.566788
[epoch20, step1907]: loss 0.605035
[epoch20, step1908]: loss 0.353048
[epoch20, step1909]: loss 0.364241
[epoch20, step1910]: loss 0.372221
[epoch20, step1911]: loss 0.265631
[epoch20, step1912]: loss 0.684109
[epoch20, step1913]: loss 0.289651
[epoch20, step1914]: loss 0.132889
[epoch20, step1915]: loss 0.578940
[epoch20, step1916]: loss 0.527593
[epoch20, step1917]: loss 0.581947
[epoch20, step1918]: loss 0.390547
[epoch20, step1919]: loss 0.434532
[epoch20, step1920]: loss 0.501738
[epoch20, step1921]: loss 0.425797
[epoch20, step1922]: loss 0.526684
[epoch20, step1923]: loss 0.543357
[epoch20, step1924]: loss 0.281479
[epoch20, step1925]: loss 0.470866
[epoch20, step1926]: loss 0.368465
[epoch20, step1927]: loss 0.572720
[epoch20, step1928]: loss 0.540667
[epoch20, step1929]: loss 0.640314
[epoch20, step1930]: loss 0.550218
[epoch20, step1931]: loss 0.307223
[epoch20, step1932]: loss 0.433189
[epoch20, step1933]: loss 0.484334
[epoch20, step1934]: loss 0.599962
[epoch20, step1935]: loss 0.598493
[epoch20, step1936]: loss 0.686796
[epoch20, step1937]: loss 0.432667
[epoch20, step1938]: loss 0.535622
[epoch20, step1939]: loss 0.602973
[epoch20, step1940]: loss 0.442142
[epoch20, step1941]: loss 0.633403
[epoch20, step1942]: loss 0.306096
[epoch20, step1943]: loss 0.492123
[epoch20, step1944]: loss 0.461030
[epoch20, step1945]: loss 0.533041
[epoch20, step1946]: loss 0.344834
[epoch20, step1947]: loss 0.397682
[epoch20, step1948]: loss 0.370581
[epoch20, step1949]: loss 0.421348
[epoch20, step1950]: loss 0.572560
[epoch20, step1951]: loss 0.593288
[epoch20, step1952]: loss 0.488601
[epoch20, step1953]: loss 0.473731
[epoch20, step1954]: loss 0.504369
[epoch20, step1955]: loss 0.208213
[epoch20, step1956]: loss 0.317802
[epoch20, step1957]: loss 0.105777
[epoch20, step1958]: loss 0.422324
[epoch20, step1959]: loss 0.265975
[epoch20, step1960]: loss 0.664329
[epoch20, step1961]: loss 0.246694
[epoch20, step1962]: loss 0.434563
[epoch20, step1963]: loss 0.299222
[epoch20, step1964]: loss 0.478837
[epoch20, step1965]: loss 0.373499
[epoch20, step1966]: loss 0.458704
[epoch20, step1967]: loss 0.516680
[epoch20, step1968]: loss 0.506529
[epoch20, step1969]: loss 0.583547
[epoch20, step1970]: loss 0.432319
[epoch20, step1971]: loss 0.595716
[epoch20, step1972]: loss 0.338737
[epoch20, step1973]: loss 0.362121
[epoch20, step1974]: loss 0.452369
[epoch20, step1975]: loss 0.368430
[epoch20, step1976]: loss 0.409388
[epoch20, step1977]: loss 0.632133
[epoch20, step1978]: loss 0.564012
[epoch20, step1979]: loss 0.471987
[epoch20, step1980]: loss 0.399665
[epoch20, step1981]: loss 0.342636
[epoch20, step1982]: loss 0.230359
[epoch20, step1983]: loss 0.413426
[epoch20, step1984]: loss 0.422015
[epoch20, step1985]: loss 0.579569
[epoch20, step1986]: loss 0.349358
[epoch20, step1987]: loss 0.453879
[epoch20, step1988]: loss 0.317267
[epoch20, step1989]: loss 0.456231
[epoch20, step1990]: loss 0.119751
[epoch20, step1991]: loss 0.323112
[epoch20, step1992]: loss 0.404909
[epoch20, step1993]: loss 0.562006
[epoch20, step1994]: loss 0.438822
[epoch20, step1995]: loss 0.355843
[epoch20, step1996]: loss 0.567017
[epoch20, step1997]: loss 0.337648
[epoch20, step1998]: loss 0.460321
[epoch20, step1999]: loss 0.504765
[epoch20, step2000]: loss 0.380983
[epoch20, step2001]: loss 0.213825
[epoch20, step2002]: loss 0.490766
[epoch20, step2003]: loss 0.396143
[epoch20, step2004]: loss 0.488257
[epoch20, step2005]: loss 0.401639
[epoch20, step2006]: loss 0.439245
[epoch20, step2007]: loss 0.577770
[epoch20, step2008]: loss 0.428431
[epoch20, step2009]: loss 0.491149
[epoch20, step2010]: loss 0.457388
[epoch20, step2011]: loss 0.401295
[epoch20, step2012]: loss 0.434837
[epoch20, step2013]: loss 0.839149
[epoch20, step2014]: loss 0.369710
[epoch20, step2015]: loss 0.496259
[epoch20, step2016]: loss 0.455291
[epoch20, step2017]: loss 0.621011
[epoch20, step2018]: loss 0.545309
[epoch20, step2019]: loss 0.379162
[epoch20, step2020]: loss 0.522907
[epoch20, step2021]: loss 0.427640
[epoch20, step2022]: loss 0.404878
[epoch20, step2023]: loss 0.325121
[epoch20, step2024]: loss 0.279538
[epoch20, step2025]: loss 0.519659
[epoch20, step2026]: loss 0.416882
[epoch20, step2027]: loss 0.586185
[epoch20, step2028]: loss 0.505894
[epoch20, step2029]: loss 0.639560
[epoch20, step2030]: loss 0.461144
[epoch20, step2031]: loss 0.361938
[epoch20, step2032]: loss 0.380133
[epoch20, step2033]: loss 0.512434
[epoch20, step2034]: loss 0.474709
[epoch20, step2035]: loss 0.531329
[epoch20, step2036]: loss 0.435072
[epoch20, step2037]: loss 0.599436
[epoch20, step2038]: loss 0.170597
[epoch20, step2039]: loss 0.525693
[epoch20, step2040]: loss 0.266891
[epoch20, step2041]: loss 0.457284
[epoch20, step2042]: loss 0.345518
[epoch20, step2043]: loss 0.511689
[epoch20, step2044]: loss 0.494991
[epoch20, step2045]: loss 0.270391
[epoch20, step2046]: loss 0.378542
[epoch20, step2047]: loss 0.245330
[epoch20, step2048]: loss 0.250586
[epoch20, step2049]: loss 0.448097
[epoch20, step2050]: loss 0.612488
[epoch20, step2051]: loss 0.372571
[epoch20, step2052]: loss 0.408374
[epoch20, step2053]: loss 0.381788
[epoch20, step2054]: loss 0.629255
[epoch20, step2055]: loss 0.497030
[epoch20, step2056]: loss 0.460850
[epoch20, step2057]: loss 0.381173
[epoch20, step2058]: loss 0.435130
[epoch20, step2059]: loss 0.335155
[epoch20, step2060]: loss 0.379982
[epoch20, step2061]: loss 0.589091
[epoch20, step2062]: loss 0.350282
[epoch20, step2063]: loss 0.370798
[epoch20, step2064]: loss 0.581829
[epoch20, step2065]: loss 0.529833
[epoch20, step2066]: loss 0.456184
[epoch20, step2067]: loss 0.369687
[epoch20, step2068]: loss 0.220327
[epoch20, step2069]: loss 0.451954
[epoch20, step2070]: loss 0.517633
[epoch20, step2071]: loss 0.618354
[epoch20, step2072]: loss 0.425810
[epoch20, step2073]: loss 0.522050
[epoch20, step2074]: loss 0.500205
[epoch20, step2075]: loss 0.485046
[epoch20, step2076]: loss 0.379757
[epoch20, step2077]: loss 0.621097
[epoch20, step2078]: loss 0.422133
[epoch20, step2079]: loss 0.462658
[epoch20, step2080]: loss 0.352044
[epoch20, step2081]: loss 0.665893
[epoch20, step2082]: loss 0.512688
[epoch20, step2083]: loss 0.347557
[epoch20, step2084]: loss 0.458565
[epoch20, step2085]: loss 0.619460
[epoch20, step2086]: loss 0.419860
[epoch20, step2087]: loss 0.600101
[epoch20, step2088]: loss 0.564441
[epoch20, step2089]: loss 0.535982
[epoch20, step2090]: loss 0.446926
[epoch20, step2091]: loss 0.452926
[epoch20, step2092]: loss 0.367726
[epoch20, step2093]: loss 0.324049
[epoch20, step2094]: loss 0.553627
[epoch20, step2095]: loss 0.652761
[epoch20, step2096]: loss 0.441147
[epoch20, step2097]: loss 0.347796
[epoch20, step2098]: loss 0.642702
[epoch20, step2099]: loss 0.466514
[epoch20, step2100]: loss 0.389259
[epoch20, step2101]: loss 0.521494
[epoch20, step2102]: loss 0.504017
[epoch20, step2103]: loss 0.538981
[epoch20, step2104]: loss 0.458977
[epoch20, step2105]: loss 0.585411
[epoch20, step2106]: loss 0.552564
[epoch20, step2107]: loss 0.575224
[epoch20, step2108]: loss 0.556018
[epoch20, step2109]: loss 0.633549
[epoch20, step2110]: loss 0.516173
[epoch20, step2111]: loss 0.368240
[epoch20, step2112]: loss 0.380800
[epoch20, step2113]: loss 0.464481
[epoch20, step2114]: loss 0.455878
[epoch20, step2115]: loss 0.340259
[epoch20, step2116]: loss 0.358904
[epoch20, step2117]: loss 0.660143
[epoch20, step2118]: loss 0.438660
[epoch20, step2119]: loss 0.377369
[epoch20, step2120]: loss 0.360468
[epoch20, step2121]: loss 0.357839
[epoch20, step2122]: loss 0.451415
[epoch20, step2123]: loss 0.439126
[epoch20, step2124]: loss 0.608337
[epoch20, step2125]: loss 0.382465
[epoch20, step2126]: loss 0.502881
[epoch20, step2127]: loss 0.405062
[epoch20, step2128]: loss 0.454486
[epoch20, step2129]: loss 0.522504
[epoch20, step2130]: loss 0.429215
[epoch20, step2131]: loss 0.593913
[epoch20, step2132]: loss 0.655035
[epoch20, step2133]: loss 0.587615
[epoch20, step2134]: loss 0.683742
[epoch20, step2135]: loss 0.402796
[epoch20, step2136]: loss 0.259403
[epoch20, step2137]: loss 0.630914
[epoch20, step2138]: loss 0.632306
[epoch20, step2139]: loss 0.362895
[epoch20, step2140]: loss 0.554932
[epoch20, step2141]: loss 0.571710
[epoch20, step2142]: loss 0.487514
[epoch20, step2143]: loss 0.440117
[epoch20, step2144]: loss 0.397624
[epoch20, step2145]: loss 0.523586
[epoch20, step2146]: loss 0.511518
[epoch20, step2147]: loss 0.641854
[epoch20, step2148]: loss 0.421040
[epoch20, step2149]: loss 0.305574
[epoch20, step2150]: loss 0.417786
[epoch20, step2151]: loss 0.493600
[epoch20, step2152]: loss 0.311167
[epoch20, step2153]: loss 0.478493
[epoch20, step2154]: loss 0.415419
[epoch20, step2155]: loss 0.354172
[epoch20, step2156]: loss 0.685587
[epoch20, step2157]: loss 0.483908
[epoch20, step2158]: loss 0.618204
[epoch20, step2159]: loss 0.399386
[epoch20, step2160]: loss 0.363758
[epoch20, step2161]: loss 0.525370
[epoch20, step2162]: loss 0.442707
[epoch20, step2163]: loss 0.309172
[epoch20, step2164]: loss 0.337211
[epoch20, step2165]: loss 0.487876
[epoch20, step2166]: loss 0.592808
[epoch20, step2167]: loss 0.499769
[epoch20, step2168]: loss 0.388401
[epoch20, step2169]: loss 0.364514
[epoch20, step2170]: loss 0.539681
[epoch20, step2171]: loss 0.517212
[epoch20, step2172]: loss 0.417974
[epoch20, step2173]: loss 0.561881
[epoch20, step2174]: loss 0.609101
[epoch20, step2175]: loss 0.649759
[epoch20, step2176]: loss 0.521878
[epoch20, step2177]: loss 0.358167
[epoch20, step2178]: loss 0.490819
[epoch20, step2179]: loss 0.385660
[epoch20, step2180]: loss 0.568594
[epoch20, step2181]: loss 0.477983
[epoch20, step2182]: loss 0.369463
[epoch20, step2183]: loss 0.486905
[epoch20, step2184]: loss 0.646334
[epoch20, step2185]: loss 0.484097
[epoch20, step2186]: loss 0.151996
[epoch20, step2187]: loss 0.565686
[epoch20, step2188]: loss 0.456194
[epoch20, step2189]: loss 0.651781
[epoch20, step2190]: loss 0.572826
[epoch20, step2191]: loss 0.523535
[epoch20, step2192]: loss 0.576833
[epoch20, step2193]: loss 0.431247
[epoch20, step2194]: loss 0.422994
[epoch20, step2195]: loss 0.478445
[epoch20, step2196]: loss 0.457413
[epoch20, step2197]: loss 0.545889
[epoch20, step2198]: loss 0.627943
[epoch20, step2199]: loss 0.416787
[epoch20, step2200]: loss 0.477763
[epoch20, step2201]: loss 0.409545
[epoch20, step2202]: loss 0.339123
[epoch20, step2203]: loss 0.582678
[epoch20, step2204]: loss 0.593360
[epoch20, step2205]: loss 0.511964
[epoch20, step2206]: loss 0.666215
[epoch20, step2207]: loss 0.535625
[epoch20, step2208]: loss 0.609527
[epoch20, step2209]: loss 0.365383
[epoch20, step2210]: loss 0.299342
[epoch20, step2211]: loss 0.569986
[epoch20, step2212]: loss 0.499953
[epoch20, step2213]: loss 0.460419
[epoch20, step2214]: loss 0.472632
[epoch20, step2215]: loss 0.391932
[epoch20, step2216]: loss 0.389293
[epoch20, step2217]: loss 0.374562
[epoch20, step2218]: loss 0.415361
[epoch20, step2219]: loss 0.408401
[epoch20, step2220]: loss 0.431869
[epoch20, step2221]: loss 0.684408
[epoch20, step2222]: loss 0.408347
[epoch20, step2223]: loss 0.455091
[epoch20, step2224]: loss 0.587969
[epoch20, step2225]: loss 0.381732
[epoch20, step2226]: loss 0.469302
[epoch20, step2227]: loss 0.486792
[epoch20, step2228]: loss 0.707802
[epoch20, step2229]: loss 0.521861
[epoch20, step2230]: loss 0.435358
[epoch20, step2231]: loss 0.471887
[epoch20, step2232]: loss 0.447977
[epoch20, step2233]: loss 0.349048
[epoch20, step2234]: loss 0.441017
[epoch20, step2235]: loss 0.389399
[epoch20, step2236]: loss 0.517861
[epoch20, step2237]: loss 0.759669
[epoch20, step2238]: loss 0.352365
[epoch20, step2239]: loss 0.501024
[epoch20, step2240]: loss 0.210856
[epoch20, step2241]: loss 0.198773
[epoch20, step2242]: loss 0.458509
[epoch20, step2243]: loss 0.541274
[epoch20, step2244]: loss 0.495335
[epoch20, step2245]: loss 0.452453
[epoch20, step2246]: loss 0.535335
[epoch20, step2247]: loss 0.411081
[epoch20, step2248]: loss 0.619809
[epoch20, step2249]: loss 0.420616
[epoch20, step2250]: loss 0.639855
[epoch20, step2251]: loss 0.517030
[epoch20, step2252]: loss 0.548185
[epoch20, step2253]: loss 0.221891
[epoch20, step2254]: loss 0.530330
[epoch20, step2255]: loss 0.695061
[epoch20, step2256]: loss 0.659053
[epoch20, step2257]: loss 0.474512
[epoch20, step2258]: loss 0.433982
[epoch20, step2259]: loss 0.373982
[epoch20, step2260]: loss 0.266699
[epoch20, step2261]: loss 0.445460
[epoch20, step2262]: loss 0.620546
[epoch20, step2263]: loss 0.221514
[epoch20, step2264]: loss 0.306383
[epoch20, step2265]: loss 0.528601
[epoch20, step2266]: loss 0.493999
[epoch20, step2267]: loss 0.316236
[epoch20, step2268]: loss 0.560961
[epoch20, step2269]: loss 0.374599
[epoch20, step2270]: loss 0.411780
[epoch20, step2271]: loss 0.693693
[epoch20, step2272]: loss 0.338071
[epoch20, step2273]: loss 0.729879
[epoch20, step2274]: loss 0.413900
[epoch20, step2275]: loss 0.617729
[epoch20, step2276]: loss 0.617043
[epoch20, step2277]: loss 0.540170
[epoch20, step2278]: loss 0.543194
[epoch20, step2279]: loss 0.403099
[epoch20, step2280]: loss 0.257933
[epoch20, step2281]: loss 0.426554
[epoch20, step2282]: loss 0.569600
[epoch20, step2283]: loss 0.581871
[epoch20, step2284]: loss 0.495372
[epoch20, step2285]: loss 0.256407
[epoch20, step2286]: loss 0.391816
[epoch20, step2287]: loss 0.185353
[epoch20, step2288]: loss 0.247175
[epoch20, step2289]: loss 0.665797
[epoch20, step2290]: loss 0.382212
[epoch20, step2291]: loss 0.340311
[epoch20, step2292]: loss 0.632870
[epoch20, step2293]: loss 0.364686
[epoch20, step2294]: loss 0.495449
[epoch20, step2295]: loss 0.464227
[epoch20, step2296]: loss 0.360528
[epoch20, step2297]: loss 0.259878
[epoch20, step2298]: loss 0.553026
[epoch20, step2299]: loss 0.369694
[epoch20, step2300]: loss 0.355223
[epoch20, step2301]: loss 0.492765
[epoch20, step2302]: loss 0.410072
[epoch20, step2303]: loss 0.219091
[epoch20, step2304]: loss 0.331415
[epoch20, step2305]: loss 0.260200
[epoch20, step2306]: loss 0.591949
[epoch20, step2307]: loss 0.560705
[epoch20, step2308]: loss 0.352202
[epoch20, step2309]: loss 0.474677
[epoch20, step2310]: loss 0.423214
[epoch20, step2311]: loss 0.323108
[epoch20, step2312]: loss 0.522714
[epoch20, step2313]: loss 0.588336
[epoch20, step2314]: loss 0.669375
[epoch20, step2315]: loss 0.376527
[epoch20, step2316]: loss 0.487699
[epoch20, step2317]: loss 0.313157
[epoch20, step2318]: loss 0.482736
[epoch20, step2319]: loss 0.429495
[epoch20, step2320]: loss 0.414408
[epoch20, step2321]: loss 0.421830
[epoch20, step2322]: loss 0.443719
[epoch20, step2323]: loss 0.547400
[epoch20, step2324]: loss 0.542228
[epoch20, step2325]: loss 0.443551
[epoch20, step2326]: loss 0.424039
[epoch20, step2327]: loss 0.382235
[epoch20, step2328]: loss 0.536733
[epoch20, step2329]: loss 0.473409
[epoch20, step2330]: loss 0.605599
[epoch20, step2331]: loss 0.419465
[epoch20, step2332]: loss 0.427544
[epoch20, step2333]: loss 0.478764
[epoch20, step2334]: loss 0.550578
[epoch20, step2335]: loss 0.581073
[epoch20, step2336]: loss 0.435300
[epoch20, step2337]: loss 0.438738
[epoch20, step2338]: loss 0.341023
[epoch20, step2339]: loss 0.420007
[epoch20, step2340]: loss 0.660137
[epoch20, step2341]: loss 0.399769
[epoch20, step2342]: loss 0.223679
[epoch20, step2343]: loss 0.458032
[epoch20, step2344]: loss 0.572422
[epoch20, step2345]: loss 0.698004
[epoch20, step2346]: loss 0.356409
[epoch20, step2347]: loss 0.711422
[epoch20, step2348]: loss 0.396130
[epoch20, step2349]: loss 0.695190
[epoch20, step2350]: loss 0.310031
[epoch20, step2351]: loss 0.309555
[epoch20, step2352]: loss 0.521669
[epoch20, step2353]: loss 0.519825
[epoch20, step2354]: loss 0.609663
[epoch20, step2355]: loss 0.351301
[epoch20, step2356]: loss 0.610560
[epoch20, step2357]: loss 0.611403
[epoch20, step2358]: loss 0.281792
[epoch20, step2359]: loss 0.343792
[epoch20, step2360]: loss 0.404602
[epoch20, step2361]: loss 0.442097
[epoch20, step2362]: loss 0.496246
[epoch20, step2363]: loss 0.315575
[epoch20, step2364]: loss 0.642745
[epoch20, step2365]: loss 0.347493
[epoch20, step2366]: loss 0.446992
[epoch20, step2367]: loss 0.554514
[epoch20, step2368]: loss 0.329913
[epoch20, step2369]: loss 0.280938
[epoch20, step2370]: loss 0.467870
[epoch20, step2371]: loss 0.457867
[epoch20, step2372]: loss 0.359732
[epoch20, step2373]: loss 0.362091
[epoch20, step2374]: loss 0.583388
[epoch20, step2375]: loss 0.372121
[epoch20, step2376]: loss 0.434424
[epoch20, step2377]: loss 0.270719
[epoch20, step2378]: loss 0.251896
[epoch20, step2379]: loss 0.630431
[epoch20, step2380]: loss 0.325323
[epoch20, step2381]: loss 0.633977
[epoch20, step2382]: loss 0.479061
[epoch20, step2383]: loss 0.658222
[epoch20, step2384]: loss 0.394532
[epoch20, step2385]: loss 0.414651
[epoch20, step2386]: loss 0.358834
[epoch20, step2387]: loss 0.319627
[epoch20, step2388]: loss 0.511590
[epoch20, step2389]: loss 0.417137
[epoch20, step2390]: loss 0.379068
[epoch20, step2391]: loss 0.364655
[epoch20, step2392]: loss 0.546679
[epoch20, step2393]: loss 0.416798
[epoch20, step2394]: loss 0.618189
[epoch20, step2395]: loss 0.562064
[epoch20, step2396]: loss 0.412969
[epoch20, step2397]: loss 0.140081
[epoch20, step2398]: loss 0.651688
[epoch20, step2399]: loss 0.334020
[epoch20, step2400]: loss 0.339248
[epoch20, step2401]: loss 0.543266
[epoch20, step2402]: loss 0.649530
[epoch20, step2403]: loss 0.483168
[epoch20, step2404]: loss 0.543436
[epoch20, step2405]: loss 0.435160
[epoch20, step2406]: loss 0.417721
[epoch20, step2407]: loss 0.352325
[epoch20, step2408]: loss 0.478298
[epoch20, step2409]: loss 0.407314
[epoch20, step2410]: loss 0.562744
[epoch20, step2411]: loss 0.354135
[epoch20, step2412]: loss 0.369660
[epoch20, step2413]: loss 0.495839
[epoch20, step2414]: loss 0.484632
[epoch20, step2415]: loss 0.418315
[epoch20, step2416]: loss 0.405818
[epoch20, step2417]: loss 0.715578
[epoch20, step2418]: loss 0.482256
[epoch20, step2419]: loss 0.485045
[epoch20, step2420]: loss 0.204872
[epoch20, step2421]: loss 0.397477
[epoch20, step2422]: loss 0.690646
[epoch20, step2423]: loss 0.247701
[epoch20, step2424]: loss 0.477848
[epoch20, step2425]: loss 0.437853
[epoch20, step2426]: loss 0.501618
[epoch20, step2427]: loss 0.540783
[epoch20, step2428]: loss 0.456386
[epoch20, step2429]: loss 0.650192
[epoch20, step2430]: loss 0.597231
[epoch20, step2431]: loss 0.489161
[epoch20, step2432]: loss 0.481718
[epoch20, step2433]: loss 0.374810
[epoch20, step2434]: loss 0.601718
[epoch20, step2435]: loss 0.451617
[epoch20, step2436]: loss 0.454371
[epoch20, step2437]: loss 0.601505
[epoch20, step2438]: loss 0.492571
[epoch20, step2439]: loss 0.565342
[epoch20, step2440]: loss 0.504332
[epoch20, step2441]: loss 0.350008
[epoch20, step2442]: loss 0.441530
[epoch20, step2443]: loss 0.604853
[epoch20, step2444]: loss 0.554037
[epoch20, step2445]: loss 0.612384
[epoch20, step2446]: loss 0.615351
[epoch20, step2447]: loss 0.387334
[epoch20, step2448]: loss 0.471402
[epoch20, step2449]: loss 0.508541
[epoch20, step2450]: loss 0.363161
[epoch20, step2451]: loss 0.509070
[epoch20, step2452]: loss 0.505851
[epoch20, step2453]: loss 0.552531
[epoch20, step2454]: loss 0.459573
[epoch20, step2455]: loss 0.584956
[epoch20, step2456]: loss 0.688367
[epoch20, step2457]: loss 0.507514
[epoch20, step2458]: loss 0.495337
[epoch20, step2459]: loss 0.580913
[epoch20, step2460]: loss 0.608813
[epoch20, step2461]: loss 0.590640
[epoch20, step2462]: loss 0.660586
[epoch20, step2463]: loss 0.240693
[epoch20, step2464]: loss 0.343481
[epoch20, step2465]: loss 0.523052
[epoch20, step2466]: loss 0.408288
[epoch20, step2467]: loss 0.523614
[epoch20, step2468]: loss 0.490141
[epoch20, step2469]: loss 0.467652
[epoch20, step2470]: loss 0.557058
[epoch20, step2471]: loss 0.606105
[epoch20, step2472]: loss 0.701225
[epoch20, step2473]: loss 0.423629
[epoch20, step2474]: loss 0.596276
[epoch20, step2475]: loss 0.615549
[epoch20, step2476]: loss 0.441940
[epoch20, step2477]: loss 0.312018
[epoch20, step2478]: loss 0.189951
[epoch20, step2479]: loss 0.487185
[epoch20, step2480]: loss 0.361933
[epoch20, step2481]: loss 0.481153
[epoch20, step2482]: loss 0.313738
[epoch20, step2483]: loss 0.142911
[epoch20, step2484]: loss 0.607687
[epoch20, step2485]: loss 0.279935
[epoch20, step2486]: loss 0.550666
[epoch20, step2487]: loss 0.317247
[epoch20, step2488]: loss 0.386015
[epoch20, step2489]: loss 0.359687
[epoch20, step2490]: loss 0.563109
[epoch20, step2491]: loss 0.639382
[epoch20, step2492]: loss 0.336171
[epoch20, step2493]: loss 0.562301
[epoch20, step2494]: loss 0.158480
[epoch20, step2495]: loss 0.608079
[epoch20, step2496]: loss 0.672239
[epoch20, step2497]: loss 0.664986
[epoch20, step2498]: loss 0.457025
[epoch20, step2499]: loss 0.542540
[epoch20, step2500]: loss 0.536171
[epoch20, step2501]: loss 0.563788
[epoch20, step2502]: loss 0.471677
[epoch20, step2503]: loss 0.413875
[epoch20, step2504]: loss 0.253649
[epoch20, step2505]: loss 0.319818
[epoch20, step2506]: loss 0.539557
[epoch20, step2507]: loss 0.492507
[epoch20, step2508]: loss 0.603301
[epoch20, step2509]: loss 0.572929
[epoch20, step2510]: loss 0.536876
[epoch20, step2511]: loss 0.616354
[epoch20, step2512]: loss 0.609738
[epoch20, step2513]: loss 0.221125
[epoch20, step2514]: loss 0.473735
[epoch20, step2515]: loss 0.602578
[epoch20, step2516]: loss 0.554010
[epoch20, step2517]: loss 0.503984
[epoch20, step2518]: loss 0.474839
[epoch20, step2519]: loss 0.497451
[epoch20, step2520]: loss 0.297012
[epoch20, step2521]: loss 0.473198
[epoch20, step2522]: loss 0.590888
[epoch20, step2523]: loss 0.482961
[epoch20, step2524]: loss 0.645360
[epoch20, step2525]: loss 0.567706
[epoch20, step2526]: loss 0.399681
[epoch20, step2527]: loss 0.368293
[epoch20, step2528]: loss 0.612952
[epoch20, step2529]: loss 0.418360
[epoch20, step2530]: loss 0.382670
[epoch20, step2531]: loss 0.648866
[epoch20, step2532]: loss 0.519184
[epoch20, step2533]: loss 0.268977
[epoch20, step2534]: loss 0.129484
[epoch20, step2535]: loss 0.446077
[epoch20, step2536]: loss 0.544940
[epoch20, step2537]: loss 0.616022
[epoch20, step2538]: loss 0.280406
[epoch20, step2539]: loss 0.487869
[epoch20, step2540]: loss 0.350781
[epoch20, step2541]: loss 0.625557
[epoch20, step2542]: loss 0.612118
[epoch20, step2543]: loss 0.471621
[epoch20, step2544]: loss 0.140145
[epoch20, step2545]: loss 0.172491
[epoch20, step2546]: loss 0.651891
[epoch20, step2547]: loss 0.399063
[epoch20, step2548]: loss 0.503137
[epoch20, step2549]: loss 0.502052
[epoch20, step2550]: loss 0.454252
[epoch20, step2551]: loss 0.425532
[epoch20, step2552]: loss 0.678066
[epoch20, step2553]: loss 0.441265
[epoch20, step2554]: loss 0.419041
[epoch20, step2555]: loss 0.382398
[epoch20, step2556]: loss 0.297868
[epoch20, step2557]: loss 0.316833
[epoch20, step2558]: loss 0.412441
[epoch20, step2559]: loss 0.565665
[epoch20, step2560]: loss 0.464496
[epoch20, step2561]: loss 0.390591
[epoch20, step2562]: loss 0.693198
[epoch20, step2563]: loss 0.424510
[epoch20, step2564]: loss 0.635134
[epoch20, step2565]: loss 0.428193
[epoch20, step2566]: loss 0.551133
[epoch20, step2567]: loss 0.246128
[epoch20, step2568]: loss 0.495248
[epoch20, step2569]: loss 0.403345
[epoch20, step2570]: loss 0.554898
[epoch20, step2571]: loss 0.320600
[epoch20, step2572]: loss 0.475612
[epoch20, step2573]: loss 0.258627
[epoch20, step2574]: loss 0.564115
[epoch20, step2575]: loss 0.485517
[epoch20, step2576]: loss 0.381211
[epoch20, step2577]: loss 0.499579
[epoch20, step2578]: loss 0.321765
[epoch20, step2579]: loss 0.453459
[epoch20, step2580]: loss 0.131257
[epoch20, step2581]: loss 0.391833
[epoch20, step2582]: loss 0.301585
[epoch20, step2583]: loss 0.392160
[epoch20, step2584]: loss 0.314835
[epoch20, step2585]: loss 0.565772
[epoch20, step2586]: loss 0.344558
[epoch20, step2587]: loss 0.659440
[epoch20, step2588]: loss 0.497950
[epoch20, step2589]: loss 0.448884
[epoch20, step2590]: loss 0.342841
[epoch20, step2591]: loss 0.450780
[epoch20, step2592]: loss 0.242051
[epoch20, step2593]: loss 0.689472
[epoch20, step2594]: loss 0.421179
[epoch20, step2595]: loss 0.299007
[epoch20, step2596]: loss 0.440824
[epoch20, step2597]: loss 0.536506
[epoch20, step2598]: loss 0.389233
[epoch20, step2599]: loss 0.309826
[epoch20, step2600]: loss 0.490869
[epoch20, step2601]: loss 0.326327
[epoch20, step2602]: loss 0.773159
[epoch20, step2603]: loss 0.428429
[epoch20, step2604]: loss 0.636012
[epoch20, step2605]: loss 0.490520
[epoch20, step2606]: loss 0.274151
[epoch20, step2607]: loss 0.331444
[epoch20, step2608]: loss 0.359883
[epoch20, step2609]: loss 0.415015
[epoch20, step2610]: loss 0.409154
[epoch20, step2611]: loss 0.633128
[epoch20, step2612]: loss 0.479522
[epoch20, step2613]: loss 0.446419
[epoch20, step2614]: loss 0.448464
[epoch20, step2615]: loss 0.415857
[epoch20, step2616]: loss 0.340007
[epoch20, step2617]: loss 0.506471
[epoch20, step2618]: loss 0.329513
[epoch20, step2619]: loss 0.615552
[epoch20, step2620]: loss 0.365669
[epoch20, step2621]: loss 0.436795
[epoch20, step2622]: loss 0.381841
[epoch20, step2623]: loss 0.621984
[epoch20, step2624]: loss 0.582861
[epoch20, step2625]: loss 0.540963
[epoch20, step2626]: loss 0.401996
[epoch20, step2627]: loss 0.284368
[epoch20, step2628]: loss 0.617775
[epoch20, step2629]: loss 0.363755
[epoch20, step2630]: loss 0.474423
[epoch20, step2631]: loss 0.400961
[epoch20, step2632]: loss 0.502801
[epoch20, step2633]: loss 0.353141
[epoch20, step2634]: loss 0.355113
[epoch20, step2635]: loss 0.427577
[epoch20, step2636]: loss 0.538259
[epoch20, step2637]: loss 0.516478
[epoch20, step2638]: loss 0.706807
[epoch20, step2639]: loss 0.532524
[epoch20, step2640]: loss 0.455707
[epoch20, step2641]: loss 0.645279
[epoch20, step2642]: loss 0.493409
[epoch20, step2643]: loss 0.311157
[epoch20, step2644]: loss 0.464604
[epoch20, step2645]: loss 0.490588
[epoch20, step2646]: loss 0.459635
[epoch20, step2647]: loss 0.531128
[epoch20, step2648]: loss 0.758629
[epoch20, step2649]: loss 0.662171
[epoch20, step2650]: loss 0.257602
[epoch20, step2651]: loss 0.611666
[epoch20, step2652]: loss 0.678799
[epoch20, step2653]: loss 0.512407
[epoch20, step2654]: loss 0.519394
[epoch20, step2655]: loss 0.371605
[epoch20, step2656]: loss 0.449698
[epoch20, step2657]: loss 0.367997
[epoch20, step2658]: loss 0.598005
[epoch20, step2659]: loss 0.433465
[epoch20, step2660]: loss 0.401994
[epoch20, step2661]: loss 0.393806
[epoch20, step2662]: loss 0.535488
[epoch20, step2663]: loss 0.451499
[epoch20, step2664]: loss 0.646082
[epoch20, step2665]: loss 0.515479
[epoch20, step2666]: loss 0.461358
[epoch20, step2667]: loss 0.543656
[epoch20, step2668]: loss 0.298108
[epoch20, step2669]: loss 0.274952
[epoch20, step2670]: loss 0.501755
[epoch20, step2671]: loss 0.406583
[epoch20, step2672]: loss 0.372500
[epoch20, step2673]: loss 0.397197
[epoch20, step2674]: loss 0.568672
[epoch20, step2675]: loss 0.253613
[epoch20, step2676]: loss 0.427508
[epoch20, step2677]: loss 0.418976
[epoch20, step2678]: loss 0.464725
[epoch20, step2679]: loss 0.328249
[epoch20, step2680]: loss 0.462269
[epoch20, step2681]: loss 0.531430
[epoch20, step2682]: loss 0.579395
[epoch20, step2683]: loss 0.494349
[epoch20, step2684]: loss 0.508872
[epoch20, step2685]: loss 0.524115
[epoch20, step2686]: loss 0.753834
[epoch20, step2687]: loss 0.532991
[epoch20, step2688]: loss 0.442263
[epoch20, step2689]: loss 0.420104
[epoch20, step2690]: loss 0.373361
[epoch20, step2691]: loss 0.595628
[epoch20, step2692]: loss 0.654866
[epoch20, step2693]: loss 0.605048
[epoch20, step2694]: loss 0.557303
[epoch20, step2695]: loss 0.446875
[epoch20, step2696]: loss 0.504470
[epoch20, step2697]: loss 0.525676
[epoch20, step2698]: loss 0.555093
[epoch20, step2699]: loss 0.243388
[epoch20, step2700]: loss 0.705647
[epoch20, step2701]: loss 0.118645
[epoch20, step2702]: loss 0.639694
[epoch20, step2703]: loss 0.485493
[epoch20, step2704]: loss 0.529463
[epoch20, step2705]: loss 0.500730
[epoch20, step2706]: loss 0.248344
[epoch20, step2707]: loss 0.404594
[epoch20, step2708]: loss 0.476623
[epoch20, step2709]: loss 0.379253
[epoch20, step2710]: loss 0.460732
[epoch20, step2711]: loss 0.300558
[epoch20, step2712]: loss 0.613882
[epoch20, step2713]: loss 0.446725
[epoch20, step2714]: loss 0.359280
[epoch20, step2715]: loss 0.247865
[epoch20, step2716]: loss 0.595054
[epoch20, step2717]: loss 0.559218
[epoch20, step2718]: loss 0.580497
[epoch20, step2719]: loss 0.430854
[epoch20, step2720]: loss 0.299144
[epoch20, step2721]: loss 0.495390
[epoch20, step2722]: loss 0.367252
[epoch20, step2723]: loss 0.346508
[epoch20, step2724]: loss 0.356385
[epoch20, step2725]: loss 0.242518
[epoch20, step2726]: loss 0.379528
[epoch20, step2727]: loss 0.469332
[epoch20, step2728]: loss 0.528979
[epoch20, step2729]: loss 0.385750
[epoch20, step2730]: loss 0.388522
[epoch20, step2731]: loss 0.263521
[epoch20, step2732]: loss 0.582035
[epoch20, step2733]: loss 0.441851
[epoch20, step2734]: loss 0.516051
[epoch20, step2735]: loss 0.312372
[epoch20, step2736]: loss 0.491889
[epoch20, step2737]: loss 0.521943
[epoch20, step2738]: loss 0.301066
[epoch20, step2739]: loss 0.613154
[epoch20, step2740]: loss 0.441713
[epoch20, step2741]: loss 0.478977
[epoch20, step2742]: loss 0.374574
[epoch20, step2743]: loss 0.254837
[epoch20, step2744]: loss 0.418816
[epoch20, step2745]: loss 0.448300
[epoch20, step2746]: loss 0.454428
[epoch20, step2747]: loss 0.306371
[epoch20, step2748]: loss 0.544375
[epoch20, step2749]: loss 0.560466
[epoch20, step2750]: loss 0.486743
[epoch20, step2751]: loss 0.505926
[epoch20, step2752]: loss 0.617665
[epoch20, step2753]: loss 0.508436
[epoch20, step2754]: loss 0.414544
[epoch20, step2755]: loss 0.315685
[epoch20, step2756]: loss 0.698091
[epoch20, step2757]: loss 0.608740
[epoch20, step2758]: loss 0.407031
[epoch20, step2759]: loss 0.358471
[epoch20, step2760]: loss 0.351288
[epoch20, step2761]: loss 0.546851
[epoch20, step2762]: loss 0.605486
[epoch20, step2763]: loss 0.592163
[epoch20, step2764]: loss 0.567501
[epoch20, step2765]: loss 0.464404
[epoch20, step2766]: loss 0.534426
[epoch20, step2767]: loss 0.251048
[epoch20, step2768]: loss 0.219343
[epoch20, step2769]: loss 0.475793
[epoch20, step2770]: loss 0.535445
[epoch20, step2771]: loss 0.707518
[epoch20, step2772]: loss 0.426298
[epoch20, step2773]: loss 0.440272
[epoch20, step2774]: loss 0.535290
[epoch20, step2775]: loss 0.653976
[epoch20, step2776]: loss 0.721892
[epoch20, step2777]: loss 0.452245
[epoch20, step2778]: loss 0.447261
[epoch20, step2779]: loss 0.747903
[epoch20, step2780]: loss 0.464549
[epoch20, step2781]: loss 0.416569
[epoch20, step2782]: loss 0.498136
[epoch20, step2783]: loss 0.500388
[epoch20, step2784]: loss 0.525951
[epoch20, step2785]: loss 0.405540
[epoch20, step2786]: loss 0.465902
[epoch20, step2787]: loss 0.601430
[epoch20, step2788]: loss 0.588366
[epoch20, step2789]: loss 0.334255
[epoch20, step2790]: loss 0.484138
[epoch20, step2791]: loss 0.281652
[epoch20, step2792]: loss 0.376295
[epoch20, step2793]: loss 0.559427
[epoch20, step2794]: loss 0.109080
[epoch20, step2795]: loss 0.452389
[epoch20, step2796]: loss 0.339934
[epoch20, step2797]: loss 0.311827
[epoch20, step2798]: loss 0.601624
[epoch20, step2799]: loss 0.646729
[epoch20, step2800]: loss 0.348084
[epoch20, step2801]: loss 0.617384
[epoch20, step2802]: loss 0.553673
[epoch20, step2803]: loss 0.628548
[epoch20, step2804]: loss 0.396614
[epoch20, step2805]: loss 0.629063
[epoch20, step2806]: loss 0.465822
[epoch20, step2807]: loss 0.539601
[epoch20, step2808]: loss 0.640066
[epoch20, step2809]: loss 0.485627
[epoch20, step2810]: loss 0.400108
[epoch20, step2811]: loss 0.635743
[epoch20, step2812]: loss 0.293716
[epoch20, step2813]: loss 0.537379
[epoch20, step2814]: loss 0.694235
[epoch20, step2815]: loss 0.587233
[epoch20, step2816]: loss 0.274809
[epoch20, step2817]: loss 0.256996
[epoch20, step2818]: loss 0.538570
[epoch20, step2819]: loss 0.380794
[epoch20, step2820]: loss 0.504926
[epoch20, step2821]: loss 0.338762
[epoch20, step2822]: loss 0.413762
[epoch20, step2823]: loss 0.331170
[epoch20, step2824]: loss 0.435166
[epoch20, step2825]: loss 0.520987
[epoch20, step2826]: loss 0.667188
[epoch20, step2827]: loss 0.584487
[epoch20, step2828]: loss 0.468731
[epoch20, step2829]: loss 0.233251
[epoch20, step2830]: loss 0.515290
[epoch20, step2831]: loss 0.151879
[epoch20, step2832]: loss 0.501319
[epoch20, step2833]: loss 0.465936
[epoch20, step2834]: loss 0.541788
[epoch20, step2835]: loss 0.267910
[epoch20, step2836]: loss 0.656904
[epoch20, step2837]: loss 0.388154
[epoch20, step2838]: loss 0.322051
[epoch20, step2839]: loss 0.309960
[epoch20, step2840]: loss 0.330571
[epoch20, step2841]: loss 0.562268
[epoch20, step2842]: loss 0.223327
[epoch20, step2843]: loss 0.427026
[epoch20, step2844]: loss 0.541948
[epoch20, step2845]: loss 0.461665
[epoch20, step2846]: loss 0.212284
[epoch20, step2847]: loss 0.347259
[epoch20, step2848]: loss 0.488475
[epoch20, step2849]: loss 0.363774
[epoch20, step2850]: loss 0.453658
[epoch20, step2851]: loss 0.368497
[epoch20, step2852]: loss 0.307099
[epoch20, step2853]: loss 0.485344
[epoch20, step2854]: loss 0.350984
[epoch20, step2855]: loss 0.332514
[epoch20, step2856]: loss 0.498707
[epoch20, step2857]: loss 0.318012
[epoch20, step2858]: loss 0.641190
[epoch20, step2859]: loss 0.394529
[epoch20, step2860]: loss 0.486932
[epoch20, step2861]: loss 0.501508
[epoch20, step2862]: loss 0.345898
[epoch20, step2863]: loss 0.546254
[epoch20, step2864]: loss 0.331555
[epoch20, step2865]: loss 0.283852
[epoch20, step2866]: loss 0.384325
[epoch20, step2867]: loss 0.250054
[epoch20, step2868]: loss 0.126917
[epoch20, step2869]: loss 0.557819
[epoch20, step2870]: loss 0.553702
[epoch20, step2871]: loss 0.379469
[epoch20, step2872]: loss 0.567388
[epoch20, step2873]: loss 0.285065
[epoch20, step2874]: loss 0.687618
[epoch20, step2875]: loss 0.527736
[epoch20, step2876]: loss 0.474050
[epoch20, step2877]: loss 0.556208
[epoch20, step2878]: loss 0.466704
[epoch20, step2879]: loss 0.667036
[epoch20, step2880]: loss 0.536325
[epoch20, step2881]: loss 0.426733
[epoch20, step2882]: loss 0.170460
[epoch20, step2883]: loss 0.315484
[epoch20, step2884]: loss 0.138680
[epoch20, step2885]: loss 0.584785
[epoch20, step2886]: loss 0.324388
[epoch20, step2887]: loss 0.616605
[epoch20, step2888]: loss 0.548482
[epoch20, step2889]: loss 0.406839
[epoch20, step2890]: loss 0.603661
[epoch20, step2891]: loss 0.503830
[epoch20, step2892]: loss 0.367413
[epoch20, step2893]: loss 0.555228
[epoch20, step2894]: loss 0.485275
[epoch20, step2895]: loss 0.430009
[epoch20, step2896]: loss 0.507285
[epoch20, step2897]: loss 0.394892
[epoch20, step2898]: loss 0.464268
[epoch20, step2899]: loss 0.529670
[epoch20, step2900]: loss 0.273672
[epoch20, step2901]: loss 0.471845
[epoch20, step2902]: loss 0.128198
[epoch20, step2903]: loss 0.563413
[epoch20, step2904]: loss 0.514490
[epoch20, step2905]: loss 0.569249
[epoch20, step2906]: loss 0.545840
[epoch20, step2907]: loss 0.593043
[epoch20, step2908]: loss 0.238385
[epoch20, step2909]: loss 0.357215
[epoch20, step2910]: loss 0.519239
[epoch20, step2911]: loss 0.339874
[epoch20, step2912]: loss 0.421170
[epoch20, step2913]: loss 0.413298
[epoch20, step2914]: loss 0.644054
[epoch20, step2915]: loss 0.508623
[epoch20, step2916]: loss 0.489930
[epoch20, step2917]: loss 0.323249
[epoch20, step2918]: loss 0.337799
[epoch20, step2919]: loss 0.483622
[epoch20, step2920]: loss 0.450808
[epoch20, step2921]: loss 0.350584
[epoch20, step2922]: loss 0.357892
[epoch20, step2923]: loss 0.317638
[epoch20, step2924]: loss 0.503056
[epoch20, step2925]: loss 0.449302
[epoch20, step2926]: loss 0.638748
[epoch20, step2927]: loss 0.480985
[epoch20, step2928]: loss 0.416506
[epoch20, step2929]: loss 0.536465
[epoch20, step2930]: loss 0.571490
[epoch20, step2931]: loss 0.273271
[epoch20, step2932]: loss 0.239148
[epoch20, step2933]: loss 0.477713
[epoch20, step2934]: loss 0.515540
[epoch20, step2935]: loss 0.402385
[epoch20, step2936]: loss 0.504174
[epoch20, step2937]: loss 0.460410
[epoch20, step2938]: loss 0.566520
[epoch20, step2939]: loss 0.713552
[epoch20, step2940]: loss 0.534503
[epoch20, step2941]: loss 0.377662
[epoch20, step2942]: loss 0.522345
[epoch20, step2943]: loss 0.602615
[epoch20, step2944]: loss 0.426844
[epoch20, step2945]: loss 0.538862
[epoch20, step2946]: loss 0.552888
[epoch20, step2947]: loss 0.393074
[epoch20, step2948]: loss 0.201854
[epoch20, step2949]: loss 0.249700
[epoch20, step2950]: loss 0.481260
[epoch20, step2951]: loss 0.434369
[epoch20, step2952]: loss 0.510106
[epoch20, step2953]: loss 0.511438
[epoch20, step2954]: loss 0.566534
[epoch20, step2955]: loss 0.426017
[epoch20, step2956]: loss 0.575252
[epoch20, step2957]: loss 0.500516
[epoch20, step2958]: loss 0.480363
[epoch20, step2959]: loss 0.416619
[epoch20, step2960]: loss 0.549492
[epoch20, step2961]: loss 0.359551
[epoch20, step2962]: loss 0.445036
[epoch20, step2963]: loss 0.310041
[epoch20, step2964]: loss 0.360195
[epoch20, step2965]: loss 0.345636
[epoch20, step2966]: loss 0.412380
[epoch20, step2967]: loss 0.146941
[epoch20, step2968]: loss 0.535904
[epoch20, step2969]: loss 0.565859
[epoch20, step2970]: loss 0.434529
[epoch20, step2971]: loss 0.553357
[epoch20, step2972]: loss 0.526774
[epoch20, step2973]: loss 0.472906
[epoch20, step2974]: loss 0.310220
[epoch20, step2975]: loss 0.670284
[epoch20, step2976]: loss 0.493659
[epoch20, step2977]: loss 0.357208
[epoch20, step2978]: loss 0.541333
[epoch20, step2979]: loss 0.153454
[epoch20, step2980]: loss 0.471470
[epoch20, step2981]: loss 0.671573
[epoch20, step2982]: loss 0.540011
[epoch20, step2983]: loss 0.619180
[epoch20, step2984]: loss 0.517186
[epoch20, step2985]: loss 0.327804
[epoch20, step2986]: loss 0.510494
[epoch20, step2987]: loss 0.458752
[epoch20, step2988]: loss 0.479791
[epoch20, step2989]: loss 0.224548
[epoch20, step2990]: loss 0.541918
[epoch20, step2991]: loss 0.703578
[epoch20, step2992]: loss 0.575096
[epoch20, step2993]: loss 0.641662
[epoch20, step2994]: loss 0.358178
[epoch20, step2995]: loss 0.531913
[epoch20, step2996]: loss 0.435632
[epoch20, step2997]: loss 0.599402
[epoch20, step2998]: loss 0.739176
[epoch20, step2999]: loss 0.321610
[epoch20, step3000]: loss 0.272943
[epoch20, step3001]: loss 0.485845
[epoch20, step3002]: loss 0.468671
[epoch20, step3003]: loss 0.442490
[epoch20, step3004]: loss 0.352152
[epoch20, step3005]: loss 0.379156
[epoch20, step3006]: loss 0.629748
[epoch20, step3007]: loss 0.213799
[epoch20, step3008]: loss 0.654194
[epoch20, step3009]: loss 0.401928
[epoch20, step3010]: loss 0.438940
[epoch20, step3011]: loss 0.519268
[epoch20, step3012]: loss 0.468962
[epoch20, step3013]: loss 0.484188
[epoch20, step3014]: loss 0.479702
[epoch20, step3015]: loss 0.489527
[epoch20, step3016]: loss 0.363029
[epoch20, step3017]: loss 0.477066
[epoch20, step3018]: loss 0.427652
[epoch20, step3019]: loss 0.512166
[epoch20, step3020]: loss 0.548844
[epoch20, step3021]: loss 0.338887
[epoch20, step3022]: loss 0.402545
[epoch20, step3023]: loss 0.343494
[epoch20, step3024]: loss 0.503226
[epoch20, step3025]: loss 0.504211
[epoch20, step3026]: loss 0.491074
[epoch20, step3027]: loss 0.563464
[epoch20, step3028]: loss 0.500559
[epoch20, step3029]: loss 0.379475
[epoch20, step3030]: loss 0.635504
[epoch20, step3031]: loss 0.384113
[epoch20, step3032]: loss 0.322736
[epoch20, step3033]: loss 0.280531
[epoch20, step3034]: loss 0.433084
[epoch20, step3035]: loss 0.236503
[epoch20, step3036]: loss 0.529224
[epoch20, step3037]: loss 0.422350
[epoch20, step3038]: loss 0.425439
[epoch20, step3039]: loss 0.574979
[epoch20, step3040]: loss 0.541720
[epoch20, step3041]: loss 0.359221
[epoch20, step3042]: loss 0.232143
[epoch20, step3043]: loss 0.594340
[epoch20, step3044]: loss 0.438190
[epoch20, step3045]: loss 0.377964
[epoch20, step3046]: loss 0.407464
[epoch20, step3047]: loss 0.441605
[epoch20, step3048]: loss 0.655600
[epoch20, step3049]: loss 0.623604
[epoch20, step3050]: loss 0.519653
[epoch20, step3051]: loss 0.547221
[epoch20, step3052]: loss 0.461870
[epoch20, step3053]: loss 0.551804
[epoch20, step3054]: loss 0.510579
[epoch20, step3055]: loss 0.420310
[epoch20, step3056]: loss 0.444220
[epoch20, step3057]: loss 0.590015
[epoch20, step3058]: loss 0.472634
[epoch20, step3059]: loss 0.098434
[epoch20, step3060]: loss 0.228153
[epoch20, step3061]: loss 0.579445
[epoch20, step3062]: loss 0.493890
[epoch20, step3063]: loss 0.519672
[epoch20, step3064]: loss 0.450228
[epoch20, step3065]: loss 0.407190
[epoch20, step3066]: loss 0.589480
[epoch20, step3067]: loss 0.513082
[epoch20, step3068]: loss 0.475776
[epoch20, step3069]: loss 0.431444
[epoch20, step3070]: loss 0.460641
[epoch20, step3071]: loss 0.295606
[epoch20, step3072]: loss 0.477278
[epoch20, step3073]: loss 0.416416
[epoch20, step3074]: loss 0.326026
[epoch20, step3075]: loss 0.509715
[epoch20, step3076]: loss 0.264349

[epoch20]: avg loss 0.264349

[TEST step1]: loss 0.387208
[TEST step2]: loss 0.361771
[TEST step3]: loss 0.447460
[TEST step4]: loss 0.486812
[TEST step5]: loss 0.442842
[TEST step6]: loss 0.478960
[TEST step7]: loss 0.751022
[TEST step8]: loss 0.467894
[TEST step9]: loss 0.557157
[TEST step10]: loss 0.615696
[TEST step11]: loss 0.363487
[TEST step12]: loss 0.113331
[TEST step13]: loss 0.401747
[TEST step14]: loss 0.528277
[TEST step15]: loss 0.557438
[TEST step16]: loss 0.575608
[TEST step17]: loss 0.495661
[TEST step18]: loss 0.616460
[TEST step19]: loss 0.245131
[TEST step20]: loss 0.294531
[TEST step21]: loss 0.747656
[TEST step22]: loss 0.637583
[TEST step23]: loss 0.478045
[TEST step24]: loss 0.443927
[TEST step25]: loss 0.687841
[TEST step26]: loss 0.568097
[TEST step27]: loss 0.452209
[TEST step28]: loss 0.389278
[TEST step29]: loss 0.348385
[TEST step30]: loss 0.329526
[TEST step31]: loss 0.490869
[TEST step32]: loss 0.723020
[TEST step33]: loss 0.511934
[TEST step34]: loss 0.552351
[TEST step35]: loss 0.160545
[TEST step36]: loss 0.540654
[TEST step37]: loss 0.476041
[TEST step38]: loss 0.383568
[TEST step39]: loss 0.490464
[TEST step40]: loss 0.348617
[TEST step41]: loss 0.481654
[TEST step42]: loss 0.371402
[TEST step43]: loss 0.388958
[TEST step44]: loss 0.449361
[TEST step45]: loss 0.463429
[TEST step46]: loss 0.488585
[TEST step47]: loss 0.558091
[TEST step48]: loss 0.451345
[TEST step49]: loss 0.553331
[TEST step50]: loss 0.471000
[TEST step51]: loss 0.275012
[TEST step52]: loss 0.545288
[TEST step53]: loss 0.438177
[TEST step54]: loss 0.457007
[TEST step55]: loss 0.433047
[TEST step56]: loss 0.422256
[TEST step57]: loss 0.514326
[TEST step58]: loss 0.662923
[TEST step59]: loss 0.582243
[TEST step60]: loss 0.419346
[TEST step61]: loss 0.309097
[TEST step62]: loss 0.549076
[TEST step63]: loss 0.625698
[TEST step64]: loss 0.401821
[TEST step65]: loss 0.524560
[TEST step66]: loss 0.577387
[TEST step67]: loss 0.429389
[TEST step68]: loss 0.332778
[TEST step69]: loss 0.446245
[TEST step70]: loss 0.270663
[TEST step71]: loss 0.540706
[TEST step72]: loss 0.365184
[TEST step73]: loss 0.599750
[TEST step74]: loss 0.496963
[TEST step75]: loss 0.181522
[TEST step76]: loss 0.528250
[TEST step77]: loss 0.241042
[TEST step78]: loss 0.337323
[TEST step79]: loss 0.539837
[TEST step80]: loss 0.515799
[TEST step81]: loss 0.548853
[TEST step82]: loss 0.636738
[TEST step83]: loss 0.460115
[TEST step84]: loss 0.423929
[TEST step85]: loss 0.441569
[TEST step86]: loss 0.529726
[TEST step87]: loss 0.327996
[TEST step88]: loss 0.476646
[TEST step89]: loss 0.154119
[TEST step90]: loss 0.611591
[TEST step91]: loss 0.503177
[TEST step92]: loss 0.411634
[TEST step93]: loss 0.282631
[TEST step94]: loss 0.496738
[TEST step95]: loss 0.480343
[TEST step96]: loss 0.526860
[TEST step97]: loss 0.285011
[TEST step98]: loss 0.400853
[TEST step99]: loss 0.651729
[TEST step100]: loss 0.479566
[TEST step101]: loss 0.619168
[TEST step102]: loss 0.660707
[TEST step103]: loss 0.388987
[TEST step104]: loss 0.349621
[TEST step105]: loss 0.470871
[TEST step106]: loss 0.376757
[TEST step107]: loss 0.628089
[TEST step108]: loss 0.570175
[TEST step109]: loss 0.477514
[TEST step110]: loss 0.429053
[TEST step111]: loss 0.446016
[TEST step112]: loss 0.465239
[TEST step113]: loss 0.366311
[TEST step114]: loss 0.587490
[TEST step115]: loss 0.345513
[TEST step116]: loss 0.350963
[TEST step117]: loss 0.328942
[TEST step118]: loss 0.492383
[TEST step119]: loss 0.349712
[TEST step120]: loss 0.531591
[TEST step121]: loss 0.246476
[TEST step122]: loss 0.465278
[TEST step123]: loss 0.600542
[TEST step124]: loss 0.280289
[TEST step125]: loss 0.570062
[TEST step126]: loss 0.560424
[TEST step127]: loss 0.499550
[TEST step128]: loss 0.205589
[TEST step129]: loss 0.587822
[TEST step130]: loss 0.673467
[TEST step131]: loss 0.373183
[TEST step132]: loss 0.516300
[TEST step133]: loss 0.663214
[TEST step134]: loss 0.429477
[TEST step135]: loss 0.438951
[TEST step136]: loss 0.366491
[TEST step137]: loss 0.202772
[TEST step138]: loss 0.379244
[TEST step139]: loss 0.429096
[TEST step140]: loss 0.542899
[TEST step141]: loss 0.430974
[TEST step142]: loss 0.301370
[TEST step143]: loss 0.564829
[TEST step144]: loss 0.571550
[TEST step145]: loss 0.426818
[TEST step146]: loss 0.284201
[TEST step147]: loss 0.658998
[TEST step148]: loss 0.552687
[TEST step149]: loss 0.487846
[TEST step150]: loss 0.439202
[TEST step151]: loss 0.494550
[TEST step152]: loss 0.426659
[TEST step153]: loss 0.419999
[TEST step154]: loss 0.533038
[TEST step155]: loss 0.459991
[TEST step156]: loss 0.550196
[TEST step157]: loss 0.557949
[TEST step158]: loss 0.336741
[TEST step159]: loss 0.437419
[TEST step160]: loss 0.373550
[TEST step161]: loss 0.658617
[TEST step162]: loss 0.499356
[TEST step163]: loss 0.597928
[TEST step164]: loss 0.334202
[TEST step165]: loss 0.280660
[TEST step166]: loss 0.601562
[TEST step167]: loss 0.398661
[TEST step168]: loss 0.437447
[TEST step169]: loss 0.323095
[TEST step170]: loss 0.683476
[TEST step171]: loss 0.497537
[TEST step172]: loss 0.557481
[TEST step173]: loss 0.327404
[TEST step174]: loss 0.346364
[TEST step175]: loss 0.751684
[TEST step176]: loss 0.554375
[TEST step177]: loss 0.451010
[TEST step178]: loss 0.607607
[TEST step179]: loss 0.277465
[TEST step180]: loss 0.393307
[TEST step181]: loss 0.397160
[TEST step182]: loss 0.464313
[TEST step183]: loss 0.484626
[TEST step184]: loss 0.615455
[TEST step185]: loss 0.419345
[TEST step186]: loss 0.467532
[TEST step187]: loss 0.552199
[TEST step188]: loss 0.317586
[TEST step189]: loss 0.459040
[TEST step190]: loss 0.434043
[TEST step191]: loss 0.360217
[TEST step192]: loss 0.665724
[TEST step193]: loss 0.468548
[TEST step194]: loss 0.563839
[TEST step195]: loss 0.659182
[TEST step196]: loss 0.398145
[TEST step197]: loss 0.623464
[TEST step198]: loss 0.254718
[TEST step199]: loss 0.346084
[TEST step200]: loss 0.697611
[TEST step201]: loss 0.390556
[TEST step202]: loss 0.614299
[TEST step203]: loss 0.415691
[TEST step204]: loss 0.435692
[TEST step205]: loss 0.589451
[TEST step206]: loss 0.602070
[TEST step207]: loss 0.430743
[TEST step208]: loss 0.673534
[TEST step209]: loss 0.408591
[TEST step210]: loss 0.254707
[TEST step211]: loss 0.636061
[TEST step212]: loss 0.378980
[TEST step213]: loss 0.655411
[TEST step214]: loss 0.477742
[TEST step215]: loss 0.450427
[TEST step216]: loss 0.457411
[TEST step217]: loss 0.555758
[TEST step218]: loss 0.490203
[TEST step219]: loss 0.619528
[TEST step220]: loss 0.358836
[TEST step221]: loss 0.273488
[TEST step222]: loss 0.420681
[TEST step223]: loss 0.537626
[TEST step224]: loss 0.522532
[TEST step225]: loss 0.592406
[TEST step226]: loss 0.487761
[TEST step227]: loss 0.211166
[TEST step228]: loss 0.410346
[TEST step229]: loss 0.558619
[TEST step230]: loss 0.453918
[TEST step231]: loss 0.412868
[TEST step232]: loss 0.358541
[TEST step233]: loss 0.650720
[TEST step234]: loss 0.508210
[TEST step235]: loss 0.561415
[TEST step236]: loss 0.314024
[TEST step237]: loss 0.629653
[TEST step238]: loss 0.614166
[TEST step239]: loss 0.371498
[TEST step240]: loss 0.531258
[TEST step241]: loss 0.331739
[TEST step242]: loss 0.414426
[TEST step243]: loss 0.489655
[TEST step244]: loss 0.377043
[TEST step245]: loss 0.546789
[TEST step246]: loss 0.232036
[TEST step247]: loss 0.521999
[TEST step248]: loss 0.428056
[TEST step249]: loss 0.441943
[TEST step250]: loss 0.715390
[TEST step251]: loss 0.509512
[TEST step252]: loss 0.240476
[TEST step253]: loss 0.452888
[TEST step254]: loss 0.528357
[TEST step255]: loss 0.559943
[TEST step256]: loss 0.588102
[TEST step257]: loss 0.351581
[TEST step258]: loss 0.534120
[TEST step259]: loss 0.551341
[TEST step260]: loss 0.288871
[TEST step261]: loss 0.230497
[TEST step262]: loss 0.511712
[TEST step263]: loss 0.115687
[TEST step264]: loss 0.523331
[TEST step265]: loss 0.369013
[TEST step266]: loss 0.296239
[TEST step267]: loss 0.513271
[TEST step268]: loss 0.390395
[TEST step269]: loss 0.460491
[TEST step270]: loss 0.269495
[TEST step271]: loss 0.351276
[TEST step272]: loss 0.520418
[TEST step273]: loss 0.546390
[TEST step274]: loss 0.522501
[TEST step275]: loss 0.374479
[TEST step276]: loss 0.664618
[TEST step277]: loss 0.386138
[TEST step278]: loss 0.464539
[TEST step279]: loss 0.471826
[TEST step280]: loss 0.565120
[TEST step281]: loss 0.482602
[TEST step282]: loss 0.505528
[TEST step283]: loss 0.175202
[TEST step284]: loss 0.518839
[TEST step285]: loss 0.414435
[TEST step286]: loss 0.579460
[TEST step287]: loss 0.500737
[TEST step288]: loss 0.496659
[TEST step289]: loss 0.483101
[TEST step290]: loss 0.314366
[TEST step291]: loss 0.623277
[TEST step292]: loss 0.331002
[TEST step293]: loss 0.379324
[TEST step294]: loss 0.566115
[TEST step295]: loss 0.335722
[TEST step296]: loss 0.444078
[TEST step297]: loss 0.535401
[TEST step298]: loss 0.173095
[TEST step299]: loss 0.404870
[TEST step300]: loss 0.359243
[TEST step301]: loss 0.503167
[TEST step302]: loss 0.528060
[TEST step303]: loss 0.477765
[TEST step304]: loss 0.476161
[TEST step305]: loss 0.635568
[TEST step306]: loss 0.598761
[TEST step307]: loss 0.574148
[TEST step308]: loss 0.366559
[TEST step309]: loss 0.397174
[TEST step310]: loss 0.415046
[TEST step311]: loss 0.399761
[TEST step312]: loss 0.312792
[TEST step313]: loss 0.476912
[TEST step314]: loss 0.435090
[TEST step315]: loss 0.633947
[TEST step316]: loss 0.437622
[TEST step317]: loss 0.414312
[TEST step318]: loss 0.468219
[TEST step319]: loss 0.471528
[TEST step320]: loss 0.250855
[TEST step321]: loss 0.341972
[TEST step322]: loss 0.546641
[TEST step323]: loss 0.286381
[TEST step324]: loss 0.421314
[TEST step325]: loss 0.440004
[TEST step326]: loss 0.270699
[TEST step327]: loss 0.562112
[TEST step328]: loss 0.703961
[TEST step329]: loss 0.520639
[TEST step330]: loss 0.526132
[TEST step331]: loss 0.468243
[TEST step332]: loss 0.467579
[TEST step333]: loss 0.630780
[TEST step334]: loss 0.629346
[TEST step335]: loss 0.513634
[TEST step336]: loss 0.694339
[TEST step337]: loss 0.235338
[TEST step338]: loss 0.487531
[TEST step339]: loss 0.588605
[TEST step340]: loss 0.574303
[TEST step341]: loss 0.642745
[TEST step342]: loss 0.381180
[TEST step343]: loss 0.240890
[TEST step344]: loss 0.548649
[TEST step345]: loss 0.279717
[TEST step346]: loss 0.333641
[TEST step347]: loss 0.208804
[TEST step348]: loss 0.434024
[TEST step349]: loss 0.245196
[TEST step350]: loss 0.493854
[TEST step351]: loss 0.589346
[TEST step352]: loss 0.471826
[TEST step353]: loss 0.372423
[TEST step354]: loss 0.406555
[TEST step355]: loss 0.594726
[TEST step356]: loss 0.262654
[TEST step357]: loss 0.542713
[TEST step358]: loss 0.461275
[TEST step359]: loss 0.403133
[TEST step360]: loss 0.412718
[TEST step361]: loss 0.448530
[TEST step362]: loss 0.379953
[TEST step363]: loss 0.683429
[TEST step364]: loss 0.563560
[TEST step365]: loss 0.261740
[TEST step366]: loss 0.657176
[TEST step367]: loss 0.634571
[TEST step368]: loss 0.508992
[TEST step369]: loss 0.569028
[TEST step370]: loss 0.333261
[TEST step371]: loss 0.574475
[TEST step372]: loss 0.545079
[TEST step373]: loss 0.360561
[TEST step374]: loss 0.432940
[TEST step375]: loss 0.250046
[TEST step376]: loss 0.410033
[TEST step377]: loss 0.494895
[TEST step378]: loss 0.551363
[TEST step379]: loss 0.355464
[TEST step380]: loss 0.392988
[TEST step381]: loss 0.482513
[TEST step382]: loss 0.244327
[TEST step383]: loss 0.379221
[TEST step384]: loss 0.248423
[TEST step385]: loss 0.533445
[TEST step386]: loss 0.289940
[TEST step387]: loss 0.473648
[TEST step388]: loss 0.649026
[TEST step389]: loss 0.481982
[TEST step390]: loss 0.553526
[TEST step391]: loss 0.300111
[TEST step392]: loss 0.589376
[TEST step393]: loss 0.585190
[TEST step394]: loss 0.481460
[TEST step395]: loss 0.254804
[TEST step396]: loss 0.379176
[TEST step397]: loss 0.273333
[TEST step398]: loss 0.246191
[TEST step399]: loss 0.103856
[TEST step400]: loss 0.316618
[TEST step401]: loss 0.388480
[TEST step402]: loss 0.649397
[TEST step403]: loss 0.543658
[TEST step404]: loss 0.517224
[TEST step405]: loss 0.390365
[TEST step406]: loss 0.654688
[TEST step407]: loss 0.442735
[TEST step408]: loss 0.379958
[TEST step409]: loss 0.261297
[TEST step410]: loss 0.530675
[TEST step411]: loss 0.525474
[TEST step412]: loss 0.266485
[TEST step413]: loss 0.385078
[TEST step414]: loss 0.498019
[TEST step415]: loss 0.449635
[TEST step416]: loss 0.124697
[TEST step417]: loss 0.463264
[TEST step418]: loss 0.332190
[TEST step419]: loss 0.510704
[TEST step420]: loss 0.520154
[TEST step421]: loss 0.433115
[TEST step422]: loss 0.440535
[TEST step423]: loss 0.589646
[TEST step424]: loss 0.512115
[TEST step425]: loss 0.416202
[TEST step426]: loss 0.463850
[TEST step427]: loss 0.681189
[TEST step428]: loss 0.528932
[TEST step429]: loss 0.366392
[TEST step430]: loss 0.420506
[TEST step431]: loss 0.426328
[TEST step432]: loss 0.573212
[TEST step433]: loss 0.595233
[TEST step434]: loss 0.508287
[TEST step435]: loss 0.362598
[TEST step436]: loss 0.615998
[TEST step437]: loss 0.259951
[TEST step438]: loss 0.498833
[TEST step439]: loss 0.383939
[TEST step440]: loss 0.577870
[TEST step441]: loss 0.224916
[TEST step442]: loss 0.522368
[TEST step443]: loss 0.365586
[TEST step444]: loss 0.454372
[TEST step445]: loss 0.450276
[TEST step446]: loss 0.386193
[TEST step447]: loss 0.337480
[TEST step448]: loss 0.360787
[TEST step449]: loss 0.522157
[TEST step450]: loss 0.257377
[TEST step451]: loss 0.446991
[TEST step452]: loss 0.677426
[TEST step453]: loss 0.566550
[TEST step454]: loss 0.355027
[TEST step455]: loss 0.553065
[TEST step456]: loss 0.410769
[TEST step457]: loss 0.450310
[TEST step458]: loss 0.445671
[TEST step459]: loss 0.407322
[TEST step460]: loss 0.572001
[TEST step461]: loss 0.342960
[TEST step462]: loss 0.334096
[TEST step463]: loss 0.399351
[TEST step464]: loss 0.251130
[TEST step465]: loss 0.433000
[TEST step466]: loss 0.618574
[TEST step467]: loss 0.373054
[TEST step468]: loss 0.442398
[TEST step469]: loss 0.507378
[TEST step470]: loss 0.585343
[TEST step471]: loss 0.445841
[TEST step472]: loss 0.684059
[TEST step473]: loss 0.719928
[TEST step474]: loss 0.266282
[TEST step475]: loss 0.443898
[TEST step476]: loss 0.391364
[TEST step477]: loss 0.462345
[TEST step478]: loss 0.567438
[TEST step479]: loss 0.561353
[TEST step480]: loss 0.230619
[TEST step481]: loss 0.428662
[TEST step482]: loss 0.449097
[TEST step483]: loss 0.471244
[TEST step484]: loss 0.384982
[TEST step485]: loss 0.445644
[TEST step486]: loss 0.483639
[TEST step487]: loss 0.544647
[TEST step488]: loss 0.351981
[TEST step489]: loss 0.550913
[TEST step490]: loss 0.424890
[TEST step491]: loss 0.487365
[TEST step492]: loss 0.401433
[TEST step493]: loss 0.434738
[TEST step494]: loss 0.336638
[TEST step495]: loss 0.333808
[TEST step496]: loss 0.410982
[TEST step497]: loss 0.286048
[TEST step498]: loss 0.594199
[TEST step499]: loss 0.700288
[TEST step500]: loss 0.403126
[TEST step501]: loss 0.652259
[TEST step502]: loss 0.533341
[TEST step503]: loss 0.462972
[TEST step504]: loss 0.581213
[TEST step505]: loss 0.403648
[TEST step506]: loss 0.443242
[TEST step507]: loss 0.624112
[TEST step508]: loss 0.614891
[TEST step509]: loss 0.310992
[TEST step510]: loss 0.525925
[TEST step511]: loss 0.356040
[TEST step512]: loss 0.448681
[TEST step513]: loss 0.530861
[TEST step514]: loss 0.620477
[TEST step515]: loss 0.413004
[TEST step516]: loss 0.529959
[TEST step517]: loss 0.355529
[TEST step518]: loss 0.413189
[TEST step519]: loss 0.664437
[TEST step520]: loss 0.298946
[TEST step521]: loss 0.566460
[TEST step522]: loss 0.587285
[TEST step523]: loss 0.218697
[TEST step524]: loss 0.558434
[TEST step525]: loss 0.519894
[TEST step526]: loss 0.343606
[TEST step527]: loss 0.209610
[TEST step528]: loss 0.556468
[TEST step529]: loss 0.547372
[TEST step530]: loss 0.422199
[TEST step531]: loss 0.508758
[TEST step532]: loss 0.269032
[TEST step533]: loss 0.390067
[TEST step534]: loss 0.576675
[TEST step535]: loss 0.455829
[TEST step536]: loss 0.401871
[TEST step537]: loss 0.483587
[TEST step538]: loss 0.251578
[TEST step539]: loss 0.393643
[TEST step540]: loss 0.483549
[TEST step541]: loss 0.535268
[TEST step542]: loss 0.428185
[TEST step543]: loss 0.598519
[TEST step544]: loss 0.339845
[TEST step545]: loss 0.430482
[TEST step546]: loss 0.368742
[TEST step547]: loss 0.395281
[TEST step548]: loss 0.267947
[TEST step549]: loss 0.273122
[TEST step550]: loss 0.455736
[TEST step551]: loss 0.129154
[TEST step552]: loss 0.321006
[TEST step553]: loss 0.229003
[TEST step554]: loss 0.394116
[TEST step555]: loss 0.668940
[TEST step556]: loss 0.604528
[TEST step557]: loss 0.321501
[TEST step558]: loss 0.292689
[TEST step559]: loss 0.109337
[TEST step560]: loss 0.221231
[TEST step561]: loss 0.339786
[TEST step562]: loss 0.508295
[TEST step563]: loss 0.310446
[TEST step564]: loss 0.253962
[TEST step565]: loss 0.578939
[TEST step566]: loss 0.436525
[TEST step567]: loss 0.354688
[TEST step568]: loss 0.235563
[TEST step569]: loss 0.545774
[TEST step570]: loss 0.458101
[TEST step571]: loss 0.533401
[TEST step572]: loss 0.454886
[TEST step573]: loss 0.270988
[TEST step574]: loss 0.295508
[TEST step575]: loss 0.436144
[TEST step576]: loss 0.623398
[TEST step577]: loss 0.550930
[TEST step578]: loss 0.535548
[TEST step579]: loss 0.515038
[TEST step580]: loss 0.245267
[TEST step581]: loss 0.487189
[TEST step582]: loss 0.332753
[TEST step583]: loss 0.406395
[TEST step584]: loss 0.614010
[TEST step585]: loss 0.454123
[TEST step586]: loss 0.485910
[TEST step587]: loss 0.683378
[TEST step588]: loss 0.497031
[TEST step589]: loss 0.467308
[TEST step590]: loss 0.373038
[TEST step591]: loss 0.431544
[TEST step592]: loss 0.262992
[TEST step593]: loss 0.098511
[TEST step594]: loss 0.479424
[TEST step595]: loss 0.303660
[TEST step596]: loss 0.446697
[TEST step597]: loss 0.550685
[TEST step598]: loss 0.277828
[TEST step599]: loss 0.572765
[TEST step600]: loss 0.418574
[TEST step601]: loss 0.593129
[TEST step602]: loss 0.185604
[TEST step603]: loss 0.558358
[TEST step604]: loss 0.100157
[TEST step605]: loss 0.464325
[TEST step606]: loss 0.217967
[TEST step607]: loss 0.402138
[TEST step608]: loss 0.386309
[TEST step609]: loss 0.397800
[TEST step610]: loss 0.473632
[TEST step611]: loss 0.264998
[TEST step612]: loss 0.479392
[TEST step613]: loss 0.479435
[TEST step614]: loss 0.472648
[TEST step615]: loss 0.423613
[TEST step616]: loss 0.474723
[TEST step617]: loss 0.593033
[TEST step618]: loss 0.496073
[TEST step619]: loss 0.538202
[TEST step620]: loss 0.590350
[TEST step621]: loss 0.609350
[TEST step622]: loss 0.545533
[TEST step623]: loss 0.574551
[TEST step624]: loss 0.650293
[TEST step625]: loss 0.494905
[TEST step626]: loss 0.625355
[TEST step627]: loss 0.517056
[TEST step628]: loss 0.396384
[TEST step629]: loss 0.376022
[TEST step630]: loss 0.244865
[TEST step631]: loss 0.613135
[TEST step632]: loss 0.493503
[TEST step633]: loss 0.218891
[TEST step634]: loss 0.201673
[TEST step635]: loss 0.388665
[TEST step636]: loss 0.383107
[TEST step637]: loss 0.557041
[TEST step638]: loss 0.541641
[TEST step639]: loss 0.530275
[TEST step640]: loss 0.542705
[TEST step641]: loss 0.386401
[TEST step642]: loss 0.545891
[TEST step643]: loss 0.327061
[TEST step644]: loss 0.668738
[TEST step645]: loss 0.480612
[TEST step646]: loss 0.165136
[TEST step647]: loss 0.443294
[TEST step648]: loss 0.456152
[TEST step649]: loss 0.503187
[TEST step650]: loss 0.288104
[TEST step651]: loss 0.472600
[TEST step652]: loss 0.583721
[TEST step653]: loss 0.522546
[TEST step654]: loss 0.347541
[TEST step655]: loss 0.445880
[TEST step656]: loss 0.605413
[TEST step657]: loss 0.616666
[TEST step658]: loss 0.490306
[TEST step659]: loss 0.676276
[TEST step660]: loss 0.398264
[TEST step661]: loss 0.514008
[TEST step662]: loss 0.397956
[TEST step663]: loss 0.269715
[TEST step664]: loss 0.496674
[TEST step665]: loss 0.685497
[TEST step666]: loss 0.431889
[TEST step667]: loss 0.574013
[TEST step668]: loss 0.417827
[TEST step669]: loss 0.508840
[TEST step670]: loss 0.676070
[TEST step671]: loss 0.399349
[TEST step672]: loss 0.315077
[TEST step673]: loss 0.322192
[TEST step674]: loss 0.382207
[TEST step675]: loss 0.304078
[TEST step676]: loss 0.540785
[TEST step677]: loss 0.531472
[TEST step678]: loss 0.342168
[TEST step679]: loss 0.444345
[TEST step680]: loss 0.391473
[TEST step681]: loss 0.494713
[TEST step682]: loss 0.567797
[TEST step683]: loss 0.254763
[TEST step684]: loss 0.630514
[TEST step685]: loss 0.438955
[TEST step686]: loss 0.470901
[TEST step687]: loss 0.296243
[TEST step688]: loss 0.364091
[TEST step689]: loss 0.456246
[TEST step690]: loss 0.331191
[TEST step691]: loss 0.464544
[TEST step692]: loss 0.614089
[TEST step693]: loss 0.388018
[TEST step694]: loss 0.565914
[TEST step695]: loss 0.371287
[TEST step696]: loss 0.209718
[TEST step697]: loss 0.250500
[TEST step698]: loss 0.338218
[TEST step699]: loss 0.520459
[TEST step700]: loss 0.694071
[TEST step701]: loss 0.415445
[TEST step702]: loss 0.495066
[TEST step703]: loss 0.569063
[TEST step704]: loss 0.369771
[TEST step705]: loss 0.362704
[TEST step706]: loss 0.539204
[TEST step707]: loss 0.392190
[TEST step708]: loss 0.539859
[TEST step709]: loss 0.411822
[TEST step710]: loss 0.427248
[TEST step711]: loss 0.461562
[TEST step712]: loss 0.424844
[TEST step713]: loss 0.408643
[TEST step714]: loss 0.420913
[TEST step715]: loss 0.320909
[TEST step716]: loss 0.556650
[TEST step717]: loss 0.443353
[TEST step718]: loss 0.555358
[TEST step719]: loss 0.434505
[TEST step720]: loss 0.467882
[TEST step721]: loss 0.376818
[TEST step722]: loss 0.471765
[TEST step723]: loss 0.387159
[TEST step724]: loss 0.511630
[TEST step725]: loss 0.464862
[TEST step726]: loss 0.409901
[TEST step727]: loss 0.504419
[TEST step728]: loss 0.346805
[TEST step729]: loss 0.499651
[TEST step730]: loss 0.440776
[TEST step731]: loss 0.302472
[TEST step732]: loss 0.326622
[TEST step733]: loss 0.507662
[TEST step734]: loss 0.137155
[TEST step735]: loss 0.468116
[TEST step736]: loss 0.495396
[TEST step737]: loss 0.410316
[TEST step738]: loss 0.440276
[TEST step739]: loss 0.429546
[TEST step740]: loss 0.575729
[TEST step741]: loss 0.368015
[TEST step742]: loss 0.734184
[TEST step743]: loss 0.645211
[TEST step744]: loss 0.416627
[TEST step745]: loss 0.360147
[TEST step746]: loss 0.500120
[TEST step747]: loss 0.426575
[TEST step748]: loss 0.644231
[TEST step749]: loss 0.559592
[TEST step750]: loss 0.317503
[TEST step751]: loss 0.563033
[TEST step752]: loss 0.408190
[TEST step753]: loss 0.360286
[TEST step754]: loss 0.492861
[TEST step755]: loss 0.378555
[TEST step756]: loss 0.511345
[TEST step757]: loss 0.431810
[TEST step758]: loss 0.547154
[TEST step759]: loss 0.267258
[TEST step760]: loss 0.549771
[TEST step761]: loss 0.326310
[TEST step762]: loss 0.620378
[TEST step763]: loss 0.372571
[TEST step764]: loss 0.491791
[TEST step765]: loss 0.632668
[TEST step766]: loss 0.479561
[TEST step767]: loss 0.548509
[TEST step768]: loss 0.520821
[TEST step769]: loss 0.445936

[TEST]: avg loss 0.445936

