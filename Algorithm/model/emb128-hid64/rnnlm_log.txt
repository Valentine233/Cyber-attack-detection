[epoch1, step1]: loss 4.372550
[epoch1, step2]: loss 4.373096
[epoch1, step3]: loss 4.371439
[epoch1, step4]: loss 4.369821
[epoch1, step5]: loss 4.370571
[epoch1, step6]: loss 4.370086
[epoch1, step7]: loss 4.369057
[epoch1, step8]: loss 4.368940
[epoch1, step9]: loss 4.368416
[epoch1, step10]: loss 4.367641
[epoch1, step11]: loss 4.366923
[epoch1, step12]: loss 4.366356
[epoch1, step13]: loss 4.365794
[epoch1, step14]: loss 4.365209
[epoch1, step15]: loss 4.364915
[epoch1, step16]: loss 4.364810
[epoch1, step17]: loss 4.363520
[epoch1, step18]: loss 4.363351
[epoch1, step19]: loss 4.362393
[epoch1, step20]: loss 4.361681
[epoch1, step21]: loss 4.361542
[epoch1, step22]: loss 4.361156
[epoch1, step23]: loss 4.360752
[epoch1, step24]: loss 4.359782
[epoch1, step25]: loss 4.359431
[epoch1, step26]: loss 4.358668
[epoch1, step27]: loss 4.358418
[epoch1, step28]: loss 4.356906
[epoch1, step29]: loss 4.356843
[epoch1, step30]: loss 4.356568
[epoch1, step31]: loss 4.356777
[epoch1, step32]: loss 4.354245
[epoch1, step33]: loss 4.354880
[epoch1, step34]: loss 4.354123
[epoch1, step35]: loss 4.353330
[epoch1, step36]: loss 4.351547
[epoch1, step37]: loss 4.351761
[epoch1, step38]: loss 4.351706
[epoch1, step39]: loss 4.350634
[epoch1, step40]: loss 4.350172
[epoch1, step41]: loss 4.348932
[epoch1, step42]: loss 4.348492
[epoch1, step43]: loss 4.346694
[epoch1, step44]: loss 4.348880
[epoch1, step45]: loss 4.346697
[epoch1, step46]: loss 4.346136
[epoch1, step47]: loss 4.345361
[epoch1, step48]: loss 4.345483
[epoch1, step49]: loss 4.344058
[epoch1, step50]: loss 4.342898
[epoch1, step51]: loss 4.343780
[epoch1, step52]: loss 4.342229
[epoch1, step53]: loss 4.340724
[epoch1, step54]: loss 4.340377
[epoch1, step55]: loss 4.339476
[epoch1, step56]: loss 4.338164
[epoch1, step57]: loss 4.338406
[epoch1, step58]: loss 4.338125
[epoch1, step59]: loss 4.336174
[epoch1, step60]: loss 4.337285
[epoch1, step61]: loss 4.336285
[epoch1, step62]: loss 4.334980
[epoch1, step63]: loss 4.332224
[epoch1, step64]: loss 4.333799
[epoch1, step65]: loss 4.329289
[epoch1, step66]: loss 4.330655
[epoch1, step67]: loss 4.329241
[epoch1, step68]: loss 4.328562
[epoch1, step69]: loss 4.330793
[epoch1, step70]: loss 4.325489
[epoch1, step71]: loss 4.324717
[epoch1, step72]: loss 4.325532
[epoch1, step73]: loss 4.322165
[epoch1, step74]: loss 4.321603
[epoch1, step75]: loss 4.319811
[epoch1, step76]: loss 4.320049
[epoch1, step77]: loss 4.318936
[epoch1, step78]: loss 4.319242
[epoch1, step79]: loss 4.320646
[epoch1, step80]: loss 4.317013
[epoch1, step81]: loss 4.316518
[epoch1, step82]: loss 4.313843
[epoch1, step83]: loss 4.314708
[epoch1, step84]: loss 4.313997
[epoch1, step85]: loss 4.310787
[epoch1, step86]: loss 4.313236
[epoch1, step87]: loss 4.309504
[epoch1, step88]: loss 4.303330
[epoch1, step89]: loss 4.308575
[epoch1, step90]: loss 4.299982
[epoch1, step91]: loss 4.301406
[epoch1, step92]: loss 4.303144
[epoch1, step93]: loss 4.298165
[epoch1, step94]: loss 4.294410
[epoch1, step95]: loss 4.298271
[epoch1, step96]: loss 4.294537
[epoch1, step97]: loss 4.290252
[epoch1, step98]: loss 4.288427
[epoch1, step99]: loss 4.286870
[epoch1, step100]: loss 4.285666
[epoch1, step101]: loss 4.282347
[epoch1, step102]: loss 4.280756
[epoch1, step103]: loss 4.278309
[epoch1, step104]: loss 4.272427
[epoch1, step105]: loss 4.271119
[epoch1, step106]: loss 4.271260
[epoch1, step107]: loss 4.265725
[epoch1, step108]: loss 4.262426
[epoch1, step109]: loss 4.257275
[epoch1, step110]: loss 4.245323
[epoch1, step111]: loss 4.255540
[epoch1, step112]: loss 4.247021
[epoch1, step113]: loss 4.243032
[epoch1, step114]: loss 4.243831
[epoch1, step115]: loss 4.230678
[epoch1, step116]: loss 4.235141
[epoch1, step117]: loss 4.215141
[epoch1, step118]: loss 4.222955
[epoch1, step119]: loss 4.216804
[epoch1, step120]: loss 4.210815
[epoch1, step121]: loss 4.179098
[epoch1, step122]: loss 4.187339
[epoch1, step123]: loss 4.190416
[epoch1, step124]: loss 4.180161
[epoch1, step125]: loss 4.165660
[epoch1, step126]: loss 4.149922
[epoch1, step127]: loss 4.137251
[epoch1, step128]: loss 4.117491
[epoch1, step129]: loss 4.120076
[epoch1, step130]: loss 4.098887
[epoch1, step131]: loss 4.103150
[epoch1, step132]: loss 4.079828
[epoch1, step133]: loss 4.065572
[epoch1, step134]: loss 4.053154
[epoch1, step135]: loss 4.038932
[epoch1, step136]: loss 4.031968
[epoch1, step137]: loss 3.991367
[epoch1, step138]: loss 3.996973
[epoch1, step139]: loss 3.989122
[epoch1, step140]: loss 3.986478
[epoch1, step141]: loss 3.966088
[epoch1, step142]: loss 3.952794
[epoch1, step143]: loss 3.973456
[epoch1, step144]: loss 3.932230
[epoch1, step145]: loss 3.923727
[epoch1, step146]: loss 3.925653
[epoch1, step147]: loss 3.929243
[epoch1, step148]: loss 4.080517
[epoch1, step149]: loss 3.896011
[epoch1, step150]: loss 3.901361
[epoch1, step151]: loss 3.857465
[epoch1, step152]: loss 3.898237
[epoch1, step153]: loss 3.891747
[epoch1, step154]: loss 3.899094
[epoch1, step155]: loss 3.875179
[epoch1, step156]: loss 3.881605
[epoch1, step157]: loss 3.854164
[epoch1, step158]: loss 3.828001
[epoch1, step159]: loss 3.860196
[epoch1, step160]: loss 3.866463
[epoch1, step161]: loss 3.842961
[epoch1, step162]: loss 3.850131
[epoch1, step163]: loss 3.821671
[epoch1, step164]: loss 3.829131
[epoch1, step165]: loss 3.837122
[epoch1, step166]: loss 3.838712
[epoch1, step167]: loss 4.004869
[epoch1, step168]: loss 3.801677
[epoch1, step169]: loss 3.814235
[epoch1, step170]: loss 3.782176
[epoch1, step171]: loss 3.773304
[epoch1, step172]: loss 3.797473
[epoch1, step173]: loss 3.798521
[epoch1, step174]: loss 3.804042
[epoch1, step175]: loss 3.777198
[epoch1, step176]: loss 3.975549
[epoch1, step177]: loss 3.763828
[epoch1, step178]: loss 3.790408
[epoch1, step179]: loss 3.758171
[epoch1, step180]: loss 3.762004
[epoch1, step181]: loss 3.739021
[epoch1, step182]: loss 3.777739
[epoch1, step183]: loss 3.781137
[epoch1, step184]: loss 3.780415
[epoch1, step185]: loss 3.748605
[epoch1, step186]: loss 3.757837
[epoch1, step187]: loss 3.747710
[epoch1, step188]: loss 3.735824
[epoch1, step189]: loss 3.765989
[epoch1, step190]: loss 3.761388
[epoch1, step191]: loss 3.745543
[epoch1, step192]: loss 3.738690
[epoch1, step193]: loss 3.735907
[epoch1, step194]: loss 3.743073
[epoch1, step195]: loss 3.734152
[epoch1, step196]: loss 3.784279
[epoch1, step197]: loss 3.771813
[epoch1, step198]: loss 3.746981
[epoch1, step199]: loss 3.728025
[epoch1, step200]: loss 3.767434
[epoch1, step201]: loss 3.695834
[epoch1, step202]: loss 3.684640
[epoch1, step203]: loss 3.762975
[epoch1, step204]: loss 3.736776
[epoch1, step205]: loss 3.704415
[epoch1, step206]: loss 3.683878
[epoch1, step207]: loss 3.741027
[epoch1, step208]: loss 3.765511
[epoch1, step209]: loss 3.710565
[epoch1, step210]: loss 3.756396
[epoch1, step211]: loss 3.681441
[epoch1, step212]: loss 3.750862
[epoch1, step213]: loss 3.755657
[epoch1, step214]: loss 3.782757
[epoch1, step215]: loss 3.791342
[epoch1, step216]: loss 3.675957
[epoch1, step217]: loss 3.756929
[epoch1, step218]: loss 3.731930
[epoch1, step219]: loss 3.738481
[epoch1, step220]: loss 3.699260
[epoch1, step221]: loss 3.680762
[epoch1, step222]: loss 3.720274
[epoch1, step223]: loss 3.724563
[epoch1, step224]: loss 3.674111
[epoch1, step225]: loss 3.753229
[epoch1, step226]: loss 3.756562
[epoch1, step227]: loss 3.681388
[epoch1, step228]: loss 3.732187
[epoch1, step229]: loss 3.688823
[epoch1, step230]: loss 3.690651
[epoch1, step231]: loss 3.643635
[epoch1, step232]: loss 3.712918
[epoch1, step233]: loss 3.757503
[epoch1, step234]: loss 3.726074
[epoch1, step235]: loss 3.684339
[epoch1, step236]: loss 3.734961
[epoch1, step237]: loss 3.725114
[epoch1, step238]: loss 3.616430
[epoch1, step239]: loss 3.664824
[epoch1, step240]: loss 3.682459
[epoch1, step241]: loss 3.711191
[epoch1, step242]: loss 3.642494
[epoch1, step243]: loss 3.743492
[epoch1, step244]: loss 3.757298
[epoch1, step245]: loss 3.712072
[epoch1, step246]: loss 3.681571
[epoch1, step247]: loss 3.655775
[epoch1, step248]: loss 3.702370
[epoch1, step249]: loss 3.704831
[epoch1, step250]: loss 3.624246
[epoch1, step251]: loss 3.696990
[epoch1, step252]: loss 3.750126
[epoch1, step253]: loss 3.606541
[epoch1, step254]: loss 3.693568
[epoch1, step255]: loss 3.666048
[epoch1, step256]: loss 3.689405
[epoch1, step257]: loss 3.704105
[epoch1, step258]: loss 3.729070
[epoch1, step259]: loss 3.617998
[epoch1, step260]: loss 3.727151
[epoch1, step261]: loss 3.655408
[epoch1, step262]: loss 3.686764
[epoch1, step263]: loss 3.918103
[epoch1, step264]: loss 3.661721
[epoch1, step265]: loss 3.675677
[epoch1, step266]: loss 3.713671
[epoch1, step267]: loss 3.685013
[epoch1, step268]: loss 3.696216
[epoch1, step269]: loss 3.716250
[epoch1, step270]: loss 3.666715
[epoch1, step271]: loss 3.686669
[epoch1, step272]: loss 3.703297
[epoch1, step273]: loss 3.720260
[epoch1, step274]: loss 3.673106
[epoch1, step275]: loss 3.654504
[epoch1, step276]: loss 3.674408
[epoch1, step277]: loss 3.864145
[epoch1, step278]: loss 3.663555
[epoch1, step279]: loss 3.694952
[epoch1, step280]: loss 3.682996
[epoch1, step281]: loss 3.661796
[epoch1, step282]: loss 3.664767
[epoch1, step283]: loss 3.650415
[epoch1, step284]: loss 3.641332
[epoch1, step285]: loss 3.703468
[epoch1, step286]: loss 3.714350
[epoch1, step287]: loss 3.670554
[epoch1, step288]: loss 3.669010
[epoch1, step289]: loss 3.643790
[epoch1, step290]: loss 3.687050
[epoch1, step291]: loss 3.745703
[epoch1, step292]: loss 3.723300
[epoch1, step293]: loss 3.654888
[epoch1, step294]: loss 3.645679
[epoch1, step295]: loss 3.662558
[epoch1, step296]: loss 3.694562
[epoch1, step297]: loss 3.632558
[epoch1, step298]: loss 3.650902
[epoch1, step299]: loss 3.634916
[epoch1, step300]: loss 3.685561
[epoch1, step301]: loss 3.671543
[epoch1, step302]: loss 3.659265
[epoch1, step303]: loss 3.686252
[epoch1, step304]: loss 3.656824
[epoch1, step305]: loss 3.670364
[epoch1, step306]: loss 3.674867
[epoch1, step307]: loss 3.715750
[epoch1, step308]: loss 3.623946
[epoch1, step309]: loss 3.643048
[epoch1, step310]: loss 3.683964
[epoch1, step311]: loss 3.685728
[epoch1, step312]: loss 3.667697
[epoch1, step313]: loss 3.618720
[epoch1, step314]: loss 3.652381
[epoch1, step315]: loss 3.619026
[epoch1, step316]: loss 3.607235
[epoch1, step317]: loss 3.680001
[epoch1, step318]: loss 3.641524
[epoch1, step319]: loss 3.673895
[epoch1, step320]: loss 3.691102
[epoch1, step321]: loss 3.671251
[epoch1, step322]: loss 3.638454
[epoch1, step323]: loss 3.666065
[epoch1, step324]: loss 3.670572
[epoch1, step325]: loss 3.654911
[epoch1, step326]: loss 3.683902
[epoch1, step327]: loss 3.642680
[epoch1, step328]: loss 3.673209
[epoch1, step329]: loss 3.636990
[epoch1, step330]: loss 3.611176
[epoch1, step331]: loss 3.695023
[epoch1, step332]: loss 3.646489
[epoch1, step333]: loss 3.608043
[epoch1, step334]: loss 3.663755
[epoch1, step335]: loss 3.652645
[epoch1, step336]: loss 3.594496
[epoch1, step337]: loss 3.686154
[epoch1, step338]: loss 3.831745
[epoch1, step339]: loss 3.611622
[epoch1, step340]: loss 3.574523
[epoch1, step341]: loss 3.684154
[epoch1, step342]: loss 3.635467
[epoch1, step343]: loss 3.639135
[epoch1, step344]: loss 3.608725
[epoch1, step345]: loss 3.670897
[epoch1, step346]: loss 3.604249
[epoch1, step347]: loss 3.561940
[epoch1, step348]: loss 3.676148
[epoch1, step349]: loss 3.613016
[epoch1, step350]: loss 3.614209
[epoch1, step351]: loss 3.614233
[epoch1, step352]: loss 3.671194
[epoch1, step353]: loss 3.636775
[epoch1, step354]: loss 3.669709
[epoch1, step355]: loss 3.647409
[epoch1, step356]: loss 3.640688
[epoch1, step357]: loss 3.634444
[epoch1, step358]: loss 3.552257
[epoch1, step359]: loss 3.654698
[epoch1, step360]: loss 3.576321
[epoch1, step361]: loss 3.633646
[epoch1, step362]: loss 3.654989
[epoch1, step363]: loss 3.647303
[epoch1, step364]: loss 3.600246
[epoch1, step365]: loss 3.644841
[epoch1, step366]: loss 3.616852
[epoch1, step367]: loss 3.614530
[epoch1, step368]: loss 3.617146
[epoch1, step369]: loss 3.636330
[epoch1, step370]: loss 3.583284
[epoch1, step371]: loss 3.655484
[epoch1, step372]: loss 3.584961
[epoch1, step373]: loss 3.612195
[epoch1, step374]: loss 3.586997
[epoch1, step375]: loss 3.649731
[epoch1, step376]: loss 3.827333
[epoch1, step377]: loss 3.661097
[epoch1, step378]: loss 3.672892
[epoch1, step379]: loss 3.581790
[epoch1, step380]: loss 3.666589
[epoch1, step381]: loss 3.638242
[epoch1, step382]: loss 3.655629
[epoch1, step383]: loss 3.658592
[epoch1, step384]: loss 3.532684
[epoch1, step385]: loss 3.525415
[epoch1, step386]: loss 3.566362
[epoch1, step387]: loss 3.647207
[epoch1, step388]: loss 3.566830
[epoch1, step389]: loss 3.600270
[epoch1, step390]: loss 3.554031
[epoch1, step391]: loss 3.551633
[epoch1, step392]: loss 3.643136
[epoch1, step393]: loss 3.614164
[epoch1, step394]: loss 3.593101
[epoch1, step395]: loss 3.497828
[epoch1, step396]: loss 3.670775
[epoch1, step397]: loss 3.644535
[epoch1, step398]: loss 3.583033
[epoch1, step399]: loss 3.625680
[epoch1, step400]: loss 3.620182
[epoch1, step401]: loss 3.592857
[epoch1, step402]: loss 3.590648
[epoch1, step403]: loss 3.614911
[epoch1, step404]: loss 3.590042
[epoch1, step405]: loss 3.786332
[epoch1, step406]: loss 3.660103
[epoch1, step407]: loss 3.629988
[epoch1, step408]: loss 3.587847
[epoch1, step409]: loss 3.656948
[epoch1, step410]: loss 3.593601
[epoch1, step411]: loss 3.634123
[epoch1, step412]: loss 3.586814
[epoch1, step413]: loss 3.626472
[epoch1, step414]: loss 3.549414
[epoch1, step415]: loss 3.764625
[epoch1, step416]: loss 3.616064
[epoch1, step417]: loss 3.595145
[epoch1, step418]: loss 3.641956
[epoch1, step419]: loss 3.550202
[epoch1, step420]: loss 3.597124
[epoch1, step421]: loss 3.546148
[epoch1, step422]: loss 3.553980
[epoch1, step423]: loss 3.547844
[epoch1, step424]: loss 3.610849
[epoch1, step425]: loss 3.617532
[epoch1, step426]: loss 3.553900
[epoch1, step427]: loss 3.670617
[epoch1, step428]: loss 3.529882
[epoch1, step429]: loss 3.553692
[epoch1, step430]: loss 3.536768
[epoch1, step431]: loss 3.605901
[epoch1, step432]: loss 3.594098
[epoch1, step433]: loss 3.558579
[epoch1, step434]: loss 3.605594
[epoch1, step435]: loss 3.591905
[epoch1, step436]: loss 3.590220
[epoch1, step437]: loss 3.600760
[epoch1, step438]: loss 3.624019
[epoch1, step439]: loss 3.559969
[epoch1, step440]: loss 3.612493
[epoch1, step441]: loss 3.511829
[epoch1, step442]: loss 3.634857
[epoch1, step443]: loss 3.551572
[epoch1, step444]: loss 3.639191
[epoch1, step445]: loss 3.563585
[epoch1, step446]: loss 3.510375
[epoch1, step447]: loss 3.634303
[epoch1, step448]: loss 3.533501
[epoch1, step449]: loss 3.560323
[epoch1, step450]: loss 3.560049
[epoch1, step451]: loss 3.583438
[epoch1, step452]: loss 3.548377
[epoch1, step453]: loss 3.602938
[epoch1, step454]: loss 3.592264
[epoch1, step455]: loss 3.614717
[epoch1, step456]: loss 3.553047
[epoch1, step457]: loss 3.547521
[epoch1, step458]: loss 3.629920
[epoch1, step459]: loss 3.619417
[epoch1, step460]: loss 3.553729
[epoch1, step461]: loss 3.629480
[epoch1, step462]: loss 3.621294
[epoch1, step463]: loss 3.511822
[epoch1, step464]: loss 3.592998
[epoch1, step465]: loss 3.620258
[epoch1, step466]: loss 3.603174
[epoch1, step467]: loss 3.495889
[epoch1, step468]: loss 3.582390
[epoch1, step469]: loss 3.536258
[epoch1, step470]: loss 3.598630
[epoch1, step471]: loss 3.596606
[epoch1, step472]: loss 3.503965
[epoch1, step473]: loss 3.523272
[epoch1, step474]: loss 3.654917
[epoch1, step475]: loss 3.587384
[epoch1, step476]: loss 3.589644
[epoch1, step477]: loss 3.655334
[epoch1, step478]: loss 3.540870
[epoch1, step479]: loss 3.519999
[epoch1, step480]: loss 3.508725
[epoch1, step481]: loss 3.491561
[epoch1, step482]: loss 3.553497
[epoch1, step483]: loss 3.521713
[epoch1, step484]: loss 3.617604
[epoch1, step485]: loss 3.492494
[epoch1, step486]: loss 3.499907
[epoch1, step487]: loss 3.538740
[epoch1, step488]: loss 3.611064
[epoch1, step489]: loss 3.517332
[epoch1, step490]: loss 3.562258
[epoch1, step491]: loss 3.507306
[epoch1, step492]: loss 3.548555
[epoch1, step493]: loss 3.504898
[epoch1, step494]: loss 3.541345
[epoch1, step495]: loss 3.532681
[epoch1, step496]: loss 3.561036
[epoch1, step497]: loss 3.600381
[epoch1, step498]: loss 3.589902
[epoch1, step499]: loss 3.517950
[epoch1, step500]: loss 3.498216
[epoch1, step501]: loss 3.483558
[epoch1, step502]: loss 3.599073
[epoch1, step503]: loss 3.504487
[epoch1, step504]: loss 3.520260
[epoch1, step505]: loss 3.534747
[epoch1, step506]: loss 3.465585
[epoch1, step507]: loss 3.543772
[epoch1, step508]: loss 3.467623
[epoch1, step509]: loss 3.465542
[epoch1, step510]: loss 3.534540
[epoch1, step511]: loss 3.559916
[epoch1, step512]: loss 3.513497
[epoch1, step513]: loss 3.603069
[epoch1, step514]: loss 3.555543
[epoch1, step515]: loss 3.523290
[epoch1, step516]: loss 3.580325
[epoch1, step517]: loss 3.570313
[epoch1, step518]: loss 3.503684
[epoch1, step519]: loss 3.513480
[epoch1, step520]: loss 3.473990
[epoch1, step521]: loss 3.616142
[epoch1, step522]: loss 3.564910
[epoch1, step523]: loss 3.490429
[epoch1, step524]: loss 3.505454
[epoch1, step525]: loss 3.502859
[epoch1, step526]: loss 3.458143
[epoch1, step527]: loss 3.586174
[epoch1, step528]: loss 3.508416
[epoch1, step529]: loss 3.458939
[epoch1, step530]: loss 3.547920
[epoch1, step531]: loss 3.529024
[epoch1, step532]: loss 3.565215
[epoch1, step533]: loss 3.455039
[epoch1, step534]: loss 3.576209
[epoch1, step535]: loss 3.515432
[epoch1, step536]: loss 3.457573
[epoch1, step537]: loss 3.542974
[epoch1, step538]: loss 3.565850
[epoch1, step539]: loss 3.493766
[epoch1, step540]: loss 3.548557
[epoch1, step541]: loss 3.487443
[epoch1, step542]: loss 3.545263
[epoch1, step543]: loss 3.531236
[epoch1, step544]: loss 3.510962
[epoch1, step545]: loss 3.510468
[epoch1, step546]: loss 3.508300
[epoch1, step547]: loss 3.448393
[epoch1, step548]: loss 3.555838
[epoch1, step549]: loss 3.547141
[epoch1, step550]: loss 3.471799
[epoch1, step551]: loss 3.512724
[epoch1, step552]: loss 3.472535
[epoch1, step553]: loss 3.524693
[epoch1, step554]: loss 3.557689
[epoch1, step555]: loss 3.441178
[epoch1, step556]: loss 3.528498
[epoch1, step557]: loss 3.553485
[epoch1, step558]: loss 3.533066
[epoch1, step559]: loss 3.506857
[epoch1, step560]: loss 3.481433
[epoch1, step561]: loss 3.515562
[epoch1, step562]: loss 3.450732
[epoch1, step563]: loss 3.444612
[epoch1, step564]: loss 3.494523
[epoch1, step565]: loss 3.491894
[epoch1, step566]: loss 3.483982
[epoch1, step567]: loss 3.460553
[epoch1, step568]: loss 3.486253
[epoch1, step569]: loss 3.499450
[epoch1, step570]: loss 3.458299
[epoch1, step571]: loss 3.479542
[epoch1, step572]: loss 3.494042
[epoch1, step573]: loss 3.541542
[epoch1, step574]: loss 3.481781
[epoch1, step575]: loss 3.492992
[epoch1, step576]: loss 3.469061
[epoch1, step577]: loss 3.512814
[epoch1, step578]: loss 3.426685
[epoch1, step579]: loss 3.436612
[epoch1, step580]: loss 3.454631
[epoch1, step581]: loss 3.459416
[epoch1, step582]: loss 3.532804
[epoch1, step583]: loss 3.458287
[epoch1, step584]: loss 3.490720
[epoch1, step585]: loss 3.457828
[epoch1, step586]: loss 3.506466
[epoch1, step587]: loss 3.396620
[epoch1, step588]: loss 3.481149
[epoch1, step589]: loss 3.477027
[epoch1, step590]: loss 3.472639
[epoch1, step591]: loss 3.430071
[epoch1, step592]: loss 3.371631
[epoch1, step593]: loss 3.446034
[epoch1, step594]: loss 3.459720
[epoch1, step595]: loss 3.509214
[epoch1, step596]: loss 3.437274
[epoch1, step597]: loss 3.441242
[epoch1, step598]: loss 3.609913
[epoch1, step599]: loss 3.334523
[epoch1, step600]: loss 3.427680
[epoch1, step601]: loss 3.436509
[epoch1, step602]: loss 3.441142
[epoch1, step603]: loss 3.520956
[epoch1, step604]: loss 3.479040
[epoch1, step605]: loss 3.564843
[epoch1, step606]: loss 3.380743
[epoch1, step607]: loss 3.421954
[epoch1, step608]: loss 3.474280
[epoch1, step609]: loss 3.404130
[epoch1, step610]: loss 3.469685
[epoch1, step611]: loss 3.392344
[epoch1, step612]: loss 3.446490
[epoch1, step613]: loss 3.513930
[epoch1, step614]: loss 3.490974
[epoch1, step615]: loss 3.467473
[epoch1, step616]: loss 3.497419
[epoch1, step617]: loss 3.524590
[epoch1, step618]: loss 3.452832
[epoch1, step619]: loss 3.440967
[epoch1, step620]: loss 3.423666
[epoch1, step621]: loss 3.478550
[epoch1, step622]: loss 3.413001
[epoch1, step623]: loss 3.519300
[epoch1, step624]: loss 3.440793
[epoch1, step625]: loss 3.526104
[epoch1, step626]: loss 3.389759
[epoch1, step627]: loss 3.480331
[epoch1, step628]: loss 3.472100
[epoch1, step629]: loss 3.409379
[epoch1, step630]: loss 3.526077
[epoch1, step631]: loss 3.464081
[epoch1, step632]: loss 3.456341
[epoch1, step633]: loss 3.494585
[epoch1, step634]: loss 3.477117
[epoch1, step635]: loss 3.297275
[epoch1, step636]: loss 3.418539
[epoch1, step637]: loss 3.416343
[epoch1, step638]: loss 3.576108
[epoch1, step639]: loss 3.352683
[epoch1, step640]: loss 3.310294
[epoch1, step641]: loss 3.425688
[epoch1, step642]: loss 3.456790
[epoch1, step643]: loss 3.430694
[epoch1, step644]: loss 3.393851
[epoch1, step645]: loss 3.446412
[epoch1, step646]: loss 3.485514
[epoch1, step647]: loss 3.464032
[epoch1, step648]: loss 3.413894
[epoch1, step649]: loss 3.333644
[epoch1, step650]: loss 3.356260
[epoch1, step651]: loss 3.355179
[epoch1, step652]: loss 3.432561
[epoch1, step653]: loss 3.321722
[epoch1, step654]: loss 3.409329
[epoch1, step655]: loss 3.441300
[epoch1, step656]: loss 3.330134
[epoch1, step657]: loss 3.515058
[epoch1, step658]: loss 3.406146
[epoch1, step659]: loss 3.435248
[epoch1, step660]: loss 3.392521
[epoch1, step661]: loss 3.371851
[epoch1, step662]: loss 3.426228
[epoch1, step663]: loss 3.393367
[epoch1, step664]: loss 3.369629
[epoch1, step665]: loss 3.386155
[epoch1, step666]: loss 3.421062
[epoch1, step667]: loss 3.450631
[epoch1, step668]: loss 3.437412
[epoch1, step669]: loss 3.436320
[epoch1, step670]: loss 3.379032
[epoch1, step671]: loss 3.335569
[epoch1, step672]: loss 3.404608
[epoch1, step673]: loss 3.482035
[epoch1, step674]: loss 3.576632
[epoch1, step675]: loss 3.344387
[epoch1, step676]: loss 3.538906
[epoch1, step677]: loss 3.342267
[epoch1, step678]: loss 3.466916
[epoch1, step679]: loss 3.384681
[epoch1, step680]: loss 3.350610
[epoch1, step681]: loss 3.404854
[epoch1, step682]: loss 3.306246
[epoch1, step683]: loss 3.348622
[epoch1, step684]: loss 3.410668
[epoch1, step685]: loss 3.443540
[epoch1, step686]: loss 3.453991
[epoch1, step687]: loss 3.458170
[epoch1, step688]: loss 3.430198
[epoch1, step689]: loss 3.481592
[epoch1, step690]: loss 3.491049
[epoch1, step691]: loss 3.364739
[epoch1, step692]: loss 3.398140
[epoch1, step693]: loss 3.450263
[epoch1, step694]: loss 3.311472
[epoch1, step695]: loss 3.423487
[epoch1, step696]: loss 3.357602
[epoch1, step697]: loss 3.354253
[epoch1, step698]: loss 3.458025
[epoch1, step699]: loss 3.381487
[epoch1, step700]: loss 3.500238
[epoch1, step701]: loss 3.372573
[epoch1, step702]: loss 3.383756
[epoch1, step703]: loss 3.359446
[epoch1, step704]: loss 3.283007
[epoch1, step705]: loss 3.380715
[epoch1, step706]: loss 3.429727
[epoch1, step707]: loss 3.193561
[epoch1, step708]: loss 3.404667
[epoch1, step709]: loss 3.348460
[epoch1, step710]: loss 3.489818
[epoch1, step711]: loss 3.394976
[epoch1, step712]: loss 3.425699
[epoch1, step713]: loss 3.278491
[epoch1, step714]: loss 3.375515
[epoch1, step715]: loss 3.366982
[epoch1, step716]: loss 3.351717
[epoch1, step717]: loss 3.470253
[epoch1, step718]: loss 3.351393
[epoch1, step719]: loss 3.329605
[epoch1, step720]: loss 3.362467
[epoch1, step721]: loss 3.317931
[epoch1, step722]: loss 3.252563
[epoch1, step723]: loss 3.347316
[epoch1, step724]: loss 3.297339
[epoch1, step725]: loss 3.347044
[epoch1, step726]: loss 3.303815
[epoch1, step727]: loss 3.294108
[epoch1, step728]: loss 3.410053
[epoch1, step729]: loss 3.447249
[epoch1, step730]: loss 3.388295
[epoch1, step731]: loss 3.229127
[epoch1, step732]: loss 3.390252
[epoch1, step733]: loss 3.431892
[epoch1, step734]: loss 3.360249
[epoch1, step735]: loss 3.372295
[epoch1, step736]: loss 3.345576
[epoch1, step737]: loss 3.240119
[epoch1, step738]: loss 3.323711
[epoch1, step739]: loss 3.322670
[epoch1, step740]: loss 3.307333
[epoch1, step741]: loss 3.358474
[epoch1, step742]: loss 3.409845
[epoch1, step743]: loss 3.335333
[epoch1, step744]: loss 3.318694
[epoch1, step745]: loss 3.282921
[epoch1, step746]: loss 3.386202
[epoch1, step747]: loss 3.288294
[epoch1, step748]: loss 3.344608
[epoch1, step749]: loss 3.367978
[epoch1, step750]: loss 3.347046
[epoch1, step751]: loss 3.331929
[epoch1, step752]: loss 3.322736
[epoch1, step753]: loss 3.289382
[epoch1, step754]: loss 3.369765
[epoch1, step755]: loss 3.239601
[epoch1, step756]: loss 3.234391
[epoch1, step757]: loss 3.306756
[epoch1, step758]: loss 3.269377
[epoch1, step759]: loss 3.195880
[epoch1, step760]: loss 3.229492
[epoch1, step761]: loss 3.296420
[epoch1, step762]: loss 3.232726
[epoch1, step763]: loss 3.271551
[epoch1, step764]: loss 3.383444
[epoch1, step765]: loss 3.299960
[epoch1, step766]: loss 3.460277
[epoch1, step767]: loss 3.296861
[epoch1, step768]: loss 3.314582
[epoch1, step769]: loss 3.237224
[epoch1, step770]: loss 3.338314
[epoch1, step771]: loss 3.187074
[epoch1, step772]: loss 3.273769
[epoch1, step773]: loss 3.348923
[epoch1, step774]: loss 3.274869
[epoch1, step775]: loss 3.352752
[epoch1, step776]: loss 3.418610
[epoch1, step777]: loss 3.256078
[epoch1, step778]: loss 3.322206
[epoch1, step779]: loss 3.328459
[epoch1, step780]: loss 3.285481
[epoch1, step781]: loss 3.275972
[epoch1, step782]: loss 3.238981
[epoch1, step783]: loss 3.268337
[epoch1, step784]: loss 3.277898
[epoch1, step785]: loss 3.404323
[epoch1, step786]: loss 3.362347
[epoch1, step787]: loss 3.240355
[epoch1, step788]: loss 3.194061
[epoch1, step789]: loss 3.310856
[epoch1, step790]: loss 3.401171
[epoch1, step791]: loss 3.332223
[epoch1, step792]: loss 3.229815
[epoch1, step793]: loss 3.220204
[epoch1, step794]: loss 3.210007
[epoch1, step795]: loss 3.250526
[epoch1, step796]: loss 3.212062
[epoch1, step797]: loss 3.338992
[epoch1, step798]: loss 3.321017
[epoch1, step799]: loss 3.267279
[epoch1, step800]: loss 3.247058
[epoch1, step801]: loss 3.239789
[epoch1, step802]: loss 3.209186
[epoch1, step803]: loss 3.298640
[epoch1, step804]: loss 3.284732
[epoch1, step805]: loss 3.269356
[epoch1, step806]: loss 3.276297
[epoch1, step807]: loss 3.155931
[epoch1, step808]: loss 3.216175
[epoch1, step809]: loss 3.319778
[epoch1, step810]: loss 3.246301
[epoch1, step811]: loss 3.256503
[epoch1, step812]: loss 3.274871
[epoch1, step813]: loss 3.246983
[epoch1, step814]: loss 3.193074
[epoch1, step815]: loss 3.339964
[epoch1, step816]: loss 3.285846
[epoch1, step817]: loss 3.324541
[epoch1, step818]: loss 3.202552
[epoch1, step819]: loss 3.235481
[epoch1, step820]: loss 3.293530
[epoch1, step821]: loss 3.330791
[epoch1, step822]: loss 3.131074
[epoch1, step823]: loss 3.146979
[epoch1, step824]: loss 3.318975
[epoch1, step825]: loss 3.221630
[epoch1, step826]: loss 3.141235
[epoch1, step827]: loss 3.257476
[epoch1, step828]: loss 3.254445
[epoch1, step829]: loss 3.259163
[epoch1, step830]: loss 3.234114
[epoch1, step831]: loss 3.225089
[epoch1, step832]: loss 3.355664
[epoch1, step833]: loss 3.159214
[epoch1, step834]: loss 3.163448
[epoch1, step835]: loss 3.118350
[epoch1, step836]: loss 3.298696
[epoch1, step837]: loss 3.228596
[epoch1, step838]: loss 3.171972
[epoch1, step839]: loss 3.231838
[epoch1, step840]: loss 3.296015
[epoch1, step841]: loss 3.213846
[epoch1, step842]: loss 3.237707
[epoch1, step843]: loss 3.282891
[epoch1, step844]: loss 3.081982
[epoch1, step845]: loss 3.205829
[epoch1, step846]: loss 3.214661
[epoch1, step847]: loss 3.233387
[epoch1, step848]: loss 3.362658
[epoch1, step849]: loss 3.209960
[epoch1, step850]: loss 3.073405
[epoch1, step851]: loss 3.267697
[epoch1, step852]: loss 3.154960
[epoch1, step853]: loss 3.246644
[epoch1, step854]: loss 3.244217
[epoch1, step855]: loss 3.196551
[epoch1, step856]: loss 3.244412
[epoch1, step857]: loss 3.270447
[epoch1, step858]: loss 3.076106
[epoch1, step859]: loss 3.124878
[epoch1, step860]: loss 3.046255
[epoch1, step861]: loss 3.218133
[epoch1, step862]: loss 3.180479
[epoch1, step863]: loss 3.135331
[epoch1, step864]: loss 3.136422
[epoch1, step865]: loss 3.102405
[epoch1, step866]: loss 3.182212
[epoch1, step867]: loss 3.258708
[epoch1, step868]: loss 3.049231
[epoch1, step869]: loss 3.260873
[epoch1, step870]: loss 3.189103
[epoch1, step871]: loss 3.132302
[epoch1, step872]: loss 3.146038
[epoch1, step873]: loss 3.165923
[epoch1, step874]: loss 3.137948
[epoch1, step875]: loss 3.235838
[epoch1, step876]: loss 3.197094
[epoch1, step877]: loss 3.189487
[epoch1, step878]: loss 3.131338
[epoch1, step879]: loss 3.107885
[epoch1, step880]: loss 3.365523
[epoch1, step881]: loss 3.111562
[epoch1, step882]: loss 3.053216
[epoch1, step883]: loss 3.096014
[epoch1, step884]: loss 3.072846
[epoch1, step885]: loss 3.105143
[epoch1, step886]: loss 3.226968
[epoch1, step887]: loss 3.153062
[epoch1, step888]: loss 3.083186
[epoch1, step889]: loss 3.209520
[epoch1, step890]: loss 3.145681
[epoch1, step891]: loss 3.245826
[epoch1, step892]: loss 3.190585
[epoch1, step893]: loss 3.127442
[epoch1, step894]: loss 3.085312
[epoch1, step895]: loss 3.310157
[epoch1, step896]: loss 3.153680
[epoch1, step897]: loss 3.161501
[epoch1, step898]: loss 3.216505
[epoch1, step899]: loss 3.116632
[epoch1, step900]: loss 3.227461
[epoch1, step901]: loss 3.091208
[epoch1, step902]: loss 3.123214
[epoch1, step903]: loss 3.039103
[epoch1, step904]: loss 3.256222
[epoch1, step905]: loss 3.031533
[epoch1, step906]: loss 3.112444
[epoch1, step907]: loss 3.094366
[epoch1, step908]: loss 2.979073
[epoch1, step909]: loss 3.122075
[epoch1, step910]: loss 3.243466
[epoch1, step911]: loss 3.139734
[epoch1, step912]: loss 3.173155
[epoch1, step913]: loss 3.200517
[epoch1, step914]: loss 3.107267
[epoch1, step915]: loss 3.160196
[epoch1, step916]: loss 3.178943
[epoch1, step917]: loss 3.178756
[epoch1, step918]: loss 3.076143
[epoch1, step919]: loss 3.155390
[epoch1, step920]: loss 2.969862
[epoch1, step921]: loss 3.259392
[epoch1, step922]: loss 3.085458
[epoch1, step923]: loss 3.107689
[epoch1, step924]: loss 3.091597
[epoch1, step925]: loss 2.954078
[epoch1, step926]: loss 3.152439
[epoch1, step927]: loss 3.084142
[epoch1, step928]: loss 3.087939
[epoch1, step929]: loss 3.217706
[epoch1, step930]: loss 3.124497
[epoch1, step931]: loss 3.129290
[epoch1, step932]: loss 3.272552
[epoch1, step933]: loss 3.133717
[epoch1, step934]: loss 3.171615
[epoch1, step935]: loss 3.225762
[epoch1, step936]: loss 3.129714
[epoch1, step937]: loss 3.238595
[epoch1, step938]: loss 3.108695
[epoch1, step939]: loss 3.077813
[epoch1, step940]: loss 3.218598
[epoch1, step941]: loss 3.184740
[epoch1, step942]: loss 3.267199
[epoch1, step943]: loss 3.048105
[epoch1, step944]: loss 3.165197
[epoch1, step945]: loss 3.083123
[epoch1, step946]: loss 3.037835
[epoch1, step947]: loss 3.050643
[epoch1, step948]: loss 3.094532
[epoch1, step949]: loss 3.002400
[epoch1, step950]: loss 3.260331
[epoch1, step951]: loss 3.035868
[epoch1, step952]: loss 3.066322
[epoch1, step953]: loss 3.040368
[epoch1, step954]: loss 2.997580
[epoch1, step955]: loss 2.948881
[epoch1, step956]: loss 3.152240
[epoch1, step957]: loss 3.129263
[epoch1, step958]: loss 3.011301
[epoch1, step959]: loss 3.048274
[epoch1, step960]: loss 3.001969
[epoch1, step961]: loss 3.171823
[epoch1, step962]: loss 2.998475
[epoch1, step963]: loss 3.166727
[epoch1, step964]: loss 2.930428
[epoch1, step965]: loss 3.091136
[epoch1, step966]: loss 3.278004
[epoch1, step967]: loss 3.168477
[epoch1, step968]: loss 2.961830
[epoch1, step969]: loss 3.065605
[epoch1, step970]: loss 3.133815
[epoch1, step971]: loss 2.996535
[epoch1, step972]: loss 3.101306
[epoch1, step973]: loss 3.154917
[epoch1, step974]: loss 3.201258
[epoch1, step975]: loss 3.036351
[epoch1, step976]: loss 3.067005
[epoch1, step977]: loss 3.102843
[epoch1, step978]: loss 3.018512
[epoch1, step979]: loss 3.110509
[epoch1, step980]: loss 2.995333
[epoch1, step981]: loss 3.140444
[epoch1, step982]: loss 3.105885
[epoch1, step983]: loss 3.175693
[epoch1, step984]: loss 3.029857
[epoch1, step985]: loss 3.213632
[epoch1, step986]: loss 2.960819
[epoch1, step987]: loss 3.073671
[epoch1, step988]: loss 3.172817
[epoch1, step989]: loss 3.056990
[epoch1, step990]: loss 2.937882
[epoch1, step991]: loss 3.049546
[epoch1, step992]: loss 3.032683
[epoch1, step993]: loss 3.008111
[epoch1, step994]: loss 3.015877
[epoch1, step995]: loss 3.122604
[epoch1, step996]: loss 2.981514
[epoch1, step997]: loss 2.979523
[epoch1, step998]: loss 3.210522
[epoch1, step999]: loss 3.125345
[epoch1, step1000]: loss 2.997262
[epoch1, step1001]: loss 3.061042
[epoch1, step1002]: loss 3.104429
[epoch1, step1003]: loss 3.003907
[epoch1, step1004]: loss 2.967925
[epoch1, step1005]: loss 3.123684
[epoch1, step1006]: loss 3.055773
[epoch1, step1007]: loss 3.096922
[epoch1, step1008]: loss 2.950986
[epoch1, step1009]: loss 3.064362
[epoch1, step1010]: loss 3.115921
[epoch1, step1011]: loss 2.944848
[epoch1, step1012]: loss 2.946896
[epoch1, step1013]: loss 3.044666
[epoch1, step1014]: loss 3.025457
[epoch1, step1015]: loss 3.075370
[epoch1, step1016]: loss 2.926060
[epoch1, step1017]: loss 3.047791
[epoch1, step1018]: loss 2.950350
[epoch1, step1019]: loss 2.926666
[epoch1, step1020]: loss 3.010498
[epoch1, step1021]: loss 3.015361
[epoch1, step1022]: loss 3.106625
[epoch1, step1023]: loss 3.053921
[epoch1, step1024]: loss 3.073697
[epoch1, step1025]: loss 3.033935
[epoch1, step1026]: loss 3.089940
[epoch1, step1027]: loss 2.979692
[epoch1, step1028]: loss 2.859352
[epoch1, step1029]: loss 2.987729
[epoch1, step1030]: loss 3.002255
[epoch1, step1031]: loss 3.087855
[epoch1, step1032]: loss 2.968674
[epoch1, step1033]: loss 2.971430
[epoch1, step1034]: loss 3.087901
[epoch1, step1035]: loss 3.100463
[epoch1, step1036]: loss 3.013596
[epoch1, step1037]: loss 2.832852
[epoch1, step1038]: loss 3.102820
[epoch1, step1039]: loss 2.981431
[epoch1, step1040]: loss 3.043972
[epoch1, step1041]: loss 2.993462
[epoch1, step1042]: loss 2.918994
[epoch1, step1043]: loss 3.038676
[epoch1, step1044]: loss 3.055227
[epoch1, step1045]: loss 2.922412
[epoch1, step1046]: loss 2.961622
[epoch1, step1047]: loss 3.021591
[epoch1, step1048]: loss 3.123498
[epoch1, step1049]: loss 3.007222
[epoch1, step1050]: loss 2.931843
[epoch1, step1051]: loss 2.931499
[epoch1, step1052]: loss 3.029105
[epoch1, step1053]: loss 3.056768
[epoch1, step1054]: loss 2.966487
[epoch1, step1055]: loss 2.972407
[epoch1, step1056]: loss 2.976533
[epoch1, step1057]: loss 2.994118
[epoch1, step1058]: loss 2.966755
[epoch1, step1059]: loss 3.055071
[epoch1, step1060]: loss 2.992357
[epoch1, step1061]: loss 3.048476
[epoch1, step1062]: loss 2.994340
[epoch1, step1063]: loss 3.000681
[epoch1, step1064]: loss 2.976807
[epoch1, step1065]: loss 2.913416
[epoch1, step1066]: loss 2.994624
[epoch1, step1067]: loss 2.944037
[epoch1, step1068]: loss 2.982590
[epoch1, step1069]: loss 2.893046
[epoch1, step1070]: loss 2.829610
[epoch1, step1071]: loss 3.057849
[epoch1, step1072]: loss 2.952717
[epoch1, step1073]: loss 2.996523
[epoch1, step1074]: loss 2.839782
[epoch1, step1075]: loss 2.919235
[epoch1, step1076]: loss 3.010931
[epoch1, step1077]: loss 2.979110
[epoch1, step1078]: loss 2.941344
[epoch1, step1079]: loss 3.125564
[epoch1, step1080]: loss 2.992454
[epoch1, step1081]: loss 2.935118
[epoch1, step1082]: loss 2.961387
[epoch1, step1083]: loss 3.026550
[epoch1, step1084]: loss 3.074574
[epoch1, step1085]: loss 2.847737
[epoch1, step1086]: loss 2.753710
[epoch1, step1087]: loss 2.985638
[epoch1, step1088]: loss 2.892558
[epoch1, step1089]: loss 3.071404
[epoch1, step1090]: loss 2.883207
[epoch1, step1091]: loss 3.070973
[epoch1, step1092]: loss 2.908314
[epoch1, step1093]: loss 2.883631
[epoch1, step1094]: loss 3.009912
[epoch1, step1095]: loss 3.040235
[epoch1, step1096]: loss 2.903872
[epoch1, step1097]: loss 2.814198
[epoch1, step1098]: loss 2.891087
[epoch1, step1099]: loss 2.880323
[epoch1, step1100]: loss 2.875297
[epoch1, step1101]: loss 2.957964
[epoch1, step1102]: loss 2.952952
[epoch1, step1103]: loss 2.851027
[epoch1, step1104]: loss 3.075932
[epoch1, step1105]: loss 2.928265
[epoch1, step1106]: loss 3.019758
[epoch1, step1107]: loss 2.957804
[epoch1, step1108]: loss 2.960378
[epoch1, step1109]: loss 2.973164
[epoch1, step1110]: loss 2.957335
[epoch1, step1111]: loss 2.633912
[epoch1, step1112]: loss 3.002484
[epoch1, step1113]: loss 2.898478
[epoch1, step1114]: loss 2.931720
[epoch1, step1115]: loss 2.936183
[epoch1, step1116]: loss 2.991430
[epoch1, step1117]: loss 2.920051
[epoch1, step1118]: loss 3.028981
[epoch1, step1119]: loss 2.878408
[epoch1, step1120]: loss 2.805663
[epoch1, step1121]: loss 2.927445
[epoch1, step1122]: loss 2.713025
[epoch1, step1123]: loss 2.777397
[epoch1, step1124]: loss 2.852594
[epoch1, step1125]: loss 2.977343
[epoch1, step1126]: loss 2.849824
[epoch1, step1127]: loss 2.727370
[epoch1, step1128]: loss 2.880958
[epoch1, step1129]: loss 2.993284
[epoch1, step1130]: loss 2.908236
[epoch1, step1131]: loss 2.994171
[epoch1, step1132]: loss 2.815743
[epoch1, step1133]: loss 2.922463
[epoch1, step1134]: loss 3.079498
[epoch1, step1135]: loss 2.839811
[epoch1, step1136]: loss 2.801538
[epoch1, step1137]: loss 2.930540
[epoch1, step1138]: loss 2.927238
[epoch1, step1139]: loss 2.944186
[epoch1, step1140]: loss 2.815918
[epoch1, step1141]: loss 2.796907
[epoch1, step1142]: loss 3.011284
[epoch1, step1143]: loss 2.998796
[epoch1, step1144]: loss 2.993345
[epoch1, step1145]: loss 2.589095
[epoch1, step1146]: loss 2.964678
[epoch1, step1147]: loss 3.007118
[epoch1, step1148]: loss 2.595065
[epoch1, step1149]: loss 2.902460
[epoch1, step1150]: loss 2.835824
[epoch1, step1151]: loss 2.730278
[epoch1, step1152]: loss 2.713651
[epoch1, step1153]: loss 2.954479
[epoch1, step1154]: loss 2.936083
[epoch1, step1155]: loss 2.932508
[epoch1, step1156]: loss 2.808426
[epoch1, step1157]: loss 2.804580
[epoch1, step1158]: loss 2.757191
[epoch1, step1159]: loss 2.735737
[epoch1, step1160]: loss 2.910160
[epoch1, step1161]: loss 2.829673
[epoch1, step1162]: loss 2.942450
[epoch1, step1163]: loss 2.853930
[epoch1, step1164]: loss 3.014443
[epoch1, step1165]: loss 2.819580
[epoch1, step1166]: loss 2.822611
[epoch1, step1167]: loss 2.871689
[epoch1, step1168]: loss 2.980435
[epoch1, step1169]: loss 2.955226
[epoch1, step1170]: loss 2.890550
[epoch1, step1171]: loss 3.023815
[epoch1, step1172]: loss 2.948999
[epoch1, step1173]: loss 2.779623
[epoch1, step1174]: loss 2.876455
[epoch1, step1175]: loss 2.927991
[epoch1, step1176]: loss 2.731615
[epoch1, step1177]: loss 2.746480
[epoch1, step1178]: loss 2.886655
[epoch1, step1179]: loss 2.787692
[epoch1, step1180]: loss 2.882508
[epoch1, step1181]: loss 2.836385
[epoch1, step1182]: loss 2.647703
[epoch1, step1183]: loss 2.532017
[epoch1, step1184]: loss 2.898433
[epoch1, step1185]: loss 2.808782
[epoch1, step1186]: loss 2.991380
[epoch1, step1187]: loss 2.765932
[epoch1, step1188]: loss 2.972600
[epoch1, step1189]: loss 2.879524
[epoch1, step1190]: loss 2.822802
[epoch1, step1191]: loss 2.744248
[epoch1, step1192]: loss 2.933166
[epoch1, step1193]: loss 2.890190
[epoch1, step1194]: loss 2.833648
[epoch1, step1195]: loss 2.837969
[epoch1, step1196]: loss 2.913131
[epoch1, step1197]: loss 2.971154
[epoch1, step1198]: loss 2.935787
[epoch1, step1199]: loss 2.856042
[epoch1, step1200]: loss 2.855217
[epoch1, step1201]: loss 2.972018
[epoch1, step1202]: loss 2.746806
[epoch1, step1203]: loss 2.809574
[epoch1, step1204]: loss 2.739278
[epoch1, step1205]: loss 2.945431
[epoch1, step1206]: loss 2.952261
[epoch1, step1207]: loss 2.869498
[epoch1, step1208]: loss 2.937937
[epoch1, step1209]: loss 2.754994
[epoch1, step1210]: loss 2.824141
[epoch1, step1211]: loss 2.840292
[epoch1, step1212]: loss 2.811460
[epoch1, step1213]: loss 2.870104
[epoch1, step1214]: loss 2.793860
[epoch1, step1215]: loss 2.827457
[epoch1, step1216]: loss 2.993013
[epoch1, step1217]: loss 2.793832
[epoch1, step1218]: loss 2.832519
[epoch1, step1219]: loss 2.843054
[epoch1, step1220]: loss 2.496535
[epoch1, step1221]: loss 2.859080
[epoch1, step1222]: loss 2.917216
[epoch1, step1223]: loss 2.678305
[epoch1, step1224]: loss 2.708778
[epoch1, step1225]: loss 3.007803
[epoch1, step1226]: loss 2.672614
[epoch1, step1227]: loss 2.950479
[epoch1, step1228]: loss 2.898949
[epoch1, step1229]: loss 2.917591
[epoch1, step1230]: loss 2.835639
[epoch1, step1231]: loss 2.670439
[epoch1, step1232]: loss 2.823533
[epoch1, step1233]: loss 2.808657
[epoch1, step1234]: loss 2.931055
[epoch1, step1235]: loss 2.865011
[epoch1, step1236]: loss 2.837276
[epoch1, step1237]: loss 2.724189
[epoch1, step1238]: loss 2.789368
[epoch1, step1239]: loss 2.781461
[epoch1, step1240]: loss 2.933104
[epoch1, step1241]: loss 2.779416
[epoch1, step1242]: loss 2.695324
[epoch1, step1243]: loss 2.888296
[epoch1, step1244]: loss 2.787098
[epoch1, step1245]: loss 2.746291
[epoch1, step1246]: loss 2.851674
[epoch1, step1247]: loss 2.771746
[epoch1, step1248]: loss 2.768330
[epoch1, step1249]: loss 2.744862
[epoch1, step1250]: loss 2.903846
[epoch1, step1251]: loss 2.914231
[epoch1, step1252]: loss 2.924377
[epoch1, step1253]: loss 2.723305
[epoch1, step1254]: loss 2.784668
[epoch1, step1255]: loss 2.800343
[epoch1, step1256]: loss 3.024747
[epoch1, step1257]: loss 2.713943
[epoch1, step1258]: loss 2.674753
[epoch1, step1259]: loss 2.715069
[epoch1, step1260]: loss 2.617256
[epoch1, step1261]: loss 2.700594
[epoch1, step1262]: loss 2.904244
[epoch1, step1263]: loss 2.648399
[epoch1, step1264]: loss 2.784521
[epoch1, step1265]: loss 2.706000
[epoch1, step1266]: loss 2.835018
[epoch1, step1267]: loss 2.708685
[epoch1, step1268]: loss 2.782358
[epoch1, step1269]: loss 2.672581
[epoch1, step1270]: loss 2.726062
[epoch1, step1271]: loss 2.460879
[epoch1, step1272]: loss 2.707701
[epoch1, step1273]: loss 2.794556
[epoch1, step1274]: loss 2.816426
[epoch1, step1275]: loss 2.832895
[epoch1, step1276]: loss 2.729294
[epoch1, step1277]: loss 2.611629
[epoch1, step1278]: loss 2.576540
[epoch1, step1279]: loss 2.895789
[epoch1, step1280]: loss 2.784220
[epoch1, step1281]: loss 2.585679
[epoch1, step1282]: loss 2.612414
[epoch1, step1283]: loss 2.834059
[epoch1, step1284]: loss 2.720436
[epoch1, step1285]: loss 2.755240
[epoch1, step1286]: loss 2.652856
[epoch1, step1287]: loss 2.805481
[epoch1, step1288]: loss 2.803390
[epoch1, step1289]: loss 2.723200
[epoch1, step1290]: loss 2.647280
[epoch1, step1291]: loss 2.649984
[epoch1, step1292]: loss 2.716388
[epoch1, step1293]: loss 2.677809
[epoch1, step1294]: loss 2.666779
[epoch1, step1295]: loss 2.807734
[epoch1, step1296]: loss 2.787948
[epoch1, step1297]: loss 2.867263
[epoch1, step1298]: loss 2.815834
[epoch1, step1299]: loss 2.756016
[epoch1, step1300]: loss 2.799318
[epoch1, step1301]: loss 2.710741
[epoch1, step1302]: loss 2.816413
[epoch1, step1303]: loss 2.426062
[epoch1, step1304]: loss 2.816540
[epoch1, step1305]: loss 2.656278
[epoch1, step1306]: loss 2.674832
[epoch1, step1307]: loss 2.786867
[epoch1, step1308]: loss 2.730153
[epoch1, step1309]: loss 2.921552
[epoch1, step1310]: loss 2.713482
[epoch1, step1311]: loss 2.655675
[epoch1, step1312]: loss 2.651256
[epoch1, step1313]: loss 2.721250
[epoch1, step1314]: loss 2.493388
[epoch1, step1315]: loss 2.682127
[epoch1, step1316]: loss 2.756386
[epoch1, step1317]: loss 2.874971
[epoch1, step1318]: loss 2.661742
[epoch1, step1319]: loss 2.684555
[epoch1, step1320]: loss 2.872273
[epoch1, step1321]: loss 2.727686
[epoch1, step1322]: loss 2.609538
[epoch1, step1323]: loss 2.705579
[epoch1, step1324]: loss 2.751106
[epoch1, step1325]: loss 2.805326
[epoch1, step1326]: loss 2.816040
[epoch1, step1327]: loss 2.511812
[epoch1, step1328]: loss 2.575768
[epoch1, step1329]: loss 2.679086
[epoch1, step1330]: loss 2.664082
[epoch1, step1331]: loss 2.680216
[epoch1, step1332]: loss 2.719016
[epoch1, step1333]: loss 2.381752
[epoch1, step1334]: loss 2.689776
[epoch1, step1335]: loss 2.725399
[epoch1, step1336]: loss 2.856337
[epoch1, step1337]: loss 2.322476
[epoch1, step1338]: loss 2.779437
[epoch1, step1339]: loss 2.649256
[epoch1, step1340]: loss 2.567163
[epoch1, step1341]: loss 2.843977
[epoch1, step1342]: loss 2.645129
[epoch1, step1343]: loss 2.743293
[epoch1, step1344]: loss 2.438351
[epoch1, step1345]: loss 2.604742
[epoch1, step1346]: loss 2.739903
[epoch1, step1347]: loss 2.421125
[epoch1, step1348]: loss 2.545404
[epoch1, step1349]: loss 2.697030
[epoch1, step1350]: loss 2.574552
[epoch1, step1351]: loss 2.734732
[epoch1, step1352]: loss 2.460843
[epoch1, step1353]: loss 2.566332
[epoch1, step1354]: loss 2.569388
[epoch1, step1355]: loss 2.538742
[epoch1, step1356]: loss 2.817049
[epoch1, step1357]: loss 2.625580
[epoch1, step1358]: loss 2.680963
[epoch1, step1359]: loss 2.625353
[epoch1, step1360]: loss 2.643021
[epoch1, step1361]: loss 2.521389
[epoch1, step1362]: loss 2.388562
[epoch1, step1363]: loss 2.676609
[epoch1, step1364]: loss 2.648642
[epoch1, step1365]: loss 2.584968
[epoch1, step1366]: loss 2.599802
[epoch1, step1367]: loss 2.844244
[epoch1, step1368]: loss 2.588764
[epoch1, step1369]: loss 2.609010
[epoch1, step1370]: loss 2.644098
[epoch1, step1371]: loss 2.533530
[epoch1, step1372]: loss 2.732766
[epoch1, step1373]: loss 2.664017
[epoch1, step1374]: loss 2.603930
[epoch1, step1375]: loss 2.700032
[epoch1, step1376]: loss 2.777157
[epoch1, step1377]: loss 2.852618
[epoch1, step1378]: loss 2.810314
[epoch1, step1379]: loss 2.593492
[epoch1, step1380]: loss 2.642357
[epoch1, step1381]: loss 2.500288
[epoch1, step1382]: loss 2.797673
[epoch1, step1383]: loss 2.737720
[epoch1, step1384]: loss 2.553816
[epoch1, step1385]: loss 2.639691
[epoch1, step1386]: loss 2.685142
[epoch1, step1387]: loss 2.771249
[epoch1, step1388]: loss 2.468246
[epoch1, step1389]: loss 2.616701
[epoch1, step1390]: loss 2.610246
[epoch1, step1391]: loss 2.713688
[epoch1, step1392]: loss 2.878662
[epoch1, step1393]: loss 2.649813
[epoch1, step1394]: loss 2.675405
[epoch1, step1395]: loss 2.433814
[epoch1, step1396]: loss 2.508360
[epoch1, step1397]: loss 2.526756
[epoch1, step1398]: loss 2.628335
[epoch1, step1399]: loss 2.605218
[epoch1, step1400]: loss 2.622439
[epoch1, step1401]: loss 2.443473
[epoch1, step1402]: loss 2.731173
[epoch1, step1403]: loss 2.454722
[epoch1, step1404]: loss 2.660310
[epoch1, step1405]: loss 2.556205
[epoch1, step1406]: loss 2.708269
[epoch1, step1407]: loss 2.641109
[epoch1, step1408]: loss 2.558980
[epoch1, step1409]: loss 2.600119
[epoch1, step1410]: loss 2.652560
[epoch1, step1411]: loss 2.679589
[epoch1, step1412]: loss 2.742794
[epoch1, step1413]: loss 2.574870
[epoch1, step1414]: loss 2.695250
[epoch1, step1415]: loss 2.571082
[epoch1, step1416]: loss 2.666053
[epoch1, step1417]: loss 2.534579
[epoch1, step1418]: loss 2.553304
[epoch1, step1419]: loss 2.580528
[epoch1, step1420]: loss 2.420948
[epoch1, step1421]: loss 2.731106
[epoch1, step1422]: loss 2.640457
[epoch1, step1423]: loss 2.577945
[epoch1, step1424]: loss 2.541885
[epoch1, step1425]: loss 2.603340
[epoch1, step1426]: loss 2.462879
[epoch1, step1427]: loss 2.541015
[epoch1, step1428]: loss 2.655844
[epoch1, step1429]: loss 2.463459
[epoch1, step1430]: loss 2.438273
[epoch1, step1431]: loss 2.779531
[epoch1, step1432]: loss 2.661290
[epoch1, step1433]: loss 2.737228
[epoch1, step1434]: loss 2.636825
[epoch1, step1435]: loss 2.654743
[epoch1, step1436]: loss 2.889007
[epoch1, step1437]: loss 2.519590
[epoch1, step1438]: loss 2.537436
[epoch1, step1439]: loss 2.619554
[epoch1, step1440]: loss 2.572981
[epoch1, step1441]: loss 2.671746
[epoch1, step1442]: loss 2.562771
[epoch1, step1443]: loss 2.712611
[epoch1, step1444]: loss 2.448684
[epoch1, step1445]: loss 2.569446
[epoch1, step1446]: loss 2.572196
[epoch1, step1447]: loss 2.803642
[epoch1, step1448]: loss 2.528338
[epoch1, step1449]: loss 2.415210
[epoch1, step1450]: loss 2.698650
[epoch1, step1451]: loss 2.619984
[epoch1, step1452]: loss 2.648829
[epoch1, step1453]: loss 2.546927
[epoch1, step1454]: loss 2.751029
[epoch1, step1455]: loss 2.552080
[epoch1, step1456]: loss 2.634578
[epoch1, step1457]: loss 2.657922
[epoch1, step1458]: loss 2.535240
[epoch1, step1459]: loss 2.746810
[epoch1, step1460]: loss 2.336561
[epoch1, step1461]: loss 2.395297
[epoch1, step1462]: loss 2.621389
[epoch1, step1463]: loss 2.686939
[epoch1, step1464]: loss 2.460874
[epoch1, step1465]: loss 2.735422
[epoch1, step1466]: loss 2.476691
[epoch1, step1467]: loss 2.494820
[epoch1, step1468]: loss 2.608252
[epoch1, step1469]: loss 2.736190
[epoch1, step1470]: loss 2.640649
[epoch1, step1471]: loss 2.658815
[epoch1, step1472]: loss 2.439727
[epoch1, step1473]: loss 2.498054
[epoch1, step1474]: loss 2.531641
[epoch1, step1475]: loss 2.487774
[epoch1, step1476]: loss 2.535882
[epoch1, step1477]: loss 2.367404
[epoch1, step1478]: loss 2.594645
[epoch1, step1479]: loss 2.600078
[epoch1, step1480]: loss 2.515959
[epoch1, step1481]: loss 2.312662
[epoch1, step1482]: loss 2.649640
[epoch1, step1483]: loss 2.606290
[epoch1, step1484]: loss 2.611477
[epoch1, step1485]: loss 2.598165
[epoch1, step1486]: loss 2.451633
[epoch1, step1487]: loss 2.614284
[epoch1, step1488]: loss 2.728678
[epoch1, step1489]: loss 2.461071
[epoch1, step1490]: loss 2.722950
[epoch1, step1491]: loss 2.467517
[epoch1, step1492]: loss 2.443119
[epoch1, step1493]: loss 2.438942
[epoch1, step1494]: loss 2.491529
[epoch1, step1495]: loss 2.582072
[epoch1, step1496]: loss 2.732876
[epoch1, step1497]: loss 2.577090
[epoch1, step1498]: loss 2.755194
[epoch1, step1499]: loss 2.555989
[epoch1, step1500]: loss 2.471539
[epoch1, step1501]: loss 2.433863
[epoch1, step1502]: loss 2.515852
[epoch1, step1503]: loss 2.618087
[epoch1, step1504]: loss 2.462723
[epoch1, step1505]: loss 2.556194
[epoch1, step1506]: loss 2.415975
[epoch1, step1507]: loss 2.491569
[epoch1, step1508]: loss 2.697654
[epoch1, step1509]: loss 2.402247
[epoch1, step1510]: loss 2.642192
[epoch1, step1511]: loss 2.484961
[epoch1, step1512]: loss 2.637964
[epoch1, step1513]: loss 2.563307
[epoch1, step1514]: loss 2.510973
[epoch1, step1515]: loss 2.485934
[epoch1, step1516]: loss 2.504322
[epoch1, step1517]: loss 2.481407
[epoch1, step1518]: loss 2.456867
[epoch1, step1519]: loss 2.344512
[epoch1, step1520]: loss 2.631262
[epoch1, step1521]: loss 2.099984
[epoch1, step1522]: loss 2.355785
[epoch1, step1523]: loss 2.427678
[epoch1, step1524]: loss 2.574971
[epoch1, step1525]: loss 2.498309
[epoch1, step1526]: loss 2.804734
[epoch1, step1527]: loss 2.470205
[epoch1, step1528]: loss 2.583894
[epoch1, step1529]: loss 2.674982
[epoch1, step1530]: loss 2.642188
[epoch1, step1531]: loss 2.489340
[epoch1, step1532]: loss 2.490329
[epoch1, step1533]: loss 2.696059
[epoch1, step1534]: loss 2.410970
[epoch1, step1535]: loss 2.225146
[epoch1, step1536]: loss 2.490628
[epoch1, step1537]: loss 2.624480
[epoch1, step1538]: loss 2.581880
[epoch1, step1539]: loss 2.436409
[epoch1, step1540]: loss 2.539505
[epoch1, step1541]: loss 2.613485
[epoch1, step1542]: loss 2.495825
[epoch1, step1543]: loss 2.404406
[epoch1, step1544]: loss 2.476892
[epoch1, step1545]: loss 2.678234
[epoch1, step1546]: loss 2.562693
[epoch1, step1547]: loss 2.358961
[epoch1, step1548]: loss 2.464336
[epoch1, step1549]: loss 2.630398
[epoch1, step1550]: loss 2.241013
[epoch1, step1551]: loss 2.420439
[epoch1, step1552]: loss 2.577897
[epoch1, step1553]: loss 2.366631
[epoch1, step1554]: loss 2.648081
[epoch1, step1555]: loss 2.619384
[epoch1, step1556]: loss 2.414927
[epoch1, step1557]: loss 2.516560
[epoch1, step1558]: loss 2.445692
[epoch1, step1559]: loss 2.462177
[epoch1, step1560]: loss 2.177577
[epoch1, step1561]: loss 2.395337
[epoch1, step1562]: loss 2.781505
[epoch1, step1563]: loss 2.651609
[epoch1, step1564]: loss 2.531735
[epoch1, step1565]: loss 2.515218
[epoch1, step1566]: loss 2.183541
[epoch1, step1567]: loss 2.415266
[epoch1, step1568]: loss 2.213795
[epoch1, step1569]: loss 2.275974
[epoch1, step1570]: loss 2.740822
[epoch1, step1571]: loss 2.467820
[epoch1, step1572]: loss 2.389803
[epoch1, step1573]: loss 2.336200
[epoch1, step1574]: loss 2.555117
[epoch1, step1575]: loss 2.446354
[epoch1, step1576]: loss 2.486183
[epoch1, step1577]: loss 2.599258
[epoch1, step1578]: loss 2.521452
[epoch1, step1579]: loss 2.441705
[epoch1, step1580]: loss 2.645578
[epoch1, step1581]: loss 2.717142
[epoch1, step1582]: loss 2.301623
[epoch1, step1583]: loss 2.484173
[epoch1, step1584]: loss 2.403646
[epoch1, step1585]: loss 2.347718
[epoch1, step1586]: loss 2.110676
[epoch1, step1587]: loss 2.517703
[epoch1, step1588]: loss 2.273972
[epoch1, step1589]: loss 2.465576
[epoch1, step1590]: loss 2.493592
[epoch1, step1591]: loss 2.370735
[epoch1, step1592]: loss 2.474059
[epoch1, step1593]: loss 2.339254
[epoch1, step1594]: loss 2.537519
[epoch1, step1595]: loss 2.649512
[epoch1, step1596]: loss 2.637093
[epoch1, step1597]: loss 2.499749
[epoch1, step1598]: loss 2.634372
[epoch1, step1599]: loss 2.401861
[epoch1, step1600]: loss 2.475404
[epoch1, step1601]: loss 2.505749
[epoch1, step1602]: loss 2.537925
[epoch1, step1603]: loss 2.434370
[epoch1, step1604]: loss 2.381906
[epoch1, step1605]: loss 2.448555
[epoch1, step1606]: loss 2.523047
[epoch1, step1607]: loss 2.569405
[epoch1, step1608]: loss 2.498604
[epoch1, step1609]: loss 2.393855
[epoch1, step1610]: loss 2.357884
[epoch1, step1611]: loss 2.417521
[epoch1, step1612]: loss 2.243574
[epoch1, step1613]: loss 2.590837
[epoch1, step1614]: loss 2.260432
[epoch1, step1615]: loss 2.336274
[epoch1, step1616]: loss 2.592592
[epoch1, step1617]: loss 2.462147
[epoch1, step1618]: loss 2.407783
[epoch1, step1619]: loss 2.560326
[epoch1, step1620]: loss 2.495863
[epoch1, step1621]: loss 2.404301
[epoch1, step1622]: loss 2.390995
[epoch1, step1623]: loss 2.273330
[epoch1, step1624]: loss 2.529575
[epoch1, step1625]: loss 2.465752
[epoch1, step1626]: loss 2.627079
[epoch1, step1627]: loss 2.430720
[epoch1, step1628]: loss 2.599629
[epoch1, step1629]: loss 2.547777
[epoch1, step1630]: loss 2.551905
[epoch1, step1631]: loss 2.370170
[epoch1, step1632]: loss 2.395470
[epoch1, step1633]: loss 2.500996
[epoch1, step1634]: loss 2.366937
[epoch1, step1635]: loss 2.289991
[epoch1, step1636]: loss 2.468243
[epoch1, step1637]: loss 2.384483
[epoch1, step1638]: loss 2.438928
[epoch1, step1639]: loss 2.650129
[epoch1, step1640]: loss 2.286080
[epoch1, step1641]: loss 2.388461
[epoch1, step1642]: loss 2.276714
[epoch1, step1643]: loss 2.331233
[epoch1, step1644]: loss 2.411521
[epoch1, step1645]: loss 2.331713
[epoch1, step1646]: loss 2.399757
[epoch1, step1647]: loss 2.519042
[epoch1, step1648]: loss 2.242759
[epoch1, step1649]: loss 2.522927
[epoch1, step1650]: loss 2.397937
[epoch1, step1651]: loss 2.648427
[epoch1, step1652]: loss 2.227978
[epoch1, step1653]: loss 2.513490
[epoch1, step1654]: loss 2.369109
[epoch1, step1655]: loss 2.285033
[epoch1, step1656]: loss 2.267882
[epoch1, step1657]: loss 2.386835
[epoch1, step1658]: loss 2.376903
[epoch1, step1659]: loss 2.459118
[epoch1, step1660]: loss 2.515283
[epoch1, step1661]: loss 2.133530
[epoch1, step1662]: loss 2.376743
[epoch1, step1663]: loss 2.277246
[epoch1, step1664]: loss 2.410195
[epoch1, step1665]: loss 2.606437
[epoch1, step1666]: loss 2.268696
[epoch1, step1667]: loss 2.245129
[epoch1, step1668]: loss 2.252264
[epoch1, step1669]: loss 2.507398
[epoch1, step1670]: loss 2.462524
[epoch1, step1671]: loss 2.507862
[epoch1, step1672]: loss 2.223361
[epoch1, step1673]: loss 2.623043
[epoch1, step1674]: loss 2.578148
[epoch1, step1675]: loss 2.506298
[epoch1, step1676]: loss 2.480053
[epoch1, step1677]: loss 2.296756
[epoch1, step1678]: loss 2.131202
[epoch1, step1679]: loss 2.418166
[epoch1, step1680]: loss 2.252838
[epoch1, step1681]: loss 2.435080
[epoch1, step1682]: loss 2.309736
[epoch1, step1683]: loss 2.449038
[epoch1, step1684]: loss 2.350175
[epoch1, step1685]: loss 2.498135
[epoch1, step1686]: loss 2.305793
[epoch1, step1687]: loss 2.444963
[epoch1, step1688]: loss 2.476231
[epoch1, step1689]: loss 2.420814
[epoch1, step1690]: loss 2.334255
[epoch1, step1691]: loss 2.308798
[epoch1, step1692]: loss 2.345116
[epoch1, step1693]: loss 2.068328
[epoch1, step1694]: loss 2.223817
[epoch1, step1695]: loss 2.317253
[epoch1, step1696]: loss 2.152340
[epoch1, step1697]: loss 2.179326
[epoch1, step1698]: loss 2.326437
[epoch1, step1699]: loss 2.468188
[epoch1, step1700]: loss 2.086068
[epoch1, step1701]: loss 2.318968
[epoch1, step1702]: loss 2.364901
[epoch1, step1703]: loss 2.262571
[epoch1, step1704]: loss 2.453314
[epoch1, step1705]: loss 2.418823
[epoch1, step1706]: loss 2.360449
[epoch1, step1707]: loss 2.438298
[epoch1, step1708]: loss 2.225348
[epoch1, step1709]: loss 2.348246
[epoch1, step1710]: loss 2.515245
[epoch1, step1711]: loss 2.501609
[epoch1, step1712]: loss 2.465699
[epoch1, step1713]: loss 2.316568
[epoch1, step1714]: loss 2.209014
[epoch1, step1715]: loss 2.223387
[epoch1, step1716]: loss 2.277706
[epoch1, step1717]: loss 2.312173
[epoch1, step1718]: loss 2.420342
[epoch1, step1719]: loss 2.126363
[epoch1, step1720]: loss 2.325191
[epoch1, step1721]: loss 1.953739
[epoch1, step1722]: loss 2.484476
[epoch1, step1723]: loss 2.254722
[epoch1, step1724]: loss 2.027946
[epoch1, step1725]: loss 1.883400
[epoch1, step1726]: loss 2.406495
[epoch1, step1727]: loss 2.590237
[epoch1, step1728]: loss 2.085196
[epoch1, step1729]: loss 2.139512
[epoch1, step1730]: loss 2.280827
[epoch1, step1731]: loss 2.281720
[epoch1, step1732]: loss 2.343071
[epoch1, step1733]: loss 2.073466
[epoch1, step1734]: loss 2.429373
[epoch1, step1735]: loss 2.246744
[epoch1, step1736]: loss 2.313350
[epoch1, step1737]: loss 2.516333
[epoch1, step1738]: loss 2.379284
[epoch1, step1739]: loss 2.413637
[epoch1, step1740]: loss 2.257919
[epoch1, step1741]: loss 2.293095
[epoch1, step1742]: loss 2.405574
[epoch1, step1743]: loss 2.309127
[epoch1, step1744]: loss 2.408678
[epoch1, step1745]: loss 2.566908
[epoch1, step1746]: loss 2.285465
[epoch1, step1747]: loss 2.293837
[epoch1, step1748]: loss 2.336494
[epoch1, step1749]: loss 2.489693
[epoch1, step1750]: loss 2.332060
[epoch1, step1751]: loss 2.382834
[epoch1, step1752]: loss 2.367479
[epoch1, step1753]: loss 2.387892
[epoch1, step1754]: loss 2.339792
[epoch1, step1755]: loss 2.326650
[epoch1, step1756]: loss 2.525588
[epoch1, step1757]: loss 2.385667
[epoch1, step1758]: loss 2.113986
[epoch1, step1759]: loss 2.377396
[epoch1, step1760]: loss 2.126351
[epoch1, step1761]: loss 2.361596
[epoch1, step1762]: loss 2.499646
[epoch1, step1763]: loss 2.291598
[epoch1, step1764]: loss 2.307368
[epoch1, step1765]: loss 2.252161
[epoch1, step1766]: loss 2.493467
[epoch1, step1767]: loss 2.364748
[epoch1, step1768]: loss 2.261867
[epoch1, step1769]: loss 2.370669
[epoch1, step1770]: loss 2.285982
[epoch1, step1771]: loss 2.235685
[epoch1, step1772]: loss 2.223016
[epoch1, step1773]: loss 2.150377
[epoch1, step1774]: loss 2.421974
[epoch1, step1775]: loss 2.251988
[epoch1, step1776]: loss 2.279106
[epoch1, step1777]: loss 2.312961
[epoch1, step1778]: loss 2.266571
[epoch1, step1779]: loss 2.266055
[epoch1, step1780]: loss 2.260447
[epoch1, step1781]: loss 2.246695
[epoch1, step1782]: loss 2.307003
[epoch1, step1783]: loss 2.194752
[epoch1, step1784]: loss 2.312886
[epoch1, step1785]: loss 2.221355
[epoch1, step1786]: loss 2.124047
[epoch1, step1787]: loss 2.298954
[epoch1, step1788]: loss 2.373152
[epoch1, step1789]: loss 2.267130
[epoch1, step1790]: loss 2.541585
[epoch1, step1791]: loss 2.120610
[epoch1, step1792]: loss 2.410339
[epoch1, step1793]: loss 2.289553
[epoch1, step1794]: loss 2.329647
[epoch1, step1795]: loss 2.203075
[epoch1, step1796]: loss 2.143199
[epoch1, step1797]: loss 2.174850
[epoch1, step1798]: loss 2.443000
[epoch1, step1799]: loss 2.190361
[epoch1, step1800]: loss 2.447439
[epoch1, step1801]: loss 2.483696
[epoch1, step1802]: loss 2.439543
[epoch1, step1803]: loss 2.328429
[epoch1, step1804]: loss 2.205252
[epoch1, step1805]: loss 2.238892
[epoch1, step1806]: loss 2.284825
[epoch1, step1807]: loss 2.473861
[epoch1, step1808]: loss 2.175349
[epoch1, step1809]: loss 2.306169
[epoch1, step1810]: loss 2.113689
[epoch1, step1811]: loss 2.230749
[epoch1, step1812]: loss 2.167157
[epoch1, step1813]: loss 2.122951
[epoch1, step1814]: loss 2.319497
[epoch1, step1815]: loss 2.304825
[epoch1, step1816]: loss 2.453818
[epoch1, step1817]: loss 2.330510
[epoch1, step1818]: loss 2.201745
[epoch1, step1819]: loss 2.176781
[epoch1, step1820]: loss 2.410079
[epoch1, step1821]: loss 2.325033
[epoch1, step1822]: loss 2.383557
[epoch1, step1823]: loss 1.784735
[epoch1, step1824]: loss 2.395126
[epoch1, step1825]: loss 2.010703
[epoch1, step1826]: loss 2.261595
[epoch1, step1827]: loss 2.213078
[epoch1, step1828]: loss 2.047483
[epoch1, step1829]: loss 2.229721
[epoch1, step1830]: loss 2.195804
[epoch1, step1831]: loss 2.382619
[epoch1, step1832]: loss 2.137634
[epoch1, step1833]: loss 2.164155
[epoch1, step1834]: loss 2.429192
[epoch1, step1835]: loss 2.324997
[epoch1, step1836]: loss 2.222801
[epoch1, step1837]: loss 2.026688
[epoch1, step1838]: loss 2.285627
[epoch1, step1839]: loss 2.241304
[epoch1, step1840]: loss 2.259404
[epoch1, step1841]: loss 2.201530
[epoch1, step1842]: loss 2.325461
[epoch1, step1843]: loss 2.499465
[epoch1, step1844]: loss 1.828764
[epoch1, step1845]: loss 2.245063
[epoch1, step1846]: loss 2.042181
[epoch1, step1847]: loss 1.932081
[epoch1, step1848]: loss 2.096566
[epoch1, step1849]: loss 2.115275
[epoch1, step1850]: loss 2.190351
[epoch1, step1851]: loss 2.376609
[epoch1, step1852]: loss 2.103552
[epoch1, step1853]: loss 2.480427
[epoch1, step1854]: loss 2.279660
[epoch1, step1855]: loss 2.471827
[epoch1, step1856]: loss 2.096925
[epoch1, step1857]: loss 2.085775
[epoch1, step1858]: loss 2.321922
[epoch1, step1859]: loss 2.238847
[epoch1, step1860]: loss 2.434897
[epoch1, step1861]: loss 2.267632
[epoch1, step1862]: loss 2.478502
[epoch1, step1863]: loss 2.177661
[epoch1, step1864]: loss 2.057902
[epoch1, step1865]: loss 2.150732
[epoch1, step1866]: loss 2.089953
[epoch1, step1867]: loss 2.176644
[epoch1, step1868]: loss 2.189078
[epoch1, step1869]: loss 2.251304
[epoch1, step1870]: loss 1.981076
[epoch1, step1871]: loss 1.948140
[epoch1, step1872]: loss 2.320746
[epoch1, step1873]: loss 2.235075
[epoch1, step1874]: loss 2.254190
[epoch1, step1875]: loss 2.383195
[epoch1, step1876]: loss 2.156436
[epoch1, step1877]: loss 2.223554
[epoch1, step1878]: loss 2.055263
[epoch1, step1879]: loss 2.169179
[epoch1, step1880]: loss 2.212349
[epoch1, step1881]: loss 2.129752
[epoch1, step1882]: loss 2.219036
[epoch1, step1883]: loss 2.253960
[epoch1, step1884]: loss 2.206064
[epoch1, step1885]: loss 2.333477
[epoch1, step1886]: loss 2.272059
[epoch1, step1887]: loss 2.006793
[epoch1, step1888]: loss 2.004280
[epoch1, step1889]: loss 2.201602
[epoch1, step1890]: loss 2.229930
[epoch1, step1891]: loss 2.319974
[epoch1, step1892]: loss 2.262666
[epoch1, step1893]: loss 2.178981
[epoch1, step1894]: loss 2.188378
[epoch1, step1895]: loss 2.195030
[epoch1, step1896]: loss 2.289634
[epoch1, step1897]: loss 2.243711
[epoch1, step1898]: loss 2.440745
[epoch1, step1899]: loss 2.262214
[epoch1, step1900]: loss 2.370209
[epoch1, step1901]: loss 1.993165
[epoch1, step1902]: loss 2.044904
[epoch1, step1903]: loss 2.124502
[epoch1, step1904]: loss 1.829313
[epoch1, step1905]: loss 2.401723
[epoch1, step1906]: loss 2.157604
[epoch1, step1907]: loss 2.259418
[epoch1, step1908]: loss 2.108088
[epoch1, step1909]: loss 2.360814
[epoch1, step1910]: loss 2.210771
[epoch1, step1911]: loss 2.040502
[epoch1, step1912]: loss 1.817048
[epoch1, step1913]: loss 2.450735
[epoch1, step1914]: loss 2.161600
[epoch1, step1915]: loss 2.212460
[epoch1, step1916]: loss 2.312271
[epoch1, step1917]: loss 2.068395
[epoch1, step1918]: loss 2.350655
[epoch1, step1919]: loss 2.006417
[epoch1, step1920]: loss 2.086475
[epoch1, step1921]: loss 2.159278
[epoch1, step1922]: loss 1.962600
[epoch1, step1923]: loss 2.244730
[epoch1, step1924]: loss 2.402165
[epoch1, step1925]: loss 2.002363
[epoch1, step1926]: loss 2.018665
[epoch1, step1927]: loss 2.203575
[epoch1, step1928]: loss 2.314143
[epoch1, step1929]: loss 2.225062
[epoch1, step1930]: loss 1.934852
[epoch1, step1931]: loss 2.114931
[epoch1, step1932]: loss 2.126608
[epoch1, step1933]: loss 2.344478
[epoch1, step1934]: loss 2.357113
[epoch1, step1935]: loss 2.238609
[epoch1, step1936]: loss 2.022115
[epoch1, step1937]: loss 2.059225
[epoch1, step1938]: loss 2.214628
[epoch1, step1939]: loss 1.873996
[epoch1, step1940]: loss 2.389721
[epoch1, step1941]: loss 2.408659
[epoch1, step1942]: loss 2.107428
[epoch1, step1943]: loss 2.250341
[epoch1, step1944]: loss 2.004436
[epoch1, step1945]: loss 1.913926
[epoch1, step1946]: loss 2.243465
[epoch1, step1947]: loss 2.275403
[epoch1, step1948]: loss 2.232309
[epoch1, step1949]: loss 2.297270
[epoch1, step1950]: loss 1.950249
[epoch1, step1951]: loss 2.189439
[epoch1, step1952]: loss 2.423280
[epoch1, step1953]: loss 2.078215
[epoch1, step1954]: loss 2.397511
[epoch1, step1955]: loss 2.170598
[epoch1, step1956]: loss 2.033448
[epoch1, step1957]: loss 2.068675
[epoch1, step1958]: loss 2.338881
[epoch1, step1959]: loss 2.048520
[epoch1, step1960]: loss 1.903301
[epoch1, step1961]: loss 2.053025
[epoch1, step1962]: loss 2.249945
[epoch1, step1963]: loss 2.258354
[epoch1, step1964]: loss 2.023590
[epoch1, step1965]: loss 2.255874
[epoch1, step1966]: loss 1.937853
[epoch1, step1967]: loss 2.028265
[epoch1, step1968]: loss 2.295661
[epoch1, step1969]: loss 2.093693
[epoch1, step1970]: loss 2.244685
[epoch1, step1971]: loss 2.327380
[epoch1, step1972]: loss 1.959956
[epoch1, step1973]: loss 2.373584
[epoch1, step1974]: loss 2.248503
[epoch1, step1975]: loss 2.142326
[epoch1, step1976]: loss 2.106218
[epoch1, step1977]: loss 2.022541
[epoch1, step1978]: loss 1.935388
[epoch1, step1979]: loss 1.983683
[epoch1, step1980]: loss 2.127044
[epoch1, step1981]: loss 2.331147
[epoch1, step1982]: loss 2.027308
[epoch1, step1983]: loss 2.431321
[epoch1, step1984]: loss 2.359656
[epoch1, step1985]: loss 2.070442
[epoch1, step1986]: loss 2.240793
[epoch1, step1987]: loss 2.043853
[epoch1, step1988]: loss 2.318596
[epoch1, step1989]: loss 2.390516
[epoch1, step1990]: loss 2.196106
[epoch1, step1991]: loss 2.033560
[epoch1, step1992]: loss 2.196233
[epoch1, step1993]: loss 2.163488
[epoch1, step1994]: loss 2.001150
[epoch1, step1995]: loss 2.371047
[epoch1, step1996]: loss 2.124978
[epoch1, step1997]: loss 2.220312
[epoch1, step1998]: loss 2.225749
[epoch1, step1999]: loss 2.157023
[epoch1, step2000]: loss 2.262262
[epoch1, step2001]: loss 2.107600
[epoch1, step2002]: loss 2.116655
[epoch1, step2003]: loss 2.389689
[epoch1, step2004]: loss 2.086875
[epoch1, step2005]: loss 2.353603
[epoch1, step2006]: loss 2.124895
[epoch1, step2007]: loss 2.185307
[epoch1, step2008]: loss 2.269737
[epoch1, step2009]: loss 2.049125
[epoch1, step2010]: loss 2.256087
[epoch1, step2011]: loss 2.358692
[epoch1, step2012]: loss 2.080973
[epoch1, step2013]: loss 2.133548
[epoch1, step2014]: loss 1.893370
[epoch1, step2015]: loss 1.911098
[epoch1, step2016]: loss 2.305586
[epoch1, step2017]: loss 2.274614
[epoch1, step2018]: loss 2.326643
[epoch1, step2019]: loss 2.307344
[epoch1, step2020]: loss 2.262130
[epoch1, step2021]: loss 2.391555
[epoch1, step2022]: loss 2.194342
[epoch1, step2023]: loss 2.219581
[epoch1, step2024]: loss 2.170984
[epoch1, step2025]: loss 2.030383
[epoch1, step2026]: loss 2.385552
[epoch1, step2027]: loss 2.026116
[epoch1, step2028]: loss 1.842071
[epoch1, step2029]: loss 2.302509
[epoch1, step2030]: loss 2.026315
[epoch1, step2031]: loss 2.116912
[epoch1, step2032]: loss 2.000993
[epoch1, step2033]: loss 2.114875
[epoch1, step2034]: loss 2.065884
[epoch1, step2035]: loss 2.422902
[epoch1, step2036]: loss 2.189684
[epoch1, step2037]: loss 2.192977
[epoch1, step2038]: loss 1.959248
[epoch1, step2039]: loss 2.007276
[epoch1, step2040]: loss 2.088624
[epoch1, step2041]: loss 2.358462
[epoch1, step2042]: loss 2.240086
[epoch1, step2043]: loss 2.321696
[epoch1, step2044]: loss 1.886272
[epoch1, step2045]: loss 2.235878
[epoch1, step2046]: loss 1.871155
[epoch1, step2047]: loss 2.083041
[epoch1, step2048]: loss 1.995436
[epoch1, step2049]: loss 2.018565
[epoch1, step2050]: loss 2.022027
[epoch1, step2051]: loss 2.144036
[epoch1, step2052]: loss 2.206429
[epoch1, step2053]: loss 2.084983
[epoch1, step2054]: loss 1.507048
[epoch1, step2055]: loss 1.915251
[epoch1, step2056]: loss 2.171293
[epoch1, step2057]: loss 2.003638
[epoch1, step2058]: loss 2.336978
[epoch1, step2059]: loss 2.375905
[epoch1, step2060]: loss 2.299111
[epoch1, step2061]: loss 2.263966
[epoch1, step2062]: loss 2.030706
[epoch1, step2063]: loss 2.044760
[epoch1, step2064]: loss 2.005349
[epoch1, step2065]: loss 2.110960
[epoch1, step2066]: loss 1.931218
[epoch1, step2067]: loss 1.951849
[epoch1, step2068]: loss 2.153062
[epoch1, step2069]: loss 2.060185
[epoch1, step2070]: loss 2.091085
[epoch1, step2071]: loss 2.295380
[epoch1, step2072]: loss 1.873273
[epoch1, step2073]: loss 2.078161
[epoch1, step2074]: loss 1.790485
[epoch1, step2075]: loss 2.048779
[epoch1, step2076]: loss 2.088412
[epoch1, step2077]: loss 1.996247
[epoch1, step2078]: loss 2.244219
[epoch1, step2079]: loss 1.919590
[epoch1, step2080]: loss 1.971403
[epoch1, step2081]: loss 1.907980
[epoch1, step2082]: loss 1.968742
[epoch1, step2083]: loss 2.168271
[epoch1, step2084]: loss 2.364139
[epoch1, step2085]: loss 1.729029
[epoch1, step2086]: loss 2.044288
[epoch1, step2087]: loss 2.279818
[epoch1, step2088]: loss 1.915483
[epoch1, step2089]: loss 1.986708
[epoch1, step2090]: loss 2.033473
[epoch1, step2091]: loss 2.235791
[epoch1, step2092]: loss 2.034580
[epoch1, step2093]: loss 2.023518
[epoch1, step2094]: loss 2.120861
[epoch1, step2095]: loss 2.139554
[epoch1, step2096]: loss 1.956898
[epoch1, step2097]: loss 2.107609
[epoch1, step2098]: loss 2.176128
[epoch1, step2099]: loss 2.303639
[epoch1, step2100]: loss 2.251152
[epoch1, step2101]: loss 2.002684
[epoch1, step2102]: loss 2.208045
[epoch1, step2103]: loss 2.001994
[epoch1, step2104]: loss 2.371279
[epoch1, step2105]: loss 2.062926
[epoch1, step2106]: loss 2.276691
[epoch1, step2107]: loss 2.026196
[epoch1, step2108]: loss 2.026822
[epoch1, step2109]: loss 2.144504
[epoch1, step2110]: loss 2.044658
[epoch1, step2111]: loss 2.105704
[epoch1, step2112]: loss 2.170082
[epoch1, step2113]: loss 2.013634
[epoch1, step2114]: loss 1.928721
[epoch1, step2115]: loss 2.227287
[epoch1, step2116]: loss 1.950991
[epoch1, step2117]: loss 2.170933
[epoch1, step2118]: loss 2.112922
[epoch1, step2119]: loss 2.061839
[epoch1, step2120]: loss 2.099314
[epoch1, step2121]: loss 1.733648
[epoch1, step2122]: loss 2.276795
[epoch1, step2123]: loss 2.281244
[epoch1, step2124]: loss 1.838082
[epoch1, step2125]: loss 1.709790
[epoch1, step2126]: loss 2.192776
[epoch1, step2127]: loss 2.169471
[epoch1, step2128]: loss 2.081909
[epoch1, step2129]: loss 2.090689
[epoch1, step2130]: loss 2.019745
[epoch1, step2131]: loss 2.098218
[epoch1, step2132]: loss 2.073528
[epoch1, step2133]: loss 2.118667
[epoch1, step2134]: loss 2.016787
[epoch1, step2135]: loss 2.175875
[epoch1, step2136]: loss 2.411446
[epoch1, step2137]: loss 2.224263
[epoch1, step2138]: loss 1.903535
[epoch1, step2139]: loss 2.099890
[epoch1, step2140]: loss 1.986822
[epoch1, step2141]: loss 2.247015
[epoch1, step2142]: loss 2.075319
[epoch1, step2143]: loss 1.907423
[epoch1, step2144]: loss 2.206665
[epoch1, step2145]: loss 2.046002
[epoch1, step2146]: loss 2.138368
[epoch1, step2147]: loss 1.632556
[epoch1, step2148]: loss 2.251124
[epoch1, step2149]: loss 1.843133
[epoch1, step2150]: loss 1.979770
[epoch1, step2151]: loss 2.011133
[epoch1, step2152]: loss 1.985767
[epoch1, step2153]: loss 1.715738
[epoch1, step2154]: loss 1.909369
[epoch1, step2155]: loss 1.924768
[epoch1, step2156]: loss 1.836628
[epoch1, step2157]: loss 2.274710
[epoch1, step2158]: loss 1.735177
[epoch1, step2159]: loss 1.913052
[epoch1, step2160]: loss 2.033694
[epoch1, step2161]: loss 1.910580
[epoch1, step2162]: loss 2.016364
[epoch1, step2163]: loss 1.831282
[epoch1, step2164]: loss 2.120480
[epoch1, step2165]: loss 2.208171
[epoch1, step2166]: loss 2.190969
[epoch1, step2167]: loss 2.205170
[epoch1, step2168]: loss 1.936602
[epoch1, step2169]: loss 1.626743
[epoch1, step2170]: loss 1.810091
[epoch1, step2171]: loss 2.037274
[epoch1, step2172]: loss 2.195424
[epoch1, step2173]: loss 2.203976
[epoch1, step2174]: loss 2.155653
[epoch1, step2175]: loss 2.163287
[epoch1, step2176]: loss 2.180755
[epoch1, step2177]: loss 1.933087
[epoch1, step2178]: loss 2.047418
[epoch1, step2179]: loss 2.081577
[epoch1, step2180]: loss 1.842800
[epoch1, step2181]: loss 2.105050
[epoch1, step2182]: loss 2.051629
[epoch1, step2183]: loss 2.051090
[epoch1, step2184]: loss 2.021162
[epoch1, step2185]: loss 1.972344
[epoch1, step2186]: loss 2.105191
[epoch1, step2187]: loss 2.161079
[epoch1, step2188]: loss 1.916258
[epoch1, step2189]: loss 1.884174
[epoch1, step2190]: loss 2.190372
[epoch1, step2191]: loss 1.847672
[epoch1, step2192]: loss 2.334965
[epoch1, step2193]: loss 2.073747
[epoch1, step2194]: loss 2.058183
[epoch1, step2195]: loss 2.088490
[epoch1, step2196]: loss 2.109137
[epoch1, step2197]: loss 2.101575
[epoch1, step2198]: loss 2.104010
[epoch1, step2199]: loss 2.198507
[epoch1, step2200]: loss 1.884664
[epoch1, step2201]: loss 1.685012
[epoch1, step2202]: loss 2.294621
[epoch1, step2203]: loss 1.978616
[epoch1, step2204]: loss 1.835228
[epoch1, step2205]: loss 1.972889
[epoch1, step2206]: loss 2.112483
[epoch1, step2207]: loss 1.730996
[epoch1, step2208]: loss 2.237287
[epoch1, step2209]: loss 2.175458
[epoch1, step2210]: loss 1.822509
[epoch1, step2211]: loss 1.958099
[epoch1, step2212]: loss 1.950547
[epoch1, step2213]: loss 1.841014
[epoch1, step2214]: loss 1.880851
[epoch1, step2215]: loss 2.027714
[epoch1, step2216]: loss 1.684838
[epoch1, step2217]: loss 1.905046
[epoch1, step2218]: loss 1.901067
[epoch1, step2219]: loss 2.015195
[epoch1, step2220]: loss 2.069400
[epoch1, step2221]: loss 1.955181
[epoch1, step2222]: loss 2.050412
[epoch1, step2223]: loss 2.170449
[epoch1, step2224]: loss 2.329660
[epoch1, step2225]: loss 1.810355
[epoch1, step2226]: loss 2.204671
[epoch1, step2227]: loss 2.199026
[epoch1, step2228]: loss 1.914560
[epoch1, step2229]: loss 2.244415
[epoch1, step2230]: loss 1.987401
[epoch1, step2231]: loss 1.732944
[epoch1, step2232]: loss 2.142551
[epoch1, step2233]: loss 1.897597
[epoch1, step2234]: loss 1.968315
[epoch1, step2235]: loss 2.089190
[epoch1, step2236]: loss 1.885652
[epoch1, step2237]: loss 1.976543
[epoch1, step2238]: loss 2.094351
[epoch1, step2239]: loss 1.944943
[epoch1, step2240]: loss 1.896229
[epoch1, step2241]: loss 1.815335
[epoch1, step2242]: loss 1.987316
[epoch1, step2243]: loss 2.011739
[epoch1, step2244]: loss 1.774446
[epoch1, step2245]: loss 2.012756
[epoch1, step2246]: loss 2.003443
[epoch1, step2247]: loss 1.988869
[epoch1, step2248]: loss 1.702580
[epoch1, step2249]: loss 1.774623
[epoch1, step2250]: loss 1.670606
[epoch1, step2251]: loss 1.997339
[epoch1, step2252]: loss 1.895349
[epoch1, step2253]: loss 2.116717
[epoch1, step2254]: loss 1.914701
[epoch1, step2255]: loss 2.184057
[epoch1, step2256]: loss 1.789622
[epoch1, step2257]: loss 2.048320
[epoch1, step2258]: loss 2.183913
[epoch1, step2259]: loss 1.981560
[epoch1, step2260]: loss 1.991302
[epoch1, step2261]: loss 1.790065
[epoch1, step2262]: loss 2.141203
[epoch1, step2263]: loss 1.995383
[epoch1, step2264]: loss 1.812825
[epoch1, step2265]: loss 2.073285
[epoch1, step2266]: loss 2.097380
[epoch1, step2267]: loss 1.978931
[epoch1, step2268]: loss 2.125098
[epoch1, step2269]: loss 2.108994
[epoch1, step2270]: loss 2.158291
[epoch1, step2271]: loss 2.019617
[epoch1, step2272]: loss 1.584983
[epoch1, step2273]: loss 1.959761
[epoch1, step2274]: loss 2.199298
[epoch1, step2275]: loss 1.971940
[epoch1, step2276]: loss 2.059656
[epoch1, step2277]: loss 2.042016
[epoch1, step2278]: loss 2.066459
[epoch1, step2279]: loss 1.945855
[epoch1, step2280]: loss 1.533874
[epoch1, step2281]: loss 2.070682
[epoch1, step2282]: loss 2.054675
[epoch1, step2283]: loss 1.948153
[epoch1, step2284]: loss 2.014748
[epoch1, step2285]: loss 1.647145
[epoch1, step2286]: loss 1.941144
[epoch1, step2287]: loss 1.944130
[epoch1, step2288]: loss 2.033733
[epoch1, step2289]: loss 2.245384
[epoch1, step2290]: loss 2.103064
[epoch1, step2291]: loss 1.929830
[epoch1, step2292]: loss 2.276141
[epoch1, step2293]: loss 1.761000
[epoch1, step2294]: loss 1.569307
[epoch1, step2295]: loss 1.748740
[epoch1, step2296]: loss 1.991313
[epoch1, step2297]: loss 2.072061
[epoch1, step2298]: loss 2.059307
[epoch1, step2299]: loss 2.067451
[epoch1, step2300]: loss 2.002154
[epoch1, step2301]: loss 1.966821
[epoch1, step2302]: loss 2.029047
[epoch1, step2303]: loss 1.702296
[epoch1, step2304]: loss 1.941645
[epoch1, step2305]: loss 1.823883
[epoch1, step2306]: loss 1.879271
[epoch1, step2307]: loss 1.941484
[epoch1, step2308]: loss 1.742419
[epoch1, step2309]: loss 1.871785
[epoch1, step2310]: loss 2.008584
[epoch1, step2311]: loss 1.995120
[epoch1, step2312]: loss 2.203020
[epoch1, step2313]: loss 1.776821
[epoch1, step2314]: loss 1.808066
[epoch1, step2315]: loss 1.909537
[epoch1, step2316]: loss 1.446116
[epoch1, step2317]: loss 2.041637
[epoch1, step2318]: loss 1.809368
[epoch1, step2319]: loss 2.072671
[epoch1, step2320]: loss 1.944237
[epoch1, step2321]: loss 1.688912
[epoch1, step2322]: loss 2.177606
[epoch1, step2323]: loss 1.920458
[epoch1, step2324]: loss 2.048251
[epoch1, step2325]: loss 1.921285
[epoch1, step2326]: loss 1.747117
[epoch1, step2327]: loss 1.925113
[epoch1, step2328]: loss 2.130519
[epoch1, step2329]: loss 1.918389
[epoch1, step2330]: loss 2.155945
[epoch1, step2331]: loss 1.806819
[epoch1, step2332]: loss 2.044023
[epoch1, step2333]: loss 2.080809
[epoch1, step2334]: loss 1.723697
[epoch1, step2335]: loss 1.811742
[epoch1, step2336]: loss 1.829249
[epoch1, step2337]: loss 2.078236
[epoch1, step2338]: loss 1.846274
[epoch1, step2339]: loss 2.075631
[epoch1, step2340]: loss 2.153606
[epoch1, step2341]: loss 1.892596
[epoch1, step2342]: loss 1.915416
[epoch1, step2343]: loss 1.766173
[epoch1, step2344]: loss 1.764568
[epoch1, step2345]: loss 1.816182
[epoch1, step2346]: loss 2.054070
[epoch1, step2347]: loss 1.779980
[epoch1, step2348]: loss 2.170079
[epoch1, step2349]: loss 2.036289
[epoch1, step2350]: loss 2.082116
[epoch1, step2351]: loss 2.127382
[epoch1, step2352]: loss 1.682628
[epoch1, step2353]: loss 1.935682
[epoch1, step2354]: loss 2.188072
[epoch1, step2355]: loss 2.104933
[epoch1, step2356]: loss 1.719795
[epoch1, step2357]: loss 1.792968
[epoch1, step2358]: loss 1.984427
[epoch1, step2359]: loss 2.079849
[epoch1, step2360]: loss 2.089423
[epoch1, step2361]: loss 1.988457
[epoch1, step2362]: loss 1.657565
[epoch1, step2363]: loss 1.858012
[epoch1, step2364]: loss 1.638021
[epoch1, step2365]: loss 2.149626
[epoch1, step2366]: loss 1.706869
[epoch1, step2367]: loss 2.000905
[epoch1, step2368]: loss 2.208478
[epoch1, step2369]: loss 1.870421
[epoch1, step2370]: loss 1.903042
[epoch1, step2371]: loss 2.042372
[epoch1, step2372]: loss 2.067032
[epoch1, step2373]: loss 1.842560
[epoch1, step2374]: loss 1.758885
[epoch1, step2375]: loss 1.910588
[epoch1, step2376]: loss 2.118947
[epoch1, step2377]: loss 1.988743
[epoch1, step2378]: loss 1.817883
[epoch1, step2379]: loss 1.599936
[epoch1, step2380]: loss 1.914594
[epoch1, step2381]: loss 1.999372
[epoch1, step2382]: loss 1.871339
[epoch1, step2383]: loss 1.990094
[epoch1, step2384]: loss 1.893669
[epoch1, step2385]: loss 2.098141
[epoch1, step2386]: loss 2.155231
[epoch1, step2387]: loss 1.969039
[epoch1, step2388]: loss 1.758794
[epoch1, step2389]: loss 1.899351
[epoch1, step2390]: loss 1.683291
[epoch1, step2391]: loss 1.971792
[epoch1, step2392]: loss 2.094671
[epoch1, step2393]: loss 1.460777
[epoch1, step2394]: loss 1.936908
[epoch1, step2395]: loss 2.063411
[epoch1, step2396]: loss 1.911785
[epoch1, step2397]: loss 1.900527
[epoch1, step2398]: loss 1.866028
[epoch1, step2399]: loss 1.976248
[epoch1, step2400]: loss 1.671765
[epoch1, step2401]: loss 1.745336
[epoch1, step2402]: loss 2.005324
[epoch1, step2403]: loss 1.689999
[epoch1, step2404]: loss 1.876770
[epoch1, step2405]: loss 1.964448
[epoch1, step2406]: loss 1.989420
[epoch1, step2407]: loss 1.711317
[epoch1, step2408]: loss 1.682929
[epoch1, step2409]: loss 1.717999
[epoch1, step2410]: loss 1.838377
[epoch1, step2411]: loss 2.018312
[epoch1, step2412]: loss 1.989544
[epoch1, step2413]: loss 1.968771
[epoch1, step2414]: loss 1.965139
[epoch1, step2415]: loss 2.002178
[epoch1, step2416]: loss 1.983306
[epoch1, step2417]: loss 1.921884
[epoch1, step2418]: loss 2.071937
[epoch1, step2419]: loss 1.519771
[epoch1, step2420]: loss 1.765050
[epoch1, step2421]: loss 2.117353
[epoch1, step2422]: loss 1.987261
[epoch1, step2423]: loss 2.028895
[epoch1, step2424]: loss 2.180665
[epoch1, step2425]: loss 1.769773
[epoch1, step2426]: loss 2.193506
[epoch1, step2427]: loss 1.894496
[epoch1, step2428]: loss 1.942783
[epoch1, step2429]: loss 1.611187
[epoch1, step2430]: loss 1.685802
[epoch1, step2431]: loss 2.127907
[epoch1, step2432]: loss 2.145011
[epoch1, step2433]: loss 1.989514
[epoch1, step2434]: loss 1.526148
[epoch1, step2435]: loss 1.518076
[epoch1, step2436]: loss 1.809154
[epoch1, step2437]: loss 1.705215
[epoch1, step2438]: loss 1.886124
[epoch1, step2439]: loss 1.746567
[epoch1, step2440]: loss 1.812843
[epoch1, step2441]: loss 1.741790
[epoch1, step2442]: loss 2.046676
[epoch1, step2443]: loss 1.902375
[epoch1, step2444]: loss 2.105442
[epoch1, step2445]: loss 1.979562
[epoch1, step2446]: loss 2.065665
[epoch1, step2447]: loss 1.818689
[epoch1, step2448]: loss 1.855950
[epoch1, step2449]: loss 2.120833
[epoch1, step2450]: loss 1.879331
[epoch1, step2451]: loss 2.018618
[epoch1, step2452]: loss 1.386706
[epoch1, step2453]: loss 1.982338
[epoch1, step2454]: loss 2.003271
[epoch1, step2455]: loss 1.628007
[epoch1, step2456]: loss 1.751753
[epoch1, step2457]: loss 1.581972
[epoch1, step2458]: loss 2.072544
[epoch1, step2459]: loss 1.986561
[epoch1, step2460]: loss 1.942731
[epoch1, step2461]: loss 1.825075
[epoch1, step2462]: loss 1.911964
[epoch1, step2463]: loss 1.948942
[epoch1, step2464]: loss 1.929058
[epoch1, step2465]: loss 2.142139
[epoch1, step2466]: loss 1.888400
[epoch1, step2467]: loss 1.880885
[epoch1, step2468]: loss 1.950282
[epoch1, step2469]: loss 2.094903
[epoch1, step2470]: loss 2.207772
[epoch1, step2471]: loss 2.110897
[epoch1, step2472]: loss 1.905159
[epoch1, step2473]: loss 1.998429
[epoch1, step2474]: loss 1.793248
[epoch1, step2475]: loss 1.865137
[epoch1, step2476]: loss 1.532221
[epoch1, step2477]: loss 1.917917
[epoch1, step2478]: loss 1.798548
[epoch1, step2479]: loss 1.610941
[epoch1, step2480]: loss 1.869363
[epoch1, step2481]: loss 1.844551
[epoch1, step2482]: loss 1.958812
[epoch1, step2483]: loss 2.043462
[epoch1, step2484]: loss 1.938681
[epoch1, step2485]: loss 1.782455
[epoch1, step2486]: loss 1.818262
[epoch1, step2487]: loss 2.069503
[epoch1, step2488]: loss 1.989157
[epoch1, step2489]: loss 1.637391
[epoch1, step2490]: loss 1.789385
[epoch1, step2491]: loss 1.724537
[epoch1, step2492]: loss 2.129007
[epoch1, step2493]: loss 2.089871
[epoch1, step2494]: loss 1.952708
[epoch1, step2495]: loss 1.983576
[epoch1, step2496]: loss 1.764108
[epoch1, step2497]: loss 1.879279
[epoch1, step2498]: loss 2.008268
[epoch1, step2499]: loss 2.093517
[epoch1, step2500]: loss 2.195417
[epoch1, step2501]: loss 1.787770
[epoch1, step2502]: loss 1.844302
[epoch1, step2503]: loss 1.412474
[epoch1, step2504]: loss 1.778571
[epoch1, step2505]: loss 1.892095
[epoch1, step2506]: loss 1.649124
[epoch1, step2507]: loss 1.891460
[epoch1, step2508]: loss 2.004930
[epoch1, step2509]: loss 1.955235
[epoch1, step2510]: loss 1.757674
[epoch1, step2511]: loss 2.106872
[epoch1, step2512]: loss 1.357976
[epoch1, step2513]: loss 2.087780
[epoch1, step2514]: loss 2.014689
[epoch1, step2515]: loss 1.940435
[epoch1, step2516]: loss 1.811703
[epoch1, step2517]: loss 1.901179
[epoch1, step2518]: loss 1.667836
[epoch1, step2519]: loss 1.854343
[epoch1, step2520]: loss 1.805452
[epoch1, step2521]: loss 2.065104
[epoch1, step2522]: loss 1.851516
[epoch1, step2523]: loss 1.580183
[epoch1, step2524]: loss 2.022042
[epoch1, step2525]: loss 1.867977
[epoch1, step2526]: loss 1.977512
[epoch1, step2527]: loss 2.023363
[epoch1, step2528]: loss 1.926241
[epoch1, step2529]: loss 1.611297
[epoch1, step2530]: loss 1.863190
[epoch1, step2531]: loss 1.510340
[epoch1, step2532]: loss 1.645540
[epoch1, step2533]: loss 1.489819
[epoch1, step2534]: loss 1.616762
[epoch1, step2535]: loss 2.172397
[epoch1, step2536]: loss 2.014268
[epoch1, step2537]: loss 1.805953
[epoch1, step2538]: loss 1.638470
[epoch1, step2539]: loss 1.861788
[epoch1, step2540]: loss 1.539041
[epoch1, step2541]: loss 1.895015
[epoch1, step2542]: loss 1.801621
[epoch1, step2543]: loss 1.661397
[epoch1, step2544]: loss 1.776567
[epoch1, step2545]: loss 1.747304
[epoch1, step2546]: loss 1.568915
[epoch1, step2547]: loss 2.008626
[epoch1, step2548]: loss 1.504229
[epoch1, step2549]: loss 1.739180
[epoch1, step2550]: loss 2.057195
[epoch1, step2551]: loss 1.883300
[epoch1, step2552]: loss 1.751175
[epoch1, step2553]: loss 1.410562
[epoch1, step2554]: loss 1.662895
[epoch1, step2555]: loss 1.747090
[epoch1, step2556]: loss 1.977862
[epoch1, step2557]: loss 1.900840
[epoch1, step2558]: loss 2.112685
[epoch1, step2559]: loss 1.801660
[epoch1, step2560]: loss 1.924569
[epoch1, step2561]: loss 1.740728
[epoch1, step2562]: loss 1.564458
[epoch1, step2563]: loss 1.996472
[epoch1, step2564]: loss 1.733012
[epoch1, step2565]: loss 1.584090
[epoch1, step2566]: loss 1.790172
[epoch1, step2567]: loss 2.047919
[epoch1, step2568]: loss 2.040589
[epoch1, step2569]: loss 1.770202
[epoch1, step2570]: loss 1.698461
[epoch1, step2571]: loss 2.108963
[epoch1, step2572]: loss 1.962083
[epoch1, step2573]: loss 1.793914
[epoch1, step2574]: loss 1.872891
[epoch1, step2575]: loss 1.765322
[epoch1, step2576]: loss 1.832522
[epoch1, step2577]: loss 1.821614
[epoch1, step2578]: loss 1.893654
[epoch1, step2579]: loss 1.945498
[epoch1, step2580]: loss 2.221320
[epoch1, step2581]: loss 2.064108
[epoch1, step2582]: loss 1.814926
[epoch1, step2583]: loss 2.017673
[epoch1, step2584]: loss 1.505876
[epoch1, step2585]: loss 1.864423
[epoch1, step2586]: loss 1.577087
[epoch1, step2587]: loss 1.845335
[epoch1, step2588]: loss 1.910131
[epoch1, step2589]: loss 2.081927
[epoch1, step2590]: loss 1.993508
[epoch1, step2591]: loss 1.880870
[epoch1, step2592]: loss 1.903167
[epoch1, step2593]: loss 1.773909
[epoch1, step2594]: loss 1.592494
[epoch1, step2595]: loss 1.866119
[epoch1, step2596]: loss 1.842367
[epoch1, step2597]: loss 1.920640
[epoch1, step2598]: loss 1.642585
[epoch1, step2599]: loss 1.778157
[epoch1, step2600]: loss 1.896409
[epoch1, step2601]: loss 1.787630
[epoch1, step2602]: loss 2.056540
[epoch1, step2603]: loss 1.928836
[epoch1, step2604]: loss 1.496705
[epoch1, step2605]: loss 1.858148
[epoch1, step2606]: loss 1.709942
[epoch1, step2607]: loss 1.380186
[epoch1, step2608]: loss 1.492825
[epoch1, step2609]: loss 1.651616
[epoch1, step2610]: loss 1.829327
[epoch1, step2611]: loss 1.477891
[epoch1, step2612]: loss 2.035606
[epoch1, step2613]: loss 1.635000
[epoch1, step2614]: loss 1.903345
[epoch1, step2615]: loss 1.801068
[epoch1, step2616]: loss 1.929231
[epoch1, step2617]: loss 2.030071
[epoch1, step2618]: loss 2.047751
[epoch1, step2619]: loss 1.794661
[epoch1, step2620]: loss 1.903300
[epoch1, step2621]: loss 1.977375
[epoch1, step2622]: loss 1.959632
[epoch1, step2623]: loss 1.740832
[epoch1, step2624]: loss 1.832644
[epoch1, step2625]: loss 1.910896
[epoch1, step2626]: loss 1.972932
[epoch1, step2627]: loss 1.902955
[epoch1, step2628]: loss 1.544206
[epoch1, step2629]: loss 1.809571
[epoch1, step2630]: loss 1.775198
[epoch1, step2631]: loss 1.913128
[epoch1, step2632]: loss 1.850770
[epoch1, step2633]: loss 1.972206
[epoch1, step2634]: loss 1.784952
[epoch1, step2635]: loss 1.849160
[epoch1, step2636]: loss 1.834380
[epoch1, step2637]: loss 1.846109
[epoch1, step2638]: loss 1.823451
[epoch1, step2639]: loss 1.766090
[epoch1, step2640]: loss 1.530898
[epoch1, step2641]: loss 1.741541
[epoch1, step2642]: loss 1.959099
[epoch1, step2643]: loss 1.751730
[epoch1, step2644]: loss 2.006119
[epoch1, step2645]: loss 1.570461
[epoch1, step2646]: loss 1.597199
[epoch1, step2647]: loss 1.848989
[epoch1, step2648]: loss 1.366084
[epoch1, step2649]: loss 1.685963
[epoch1, step2650]: loss 1.560217
[epoch1, step2651]: loss 1.410273
[epoch1, step2652]: loss 1.698759
[epoch1, step2653]: loss 1.873410
[epoch1, step2654]: loss 1.896314
[epoch1, step2655]: loss 1.693888
[epoch1, step2656]: loss 1.755452
[epoch1, step2657]: loss 1.587953
[epoch1, step2658]: loss 1.745063
[epoch1, step2659]: loss 1.662071
[epoch1, step2660]: loss 2.111099
[epoch1, step2661]: loss 2.019777
[epoch1, step2662]: loss 1.563783
[epoch1, step2663]: loss 1.473539
[epoch1, step2664]: loss 2.095382
[epoch1, step2665]: loss 1.865652
[epoch1, step2666]: loss 1.747319
[epoch1, step2667]: loss 1.579332
[epoch1, step2668]: loss 1.543870
[epoch1, step2669]: loss 1.899844
[epoch1, step2670]: loss 1.625863
[epoch1, step2671]: loss 1.641098
[epoch1, step2672]: loss 1.840204
[epoch1, step2673]: loss 1.700147
[epoch1, step2674]: loss 1.686972
[epoch1, step2675]: loss 1.876707
[epoch1, step2676]: loss 1.711326
[epoch1, step2677]: loss 1.900998
[epoch1, step2678]: loss 1.746215
[epoch1, step2679]: loss 1.733638
[epoch1, step2680]: loss 1.982016
[epoch1, step2681]: loss 1.633868
[epoch1, step2682]: loss 1.792362
[epoch1, step2683]: loss 1.374634
[epoch1, step2684]: loss 1.454241
[epoch1, step2685]: loss 1.993860
[epoch1, step2686]: loss 1.547613
[epoch1, step2687]: loss 1.846212
[epoch1, step2688]: loss 1.660864
[epoch1, step2689]: loss 1.654958
[epoch1, step2690]: loss 1.716555
[epoch1, step2691]: loss 1.330570
[epoch1, step2692]: loss 1.673913
[epoch1, step2693]: loss 1.632134
[epoch1, step2694]: loss 1.534641
[epoch1, step2695]: loss 1.906651
[epoch1, step2696]: loss 1.732507
[epoch1, step2697]: loss 1.646548
[epoch1, step2698]: loss 1.520570
[epoch1, step2699]: loss 2.106110
[epoch1, step2700]: loss 1.822340
[epoch1, step2701]: loss 1.988234
[epoch1, step2702]: loss 2.118905
[epoch1, step2703]: loss 1.649022
[epoch1, step2704]: loss 1.742501
[epoch1, step2705]: loss 1.896967
[epoch1, step2706]: loss 1.723236
[epoch1, step2707]: loss 1.950802
[epoch1, step2708]: loss 1.846224
[epoch1, step2709]: loss 1.911826
[epoch1, step2710]: loss 1.683405
[epoch1, step2711]: loss 1.766171
[epoch1, step2712]: loss 1.852757
[epoch1, step2713]: loss 1.449883
[epoch1, step2714]: loss 1.699804
[epoch1, step2715]: loss 1.938480
[epoch1, step2716]: loss 1.832900
[epoch1, step2717]: loss 1.952091
[epoch1, step2718]: loss 1.776669
[epoch1, step2719]: loss 2.082148
[epoch1, step2720]: loss 1.578649
[epoch1, step2721]: loss 1.896775
[epoch1, step2722]: loss 1.227193
[epoch1, step2723]: loss 1.792352
[epoch1, step2724]: loss 1.513548
[epoch1, step2725]: loss 1.825504
[epoch1, step2726]: loss 1.861290
[epoch1, step2727]: loss 1.433064
[epoch1, step2728]: loss 1.924416
[epoch1, step2729]: loss 1.710702
[epoch1, step2730]: loss 1.991564
[epoch1, step2731]: loss 2.055707
[epoch1, step2732]: loss 1.483532
[epoch1, step2733]: loss 1.571983
[epoch1, step2734]: loss 1.650483
[epoch1, step2735]: loss 1.732623
[epoch1, step2736]: loss 1.948584
[epoch1, step2737]: loss 1.490145
[epoch1, step2738]: loss 1.709512
[epoch1, step2739]: loss 1.877897
[epoch1, step2740]: loss 1.614049
[epoch1, step2741]: loss 1.845509
[epoch1, step2742]: loss 1.979681
[epoch1, step2743]: loss 1.613718
[epoch1, step2744]: loss 1.513967
[epoch1, step2745]: loss 1.717207
[epoch1, step2746]: loss 1.726325
[epoch1, step2747]: loss 1.875894
[epoch1, step2748]: loss 1.581120
[epoch1, step2749]: loss 1.703091
[epoch1, step2750]: loss 1.810285
[epoch1, step2751]: loss 1.751230
[epoch1, step2752]: loss 1.528977
[epoch1, step2753]: loss 2.043117
[epoch1, step2754]: loss 2.046079
[epoch1, step2755]: loss 1.692060
[epoch1, step2756]: loss 1.892022
[epoch1, step2757]: loss 1.892125
[epoch1, step2758]: loss 1.862831
[epoch1, step2759]: loss 1.823321
[epoch1, step2760]: loss 1.975325
[epoch1, step2761]: loss 1.617927
[epoch1, step2762]: loss 1.546737
[epoch1, step2763]: loss 1.591876
[epoch1, step2764]: loss 1.933095
[epoch1, step2765]: loss 1.634631
[epoch1, step2766]: loss 1.463496
[epoch1, step2767]: loss 1.946510
[epoch1, step2768]: loss 1.655114
[epoch1, step2769]: loss 1.822707
[epoch1, step2770]: loss 1.654506
[epoch1, step2771]: loss 1.609441
[epoch1, step2772]: loss 1.887979
[epoch1, step2773]: loss 1.780418
[epoch1, step2774]: loss 2.099428
[epoch1, step2775]: loss 1.513802
[epoch1, step2776]: loss 1.807096
[epoch1, step2777]: loss 1.219600
[epoch1, step2778]: loss 1.621472
[epoch1, step2779]: loss 1.711678
[epoch1, step2780]: loss 1.663971
[epoch1, step2781]: loss 1.659301
[epoch1, step2782]: loss 1.599392
[epoch1, step2783]: loss 1.506118
[epoch1, step2784]: loss 2.158511
[epoch1, step2785]: loss 1.709959
[epoch1, step2786]: loss 1.751760
[epoch1, step2787]: loss 1.809327
[epoch1, step2788]: loss 1.936130
[epoch1, step2789]: loss 1.649495
[epoch1, step2790]: loss 1.989646
[epoch1, step2791]: loss 1.780169
[epoch1, step2792]: loss 1.985701
[epoch1, step2793]: loss 1.664292
[epoch1, step2794]: loss 1.951699
[epoch1, step2795]: loss 1.585215
[epoch1, step2796]: loss 1.667580
[epoch1, step2797]: loss 1.840637
[epoch1, step2798]: loss 1.873693
[epoch1, step2799]: loss 2.067710
[epoch1, step2800]: loss 1.893955
[epoch1, step2801]: loss 1.819141
[epoch1, step2802]: loss 1.785692
[epoch1, step2803]: loss 1.857420
[epoch1, step2804]: loss 2.147002
[epoch1, step2805]: loss 1.771090
[epoch1, step2806]: loss 1.811289
[epoch1, step2807]: loss 1.790098
[epoch1, step2808]: loss 1.804831
[epoch1, step2809]: loss 1.468007
[epoch1, step2810]: loss 1.717361
[epoch1, step2811]: loss 1.815829
[epoch1, step2812]: loss 1.719128
[epoch1, step2813]: loss 1.590277
[epoch1, step2814]: loss 1.462402
[epoch1, step2815]: loss 1.782656
[epoch1, step2816]: loss 1.787405
[epoch1, step2817]: loss 1.883220
[epoch1, step2818]: loss 2.114669
[epoch1, step2819]: loss 1.595519
[epoch1, step2820]: loss 1.861997
[epoch1, step2821]: loss 1.718037
[epoch1, step2822]: loss 1.758619
[epoch1, step2823]: loss 1.872665
[epoch1, step2824]: loss 1.820552
[epoch1, step2825]: loss 1.941919
[epoch1, step2826]: loss 1.883383
[epoch1, step2827]: loss 1.689590
[epoch1, step2828]: loss 1.495860
[epoch1, step2829]: loss 1.571619
[epoch1, step2830]: loss 1.685911
[epoch1, step2831]: loss 1.892058
[epoch1, step2832]: loss 1.736323
[epoch1, step2833]: loss 1.782522
[epoch1, step2834]: loss 1.776798
[epoch1, step2835]: loss 1.284999
[epoch1, step2836]: loss 1.832794
[epoch1, step2837]: loss 1.460326
[epoch1, step2838]: loss 1.712734
[epoch1, step2839]: loss 1.921721
[epoch1, step2840]: loss 1.530807
[epoch1, step2841]: loss 1.476458
[epoch1, step2842]: loss 1.345150
[epoch1, step2843]: loss 1.747278
[epoch1, step2844]: loss 1.558931
[epoch1, step2845]: loss 1.743449
[epoch1, step2846]: loss 2.002963
[epoch1, step2847]: loss 1.695192
[epoch1, step2848]: loss 1.725871
[epoch1, step2849]: loss 1.908958
[epoch1, step2850]: loss 1.593412
[epoch1, step2851]: loss 1.682859
[epoch1, step2852]: loss 1.851492
[epoch1, step2853]: loss 1.164948
[epoch1, step2854]: loss 1.690265
[epoch1, step2855]: loss 1.608416
[epoch1, step2856]: loss 1.762927
[epoch1, step2857]: loss 1.862443
[epoch1, step2858]: loss 2.066192
[epoch1, step2859]: loss 1.615238
[epoch1, step2860]: loss 2.043560
[epoch1, step2861]: loss 1.630795
[epoch1, step2862]: loss 1.776175
[epoch1, step2863]: loss 1.743746
[epoch1, step2864]: loss 1.638131
[epoch1, step2865]: loss 1.378355
[epoch1, step2866]: loss 1.800574
[epoch1, step2867]: loss 1.806520
[epoch1, step2868]: loss 1.931802
[epoch1, step2869]: loss 1.746381
[epoch1, step2870]: loss 1.561068
[epoch1, step2871]: loss 1.480045
[epoch1, step2872]: loss 1.663238
[epoch1, step2873]: loss 1.785321
[epoch1, step2874]: loss 1.862047
[epoch1, step2875]: loss 1.775371
[epoch1, step2876]: loss 1.964900
[epoch1, step2877]: loss 1.686703
[epoch1, step2878]: loss 1.743534
[epoch1, step2879]: loss 1.752494
[epoch1, step2880]: loss 1.465194
[epoch1, step2881]: loss 1.779212
[epoch1, step2882]: loss 1.762665
[epoch1, step2883]: loss 1.743214
[epoch1, step2884]: loss 1.874052
[epoch1, step2885]: loss 1.671287
[epoch1, step2886]: loss 1.649940
[epoch1, step2887]: loss 1.614717
[epoch1, step2888]: loss 1.591154
[epoch1, step2889]: loss 1.559999
[epoch1, step2890]: loss 1.593465
[epoch1, step2891]: loss 1.261210
[epoch1, step2892]: loss 1.791380
[epoch1, step2893]: loss 1.482543
[epoch1, step2894]: loss 1.665604
[epoch1, step2895]: loss 1.940695
[epoch1, step2896]: loss 1.885049
[epoch1, step2897]: loss 1.727949
[epoch1, step2898]: loss 1.363383
[epoch1, step2899]: loss 1.570320
[epoch1, step2900]: loss 2.044423
[epoch1, step2901]: loss 1.676191
[epoch1, step2902]: loss 1.818910
[epoch1, step2903]: loss 1.676982
[epoch1, step2904]: loss 1.704109
[epoch1, step2905]: loss 1.798793
[epoch1, step2906]: loss 1.426236
[epoch1, step2907]: loss 1.937267
[epoch1, step2908]: loss 1.542735
[epoch1, step2909]: loss 1.959305
[epoch1, step2910]: loss 1.731398
[epoch1, step2911]: loss 1.774090
[epoch1, step2912]: loss 1.857406
[epoch1, step2913]: loss 1.572935
[epoch1, step2914]: loss 1.658355
[epoch1, step2915]: loss 1.687482
[epoch1, step2916]: loss 1.492415
[epoch1, step2917]: loss 1.880209
[epoch1, step2918]: loss 1.832036
[epoch1, step2919]: loss 1.675721
[epoch1, step2920]: loss 1.488138
[epoch1, step2921]: loss 1.718622
[epoch1, step2922]: loss 1.844465
[epoch1, step2923]: loss 1.642167
[epoch1, step2924]: loss 1.706241
[epoch1, step2925]: loss 1.692308
[epoch1, step2926]: loss 1.642574
[epoch1, step2927]: loss 1.185616
[epoch1, step2928]: loss 1.614780
[epoch1, step2929]: loss 1.260074
[epoch1, step2930]: loss 1.853502
[epoch1, step2931]: loss 1.601921
[epoch1, step2932]: loss 1.687516
[epoch1, step2933]: loss 1.618962
[epoch1, step2934]: loss 1.701405
[epoch1, step2935]: loss 1.826725
[epoch1, step2936]: loss 1.937516
[epoch1, step2937]: loss 1.715646
[epoch1, step2938]: loss 2.078106
[epoch1, step2939]: loss 1.651729
[epoch1, step2940]: loss 1.635534
[epoch1, step2941]: loss 1.775286
[epoch1, step2942]: loss 1.586177
[epoch1, step2943]: loss 1.732908
[epoch1, step2944]: loss 1.843005
[epoch1, step2945]: loss 1.646875
[epoch1, step2946]: loss 1.691055
[epoch1, step2947]: loss 1.878230
[epoch1, step2948]: loss 1.390139
[epoch1, step2949]: loss 1.467830
[epoch1, step2950]: loss 1.585296
[epoch1, step2951]: loss 1.625589
[epoch1, step2952]: loss 1.838009
[epoch1, step2953]: loss 1.454585
[epoch1, step2954]: loss 1.685085
[epoch1, step2955]: loss 1.437429
[epoch1, step2956]: loss 1.535048
[epoch1, step2957]: loss 1.787395
[epoch1, step2958]: loss 1.836412
[epoch1, step2959]: loss 1.719035
[epoch1, step2960]: loss 1.208943
[epoch1, step2961]: loss 1.466493
[epoch1, step2962]: loss 1.567540
[epoch1, step2963]: loss 1.879138
[epoch1, step2964]: loss 1.705461
[epoch1, step2965]: loss 1.665262
[epoch1, step2966]: loss 1.664230
[epoch1, step2967]: loss 1.460576
[epoch1, step2968]: loss 1.694309
[epoch1, step2969]: loss 1.380231
[epoch1, step2970]: loss 1.449630
[epoch1, step2971]: loss 1.717358
[epoch1, step2972]: loss 1.517029
[epoch1, step2973]: loss 1.766521
[epoch1, step2974]: loss 1.680915
[epoch1, step2975]: loss 1.761091
[epoch1, step2976]: loss 1.544777
[epoch1, step2977]: loss 1.814962
[epoch1, step2978]: loss 1.661690
[epoch1, step2979]: loss 1.992091
[epoch1, step2980]: loss 1.864418
[epoch1, step2981]: loss 1.262040
[epoch1, step2982]: loss 1.571869
[epoch1, step2983]: loss 1.346844
[epoch1, step2984]: loss 1.377638
[epoch1, step2985]: loss 1.702849
[epoch1, step2986]: loss 1.643412
[epoch1, step2987]: loss 1.723653
[epoch1, step2988]: loss 1.653067
[epoch1, step2989]: loss 1.826387
[epoch1, step2990]: loss 1.623215
[epoch1, step2991]: loss 1.758809
[epoch1, step2992]: loss 1.335944
[epoch1, step2993]: loss 1.447743
[epoch1, step2994]: loss 1.373387
[epoch1, step2995]: loss 1.496184
[epoch1, step2996]: loss 1.684458
[epoch1, step2997]: loss 1.625971
[epoch1, step2998]: loss 1.903818
[epoch1, step2999]: loss 1.894492
[epoch1, step3000]: loss 1.884512
[epoch1, step3001]: loss 1.834196
[epoch1, step3002]: loss 1.469056
[epoch1, step3003]: loss 1.539219
[epoch1, step3004]: loss 1.371539
[epoch1, step3005]: loss 1.453018
[epoch1, step3006]: loss 1.466540
[epoch1, step3007]: loss 1.551547
[epoch1, step3008]: loss 1.728456
[epoch1, step3009]: loss 1.730817
[epoch1, step3010]: loss 1.916637
[epoch1, step3011]: loss 1.664360
[epoch1, step3012]: loss 1.910954
[epoch1, step3013]: loss 1.795316
[epoch1, step3014]: loss 1.652503
[epoch1, step3015]: loss 1.733089
[epoch1, step3016]: loss 1.664741
[epoch1, step3017]: loss 1.787806
[epoch1, step3018]: loss 1.514755
[epoch1, step3019]: loss 1.435519
[epoch1, step3020]: loss 1.517000
[epoch1, step3021]: loss 1.115435
[epoch1, step3022]: loss 1.821366
[epoch1, step3023]: loss 1.566511
[epoch1, step3024]: loss 1.421639
[epoch1, step3025]: loss 1.584070
[epoch1, step3026]: loss 1.666200
[epoch1, step3027]: loss 1.997539
[epoch1, step3028]: loss 1.394986
[epoch1, step3029]: loss 1.727902
[epoch1, step3030]: loss 1.809636
[epoch1, step3031]: loss 1.837260
[epoch1, step3032]: loss 1.271028
[epoch1, step3033]: loss 1.642451
[epoch1, step3034]: loss 1.692480
[epoch1, step3035]: loss 1.381554
[epoch1, step3036]: loss 1.678978
[epoch1, step3037]: loss 1.438183
[epoch1, step3038]: loss 1.436895
[epoch1, step3039]: loss 1.591890
[epoch1, step3040]: loss 1.788964
[epoch1, step3041]: loss 1.291852
[epoch1, step3042]: loss 1.738335
[epoch1, step3043]: loss 1.719715
[epoch1, step3044]: loss 1.545069
[epoch1, step3045]: loss 1.375100
[epoch1, step3046]: loss 1.449419
[epoch1, step3047]: loss 1.892732
[epoch1, step3048]: loss 1.758538
[epoch1, step3049]: loss 1.732737
[epoch1, step3050]: loss 1.218481
[epoch1, step3051]: loss 1.820729
[epoch1, step3052]: loss 1.648283
[epoch1, step3053]: loss 1.674285
[epoch1, step3054]: loss 1.541469
[epoch1, step3055]: loss 1.856567
[epoch1, step3056]: loss 1.850450
[epoch1, step3057]: loss 1.599389
[epoch1, step3058]: loss 1.821037
[epoch1, step3059]: loss 1.965741
[epoch1, step3060]: loss 1.827780
[epoch1, step3061]: loss 1.716926
[epoch1, step3062]: loss 1.813975
[epoch1, step3063]: loss 1.760097
[epoch1, step3064]: loss 1.573934
[epoch1, step3065]: loss 1.467223
[epoch1, step3066]: loss 1.583945
[epoch1, step3067]: loss 1.631375
[epoch1, step3068]: loss 1.670195
[epoch1, step3069]: loss 1.541514
[epoch1, step3070]: loss 1.611845
[epoch1, step3071]: loss 1.225770
[epoch1, step3072]: loss 1.715076
[epoch1, step3073]: loss 1.806832
[epoch1, step3074]: loss 1.761528
[epoch1, step3075]: loss 1.315127
[epoch1, step3076]: loss 1.876782

[epoch1]: avg loss 1.876782

[epoch2, step1]: loss 1.880745
[epoch2, step2]: loss 1.632892
[epoch2, step3]: loss 1.755475
[epoch2, step4]: loss 1.519943
[epoch2, step5]: loss 1.706675
[epoch2, step6]: loss 1.825052
[epoch2, step7]: loss 1.752628
[epoch2, step8]: loss 1.665044
[epoch2, step9]: loss 1.038728
[epoch2, step10]: loss 1.650820
[epoch2, step11]: loss 1.427650
[epoch2, step12]: loss 1.673133
[epoch2, step13]: loss 1.556414
[epoch2, step14]: loss 1.917394
[epoch2, step15]: loss 1.600610
[epoch2, step16]: loss 1.415288
[epoch2, step17]: loss 1.306735
[epoch2, step18]: loss 1.651974
[epoch2, step19]: loss 1.837938
[epoch2, step20]: loss 1.566782
[epoch2, step21]: loss 1.798311
[epoch2, step22]: loss 1.382186
[epoch2, step23]: loss 1.775277
[epoch2, step24]: loss 1.647548
[epoch2, step25]: loss 1.855046
[epoch2, step26]: loss 1.726432
[epoch2, step27]: loss 1.698362
[epoch2, step28]: loss 1.947262
[epoch2, step29]: loss 1.470525
[epoch2, step30]: loss 1.522928
[epoch2, step31]: loss 1.570360
[epoch2, step32]: loss 1.553066
[epoch2, step33]: loss 1.769026
[epoch2, step34]: loss 1.440016
[epoch2, step35]: loss 1.855879
[epoch2, step36]: loss 1.808495
[epoch2, step37]: loss 1.786222
[epoch2, step38]: loss 1.702788
[epoch2, step39]: loss 1.435963
[epoch2, step40]: loss 1.835798
[epoch2, step41]: loss 1.549016
[epoch2, step42]: loss 1.943302
[epoch2, step43]: loss 1.338217
[epoch2, step44]: loss 1.780422
[epoch2, step45]: loss 1.673051
[epoch2, step46]: loss 1.463866
[epoch2, step47]: loss 1.587691
[epoch2, step48]: loss 1.540899
[epoch2, step49]: loss 1.802380
[epoch2, step50]: loss 1.516223
[epoch2, step51]: loss 2.053080
[epoch2, step52]: loss 1.867581
[epoch2, step53]: loss 1.632102
[epoch2, step54]: loss 1.639214
[epoch2, step55]: loss 1.474109
[epoch2, step56]: loss 1.790453
[epoch2, step57]: loss 1.358871
[epoch2, step58]: loss 1.688171
[epoch2, step59]: loss 1.439158
[epoch2, step60]: loss 1.656100
[epoch2, step61]: loss 1.728205
[epoch2, step62]: loss 1.862652
[epoch2, step63]: loss 1.677378
[epoch2, step64]: loss 1.747542
[epoch2, step65]: loss 1.410403
[epoch2, step66]: loss 1.485973
[epoch2, step67]: loss 1.193623
[epoch2, step68]: loss 1.606777
[epoch2, step69]: loss 1.720515
[epoch2, step70]: loss 1.483823
[epoch2, step71]: loss 1.778121
[epoch2, step72]: loss 1.590590
[epoch2, step73]: loss 1.712623
[epoch2, step74]: loss 1.776240
[epoch2, step75]: loss 1.407399
[epoch2, step76]: loss 1.728789
[epoch2, step77]: loss 1.464215
[epoch2, step78]: loss 1.424831
[epoch2, step79]: loss 1.874173
[epoch2, step80]: loss 1.469414
[epoch2, step81]: loss 1.485489
[epoch2, step82]: loss 1.305691
[epoch2, step83]: loss 1.480784
[epoch2, step84]: loss 1.694226
[epoch2, step85]: loss 1.805894
[epoch2, step86]: loss 1.922761
[epoch2, step87]: loss 1.692342
[epoch2, step88]: loss 1.978719
[epoch2, step89]: loss 1.232277
[epoch2, step90]: loss 1.454600
[epoch2, step91]: loss 1.544588
[epoch2, step92]: loss 1.624825
[epoch2, step93]: loss 1.860578
[epoch2, step94]: loss 1.210765
[epoch2, step95]: loss 1.730935
[epoch2, step96]: loss 1.788044
[epoch2, step97]: loss 1.389436
[epoch2, step98]: loss 1.438695
[epoch2, step99]: loss 1.584395
[epoch2, step100]: loss 1.480269
[epoch2, step101]: loss 1.534305
[epoch2, step102]: loss 1.744884
[epoch2, step103]: loss 1.716780
[epoch2, step104]: loss 1.491918
[epoch2, step105]: loss 1.676573
[epoch2, step106]: loss 1.278844
[epoch2, step107]: loss 1.611099
[epoch2, step108]: loss 1.543678
[epoch2, step109]: loss 1.427261
[epoch2, step110]: loss 1.775632
[epoch2, step111]: loss 1.378597
[epoch2, step112]: loss 1.471317
[epoch2, step113]: loss 1.354077
[epoch2, step114]: loss 1.465242
[epoch2, step115]: loss 1.664566
[epoch2, step116]: loss 1.477384
[epoch2, step117]: loss 1.565619
[epoch2, step118]: loss 1.582012
[epoch2, step119]: loss 1.558350
[epoch2, step120]: loss 1.798845
[epoch2, step121]: loss 1.771552
[epoch2, step122]: loss 1.607131
[epoch2, step123]: loss 1.574871
[epoch2, step124]: loss 1.510783
[epoch2, step125]: loss 1.641148
[epoch2, step126]: loss 1.404112
[epoch2, step127]: loss 1.807876
[epoch2, step128]: loss 1.602890
[epoch2, step129]: loss 1.227599
[epoch2, step130]: loss 1.688759
[epoch2, step131]: loss 1.739932
[epoch2, step132]: loss 1.369480
[epoch2, step133]: loss 1.756248
[epoch2, step134]: loss 1.620490
[epoch2, step135]: loss 1.359446
[epoch2, step136]: loss 1.578301
[epoch2, step137]: loss 1.583928
[epoch2, step138]: loss 1.832386
[epoch2, step139]: loss 1.833724
[epoch2, step140]: loss 1.556636
[epoch2, step141]: loss 1.376418
[epoch2, step142]: loss 1.638888
[epoch2, step143]: loss 1.589023
[epoch2, step144]: loss 1.678736
[epoch2, step145]: loss 1.632286
[epoch2, step146]: loss 1.789130
[epoch2, step147]: loss 1.568107
[epoch2, step148]: loss 1.660441
[epoch2, step149]: loss 1.441862
[epoch2, step150]: loss 1.622902
[epoch2, step151]: loss 1.526330
[epoch2, step152]: loss 1.534364
[epoch2, step153]: loss 1.669869
[epoch2, step154]: loss 1.454129
[epoch2, step155]: loss 1.663537
[epoch2, step156]: loss 1.614531
[epoch2, step157]: loss 1.457904
[epoch2, step158]: loss 1.676164
[epoch2, step159]: loss 1.173572
[epoch2, step160]: loss 1.630965
[epoch2, step161]: loss 1.568119
[epoch2, step162]: loss 1.639722
[epoch2, step163]: loss 1.526858
[epoch2, step164]: loss 1.915730
[epoch2, step165]: loss 1.451129
[epoch2, step166]: loss 1.850099
[epoch2, step167]: loss 1.452894
[epoch2, step168]: loss 1.662095
[epoch2, step169]: loss 1.507981
[epoch2, step170]: loss 1.568107
[epoch2, step171]: loss 1.589885
[epoch2, step172]: loss 1.480031
[epoch2, step173]: loss 1.641542
[epoch2, step174]: loss 1.718617
[epoch2, step175]: loss 1.712514
[epoch2, step176]: loss 1.632203
[epoch2, step177]: loss 1.506191
[epoch2, step178]: loss 1.315571
[epoch2, step179]: loss 1.519994
[epoch2, step180]: loss 1.447817
[epoch2, step181]: loss 1.774730
[epoch2, step182]: loss 1.343371
[epoch2, step183]: loss 1.637442
[epoch2, step184]: loss 1.619590
[epoch2, step185]: loss 1.726436
[epoch2, step186]: loss 1.549936
[epoch2, step187]: loss 1.456971
[epoch2, step188]: loss 1.741392
[epoch2, step189]: loss 1.593692
[epoch2, step190]: loss 1.178081
[epoch2, step191]: loss 1.289302
[epoch2, step192]: loss 1.510041
[epoch2, step193]: loss 1.870401
[epoch2, step194]: loss 1.461492
[epoch2, step195]: loss 1.611735
[epoch2, step196]: loss 1.803448
[epoch2, step197]: loss 1.728905
[epoch2, step198]: loss 1.578104
[epoch2, step199]: loss 1.393513
[epoch2, step200]: loss 1.559792
[epoch2, step201]: loss 1.532655
[epoch2, step202]: loss 1.636888
[epoch2, step203]: loss 1.339024
[epoch2, step204]: loss 1.580207
[epoch2, step205]: loss 1.689747
[epoch2, step206]: loss 1.524244
[epoch2, step207]: loss 1.547460
[epoch2, step208]: loss 1.479651
[epoch2, step209]: loss 1.430951
[epoch2, step210]: loss 1.593765
[epoch2, step211]: loss 1.608898
[epoch2, step212]: loss 1.375196
[epoch2, step213]: loss 1.568812
[epoch2, step214]: loss 1.734218
[epoch2, step215]: loss 1.560382
[epoch2, step216]: loss 1.286494
[epoch2, step217]: loss 1.352237
[epoch2, step218]: loss 1.518828
[epoch2, step219]: loss 1.636496
[epoch2, step220]: loss 1.650255
[epoch2, step221]: loss 1.390814
[epoch2, step222]: loss 1.156767
[epoch2, step223]: loss 1.189184
[epoch2, step224]: loss 1.716789
[epoch2, step225]: loss 1.586503
[epoch2, step226]: loss 1.609407
[epoch2, step227]: loss 1.606528
[epoch2, step228]: loss 1.475641
[epoch2, step229]: loss 1.307130
[epoch2, step230]: loss 1.874760
[epoch2, step231]: loss 1.683936
[epoch2, step232]: loss 1.484366
[epoch2, step233]: loss 1.664365
[epoch2, step234]: loss 1.650759
[epoch2, step235]: loss 1.462808
[epoch2, step236]: loss 1.735994
[epoch2, step237]: loss 1.616057
[epoch2, step238]: loss 1.624659
[epoch2, step239]: loss 1.620826
[epoch2, step240]: loss 1.493955
[epoch2, step241]: loss 1.827074
[epoch2, step242]: loss 1.297880
[epoch2, step243]: loss 1.303082
[epoch2, step244]: loss 1.355874
[epoch2, step245]: loss 1.588822
[epoch2, step246]: loss 1.677240
[epoch2, step247]: loss 1.224357
[epoch2, step248]: loss 1.757970
[epoch2, step249]: loss 1.522081
[epoch2, step250]: loss 1.220816
[epoch2, step251]: loss 1.863524
[epoch2, step252]: loss 1.508705
[epoch2, step253]: loss 1.582827
[epoch2, step254]: loss 1.604367
[epoch2, step255]: loss 1.461141
[epoch2, step256]: loss 1.561865
[epoch2, step257]: loss 1.313363
[epoch2, step258]: loss 1.449902
[epoch2, step259]: loss 1.453382
[epoch2, step260]: loss 1.473232
[epoch2, step261]: loss 1.608734
[epoch2, step262]: loss 1.673754
[epoch2, step263]: loss 1.449384
[epoch2, step264]: loss 1.494373
[epoch2, step265]: loss 1.636103
[epoch2, step266]: loss 1.364926
[epoch2, step267]: loss 1.462372
[epoch2, step268]: loss 1.474822
[epoch2, step269]: loss 1.688287
[epoch2, step270]: loss 1.708473
[epoch2, step271]: loss 1.372446
[epoch2, step272]: loss 1.731782
[epoch2, step273]: loss 1.858821
[epoch2, step274]: loss 1.359218
[epoch2, step275]: loss 1.394647
[epoch2, step276]: loss 1.789193
[epoch2, step277]: loss 1.647707
[epoch2, step278]: loss 1.490937
[epoch2, step279]: loss 1.776530
[epoch2, step280]: loss 1.600299
[epoch2, step281]: loss 1.483198
[epoch2, step282]: loss 1.771970
[epoch2, step283]: loss 1.611363
[epoch2, step284]: loss 1.599899
[epoch2, step285]: loss 1.479043
[epoch2, step286]: loss 1.346506
[epoch2, step287]: loss 1.483602
[epoch2, step288]: loss 1.353687
[epoch2, step289]: loss 1.300914
[epoch2, step290]: loss 1.518859
[epoch2, step291]: loss 1.654899
[epoch2, step292]: loss 1.655424
[epoch2, step293]: loss 1.758257
[epoch2, step294]: loss 1.263958
[epoch2, step295]: loss 1.694197
[epoch2, step296]: loss 1.668392
[epoch2, step297]: loss 1.207087
[epoch2, step298]: loss 1.586675
[epoch2, step299]: loss 1.306320
[epoch2, step300]: loss 1.582232
[epoch2, step301]: loss 1.668232
[epoch2, step302]: loss 1.683846
[epoch2, step303]: loss 1.124814
[epoch2, step304]: loss 1.711198
[epoch2, step305]: loss 1.705458
[epoch2, step306]: loss 1.754531
[epoch2, step307]: loss 1.505663
[epoch2, step308]: loss 1.404995
[epoch2, step309]: loss 1.461459
[epoch2, step310]: loss 1.558086
[epoch2, step311]: loss 1.522159
[epoch2, step312]: loss 1.337130
[epoch2, step313]: loss 1.334940
[epoch2, step314]: loss 1.473270
[epoch2, step315]: loss 1.415281
[epoch2, step316]: loss 1.789203
[epoch2, step317]: loss 1.625341
[epoch2, step318]: loss 1.753278
[epoch2, step319]: loss 1.300530
[epoch2, step320]: loss 1.422737
[epoch2, step321]: loss 1.326937
[epoch2, step322]: loss 1.708129
[epoch2, step323]: loss 1.509896
[epoch2, step324]: loss 1.417599
[epoch2, step325]: loss 1.683818
[epoch2, step326]: loss 1.279004
[epoch2, step327]: loss 1.299911
[epoch2, step328]: loss 1.548032
[epoch2, step329]: loss 1.503350
[epoch2, step330]: loss 1.857012
[epoch2, step331]: loss 1.598074
[epoch2, step332]: loss 1.111147
[epoch2, step333]: loss 1.744272
[epoch2, step334]: loss 1.405334
[epoch2, step335]: loss 1.561233
[epoch2, step336]: loss 1.681408
[epoch2, step337]: loss 1.749445
[epoch2, step338]: loss 1.388177
[epoch2, step339]: loss 1.516896
[epoch2, step340]: loss 1.468740
[epoch2, step341]: loss 1.628803
[epoch2, step342]: loss 1.500537
[epoch2, step343]: loss 1.527765
[epoch2, step344]: loss 1.589920
[epoch2, step345]: loss 1.548690
[epoch2, step346]: loss 1.305666
[epoch2, step347]: loss 1.270145
[epoch2, step348]: loss 1.285662
[epoch2, step349]: loss 1.469479
[epoch2, step350]: loss 1.614189
[epoch2, step351]: loss 1.195984
[epoch2, step352]: loss 1.391878
[epoch2, step353]: loss 1.378159
[epoch2, step354]: loss 1.461301
[epoch2, step355]: loss 1.714243
[epoch2, step356]: loss 1.478423
[epoch2, step357]: loss 1.207663
[epoch2, step358]: loss 1.539577
[epoch2, step359]: loss 1.496710
[epoch2, step360]: loss 1.778011
[epoch2, step361]: loss 1.185422
[epoch2, step362]: loss 1.683414
[epoch2, step363]: loss 1.127260
[epoch2, step364]: loss 1.191126
[epoch2, step365]: loss 1.488238
[epoch2, step366]: loss 1.119459
[epoch2, step367]: loss 1.603901
[epoch2, step368]: loss 1.606813
[epoch2, step369]: loss 1.519696
[epoch2, step370]: loss 1.836260
[epoch2, step371]: loss 1.596956
[epoch2, step372]: loss 1.477901
[epoch2, step373]: loss 1.562411
[epoch2, step374]: loss 1.688918
[epoch2, step375]: loss 1.735357
[epoch2, step376]: loss 1.564849
[epoch2, step377]: loss 1.601624
[epoch2, step378]: loss 1.425205
[epoch2, step379]: loss 1.454355
[epoch2, step380]: loss 1.324111
[epoch2, step381]: loss 1.554082
[epoch2, step382]: loss 1.704860
[epoch2, step383]: loss 1.444120
[epoch2, step384]: loss 1.272834
[epoch2, step385]: loss 1.416246
[epoch2, step386]: loss 1.191549
[epoch2, step387]: loss 1.596143
[epoch2, step388]: loss 1.717481
[epoch2, step389]: loss 1.753881
[epoch2, step390]: loss 1.652189
[epoch2, step391]: loss 1.797654
[epoch2, step392]: loss 1.356905
[epoch2, step393]: loss 1.645296
[epoch2, step394]: loss 1.705306
[epoch2, step395]: loss 1.517239
[epoch2, step396]: loss 1.430533
[epoch2, step397]: loss 1.803843
[epoch2, step398]: loss 1.345551
[epoch2, step399]: loss 1.267090
[epoch2, step400]: loss 1.454288
[epoch2, step401]: loss 1.477947
[epoch2, step402]: loss 1.672839
[epoch2, step403]: loss 1.815865
[epoch2, step404]: loss 1.190579
[epoch2, step405]: loss 1.422852
[epoch2, step406]: loss 1.146275
[epoch2, step407]: loss 1.599795
[epoch2, step408]: loss 1.430749
[epoch2, step409]: loss 1.264880
[epoch2, step410]: loss 1.687485
[epoch2, step411]: loss 1.759624
[epoch2, step412]: loss 1.306455
[epoch2, step413]: loss 1.194270
[epoch2, step414]: loss 1.606886
[epoch2, step415]: loss 1.692843
[epoch2, step416]: loss 1.298754
[epoch2, step417]: loss 1.577788
[epoch2, step418]: loss 1.473364
[epoch2, step419]: loss 1.587072
[epoch2, step420]: loss 1.621008
[epoch2, step421]: loss 1.232315
[epoch2, step422]: loss 1.352761
[epoch2, step423]: loss 1.221276
[epoch2, step424]: loss 1.599958
[epoch2, step425]: loss 1.260303
[epoch2, step426]: loss 1.550733
[epoch2, step427]: loss 1.513731
[epoch2, step428]: loss 1.366275
[epoch2, step429]: loss 1.695361
[epoch2, step430]: loss 1.398695
[epoch2, step431]: loss 1.393447
[epoch2, step432]: loss 1.847996
[epoch2, step433]: loss 1.213585
[epoch2, step434]: loss 1.328862
[epoch2, step435]: loss 1.787235
[epoch2, step436]: loss 1.477855
[epoch2, step437]: loss 1.479075
[epoch2, step438]: loss 1.601597
[epoch2, step439]: loss 1.309325
[epoch2, step440]: loss 1.647714
[epoch2, step441]: loss 1.218848
[epoch2, step442]: loss 1.852941
[epoch2, step443]: loss 1.569768
[epoch2, step444]: loss 1.284662
[epoch2, step445]: loss 1.628340
[epoch2, step446]: loss 1.498238
[epoch2, step447]: loss 1.661283
[epoch2, step448]: loss 1.474829
[epoch2, step449]: loss 1.301960
[epoch2, step450]: loss 1.398465
[epoch2, step451]: loss 1.598848
[epoch2, step452]: loss 1.652785
[epoch2, step453]: loss 1.792502
[epoch2, step454]: loss 1.188143
[epoch2, step455]: loss 1.539349
[epoch2, step456]: loss 1.337260
[epoch2, step457]: loss 1.720079
[epoch2, step458]: loss 1.331429
[epoch2, step459]: loss 1.289347
[epoch2, step460]: loss 1.691149
[epoch2, step461]: loss 1.525462
[epoch2, step462]: loss 1.879821
[epoch2, step463]: loss 1.667228
[epoch2, step464]: loss 1.404221
[epoch2, step465]: loss 1.602849
[epoch2, step466]: loss 1.361465
[epoch2, step467]: loss 1.633042
[epoch2, step468]: loss 1.623049
[epoch2, step469]: loss 1.472480
[epoch2, step470]: loss 1.252007
[epoch2, step471]: loss 1.616347
[epoch2, step472]: loss 1.535683
[epoch2, step473]: loss 1.786435
[epoch2, step474]: loss 1.578914
[epoch2, step475]: loss 1.765309
[epoch2, step476]: loss 1.427276
[epoch2, step477]: loss 1.243544
[epoch2, step478]: loss 1.474895
[epoch2, step479]: loss 1.701401
[epoch2, step480]: loss 1.275277
[epoch2, step481]: loss 1.576120
[epoch2, step482]: loss 1.451815
[epoch2, step483]: loss 1.434221
[epoch2, step484]: loss 1.031874
[epoch2, step485]: loss 1.242153
[epoch2, step486]: loss 1.439806
[epoch2, step487]: loss 1.451619
[epoch2, step488]: loss 1.422482
[epoch2, step489]: loss 1.285696
[epoch2, step490]: loss 1.361073
[epoch2, step491]: loss 1.394435
[epoch2, step492]: loss 1.731232
[epoch2, step493]: loss 1.304758
[epoch2, step494]: loss 1.560977
[epoch2, step495]: loss 1.631214
[epoch2, step496]: loss 1.209842
[epoch2, step497]: loss 1.381289
[epoch2, step498]: loss 1.481502
[epoch2, step499]: loss 1.432217
[epoch2, step500]: loss 1.385828
[epoch2, step501]: loss 1.491567
[epoch2, step502]: loss 1.328317
[epoch2, step503]: loss 1.416073
[epoch2, step504]: loss 1.682670
[epoch2, step505]: loss 1.283775
[epoch2, step506]: loss 1.035107
[epoch2, step507]: loss 1.277334
[epoch2, step508]: loss 1.328204
[epoch2, step509]: loss 1.568316
[epoch2, step510]: loss 1.406840
[epoch2, step511]: loss 1.568629
[epoch2, step512]: loss 1.552996
[epoch2, step513]: loss 1.570436
[epoch2, step514]: loss 1.311330
[epoch2, step515]: loss 1.604761
[epoch2, step516]: loss 1.545196
[epoch2, step517]: loss 1.264113
[epoch2, step518]: loss 1.561541
[epoch2, step519]: loss 1.569330
[epoch2, step520]: loss 1.427283
[epoch2, step521]: loss 1.202984
[epoch2, step522]: loss 1.705377
[epoch2, step523]: loss 1.393241
[epoch2, step524]: loss 1.622663
[epoch2, step525]: loss 1.071905
[epoch2, step526]: loss 1.554817
[epoch2, step527]: loss 1.371659
[epoch2, step528]: loss 1.462798
[epoch2, step529]: loss 1.671335
[epoch2, step530]: loss 1.514051
[epoch2, step531]: loss 1.488039
[epoch2, step532]: loss 1.633670
[epoch2, step533]: loss 1.520341
[epoch2, step534]: loss 1.306072
[epoch2, step535]: loss 1.370310
[epoch2, step536]: loss 1.789848
[epoch2, step537]: loss 1.522388
[epoch2, step538]: loss 1.649224
[epoch2, step539]: loss 1.169519
[epoch2, step540]: loss 1.491679
[epoch2, step541]: loss 1.262130
[epoch2, step542]: loss 1.568710
[epoch2, step543]: loss 1.198721
[epoch2, step544]: loss 1.554895
[epoch2, step545]: loss 1.596945
[epoch2, step546]: loss 1.629402
[epoch2, step547]: loss 1.468975
[epoch2, step548]: loss 1.336804
[epoch2, step549]: loss 1.542134
[epoch2, step550]: loss 0.964692
[epoch2, step551]: loss 0.959189
[epoch2, step552]: loss 1.496717
[epoch2, step553]: loss 1.649615
[epoch2, step554]: loss 1.235298
[epoch2, step555]: loss 1.335560
[epoch2, step556]: loss 1.629831
[epoch2, step557]: loss 1.667389
[epoch2, step558]: loss 1.641968
[epoch2, step559]: loss 1.167184
[epoch2, step560]: loss 1.230207
[epoch2, step561]: loss 1.191379
[epoch2, step562]: loss 1.778639
[epoch2, step563]: loss 1.299479
[epoch2, step564]: loss 1.388949
[epoch2, step565]: loss 1.422412
[epoch2, step566]: loss 1.327024
[epoch2, step567]: loss 1.416468
[epoch2, step568]: loss 1.124323
[epoch2, step569]: loss 1.373538
[epoch2, step570]: loss 1.539627
[epoch2, step571]: loss 1.375562
[epoch2, step572]: loss 1.558305
[epoch2, step573]: loss 1.496757
[epoch2, step574]: loss 1.656406
[epoch2, step575]: loss 1.465888
[epoch2, step576]: loss 1.345381
[epoch2, step577]: loss 1.465192
[epoch2, step578]: loss 1.208915
[epoch2, step579]: loss 1.658006
[epoch2, step580]: loss 1.370279
[epoch2, step581]: loss 1.437778
[epoch2, step582]: loss 1.320215
[epoch2, step583]: loss 1.250439
[epoch2, step584]: loss 1.455456
[epoch2, step585]: loss 1.413191
[epoch2, step586]: loss 1.393870
[epoch2, step587]: loss 1.505777
[epoch2, step588]: loss 1.421398
[epoch2, step589]: loss 1.194728
[epoch2, step590]: loss 1.255573
[epoch2, step591]: loss 1.082483
[epoch2, step592]: loss 1.459885
[epoch2, step593]: loss 1.454209
[epoch2, step594]: loss 1.329194
[epoch2, step595]: loss 1.282210
[epoch2, step596]: loss 1.579671
[epoch2, step597]: loss 1.113005
[epoch2, step598]: loss 1.480000
[epoch2, step599]: loss 1.522179
[epoch2, step600]: loss 1.483746
[epoch2, step601]: loss 1.031830
[epoch2, step602]: loss 1.800740
[epoch2, step603]: loss 1.755094
[epoch2, step604]: loss 1.620001
[epoch2, step605]: loss 1.362201
[epoch2, step606]: loss 1.495030
[epoch2, step607]: loss 1.261671
[epoch2, step608]: loss 1.684902
[epoch2, step609]: loss 1.394592
[epoch2, step610]: loss 1.605216
[epoch2, step611]: loss 1.630400
[epoch2, step612]: loss 1.060665
[epoch2, step613]: loss 1.691848
[epoch2, step614]: loss 1.251833
[epoch2, step615]: loss 1.748324
[epoch2, step616]: loss 1.643626
[epoch2, step617]: loss 1.658213
[epoch2, step618]: loss 1.487924
[epoch2, step619]: loss 1.402622
[epoch2, step620]: loss 1.363947
[epoch2, step621]: loss 1.190246
[epoch2, step622]: loss 1.300197
[epoch2, step623]: loss 1.772883
[epoch2, step624]: loss 1.500958
[epoch2, step625]: loss 1.535295
[epoch2, step626]: loss 1.593045
[epoch2, step627]: loss 1.578863
[epoch2, step628]: loss 1.513177
[epoch2, step629]: loss 1.147147
[epoch2, step630]: loss 1.289804
[epoch2, step631]: loss 1.407972
[epoch2, step632]: loss 1.208679
[epoch2, step633]: loss 1.567715
[epoch2, step634]: loss 1.332357
[epoch2, step635]: loss 1.547007
[epoch2, step636]: loss 1.282562
[epoch2, step637]: loss 1.682910
[epoch2, step638]: loss 1.334885
[epoch2, step639]: loss 1.377506
[epoch2, step640]: loss 0.897936
[epoch2, step641]: loss 1.014715
[epoch2, step642]: loss 1.491354
[epoch2, step643]: loss 1.673398
[epoch2, step644]: loss 1.427716
[epoch2, step645]: loss 1.492214
[epoch2, step646]: loss 1.673436
[epoch2, step647]: loss 1.416499
[epoch2, step648]: loss 1.557755
[epoch2, step649]: loss 1.174973
[epoch2, step650]: loss 1.530731
[epoch2, step651]: loss 1.336804
[epoch2, step652]: loss 1.289083
[epoch2, step653]: loss 1.505601
[epoch2, step654]: loss 1.471362
[epoch2, step655]: loss 1.216561
[epoch2, step656]: loss 1.330000
[epoch2, step657]: loss 1.486653
[epoch2, step658]: loss 1.165327
[epoch2, step659]: loss 1.237959
[epoch2, step660]: loss 1.077521
[epoch2, step661]: loss 1.128248
[epoch2, step662]: loss 1.358017
[epoch2, step663]: loss 1.593401
[epoch2, step664]: loss 1.257009
[epoch2, step665]: loss 1.310620
[epoch2, step666]: loss 1.337940
[epoch2, step667]: loss 1.342477
[epoch2, step668]: loss 1.569964
[epoch2, step669]: loss 1.330783
[epoch2, step670]: loss 1.622469
[epoch2, step671]: loss 1.666011
[epoch2, step672]: loss 1.208427
[epoch2, step673]: loss 0.987274
[epoch2, step674]: loss 1.458115
[epoch2, step675]: loss 1.503446
[epoch2, step676]: loss 1.326166
[epoch2, step677]: loss 1.523566
[epoch2, step678]: loss 1.544274
[epoch2, step679]: loss 1.435741
[epoch2, step680]: loss 1.318285
[epoch2, step681]: loss 1.767285
[epoch2, step682]: loss 1.584939
[epoch2, step683]: loss 1.541937
[epoch2, step684]: loss 1.389125
[epoch2, step685]: loss 1.410443
[epoch2, step686]: loss 1.540026
[epoch2, step687]: loss 1.598584
[epoch2, step688]: loss 1.418252
[epoch2, step689]: loss 1.565102
[epoch2, step690]: loss 1.153666
[epoch2, step691]: loss 1.591335
[epoch2, step692]: loss 1.555044
[epoch2, step693]: loss 1.717340
[epoch2, step694]: loss 1.528946
[epoch2, step695]: loss 1.585752
[epoch2, step696]: loss 1.524705
[epoch2, step697]: loss 1.808544
[epoch2, step698]: loss 1.020154
[epoch2, step699]: loss 1.385722
[epoch2, step700]: loss 1.452909
[epoch2, step701]: loss 1.689219
[epoch2, step702]: loss 1.487472
[epoch2, step703]: loss 1.275328
[epoch2, step704]: loss 1.341480
[epoch2, step705]: loss 1.530026
[epoch2, step706]: loss 1.216472
[epoch2, step707]: loss 1.078980
[epoch2, step708]: loss 1.385985
[epoch2, step709]: loss 1.115932
[epoch2, step710]: loss 1.405852
[epoch2, step711]: loss 1.445042
[epoch2, step712]: loss 1.459581
[epoch2, step713]: loss 1.478761
[epoch2, step714]: loss 1.383345
[epoch2, step715]: loss 1.545471
[epoch2, step716]: loss 1.241081
[epoch2, step717]: loss 1.541489
[epoch2, step718]: loss 1.660097
[epoch2, step719]: loss 1.310994
[epoch2, step720]: loss 1.173579
[epoch2, step721]: loss 1.117779
[epoch2, step722]: loss 1.293077
[epoch2, step723]: loss 1.326389
[epoch2, step724]: loss 1.434114
[epoch2, step725]: loss 1.511589
[epoch2, step726]: loss 1.513382
[epoch2, step727]: loss 1.603780
[epoch2, step728]: loss 1.643273
[epoch2, step729]: loss 1.509453
[epoch2, step730]: loss 1.361569
[epoch2, step731]: loss 1.251267
[epoch2, step732]: loss 1.200965
[epoch2, step733]: loss 1.602201
[epoch2, step734]: loss 1.386114
[epoch2, step735]: loss 1.334477
[epoch2, step736]: loss 1.064287
[epoch2, step737]: loss 1.436941
[epoch2, step738]: loss 1.340417
[epoch2, step739]: loss 1.349502
[epoch2, step740]: loss 1.049081
[epoch2, step741]: loss 1.537326
[epoch2, step742]: loss 1.397699
[epoch2, step743]: loss 1.387129
[epoch2, step744]: loss 1.507192
[epoch2, step745]: loss 1.246058
[epoch2, step746]: loss 1.460018
[epoch2, step747]: loss 1.504615
[epoch2, step748]: loss 1.259681
[epoch2, step749]: loss 1.274949
[epoch2, step750]: loss 1.637037
[epoch2, step751]: loss 1.304415
[epoch2, step752]: loss 1.670292
[epoch2, step753]: loss 1.511252
[epoch2, step754]: loss 1.415511
[epoch2, step755]: loss 1.232917
[epoch2, step756]: loss 1.334387
[epoch2, step757]: loss 1.213538
[epoch2, step758]: loss 1.480145
[epoch2, step759]: loss 1.553406
[epoch2, step760]: loss 1.571559
[epoch2, step761]: loss 1.474308
[epoch2, step762]: loss 1.246183
[epoch2, step763]: loss 1.704682
[epoch2, step764]: loss 1.574011
[epoch2, step765]: loss 1.280252
[epoch2, step766]: loss 1.531668
[epoch2, step767]: loss 1.499013
[epoch2, step768]: loss 1.642248
[epoch2, step769]: loss 1.556520
[epoch2, step770]: loss 1.183936
[epoch2, step771]: loss 1.160441
[epoch2, step772]: loss 1.556680
[epoch2, step773]: loss 1.397860
[epoch2, step774]: loss 1.282854
[epoch2, step775]: loss 1.468036
[epoch2, step776]: loss 1.594475
[epoch2, step777]: loss 1.357448
[epoch2, step778]: loss 1.344123
[epoch2, step779]: loss 1.222382
[epoch2, step780]: loss 1.409154
[epoch2, step781]: loss 1.213767
[epoch2, step782]: loss 1.351149
[epoch2, step783]: loss 1.369232
[epoch2, step784]: loss 1.241281
[epoch2, step785]: loss 1.316911
[epoch2, step786]: loss 1.344380
[epoch2, step787]: loss 1.312728
[epoch2, step788]: loss 1.246950
[epoch2, step789]: loss 1.321465
[epoch2, step790]: loss 1.259997
[epoch2, step791]: loss 1.261149
[epoch2, step792]: loss 1.525610
[epoch2, step793]: loss 1.540000
[epoch2, step794]: loss 1.615505
[epoch2, step795]: loss 1.462370
[epoch2, step796]: loss 0.994807
[epoch2, step797]: loss 1.530582
[epoch2, step798]: loss 1.254960
[epoch2, step799]: loss 1.333239
[epoch2, step800]: loss 1.365424
[epoch2, step801]: loss 1.215549
[epoch2, step802]: loss 1.402621
[epoch2, step803]: loss 1.462285
[epoch2, step804]: loss 1.288612
[epoch2, step805]: loss 1.333308
[epoch2, step806]: loss 1.344692
[epoch2, step807]: loss 1.499768
[epoch2, step808]: loss 1.412786
[epoch2, step809]: loss 1.479410
[epoch2, step810]: loss 1.421708
[epoch2, step811]: loss 1.331168
[epoch2, step812]: loss 1.178056
[epoch2, step813]: loss 1.030863
[epoch2, step814]: loss 1.448606
[epoch2, step815]: loss 1.313625
[epoch2, step816]: loss 1.391680
[epoch2, step817]: loss 1.623215
[epoch2, step818]: loss 1.424474
[epoch2, step819]: loss 1.229109
[epoch2, step820]: loss 1.709311
[epoch2, step821]: loss 1.327966
[epoch2, step822]: loss 1.546232
[epoch2, step823]: loss 1.091561
[epoch2, step824]: loss 1.259612
[epoch2, step825]: loss 1.352600
[epoch2, step826]: loss 1.341528
[epoch2, step827]: loss 1.489034
[epoch2, step828]: loss 1.434798
[epoch2, step829]: loss 1.528885
[epoch2, step830]: loss 1.590029
[epoch2, step831]: loss 1.666356
[epoch2, step832]: loss 1.292092
[epoch2, step833]: loss 1.193285
[epoch2, step834]: loss 1.556600
[epoch2, step835]: loss 1.552178
[epoch2, step836]: loss 1.747357
[epoch2, step837]: loss 1.338030
[epoch2, step838]: loss 1.313662
[epoch2, step839]: loss 1.396696
[epoch2, step840]: loss 0.766168
[epoch2, step841]: loss 1.463103
[epoch2, step842]: loss 1.340518
[epoch2, step843]: loss 1.476389
[epoch2, step844]: loss 1.184753
[epoch2, step845]: loss 1.089333
[epoch2, step846]: loss 1.170372
[epoch2, step847]: loss 1.478002
[epoch2, step848]: loss 1.373406
[epoch2, step849]: loss 1.634834
[epoch2, step850]: loss 1.506939
[epoch2, step851]: loss 1.556434
[epoch2, step852]: loss 1.409025
[epoch2, step853]: loss 1.269290
[epoch2, step854]: loss 1.147315
[epoch2, step855]: loss 1.388838
[epoch2, step856]: loss 1.360877
[epoch2, step857]: loss 1.479789
[epoch2, step858]: loss 1.483425
[epoch2, step859]: loss 1.546409
[epoch2, step860]: loss 1.415051
[epoch2, step861]: loss 1.561843
[epoch2, step862]: loss 1.528010
[epoch2, step863]: loss 1.253630
[epoch2, step864]: loss 1.472108
[epoch2, step865]: loss 1.177569
[epoch2, step866]: loss 1.382274
[epoch2, step867]: loss 1.045456
[epoch2, step868]: loss 1.172669
[epoch2, step869]: loss 1.371871
[epoch2, step870]: loss 1.420111
[epoch2, step871]: loss 1.294331
[epoch2, step872]: loss 1.483427
[epoch2, step873]: loss 1.576331
[epoch2, step874]: loss 1.240494
[epoch2, step875]: loss 1.201446
[epoch2, step876]: loss 1.239549
[epoch2, step877]: loss 1.288537
[epoch2, step878]: loss 1.600930
[epoch2, step879]: loss 1.582819
[epoch2, step880]: loss 1.073046
[epoch2, step881]: loss 1.566737
[epoch2, step882]: loss 1.242739
[epoch2, step883]: loss 1.249379
[epoch2, step884]: loss 1.273368
[epoch2, step885]: loss 1.501854
[epoch2, step886]: loss 1.325708
[epoch2, step887]: loss 1.630443
[epoch2, step888]: loss 1.379099
[epoch2, step889]: loss 1.205723
[epoch2, step890]: loss 1.482932
[epoch2, step891]: loss 1.655818
[epoch2, step892]: loss 1.843850
[epoch2, step893]: loss 1.320389
[epoch2, step894]: loss 1.498446
[epoch2, step895]: loss 1.632739
[epoch2, step896]: loss 1.208320
[epoch2, step897]: loss 1.601022
[epoch2, step898]: loss 1.489550
[epoch2, step899]: loss 1.516434
[epoch2, step900]: loss 1.686668
[epoch2, step901]: loss 1.512443
[epoch2, step902]: loss 1.279352
[epoch2, step903]: loss 1.585127
[epoch2, step904]: loss 1.344272
[epoch2, step905]: loss 1.086430
[epoch2, step906]: loss 1.385880
[epoch2, step907]: loss 1.730845
[epoch2, step908]: loss 1.481633
[epoch2, step909]: loss 1.138216
[epoch2, step910]: loss 1.228469
[epoch2, step911]: loss 1.284587
[epoch2, step912]: loss 1.204063
[epoch2, step913]: loss 1.684506
[epoch2, step914]: loss 1.463233
[epoch2, step915]: loss 1.171050
[epoch2, step916]: loss 1.572140
[epoch2, step917]: loss 1.211375
[epoch2, step918]: loss 1.225745
[epoch2, step919]: loss 1.482876
[epoch2, step920]: loss 1.571087
[epoch2, step921]: loss 1.114068
[epoch2, step922]: loss 1.200600
[epoch2, step923]: loss 1.289202
[epoch2, step924]: loss 0.900280
[epoch2, step925]: loss 1.778486
[epoch2, step926]: loss 1.342719
[epoch2, step927]: loss 1.529244
[epoch2, step928]: loss 1.467573
[epoch2, step929]: loss 1.240206
[epoch2, step930]: loss 1.325730
[epoch2, step931]: loss 1.479974
[epoch2, step932]: loss 1.610831
[epoch2, step933]: loss 1.419296
[epoch2, step934]: loss 1.229344
[epoch2, step935]: loss 1.609095
[epoch2, step936]: loss 1.509895
[epoch2, step937]: loss 1.214166
[epoch2, step938]: loss 1.390587
[epoch2, step939]: loss 1.263357
[epoch2, step940]: loss 1.353905
[epoch2, step941]: loss 1.165375
[epoch2, step942]: loss 1.412441
[epoch2, step943]: loss 1.665757
[epoch2, step944]: loss 1.184776
[epoch2, step945]: loss 1.281893
[epoch2, step946]: loss 1.064999
[epoch2, step947]: loss 1.335863
[epoch2, step948]: loss 1.361825
[epoch2, step949]: loss 1.558020
[epoch2, step950]: loss 1.116620
[epoch2, step951]: loss 1.622599
[epoch2, step952]: loss 1.523391
[epoch2, step953]: loss 1.321177
[epoch2, step954]: loss 1.489970
[epoch2, step955]: loss 1.712116
[epoch2, step956]: loss 1.177891
[epoch2, step957]: loss 1.608165
[epoch2, step958]: loss 1.046246
[epoch2, step959]: loss 1.317670
[epoch2, step960]: loss 1.511603
[epoch2, step961]: loss 1.377815
[epoch2, step962]: loss 1.368605
[epoch2, step963]: loss 0.994405
[epoch2, step964]: loss 1.420845
[epoch2, step965]: loss 1.508402
[epoch2, step966]: loss 1.528796
[epoch2, step967]: loss 1.575601
[epoch2, step968]: loss 1.777366
[epoch2, step969]: loss 1.519446
[epoch2, step970]: loss 1.523838
[epoch2, step971]: loss 1.418095
[epoch2, step972]: loss 1.414996
[epoch2, step973]: loss 1.000604
[epoch2, step974]: loss 1.505946
[epoch2, step975]: loss 1.369212
[epoch2, step976]: loss 1.437217
[epoch2, step977]: loss 1.302165
[epoch2, step978]: loss 1.467210
[epoch2, step979]: loss 1.161804
[epoch2, step980]: loss 1.614938
[epoch2, step981]: loss 1.371000
[epoch2, step982]: loss 1.463704
[epoch2, step983]: loss 1.600683
[epoch2, step984]: loss 1.499461
[epoch2, step985]: loss 1.568727
[epoch2, step986]: loss 1.227090
[epoch2, step987]: loss 1.167545
[epoch2, step988]: loss 1.422781
[epoch2, step989]: loss 1.264433
[epoch2, step990]: loss 1.409787
[epoch2, step991]: loss 1.688830
[epoch2, step992]: loss 1.279880
[epoch2, step993]: loss 1.344696
[epoch2, step994]: loss 1.454533
[epoch2, step995]: loss 1.504849
[epoch2, step996]: loss 1.304293
[epoch2, step997]: loss 1.176610
[epoch2, step998]: loss 1.082111
[epoch2, step999]: loss 1.187209
[epoch2, step1000]: loss 1.686408
[epoch2, step1001]: loss 1.454584
[epoch2, step1002]: loss 1.682007
[epoch2, step1003]: loss 1.382028
[epoch2, step1004]: loss 1.264778
[epoch2, step1005]: loss 1.069012
[epoch2, step1006]: loss 1.338010
[epoch2, step1007]: loss 1.509737
[epoch2, step1008]: loss 1.236642
[epoch2, step1009]: loss 1.551132
[epoch2, step1010]: loss 1.388065
[epoch2, step1011]: loss 1.383312
[epoch2, step1012]: loss 1.329113
[epoch2, step1013]: loss 1.431659
[epoch2, step1014]: loss 1.539969
[epoch2, step1015]: loss 1.514456
[epoch2, step1016]: loss 1.483312
[epoch2, step1017]: loss 1.255567
[epoch2, step1018]: loss 1.107491
[epoch2, step1019]: loss 1.049591
[epoch2, step1020]: loss 1.368992
[epoch2, step1021]: loss 1.618369
[epoch2, step1022]: loss 1.185103
[epoch2, step1023]: loss 1.071696
[epoch2, step1024]: loss 1.554704
[epoch2, step1025]: loss 1.582041
[epoch2, step1026]: loss 1.331558
[epoch2, step1027]: loss 1.450410
[epoch2, step1028]: loss 1.348746
[epoch2, step1029]: loss 1.291383
[epoch2, step1030]: loss 1.408138
[epoch2, step1031]: loss 1.505000
[epoch2, step1032]: loss 1.078053
[epoch2, step1033]: loss 1.394153
[epoch2, step1034]: loss 1.497699
[epoch2, step1035]: loss 1.311048
[epoch2, step1036]: loss 1.412444
[epoch2, step1037]: loss 1.644786
[epoch2, step1038]: loss 1.523254
[epoch2, step1039]: loss 1.254939
[epoch2, step1040]: loss 1.233095
[epoch2, step1041]: loss 1.418382
[epoch2, step1042]: loss 1.488954
[epoch2, step1043]: loss 1.277136
[epoch2, step1044]: loss 1.598133
[epoch2, step1045]: loss 1.092751
[epoch2, step1046]: loss 1.317825
[epoch2, step1047]: loss 1.558272
[epoch2, step1048]: loss 1.222198
[epoch2, step1049]: loss 1.288726
[epoch2, step1050]: loss 1.414831
[epoch2, step1051]: loss 1.396993
[epoch2, step1052]: loss 1.236516
[epoch2, step1053]: loss 1.222392
[epoch2, step1054]: loss 1.326954
[epoch2, step1055]: loss 1.604114
[epoch2, step1056]: loss 1.426841
[epoch2, step1057]: loss 1.757417
[epoch2, step1058]: loss 1.300231
[epoch2, step1059]: loss 1.435622
[epoch2, step1060]: loss 1.379851
[epoch2, step1061]: loss 1.627234
[epoch2, step1062]: loss 1.444318
[epoch2, step1063]: loss 1.538553
[epoch2, step1064]: loss 1.552224
[epoch2, step1065]: loss 1.310782
[epoch2, step1066]: loss 1.562403
[epoch2, step1067]: loss 1.277919
[epoch2, step1068]: loss 1.519511
[epoch2, step1069]: loss 1.342420
[epoch2, step1070]: loss 1.426748
[epoch2, step1071]: loss 1.501471
[epoch2, step1072]: loss 1.358618
[epoch2, step1073]: loss 1.144977
[epoch2, step1074]: loss 1.349169
[epoch2, step1075]: loss 1.330293
[epoch2, step1076]: loss 1.452428
[epoch2, step1077]: loss 1.181127
[epoch2, step1078]: loss 1.575357
[epoch2, step1079]: loss 1.043667
[epoch2, step1080]: loss 1.500309
[epoch2, step1081]: loss 1.098308
[epoch2, step1082]: loss 1.334415
[epoch2, step1083]: loss 1.528332
[epoch2, step1084]: loss 1.292009
[epoch2, step1085]: loss 1.126980
[epoch2, step1086]: loss 1.558913
[epoch2, step1087]: loss 1.503707
[epoch2, step1088]: loss 1.044534
[epoch2, step1089]: loss 1.079483
[epoch2, step1090]: loss 1.210330
[epoch2, step1091]: loss 1.164535
[epoch2, step1092]: loss 1.567647
[epoch2, step1093]: loss 1.310605
[epoch2, step1094]: loss 1.021728
[epoch2, step1095]: loss 1.558885
[epoch2, step1096]: loss 1.119606
[epoch2, step1097]: loss 1.308609
[epoch2, step1098]: loss 0.692346
[epoch2, step1099]: loss 1.278613
[epoch2, step1100]: loss 1.375295
[epoch2, step1101]: loss 1.351457
[epoch2, step1102]: loss 1.208234
[epoch2, step1103]: loss 1.293167
[epoch2, step1104]: loss 1.207983
[epoch2, step1105]: loss 1.116728
[epoch2, step1106]: loss 1.408365
[epoch2, step1107]: loss 1.559538
[epoch2, step1108]: loss 1.077325
[epoch2, step1109]: loss 1.167171
[epoch2, step1110]: loss 1.497499
[epoch2, step1111]: loss 1.315891
[epoch2, step1112]: loss 1.149139
[epoch2, step1113]: loss 1.384830
[epoch2, step1114]: loss 1.332332
[epoch2, step1115]: loss 1.467929
[epoch2, step1116]: loss 1.388605
[epoch2, step1117]: loss 1.391286
[epoch2, step1118]: loss 1.161678
[epoch2, step1119]: loss 1.294464
[epoch2, step1120]: loss 1.198620
[epoch2, step1121]: loss 1.279859
[epoch2, step1122]: loss 1.242315
[epoch2, step1123]: loss 1.542005
[epoch2, step1124]: loss 1.072353
[epoch2, step1125]: loss 1.258982
[epoch2, step1126]: loss 1.356774
[epoch2, step1127]: loss 1.633894
[epoch2, step1128]: loss 1.739177
[epoch2, step1129]: loss 1.552229
[epoch2, step1130]: loss 1.202665
[epoch2, step1131]: loss 1.106537
[epoch2, step1132]: loss 1.486882
[epoch2, step1133]: loss 1.225166
[epoch2, step1134]: loss 0.745012
[epoch2, step1135]: loss 1.435402
[epoch2, step1136]: loss 1.119129
[epoch2, step1137]: loss 1.311454
[epoch2, step1138]: loss 1.160017
[epoch2, step1139]: loss 1.599550
[epoch2, step1140]: loss 1.472848
[epoch2, step1141]: loss 1.258096
[epoch2, step1142]: loss 1.241691
[epoch2, step1143]: loss 1.178587
[epoch2, step1144]: loss 1.303540
[epoch2, step1145]: loss 1.152364
[epoch2, step1146]: loss 1.161634
[epoch2, step1147]: loss 1.696033
[epoch2, step1148]: loss 1.320431
[epoch2, step1149]: loss 1.187375
[epoch2, step1150]: loss 1.571455
[epoch2, step1151]: loss 1.467703
[epoch2, step1152]: loss 1.345835
[epoch2, step1153]: loss 0.828778
[epoch2, step1154]: loss 1.413481
[epoch2, step1155]: loss 1.078290
[epoch2, step1156]: loss 1.438776
[epoch2, step1157]: loss 1.204424
[epoch2, step1158]: loss 1.349011
[epoch2, step1159]: loss 1.572146
[epoch2, step1160]: loss 1.156479
[epoch2, step1161]: loss 1.444266
[epoch2, step1162]: loss 1.442985
[epoch2, step1163]: loss 1.451040
[epoch2, step1164]: loss 1.604170
[epoch2, step1165]: loss 1.032629
[epoch2, step1166]: loss 1.295992
[epoch2, step1167]: loss 1.067900
[epoch2, step1168]: loss 1.180869
[epoch2, step1169]: loss 1.473695
[epoch2, step1170]: loss 1.097855
[epoch2, step1171]: loss 1.345611
[epoch2, step1172]: loss 0.963085
[epoch2, step1173]: loss 1.136642
[epoch2, step1174]: loss 1.511014
[epoch2, step1175]: loss 1.575915
[epoch2, step1176]: loss 1.290938
[epoch2, step1177]: loss 1.500849
[epoch2, step1178]: loss 1.331975
[epoch2, step1179]: loss 1.377027
[epoch2, step1180]: loss 1.705994
[epoch2, step1181]: loss 1.602658
[epoch2, step1182]: loss 1.349863
[epoch2, step1183]: loss 1.067458
[epoch2, step1184]: loss 1.390385
[epoch2, step1185]: loss 0.883063
[epoch2, step1186]: loss 1.408997
[epoch2, step1187]: loss 1.274263
[epoch2, step1188]: loss 1.164463
[epoch2, step1189]: loss 1.200027
[epoch2, step1190]: loss 1.217798
[epoch2, step1191]: loss 1.041345
[epoch2, step1192]: loss 1.232108
[epoch2, step1193]: loss 0.823773
[epoch2, step1194]: loss 1.355583
[epoch2, step1195]: loss 1.116397
[epoch2, step1196]: loss 1.534771
[epoch2, step1197]: loss 1.225445
[epoch2, step1198]: loss 1.026312
[epoch2, step1199]: loss 1.366656
[epoch2, step1200]: loss 1.027758
[epoch2, step1201]: loss 1.499799
[epoch2, step1202]: loss 0.795723
[epoch2, step1203]: loss 1.515502
[epoch2, step1204]: loss 1.119217
[epoch2, step1205]: loss 1.334108
[epoch2, step1206]: loss 1.297239
[epoch2, step1207]: loss 1.694641
[epoch2, step1208]: loss 1.287997
[epoch2, step1209]: loss 1.010535
[epoch2, step1210]: loss 1.438754
[epoch2, step1211]: loss 1.369588
[epoch2, step1212]: loss 1.102162
[epoch2, step1213]: loss 1.408121
[epoch2, step1214]: loss 1.452152
[epoch2, step1215]: loss 1.333915
[epoch2, step1216]: loss 1.224022
[epoch2, step1217]: loss 1.495076
[epoch2, step1218]: loss 1.660262
[epoch2, step1219]: loss 0.987300
[epoch2, step1220]: loss 1.166179
[epoch2, step1221]: loss 1.570098
[epoch2, step1222]: loss 1.529972
[epoch2, step1223]: loss 1.443549
[epoch2, step1224]: loss 1.542102
[epoch2, step1225]: loss 1.178586
[epoch2, step1226]: loss 1.295465
[epoch2, step1227]: loss 1.148203
[epoch2, step1228]: loss 1.110361
[epoch2, step1229]: loss 1.421913
[epoch2, step1230]: loss 1.374001
[epoch2, step1231]: loss 1.528456
[epoch2, step1232]: loss 1.279936
[epoch2, step1233]: loss 1.278497
[epoch2, step1234]: loss 1.298275
[epoch2, step1235]: loss 1.323696
[epoch2, step1236]: loss 1.244802
[epoch2, step1237]: loss 1.498260
[epoch2, step1238]: loss 1.066697
[epoch2, step1239]: loss 1.584931
[epoch2, step1240]: loss 1.112514
[epoch2, step1241]: loss 1.616424
[epoch2, step1242]: loss 1.134526
[epoch2, step1243]: loss 1.266955
[epoch2, step1244]: loss 1.383302
[epoch2, step1245]: loss 1.222233
[epoch2, step1246]: loss 1.579159
[epoch2, step1247]: loss 1.441118
[epoch2, step1248]: loss 1.236603
[epoch2, step1249]: loss 1.312567
[epoch2, step1250]: loss 1.415454
[epoch2, step1251]: loss 1.118671
[epoch2, step1252]: loss 0.926473
[epoch2, step1253]: loss 1.557857
[epoch2, step1254]: loss 1.517504
[epoch2, step1255]: loss 1.350063
[epoch2, step1256]: loss 1.061128
[epoch2, step1257]: loss 1.365519
[epoch2, step1258]: loss 1.222027
[epoch2, step1259]: loss 1.352742
[epoch2, step1260]: loss 1.149611
[epoch2, step1261]: loss 1.282127
[epoch2, step1262]: loss 1.088179
[epoch2, step1263]: loss 1.459034
[epoch2, step1264]: loss 1.256323
[epoch2, step1265]: loss 1.426326
[epoch2, step1266]: loss 1.201777
[epoch2, step1267]: loss 1.458842
[epoch2, step1268]: loss 1.068474
[epoch2, step1269]: loss 1.081535
[epoch2, step1270]: loss 1.407605
[epoch2, step1271]: loss 1.421171
[epoch2, step1272]: loss 1.225353
[epoch2, step1273]: loss 1.614374
[epoch2, step1274]: loss 1.137164
[epoch2, step1275]: loss 1.223938
[epoch2, step1276]: loss 1.242723
[epoch2, step1277]: loss 1.108211
[epoch2, step1278]: loss 1.178193
[epoch2, step1279]: loss 1.437863
[epoch2, step1280]: loss 1.400729
[epoch2, step1281]: loss 1.134736
[epoch2, step1282]: loss 1.519428
[epoch2, step1283]: loss 1.323091
[epoch2, step1284]: loss 1.325758
[epoch2, step1285]: loss 1.425706
[epoch2, step1286]: loss 0.958632
[epoch2, step1287]: loss 1.275049
[epoch2, step1288]: loss 1.204619
[epoch2, step1289]: loss 1.386423
[epoch2, step1290]: loss 1.487424
[epoch2, step1291]: loss 1.228137
[epoch2, step1292]: loss 1.252555
[epoch2, step1293]: loss 1.135076
[epoch2, step1294]: loss 1.501416
[epoch2, step1295]: loss 1.388908
[epoch2, step1296]: loss 1.127168
[epoch2, step1297]: loss 1.470205
[epoch2, step1298]: loss 1.564366
[epoch2, step1299]: loss 0.974361
[epoch2, step1300]: loss 1.215388
[epoch2, step1301]: loss 1.097561
[epoch2, step1302]: loss 1.327415
[epoch2, step1303]: loss 1.537086
[epoch2, step1304]: loss 1.097132
[epoch2, step1305]: loss 1.350489
[epoch2, step1306]: loss 1.434799
[epoch2, step1307]: loss 0.911811
[epoch2, step1308]: loss 1.084880
[epoch2, step1309]: loss 0.885034
[epoch2, step1310]: loss 1.263655
[epoch2, step1311]: loss 1.481249
[epoch2, step1312]: loss 1.128823
[epoch2, step1313]: loss 1.559980
[epoch2, step1314]: loss 1.286419
[epoch2, step1315]: loss 1.173917
[epoch2, step1316]: loss 1.090991
[epoch2, step1317]: loss 1.537221
[epoch2, step1318]: loss 1.200898
[epoch2, step1319]: loss 1.101886
[epoch2, step1320]: loss 0.751624
[epoch2, step1321]: loss 0.949159
[epoch2, step1322]: loss 1.286212
[epoch2, step1323]: loss 1.390746
[epoch2, step1324]: loss 1.096740
[epoch2, step1325]: loss 1.510720
[epoch2, step1326]: loss 1.487991
[epoch2, step1327]: loss 1.247137
[epoch2, step1328]: loss 1.529932
[epoch2, step1329]: loss 1.605339
[epoch2, step1330]: loss 1.416665
[epoch2, step1331]: loss 1.185842
[epoch2, step1332]: loss 1.124228
[epoch2, step1333]: loss 1.228954
[epoch2, step1334]: loss 1.247537
[epoch2, step1335]: loss 1.288376
[epoch2, step1336]: loss 1.473395
[epoch2, step1337]: loss 1.233889
[epoch2, step1338]: loss 1.445123
[epoch2, step1339]: loss 1.492939
[epoch2, step1340]: loss 1.345075
[epoch2, step1341]: loss 1.581808
[epoch2, step1342]: loss 1.196106
[epoch2, step1343]: loss 1.315054
[epoch2, step1344]: loss 1.339117
[epoch2, step1345]: loss 1.247737
[epoch2, step1346]: loss 0.999916
[epoch2, step1347]: loss 1.151029
[epoch2, step1348]: loss 1.019585
[epoch2, step1349]: loss 1.203735
[epoch2, step1350]: loss 1.491246
[epoch2, step1351]: loss 1.533991
[epoch2, step1352]: loss 1.550954
[epoch2, step1353]: loss 1.341285
[epoch2, step1354]: loss 1.460683
[epoch2, step1355]: loss 1.083228
[epoch2, step1356]: loss 1.169253
[epoch2, step1357]: loss 1.299245
[epoch2, step1358]: loss 1.343972
[epoch2, step1359]: loss 1.485151
[epoch2, step1360]: loss 1.415270
[epoch2, step1361]: loss 1.055511
[epoch2, step1362]: loss 1.252040
[epoch2, step1363]: loss 1.087723
[epoch2, step1364]: loss 1.404233
[epoch2, step1365]: loss 1.537824
[epoch2, step1366]: loss 1.266322
[epoch2, step1367]: loss 1.386445
[epoch2, step1368]: loss 1.221008
[epoch2, step1369]: loss 0.996232
[epoch2, step1370]: loss 1.049742
[epoch2, step1371]: loss 1.205227
[epoch2, step1372]: loss 1.277192
[epoch2, step1373]: loss 1.243330
[epoch2, step1374]: loss 1.303176
[epoch2, step1375]: loss 1.366922
[epoch2, step1376]: loss 1.170016
[epoch2, step1377]: loss 1.465074
[epoch2, step1378]: loss 1.041073
[epoch2, step1379]: loss 1.289329
[epoch2, step1380]: loss 1.589496
[epoch2, step1381]: loss 1.260579
[epoch2, step1382]: loss 1.265714
[epoch2, step1383]: loss 1.275239
[epoch2, step1384]: loss 1.639331
[epoch2, step1385]: loss 1.336593
[epoch2, step1386]: loss 1.485289
[epoch2, step1387]: loss 1.047887
[epoch2, step1388]: loss 1.103659
[epoch2, step1389]: loss 1.222736
[epoch2, step1390]: loss 1.133615
[epoch2, step1391]: loss 1.329530
[epoch2, step1392]: loss 0.965255
[epoch2, step1393]: loss 1.600747
[epoch2, step1394]: loss 1.137670
[epoch2, step1395]: loss 1.266813
[epoch2, step1396]: loss 1.522640
[epoch2, step1397]: loss 1.256322
[epoch2, step1398]: loss 1.435113
[epoch2, step1399]: loss 1.348495
[epoch2, step1400]: loss 1.242268
[epoch2, step1401]: loss 0.994218
[epoch2, step1402]: loss 1.073945
[epoch2, step1403]: loss 0.916410
[epoch2, step1404]: loss 1.077091
[epoch2, step1405]: loss 1.253640
[epoch2, step1406]: loss 1.381486
[epoch2, step1407]: loss 1.366680
[epoch2, step1408]: loss 1.430372
[epoch2, step1409]: loss 1.437278
[epoch2, step1410]: loss 1.389140
[epoch2, step1411]: loss 1.138027
[epoch2, step1412]: loss 1.534893
[epoch2, step1413]: loss 1.495860
[epoch2, step1414]: loss 1.201217
[epoch2, step1415]: loss 1.275493
[epoch2, step1416]: loss 1.338331
[epoch2, step1417]: loss 1.172085
[epoch2, step1418]: loss 1.358735
[epoch2, step1419]: loss 0.823773
[epoch2, step1420]: loss 1.512975
[epoch2, step1421]: loss 1.403097
[epoch2, step1422]: loss 1.571571
[epoch2, step1423]: loss 1.255145
[epoch2, step1424]: loss 0.985124
[epoch2, step1425]: loss 1.373838
[epoch2, step1426]: loss 1.156581
[epoch2, step1427]: loss 1.538975
[epoch2, step1428]: loss 1.225732
[epoch2, step1429]: loss 1.164639
[epoch2, step1430]: loss 1.061503
[epoch2, step1431]: loss 1.504641
[epoch2, step1432]: loss 1.177642
[epoch2, step1433]: loss 1.445418
[epoch2, step1434]: loss 1.169546
[epoch2, step1435]: loss 1.311432
[epoch2, step1436]: loss 1.312662
[epoch2, step1437]: loss 1.320578
[epoch2, step1438]: loss 1.048550
[epoch2, step1439]: loss 1.469876
[epoch2, step1440]: loss 1.218393
[epoch2, step1441]: loss 1.353062
[epoch2, step1442]: loss 1.349630
[epoch2, step1443]: loss 1.262030
[epoch2, step1444]: loss 1.335138
[epoch2, step1445]: loss 1.146492
[epoch2, step1446]: loss 1.185636
[epoch2, step1447]: loss 1.205435
[epoch2, step1448]: loss 1.310270
[epoch2, step1449]: loss 1.254341
[epoch2, step1450]: loss 1.367581
[epoch2, step1451]: loss 1.114441
[epoch2, step1452]: loss 1.066463
[epoch2, step1453]: loss 1.479929
[epoch2, step1454]: loss 1.328151
[epoch2, step1455]: loss 1.417271
[epoch2, step1456]: loss 1.148781
[epoch2, step1457]: loss 1.555103
[epoch2, step1458]: loss 1.140602
[epoch2, step1459]: loss 1.384668
[epoch2, step1460]: loss 1.302446
[epoch2, step1461]: loss 1.082534
[epoch2, step1462]: loss 1.301451
[epoch2, step1463]: loss 1.290378
[epoch2, step1464]: loss 1.289097
[epoch2, step1465]: loss 1.002491
[epoch2, step1466]: loss 1.095461
[epoch2, step1467]: loss 1.207453
[epoch2, step1468]: loss 1.045970
[epoch2, step1469]: loss 1.271212
[epoch2, step1470]: loss 1.085584
[epoch2, step1471]: loss 1.194783
[epoch2, step1472]: loss 1.405559
[epoch2, step1473]: loss 1.428739
[epoch2, step1474]: loss 1.433306
[epoch2, step1475]: loss 1.591931
[epoch2, step1476]: loss 1.221827
[epoch2, step1477]: loss 1.270842
[epoch2, step1478]: loss 1.247297
[epoch2, step1479]: loss 1.483715
[epoch2, step1480]: loss 0.924339
[epoch2, step1481]: loss 1.360831
[epoch2, step1482]: loss 1.306004
[epoch2, step1483]: loss 1.218110
[epoch2, step1484]: loss 1.087770
[epoch2, step1485]: loss 1.174826
[epoch2, step1486]: loss 1.041171
[epoch2, step1487]: loss 1.392115
[epoch2, step1488]: loss 1.163916
[epoch2, step1489]: loss 1.074447
[epoch2, step1490]: loss 1.490171
[epoch2, step1491]: loss 1.460107
[epoch2, step1492]: loss 1.094711
[epoch2, step1493]: loss 0.766644
[epoch2, step1494]: loss 1.340733
[epoch2, step1495]: loss 1.157120
[epoch2, step1496]: loss 1.256229
[epoch2, step1497]: loss 1.034901
[epoch2, step1498]: loss 1.436980
[epoch2, step1499]: loss 1.348454
[epoch2, step1500]: loss 1.290564
[epoch2, step1501]: loss 1.433277
[epoch2, step1502]: loss 1.319720
[epoch2, step1503]: loss 1.300717
[epoch2, step1504]: loss 1.163907
[epoch2, step1505]: loss 1.293381
[epoch2, step1506]: loss 1.237337
[epoch2, step1507]: loss 1.241403
[epoch2, step1508]: loss 0.755873
[epoch2, step1509]: loss 1.017781
[epoch2, step1510]: loss 0.950637
[epoch2, step1511]: loss 1.194191
[epoch2, step1512]: loss 1.461174
[epoch2, step1513]: loss 1.373071
[epoch2, step1514]: loss 1.497271
[epoch2, step1515]: loss 1.349464
[epoch2, step1516]: loss 1.241325
[epoch2, step1517]: loss 1.169132
[epoch2, step1518]: loss 1.329225
[epoch2, step1519]: loss 1.005821
[epoch2, step1520]: loss 1.491425
[epoch2, step1521]: loss 1.412376
[epoch2, step1522]: loss 0.979329
[epoch2, step1523]: loss 1.116845
[epoch2, step1524]: loss 1.294601
[epoch2, step1525]: loss 1.481241
[epoch2, step1526]: loss 1.194973
[epoch2, step1527]: loss 1.358043
[epoch2, step1528]: loss 0.952229
[epoch2, step1529]: loss 1.079039
[epoch2, step1530]: loss 1.249600
[epoch2, step1531]: loss 1.473168
[epoch2, step1532]: loss 1.487146
[epoch2, step1533]: loss 1.048172
[epoch2, step1534]: loss 1.207685
[epoch2, step1535]: loss 1.469248
[epoch2, step1536]: loss 0.931243
[epoch2, step1537]: loss 1.181554
[epoch2, step1538]: loss 1.250162
[epoch2, step1539]: loss 1.271343
[epoch2, step1540]: loss 1.228675
[epoch2, step1541]: loss 1.339023
[epoch2, step1542]: loss 0.880804
[epoch2, step1543]: loss 1.312892
[epoch2, step1544]: loss 1.142202
[epoch2, step1545]: loss 1.044831
[epoch2, step1546]: loss 1.589619
[epoch2, step1547]: loss 1.175535
[epoch2, step1548]: loss 0.981148
[epoch2, step1549]: loss 0.834075
[epoch2, step1550]: loss 1.448875
[epoch2, step1551]: loss 1.370018
[epoch2, step1552]: loss 1.352614
[epoch2, step1553]: loss 1.407606
[epoch2, step1554]: loss 1.251134
[epoch2, step1555]: loss 1.201376
[epoch2, step1556]: loss 1.248588
[epoch2, step1557]: loss 1.263723
[epoch2, step1558]: loss 1.549006
[epoch2, step1559]: loss 1.234553
[epoch2, step1560]: loss 1.253437
[epoch2, step1561]: loss 1.247569
[epoch2, step1562]: loss 1.220506
[epoch2, step1563]: loss 1.016491
[epoch2, step1564]: loss 1.254660
[epoch2, step1565]: loss 1.023528
[epoch2, step1566]: loss 1.398248
[epoch2, step1567]: loss 1.333217
[epoch2, step1568]: loss 1.519340
[epoch2, step1569]: loss 1.323243
[epoch2, step1570]: loss 1.115850
[epoch2, step1571]: loss 1.072416
[epoch2, step1572]: loss 1.300241
[epoch2, step1573]: loss 1.236420
[epoch2, step1574]: loss 1.315250
[epoch2, step1575]: loss 1.241482
[epoch2, step1576]: loss 1.516208
[epoch2, step1577]: loss 1.298108
[epoch2, step1578]: loss 1.310740
[epoch2, step1579]: loss 1.061627
[epoch2, step1580]: loss 1.058087
[epoch2, step1581]: loss 1.027727
[epoch2, step1582]: loss 1.139251
[epoch2, step1583]: loss 1.390045
[epoch2, step1584]: loss 1.070613
[epoch2, step1585]: loss 1.269356
[epoch2, step1586]: loss 0.910399
[epoch2, step1587]: loss 1.044307
[epoch2, step1588]: loss 1.275006
[epoch2, step1589]: loss 1.449808
[epoch2, step1590]: loss 1.358478
[epoch2, step1591]: loss 1.350260
[epoch2, step1592]: loss 1.275623
[epoch2, step1593]: loss 1.270237
[epoch2, step1594]: loss 1.165686
[epoch2, step1595]: loss 1.343313
[epoch2, step1596]: loss 1.472688
[epoch2, step1597]: loss 1.206698
[epoch2, step1598]: loss 1.541102
[epoch2, step1599]: loss 1.264953
[epoch2, step1600]: loss 1.378596
[epoch2, step1601]: loss 1.480240
[epoch2, step1602]: loss 0.982344
[epoch2, step1603]: loss 1.107054
[epoch2, step1604]: loss 1.424701
[epoch2, step1605]: loss 1.089407
[epoch2, step1606]: loss 1.214110
[epoch2, step1607]: loss 1.410331
[epoch2, step1608]: loss 1.376851
[epoch2, step1609]: loss 1.116998
[epoch2, step1610]: loss 1.327999
[epoch2, step1611]: loss 1.254872
[epoch2, step1612]: loss 1.290278
[epoch2, step1613]: loss 1.033585
[epoch2, step1614]: loss 1.439314
[epoch2, step1615]: loss 1.101355
[epoch2, step1616]: loss 1.239909
[epoch2, step1617]: loss 1.290879
[epoch2, step1618]: loss 0.948741
[epoch2, step1619]: loss 1.337054
[epoch2, step1620]: loss 1.331186
[epoch2, step1621]: loss 1.192046
[epoch2, step1622]: loss 1.313507
[epoch2, step1623]: loss 1.217739
[epoch2, step1624]: loss 0.925877
[epoch2, step1625]: loss 1.402752
[epoch2, step1626]: loss 1.043973
[epoch2, step1627]: loss 1.226897
[epoch2, step1628]: loss 1.322850
[epoch2, step1629]: loss 1.448806
[epoch2, step1630]: loss 1.249324
[epoch2, step1631]: loss 1.217995
[epoch2, step1632]: loss 1.333261
[epoch2, step1633]: loss 1.344808
[epoch2, step1634]: loss 1.468020
[epoch2, step1635]: loss 1.550279
[epoch2, step1636]: loss 1.299786
[epoch2, step1637]: loss 1.276826
[epoch2, step1638]: loss 1.442817
[epoch2, step1639]: loss 1.302893
[epoch2, step1640]: loss 1.001499
[epoch2, step1641]: loss 1.379894
[epoch2, step1642]: loss 1.217221
[epoch2, step1643]: loss 1.256413
[epoch2, step1644]: loss 1.123515
[epoch2, step1645]: loss 1.289413
[epoch2, step1646]: loss 1.553562
[epoch2, step1647]: loss 1.172495
[epoch2, step1648]: loss 1.377761
[epoch2, step1649]: loss 1.218914
[epoch2, step1650]: loss 1.101689
[epoch2, step1651]: loss 1.600435
[epoch2, step1652]: loss 1.232622
[epoch2, step1653]: loss 1.538982
[epoch2, step1654]: loss 1.164264
[epoch2, step1655]: loss 1.151350
[epoch2, step1656]: loss 1.089858
[epoch2, step1657]: loss 1.376595
[epoch2, step1658]: loss 1.104919
[epoch2, step1659]: loss 1.144302
[epoch2, step1660]: loss 1.047565
[epoch2, step1661]: loss 1.278949
[epoch2, step1662]: loss 1.099170
[epoch2, step1663]: loss 1.279863
[epoch2, step1664]: loss 1.299611
[epoch2, step1665]: loss 1.411871
[epoch2, step1666]: loss 1.020991
[epoch2, step1667]: loss 1.459121
[epoch2, step1668]: loss 1.476359
[epoch2, step1669]: loss 1.062184
[epoch2, step1670]: loss 1.126983
[epoch2, step1671]: loss 1.338086
[epoch2, step1672]: loss 1.487517
[epoch2, step1673]: loss 1.133782
[epoch2, step1674]: loss 0.935924
[epoch2, step1675]: loss 1.409817
[epoch2, step1676]: loss 1.482316
[epoch2, step1677]: loss 1.058020
[epoch2, step1678]: loss 1.170848
[epoch2, step1679]: loss 1.333409
[epoch2, step1680]: loss 1.147564
[epoch2, step1681]: loss 1.205597
[epoch2, step1682]: loss 1.423817
[epoch2, step1683]: loss 1.327962
[epoch2, step1684]: loss 1.136596
[epoch2, step1685]: loss 1.341371
[epoch2, step1686]: loss 1.087671
[epoch2, step1687]: loss 1.347984
[epoch2, step1688]: loss 1.138885
[epoch2, step1689]: loss 0.924268
[epoch2, step1690]: loss 0.643988
[epoch2, step1691]: loss 1.322600
[epoch2, step1692]: loss 1.329366
[epoch2, step1693]: loss 1.229143
[epoch2, step1694]: loss 1.462453
[epoch2, step1695]: loss 1.013362
[epoch2, step1696]: loss 1.143111
[epoch2, step1697]: loss 0.738045
[epoch2, step1698]: loss 1.418728
[epoch2, step1699]: loss 1.132951
[epoch2, step1700]: loss 0.915855
[epoch2, step1701]: loss 1.326492
[epoch2, step1702]: loss 1.301517
[epoch2, step1703]: loss 1.281105
[epoch2, step1704]: loss 1.418197
[epoch2, step1705]: loss 1.068061
[epoch2, step1706]: loss 1.180041
[epoch2, step1707]: loss 1.361645
[epoch2, step1708]: loss 0.925798
[epoch2, step1709]: loss 1.492750
[epoch2, step1710]: loss 1.362970
[epoch2, step1711]: loss 1.155844
[epoch2, step1712]: loss 1.245920
[epoch2, step1713]: loss 1.362806
[epoch2, step1714]: loss 1.298939
[epoch2, step1715]: loss 1.466844
[epoch2, step1716]: loss 1.223608
[epoch2, step1717]: loss 1.017328
[epoch2, step1718]: loss 1.321164
[epoch2, step1719]: loss 1.406276
[epoch2, step1720]: loss 1.529835
[epoch2, step1721]: loss 1.165487
[epoch2, step1722]: loss 1.316946
[epoch2, step1723]: loss 0.793422
[epoch2, step1724]: loss 1.377692
[epoch2, step1725]: loss 1.466207
[epoch2, step1726]: loss 1.237074
[epoch2, step1727]: loss 1.329908
[epoch2, step1728]: loss 1.345769
[epoch2, step1729]: loss 1.110856
[epoch2, step1730]: loss 1.138475
[epoch2, step1731]: loss 1.230479
[epoch2, step1732]: loss 1.215329
[epoch2, step1733]: loss 0.979293
[epoch2, step1734]: loss 1.203239
[epoch2, step1735]: loss 1.339119
[epoch2, step1736]: loss 1.193816
[epoch2, step1737]: loss 1.395131
[epoch2, step1738]: loss 0.970757
[epoch2, step1739]: loss 0.869858
[epoch2, step1740]: loss 1.362697
[epoch2, step1741]: loss 1.209725
[epoch2, step1742]: loss 1.172637
[epoch2, step1743]: loss 1.282029
[epoch2, step1744]: loss 1.339195
[epoch2, step1745]: loss 1.430483
[epoch2, step1746]: loss 1.148353
[epoch2, step1747]: loss 1.061141
[epoch2, step1748]: loss 1.352394
[epoch2, step1749]: loss 1.064160
[epoch2, step1750]: loss 1.468084
[epoch2, step1751]: loss 1.329796
[epoch2, step1752]: loss 1.143414
[epoch2, step1753]: loss 1.065813
[epoch2, step1754]: loss 1.208224
[epoch2, step1755]: loss 1.184421
[epoch2, step1756]: loss 1.256021
[epoch2, step1757]: loss 0.915103
[epoch2, step1758]: loss 1.090643
[epoch2, step1759]: loss 1.016254
[epoch2, step1760]: loss 1.238310
[epoch2, step1761]: loss 1.124884
[epoch2, step1762]: loss 1.451915
[epoch2, step1763]: loss 1.310348
[epoch2, step1764]: loss 1.304791
[epoch2, step1765]: loss 1.377080
[epoch2, step1766]: loss 0.941434
[epoch2, step1767]: loss 1.360309
[epoch2, step1768]: loss 1.224155
[epoch2, step1769]: loss 1.249008
[epoch2, step1770]: loss 1.408681
[epoch2, step1771]: loss 0.961920
[epoch2, step1772]: loss 1.364198
[epoch2, step1773]: loss 1.268134
[epoch2, step1774]: loss 0.805507
[epoch2, step1775]: loss 1.123307
[epoch2, step1776]: loss 1.190621
[epoch2, step1777]: loss 1.190436
[epoch2, step1778]: loss 1.455567
[epoch2, step1779]: loss 0.998449
[epoch2, step1780]: loss 1.096684
[epoch2, step1781]: loss 1.161383
[epoch2, step1782]: loss 1.111350
[epoch2, step1783]: loss 1.463995
[epoch2, step1784]: loss 1.315545
[epoch2, step1785]: loss 1.087113
[epoch2, step1786]: loss 1.276472
[epoch2, step1787]: loss 1.212476
[epoch2, step1788]: loss 1.227271
[epoch2, step1789]: loss 1.236431
[epoch2, step1790]: loss 1.257800
[epoch2, step1791]: loss 0.964464
[epoch2, step1792]: loss 1.102145
[epoch2, step1793]: loss 1.448762
[epoch2, step1794]: loss 1.291924
[epoch2, step1795]: loss 1.525842
[epoch2, step1796]: loss 1.166395
[epoch2, step1797]: loss 1.058150
[epoch2, step1798]: loss 1.277796
[epoch2, step1799]: loss 1.354695
[epoch2, step1800]: loss 1.194278
[epoch2, step1801]: loss 1.079530
[epoch2, step1802]: loss 1.429507
[epoch2, step1803]: loss 1.119736
[epoch2, step1804]: loss 1.584128
[epoch2, step1805]: loss 0.850611
[epoch2, step1806]: loss 1.365651
[epoch2, step1807]: loss 1.383386
[epoch2, step1808]: loss 1.255466
[epoch2, step1809]: loss 0.919844
[epoch2, step1810]: loss 1.003197
[epoch2, step1811]: loss 1.492994
[epoch2, step1812]: loss 1.038266
[epoch2, step1813]: loss 1.369008
[epoch2, step1814]: loss 1.323785
[epoch2, step1815]: loss 1.357668
[epoch2, step1816]: loss 1.207082
[epoch2, step1817]: loss 1.162209
[epoch2, step1818]: loss 1.393243
[epoch2, step1819]: loss 1.103793
[epoch2, step1820]: loss 1.451884
[epoch2, step1821]: loss 0.820351
[epoch2, step1822]: loss 1.337261
[epoch2, step1823]: loss 1.515974
[epoch2, step1824]: loss 1.277294
[epoch2, step1825]: loss 0.986246
[epoch2, step1826]: loss 0.991474
[epoch2, step1827]: loss 1.377582
[epoch2, step1828]: loss 1.199191
[epoch2, step1829]: loss 1.415764
[epoch2, step1830]: loss 1.196996
[epoch2, step1831]: loss 1.169419
[epoch2, step1832]: loss 1.026660
[epoch2, step1833]: loss 0.951903
[epoch2, step1834]: loss 1.240689
[epoch2, step1835]: loss 1.415160
[epoch2, step1836]: loss 1.168763
[epoch2, step1837]: loss 0.925894
[epoch2, step1838]: loss 0.979826
[epoch2, step1839]: loss 1.136176
[epoch2, step1840]: loss 1.351324
[epoch2, step1841]: loss 1.276202
[epoch2, step1842]: loss 1.035835
[epoch2, step1843]: loss 1.062282
[epoch2, step1844]: loss 1.234690
[epoch2, step1845]: loss 1.311466
[epoch2, step1846]: loss 1.135124
[epoch2, step1847]: loss 1.054608
[epoch2, step1848]: loss 1.243238
[epoch2, step1849]: loss 0.939955
[epoch2, step1850]: loss 1.322529
[epoch2, step1851]: loss 1.245017
[epoch2, step1852]: loss 1.287736
[epoch2, step1853]: loss 1.198864
[epoch2, step1854]: loss 1.099243
[epoch2, step1855]: loss 0.920034
[epoch2, step1856]: loss 0.995044
[epoch2, step1857]: loss 0.985229
[epoch2, step1858]: loss 1.010699
[epoch2, step1859]: loss 1.272772
[epoch2, step1860]: loss 1.179608
[epoch2, step1861]: loss 0.846171
[epoch2, step1862]: loss 1.016731
[epoch2, step1863]: loss 1.126228
[epoch2, step1864]: loss 1.439456
[epoch2, step1865]: loss 1.314325
[epoch2, step1866]: loss 1.063714
[epoch2, step1867]: loss 1.415693
[epoch2, step1868]: loss 1.213152
[epoch2, step1869]: loss 0.915315
[epoch2, step1870]: loss 1.171275
[epoch2, step1871]: loss 1.262879
[epoch2, step1872]: loss 1.265212
[epoch2, step1873]: loss 1.179901
[epoch2, step1874]: loss 0.956184
[epoch2, step1875]: loss 1.244620
[epoch2, step1876]: loss 0.901191
[epoch2, step1877]: loss 0.743547
[epoch2, step1878]: loss 1.330829
[epoch2, step1879]: loss 0.814147
[epoch2, step1880]: loss 1.123359
[epoch2, step1881]: loss 1.580689
[epoch2, step1882]: loss 1.446350
[epoch2, step1883]: loss 0.913891
[epoch2, step1884]: loss 1.507329
[epoch2, step1885]: loss 1.358512
[epoch2, step1886]: loss 1.124382
[epoch2, step1887]: loss 1.057622
[epoch2, step1888]: loss 1.280410
[epoch2, step1889]: loss 1.096743
[epoch2, step1890]: loss 1.490961
[epoch2, step1891]: loss 1.109660
[epoch2, step1892]: loss 1.022595
[epoch2, step1893]: loss 1.240743
[epoch2, step1894]: loss 1.414798
[epoch2, step1895]: loss 1.532330
[epoch2, step1896]: loss 1.177543
[epoch2, step1897]: loss 1.220576
[epoch2, step1898]: loss 1.184221
[epoch2, step1899]: loss 1.082703
[epoch2, step1900]: loss 1.328348
[epoch2, step1901]: loss 1.134242
[epoch2, step1902]: loss 1.169791
[epoch2, step1903]: loss 1.176946
[epoch2, step1904]: loss 1.322463
[epoch2, step1905]: loss 1.173095
[epoch2, step1906]: loss 0.992424
[epoch2, step1907]: loss 1.337403
[epoch2, step1908]: loss 1.147060
[epoch2, step1909]: loss 1.215915
[epoch2, step1910]: loss 1.165268
[epoch2, step1911]: loss 1.213007
[epoch2, step1912]: loss 1.086724
[epoch2, step1913]: loss 1.031428
[epoch2, step1914]: loss 0.974177
[epoch2, step1915]: loss 1.060483
[epoch2, step1916]: loss 1.288548
[epoch2, step1917]: loss 1.425146
[epoch2, step1918]: loss 1.167972
[epoch2, step1919]: loss 1.401733
[epoch2, step1920]: loss 0.977901
[epoch2, step1921]: loss 1.299030
[epoch2, step1922]: loss 1.321418
[epoch2, step1923]: loss 0.898549
[epoch2, step1924]: loss 0.867936
[epoch2, step1925]: loss 0.704008
[epoch2, step1926]: loss 1.075422
[epoch2, step1927]: loss 1.164862
[epoch2, step1928]: loss 1.194731
[epoch2, step1929]: loss 1.210367
[epoch2, step1930]: loss 1.083817
[epoch2, step1931]: loss 0.891960
[epoch2, step1932]: loss 0.948253
[epoch2, step1933]: loss 1.277377
[epoch2, step1934]: loss 1.174230
[epoch2, step1935]: loss 0.944207
[epoch2, step1936]: loss 1.093239
[epoch2, step1937]: loss 1.325373
[epoch2, step1938]: loss 0.884023
[epoch2, step1939]: loss 1.469049
[epoch2, step1940]: loss 1.291490
[epoch2, step1941]: loss 1.266267
[epoch2, step1942]: loss 1.279689
[epoch2, step1943]: loss 1.229503
[epoch2, step1944]: loss 1.344991
[epoch2, step1945]: loss 1.400217
[epoch2, step1946]: loss 1.324833
[epoch2, step1947]: loss 1.145415
[epoch2, step1948]: loss 1.124533
[epoch2, step1949]: loss 0.977924
[epoch2, step1950]: loss 1.343659
[epoch2, step1951]: loss 1.253588
[epoch2, step1952]: loss 1.222028
[epoch2, step1953]: loss 1.094253
[epoch2, step1954]: loss 1.457415
[epoch2, step1955]: loss 1.121216
[epoch2, step1956]: loss 1.333052
[epoch2, step1957]: loss 1.572770
[epoch2, step1958]: loss 1.045679
[epoch2, step1959]: loss 1.373688
[epoch2, step1960]: loss 1.343186
[epoch2, step1961]: loss 1.241709
[epoch2, step1962]: loss 1.344137
[epoch2, step1963]: loss 1.054811
[epoch2, step1964]: loss 1.103848
[epoch2, step1965]: loss 1.103625
[epoch2, step1966]: loss 1.055684
[epoch2, step1967]: loss 1.243475
[epoch2, step1968]: loss 1.186083
[epoch2, step1969]: loss 1.295654
[epoch2, step1970]: loss 1.147649
[epoch2, step1971]: loss 1.089544
[epoch2, step1972]: loss 1.399172
[epoch2, step1973]: loss 1.055499
[epoch2, step1974]: loss 1.118301
[epoch2, step1975]: loss 1.090776
[epoch2, step1976]: loss 1.250679
[epoch2, step1977]: loss 0.993842
[epoch2, step1978]: loss 1.150855
[epoch2, step1979]: loss 1.368039
[epoch2, step1980]: loss 0.882287
[epoch2, step1981]: loss 1.088728
[epoch2, step1982]: loss 1.178591
[epoch2, step1983]: loss 1.348834
[epoch2, step1984]: loss 1.042255
[epoch2, step1985]: loss 1.198574
[epoch2, step1986]: loss 1.350853
[epoch2, step1987]: loss 1.255067
[epoch2, step1988]: loss 1.331854
[epoch2, step1989]: loss 0.930763
[epoch2, step1990]: loss 1.369285
[epoch2, step1991]: loss 1.254079
[epoch2, step1992]: loss 0.977170
[epoch2, step1993]: loss 1.326514
[epoch2, step1994]: loss 0.984216
[epoch2, step1995]: loss 1.345472
[epoch2, step1996]: loss 0.971915
[epoch2, step1997]: loss 1.149517
[epoch2, step1998]: loss 1.055851
[epoch2, step1999]: loss 1.103142
[epoch2, step2000]: loss 1.083681
[epoch2, step2001]: loss 1.268864
[epoch2, step2002]: loss 1.313871
[epoch2, step2003]: loss 1.284866
[epoch2, step2004]: loss 1.086460
[epoch2, step2005]: loss 1.060982
[epoch2, step2006]: loss 1.472004
[epoch2, step2007]: loss 1.124337
[epoch2, step2008]: loss 1.120011
[epoch2, step2009]: loss 1.186437
[epoch2, step2010]: loss 1.052169
[epoch2, step2011]: loss 1.346227
[epoch2, step2012]: loss 1.400466
[epoch2, step2013]: loss 1.053156
[epoch2, step2014]: loss 1.307691
[epoch2, step2015]: loss 1.266311
[epoch2, step2016]: loss 1.252409
[epoch2, step2017]: loss 1.239789
[epoch2, step2018]: loss 0.720535
[epoch2, step2019]: loss 1.384825
[epoch2, step2020]: loss 1.451411
[epoch2, step2021]: loss 1.137721
[epoch2, step2022]: loss 1.330189
[epoch2, step2023]: loss 1.044756
[epoch2, step2024]: loss 1.172727
[epoch2, step2025]: loss 1.509112
[epoch2, step2026]: loss 1.101829
[epoch2, step2027]: loss 1.171863
[epoch2, step2028]: loss 1.054659
[epoch2, step2029]: loss 1.027300
[epoch2, step2030]: loss 1.242343
[epoch2, step2031]: loss 1.309153
[epoch2, step2032]: loss 1.291950
[epoch2, step2033]: loss 1.491850
[epoch2, step2034]: loss 1.424570
[epoch2, step2035]: loss 1.135775
[epoch2, step2036]: loss 1.321583
[epoch2, step2037]: loss 1.113636
[epoch2, step2038]: loss 1.104222
[epoch2, step2039]: loss 1.194792
[epoch2, step2040]: loss 1.334537
[epoch2, step2041]: loss 1.399101
[epoch2, step2042]: loss 1.130369
[epoch2, step2043]: loss 0.922912
[epoch2, step2044]: loss 1.191858
[epoch2, step2045]: loss 1.389747
[epoch2, step2046]: loss 1.096150
[epoch2, step2047]: loss 1.311616
[epoch2, step2048]: loss 1.288005
[epoch2, step2049]: loss 1.322864
[epoch2, step2050]: loss 1.180421
[epoch2, step2051]: loss 0.997348
[epoch2, step2052]: loss 1.354788
[epoch2, step2053]: loss 1.282306
[epoch2, step2054]: loss 0.934765
[epoch2, step2055]: loss 1.170265
[epoch2, step2056]: loss 1.005774
[epoch2, step2057]: loss 1.085345
[epoch2, step2058]: loss 1.292220
[epoch2, step2059]: loss 0.940690
[epoch2, step2060]: loss 1.164154
[epoch2, step2061]: loss 1.333685
[epoch2, step2062]: loss 1.143727
[epoch2, step2063]: loss 0.969853
[epoch2, step2064]: loss 0.952585
[epoch2, step2065]: loss 1.163941
[epoch2, step2066]: loss 0.991163
[epoch2, step2067]: loss 0.999200
[epoch2, step2068]: loss 0.999577
[epoch2, step2069]: loss 1.299957
[epoch2, step2070]: loss 1.282598
[epoch2, step2071]: loss 1.201551
[epoch2, step2072]: loss 0.986917
[epoch2, step2073]: loss 1.351807
[epoch2, step2074]: loss 1.217702
[epoch2, step2075]: loss 1.290728
[epoch2, step2076]: loss 1.109665
[epoch2, step2077]: loss 1.164748
[epoch2, step2078]: loss 1.294956
[epoch2, step2079]: loss 0.996976
[epoch2, step2080]: loss 0.815365
[epoch2, step2081]: loss 1.401980
[epoch2, step2082]: loss 1.010012
[epoch2, step2083]: loss 0.731660
[epoch2, step2084]: loss 1.145824
[epoch2, step2085]: loss 0.946010
[epoch2, step2086]: loss 1.479054
[epoch2, step2087]: loss 1.209368
[epoch2, step2088]: loss 1.444536
[epoch2, step2089]: loss 1.307079
[epoch2, step2090]: loss 1.378842
[epoch2, step2091]: loss 1.033641
[epoch2, step2092]: loss 1.409753
[epoch2, step2093]: loss 1.240414
[epoch2, step2094]: loss 1.292682
[epoch2, step2095]: loss 1.235114
[epoch2, step2096]: loss 1.144187
[epoch2, step2097]: loss 1.180656
[epoch2, step2098]: loss 1.365391
[epoch2, step2099]: loss 1.011349
[epoch2, step2100]: loss 1.054696
[epoch2, step2101]: loss 0.923640
[epoch2, step2102]: loss 1.174486
[epoch2, step2103]: loss 1.432917
[epoch2, step2104]: loss 1.514978
[epoch2, step2105]: loss 1.539340
[epoch2, step2106]: loss 1.251708
[epoch2, step2107]: loss 1.076675
[epoch2, step2108]: loss 0.875415
[epoch2, step2109]: loss 1.177692
[epoch2, step2110]: loss 1.182918
[epoch2, step2111]: loss 1.349992
[epoch2, step2112]: loss 0.828312
[epoch2, step2113]: loss 1.211920
[epoch2, step2114]: loss 0.896463
[epoch2, step2115]: loss 1.061368
[epoch2, step2116]: loss 1.101501
[epoch2, step2117]: loss 0.729655
[epoch2, step2118]: loss 1.401696
[epoch2, step2119]: loss 1.321226
[epoch2, step2120]: loss 1.451042
[epoch2, step2121]: loss 1.322366
[epoch2, step2122]: loss 0.775670
[epoch2, step2123]: loss 1.434160
[epoch2, step2124]: loss 0.988494
[epoch2, step2125]: loss 1.020490
[epoch2, step2126]: loss 1.272023
[epoch2, step2127]: loss 1.395641
[epoch2, step2128]: loss 1.200774
[epoch2, step2129]: loss 1.587901
[epoch2, step2130]: loss 0.978640
[epoch2, step2131]: loss 0.817796
[epoch2, step2132]: loss 1.170210
[epoch2, step2133]: loss 1.152794
[epoch2, step2134]: loss 1.109393
[epoch2, step2135]: loss 1.051646
[epoch2, step2136]: loss 1.118622
[epoch2, step2137]: loss 1.240890
[epoch2, step2138]: loss 1.242244
[epoch2, step2139]: loss 1.303720
[epoch2, step2140]: loss 1.329235
[epoch2, step2141]: loss 1.048143
[epoch2, step2142]: loss 1.238634
[epoch2, step2143]: loss 1.371614
[epoch2, step2144]: loss 0.637583
[epoch2, step2145]: loss 1.083411
[epoch2, step2146]: loss 1.268637
[epoch2, step2147]: loss 0.922020
[epoch2, step2148]: loss 1.251757
[epoch2, step2149]: loss 0.656026
[epoch2, step2150]: loss 1.487622
[epoch2, step2151]: loss 1.048626
[epoch2, step2152]: loss 0.932088
[epoch2, step2153]: loss 1.039040
[epoch2, step2154]: loss 0.981171
[epoch2, step2155]: loss 1.195136
[epoch2, step2156]: loss 1.337949
[epoch2, step2157]: loss 1.342757
[epoch2, step2158]: loss 0.965870
[epoch2, step2159]: loss 1.037031
[epoch2, step2160]: loss 1.063683
[epoch2, step2161]: loss 1.223149
[epoch2, step2162]: loss 1.010437
[epoch2, step2163]: loss 1.222034
[epoch2, step2164]: loss 1.227775
[epoch2, step2165]: loss 0.952185
[epoch2, step2166]: loss 1.230348
[epoch2, step2167]: loss 1.197119
[epoch2, step2168]: loss 1.224788
[epoch2, step2169]: loss 1.076440
[epoch2, step2170]: loss 1.019377
[epoch2, step2171]: loss 1.312085
[epoch2, step2172]: loss 1.193902
[epoch2, step2173]: loss 1.249131
[epoch2, step2174]: loss 1.308519
[epoch2, step2175]: loss 1.079110
[epoch2, step2176]: loss 1.142269
[epoch2, step2177]: loss 1.314438
[epoch2, step2178]: loss 1.073081
[epoch2, step2179]: loss 1.086156
[epoch2, step2180]: loss 1.312262
[epoch2, step2181]: loss 1.147116
[epoch2, step2182]: loss 1.223076
[epoch2, step2183]: loss 1.193381
[epoch2, step2184]: loss 1.299951
[epoch2, step2185]: loss 1.453581
[epoch2, step2186]: loss 1.300044
[epoch2, step2187]: loss 1.279601
[epoch2, step2188]: loss 1.245672
[epoch2, step2189]: loss 1.314397
[epoch2, step2190]: loss 1.289630
[epoch2, step2191]: loss 1.278092
[epoch2, step2192]: loss 1.202252
[epoch2, step2193]: loss 1.147907
[epoch2, step2194]: loss 1.224112
[epoch2, step2195]: loss 1.339495
[epoch2, step2196]: loss 1.280726
[epoch2, step2197]: loss 1.252599
[epoch2, step2198]: loss 1.214203
[epoch2, step2199]: loss 1.308044
[epoch2, step2200]: loss 1.110024
[epoch2, step2201]: loss 0.998265
[epoch2, step2202]: loss 1.136875
[epoch2, step2203]: loss 1.386261
[epoch2, step2204]: loss 1.241102
[epoch2, step2205]: loss 0.897092
[epoch2, step2206]: loss 1.247169
[epoch2, step2207]: loss 1.088438
[epoch2, step2208]: loss 1.312747
[epoch2, step2209]: loss 1.166072
[epoch2, step2210]: loss 1.417899
[epoch2, step2211]: loss 1.139790
[epoch2, step2212]: loss 1.039235
[epoch2, step2213]: loss 1.245609
[epoch2, step2214]: loss 1.060594
[epoch2, step2215]: loss 1.479156
[epoch2, step2216]: loss 0.963086
[epoch2, step2217]: loss 1.086625
[epoch2, step2218]: loss 1.017754
[epoch2, step2219]: loss 1.299038
[epoch2, step2220]: loss 1.267503
[epoch2, step2221]: loss 1.194875
[epoch2, step2222]: loss 0.919301
[epoch2, step2223]: loss 1.093145
[epoch2, step2224]: loss 1.110195
[epoch2, step2225]: loss 1.175406
[epoch2, step2226]: loss 0.587364
[epoch2, step2227]: loss 1.175642
[epoch2, step2228]: loss 0.744783
[epoch2, step2229]: loss 1.234185
[epoch2, step2230]: loss 1.317303
[epoch2, step2231]: loss 1.046651
[epoch2, step2232]: loss 0.960002
[epoch2, step2233]: loss 0.984277
[epoch2, step2234]: loss 1.255567
[epoch2, step2235]: loss 1.269121
[epoch2, step2236]: loss 1.052076
[epoch2, step2237]: loss 1.094493
[epoch2, step2238]: loss 1.056329
[epoch2, step2239]: loss 1.054495
[epoch2, step2240]: loss 1.384120
[epoch2, step2241]: loss 1.266458
[epoch2, step2242]: loss 1.111249
[epoch2, step2243]: loss 1.319368
[epoch2, step2244]: loss 1.031301
[epoch2, step2245]: loss 1.310382
[epoch2, step2246]: loss 0.971927
[epoch2, step2247]: loss 1.346190
[epoch2, step2248]: loss 1.037250
[epoch2, step2249]: loss 0.821810
[epoch2, step2250]: loss 0.918368
[epoch2, step2251]: loss 0.877360
[epoch2, step2252]: loss 1.314856
[epoch2, step2253]: loss 1.181187
[epoch2, step2254]: loss 1.066901
[epoch2, step2255]: loss 1.033838
[epoch2, step2256]: loss 0.921009
[epoch2, step2257]: loss 1.037147
[epoch2, step2258]: loss 1.266474
[epoch2, step2259]: loss 1.067697
[epoch2, step2260]: loss 1.387462
[epoch2, step2261]: loss 1.159091
[epoch2, step2262]: loss 1.175576
[epoch2, step2263]: loss 1.089205
[epoch2, step2264]: loss 1.111050
[epoch2, step2265]: loss 0.863508
[epoch2, step2266]: loss 1.097148
[epoch2, step2267]: loss 1.073900
[epoch2, step2268]: loss 1.318947
[epoch2, step2269]: loss 1.037607
[epoch2, step2270]: loss 0.991360
[epoch2, step2271]: loss 1.110922
[epoch2, step2272]: loss 1.041317
[epoch2, step2273]: loss 1.271855
[epoch2, step2274]: loss 1.238451
[epoch2, step2275]: loss 0.989416
[epoch2, step2276]: loss 0.988995
[epoch2, step2277]: loss 1.241031
[epoch2, step2278]: loss 0.881005
[epoch2, step2279]: loss 1.422809
[epoch2, step2280]: loss 0.911674
[epoch2, step2281]: loss 1.297355
[epoch2, step2282]: loss 1.380611
[epoch2, step2283]: loss 0.800109
[epoch2, step2284]: loss 1.275076
[epoch2, step2285]: loss 1.107275
[epoch2, step2286]: loss 1.252674
[epoch2, step2287]: loss 1.157066
[epoch2, step2288]: loss 1.045000
[epoch2, step2289]: loss 1.178028
[epoch2, step2290]: loss 0.889092
[epoch2, step2291]: loss 1.161742
[epoch2, step2292]: loss 1.041104
[epoch2, step2293]: loss 1.234320
[epoch2, step2294]: loss 1.043971
[epoch2, step2295]: loss 1.119174
[epoch2, step2296]: loss 1.204821
[epoch2, step2297]: loss 1.237069
[epoch2, step2298]: loss 1.291249
[epoch2, step2299]: loss 0.875175
[epoch2, step2300]: loss 1.106474
[epoch2, step2301]: loss 1.230242
[epoch2, step2302]: loss 0.987362
[epoch2, step2303]: loss 1.298906
[epoch2, step2304]: loss 0.860430
[epoch2, step2305]: loss 1.261353
[epoch2, step2306]: loss 1.300174
[epoch2, step2307]: loss 1.154258
[epoch2, step2308]: loss 1.299704
[epoch2, step2309]: loss 1.019019
[epoch2, step2310]: loss 1.089309
[epoch2, step2311]: loss 0.983063
[epoch2, step2312]: loss 1.164512
[epoch2, step2313]: loss 1.029295
[epoch2, step2314]: loss 1.155845
[epoch2, step2315]: loss 1.061029
[epoch2, step2316]: loss 1.141597
[epoch2, step2317]: loss 1.396283
[epoch2, step2318]: loss 1.285035
[epoch2, step2319]: loss 1.071622
[epoch2, step2320]: loss 1.050617
[epoch2, step2321]: loss 1.097807
[epoch2, step2322]: loss 1.060639
[epoch2, step2323]: loss 0.915587
[epoch2, step2324]: loss 1.143103
[epoch2, step2325]: loss 1.157163
[epoch2, step2326]: loss 1.415171
[epoch2, step2327]: loss 1.152894
[epoch2, step2328]: loss 1.076002
[epoch2, step2329]: loss 0.848897
[epoch2, step2330]: loss 1.223352
[epoch2, step2331]: loss 1.372911
[epoch2, step2332]: loss 1.169967
[epoch2, step2333]: loss 0.670649
[epoch2, step2334]: loss 1.057808
[epoch2, step2335]: loss 1.218767
[epoch2, step2336]: loss 1.054084
[epoch2, step2337]: loss 0.897055
[epoch2, step2338]: loss 1.020477
[epoch2, step2339]: loss 1.144318
[epoch2, step2340]: loss 1.191224
[epoch2, step2341]: loss 1.245380
[epoch2, step2342]: loss 1.202579
[epoch2, step2343]: loss 1.084002
[epoch2, step2344]: loss 1.348711
[epoch2, step2345]: loss 1.086315
[epoch2, step2346]: loss 1.203511
[epoch2, step2347]: loss 1.367213
[epoch2, step2348]: loss 1.419578
[epoch2, step2349]: loss 1.363209
[epoch2, step2350]: loss 1.009330
[epoch2, step2351]: loss 1.290622
[epoch2, step2352]: loss 0.972316
[epoch2, step2353]: loss 0.917547
[epoch2, step2354]: loss 1.119651
[epoch2, step2355]: loss 1.062486
[epoch2, step2356]: loss 1.060497
[epoch2, step2357]: loss 1.336463
[epoch2, step2358]: loss 1.201506
[epoch2, step2359]: loss 1.210310
[epoch2, step2360]: loss 0.960201
[epoch2, step2361]: loss 0.812392
[epoch2, step2362]: loss 1.090747
[epoch2, step2363]: loss 1.249501
[epoch2, step2364]: loss 0.997679
[epoch2, step2365]: loss 1.182580
[epoch2, step2366]: loss 1.146467
[epoch2, step2367]: loss 1.338756
[epoch2, step2368]: loss 1.133372
[epoch2, step2369]: loss 0.882478
[epoch2, step2370]: loss 0.973675
[epoch2, step2371]: loss 0.994962
[epoch2, step2372]: loss 1.126823
[epoch2, step2373]: loss 1.067015
[epoch2, step2374]: loss 1.056699
[epoch2, step2375]: loss 0.779147
[epoch2, step2376]: loss 1.245946
[epoch2, step2377]: loss 1.257207
[epoch2, step2378]: loss 1.257696
[epoch2, step2379]: loss 0.982703
[epoch2, step2380]: loss 0.995747
[epoch2, step2381]: loss 1.191590
[epoch2, step2382]: loss 1.145503
[epoch2, step2383]: loss 1.100439
[epoch2, step2384]: loss 1.061003
[epoch2, step2385]: loss 1.108399
[epoch2, step2386]: loss 1.254906
[epoch2, step2387]: loss 1.159094
[epoch2, step2388]: loss 1.267446
[epoch2, step2389]: loss 1.320289
[epoch2, step2390]: loss 1.079002
[epoch2, step2391]: loss 1.323211
[epoch2, step2392]: loss 1.152780
[epoch2, step2393]: loss 1.299376
[epoch2, step2394]: loss 0.951148
[epoch2, step2395]: loss 1.086454
[epoch2, step2396]: loss 1.174675
[epoch2, step2397]: loss 1.209143
[epoch2, step2398]: loss 1.071023
[epoch2, step2399]: loss 1.018517
[epoch2, step2400]: loss 1.281581
[epoch2, step2401]: loss 1.133881
[epoch2, step2402]: loss 1.050500
[epoch2, step2403]: loss 1.210024
[epoch2, step2404]: loss 1.186741
[epoch2, step2405]: loss 0.981518
[epoch2, step2406]: loss 1.396487
[epoch2, step2407]: loss 1.074817
[epoch2, step2408]: loss 1.044871
[epoch2, step2409]: loss 0.926308
[epoch2, step2410]: loss 1.216642
[epoch2, step2411]: loss 1.069044
[epoch2, step2412]: loss 1.023596
[epoch2, step2413]: loss 1.357387
[epoch2, step2414]: loss 1.106472
[epoch2, step2415]: loss 1.273046
[epoch2, step2416]: loss 1.072266
[epoch2, step2417]: loss 1.201725
[epoch2, step2418]: loss 0.699025
[epoch2, step2419]: loss 0.918578
[epoch2, step2420]: loss 1.286460
[epoch2, step2421]: loss 1.042567
[epoch2, step2422]: loss 1.017686
[epoch2, step2423]: loss 1.234935
[epoch2, step2424]: loss 1.367955
[epoch2, step2425]: loss 0.770384
[epoch2, step2426]: loss 1.053555
[epoch2, step2427]: loss 1.384040
[epoch2, step2428]: loss 0.800689
[epoch2, step2429]: loss 1.200248
[epoch2, step2430]: loss 1.328871
[epoch2, step2431]: loss 1.396164
[epoch2, step2432]: loss 1.211565
[epoch2, step2433]: loss 1.451480
[epoch2, step2434]: loss 1.230860
[epoch2, step2435]: loss 1.235870
[epoch2, step2436]: loss 1.162937
[epoch2, step2437]: loss 0.890931
[epoch2, step2438]: loss 1.346982
[epoch2, step2439]: loss 0.911771
[epoch2, step2440]: loss 1.195987
[epoch2, step2441]: loss 1.081896
[epoch2, step2442]: loss 1.199604
[epoch2, step2443]: loss 0.970490
[epoch2, step2444]: loss 1.030656
[epoch2, step2445]: loss 1.164078
[epoch2, step2446]: loss 1.012083
[epoch2, step2447]: loss 1.055399
[epoch2, step2448]: loss 1.008179
[epoch2, step2449]: loss 0.912672
[epoch2, step2450]: loss 1.009235
[epoch2, step2451]: loss 1.294249
[epoch2, step2452]: loss 0.850173
[epoch2, step2453]: loss 0.955494
[epoch2, step2454]: loss 1.180200
[epoch2, step2455]: loss 1.396568
[epoch2, step2456]: loss 1.128782
[epoch2, step2457]: loss 1.298603
[epoch2, step2458]: loss 1.194510
[epoch2, step2459]: loss 1.263430
[epoch2, step2460]: loss 0.712384
[epoch2, step2461]: loss 0.862420
[epoch2, step2462]: loss 1.185503
[epoch2, step2463]: loss 1.078505
[epoch2, step2464]: loss 1.208444
[epoch2, step2465]: loss 1.119958
[epoch2, step2466]: loss 1.137556
[epoch2, step2467]: loss 0.895127
[epoch2, step2468]: loss 1.210457
[epoch2, step2469]: loss 1.157826
[epoch2, step2470]: loss 0.873573
[epoch2, step2471]: loss 1.269297
[epoch2, step2472]: loss 1.211545
[epoch2, step2473]: loss 1.121501
[epoch2, step2474]: loss 1.242033
[epoch2, step2475]: loss 1.111180
[epoch2, step2476]: loss 1.170290
[epoch2, step2477]: loss 1.100543
[epoch2, step2478]: loss 1.104630
[epoch2, step2479]: loss 0.985954
[epoch2, step2480]: loss 1.068081
[epoch2, step2481]: loss 1.158266
[epoch2, step2482]: loss 1.250042
[epoch2, step2483]: loss 0.881380
[epoch2, step2484]: loss 1.029724
[epoch2, step2485]: loss 0.910832
[epoch2, step2486]: loss 1.129709
[epoch2, step2487]: loss 1.045561
[epoch2, step2488]: loss 1.046413
[epoch2, step2489]: loss 1.221643
[epoch2, step2490]: loss 1.176838
[epoch2, step2491]: loss 1.105374
[epoch2, step2492]: loss 1.271947
[epoch2, step2493]: loss 1.036225
[epoch2, step2494]: loss 1.152844
[epoch2, step2495]: loss 1.347014
[epoch2, step2496]: loss 1.102091
[epoch2, step2497]: loss 1.073826
[epoch2, step2498]: loss 1.085539
[epoch2, step2499]: loss 1.079107
[epoch2, step2500]: loss 1.192444
[epoch2, step2501]: loss 1.287147
[epoch2, step2502]: loss 0.833208
[epoch2, step2503]: loss 0.675758
[epoch2, step2504]: loss 1.253000
[epoch2, step2505]: loss 1.059082
[epoch2, step2506]: loss 1.223951
[epoch2, step2507]: loss 1.391274
[epoch2, step2508]: loss 1.042640
[epoch2, step2509]: loss 0.867847
[epoch2, step2510]: loss 1.193417
[epoch2, step2511]: loss 0.979824
[epoch2, step2512]: loss 1.437370
[epoch2, step2513]: loss 1.110767
[epoch2, step2514]: loss 0.853507
[epoch2, step2515]: loss 1.166468
[epoch2, step2516]: loss 1.036538
[epoch2, step2517]: loss 1.202222
[epoch2, step2518]: loss 1.096919
[epoch2, step2519]: loss 1.195388
[epoch2, step2520]: loss 1.154264
[epoch2, step2521]: loss 0.974241
[epoch2, step2522]: loss 0.997398
[epoch2, step2523]: loss 1.081659
[epoch2, step2524]: loss 1.153844
[epoch2, step2525]: loss 1.170403
[epoch2, step2526]: loss 0.876718
[epoch2, step2527]: loss 1.049790
[epoch2, step2528]: loss 1.090284
[epoch2, step2529]: loss 1.078965
[epoch2, step2530]: loss 1.328797
[epoch2, step2531]: loss 0.880004
[epoch2, step2532]: loss 1.292704
[epoch2, step2533]: loss 1.334524
[epoch2, step2534]: loss 1.323116
[epoch2, step2535]: loss 0.856187
[epoch2, step2536]: loss 1.228174
[epoch2, step2537]: loss 0.794399
[epoch2, step2538]: loss 0.978704
[epoch2, step2539]: loss 1.484543
[epoch2, step2540]: loss 1.197133
[epoch2, step2541]: loss 1.246187
[epoch2, step2542]: loss 1.018443
[epoch2, step2543]: loss 1.148866
[epoch2, step2544]: loss 1.068619
[epoch2, step2545]: loss 0.955248
[epoch2, step2546]: loss 1.179186
[epoch2, step2547]: loss 1.383729
[epoch2, step2548]: loss 1.078077
[epoch2, step2549]: loss 1.183546
[epoch2, step2550]: loss 1.329785
[epoch2, step2551]: loss 1.212311
[epoch2, step2552]: loss 1.255735
[epoch2, step2553]: loss 0.626117
[epoch2, step2554]: loss 0.925660
[epoch2, step2555]: loss 1.041848
[epoch2, step2556]: loss 0.744694
[epoch2, step2557]: loss 0.618779
[epoch2, step2558]: loss 1.249946
[epoch2, step2559]: loss 1.385746
[epoch2, step2560]: loss 1.309765
[epoch2, step2561]: loss 1.322553
[epoch2, step2562]: loss 1.005452
[epoch2, step2563]: loss 0.769883
[epoch2, step2564]: loss 1.164819
[epoch2, step2565]: loss 1.137630
[epoch2, step2566]: loss 1.189842
[epoch2, step2567]: loss 1.452330
[epoch2, step2568]: loss 0.993721
[epoch2, step2569]: loss 0.846750
[epoch2, step2570]: loss 1.224959
[epoch2, step2571]: loss 1.201314
[epoch2, step2572]: loss 1.139104
[epoch2, step2573]: loss 1.160182
[epoch2, step2574]: loss 0.962928
[epoch2, step2575]: loss 1.397638
[epoch2, step2576]: loss 1.116639
[epoch2, step2577]: loss 0.909126
[epoch2, step2578]: loss 1.109593
[epoch2, step2579]: loss 1.222192
[epoch2, step2580]: loss 0.745939
[epoch2, step2581]: loss 1.028386
[epoch2, step2582]: loss 0.789748
[epoch2, step2583]: loss 1.046860
[epoch2, step2584]: loss 1.387801
[epoch2, step2585]: loss 1.262120
[epoch2, step2586]: loss 1.058244
[epoch2, step2587]: loss 0.903665
[epoch2, step2588]: loss 1.192884
[epoch2, step2589]: loss 0.905406
[epoch2, step2590]: loss 1.323918
[epoch2, step2591]: loss 0.838555
[epoch2, step2592]: loss 1.398088
[epoch2, step2593]: loss 1.053382
[epoch2, step2594]: loss 0.833549
[epoch2, step2595]: loss 1.323615
[epoch2, step2596]: loss 0.927319
[epoch2, step2597]: loss 1.165371
[epoch2, step2598]: loss 1.300124
[epoch2, step2599]: loss 1.029489
[epoch2, step2600]: loss 0.769166
[epoch2, step2601]: loss 1.095573
[epoch2, step2602]: loss 0.923798
[epoch2, step2603]: loss 1.274980
[epoch2, step2604]: loss 1.125878
[epoch2, step2605]: loss 1.156449
[epoch2, step2606]: loss 1.070219
[epoch2, step2607]: loss 1.012050
[epoch2, step2608]: loss 1.168866
[epoch2, step2609]: loss 1.153474
[epoch2, step2610]: loss 0.960581
[epoch2, step2611]: loss 1.166798
[epoch2, step2612]: loss 0.946291
[epoch2, step2613]: loss 1.298492
[epoch2, step2614]: loss 1.272508
[epoch2, step2615]: loss 1.020578
[epoch2, step2616]: loss 0.964550
[epoch2, step2617]: loss 0.960974
[epoch2, step2618]: loss 1.075470
[epoch2, step2619]: loss 1.005352
[epoch2, step2620]: loss 1.261900
[epoch2, step2621]: loss 0.875035
[epoch2, step2622]: loss 1.203342
[epoch2, step2623]: loss 1.107634
[epoch2, step2624]: loss 1.260281
[epoch2, step2625]: loss 0.932858
[epoch2, step2626]: loss 1.222283
[epoch2, step2627]: loss 1.163307
[epoch2, step2628]: loss 1.144899
[epoch2, step2629]: loss 1.155596
[epoch2, step2630]: loss 1.267428
[epoch2, step2631]: loss 1.136621
[epoch2, step2632]: loss 1.186505
[epoch2, step2633]: loss 1.284789
[epoch2, step2634]: loss 1.198238
[epoch2, step2635]: loss 1.081440
[epoch2, step2636]: loss 1.236084
[epoch2, step2637]: loss 0.881956
[epoch2, step2638]: loss 1.017363
[epoch2, step2639]: loss 1.091345
[epoch2, step2640]: loss 0.750695
[epoch2, step2641]: loss 0.931738
[epoch2, step2642]: loss 1.184868
[epoch2, step2643]: loss 0.986657
[epoch2, step2644]: loss 1.164620
[epoch2, step2645]: loss 1.154272
[epoch2, step2646]: loss 1.043414
[epoch2, step2647]: loss 0.788040
[epoch2, step2648]: loss 1.056323
[epoch2, step2649]: loss 0.984469
[epoch2, step2650]: loss 1.003882
[epoch2, step2651]: loss 1.385204
[epoch2, step2652]: loss 1.179514
[epoch2, step2653]: loss 1.087888
[epoch2, step2654]: loss 0.886181
[epoch2, step2655]: loss 0.942688
[epoch2, step2656]: loss 0.895968
[epoch2, step2657]: loss 1.073034
[epoch2, step2658]: loss 1.146775
[epoch2, step2659]: loss 1.158073
[epoch2, step2660]: loss 1.257897
[epoch2, step2661]: loss 1.154686
[epoch2, step2662]: loss 1.328130
[epoch2, step2663]: loss 1.276438
[epoch2, step2664]: loss 0.990055
[epoch2, step2665]: loss 1.359226
[epoch2, step2666]: loss 1.130345
[epoch2, step2667]: loss 1.073467
[epoch2, step2668]: loss 1.167599
[epoch2, step2669]: loss 0.697110
[epoch2, step2670]: loss 0.890459
[epoch2, step2671]: loss 1.343716
[epoch2, step2672]: loss 0.880509
[epoch2, step2673]: loss 1.225179
[epoch2, step2674]: loss 1.285566
[epoch2, step2675]: loss 0.985036
[epoch2, step2676]: loss 1.311557
[epoch2, step2677]: loss 0.982487
[epoch2, step2678]: loss 0.892332
[epoch2, step2679]: loss 1.275072
[epoch2, step2680]: loss 1.264538
[epoch2, step2681]: loss 1.222778
[epoch2, step2682]: loss 1.356318
[epoch2, step2683]: loss 0.829342
[epoch2, step2684]: loss 0.804724
[epoch2, step2685]: loss 1.127529
[epoch2, step2686]: loss 1.067049
[epoch2, step2687]: loss 0.983982
[epoch2, step2688]: loss 0.921514
[epoch2, step2689]: loss 1.267595
[epoch2, step2690]: loss 1.007931
[epoch2, step2691]: loss 0.697309
[epoch2, step2692]: loss 0.884300
[epoch2, step2693]: loss 1.044776
[epoch2, step2694]: loss 1.056309
[epoch2, step2695]: loss 1.146759
[epoch2, step2696]: loss 1.249422
[epoch2, step2697]: loss 0.876696
[epoch2, step2698]: loss 0.871448
[epoch2, step2699]: loss 1.223830
[epoch2, step2700]: loss 1.334197
[epoch2, step2701]: loss 1.043789
[epoch2, step2702]: loss 0.880981
[epoch2, step2703]: loss 0.726299
[epoch2, step2704]: loss 1.064692
[epoch2, step2705]: loss 1.047553
[epoch2, step2706]: loss 1.038623
[epoch2, step2707]: loss 0.908057
[epoch2, step2708]: loss 1.130662
[epoch2, step2709]: loss 1.207207
[epoch2, step2710]: loss 1.258895
[epoch2, step2711]: loss 1.274937
[epoch2, step2712]: loss 0.922079
[epoch2, step2713]: loss 1.083054
[epoch2, step2714]: loss 1.027509
[epoch2, step2715]: loss 1.154181
[epoch2, step2716]: loss 0.899182
[epoch2, step2717]: loss 1.177474
[epoch2, step2718]: loss 1.230289
[epoch2, step2719]: loss 0.966146
[epoch2, step2720]: loss 1.320800
[epoch2, step2721]: loss 1.211174
[epoch2, step2722]: loss 1.281456
[epoch2, step2723]: loss 0.773780
[epoch2, step2724]: loss 1.034610
[epoch2, step2725]: loss 1.228930
[epoch2, step2726]: loss 1.095590
[epoch2, step2727]: loss 0.870609
[epoch2, step2728]: loss 1.064417
[epoch2, step2729]: loss 1.233580
[epoch2, step2730]: loss 1.136618
[epoch2, step2731]: loss 1.222837
[epoch2, step2732]: loss 1.244704
[epoch2, step2733]: loss 1.227159
[epoch2, step2734]: loss 1.218144
[epoch2, step2735]: loss 1.331914
[epoch2, step2736]: loss 0.959032
[epoch2, step2737]: loss 1.117033
[epoch2, step2738]: loss 1.284991
[epoch2, step2739]: loss 1.249123
[epoch2, step2740]: loss 0.698617
[epoch2, step2741]: loss 0.971595
[epoch2, step2742]: loss 0.922949
[epoch2, step2743]: loss 1.268032
[epoch2, step2744]: loss 0.897282
[epoch2, step2745]: loss 0.968426
[epoch2, step2746]: loss 1.355693
[epoch2, step2747]: loss 0.977202
[epoch2, step2748]: loss 1.121177
[epoch2, step2749]: loss 1.249641
[epoch2, step2750]: loss 1.020840
[epoch2, step2751]: loss 1.107942
[epoch2, step2752]: loss 1.175814
[epoch2, step2753]: loss 1.036979
[epoch2, step2754]: loss 1.172692
[epoch2, step2755]: loss 1.252196
[epoch2, step2756]: loss 0.810402
[epoch2, step2757]: loss 1.112824
[epoch2, step2758]: loss 0.798095
[epoch2, step2759]: loss 1.155726
[epoch2, step2760]: loss 1.278485
[epoch2, step2761]: loss 1.267202
[epoch2, step2762]: loss 1.057997
[epoch2, step2763]: loss 1.198471
[epoch2, step2764]: loss 1.270695
[epoch2, step2765]: loss 1.138189
[epoch2, step2766]: loss 1.275763
[epoch2, step2767]: loss 0.703560
[epoch2, step2768]: loss 0.901841
[epoch2, step2769]: loss 0.770698
[epoch2, step2770]: loss 1.147734
[epoch2, step2771]: loss 1.177843
[epoch2, step2772]: loss 1.015340
[epoch2, step2773]: loss 1.301991
[epoch2, step2774]: loss 0.950204
[epoch2, step2775]: loss 1.376572
[epoch2, step2776]: loss 1.212810
[epoch2, step2777]: loss 1.300979
[epoch2, step2778]: loss 1.158455
[epoch2, step2779]: loss 1.120655
[epoch2, step2780]: loss 1.140084
[epoch2, step2781]: loss 1.338210
[epoch2, step2782]: loss 1.266006
[epoch2, step2783]: loss 1.086152
[epoch2, step2784]: loss 0.994373
[epoch2, step2785]: loss 1.272063
[epoch2, step2786]: loss 1.007039
[epoch2, step2787]: loss 0.869518
[epoch2, step2788]: loss 0.891025
[epoch2, step2789]: loss 1.329654
[epoch2, step2790]: loss 0.776133
[epoch2, step2791]: loss 1.171435
[epoch2, step2792]: loss 1.019117
[epoch2, step2793]: loss 1.274508
[epoch2, step2794]: loss 1.126913
[epoch2, step2795]: loss 1.282288
[epoch2, step2796]: loss 1.182358
[epoch2, step2797]: loss 0.944507
[epoch2, step2798]: loss 1.068277
[epoch2, step2799]: loss 1.028183
[epoch2, step2800]: loss 0.918651
[epoch2, step2801]: loss 0.693363
[epoch2, step2802]: loss 1.087261
[epoch2, step2803]: loss 1.243773
[epoch2, step2804]: loss 1.237307
[epoch2, step2805]: loss 0.943126
[epoch2, step2806]: loss 0.836675
[epoch2, step2807]: loss 1.252907
[epoch2, step2808]: loss 1.287441
[epoch2, step2809]: loss 1.213241
[epoch2, step2810]: loss 1.058566
[epoch2, step2811]: loss 1.172396
[epoch2, step2812]: loss 1.294937
[epoch2, step2813]: loss 1.286726
[epoch2, step2814]: loss 1.032729
[epoch2, step2815]: loss 0.955015
[epoch2, step2816]: loss 1.082641
[epoch2, step2817]: loss 1.312932
[epoch2, step2818]: loss 1.052816
[epoch2, step2819]: loss 1.173659
[epoch2, step2820]: loss 1.074381
[epoch2, step2821]: loss 1.007664
[epoch2, step2822]: loss 1.045437
[epoch2, step2823]: loss 0.879937
[epoch2, step2824]: loss 1.286811
[epoch2, step2825]: loss 0.909809
[epoch2, step2826]: loss 0.906992
[epoch2, step2827]: loss 0.812922
[epoch2, step2828]: loss 0.700121
[epoch2, step2829]: loss 0.921091
[epoch2, step2830]: loss 1.251344
[epoch2, step2831]: loss 1.056338
[epoch2, step2832]: loss 1.043779
[epoch2, step2833]: loss 1.065245
[epoch2, step2834]: loss 1.077981
[epoch2, step2835]: loss 1.295979
[epoch2, step2836]: loss 1.179932
[epoch2, step2837]: loss 1.029591
[epoch2, step2838]: loss 1.056624
[epoch2, step2839]: loss 0.801485
[epoch2, step2840]: loss 0.917355
[epoch2, step2841]: loss 1.048885
[epoch2, step2842]: loss 1.280126
[epoch2, step2843]: loss 1.085754
[epoch2, step2844]: loss 1.173242
[epoch2, step2845]: loss 0.852254
[epoch2, step2846]: loss 0.973980
[epoch2, step2847]: loss 0.962497
[epoch2, step2848]: loss 1.168817
[epoch2, step2849]: loss 0.959564
[epoch2, step2850]: loss 0.676297
[epoch2, step2851]: loss 1.224889
[epoch2, step2852]: loss 1.120953
[epoch2, step2853]: loss 1.133386
[epoch2, step2854]: loss 1.223686
[epoch2, step2855]: loss 0.725071
[epoch2, step2856]: loss 0.658994
[epoch2, step2857]: loss 1.379233
[epoch2, step2858]: loss 1.116543
[epoch2, step2859]: loss 0.721586
[epoch2, step2860]: loss 1.111613
[epoch2, step2861]: loss 1.112747
[epoch2, step2862]: loss 0.805280
[epoch2, step2863]: loss 1.021766
[epoch2, step2864]: loss 1.066552
[epoch2, step2865]: loss 1.042164
[epoch2, step2866]: loss 1.068648
[epoch2, step2867]: loss 0.804285
[epoch2, step2868]: loss 0.839070
[epoch2, step2869]: loss 0.870971
[epoch2, step2870]: loss 0.973420
[epoch2, step2871]: loss 1.145059
[epoch2, step2872]: loss 1.180091
[epoch2, step2873]: loss 0.978605
[epoch2, step2874]: loss 1.132803
[epoch2, step2875]: loss 0.987477
[epoch2, step2876]: loss 0.937305
[epoch2, step2877]: loss 1.034518
[epoch2, step2878]: loss 0.982891
[epoch2, step2879]: loss 1.183463
[epoch2, step2880]: loss 1.265659
[epoch2, step2881]: loss 1.005339
[epoch2, step2882]: loss 1.228595
[epoch2, step2883]: loss 0.918604
[epoch2, step2884]: loss 1.041718
[epoch2, step2885]: loss 0.989692
[epoch2, step2886]: loss 0.999468
[epoch2, step2887]: loss 1.286447
[epoch2, step2888]: loss 1.276340
[epoch2, step2889]: loss 0.970869
[epoch2, step2890]: loss 1.180226
[epoch2, step2891]: loss 1.180035
[epoch2, step2892]: loss 1.116507
[epoch2, step2893]: loss 0.721682
[epoch2, step2894]: loss 1.095493
[epoch2, step2895]: loss 1.142298
[epoch2, step2896]: loss 1.130992
[epoch2, step2897]: loss 1.123278
[epoch2, step2898]: loss 1.227640
[epoch2, step2899]: loss 1.128870
[epoch2, step2900]: loss 0.932853
[epoch2, step2901]: loss 0.848330
[epoch2, step2902]: loss 1.074217
[epoch2, step2903]: loss 1.321534
[epoch2, step2904]: loss 1.079561
[epoch2, step2905]: loss 1.174123
[epoch2, step2906]: loss 1.337429
[epoch2, step2907]: loss 1.116233
[epoch2, step2908]: loss 1.060393
[epoch2, step2909]: loss 1.086416
[epoch2, step2910]: loss 1.030496
[epoch2, step2911]: loss 1.107527
[epoch2, step2912]: loss 1.188233
[epoch2, step2913]: loss 1.073259
[epoch2, step2914]: loss 0.918137
[epoch2, step2915]: loss 1.224749
[epoch2, step2916]: loss 1.369711
[epoch2, step2917]: loss 0.920837
[epoch2, step2918]: loss 0.983362
[epoch2, step2919]: loss 1.130680
[epoch2, step2920]: loss 0.712046
[epoch2, step2921]: loss 0.789819
[epoch2, step2922]: loss 1.127256
[epoch2, step2923]: loss 1.172353
[epoch2, step2924]: loss 0.922011
[epoch2, step2925]: loss 1.150599
[epoch2, step2926]: loss 0.938741
[epoch2, step2927]: loss 0.901626
[epoch2, step2928]: loss 1.180738
[epoch2, step2929]: loss 1.132446
[epoch2, step2930]: loss 1.157916
[epoch2, step2931]: loss 1.135800
[epoch2, step2932]: loss 1.047971
[epoch2, step2933]: loss 0.861143
[epoch2, step2934]: loss 1.141846
[epoch2, step2935]: loss 1.143441
[epoch2, step2936]: loss 0.839446
[epoch2, step2937]: loss 0.900523
[epoch2, step2938]: loss 1.308403
[epoch2, step2939]: loss 0.918894
[epoch2, step2940]: loss 1.201278
[epoch2, step2941]: loss 1.009537
[epoch2, step2942]: loss 0.804041
[epoch2, step2943]: loss 0.875791
[epoch2, step2944]: loss 1.111672
[epoch2, step2945]: loss 0.673946
[epoch2, step2946]: loss 1.120542
[epoch2, step2947]: loss 1.161613
[epoch2, step2948]: loss 1.082346
[epoch2, step2949]: loss 1.215125
[epoch2, step2950]: loss 1.216980
[epoch2, step2951]: loss 0.948237
[epoch2, step2952]: loss 0.886997
[epoch2, step2953]: loss 1.200840
[epoch2, step2954]: loss 0.976628
[epoch2, step2955]: loss 1.038341
[epoch2, step2956]: loss 0.863428
[epoch2, step2957]: loss 1.158683
[epoch2, step2958]: loss 0.967561
[epoch2, step2959]: loss 0.648296
[epoch2, step2960]: loss 0.852183
[epoch2, step2961]: loss 1.125211
[epoch2, step2962]: loss 0.973053
[epoch2, step2963]: loss 1.133771
[epoch2, step2964]: loss 1.066970
[epoch2, step2965]: loss 1.152505
[epoch2, step2966]: loss 1.168209
[epoch2, step2967]: loss 1.085861
[epoch2, step2968]: loss 1.211594
[epoch2, step2969]: loss 1.011537
[epoch2, step2970]: loss 1.060314
[epoch2, step2971]: loss 1.394686
[epoch2, step2972]: loss 1.200655
[epoch2, step2973]: loss 0.588028
[epoch2, step2974]: loss 1.255286
[epoch2, step2975]: loss 1.397982
[epoch2, step2976]: loss 0.942711
[epoch2, step2977]: loss 0.929050
[epoch2, step2978]: loss 1.355648
[epoch2, step2979]: loss 1.014222
[epoch2, step2980]: loss 1.110679
[epoch2, step2981]: loss 1.157685
[epoch2, step2982]: loss 1.241715
[epoch2, step2983]: loss 0.954729
[epoch2, step2984]: loss 0.830078
[epoch2, step2985]: loss 1.170049
[epoch2, step2986]: loss 1.125663
[epoch2, step2987]: loss 0.873528
[epoch2, step2988]: loss 0.891916
[epoch2, step2989]: loss 1.002200
[epoch2, step2990]: loss 0.998284
[epoch2, step2991]: loss 1.130715
[epoch2, step2992]: loss 0.998540
[epoch2, step2993]: loss 1.355088
[epoch2, step2994]: loss 0.993518
[epoch2, step2995]: loss 1.035352
[epoch2, step2996]: loss 0.791083
[epoch2, step2997]: loss 0.862326
[epoch2, step2998]: loss 1.099044
[epoch2, step2999]: loss 0.973821
[epoch2, step3000]: loss 1.074319
[epoch2, step3001]: loss 1.044302
[epoch2, step3002]: loss 1.228500
[epoch2, step3003]: loss 0.963552
[epoch2, step3004]: loss 0.733522
[epoch2, step3005]: loss 1.047464
[epoch2, step3006]: loss 1.018893
[epoch2, step3007]: loss 0.829976
[epoch2, step3008]: loss 1.351228
[epoch2, step3009]: loss 0.884573
[epoch2, step3010]: loss 1.235312
[epoch2, step3011]: loss 1.298171
[epoch2, step3012]: loss 1.168792
[epoch2, step3013]: loss 1.153879
[epoch2, step3014]: loss 1.054670
[epoch2, step3015]: loss 1.138017
[epoch2, step3016]: loss 1.207975
[epoch2, step3017]: loss 1.170048
[epoch2, step3018]: loss 1.215661
[epoch2, step3019]: loss 0.852271
[epoch2, step3020]: loss 1.014434
[epoch2, step3021]: loss 1.031372
[epoch2, step3022]: loss 0.807015
[epoch2, step3023]: loss 0.967966
[epoch2, step3024]: loss 1.039076
[epoch2, step3025]: loss 1.074868
[epoch2, step3026]: loss 0.704159
[epoch2, step3027]: loss 1.243297
[epoch2, step3028]: loss 1.064861
[epoch2, step3029]: loss 1.065285
[epoch2, step3030]: loss 0.541253
[epoch2, step3031]: loss 1.039781
[epoch2, step3032]: loss 1.052645
[epoch2, step3033]: loss 0.979632
[epoch2, step3034]: loss 1.113928
[epoch2, step3035]: loss 0.673074
[epoch2, step3036]: loss 1.184525
[epoch2, step3037]: loss 1.028380
[epoch2, step3038]: loss 0.995329
[epoch2, step3039]: loss 1.098340
[epoch2, step3040]: loss 1.106516
[epoch2, step3041]: loss 1.009269
[epoch2, step3042]: loss 0.880807
[epoch2, step3043]: loss 0.669080
[epoch2, step3044]: loss 1.381790
[epoch2, step3045]: loss 1.001388
[epoch2, step3046]: loss 0.890519
[epoch2, step3047]: loss 1.177596
[epoch2, step3048]: loss 0.897167
[epoch2, step3049]: loss 1.112222
[epoch2, step3050]: loss 1.065019
[epoch2, step3051]: loss 1.054746
[epoch2, step3052]: loss 1.120664
[epoch2, step3053]: loss 0.549999
[epoch2, step3054]: loss 0.971780
[epoch2, step3055]: loss 1.334219
[epoch2, step3056]: loss 1.055884
[epoch2, step3057]: loss 1.312089
[epoch2, step3058]: loss 1.108056
[epoch2, step3059]: loss 0.992402
[epoch2, step3060]: loss 1.119478
[epoch2, step3061]: loss 0.691529
[epoch2, step3062]: loss 0.970482
[epoch2, step3063]: loss 0.832514
[epoch2, step3064]: loss 1.079669
[epoch2, step3065]: loss 1.030613
[epoch2, step3066]: loss 1.222927
[epoch2, step3067]: loss 1.097454
[epoch2, step3068]: loss 1.187477
[epoch2, step3069]: loss 0.810449
[epoch2, step3070]: loss 0.617954
[epoch2, step3071]: loss 0.851142
[epoch2, step3072]: loss 0.828664
[epoch2, step3073]: loss 0.811414
[epoch2, step3074]: loss 0.962425
[epoch2, step3075]: loss 1.043655
[epoch2, step3076]: loss 0.802617

[epoch2]: avg loss 0.802617

[epoch3, step1]: loss 0.769207
[epoch3, step2]: loss 0.817218
[epoch3, step3]: loss 0.965672
[epoch3, step4]: loss 0.791250
[epoch3, step5]: loss 1.056079
[epoch3, step6]: loss 1.204538
[epoch3, step7]: loss 0.969094
[epoch3, step8]: loss 1.169354
[epoch3, step9]: loss 1.144036
[epoch3, step10]: loss 1.001795
[epoch3, step11]: loss 1.004534
[epoch3, step12]: loss 1.186828
[epoch3, step13]: loss 0.916342
[epoch3, step14]: loss 0.837334
[epoch3, step15]: loss 1.206538
[epoch3, step16]: loss 0.821955
[epoch3, step17]: loss 0.903860
[epoch3, step18]: loss 0.996261
[epoch3, step19]: loss 0.980280
[epoch3, step20]: loss 1.134685
[epoch3, step21]: loss 1.067173
[epoch3, step22]: loss 0.795395
[epoch3, step23]: loss 0.939772
[epoch3, step24]: loss 1.070720
[epoch3, step25]: loss 0.750831
[epoch3, step26]: loss 0.970418
[epoch3, step27]: loss 1.282833
[epoch3, step28]: loss 1.294704
[epoch3, step29]: loss 1.213892
[epoch3, step30]: loss 0.862654
[epoch3, step31]: loss 1.041960
[epoch3, step32]: loss 0.824373
[epoch3, step33]: loss 1.135641
[epoch3, step34]: loss 0.786513
[epoch3, step35]: loss 1.245545
[epoch3, step36]: loss 1.127482
[epoch3, step37]: loss 0.584577
[epoch3, step38]: loss 0.960977
[epoch3, step39]: loss 1.151529
[epoch3, step40]: loss 0.989459
[epoch3, step41]: loss 1.141526
[epoch3, step42]: loss 0.875792
[epoch3, step43]: loss 1.177444
[epoch3, step44]: loss 1.362953
[epoch3, step45]: loss 0.745435
[epoch3, step46]: loss 1.230343
[epoch3, step47]: loss 0.751232
[epoch3, step48]: loss 1.219119
[epoch3, step49]: loss 0.865608
[epoch3, step50]: loss 1.094486
[epoch3, step51]: loss 1.220350
[epoch3, step52]: loss 1.170462
[epoch3, step53]: loss 1.237134
[epoch3, step54]: loss 1.013784
[epoch3, step55]: loss 1.190135
[epoch3, step56]: loss 1.120766
[epoch3, step57]: loss 1.215804
[epoch3, step58]: loss 0.956477
[epoch3, step59]: loss 1.131974
[epoch3, step60]: loss 0.795658
[epoch3, step61]: loss 0.969815
[epoch3, step62]: loss 1.100918
[epoch3, step63]: loss 1.208730
[epoch3, step64]: loss 1.228078
[epoch3, step65]: loss 1.290728
[epoch3, step66]: loss 1.291260
[epoch3, step67]: loss 0.813482
[epoch3, step68]: loss 1.162051
[epoch3, step69]: loss 1.292701
[epoch3, step70]: loss 1.282826
[epoch3, step71]: loss 1.087027
[epoch3, step72]: loss 0.825846
[epoch3, step73]: loss 0.726453
[epoch3, step74]: loss 0.821185
[epoch3, step75]: loss 1.177987
[epoch3, step76]: loss 0.897022
[epoch3, step77]: loss 1.281743
[epoch3, step78]: loss 1.107500
[epoch3, step79]: loss 0.841651
[epoch3, step80]: loss 1.293415
[epoch3, step81]: loss 1.014107
[epoch3, step82]: loss 1.128011
[epoch3, step83]: loss 1.110740
[epoch3, step84]: loss 0.909552
[epoch3, step85]: loss 0.827717
[epoch3, step86]: loss 1.081068
[epoch3, step87]: loss 1.195467
[epoch3, step88]: loss 1.110652
[epoch3, step89]: loss 1.001594
[epoch3, step90]: loss 1.225194
[epoch3, step91]: loss 0.890118
[epoch3, step92]: loss 1.071764
[epoch3, step93]: loss 1.195872
[epoch3, step94]: loss 0.887947
[epoch3, step95]: loss 0.815046
[epoch3, step96]: loss 1.074278
[epoch3, step97]: loss 1.255071
[epoch3, step98]: loss 0.994060
[epoch3, step99]: loss 1.030742
[epoch3, step100]: loss 0.979271
[epoch3, step101]: loss 1.162786
[epoch3, step102]: loss 1.125554
[epoch3, step103]: loss 1.209214
[epoch3, step104]: loss 1.014781
[epoch3, step105]: loss 1.090791
[epoch3, step106]: loss 0.975076
[epoch3, step107]: loss 1.067336
[epoch3, step108]: loss 0.996284
[epoch3, step109]: loss 1.005543
[epoch3, step110]: loss 1.193599
[epoch3, step111]: loss 1.229862
[epoch3, step112]: loss 0.876983
[epoch3, step113]: loss 1.240421
[epoch3, step114]: loss 0.920646
[epoch3, step115]: loss 0.818822
[epoch3, step116]: loss 1.004215
[epoch3, step117]: loss 0.941057
[epoch3, step118]: loss 1.141939
[epoch3, step119]: loss 0.908244
[epoch3, step120]: loss 0.936057
[epoch3, step121]: loss 0.636219
[epoch3, step122]: loss 0.890133
[epoch3, step123]: loss 1.126598
[epoch3, step124]: loss 1.175652
[epoch3, step125]: loss 1.010370
[epoch3, step126]: loss 0.474608
[epoch3, step127]: loss 0.909956
[epoch3, step128]: loss 1.069271
[epoch3, step129]: loss 0.850110
[epoch3, step130]: loss 0.991553
[epoch3, step131]: loss 0.963699
[epoch3, step132]: loss 0.772050
[epoch3, step133]: loss 0.986574
[epoch3, step134]: loss 0.945993
[epoch3, step135]: loss 0.980293
[epoch3, step136]: loss 0.960385
[epoch3, step137]: loss 0.996895
[epoch3, step138]: loss 0.898629
[epoch3, step139]: loss 0.907607
[epoch3, step140]: loss 0.945128
[epoch3, step141]: loss 1.040570
[epoch3, step142]: loss 1.262703
[epoch3, step143]: loss 0.933318
[epoch3, step144]: loss 1.147126
[epoch3, step145]: loss 1.156767
[epoch3, step146]: loss 1.041381
[epoch3, step147]: loss 0.850535
[epoch3, step148]: loss 0.816821
[epoch3, step149]: loss 0.963818
[epoch3, step150]: loss 1.055769
[epoch3, step151]: loss 1.061597
[epoch3, step152]: loss 1.185698
[epoch3, step153]: loss 1.072868
[epoch3, step154]: loss 0.982995
[epoch3, step155]: loss 1.059523
[epoch3, step156]: loss 1.364527
[epoch3, step157]: loss 1.139971
[epoch3, step158]: loss 0.715129
[epoch3, step159]: loss 1.128990
[epoch3, step160]: loss 1.090831
[epoch3, step161]: loss 1.204729
[epoch3, step162]: loss 1.066059
[epoch3, step163]: loss 0.842695
[epoch3, step164]: loss 1.105601
[epoch3, step165]: loss 1.067112
[epoch3, step166]: loss 1.036862
[epoch3, step167]: loss 1.096707
[epoch3, step168]: loss 1.016472
[epoch3, step169]: loss 1.153191
[epoch3, step170]: loss 0.935488
[epoch3, step171]: loss 1.126963
[epoch3, step172]: loss 0.934413
[epoch3, step173]: loss 1.240279
[epoch3, step174]: loss 1.271648
[epoch3, step175]: loss 1.147174
[epoch3, step176]: loss 0.781168
[epoch3, step177]: loss 0.963103
[epoch3, step178]: loss 1.061840
[epoch3, step179]: loss 1.097116
[epoch3, step180]: loss 0.962811
[epoch3, step181]: loss 0.932432
[epoch3, step182]: loss 1.152283
[epoch3, step183]: loss 0.809430
[epoch3, step184]: loss 1.084687
[epoch3, step185]: loss 0.704334
[epoch3, step186]: loss 1.205785
[epoch3, step187]: loss 1.127063
[epoch3, step188]: loss 1.220693
[epoch3, step189]: loss 0.923233
[epoch3, step190]: loss 1.061571
[epoch3, step191]: loss 0.866493
[epoch3, step192]: loss 1.190885
[epoch3, step193]: loss 1.049361
[epoch3, step194]: loss 1.108408
[epoch3, step195]: loss 1.361353
[epoch3, step196]: loss 1.141423
[epoch3, step197]: loss 1.366060
[epoch3, step198]: loss 0.709283
[epoch3, step199]: loss 0.954681
[epoch3, step200]: loss 1.231687
[epoch3, step201]: loss 1.062731
[epoch3, step202]: loss 0.976764
[epoch3, step203]: loss 1.137897
[epoch3, step204]: loss 1.289562
[epoch3, step205]: loss 1.115980
[epoch3, step206]: loss 1.150617
[epoch3, step207]: loss 0.689022
[epoch3, step208]: loss 0.850957
[epoch3, step209]: loss 1.141578
[epoch3, step210]: loss 0.888154
[epoch3, step211]: loss 1.265856
[epoch3, step212]: loss 1.011365
[epoch3, step213]: loss 0.955984
[epoch3, step214]: loss 0.908494
[epoch3, step215]: loss 1.114584
[epoch3, step216]: loss 0.987221
[epoch3, step217]: loss 0.968133
[epoch3, step218]: loss 1.099112
[epoch3, step219]: loss 1.239179
[epoch3, step220]: loss 1.266032
[epoch3, step221]: loss 0.983217
[epoch3, step222]: loss 1.213672
[epoch3, step223]: loss 1.154662
[epoch3, step224]: loss 0.955455
[epoch3, step225]: loss 0.874770
[epoch3, step226]: loss 1.120515
[epoch3, step227]: loss 1.011363
[epoch3, step228]: loss 1.237356
[epoch3, step229]: loss 1.161272
[epoch3, step230]: loss 0.919051
[epoch3, step231]: loss 0.822579
[epoch3, step232]: loss 0.864262
[epoch3, step233]: loss 1.069821
[epoch3, step234]: loss 0.943105
[epoch3, step235]: loss 0.656732
[epoch3, step236]: loss 1.040156
[epoch3, step237]: loss 1.414686
[epoch3, step238]: loss 1.101081
[epoch3, step239]: loss 1.379097
[epoch3, step240]: loss 0.977514
[epoch3, step241]: loss 0.836726
[epoch3, step242]: loss 0.911392
[epoch3, step243]: loss 1.130154
[epoch3, step244]: loss 0.930319
[epoch3, step245]: loss 1.196196
[epoch3, step246]: loss 0.749627
[epoch3, step247]: loss 1.053390
[epoch3, step248]: loss 1.210397
[epoch3, step249]: loss 0.966711
[epoch3, step250]: loss 1.044975
[epoch3, step251]: loss 0.749995
[epoch3, step252]: loss 0.873241
[epoch3, step253]: loss 0.693582
[epoch3, step254]: loss 1.051386
[epoch3, step255]: loss 1.001415
[epoch3, step256]: loss 0.998358
[epoch3, step257]: loss 0.940909
[epoch3, step258]: loss 0.915680
[epoch3, step259]: loss 0.926602
[epoch3, step260]: loss 1.012303
[epoch3, step261]: loss 1.224468
[epoch3, step262]: loss 0.940050
[epoch3, step263]: loss 0.883638
[epoch3, step264]: loss 0.723071
[epoch3, step265]: loss 1.064406
[epoch3, step266]: loss 0.997176
[epoch3, step267]: loss 1.163677
[epoch3, step268]: loss 1.039906
[epoch3, step269]: loss 0.987138
[epoch3, step270]: loss 0.987083
[epoch3, step271]: loss 1.355215
[epoch3, step272]: loss 1.083992
[epoch3, step273]: loss 1.146527
[epoch3, step274]: loss 1.029461
[epoch3, step275]: loss 1.234614
[epoch3, step276]: loss 1.150421
[epoch3, step277]: loss 0.740570
[epoch3, step278]: loss 1.050955
[epoch3, step279]: loss 1.010435
[epoch3, step280]: loss 1.120251
[epoch3, step281]: loss 0.742491
[epoch3, step282]: loss 0.903612
[epoch3, step283]: loss 1.340208
[epoch3, step284]: loss 0.849866
[epoch3, step285]: loss 1.147132
[epoch3, step286]: loss 0.873038
[epoch3, step287]: loss 0.899764
[epoch3, step288]: loss 0.995183
[epoch3, step289]: loss 1.023586
[epoch3, step290]: loss 1.076131
[epoch3, step291]: loss 1.034262
[epoch3, step292]: loss 1.115410
[epoch3, step293]: loss 0.843654
[epoch3, step294]: loss 1.126411
[epoch3, step295]: loss 1.013198
[epoch3, step296]: loss 1.185208
[epoch3, step297]: loss 0.749113
[epoch3, step298]: loss 1.071454
[epoch3, step299]: loss 0.797404
[epoch3, step300]: loss 1.253558
[epoch3, step301]: loss 1.006695
[epoch3, step302]: loss 0.992013
[epoch3, step303]: loss 0.939431
[epoch3, step304]: loss 1.205810
[epoch3, step305]: loss 0.985788
[epoch3, step306]: loss 1.012002
[epoch3, step307]: loss 0.674800
[epoch3, step308]: loss 1.075431
[epoch3, step309]: loss 1.185014
[epoch3, step310]: loss 1.133869
[epoch3, step311]: loss 1.059024
[epoch3, step312]: loss 0.951138
[epoch3, step313]: loss 0.726557
[epoch3, step314]: loss 0.957127
[epoch3, step315]: loss 1.366759
[epoch3, step316]: loss 1.045538
[epoch3, step317]: loss 1.127978
[epoch3, step318]: loss 1.089224
[epoch3, step319]: loss 0.927166
[epoch3, step320]: loss 1.268808
[epoch3, step321]: loss 0.966167
[epoch3, step322]: loss 1.039063
[epoch3, step323]: loss 1.155288
[epoch3, step324]: loss 1.127881
[epoch3, step325]: loss 1.283501
[epoch3, step326]: loss 1.242997
[epoch3, step327]: loss 0.665708
[epoch3, step328]: loss 1.246018
[epoch3, step329]: loss 0.910884
[epoch3, step330]: loss 1.189899
[epoch3, step331]: loss 0.954122
[epoch3, step332]: loss 1.101262
[epoch3, step333]: loss 0.881653
[epoch3, step334]: loss 0.923568
[epoch3, step335]: loss 1.102746
[epoch3, step336]: loss 0.934476
[epoch3, step337]: loss 1.160552
[epoch3, step338]: loss 1.060403
[epoch3, step339]: loss 0.689862
[epoch3, step340]: loss 1.406668
[epoch3, step341]: loss 1.092951
[epoch3, step342]: loss 1.050845
[epoch3, step343]: loss 1.099039
[epoch3, step344]: loss 1.115205
[epoch3, step345]: loss 1.019929
[epoch3, step346]: loss 0.834806
[epoch3, step347]: loss 1.072620
[epoch3, step348]: loss 1.230668
[epoch3, step349]: loss 1.209964
[epoch3, step350]: loss 0.804031
[epoch3, step351]: loss 0.481529
[epoch3, step352]: loss 0.861969
[epoch3, step353]: loss 0.816439
[epoch3, step354]: loss 0.916656
[epoch3, step355]: loss 1.142274
[epoch3, step356]: loss 0.972633
[epoch3, step357]: loss 0.913517
[epoch3, step358]: loss 0.972042
[epoch3, step359]: loss 1.158283
[epoch3, step360]: loss 1.107378
[epoch3, step361]: loss 0.865842
[epoch3, step362]: loss 0.909042
[epoch3, step363]: loss 0.966112
[epoch3, step364]: loss 1.227853
[epoch3, step365]: loss 1.110855
[epoch3, step366]: loss 0.879947
[epoch3, step367]: loss 0.860647
[epoch3, step368]: loss 1.062609
[epoch3, step369]: loss 1.120305
[epoch3, step370]: loss 1.000960
[epoch3, step371]: loss 0.893715
[epoch3, step372]: loss 1.027189
[epoch3, step373]: loss 0.776294
[epoch3, step374]: loss 1.057751
[epoch3, step375]: loss 0.815434
[epoch3, step376]: loss 1.013253
[epoch3, step377]: loss 1.189761
[epoch3, step378]: loss 0.705647
[epoch3, step379]: loss 0.981789
[epoch3, step380]: loss 1.038798
[epoch3, step381]: loss 1.070053
[epoch3, step382]: loss 0.941446
[epoch3, step383]: loss 1.105936
[epoch3, step384]: loss 0.974545
[epoch3, step385]: loss 0.982819
[epoch3, step386]: loss 0.957112
[epoch3, step387]: loss 0.915772
[epoch3, step388]: loss 1.045718
[epoch3, step389]: loss 0.939626
[epoch3, step390]: loss 1.020685
[epoch3, step391]: loss 0.600277
[epoch3, step392]: loss 1.011137
[epoch3, step393]: loss 1.189619
[epoch3, step394]: loss 0.988874
[epoch3, step395]: loss 0.817175
[epoch3, step396]: loss 0.826421
[epoch3, step397]: loss 0.714665
[epoch3, step398]: loss 1.065048
[epoch3, step399]: loss 0.807337
[epoch3, step400]: loss 1.127077
[epoch3, step401]: loss 0.922806
[epoch3, step402]: loss 1.186475
[epoch3, step403]: loss 1.015319
[epoch3, step404]: loss 1.123808
[epoch3, step405]: loss 0.825961
[epoch3, step406]: loss 1.137867
[epoch3, step407]: loss 1.052143
[epoch3, step408]: loss 1.060062
[epoch3, step409]: loss 1.105301
[epoch3, step410]: loss 1.044496
[epoch3, step411]: loss 1.199861
[epoch3, step412]: loss 1.223351
[epoch3, step413]: loss 0.852594
[epoch3, step414]: loss 0.924817
[epoch3, step415]: loss 0.995334
[epoch3, step416]: loss 0.795976
[epoch3, step417]: loss 1.080612
[epoch3, step418]: loss 0.957741
[epoch3, step419]: loss 1.114029
[epoch3, step420]: loss 1.041075
[epoch3, step421]: loss 1.245436
[epoch3, step422]: loss 1.103878
[epoch3, step423]: loss 1.142858
[epoch3, step424]: loss 1.166764
[epoch3, step425]: loss 0.977540
[epoch3, step426]: loss 1.083172
[epoch3, step427]: loss 1.143995
[epoch3, step428]: loss 1.161833
[epoch3, step429]: loss 1.038276
[epoch3, step430]: loss 0.948319
[epoch3, step431]: loss 1.067791
[epoch3, step432]: loss 0.975182
[epoch3, step433]: loss 1.008553
[epoch3, step434]: loss 0.910992
[epoch3, step435]: loss 1.387400
[epoch3, step436]: loss 1.198167
[epoch3, step437]: loss 0.897865
[epoch3, step438]: loss 0.789539
[epoch3, step439]: loss 1.201543
[epoch3, step440]: loss 1.183443
[epoch3, step441]: loss 1.013960
[epoch3, step442]: loss 1.113843
[epoch3, step443]: loss 1.008244
[epoch3, step444]: loss 0.902723
[epoch3, step445]: loss 0.723665
[epoch3, step446]: loss 1.057062
[epoch3, step447]: loss 0.988862
[epoch3, step448]: loss 1.276934
[epoch3, step449]: loss 1.153922
[epoch3, step450]: loss 1.026474
[epoch3, step451]: loss 0.859961
[epoch3, step452]: loss 1.262534
[epoch3, step453]: loss 1.202152
[epoch3, step454]: loss 0.969321
[epoch3, step455]: loss 1.196029
[epoch3, step456]: loss 0.842855
[epoch3, step457]: loss 1.117809
[epoch3, step458]: loss 0.863100
[epoch3, step459]: loss 1.133196
[epoch3, step460]: loss 0.966885
[epoch3, step461]: loss 0.922277
[epoch3, step462]: loss 0.888229
[epoch3, step463]: loss 0.764501
[epoch3, step464]: loss 1.098653
[epoch3, step465]: loss 0.883687
[epoch3, step466]: loss 0.729257
[epoch3, step467]: loss 0.887842
[epoch3, step468]: loss 0.935890
[epoch3, step469]: loss 0.986302
[epoch3, step470]: loss 0.798021
[epoch3, step471]: loss 0.958228
[epoch3, step472]: loss 1.169639
[epoch3, step473]: loss 0.931097
[epoch3, step474]: loss 0.577332
[epoch3, step475]: loss 0.803095
[epoch3, step476]: loss 0.927991
[epoch3, step477]: loss 1.136084
[epoch3, step478]: loss 0.982709
[epoch3, step479]: loss 1.148733
[epoch3, step480]: loss 0.725994
[epoch3, step481]: loss 0.649112
[epoch3, step482]: loss 0.632197
[epoch3, step483]: loss 0.975663
[epoch3, step484]: loss 0.845499
[epoch3, step485]: loss 0.882984
[epoch3, step486]: loss 1.010733
[epoch3, step487]: loss 1.254077
[epoch3, step488]: loss 1.232785
[epoch3, step489]: loss 1.033528
[epoch3, step490]: loss 0.767846
[epoch3, step491]: loss 1.294041
[epoch3, step492]: loss 0.922837
[epoch3, step493]: loss 0.896982
[epoch3, step494]: loss 0.945212
[epoch3, step495]: loss 0.959800
[epoch3, step496]: loss 0.865500
[epoch3, step497]: loss 1.179741
[epoch3, step498]: loss 0.900312
[epoch3, step499]: loss 1.153569
[epoch3, step500]: loss 0.700036
[epoch3, step501]: loss 1.123903
[epoch3, step502]: loss 0.954815
[epoch3, step503]: loss 1.077141
[epoch3, step504]: loss 1.156263
[epoch3, step505]: loss 0.857821
[epoch3, step506]: loss 1.143174
[epoch3, step507]: loss 0.918937
[epoch3, step508]: loss 1.042241
[epoch3, step509]: loss 0.931139
[epoch3, step510]: loss 0.590711
[epoch3, step511]: loss 1.298234
[epoch3, step512]: loss 1.117891
[epoch3, step513]: loss 1.003704
[epoch3, step514]: loss 1.080993
[epoch3, step515]: loss 1.277445
[epoch3, step516]: loss 0.808387
[epoch3, step517]: loss 1.068913
[epoch3, step518]: loss 0.996701
[epoch3, step519]: loss 1.082659
[epoch3, step520]: loss 1.107402
[epoch3, step521]: loss 0.530061
[epoch3, step522]: loss 1.053724
[epoch3, step523]: loss 0.960538
[epoch3, step524]: loss 1.065015
[epoch3, step525]: loss 0.814752
[epoch3, step526]: loss 1.114968
[epoch3, step527]: loss 1.366381
[epoch3, step528]: loss 0.802427
[epoch3, step529]: loss 0.891674
[epoch3, step530]: loss 1.038437
[epoch3, step531]: loss 0.750631
[epoch3, step532]: loss 1.245472
[epoch3, step533]: loss 1.005119
[epoch3, step534]: loss 1.202622
[epoch3, step535]: loss 0.941275
[epoch3, step536]: loss 0.937838
[epoch3, step537]: loss 0.927561
[epoch3, step538]: loss 1.087977
[epoch3, step539]: loss 0.780226
[epoch3, step540]: loss 0.962424
[epoch3, step541]: loss 1.131144
[epoch3, step542]: loss 1.289719
[epoch3, step543]: loss 1.017821
[epoch3, step544]: loss 0.496110
[epoch3, step545]: loss 0.861607
[epoch3, step546]: loss 0.775292
[epoch3, step547]: loss 1.054399
[epoch3, step548]: loss 0.967168
[epoch3, step549]: loss 1.001381
[epoch3, step550]: loss 0.874547
[epoch3, step551]: loss 0.934150
[epoch3, step552]: loss 0.878927
[epoch3, step553]: loss 0.763430
[epoch3, step554]: loss 0.997889
[epoch3, step555]: loss 0.951147
[epoch3, step556]: loss 0.948339
[epoch3, step557]: loss 1.198516
[epoch3, step558]: loss 1.023639
[epoch3, step559]: loss 0.977772
[epoch3, step560]: loss 1.003983
[epoch3, step561]: loss 1.058897
[epoch3, step562]: loss 1.109004
[epoch3, step563]: loss 1.125781
[epoch3, step564]: loss 0.977271
[epoch3, step565]: loss 1.164227
[epoch3, step566]: loss 0.993837
[epoch3, step567]: loss 0.861082
[epoch3, step568]: loss 1.272283
[epoch3, step569]: loss 0.934159
[epoch3, step570]: loss 0.982858
[epoch3, step571]: loss 1.286367
[epoch3, step572]: loss 1.152756
[epoch3, step573]: loss 1.127197
[epoch3, step574]: loss 1.201048
[epoch3, step575]: loss 1.265154
[epoch3, step576]: loss 0.741236
[epoch3, step577]: loss 0.944365
[epoch3, step578]: loss 1.004711
[epoch3, step579]: loss 0.822904
[epoch3, step580]: loss 0.909074
[epoch3, step581]: loss 0.865824
[epoch3, step582]: loss 1.317986
[epoch3, step583]: loss 1.009780
[epoch3, step584]: loss 0.964141
[epoch3, step585]: loss 1.063707
[epoch3, step586]: loss 1.045826
[epoch3, step587]: loss 0.870232
[epoch3, step588]: loss 0.993407
[epoch3, step589]: loss 0.901029
[epoch3, step590]: loss 1.108157
[epoch3, step591]: loss 0.648448
[epoch3, step592]: loss 0.833489
[epoch3, step593]: loss 1.071321
[epoch3, step594]: loss 1.051062
[epoch3, step595]: loss 0.958929
[epoch3, step596]: loss 1.184470
[epoch3, step597]: loss 0.749819
[epoch3, step598]: loss 0.971251
[epoch3, step599]: loss 0.932437
[epoch3, step600]: loss 0.817412
[epoch3, step601]: loss 0.951633
[epoch3, step602]: loss 1.070481
[epoch3, step603]: loss 0.750786
[epoch3, step604]: loss 0.829293
[epoch3, step605]: loss 1.070006
[epoch3, step606]: loss 0.944415
[epoch3, step607]: loss 0.789767
[epoch3, step608]: loss 1.243739
[epoch3, step609]: loss 1.050038
[epoch3, step610]: loss 1.032607
[epoch3, step611]: loss 0.893101
[epoch3, step612]: loss 1.021508
[epoch3, step613]: loss 1.217291
[epoch3, step614]: loss 1.182932
[epoch3, step615]: loss 0.727688
[epoch3, step616]: loss 0.944013
[epoch3, step617]: loss 0.943549
[epoch3, step618]: loss 0.910808
[epoch3, step619]: loss 0.957957
[epoch3, step620]: loss 0.853588
[epoch3, step621]: loss 0.925841
[epoch3, step622]: loss 1.012302
[epoch3, step623]: loss 0.866528
[epoch3, step624]: loss 1.143800
[epoch3, step625]: loss 0.946287
[epoch3, step626]: loss 0.981067
[epoch3, step627]: loss 1.210339
[epoch3, step628]: loss 0.950993
[epoch3, step629]: loss 1.075387
[epoch3, step630]: loss 1.047099
[epoch3, step631]: loss 1.110237
[epoch3, step632]: loss 1.232774
[epoch3, step633]: loss 1.203279
[epoch3, step634]: loss 0.921364
[epoch3, step635]: loss 0.705382
[epoch3, step636]: loss 0.997976
[epoch3, step637]: loss 1.053964
[epoch3, step638]: loss 1.164248
[epoch3, step639]: loss 1.019998
[epoch3, step640]: loss 1.096307
[epoch3, step641]: loss 1.061964
[epoch3, step642]: loss 1.116442
[epoch3, step643]: loss 1.090640
[epoch3, step644]: loss 1.156862
[epoch3, step645]: loss 0.967645
[epoch3, step646]: loss 1.010238
[epoch3, step647]: loss 0.957217
[epoch3, step648]: loss 1.084118
[epoch3, step649]: loss 0.947245
[epoch3, step650]: loss 1.122247
[epoch3, step651]: loss 1.020736
[epoch3, step652]: loss 0.557213
[epoch3, step653]: loss 0.813447
[epoch3, step654]: loss 1.133607
[epoch3, step655]: loss 1.073525
[epoch3, step656]: loss 0.952886
[epoch3, step657]: loss 0.772678
[epoch3, step658]: loss 1.023509
[epoch3, step659]: loss 1.139106
[epoch3, step660]: loss 0.962394
[epoch3, step661]: loss 1.099217
[epoch3, step662]: loss 1.138667
[epoch3, step663]: loss 1.181026
[epoch3, step664]: loss 0.683708
[epoch3, step665]: loss 1.019148
[epoch3, step666]: loss 1.195442
[epoch3, step667]: loss 0.746581
[epoch3, step668]: loss 0.494395
[epoch3, step669]: loss 0.838495
[epoch3, step670]: loss 0.863901
[epoch3, step671]: loss 1.094391
[epoch3, step672]: loss 1.177857
[epoch3, step673]: loss 0.890650
[epoch3, step674]: loss 1.231776
[epoch3, step675]: loss 1.160597
[epoch3, step676]: loss 0.847642
[epoch3, step677]: loss 0.791447
[epoch3, step678]: loss 1.000706
[epoch3, step679]: loss 1.181582
[epoch3, step680]: loss 0.695479
[epoch3, step681]: loss 1.089853
[epoch3, step682]: loss 0.910481
[epoch3, step683]: loss 0.993464
[epoch3, step684]: loss 0.786949
[epoch3, step685]: loss 1.092781
[epoch3, step686]: loss 0.886415
[epoch3, step687]: loss 0.819378
[epoch3, step688]: loss 0.735546
[epoch3, step689]: loss 1.186412
[epoch3, step690]: loss 0.804841
[epoch3, step691]: loss 1.062072
[epoch3, step692]: loss 1.090249
[epoch3, step693]: loss 0.993564
[epoch3, step694]: loss 1.071193
[epoch3, step695]: loss 1.009100
[epoch3, step696]: loss 1.246625
[epoch3, step697]: loss 1.123014
[epoch3, step698]: loss 0.792775
[epoch3, step699]: loss 1.379999
[epoch3, step700]: loss 0.921667
[epoch3, step701]: loss 0.757045
[epoch3, step702]: loss 0.640845
[epoch3, step703]: loss 1.036170
[epoch3, step704]: loss 1.000636
[epoch3, step705]: loss 0.870192
[epoch3, step706]: loss 1.049825
[epoch3, step707]: loss 1.090798
[epoch3, step708]: loss 1.021730
[epoch3, step709]: loss 1.050720
[epoch3, step710]: loss 0.883821
[epoch3, step711]: loss 1.036574
[epoch3, step712]: loss 1.130077
[epoch3, step713]: loss 1.159694
[epoch3, step714]: loss 1.350158
[epoch3, step715]: loss 1.038672
[epoch3, step716]: loss 0.706339
[epoch3, step717]: loss 1.134013
[epoch3, step718]: loss 1.041182
[epoch3, step719]: loss 0.823175
[epoch3, step720]: loss 1.106663
[epoch3, step721]: loss 0.908862
[epoch3, step722]: loss 1.203381
[epoch3, step723]: loss 1.084332
[epoch3, step724]: loss 1.297218
[epoch3, step725]: loss 1.105558
[epoch3, step726]: loss 0.866136
[epoch3, step727]: loss 1.274639
[epoch3, step728]: loss 0.987961
[epoch3, step729]: loss 0.651901
[epoch3, step730]: loss 1.287911
[epoch3, step731]: loss 0.813734
[epoch3, step732]: loss 1.062342
[epoch3, step733]: loss 0.973280
[epoch3, step734]: loss 1.233031
[epoch3, step735]: loss 1.283965
[epoch3, step736]: loss 0.946525
[epoch3, step737]: loss 0.914735
[epoch3, step738]: loss 1.027179
[epoch3, step739]: loss 1.057314
[epoch3, step740]: loss 0.949860
[epoch3, step741]: loss 0.895158
[epoch3, step742]: loss 0.983898
[epoch3, step743]: loss 0.999315
[epoch3, step744]: loss 0.787767
[epoch3, step745]: loss 0.598273
[epoch3, step746]: loss 1.125352
[epoch3, step747]: loss 1.136861
[epoch3, step748]: loss 1.096008
[epoch3, step749]: loss 0.917955
[epoch3, step750]: loss 0.810165
[epoch3, step751]: loss 1.117705
[epoch3, step752]: loss 1.136998
[epoch3, step753]: loss 1.151695
[epoch3, step754]: loss 1.023167
[epoch3, step755]: loss 0.787221
[epoch3, step756]: loss 1.085786
[epoch3, step757]: loss 0.939860
[epoch3, step758]: loss 0.961994
[epoch3, step759]: loss 1.262169
[epoch3, step760]: loss 0.990906
[epoch3, step761]: loss 1.184881
[epoch3, step762]: loss 0.866679
[epoch3, step763]: loss 1.077413
[epoch3, step764]: loss 0.967208
[epoch3, step765]: loss 0.604997
[epoch3, step766]: loss 0.933439
[epoch3, step767]: loss 0.977747
[epoch3, step768]: loss 0.817173
[epoch3, step769]: loss 0.779023
[epoch3, step770]: loss 0.971679
[epoch3, step771]: loss 0.483633
[epoch3, step772]: loss 1.023302
[epoch3, step773]: loss 0.959657
[epoch3, step774]: loss 1.134099
[epoch3, step775]: loss 0.962457
[epoch3, step776]: loss 0.968604
[epoch3, step777]: loss 1.088374
[epoch3, step778]: loss 1.141969
[epoch3, step779]: loss 1.006194
[epoch3, step780]: loss 1.001527
[epoch3, step781]: loss 1.030593
[epoch3, step782]: loss 0.819193
[epoch3, step783]: loss 0.687214
[epoch3, step784]: loss 0.980800
[epoch3, step785]: loss 0.886703
[epoch3, step786]: loss 1.271580
[epoch3, step787]: loss 1.255673
[epoch3, step788]: loss 0.634250
[epoch3, step789]: loss 1.180593
[epoch3, step790]: loss 1.131982
[epoch3, step791]: loss 1.130033
[epoch3, step792]: loss 1.037126
[epoch3, step793]: loss 0.814378
[epoch3, step794]: loss 0.849277
[epoch3, step795]: loss 0.739813
[epoch3, step796]: loss 0.722976
[epoch3, step797]: loss 0.532087
[epoch3, step798]: loss 0.787276
[epoch3, step799]: loss 0.890168
[epoch3, step800]: loss 1.112208
[epoch3, step801]: loss 1.120050
[epoch3, step802]: loss 0.804688
[epoch3, step803]: loss 1.023185
[epoch3, step804]: loss 1.247887
[epoch3, step805]: loss 1.126244
[epoch3, step806]: loss 1.129128
[epoch3, step807]: loss 1.198036
[epoch3, step808]: loss 1.061286
[epoch3, step809]: loss 0.996762
[epoch3, step810]: loss 1.069589
[epoch3, step811]: loss 1.112772
[epoch3, step812]: loss 1.251100
[epoch3, step813]: loss 1.032083
[epoch3, step814]: loss 1.014120
[epoch3, step815]: loss 1.005029
[epoch3, step816]: loss 0.973007
[epoch3, step817]: loss 0.962061
[epoch3, step818]: loss 0.770378
[epoch3, step819]: loss 0.960185
[epoch3, step820]: loss 0.905989
[epoch3, step821]: loss 1.040189
[epoch3, step822]: loss 1.036178
[epoch3, step823]: loss 0.863451
[epoch3, step824]: loss 0.995711
[epoch3, step825]: loss 0.956388
[epoch3, step826]: loss 0.964478
[epoch3, step827]: loss 0.960633
[epoch3, step828]: loss 1.213310
[epoch3, step829]: loss 1.069861
[epoch3, step830]: loss 0.946408
[epoch3, step831]: loss 0.798283
[epoch3, step832]: loss 0.751887
[epoch3, step833]: loss 0.889990
[epoch3, step834]: loss 0.708937
[epoch3, step835]: loss 0.961644
[epoch3, step836]: loss 1.300336
[epoch3, step837]: loss 0.686419
[epoch3, step838]: loss 0.921959
[epoch3, step839]: loss 1.065951
[epoch3, step840]: loss 0.872963
[epoch3, step841]: loss 0.982153
[epoch3, step842]: loss 0.646328
[epoch3, step843]: loss 0.909728
[epoch3, step844]: loss 0.975469
[epoch3, step845]: loss 0.699518
[epoch3, step846]: loss 0.949668
[epoch3, step847]: loss 1.107289
[epoch3, step848]: loss 1.174460
[epoch3, step849]: loss 0.784681
[epoch3, step850]: loss 1.064260
[epoch3, step851]: loss 0.931315
[epoch3, step852]: loss 1.000161
[epoch3, step853]: loss 1.219483
[epoch3, step854]: loss 0.585645
[epoch3, step855]: loss 0.816980
[epoch3, step856]: loss 0.884944
[epoch3, step857]: loss 0.845471
[epoch3, step858]: loss 1.018598
[epoch3, step859]: loss 1.208994
[epoch3, step860]: loss 0.940411
[epoch3, step861]: loss 1.008071
[epoch3, step862]: loss 1.168909
[epoch3, step863]: loss 1.173025
[epoch3, step864]: loss 0.868184
[epoch3, step865]: loss 0.993154
[epoch3, step866]: loss 1.411538
[epoch3, step867]: loss 0.638049
[epoch3, step868]: loss 1.100367
[epoch3, step869]: loss 1.366857
[epoch3, step870]: loss 0.866665
[epoch3, step871]: loss 0.665659
[epoch3, step872]: loss 0.868679
[epoch3, step873]: loss 1.029106
[epoch3, step874]: loss 0.799495
[epoch3, step875]: loss 1.158298
[epoch3, step876]: loss 1.137569
[epoch3, step877]: loss 1.039404
[epoch3, step878]: loss 1.105846
[epoch3, step879]: loss 1.065161
[epoch3, step880]: loss 1.014231
[epoch3, step881]: loss 0.891797
[epoch3, step882]: loss 0.807531
[epoch3, step883]: loss 0.956529
[epoch3, step884]: loss 1.132921
[epoch3, step885]: loss 0.683683
[epoch3, step886]: loss 0.918981
[epoch3, step887]: loss 0.932373
[epoch3, step888]: loss 1.107603
[epoch3, step889]: loss 0.869696
[epoch3, step890]: loss 1.078927
[epoch3, step891]: loss 0.894653
[epoch3, step892]: loss 0.758379
[epoch3, step893]: loss 1.173215
[epoch3, step894]: loss 1.226893
[epoch3, step895]: loss 0.978605
[epoch3, step896]: loss 0.821192
[epoch3, step897]: loss 1.220970
[epoch3, step898]: loss 1.048931
[epoch3, step899]: loss 0.920177
[epoch3, step900]: loss 0.798086
[epoch3, step901]: loss 0.609934
[epoch3, step902]: loss 0.737202
[epoch3, step903]: loss 0.892168
[epoch3, step904]: loss 0.952662
[epoch3, step905]: loss 1.070721
[epoch3, step906]: loss 1.032997
[epoch3, step907]: loss 1.041291
[epoch3, step908]: loss 1.035326
[epoch3, step909]: loss 0.828159
[epoch3, step910]: loss 1.069872
[epoch3, step911]: loss 1.073900
[epoch3, step912]: loss 0.717402
[epoch3, step913]: loss 0.791599
[epoch3, step914]: loss 1.047015
[epoch3, step915]: loss 0.878644
[epoch3, step916]: loss 0.617980
[epoch3, step917]: loss 1.154804
[epoch3, step918]: loss 1.000142
[epoch3, step919]: loss 0.949823
[epoch3, step920]: loss 0.914033
[epoch3, step921]: loss 0.963575
[epoch3, step922]: loss 0.930615
[epoch3, step923]: loss 1.149326
[epoch3, step924]: loss 1.310628
[epoch3, step925]: loss 0.940958
[epoch3, step926]: loss 0.965191
[epoch3, step927]: loss 0.892384
[epoch3, step928]: loss 1.309795
[epoch3, step929]: loss 0.976801
[epoch3, step930]: loss 0.620969
[epoch3, step931]: loss 0.567740
[epoch3, step932]: loss 0.940883
[epoch3, step933]: loss 0.957996
[epoch3, step934]: loss 0.872063
[epoch3, step935]: loss 1.138782
[epoch3, step936]: loss 0.958775
[epoch3, step937]: loss 1.163887
[epoch3, step938]: loss 0.928869
[epoch3, step939]: loss 1.307522
[epoch3, step940]: loss 0.996663
[epoch3, step941]: loss 1.114483
[epoch3, step942]: loss 1.048292
[epoch3, step943]: loss 0.925429
[epoch3, step944]: loss 1.146610
[epoch3, step945]: loss 1.087871
[epoch3, step946]: loss 0.997783
[epoch3, step947]: loss 0.972517
[epoch3, step948]: loss 0.994807
[epoch3, step949]: loss 0.939427
[epoch3, step950]: loss 0.978178
[epoch3, step951]: loss 1.010906
[epoch3, step952]: loss 1.213321
[epoch3, step953]: loss 1.135744
[epoch3, step954]: loss 1.218611
[epoch3, step955]: loss 1.013217
[epoch3, step956]: loss 1.251090
[epoch3, step957]: loss 0.835890
[epoch3, step958]: loss 1.068724
[epoch3, step959]: loss 1.028812
[epoch3, step960]: loss 1.076592
[epoch3, step961]: loss 1.001732
[epoch3, step962]: loss 1.018685
[epoch3, step963]: loss 1.030531
[epoch3, step964]: loss 0.765709
[epoch3, step965]: loss 0.814023
[epoch3, step966]: loss 0.961253
[epoch3, step967]: loss 0.868628
[epoch3, step968]: loss 0.928421
[epoch3, step969]: loss 1.028194
[epoch3, step970]: loss 0.817771
[epoch3, step971]: loss 1.033653
[epoch3, step972]: loss 1.088486
[epoch3, step973]: loss 1.185309
[epoch3, step974]: loss 1.197136
[epoch3, step975]: loss 1.149942
[epoch3, step976]: loss 0.928556
[epoch3, step977]: loss 0.939639
[epoch3, step978]: loss 0.955850
[epoch3, step979]: loss 0.950504
[epoch3, step980]: loss 0.909294
[epoch3, step981]: loss 0.788068
[epoch3, step982]: loss 0.918972
[epoch3, step983]: loss 0.967157
[epoch3, step984]: loss 0.952953
[epoch3, step985]: loss 0.825034
[epoch3, step986]: loss 0.831258
[epoch3, step987]: loss 0.937387
[epoch3, step988]: loss 0.828844
[epoch3, step989]: loss 0.984808
[epoch3, step990]: loss 0.943555
[epoch3, step991]: loss 1.035725
[epoch3, step992]: loss 1.177931
[epoch3, step993]: loss 0.894198
[epoch3, step994]: loss 0.818697
[epoch3, step995]: loss 0.519296
[epoch3, step996]: loss 1.075462
[epoch3, step997]: loss 1.000181
[epoch3, step998]: loss 0.924759
[epoch3, step999]: loss 1.019462
[epoch3, step1000]: loss 1.179198
[epoch3, step1001]: loss 0.781124
[epoch3, step1002]: loss 0.601861
[epoch3, step1003]: loss 0.715579
[epoch3, step1004]: loss 0.722318
[epoch3, step1005]: loss 0.636188
[epoch3, step1006]: loss 1.099250
[epoch3, step1007]: loss 0.624842
[epoch3, step1008]: loss 1.173098
[epoch3, step1009]: loss 1.118701
[epoch3, step1010]: loss 0.814765
[epoch3, step1011]: loss 1.155671
[epoch3, step1012]: loss 0.560711
[epoch3, step1013]: loss 0.818546
[epoch3, step1014]: loss 1.208413
[epoch3, step1015]: loss 1.099844
[epoch3, step1016]: loss 0.624436
[epoch3, step1017]: loss 1.018740
[epoch3, step1018]: loss 1.171081
[epoch3, step1019]: loss 0.648880
[epoch3, step1020]: loss 0.890003
[epoch3, step1021]: loss 0.866927
[epoch3, step1022]: loss 1.029742
[epoch3, step1023]: loss 1.045448
[epoch3, step1024]: loss 0.940724
[epoch3, step1025]: loss 0.753092
[epoch3, step1026]: loss 0.636811
[epoch3, step1027]: loss 1.098725
[epoch3, step1028]: loss 0.740464
[epoch3, step1029]: loss 0.669852
[epoch3, step1030]: loss 1.005977
[epoch3, step1031]: loss 0.804139
[epoch3, step1032]: loss 0.931419
[epoch3, step1033]: loss 0.672492
[epoch3, step1034]: loss 0.950823
[epoch3, step1035]: loss 1.151674
[epoch3, step1036]: loss 1.043192
[epoch3, step1037]: loss 1.061797
[epoch3, step1038]: loss 1.021091
[epoch3, step1039]: loss 0.644155
[epoch3, step1040]: loss 0.905305
[epoch3, step1041]: loss 0.816207
[epoch3, step1042]: loss 1.241613
[epoch3, step1043]: loss 0.988949
[epoch3, step1044]: loss 0.643487
[epoch3, step1045]: loss 1.070382
[epoch3, step1046]: loss 0.958966
[epoch3, step1047]: loss 0.914779
[epoch3, step1048]: loss 1.206278
[epoch3, step1049]: loss 0.983422
[epoch3, step1050]: loss 0.945999
[epoch3, step1051]: loss 1.200642
[epoch3, step1052]: loss 1.148474
[epoch3, step1053]: loss 1.063343
[epoch3, step1054]: loss 1.073061
[epoch3, step1055]: loss 0.854041
[epoch3, step1056]: loss 1.173232
[epoch3, step1057]: loss 1.124834
[epoch3, step1058]: loss 1.073591
[epoch3, step1059]: loss 1.116198
[epoch3, step1060]: loss 0.743688
[epoch3, step1061]: loss 0.831535
[epoch3, step1062]: loss 1.057435
[epoch3, step1063]: loss 0.918631
[epoch3, step1064]: loss 1.189372
[epoch3, step1065]: loss 0.826017
[epoch3, step1066]: loss 0.960476
[epoch3, step1067]: loss 0.922687
[epoch3, step1068]: loss 1.073874
[epoch3, step1069]: loss 0.854827
[epoch3, step1070]: loss 1.051737
[epoch3, step1071]: loss 1.018576
[epoch3, step1072]: loss 1.033133
[epoch3, step1073]: loss 0.733370
[epoch3, step1074]: loss 0.948552
[epoch3, step1075]: loss 0.849241
[epoch3, step1076]: loss 0.964202
[epoch3, step1077]: loss 1.003426
[epoch3, step1078]: loss 1.185338
[epoch3, step1079]: loss 1.124351
[epoch3, step1080]: loss 1.105757
[epoch3, step1081]: loss 0.850199
[epoch3, step1082]: loss 1.081173
[epoch3, step1083]: loss 0.951794
[epoch3, step1084]: loss 0.924360
[epoch3, step1085]: loss 0.867235
[epoch3, step1086]: loss 1.068053
[epoch3, step1087]: loss 0.902836
[epoch3, step1088]: loss 0.931925
[epoch3, step1089]: loss 0.752638
[epoch3, step1090]: loss 0.753531
[epoch3, step1091]: loss 0.858476
[epoch3, step1092]: loss 0.887608
[epoch3, step1093]: loss 1.027989
[epoch3, step1094]: loss 1.062220
[epoch3, step1095]: loss 0.907956
[epoch3, step1096]: loss 1.048321
[epoch3, step1097]: loss 0.664151
[epoch3, step1098]: loss 1.135924
[epoch3, step1099]: loss 1.004178
[epoch3, step1100]: loss 0.863916
[epoch3, step1101]: loss 0.851629
[epoch3, step1102]: loss 0.978674
[epoch3, step1103]: loss 0.938894
[epoch3, step1104]: loss 1.071360
[epoch3, step1105]: loss 1.108331
[epoch3, step1106]: loss 0.941350
[epoch3, step1107]: loss 0.838395
[epoch3, step1108]: loss 0.974682
[epoch3, step1109]: loss 0.955082
[epoch3, step1110]: loss 0.833081
[epoch3, step1111]: loss 0.862682
[epoch3, step1112]: loss 0.994986
[epoch3, step1113]: loss 0.747267
[epoch3, step1114]: loss 0.923364
[epoch3, step1115]: loss 0.646832
[epoch3, step1116]: loss 0.917770
[epoch3, step1117]: loss 1.013721
[epoch3, step1118]: loss 1.181741
[epoch3, step1119]: loss 0.899309
[epoch3, step1120]: loss 0.989242
[epoch3, step1121]: loss 0.814383
[epoch3, step1122]: loss 0.942122
[epoch3, step1123]: loss 0.900458
[epoch3, step1124]: loss 0.597617
[epoch3, step1125]: loss 0.642848
[epoch3, step1126]: loss 0.710181
[epoch3, step1127]: loss 1.213610
[epoch3, step1128]: loss 0.569378
[epoch3, step1129]: loss 0.732191
[epoch3, step1130]: loss 0.772427
[epoch3, step1131]: loss 0.829690
[epoch3, step1132]: loss 0.738429
[epoch3, step1133]: loss 0.852782
[epoch3, step1134]: loss 0.750639
[epoch3, step1135]: loss 0.507456
[epoch3, step1136]: loss 0.906524
[epoch3, step1137]: loss 1.156899
[epoch3, step1138]: loss 1.046699
[epoch3, step1139]: loss 1.030048
[epoch3, step1140]: loss 0.863383
[epoch3, step1141]: loss 0.745323
[epoch3, step1142]: loss 0.768332
[epoch3, step1143]: loss 1.039869
[epoch3, step1144]: loss 1.037221
[epoch3, step1145]: loss 0.825658
[epoch3, step1146]: loss 0.979670
[epoch3, step1147]: loss 1.192423
[epoch3, step1148]: loss 0.984573
[epoch3, step1149]: loss 0.817177
[epoch3, step1150]: loss 0.751649
[epoch3, step1151]: loss 0.927374
[epoch3, step1152]: loss 0.843279
[epoch3, step1153]: loss 0.889676
[epoch3, step1154]: loss 1.032271
[epoch3, step1155]: loss 1.321734
[epoch3, step1156]: loss 0.979445
[epoch3, step1157]: loss 0.704318
[epoch3, step1158]: loss 0.931865
[epoch3, step1159]: loss 0.972013
[epoch3, step1160]: loss 0.840429
[epoch3, step1161]: loss 1.045398
[epoch3, step1162]: loss 1.030914
[epoch3, step1163]: loss 1.062759
[epoch3, step1164]: loss 1.094813
[epoch3, step1165]: loss 0.886724
[epoch3, step1166]: loss 0.830004
[epoch3, step1167]: loss 0.789474
[epoch3, step1168]: loss 0.934929
[epoch3, step1169]: loss 1.021460
[epoch3, step1170]: loss 0.832242
[epoch3, step1171]: loss 0.900218
[epoch3, step1172]: loss 0.925730
[epoch3, step1173]: loss 0.872449
[epoch3, step1174]: loss 0.765057
[epoch3, step1175]: loss 1.058562
[epoch3, step1176]: loss 1.101378
[epoch3, step1177]: loss 0.732192
[epoch3, step1178]: loss 1.034195
[epoch3, step1179]: loss 0.943335
[epoch3, step1180]: loss 1.166872
[epoch3, step1181]: loss 0.578351
[epoch3, step1182]: loss 0.880912
[epoch3, step1183]: loss 1.119704
[epoch3, step1184]: loss 1.133948
[epoch3, step1185]: loss 0.783928
[epoch3, step1186]: loss 1.152640
[epoch3, step1187]: loss 0.912960
[epoch3, step1188]: loss 1.034191
[epoch3, step1189]: loss 1.203974
[epoch3, step1190]: loss 0.803390
[epoch3, step1191]: loss 0.832119
[epoch3, step1192]: loss 0.725893
[epoch3, step1193]: loss 0.810108
[epoch3, step1194]: loss 0.899759
[epoch3, step1195]: loss 1.181832
[epoch3, step1196]: loss 0.843093
[epoch3, step1197]: loss 0.758335
[epoch3, step1198]: loss 0.966204
[epoch3, step1199]: loss 0.984520
[epoch3, step1200]: loss 0.699975
[epoch3, step1201]: loss 1.062420
[epoch3, step1202]: loss 1.091826
[epoch3, step1203]: loss 0.839198
[epoch3, step1204]: loss 1.033426
[epoch3, step1205]: loss 0.850129
[epoch3, step1206]: loss 0.912358
[epoch3, step1207]: loss 0.766238
[epoch3, step1208]: loss 0.872903
[epoch3, step1209]: loss 1.236831
[epoch3, step1210]: loss 0.623429
[epoch3, step1211]: loss 1.193041
[epoch3, step1212]: loss 0.668720
[epoch3, step1213]: loss 0.860158
[epoch3, step1214]: loss 1.133705
[epoch3, step1215]: loss 0.810329
[epoch3, step1216]: loss 1.039484
[epoch3, step1217]: loss 1.087212
[epoch3, step1218]: loss 1.002183
[epoch3, step1219]: loss 0.788558
[epoch3, step1220]: loss 0.984548
[epoch3, step1221]: loss 0.943881
[epoch3, step1222]: loss 0.804667
[epoch3, step1223]: loss 0.900672
[epoch3, step1224]: loss 1.069802
[epoch3, step1225]: loss 1.036699
[epoch3, step1226]: loss 0.993738
[epoch3, step1227]: loss 0.984738
[epoch3, step1228]: loss 1.130677
[epoch3, step1229]: loss 0.938307
[epoch3, step1230]: loss 1.086745
[epoch3, step1231]: loss 0.723085
[epoch3, step1232]: loss 1.044019
[epoch3, step1233]: loss 1.146582
[epoch3, step1234]: loss 0.971202
[epoch3, step1235]: loss 1.163181
[epoch3, step1236]: loss 0.969867
[epoch3, step1237]: loss 0.919489
[epoch3, step1238]: loss 0.814319
[epoch3, step1239]: loss 0.905247
[epoch3, step1240]: loss 1.095200
[epoch3, step1241]: loss 0.966446
[epoch3, step1242]: loss 0.858222
[epoch3, step1243]: loss 0.910220
[epoch3, step1244]: loss 0.868142
[epoch3, step1245]: loss 0.950799
[epoch3, step1246]: loss 0.920876
[epoch3, step1247]: loss 0.868065
[epoch3, step1248]: loss 0.886698
[epoch3, step1249]: loss 1.212583
[epoch3, step1250]: loss 0.786584
[epoch3, step1251]: loss 0.533833
[epoch3, step1252]: loss 0.874598
[epoch3, step1253]: loss 1.028453
[epoch3, step1254]: loss 0.872081
[epoch3, step1255]: loss 1.112067
[epoch3, step1256]: loss 0.818191
[epoch3, step1257]: loss 0.990639
[epoch3, step1258]: loss 0.985154
[epoch3, step1259]: loss 1.024769
[epoch3, step1260]: loss 0.748594
[epoch3, step1261]: loss 1.168231
[epoch3, step1262]: loss 0.925792
[epoch3, step1263]: loss 1.006445
[epoch3, step1264]: loss 0.725628
[epoch3, step1265]: loss 0.839777
[epoch3, step1266]: loss 1.031766
[epoch3, step1267]: loss 0.837055
[epoch3, step1268]: loss 1.077184
[epoch3, step1269]: loss 0.571249
[epoch3, step1270]: loss 0.935538
[epoch3, step1271]: loss 0.879224
[epoch3, step1272]: loss 0.756035
[epoch3, step1273]: loss 0.484312
[epoch3, step1274]: loss 0.814157
[epoch3, step1275]: loss 1.200333
[epoch3, step1276]: loss 0.881790
[epoch3, step1277]: loss 0.768329
[epoch3, step1278]: loss 0.660271
[epoch3, step1279]: loss 0.908658
[epoch3, step1280]: loss 0.874816
[epoch3, step1281]: loss 0.916565
[epoch3, step1282]: loss 0.643109
[epoch3, step1283]: loss 0.959402
[epoch3, step1284]: loss 0.677124
[epoch3, step1285]: loss 1.044653
[epoch3, step1286]: loss 0.995494
[epoch3, step1287]: loss 1.136899
[epoch3, step1288]: loss 0.750773
[epoch3, step1289]: loss 0.807061
[epoch3, step1290]: loss 0.915107
[epoch3, step1291]: loss 0.893834
[epoch3, step1292]: loss 0.950319
[epoch3, step1293]: loss 0.926696
[epoch3, step1294]: loss 0.964452
[epoch3, step1295]: loss 1.051057
[epoch3, step1296]: loss 0.679278
[epoch3, step1297]: loss 1.053705
[epoch3, step1298]: loss 1.175569
[epoch3, step1299]: loss 0.656745
[epoch3, step1300]: loss 1.124984
[epoch3, step1301]: loss 0.971509
[epoch3, step1302]: loss 0.777629
[epoch3, step1303]: loss 1.197508
[epoch3, step1304]: loss 0.843480
[epoch3, step1305]: loss 0.711919
[epoch3, step1306]: loss 1.213933
[epoch3, step1307]: loss 1.033570
[epoch3, step1308]: loss 1.013778
[epoch3, step1309]: loss 0.936003
[epoch3, step1310]: loss 0.968947
[epoch3, step1311]: loss 0.939502
[epoch3, step1312]: loss 0.954445
[epoch3, step1313]: loss 0.976763
[epoch3, step1314]: loss 1.008124
[epoch3, step1315]: loss 1.149895
[epoch3, step1316]: loss 0.929048
[epoch3, step1317]: loss 0.796787
[epoch3, step1318]: loss 0.786245
[epoch3, step1319]: loss 0.598187
[epoch3, step1320]: loss 0.725431
[epoch3, step1321]: loss 0.694330
[epoch3, step1322]: loss 1.077605
[epoch3, step1323]: loss 0.884689
[epoch3, step1324]: loss 1.049023
[epoch3, step1325]: loss 0.809297
[epoch3, step1326]: loss 0.779726
[epoch3, step1327]: loss 0.912364
[epoch3, step1328]: loss 0.772421
[epoch3, step1329]: loss 0.990951
[epoch3, step1330]: loss 1.093460
[epoch3, step1331]: loss 0.784357
[epoch3, step1332]: loss 1.142226
[epoch3, step1333]: loss 0.969272
[epoch3, step1334]: loss 1.046374
[epoch3, step1335]: loss 0.899579
[epoch3, step1336]: loss 0.899643
[epoch3, step1337]: loss 1.075042
[epoch3, step1338]: loss 1.016983
[epoch3, step1339]: loss 1.251511
[epoch3, step1340]: loss 0.614688
[epoch3, step1341]: loss 0.997218
[epoch3, step1342]: loss 0.852931
[epoch3, step1343]: loss 1.032244
[epoch3, step1344]: loss 0.865134
[epoch3, step1345]: loss 1.057485
[epoch3, step1346]: loss 0.838558
[epoch3, step1347]: loss 1.009106
[epoch3, step1348]: loss 0.980680
[epoch3, step1349]: loss 0.945573
[epoch3, step1350]: loss 1.249834
[epoch3, step1351]: loss 0.416808
[epoch3, step1352]: loss 0.820014
[epoch3, step1353]: loss 0.774977
[epoch3, step1354]: loss 0.909866
[epoch3, step1355]: loss 0.744142
[epoch3, step1356]: loss 1.131700
[epoch3, step1357]: loss 0.986891
[epoch3, step1358]: loss 0.950219
[epoch3, step1359]: loss 1.053822
[epoch3, step1360]: loss 1.029080
[epoch3, step1361]: loss 0.942123
[epoch3, step1362]: loss 1.065206
[epoch3, step1363]: loss 0.809458
[epoch3, step1364]: loss 0.853048
[epoch3, step1365]: loss 0.681271
[epoch3, step1366]: loss 0.843793
[epoch3, step1367]: loss 0.843712
[epoch3, step1368]: loss 0.915954
[epoch3, step1369]: loss 0.793807
[epoch3, step1370]: loss 0.988985
[epoch3, step1371]: loss 0.830372
[epoch3, step1372]: loss 0.871976
[epoch3, step1373]: loss 1.017877
[epoch3, step1374]: loss 1.108525
[epoch3, step1375]: loss 0.931505
[epoch3, step1376]: loss 0.736271
[epoch3, step1377]: loss 1.083386
[epoch3, step1378]: loss 1.236950
[epoch3, step1379]: loss 0.810033
[epoch3, step1380]: loss 1.013925
[epoch3, step1381]: loss 0.784000
[epoch3, step1382]: loss 1.154147
[epoch3, step1383]: loss 1.061417
[epoch3, step1384]: loss 1.102913
[epoch3, step1385]: loss 0.948871
[epoch3, step1386]: loss 0.996180
[epoch3, step1387]: loss 0.947678
[epoch3, step1388]: loss 0.845088
[epoch3, step1389]: loss 0.900790
[epoch3, step1390]: loss 0.974243
[epoch3, step1391]: loss 0.854168
[epoch3, step1392]: loss 0.827562
[epoch3, step1393]: loss 1.063291
[epoch3, step1394]: loss 0.796601
[epoch3, step1395]: loss 1.009482
[epoch3, step1396]: loss 0.734621
[epoch3, step1397]: loss 0.723494
[epoch3, step1398]: loss 1.063741
[epoch3, step1399]: loss 0.967977
[epoch3, step1400]: loss 0.942928
[epoch3, step1401]: loss 0.869493
[epoch3, step1402]: loss 1.083535
[epoch3, step1403]: loss 0.744353
[epoch3, step1404]: loss 0.520431
[epoch3, step1405]: loss 0.910585
[epoch3, step1406]: loss 0.872781
[epoch3, step1407]: loss 1.062060
[epoch3, step1408]: loss 1.200916
[epoch3, step1409]: loss 0.836825
[epoch3, step1410]: loss 0.653177
[epoch3, step1411]: loss 0.900791
[epoch3, step1412]: loss 0.715618
[epoch3, step1413]: loss 0.947762
[epoch3, step1414]: loss 1.016705
[epoch3, step1415]: loss 0.961732
[epoch3, step1416]: loss 0.667606
[epoch3, step1417]: loss 0.856199
[epoch3, step1418]: loss 0.866250
[epoch3, step1419]: loss 1.023759
[epoch3, step1420]: loss 1.033809
[epoch3, step1421]: loss 1.037439
[epoch3, step1422]: loss 1.082132
[epoch3, step1423]: loss 0.921281
[epoch3, step1424]: loss 1.127735
[epoch3, step1425]: loss 1.088177
[epoch3, step1426]: loss 0.808058
[epoch3, step1427]: loss 1.068388
[epoch3, step1428]: loss 1.003211
[epoch3, step1429]: loss 0.898787
[epoch3, step1430]: loss 1.104949
[epoch3, step1431]: loss 1.105450
[epoch3, step1432]: loss 0.748916
[epoch3, step1433]: loss 1.020199
[epoch3, step1434]: loss 1.019195
[epoch3, step1435]: loss 0.729994
[epoch3, step1436]: loss 0.847413
[epoch3, step1437]: loss 0.906748
[epoch3, step1438]: loss 0.562350
[epoch3, step1439]: loss 0.775097
[epoch3, step1440]: loss 0.742135
[epoch3, step1441]: loss 0.764815
[epoch3, step1442]: loss 0.845119
[epoch3, step1443]: loss 0.815958
[epoch3, step1444]: loss 1.103207
[epoch3, step1445]: loss 0.915870
[epoch3, step1446]: loss 0.847685
[epoch3, step1447]: loss 0.994740
[epoch3, step1448]: loss 0.779540
[epoch3, step1449]: loss 1.127674
[epoch3, step1450]: loss 1.003630
[epoch3, step1451]: loss 1.109859
[epoch3, step1452]: loss 0.843898
[epoch3, step1453]: loss 1.274943
[epoch3, step1454]: loss 1.140088
[epoch3, step1455]: loss 1.117000
[epoch3, step1456]: loss 0.968564
[epoch3, step1457]: loss 0.803689
[epoch3, step1458]: loss 0.763906
[epoch3, step1459]: loss 0.728681
[epoch3, step1460]: loss 0.645411
[epoch3, step1461]: loss 0.953321
[epoch3, step1462]: loss 0.721791
[epoch3, step1463]: loss 0.804562
[epoch3, step1464]: loss 0.921829
[epoch3, step1465]: loss 0.929340
[epoch3, step1466]: loss 0.894468
[epoch3, step1467]: loss 0.599824
[epoch3, step1468]: loss 0.989217
[epoch3, step1469]: loss 0.925289
[epoch3, step1470]: loss 0.885642
[epoch3, step1471]: loss 0.843119
[epoch3, step1472]: loss 0.916857
[epoch3, step1473]: loss 1.178768
[epoch3, step1474]: loss 0.915370
[epoch3, step1475]: loss 0.758203
[epoch3, step1476]: loss 0.717566
[epoch3, step1477]: loss 0.818272
[epoch3, step1478]: loss 0.923956
[epoch3, step1479]: loss 1.019165
[epoch3, step1480]: loss 0.806503
[epoch3, step1481]: loss 0.906919
[epoch3, step1482]: loss 1.064965
[epoch3, step1483]: loss 0.807589
[epoch3, step1484]: loss 0.907594
[epoch3, step1485]: loss 0.683531
[epoch3, step1486]: loss 0.598645
[epoch3, step1487]: loss 1.120177
[epoch3, step1488]: loss 0.996796
[epoch3, step1489]: loss 0.901131
[epoch3, step1490]: loss 1.148376
[epoch3, step1491]: loss 0.817050
[epoch3, step1492]: loss 1.006360
[epoch3, step1493]: loss 1.072827
[epoch3, step1494]: loss 1.028579
[epoch3, step1495]: loss 0.721871
[epoch3, step1496]: loss 0.902768
[epoch3, step1497]: loss 0.903361
[epoch3, step1498]: loss 1.082374
[epoch3, step1499]: loss 0.899588
[epoch3, step1500]: loss 0.843337
[epoch3, step1501]: loss 0.900680
[epoch3, step1502]: loss 0.690503
[epoch3, step1503]: loss 1.055469
[epoch3, step1504]: loss 1.182605
[epoch3, step1505]: loss 0.956438
[epoch3, step1506]: loss 1.102654
[epoch3, step1507]: loss 0.849808
[epoch3, step1508]: loss 0.656302
[epoch3, step1509]: loss 1.096240
[epoch3, step1510]: loss 1.054299
[epoch3, step1511]: loss 0.770640
[epoch3, step1512]: loss 0.859415
[epoch3, step1513]: loss 0.877090
[epoch3, step1514]: loss 0.771869
[epoch3, step1515]: loss 0.836185
[epoch3, step1516]: loss 0.778003
[epoch3, step1517]: loss 0.719066
[epoch3, step1518]: loss 1.106213
[epoch3, step1519]: loss 0.930614
[epoch3, step1520]: loss 1.036653
[epoch3, step1521]: loss 0.838235
[epoch3, step1522]: loss 0.881239
[epoch3, step1523]: loss 0.749798
[epoch3, step1524]: loss 0.911965
[epoch3, step1525]: loss 0.708676
[epoch3, step1526]: loss 0.949540
[epoch3, step1527]: loss 1.065540
[epoch3, step1528]: loss 1.037050
[epoch3, step1529]: loss 0.879703
[epoch3, step1530]: loss 0.651022
[epoch3, step1531]: loss 0.792936
[epoch3, step1532]: loss 0.582951
[epoch3, step1533]: loss 0.894104
[epoch3, step1534]: loss 1.013801
[epoch3, step1535]: loss 1.059740
[epoch3, step1536]: loss 0.850976
[epoch3, step1537]: loss 0.828299
[epoch3, step1538]: loss 0.581562
[epoch3, step1539]: loss 0.733640
[epoch3, step1540]: loss 0.745478
[epoch3, step1541]: loss 0.777660
[epoch3, step1542]: loss 0.960570
[epoch3, step1543]: loss 0.686753
[epoch3, step1544]: loss 0.921814
[epoch3, step1545]: loss 0.656891
[epoch3, step1546]: loss 0.890443
[epoch3, step1547]: loss 1.322577
[epoch3, step1548]: loss 0.825824
[epoch3, step1549]: loss 0.653820
[epoch3, step1550]: loss 0.988993
[epoch3, step1551]: loss 1.040461
[epoch3, step1552]: loss 1.100546
[epoch3, step1553]: loss 1.230390
[epoch3, step1554]: loss 0.836094
[epoch3, step1555]: loss 0.846593
[epoch3, step1556]: loss 0.698699
[epoch3, step1557]: loss 0.873600
[epoch3, step1558]: loss 1.019232
[epoch3, step1559]: loss 0.937321
[epoch3, step1560]: loss 0.950552
[epoch3, step1561]: loss 0.866428
[epoch3, step1562]: loss 1.025329
[epoch3, step1563]: loss 1.225097
[epoch3, step1564]: loss 0.749119
[epoch3, step1565]: loss 0.813011
[epoch3, step1566]: loss 0.866467
[epoch3, step1567]: loss 0.725351
[epoch3, step1568]: loss 0.969874
[epoch3, step1569]: loss 0.756047
[epoch3, step1570]: loss 0.792360
[epoch3, step1571]: loss 0.770277
[epoch3, step1572]: loss 1.058906
[epoch3, step1573]: loss 0.931451
[epoch3, step1574]: loss 0.921164
[epoch3, step1575]: loss 0.851725
[epoch3, step1576]: loss 0.886147
[epoch3, step1577]: loss 0.741914
[epoch3, step1578]: loss 1.116192
[epoch3, step1579]: loss 1.150727
[epoch3, step1580]: loss 0.612377
[epoch3, step1581]: loss 0.990414
[epoch3, step1582]: loss 0.945350
[epoch3, step1583]: loss 0.975403
[epoch3, step1584]: loss 0.910454
[epoch3, step1585]: loss 0.707958
[epoch3, step1586]: loss 1.136665
[epoch3, step1587]: loss 0.877564
[epoch3, step1588]: loss 0.869307
[epoch3, step1589]: loss 0.986453
[epoch3, step1590]: loss 0.940848
[epoch3, step1591]: loss 0.939563
[epoch3, step1592]: loss 0.784950
[epoch3, step1593]: loss 0.976698
[epoch3, step1594]: loss 1.001858
[epoch3, step1595]: loss 1.094390
[epoch3, step1596]: loss 0.616684
[epoch3, step1597]: loss 0.955079
[epoch3, step1598]: loss 0.743208
[epoch3, step1599]: loss 0.986568
[epoch3, step1600]: loss 1.223015
[epoch3, step1601]: loss 1.051169
[epoch3, step1602]: loss 0.983612
[epoch3, step1603]: loss 0.764808
[epoch3, step1604]: loss 0.645934
[epoch3, step1605]: loss 0.944029
[epoch3, step1606]: loss 1.014783
[epoch3, step1607]: loss 0.925483
[epoch3, step1608]: loss 0.993611
[epoch3, step1609]: loss 1.135347
[epoch3, step1610]: loss 1.148676
[epoch3, step1611]: loss 0.854795
[epoch3, step1612]: loss 1.051791
[epoch3, step1613]: loss 0.937232
[epoch3, step1614]: loss 1.019366
[epoch3, step1615]: loss 1.045992
[epoch3, step1616]: loss 0.680553
[epoch3, step1617]: loss 0.664122
[epoch3, step1618]: loss 0.643918
[epoch3, step1619]: loss 1.094400
[epoch3, step1620]: loss 0.952608
[epoch3, step1621]: loss 0.910032
[epoch3, step1622]: loss 1.141641
[epoch3, step1623]: loss 0.702694
[epoch3, step1624]: loss 0.974016
[epoch3, step1625]: loss 0.991878
[epoch3, step1626]: loss 1.030176
[epoch3, step1627]: loss 0.624550
[epoch3, step1628]: loss 0.881382
[epoch3, step1629]: loss 1.074552
[epoch3, step1630]: loss 0.739581
[epoch3, step1631]: loss 0.960129
[epoch3, step1632]: loss 0.544100
[epoch3, step1633]: loss 0.600471
[epoch3, step1634]: loss 0.872386
[epoch3, step1635]: loss 1.101004
[epoch3, step1636]: loss 1.112287
[epoch3, step1637]: loss 0.964662
[epoch3, step1638]: loss 1.051978
[epoch3, step1639]: loss 0.899383
[epoch3, step1640]: loss 0.900704
[epoch3, step1641]: loss 1.013876
[epoch3, step1642]: loss 0.869097
[epoch3, step1643]: loss 0.855427
[epoch3, step1644]: loss 0.536145
[epoch3, step1645]: loss 0.906874
[epoch3, step1646]: loss 0.591456
[epoch3, step1647]: loss 0.460699
[epoch3, step1648]: loss 0.715035
[epoch3, step1649]: loss 0.718572
[epoch3, step1650]: loss 1.032843
[epoch3, step1651]: loss 0.761506
[epoch3, step1652]: loss 1.097271
[epoch3, step1653]: loss 1.166166
[epoch3, step1654]: loss 1.071044
[epoch3, step1655]: loss 0.675558
[epoch3, step1656]: loss 0.860569
[epoch3, step1657]: loss 0.829624
[epoch3, step1658]: loss 0.964171
[epoch3, step1659]: loss 1.015473
[epoch3, step1660]: loss 0.799628
[epoch3, step1661]: loss 1.061806
[epoch3, step1662]: loss 0.811809
[epoch3, step1663]: loss 0.885737
[epoch3, step1664]: loss 0.751470
[epoch3, step1665]: loss 0.926365
[epoch3, step1666]: loss 0.887487
[epoch3, step1667]: loss 0.905501
[epoch3, step1668]: loss 1.183219
[epoch3, step1669]: loss 0.535859
[epoch3, step1670]: loss 0.962827
[epoch3, step1671]: loss 0.972221
[epoch3, step1672]: loss 0.910212
[epoch3, step1673]: loss 0.972569
[epoch3, step1674]: loss 0.686290
[epoch3, step1675]: loss 0.607296
[epoch3, step1676]: loss 0.864953
[epoch3, step1677]: loss 1.091063
[epoch3, step1678]: loss 0.556611
[epoch3, step1679]: loss 0.896997
[epoch3, step1680]: loss 0.710739
[epoch3, step1681]: loss 0.986299
[epoch3, step1682]: loss 0.905205
[epoch3, step1683]: loss 0.602201
[epoch3, step1684]: loss 1.045918
[epoch3, step1685]: loss 1.127270
[epoch3, step1686]: loss 0.881292
[epoch3, step1687]: loss 0.979259
[epoch3, step1688]: loss 1.167304
[epoch3, step1689]: loss 1.029669
[epoch3, step1690]: loss 0.473792
[epoch3, step1691]: loss 0.752673
[epoch3, step1692]: loss 1.110929
[epoch3, step1693]: loss 1.105327
[epoch3, step1694]: loss 1.004475
[epoch3, step1695]: loss 0.574144
[epoch3, step1696]: loss 0.993270
[epoch3, step1697]: loss 0.976117
[epoch3, step1698]: loss 0.532682
[epoch3, step1699]: loss 0.915178
[epoch3, step1700]: loss 1.050586
[epoch3, step1701]: loss 0.909268
[epoch3, step1702]: loss 1.056880
[epoch3, step1703]: loss 0.903963
[epoch3, step1704]: loss 0.898791
[epoch3, step1705]: loss 0.793815
[epoch3, step1706]: loss 0.872395
[epoch3, step1707]: loss 0.757574
[epoch3, step1708]: loss 1.029500
[epoch3, step1709]: loss 0.995428
[epoch3, step1710]: loss 0.766730
[epoch3, step1711]: loss 1.197365
[epoch3, step1712]: loss 1.282815
[epoch3, step1713]: loss 1.029665
[epoch3, step1714]: loss 1.024583
[epoch3, step1715]: loss 0.779019
[epoch3, step1716]: loss 0.903826
[epoch3, step1717]: loss 1.038773
[epoch3, step1718]: loss 0.807925
[epoch3, step1719]: loss 0.886312
[epoch3, step1720]: loss 0.771723
[epoch3, step1721]: loss 1.132105
[epoch3, step1722]: loss 0.763429
[epoch3, step1723]: loss 0.820943
[epoch3, step1724]: loss 0.843576
[epoch3, step1725]: loss 1.084282
[epoch3, step1726]: loss 0.970982
[epoch3, step1727]: loss 0.667601
[epoch3, step1728]: loss 0.901666
[epoch3, step1729]: loss 1.105931
[epoch3, step1730]: loss 0.901862
[epoch3, step1731]: loss 0.951174
[epoch3, step1732]: loss 0.828674
[epoch3, step1733]: loss 0.913519
[epoch3, step1734]: loss 1.016764
[epoch3, step1735]: loss 0.680490
[epoch3, step1736]: loss 0.824658
[epoch3, step1737]: loss 0.848619
[epoch3, step1738]: loss 0.815732
[epoch3, step1739]: loss 0.999322
[epoch3, step1740]: loss 0.702949
[epoch3, step1741]: loss 0.884596
[epoch3, step1742]: loss 0.952857
[epoch3, step1743]: loss 0.903797
[epoch3, step1744]: loss 0.958930
[epoch3, step1745]: loss 0.946291
[epoch3, step1746]: loss 1.015341
[epoch3, step1747]: loss 0.993036
[epoch3, step1748]: loss 0.949320
[epoch3, step1749]: loss 1.059342
[epoch3, step1750]: loss 0.957299
[epoch3, step1751]: loss 0.970710
[epoch3, step1752]: loss 0.932695
[epoch3, step1753]: loss 1.146535
[epoch3, step1754]: loss 0.774957
[epoch3, step1755]: loss 0.868655
[epoch3, step1756]: loss 0.950157
[epoch3, step1757]: loss 0.809194
[epoch3, step1758]: loss 1.144745
[epoch3, step1759]: loss 0.829074
[epoch3, step1760]: loss 0.948833
[epoch3, step1761]: loss 0.832267
[epoch3, step1762]: loss 0.983383
[epoch3, step1763]: loss 1.055201
[epoch3, step1764]: loss 1.194631
[epoch3, step1765]: loss 0.813073
[epoch3, step1766]: loss 0.992019
[epoch3, step1767]: loss 0.611938
[epoch3, step1768]: loss 0.827669
[epoch3, step1769]: loss 0.841069
[epoch3, step1770]: loss 0.887155
[epoch3, step1771]: loss 1.003034
[epoch3, step1772]: loss 0.784594
[epoch3, step1773]: loss 0.954650
[epoch3, step1774]: loss 0.701658
[epoch3, step1775]: loss 0.930722
[epoch3, step1776]: loss 1.099805
[epoch3, step1777]: loss 0.885809
[epoch3, step1778]: loss 0.940922
[epoch3, step1779]: loss 0.866852
[epoch3, step1780]: loss 0.965375
[epoch3, step1781]: loss 0.701868
[epoch3, step1782]: loss 0.851082
[epoch3, step1783]: loss 0.720145
[epoch3, step1784]: loss 0.712563
[epoch3, step1785]: loss 0.942769
[epoch3, step1786]: loss 0.877113
[epoch3, step1787]: loss 0.839133
[epoch3, step1788]: loss 0.792875
[epoch3, step1789]: loss 0.973464
[epoch3, step1790]: loss 0.875909
[epoch3, step1791]: loss 0.904850
[epoch3, step1792]: loss 0.964292
[epoch3, step1793]: loss 1.064930
[epoch3, step1794]: loss 0.802870
[epoch3, step1795]: loss 1.184105
[epoch3, step1796]: loss 0.851970
[epoch3, step1797]: loss 0.832315
[epoch3, step1798]: loss 0.889369
[epoch3, step1799]: loss 0.912865
[epoch3, step1800]: loss 0.530967
[epoch3, step1801]: loss 0.898478
[epoch3, step1802]: loss 0.929785
[epoch3, step1803]: loss 1.027977
[epoch3, step1804]: loss 0.905371
[epoch3, step1805]: loss 0.982907
[epoch3, step1806]: loss 1.057142
[epoch3, step1807]: loss 1.032667
[epoch3, step1808]: loss 1.072259
[epoch3, step1809]: loss 0.886595
[epoch3, step1810]: loss 0.662775
[epoch3, step1811]: loss 0.908269
[epoch3, step1812]: loss 0.689288
[epoch3, step1813]: loss 1.004574
[epoch3, step1814]: loss 0.927147
[epoch3, step1815]: loss 0.981940
[epoch3, step1816]: loss 0.533131
[epoch3, step1817]: loss 0.733211
[epoch3, step1818]: loss 0.367463
[epoch3, step1819]: loss 0.926030
[epoch3, step1820]: loss 1.087951
[epoch3, step1821]: loss 1.191532
[epoch3, step1822]: loss 1.093150
[epoch3, step1823]: loss 0.991371
[epoch3, step1824]: loss 0.883678
[epoch3, step1825]: loss 1.139916
[epoch3, step1826]: loss 0.947617
[epoch3, step1827]: loss 0.644553
[epoch3, step1828]: loss 1.151385
[epoch3, step1829]: loss 1.052017
[epoch3, step1830]: loss 0.977235
[epoch3, step1831]: loss 0.841623
[epoch3, step1832]: loss 0.993431
[epoch3, step1833]: loss 0.862932
[epoch3, step1834]: loss 1.099499
[epoch3, step1835]: loss 0.662104
[epoch3, step1836]: loss 0.859372
[epoch3, step1837]: loss 0.886714
[epoch3, step1838]: loss 1.107680
[epoch3, step1839]: loss 0.853038
[epoch3, step1840]: loss 0.974502
[epoch3, step1841]: loss 0.706410
[epoch3, step1842]: loss 1.024066
[epoch3, step1843]: loss 0.963265
[epoch3, step1844]: loss 0.897958
[epoch3, step1845]: loss 1.126672
[epoch3, step1846]: loss 0.866791
[epoch3, step1847]: loss 0.741994
[epoch3, step1848]: loss 0.891732
[epoch3, step1849]: loss 0.946487
[epoch3, step1850]: loss 0.924119
[epoch3, step1851]: loss 1.112201
[epoch3, step1852]: loss 0.898095
[epoch3, step1853]: loss 1.052056
[epoch3, step1854]: loss 0.712470
[epoch3, step1855]: loss 1.055118
[epoch3, step1856]: loss 1.080082
[epoch3, step1857]: loss 0.855942
[epoch3, step1858]: loss 0.811399
[epoch3, step1859]: loss 0.802601
[epoch3, step1860]: loss 0.981778
[epoch3, step1861]: loss 1.045508
[epoch3, step1862]: loss 0.877086
[epoch3, step1863]: loss 1.028569
[epoch3, step1864]: loss 0.673328
[epoch3, step1865]: loss 1.019913
[epoch3, step1866]: loss 0.969752
[epoch3, step1867]: loss 1.062411
[epoch3, step1868]: loss 1.097826
[epoch3, step1869]: loss 1.052201
[epoch3, step1870]: loss 1.068081
[epoch3, step1871]: loss 0.988816
[epoch3, step1872]: loss 0.967887
[epoch3, step1873]: loss 0.993337
[epoch3, step1874]: loss 0.990650
[epoch3, step1875]: loss 0.827307
[epoch3, step1876]: loss 0.612355
[epoch3, step1877]: loss 0.661066
[epoch3, step1878]: loss 0.610863
[epoch3, step1879]: loss 0.934512
[epoch3, step1880]: loss 1.221740
[epoch3, step1881]: loss 0.933065
[epoch3, step1882]: loss 0.913220
[epoch3, step1883]: loss 0.607169
[epoch3, step1884]: loss 0.875788
[epoch3, step1885]: loss 1.156446
[epoch3, step1886]: loss 0.803428
[epoch3, step1887]: loss 1.040768
[epoch3, step1888]: loss 0.887576
[epoch3, step1889]: loss 0.856691
[epoch3, step1890]: loss 0.683833
[epoch3, step1891]: loss 1.099628
[epoch3, step1892]: loss 1.067343
[epoch3, step1893]: loss 1.046202
[epoch3, step1894]: loss 0.824467
[epoch3, step1895]: loss 0.920175
[epoch3, step1896]: loss 0.621913
[epoch3, step1897]: loss 1.015091
[epoch3, step1898]: loss 0.869174
[epoch3, step1899]: loss 1.062772
[epoch3, step1900]: loss 0.862432
[epoch3, step1901]: loss 0.973550
[epoch3, step1902]: loss 0.859876
[epoch3, step1903]: loss 1.056022
[epoch3, step1904]: loss 0.979662
[epoch3, step1905]: loss 0.956546
[epoch3, step1906]: loss 1.130107
[epoch3, step1907]: loss 0.695408
[epoch3, step1908]: loss 0.607968
[epoch3, step1909]: loss 1.076494
[epoch3, step1910]: loss 0.626684
[epoch3, step1911]: loss 1.002128
[epoch3, step1912]: loss 0.865471
[epoch3, step1913]: loss 0.797271
[epoch3, step1914]: loss 0.927875
[epoch3, step1915]: loss 0.639574
[epoch3, step1916]: loss 0.931790
[epoch3, step1917]: loss 0.593946
[epoch3, step1918]: loss 0.979793
[epoch3, step1919]: loss 0.526127
[epoch3, step1920]: loss 0.911264
[epoch3, step1921]: loss 1.136093
[epoch3, step1922]: loss 1.006914
[epoch3, step1923]: loss 1.012741
[epoch3, step1924]: loss 0.826581
[epoch3, step1925]: loss 0.665514
[epoch3, step1926]: loss 0.982722
[epoch3, step1927]: loss 0.975295
[epoch3, step1928]: loss 0.911645
[epoch3, step1929]: loss 1.058933
[epoch3, step1930]: loss 0.846370
[epoch3, step1931]: loss 0.848900
[epoch3, step1932]: loss 1.023895
[epoch3, step1933]: loss 0.656642
[epoch3, step1934]: loss 0.950599
[epoch3, step1935]: loss 0.817773
[epoch3, step1936]: loss 1.003060
[epoch3, step1937]: loss 0.613637
[epoch3, step1938]: loss 0.707844
[epoch3, step1939]: loss 0.644528
[epoch3, step1940]: loss 0.779560
[epoch3, step1941]: loss 0.998724
[epoch3, step1942]: loss 0.855576
[epoch3, step1943]: loss 0.782555
[epoch3, step1944]: loss 0.729050
[epoch3, step1945]: loss 0.912199
[epoch3, step1946]: loss 0.870034
[epoch3, step1947]: loss 0.845946
[epoch3, step1948]: loss 1.011661
[epoch3, step1949]: loss 0.723487
[epoch3, step1950]: loss 0.869580
[epoch3, step1951]: loss 0.943560
[epoch3, step1952]: loss 0.614933
[epoch3, step1953]: loss 0.819383
[epoch3, step1954]: loss 0.824075
[epoch3, step1955]: loss 0.762206
[epoch3, step1956]: loss 0.666116
[epoch3, step1957]: loss 1.027595
[epoch3, step1958]: loss 0.814992
[epoch3, step1959]: loss 1.035315
[epoch3, step1960]: loss 1.015757
[epoch3, step1961]: loss 0.890181
[epoch3, step1962]: loss 1.031584
[epoch3, step1963]: loss 1.056934
[epoch3, step1964]: loss 0.974593
[epoch3, step1965]: loss 0.900548
[epoch3, step1966]: loss 0.672926
[epoch3, step1967]: loss 0.818337
[epoch3, step1968]: loss 0.447251
[epoch3, step1969]: loss 0.725294
[epoch3, step1970]: loss 0.853385
[epoch3, step1971]: loss 0.545638
[epoch3, step1972]: loss 0.966728
[epoch3, step1973]: loss 0.757619
[epoch3, step1974]: loss 0.843006
[epoch3, step1975]: loss 0.856821
[epoch3, step1976]: loss 1.042427
[epoch3, step1977]: loss 0.686793
[epoch3, step1978]: loss 1.051625
[epoch3, step1979]: loss 0.738376
[epoch3, step1980]: loss 0.791748
[epoch3, step1981]: loss 0.972865
[epoch3, step1982]: loss 0.561018
[epoch3, step1983]: loss 0.904365
[epoch3, step1984]: loss 1.051752
[epoch3, step1985]: loss 0.875287
[epoch3, step1986]: loss 1.056306
[epoch3, step1987]: loss 1.055749
[epoch3, step1988]: loss 1.081195
[epoch3, step1989]: loss 0.699726
[epoch3, step1990]: loss 0.833127
[epoch3, step1991]: loss 1.093775
[epoch3, step1992]: loss 0.818433
[epoch3, step1993]: loss 0.879317
[epoch3, step1994]: loss 0.808531
[epoch3, step1995]: loss 0.994280
[epoch3, step1996]: loss 0.926118
[epoch3, step1997]: loss 0.762689
[epoch3, step1998]: loss 0.632443
[epoch3, step1999]: loss 0.681163
[epoch3, step2000]: loss 1.095176
[epoch3, step2001]: loss 1.013380
[epoch3, step2002]: loss 1.070432
[epoch3, step2003]: loss 1.163177
[epoch3, step2004]: loss 0.792095
[epoch3, step2005]: loss 0.958766
[epoch3, step2006]: loss 0.938682
[epoch3, step2007]: loss 0.917653
[epoch3, step2008]: loss 0.980407
[epoch3, step2009]: loss 0.538372
[epoch3, step2010]: loss 0.890456
[epoch3, step2011]: loss 1.089328
[epoch3, step2012]: loss 1.263870
[epoch3, step2013]: loss 1.146941
[epoch3, step2014]: loss 0.904289
[epoch3, step2015]: loss 1.177056
[epoch3, step2016]: loss 1.158980
[epoch3, step2017]: loss 0.807351
[epoch3, step2018]: loss 0.743670
[epoch3, step2019]: loss 0.831686
[epoch3, step2020]: loss 0.528340
[epoch3, step2021]: loss 0.937583
[epoch3, step2022]: loss 0.882699
[epoch3, step2023]: loss 0.655708
[epoch3, step2024]: loss 0.745122
[epoch3, step2025]: loss 0.969614
[epoch3, step2026]: loss 0.985573
[epoch3, step2027]: loss 1.070737
[epoch3, step2028]: loss 0.648523
[epoch3, step2029]: loss 0.962505
[epoch3, step2030]: loss 0.853825
[epoch3, step2031]: loss 0.898725
[epoch3, step2032]: loss 0.969842
[epoch3, step2033]: loss 1.012028
[epoch3, step2034]: loss 0.813424
[epoch3, step2035]: loss 1.007742
[epoch3, step2036]: loss 0.995822
[epoch3, step2037]: loss 0.679459
[epoch3, step2038]: loss 1.001150
[epoch3, step2039]: loss 0.753939
[epoch3, step2040]: loss 1.088092
[epoch3, step2041]: loss 1.068276
[epoch3, step2042]: loss 0.809881
[epoch3, step2043]: loss 0.974211
[epoch3, step2044]: loss 0.794776
[epoch3, step2045]: loss 0.890362
[epoch3, step2046]: loss 1.046368
[epoch3, step2047]: loss 0.589564
[epoch3, step2048]: loss 0.525190
[epoch3, step2049]: loss 1.095778
[epoch3, step2050]: loss 0.740983
[epoch3, step2051]: loss 1.098915
[epoch3, step2052]: loss 1.104444
[epoch3, step2053]: loss 0.996404
[epoch3, step2054]: loss 1.019704
[epoch3, step2055]: loss 1.034011
[epoch3, step2056]: loss 0.489972
[epoch3, step2057]: loss 0.722411
[epoch3, step2058]: loss 0.926600
[epoch3, step2059]: loss 1.078264
[epoch3, step2060]: loss 0.352295
[epoch3, step2061]: loss 1.077102
[epoch3, step2062]: loss 0.743566
[epoch3, step2063]: loss 1.096086
[epoch3, step2064]: loss 0.819402
[epoch3, step2065]: loss 0.963798
[epoch3, step2066]: loss 1.118978
[epoch3, step2067]: loss 1.103645
[epoch3, step2068]: loss 0.742582
[epoch3, step2069]: loss 1.012904
[epoch3, step2070]: loss 1.157989
[epoch3, step2071]: loss 1.029734
[epoch3, step2072]: loss 0.886711
[epoch3, step2073]: loss 0.972761
[epoch3, step2074]: loss 0.378471
[epoch3, step2075]: loss 0.947347
[epoch3, step2076]: loss 0.866432
[epoch3, step2077]: loss 0.776870
[epoch3, step2078]: loss 0.803007
[epoch3, step2079]: loss 0.660099
[epoch3, step2080]: loss 0.792175
[epoch3, step2081]: loss 0.860165
[epoch3, step2082]: loss 0.858033
[epoch3, step2083]: loss 0.931000
[epoch3, step2084]: loss 0.966201
[epoch3, step2085]: loss 0.946936
[epoch3, step2086]: loss 1.000607
[epoch3, step2087]: loss 0.827980
[epoch3, step2088]: loss 0.933797
[epoch3, step2089]: loss 0.952999
[epoch3, step2090]: loss 0.648569
[epoch3, step2091]: loss 1.151112
[epoch3, step2092]: loss 0.967424
[epoch3, step2093]: loss 1.025230
[epoch3, step2094]: loss 0.668375
[epoch3, step2095]: loss 0.838614
[epoch3, step2096]: loss 0.713293
[epoch3, step2097]: loss 0.906824
[epoch3, step2098]: loss 1.123249
[epoch3, step2099]: loss 1.118411
[epoch3, step2100]: loss 0.592704
[epoch3, step2101]: loss 0.843726
[epoch3, step2102]: loss 0.700218
[epoch3, step2103]: loss 1.008222
[epoch3, step2104]: loss 0.863162
[epoch3, step2105]: loss 1.165810
[epoch3, step2106]: loss 0.950740
[epoch3, step2107]: loss 0.828554
[epoch3, step2108]: loss 0.884452
[epoch3, step2109]: loss 0.903809
[epoch3, step2110]: loss 0.953606
[epoch3, step2111]: loss 1.128107
[epoch3, step2112]: loss 0.831205
[epoch3, step2113]: loss 1.354242
[epoch3, step2114]: loss 0.872824
[epoch3, step2115]: loss 0.836769
[epoch3, step2116]: loss 0.435944
[epoch3, step2117]: loss 0.988920
[epoch3, step2118]: loss 0.698735
[epoch3, step2119]: loss 0.812498
[epoch3, step2120]: loss 1.108783
[epoch3, step2121]: loss 1.033583
[epoch3, step2122]: loss 0.837224
[epoch3, step2123]: loss 0.763111
[epoch3, step2124]: loss 1.007046
[epoch3, step2125]: loss 0.550329
[epoch3, step2126]: loss 0.474261
[epoch3, step2127]: loss 1.059929
[epoch3, step2128]: loss 0.857531
[epoch3, step2129]: loss 0.992202
[epoch3, step2130]: loss 0.913063
[epoch3, step2131]: loss 0.959239
[epoch3, step2132]: loss 1.116096
[epoch3, step2133]: loss 0.904876
[epoch3, step2134]: loss 0.951842
[epoch3, step2135]: loss 0.734525
[epoch3, step2136]: loss 1.140795
[epoch3, step2137]: loss 0.872794
[epoch3, step2138]: loss 0.754130
[epoch3, step2139]: loss 0.930003
[epoch3, step2140]: loss 0.925642
[epoch3, step2141]: loss 0.937071
[epoch3, step2142]: loss 0.897066
[epoch3, step2143]: loss 0.796013
[epoch3, step2144]: loss 0.781060
[epoch3, step2145]: loss 0.711506
[epoch3, step2146]: loss 1.225197
[epoch3, step2147]: loss 0.869707
[epoch3, step2148]: loss 0.659622
[epoch3, step2149]: loss 0.976788
[epoch3, step2150]: loss 0.760524
[epoch3, step2151]: loss 0.593419
[epoch3, step2152]: loss 0.747610
[epoch3, step2153]: loss 0.793567
[epoch3, step2154]: loss 0.580141
[epoch3, step2155]: loss 0.904512
[epoch3, step2156]: loss 0.965689
[epoch3, step2157]: loss 0.767623
[epoch3, step2158]: loss 0.578085
[epoch3, step2159]: loss 0.965413
[epoch3, step2160]: loss 0.998370
[epoch3, step2161]: loss 0.912474
[epoch3, step2162]: loss 0.675118
[epoch3, step2163]: loss 0.993591
[epoch3, step2164]: loss 1.050870
[epoch3, step2165]: loss 0.825292
[epoch3, step2166]: loss 0.941644
[epoch3, step2167]: loss 0.710734
[epoch3, step2168]: loss 1.023993
[epoch3, step2169]: loss 0.977875
[epoch3, step2170]: loss 0.950497
[epoch3, step2171]: loss 0.802775
[epoch3, step2172]: loss 0.816707
[epoch3, step2173]: loss 0.612075
[epoch3, step2174]: loss 0.905962
[epoch3, step2175]: loss 0.568641
[epoch3, step2176]: loss 0.855670
[epoch3, step2177]: loss 0.996488
[epoch3, step2178]: loss 0.807352
[epoch3, step2179]: loss 0.910762
[epoch3, step2180]: loss 0.816125
[epoch3, step2181]: loss 1.041257
[epoch3, step2182]: loss 0.639767
[epoch3, step2183]: loss 0.947083
[epoch3, step2184]: loss 0.963752
[epoch3, step2185]: loss 0.822302
[epoch3, step2186]: loss 0.964160
[epoch3, step2187]: loss 1.134016
[epoch3, step2188]: loss 1.113507
[epoch3, step2189]: loss 1.143928
[epoch3, step2190]: loss 0.683439
[epoch3, step2191]: loss 0.795043
[epoch3, step2192]: loss 0.802324
[epoch3, step2193]: loss 0.774383
[epoch3, step2194]: loss 0.589690
[epoch3, step2195]: loss 0.977488
[epoch3, step2196]: loss 0.969533
[epoch3, step2197]: loss 0.814023
[epoch3, step2198]: loss 0.716724
[epoch3, step2199]: loss 0.982354
[epoch3, step2200]: loss 1.010478
[epoch3, step2201]: loss 0.931441
[epoch3, step2202]: loss 0.865576
[epoch3, step2203]: loss 0.802487
[epoch3, step2204]: loss 0.966341
[epoch3, step2205]: loss 0.833472
[epoch3, step2206]: loss 1.029159
[epoch3, step2207]: loss 0.837122
[epoch3, step2208]: loss 1.002905
[epoch3, step2209]: loss 1.000158
[epoch3, step2210]: loss 0.990400
[epoch3, step2211]: loss 0.793884
[epoch3, step2212]: loss 1.176819
[epoch3, step2213]: loss 0.670847
[epoch3, step2214]: loss 0.855227
[epoch3, step2215]: loss 1.178009
[epoch3, step2216]: loss 0.907499
[epoch3, step2217]: loss 1.004682
[epoch3, step2218]: loss 0.928331
[epoch3, step2219]: loss 0.635547
[epoch3, step2220]: loss 0.656524
[epoch3, step2221]: loss 0.662228
[epoch3, step2222]: loss 0.400808
[epoch3, step2223]: loss 0.879539
[epoch3, step2224]: loss 1.003317
[epoch3, step2225]: loss 0.831068
[epoch3, step2226]: loss 0.821036
[epoch3, step2227]: loss 0.848275
[epoch3, step2228]: loss 0.958553
[epoch3, step2229]: loss 0.972138
[epoch3, step2230]: loss 0.953528
[epoch3, step2231]: loss 0.773969
[epoch3, step2232]: loss 0.808861
[epoch3, step2233]: loss 0.995587
[epoch3, step2234]: loss 0.914122
[epoch3, step2235]: loss 0.511213
[epoch3, step2236]: loss 1.129192
[epoch3, step2237]: loss 1.014299
[epoch3, step2238]: loss 0.991538
[epoch3, step2239]: loss 0.866668
[epoch3, step2240]: loss 0.756179
[epoch3, step2241]: loss 0.969618
[epoch3, step2242]: loss 1.193243
[epoch3, step2243]: loss 0.596476
[epoch3, step2244]: loss 0.979731
[epoch3, step2245]: loss 0.576485
[epoch3, step2246]: loss 0.852012
[epoch3, step2247]: loss 0.928275
[epoch3, step2248]: loss 0.706277
[epoch3, step2249]: loss 1.136355
[epoch3, step2250]: loss 0.959090
[epoch3, step2251]: loss 0.848664
[epoch3, step2252]: loss 0.651595
[epoch3, step2253]: loss 0.857266
[epoch3, step2254]: loss 0.555640
[epoch3, step2255]: loss 0.586294
[epoch3, step2256]: loss 0.927931
[epoch3, step2257]: loss 0.788195
[epoch3, step2258]: loss 0.936039
[epoch3, step2259]: loss 0.802052
[epoch3, step2260]: loss 0.938122
[epoch3, step2261]: loss 0.994358
[epoch3, step2262]: loss 0.968789
[epoch3, step2263]: loss 1.077812
[epoch3, step2264]: loss 0.848726
[epoch3, step2265]: loss 0.997938
[epoch3, step2266]: loss 0.822378
[epoch3, step2267]: loss 0.860966
[epoch3, step2268]: loss 1.103457
[epoch3, step2269]: loss 1.140151
[epoch3, step2270]: loss 0.750720
[epoch3, step2271]: loss 0.972941
[epoch3, step2272]: loss 1.003252
[epoch3, step2273]: loss 0.778761
[epoch3, step2274]: loss 0.994705
[epoch3, step2275]: loss 0.825583
[epoch3, step2276]: loss 0.968897
[epoch3, step2277]: loss 0.853043
[epoch3, step2278]: loss 0.996737
[epoch3, step2279]: loss 1.118170
[epoch3, step2280]: loss 0.953801
[epoch3, step2281]: loss 1.187581
[epoch3, step2282]: loss 0.906561
[epoch3, step2283]: loss 0.957663
[epoch3, step2284]: loss 0.720378
[epoch3, step2285]: loss 0.725722
[epoch3, step2286]: loss 0.710804
[epoch3, step2287]: loss 0.674042
[epoch3, step2288]: loss 0.858775
[epoch3, step2289]: loss 0.787418
[epoch3, step2290]: loss 0.789249
[epoch3, step2291]: loss 0.925869
[epoch3, step2292]: loss 0.900990
[epoch3, step2293]: loss 0.887618
[epoch3, step2294]: loss 0.897276
[epoch3, step2295]: loss 0.991585
[epoch3, step2296]: loss 0.688861
[epoch3, step2297]: loss 0.676196
[epoch3, step2298]: loss 0.737615
[epoch3, step2299]: loss 0.766176
[epoch3, step2300]: loss 0.666293
[epoch3, step2301]: loss 0.653344
[epoch3, step2302]: loss 0.745450
[epoch3, step2303]: loss 0.938396
[epoch3, step2304]: loss 1.037825
[epoch3, step2305]: loss 0.974428
[epoch3, step2306]: loss 0.942730
[epoch3, step2307]: loss 0.806553
[epoch3, step2308]: loss 0.802386
[epoch3, step2309]: loss 1.235236
[epoch3, step2310]: loss 1.094945
[epoch3, step2311]: loss 0.934541
[epoch3, step2312]: loss 0.684743
[epoch3, step2313]: loss 0.922088
[epoch3, step2314]: loss 1.030300
[epoch3, step2315]: loss 0.948319
[epoch3, step2316]: loss 0.645208
[epoch3, step2317]: loss 0.833134
[epoch3, step2318]: loss 0.722445
[epoch3, step2319]: loss 0.819376
[epoch3, step2320]: loss 0.993608
[epoch3, step2321]: loss 0.620625
[epoch3, step2322]: loss 0.830967
[epoch3, step2323]: loss 0.665424
[epoch3, step2324]: loss 1.059884
[epoch3, step2325]: loss 0.776017
[epoch3, step2326]: loss 0.804019
[epoch3, step2327]: loss 1.046774
[epoch3, step2328]: loss 0.702064
[epoch3, step2329]: loss 0.919547
[epoch3, step2330]: loss 1.136224
[epoch3, step2331]: loss 0.779866
[epoch3, step2332]: loss 0.984783
[epoch3, step2333]: loss 0.924721
[epoch3, step2334]: loss 0.836809
[epoch3, step2335]: loss 0.579753
[epoch3, step2336]: loss 0.826573
[epoch3, step2337]: loss 1.012270
[epoch3, step2338]: loss 0.944295
[epoch3, step2339]: loss 0.642409
[epoch3, step2340]: loss 1.127843
[epoch3, step2341]: loss 0.902290
[epoch3, step2342]: loss 0.851495
[epoch3, step2343]: loss 0.827972
[epoch3, step2344]: loss 0.983130
[epoch3, step2345]: loss 0.778126
[epoch3, step2346]: loss 0.920529
[epoch3, step2347]: loss 0.761362
[epoch3, step2348]: loss 0.514535
[epoch3, step2349]: loss 1.177430
[epoch3, step2350]: loss 0.884661
[epoch3, step2351]: loss 0.528317
[epoch3, step2352]: loss 0.762185
[epoch3, step2353]: loss 0.914946
[epoch3, step2354]: loss 0.929195
[epoch3, step2355]: loss 0.990167
[epoch3, step2356]: loss 0.617117
[epoch3, step2357]: loss 0.625928
[epoch3, step2358]: loss 0.565996
[epoch3, step2359]: loss 0.975773
[epoch3, step2360]: loss 0.784988
[epoch3, step2361]: loss 0.840092
[epoch3, step2362]: loss 0.942949
[epoch3, step2363]: loss 0.869978
[epoch3, step2364]: loss 0.808148
[epoch3, step2365]: loss 1.020146
[epoch3, step2366]: loss 0.807958
[epoch3, step2367]: loss 1.035402
[epoch3, step2368]: loss 0.697773
[epoch3, step2369]: loss 0.857340
[epoch3, step2370]: loss 0.904666
[epoch3, step2371]: loss 0.674121
[epoch3, step2372]: loss 0.971311
[epoch3, step2373]: loss 0.892808
[epoch3, step2374]: loss 0.699420
[epoch3, step2375]: loss 0.763410
[epoch3, step2376]: loss 0.740199
[epoch3, step2377]: loss 0.574676
[epoch3, step2378]: loss 0.913640
[epoch3, step2379]: loss 0.813805
[epoch3, step2380]: loss 0.805288
[epoch3, step2381]: loss 0.919681
[epoch3, step2382]: loss 1.144453
[epoch3, step2383]: loss 1.092730
[epoch3, step2384]: loss 1.049846
[epoch3, step2385]: loss 0.807000
[epoch3, step2386]: loss 1.088139
[epoch3, step2387]: loss 0.592449
[epoch3, step2388]: loss 0.593601
[epoch3, step2389]: loss 0.650831
[epoch3, step2390]: loss 0.960158
[epoch3, step2391]: loss 0.947807
[epoch3, step2392]: loss 0.775766
[epoch3, step2393]: loss 0.912321
[epoch3, step2394]: loss 0.897280
[epoch3, step2395]: loss 1.146224
[epoch3, step2396]: loss 0.698684
[epoch3, step2397]: loss 0.841937
[epoch3, step2398]: loss 0.648714
[epoch3, step2399]: loss 1.038330
[epoch3, step2400]: loss 0.933875
[epoch3, step2401]: loss 0.721408
[epoch3, step2402]: loss 0.814409
[epoch3, step2403]: loss 0.877635
[epoch3, step2404]: loss 0.718109
[epoch3, step2405]: loss 0.854445
[epoch3, step2406]: loss 0.939675
[epoch3, step2407]: loss 0.972179
[epoch3, step2408]: loss 0.779262
[epoch3, step2409]: loss 0.805310
[epoch3, step2410]: loss 0.882559
[epoch3, step2411]: loss 1.069428
[epoch3, step2412]: loss 0.921995
[epoch3, step2413]: loss 0.979975
[epoch3, step2414]: loss 0.361830
[epoch3, step2415]: loss 0.830420
[epoch3, step2416]: loss 1.098936
[epoch3, step2417]: loss 0.584132
[epoch3, step2418]: loss 1.072934
[epoch3, step2419]: loss 1.202289
[epoch3, step2420]: loss 0.623106
[epoch3, step2421]: loss 1.048396
[epoch3, step2422]: loss 0.677459
[epoch3, step2423]: loss 1.167285
[epoch3, step2424]: loss 0.913173
[epoch3, step2425]: loss 0.877470
[epoch3, step2426]: loss 0.996556
[epoch3, step2427]: loss 0.904315
[epoch3, step2428]: loss 0.761121
[epoch3, step2429]: loss 1.043693
[epoch3, step2430]: loss 0.950263
[epoch3, step2431]: loss 1.036696
[epoch3, step2432]: loss 0.616443
[epoch3, step2433]: loss 0.810274
[epoch3, step2434]: loss 0.919047
[epoch3, step2435]: loss 0.778577
[epoch3, step2436]: loss 0.854491
[epoch3, step2437]: loss 0.899112
[epoch3, step2438]: loss 0.676560
[epoch3, step2439]: loss 0.820163
[epoch3, step2440]: loss 0.887833
[epoch3, step2441]: loss 0.640183
[epoch3, step2442]: loss 0.945977
[epoch3, step2443]: loss 0.750396
[epoch3, step2444]: loss 0.880861
[epoch3, step2445]: loss 0.894183
[epoch3, step2446]: loss 0.631237
[epoch3, step2447]: loss 0.930669
[epoch3, step2448]: loss 0.820455
[epoch3, step2449]: loss 0.966433
[epoch3, step2450]: loss 0.683690
[epoch3, step2451]: loss 0.960274
[epoch3, step2452]: loss 0.677077
[epoch3, step2453]: loss 0.674148
[epoch3, step2454]: loss 1.102868
[epoch3, step2455]: loss 0.939007
[epoch3, step2456]: loss 0.952919
[epoch3, step2457]: loss 0.796458
[epoch3, step2458]: loss 0.648928
[epoch3, step2459]: loss 1.036192
[epoch3, step2460]: loss 0.873982
[epoch3, step2461]: loss 1.086758
[epoch3, step2462]: loss 0.745426
[epoch3, step2463]: loss 0.734524
[epoch3, step2464]: loss 0.901368
[epoch3, step2465]: loss 0.816475
[epoch3, step2466]: loss 0.513700
[epoch3, step2467]: loss 0.991226
[epoch3, step2468]: loss 0.922535
[epoch3, step2469]: loss 0.706375
[epoch3, step2470]: loss 0.790966
[epoch3, step2471]: loss 0.897302
[epoch3, step2472]: loss 0.923921
[epoch3, step2473]: loss 0.513852
[epoch3, step2474]: loss 0.850129
[epoch3, step2475]: loss 0.636668
[epoch3, step2476]: loss 0.889673
[epoch3, step2477]: loss 0.790690
[epoch3, step2478]: loss 1.012338
[epoch3, step2479]: loss 0.882865
[epoch3, step2480]: loss 1.102036
[epoch3, step2481]: loss 0.780941
[epoch3, step2482]: loss 1.101433
[epoch3, step2483]: loss 0.777676
[epoch3, step2484]: loss 0.641973
[epoch3, step2485]: loss 1.010418
[epoch3, step2486]: loss 0.809432
[epoch3, step2487]: loss 0.862934
[epoch3, step2488]: loss 0.554968
[epoch3, step2489]: loss 0.914011
[epoch3, step2490]: loss 0.838421
[epoch3, step2491]: loss 1.268783
[epoch3, step2492]: loss 1.055974
[epoch3, step2493]: loss 0.874524
[epoch3, step2494]: loss 0.873408
[epoch3, step2495]: loss 0.646802
[epoch3, step2496]: loss 0.947583
[epoch3, step2497]: loss 0.847589
[epoch3, step2498]: loss 0.920420
[epoch3, step2499]: loss 0.952145
[epoch3, step2500]: loss 1.034402
[epoch3, step2501]: loss 0.923580
[epoch3, step2502]: loss 0.995411
[epoch3, step2503]: loss 0.706783
[epoch3, step2504]: loss 1.078601
[epoch3, step2505]: loss 0.891303
[epoch3, step2506]: loss 0.973312
[epoch3, step2507]: loss 0.950665
[epoch3, step2508]: loss 1.163640
[epoch3, step2509]: loss 0.782541
[epoch3, step2510]: loss 0.980236
[epoch3, step2511]: loss 0.893343
[epoch3, step2512]: loss 0.923055
[epoch3, step2513]: loss 0.817155
[epoch3, step2514]: loss 0.918326
[epoch3, step2515]: loss 1.114804
[epoch3, step2516]: loss 0.957488
[epoch3, step2517]: loss 1.103858
[epoch3, step2518]: loss 0.907324
[epoch3, step2519]: loss 0.742217
[epoch3, step2520]: loss 0.867908
[epoch3, step2521]: loss 0.781704
[epoch3, step2522]: loss 0.696250
[epoch3, step2523]: loss 1.215361
[epoch3, step2524]: loss 0.768016
[epoch3, step2525]: loss 0.884585
[epoch3, step2526]: loss 0.931484
[epoch3, step2527]: loss 0.776009
[epoch3, step2528]: loss 0.738650
[epoch3, step2529]: loss 0.948887
[epoch3, step2530]: loss 0.800897
[epoch3, step2531]: loss 0.725533
[epoch3, step2532]: loss 1.008644
[epoch3, step2533]: loss 0.583528
[epoch3, step2534]: loss 1.031795
[epoch3, step2535]: loss 1.196830
[epoch3, step2536]: loss 1.077188
[epoch3, step2537]: loss 0.953697
[epoch3, step2538]: loss 0.826445
[epoch3, step2539]: loss 0.819841
[epoch3, step2540]: loss 1.122015
[epoch3, step2541]: loss 1.216916
[epoch3, step2542]: loss 1.012097
[epoch3, step2543]: loss 0.500479
[epoch3, step2544]: loss 0.903545
[epoch3, step2545]: loss 0.858439
[epoch3, step2546]: loss 1.112706
[epoch3, step2547]: loss 0.875817
[epoch3, step2548]: loss 1.092541
[epoch3, step2549]: loss 0.515757
[epoch3, step2550]: loss 0.598170
[epoch3, step2551]: loss 0.991327
[epoch3, step2552]: loss 1.135638
[epoch3, step2553]: loss 0.738742
[epoch3, step2554]: loss 0.970247
[epoch3, step2555]: loss 1.228588
[epoch3, step2556]: loss 0.872232
[epoch3, step2557]: loss 0.799620
[epoch3, step2558]: loss 0.820040
[epoch3, step2559]: loss 1.063163
[epoch3, step2560]: loss 0.760024
[epoch3, step2561]: loss 1.175553
[epoch3, step2562]: loss 1.121422
[epoch3, step2563]: loss 0.617931
[epoch3, step2564]: loss 0.723928
[epoch3, step2565]: loss 1.008213
[epoch3, step2566]: loss 0.562010
[epoch3, step2567]: loss 1.050655
[epoch3, step2568]: loss 0.530997
[epoch3, step2569]: loss 0.810193
[epoch3, step2570]: loss 1.192440
[epoch3, step2571]: loss 0.806231
[epoch3, step2572]: loss 0.716028
[epoch3, step2573]: loss 0.797879
[epoch3, step2574]: loss 0.973396
[epoch3, step2575]: loss 0.814039
[epoch3, step2576]: loss 0.544402
[epoch3, step2577]: loss 0.685409
[epoch3, step2578]: loss 1.028970
[epoch3, step2579]: loss 0.806358
[epoch3, step2580]: loss 0.783781
[epoch3, step2581]: loss 0.827556
[epoch3, step2582]: loss 1.086765
[epoch3, step2583]: loss 0.528707
[epoch3, step2584]: loss 0.754244
[epoch3, step2585]: loss 0.603507
[epoch3, step2586]: loss 1.106248
[epoch3, step2587]: loss 0.857685
[epoch3, step2588]: loss 0.994404
[epoch3, step2589]: loss 0.739598
[epoch3, step2590]: loss 0.937536
[epoch3, step2591]: loss 0.685989
[epoch3, step2592]: loss 0.914772
[epoch3, step2593]: loss 1.075972
[epoch3, step2594]: loss 1.077912
[epoch3, step2595]: loss 0.520999
[epoch3, step2596]: loss 0.806713
[epoch3, step2597]: loss 0.883746
[epoch3, step2598]: loss 0.864770
[epoch3, step2599]: loss 0.777850
[epoch3, step2600]: loss 0.712970
[epoch3, step2601]: loss 0.827138
[epoch3, step2602]: loss 0.589533
[epoch3, step2603]: loss 0.871488
[epoch3, step2604]: loss 0.991798
[epoch3, step2605]: loss 0.774988
[epoch3, step2606]: loss 0.905777
[epoch3, step2607]: loss 0.821677
[epoch3, step2608]: loss 0.678542
[epoch3, step2609]: loss 0.764340
[epoch3, step2610]: loss 1.025459
[epoch3, step2611]: loss 0.928356
[epoch3, step2612]: loss 0.662007
[epoch3, step2613]: loss 0.360627
[epoch3, step2614]: loss 0.808794
[epoch3, step2615]: loss 0.936929
[epoch3, step2616]: loss 0.976346
[epoch3, step2617]: loss 0.586852
[epoch3, step2618]: loss 1.123663
[epoch3, step2619]: loss 1.035597
[epoch3, step2620]: loss 0.784739
[epoch3, step2621]: loss 0.837148
[epoch3, step2622]: loss 0.477258
[epoch3, step2623]: loss 1.010884
[epoch3, step2624]: loss 1.102624
[epoch3, step2625]: loss 0.947654
[epoch3, step2626]: loss 1.097292
[epoch3, step2627]: loss 0.577959
[epoch3, step2628]: loss 0.836963
[epoch3, step2629]: loss 0.851521
[epoch3, step2630]: loss 0.862584
[epoch3, step2631]: loss 0.680219
[epoch3, step2632]: loss 1.100918
[epoch3, step2633]: loss 1.091288
[epoch3, step2634]: loss 0.982482
[epoch3, step2635]: loss 0.806277
[epoch3, step2636]: loss 0.703752
[epoch3, step2637]: loss 0.931703
[epoch3, step2638]: loss 0.949498
[epoch3, step2639]: loss 1.058494
[epoch3, step2640]: loss 0.927767
[epoch3, step2641]: loss 0.869173
[epoch3, step2642]: loss 0.484885
[epoch3, step2643]: loss 0.664363
[epoch3, step2644]: loss 1.017173
[epoch3, step2645]: loss 0.991753
[epoch3, step2646]: loss 0.756704
[epoch3, step2647]: loss 0.833455
[epoch3, step2648]: loss 1.109282
[epoch3, step2649]: loss 0.549575
[epoch3, step2650]: loss 0.804887
[epoch3, step2651]: loss 0.949073
[epoch3, step2652]: loss 0.949301
[epoch3, step2653]: loss 0.734575
[epoch3, step2654]: loss 1.164951
[epoch3, step2655]: loss 0.577690
[epoch3, step2656]: loss 0.824059
[epoch3, step2657]: loss 0.565235
[epoch3, step2658]: loss 0.855027
[epoch3, step2659]: loss 1.041629
[epoch3, step2660]: loss 0.660662
[epoch3, step2661]: loss 0.733721
[epoch3, step2662]: loss 0.987527
[epoch3, step2663]: loss 0.688487
[epoch3, step2664]: loss 0.989798
[epoch3, step2665]: loss 0.565810
[epoch3, step2666]: loss 0.857798
[epoch3, step2667]: loss 0.842794
[epoch3, step2668]: loss 0.960607
[epoch3, step2669]: loss 0.970585
[epoch3, step2670]: loss 0.555954
[epoch3, step2671]: loss 0.958444
[epoch3, step2672]: loss 0.838348
[epoch3, step2673]: loss 0.782938
[epoch3, step2674]: loss 0.908083
[epoch3, step2675]: loss 0.509742
[epoch3, step2676]: loss 0.741417
[epoch3, step2677]: loss 0.665476
[epoch3, step2678]: loss 0.828782
[epoch3, step2679]: loss 0.950197
[epoch3, step2680]: loss 1.021825
[epoch3, step2681]: loss 0.907029
[epoch3, step2682]: loss 0.733678
[epoch3, step2683]: loss 0.725494
[epoch3, step2684]: loss 0.924488
[epoch3, step2685]: loss 1.048927
[epoch3, step2686]: loss 0.629414
[epoch3, step2687]: loss 0.762169
[epoch3, step2688]: loss 0.809112
[epoch3, step2689]: loss 1.136810
[epoch3, step2690]: loss 0.878712
[epoch3, step2691]: loss 0.936584
[epoch3, step2692]: loss 0.836758
[epoch3, step2693]: loss 0.577204
[epoch3, step2694]: loss 0.643447
[epoch3, step2695]: loss 0.516268
[epoch3, step2696]: loss 0.732858
[epoch3, step2697]: loss 0.742090
[epoch3, step2698]: loss 0.711406
[epoch3, step2699]: loss 0.994714
[epoch3, step2700]: loss 0.825043
[epoch3, step2701]: loss 1.001079
[epoch3, step2702]: loss 1.025636
[epoch3, step2703]: loss 0.735951
[epoch3, step2704]: loss 0.636760
[epoch3, step2705]: loss 0.813221
[epoch3, step2706]: loss 0.669635
[epoch3, step2707]: loss 1.031847
[epoch3, step2708]: loss 0.825645
[epoch3, step2709]: loss 1.147706
[epoch3, step2710]: loss 0.779274
[epoch3, step2711]: loss 0.768804
[epoch3, step2712]: loss 0.740788
[epoch3, step2713]: loss 0.939692
[epoch3, step2714]: loss 0.555370
[epoch3, step2715]: loss 0.989865
[epoch3, step2716]: loss 1.107680
[epoch3, step2717]: loss 0.840133
[epoch3, step2718]: loss 0.604716
[epoch3, step2719]: loss 1.127135
[epoch3, step2720]: loss 1.093658
[epoch3, step2721]: loss 0.881863
[epoch3, step2722]: loss 1.062044
[epoch3, step2723]: loss 0.781816
[epoch3, step2724]: loss 0.995453
[epoch3, step2725]: loss 0.646721
[epoch3, step2726]: loss 0.733935
[epoch3, step2727]: loss 0.740277
[epoch3, step2728]: loss 0.718453
[epoch3, step2729]: loss 0.671606
[epoch3, step2730]: loss 0.582825
[epoch3, step2731]: loss 0.939020
[epoch3, step2732]: loss 1.061238
[epoch3, step2733]: loss 0.638264
[epoch3, step2734]: loss 0.937364
[epoch3, step2735]: loss 1.120217
[epoch3, step2736]: loss 1.127883
[epoch3, step2737]: loss 0.866343
[epoch3, step2738]: loss 0.842285
[epoch3, step2739]: loss 0.799617
[epoch3, step2740]: loss 1.096596
[epoch3, step2741]: loss 0.751875
[epoch3, step2742]: loss 0.679154
[epoch3, step2743]: loss 1.184623
[epoch3, step2744]: loss 0.915532
[epoch3, step2745]: loss 0.835932
[epoch3, step2746]: loss 0.606709
[epoch3, step2747]: loss 1.028429
[epoch3, step2748]: loss 0.747521
[epoch3, step2749]: loss 0.730996
[epoch3, step2750]: loss 1.143071
[epoch3, step2751]: loss 0.881495
[epoch3, step2752]: loss 1.034791
[epoch3, step2753]: loss 1.029799
[epoch3, step2754]: loss 0.727217
[epoch3, step2755]: loss 0.524691
[epoch3, step2756]: loss 0.943472
[epoch3, step2757]: loss 1.119932
[epoch3, step2758]: loss 0.736012
[epoch3, step2759]: loss 0.718803
[epoch3, step2760]: loss 0.768237
[epoch3, step2761]: loss 0.860296
[epoch3, step2762]: loss 0.920492
[epoch3, step2763]: loss 1.194966
[epoch3, step2764]: loss 0.864572
[epoch3, step2765]: loss 0.899823
[epoch3, step2766]: loss 0.840294
[epoch3, step2767]: loss 0.501401
[epoch3, step2768]: loss 0.544327
[epoch3, step2769]: loss 0.849358
[epoch3, step2770]: loss 1.058064
[epoch3, step2771]: loss 0.794773
[epoch3, step2772]: loss 0.910141
[epoch3, step2773]: loss 0.608631
[epoch3, step2774]: loss 0.901935
[epoch3, step2775]: loss 0.598352
[epoch3, step2776]: loss 0.629781
[epoch3, step2777]: loss 0.896927
[epoch3, step2778]: loss 0.859028
[epoch3, step2779]: loss 1.063055
[epoch3, step2780]: loss 0.916847
[epoch3, step2781]: loss 1.045229
[epoch3, step2782]: loss 0.840942
[epoch3, step2783]: loss 0.877983
[epoch3, step2784]: loss 0.563915
[epoch3, step2785]: loss 1.142291
[epoch3, step2786]: loss 0.871713
[epoch3, step2787]: loss 0.640279
[epoch3, step2788]: loss 0.553897
[epoch3, step2789]: loss 0.540853
[epoch3, step2790]: loss 0.789128
[epoch3, step2791]: loss 0.736875
[epoch3, step2792]: loss 0.964867
[epoch3, step2793]: loss 0.746567
[epoch3, step2794]: loss 0.978713
[epoch3, step2795]: loss 0.939068
[epoch3, step2796]: loss 0.917835
[epoch3, step2797]: loss 0.605065
[epoch3, step2798]: loss 0.715418
[epoch3, step2799]: loss 0.681395
[epoch3, step2800]: loss 0.989733
[epoch3, step2801]: loss 0.648205
[epoch3, step2802]: loss 0.961716
[epoch3, step2803]: loss 0.965629
[epoch3, step2804]: loss 0.937720
[epoch3, step2805]: loss 0.728757
[epoch3, step2806]: loss 0.392678
[epoch3, step2807]: loss 0.820663
[epoch3, step2808]: loss 0.942877
[epoch3, step2809]: loss 0.926781
[epoch3, step2810]: loss 0.958251
[epoch3, step2811]: loss 0.992811
[epoch3, step2812]: loss 1.030099
[epoch3, step2813]: loss 0.735923
[epoch3, step2814]: loss 0.882555
[epoch3, step2815]: loss 0.650007
[epoch3, step2816]: loss 0.706329
[epoch3, step2817]: loss 1.057710
[epoch3, step2818]: loss 0.847508
[epoch3, step2819]: loss 0.931805
[epoch3, step2820]: loss 0.942922
[epoch3, step2821]: loss 0.977378
[epoch3, step2822]: loss 0.458719
[epoch3, step2823]: loss 0.715248
[epoch3, step2824]: loss 1.025817
[epoch3, step2825]: loss 1.037397
[epoch3, step2826]: loss 0.937229
[epoch3, step2827]: loss 0.718318
[epoch3, step2828]: loss 0.927577
[epoch3, step2829]: loss 0.748239
[epoch3, step2830]: loss 0.593060
[epoch3, step2831]: loss 1.024383
[epoch3, step2832]: loss 0.766915
[epoch3, step2833]: loss 0.700510
[epoch3, step2834]: loss 0.793531
[epoch3, step2835]: loss 1.308779
[epoch3, step2836]: loss 0.782908
[epoch3, step2837]: loss 1.043967
[epoch3, step2838]: loss 0.637638
[epoch3, step2839]: loss 0.815927
[epoch3, step2840]: loss 0.798641
[epoch3, step2841]: loss 0.814692
[epoch3, step2842]: loss 0.637625
[epoch3, step2843]: loss 0.799728
[epoch3, step2844]: loss 0.725636
[epoch3, step2845]: loss 0.665418
[epoch3, step2846]: loss 0.751917
[epoch3, step2847]: loss 0.682929
[epoch3, step2848]: loss 0.736693
[epoch3, step2849]: loss 0.874413
[epoch3, step2850]: loss 0.591624
[epoch3, step2851]: loss 1.010738
[epoch3, step2852]: loss 0.898778
[epoch3, step2853]: loss 1.007623
[epoch3, step2854]: loss 0.872683
[epoch3, step2855]: loss 1.141903
[epoch3, step2856]: loss 0.957051
[epoch3, step2857]: loss 1.033333
[epoch3, step2858]: loss 0.647019
[epoch3, step2859]: loss 0.758537
[epoch3, step2860]: loss 0.702925
[epoch3, step2861]: loss 0.988373
[epoch3, step2862]: loss 1.040569
[epoch3, step2863]: loss 1.134952
[epoch3, step2864]: loss 0.864876
[epoch3, step2865]: loss 0.693854
[epoch3, step2866]: loss 0.890439
[epoch3, step2867]: loss 1.062849
[epoch3, step2868]: loss 0.868471
[epoch3, step2869]: loss 0.781457
[epoch3, step2870]: loss 0.821881
[epoch3, step2871]: loss 0.630995
[epoch3, step2872]: loss 0.977508
[epoch3, step2873]: loss 0.661087
[epoch3, step2874]: loss 0.956454
[epoch3, step2875]: loss 1.000461
[epoch3, step2876]: loss 0.826136
[epoch3, step2877]: loss 0.845545
[epoch3, step2878]: loss 0.906169
[epoch3, step2879]: loss 1.039509
[epoch3, step2880]: loss 1.041383
[epoch3, step2881]: loss 0.806038
[epoch3, step2882]: loss 0.857956
[epoch3, step2883]: loss 0.764141
[epoch3, step2884]: loss 1.074198
[epoch3, step2885]: loss 0.709929
[epoch3, step2886]: loss 1.019352
[epoch3, step2887]: loss 0.753137
[epoch3, step2888]: loss 0.563968
[epoch3, step2889]: loss 0.836417
[epoch3, step2890]: loss 1.080535
[epoch3, step2891]: loss 0.771788
[epoch3, step2892]: loss 0.750476
[epoch3, step2893]: loss 0.843947
[epoch3, step2894]: loss 0.859117
[epoch3, step2895]: loss 0.907239
[epoch3, step2896]: loss 0.900345
[epoch3, step2897]: loss 0.734105
[epoch3, step2898]: loss 0.909412
[epoch3, step2899]: loss 1.032598
[epoch3, step2900]: loss 0.632553
[epoch3, step2901]: loss 0.807344
[epoch3, step2902]: loss 0.862935
[epoch3, step2903]: loss 0.942818
[epoch3, step2904]: loss 0.885276
[epoch3, step2905]: loss 0.889343
[epoch3, step2906]: loss 0.756307
[epoch3, step2907]: loss 0.669794
[epoch3, step2908]: loss 0.625317
[epoch3, step2909]: loss 0.800675
[epoch3, step2910]: loss 0.945768
[epoch3, step2911]: loss 0.490473
[epoch3, step2912]: loss 0.814946
[epoch3, step2913]: loss 0.715557
[epoch3, step2914]: loss 1.003552
[epoch3, step2915]: loss 0.756323
[epoch3, step2916]: loss 0.725282
[epoch3, step2917]: loss 0.722661
[epoch3, step2918]: loss 1.243224
[epoch3, step2919]: loss 0.795443
[epoch3, step2920]: loss 0.350711
[epoch3, step2921]: loss 0.823793
[epoch3, step2922]: loss 0.620759
[epoch3, step2923]: loss 0.860546
[epoch3, step2924]: loss 0.747237
[epoch3, step2925]: loss 0.921244
[epoch3, step2926]: loss 1.091620
[epoch3, step2927]: loss 0.892929
[epoch3, step2928]: loss 0.636780
[epoch3, step2929]: loss 0.895261
[epoch3, step2930]: loss 0.780058
[epoch3, step2931]: loss 0.958507
[epoch3, step2932]: loss 0.713717
[epoch3, step2933]: loss 0.870385
[epoch3, step2934]: loss 0.962802
[epoch3, step2935]: loss 0.836362
[epoch3, step2936]: loss 0.496289
[epoch3, step2937]: loss 0.687264
[epoch3, step2938]: loss 0.840300
[epoch3, step2939]: loss 0.898566
[epoch3, step2940]: loss 0.912482
[epoch3, step2941]: loss 1.001759
[epoch3, step2942]: loss 0.878994
[epoch3, step2943]: loss 1.086289
[epoch3, step2944]: loss 0.566865
[epoch3, step2945]: loss 0.812760
[epoch3, step2946]: loss 0.823102
[epoch3, step2947]: loss 0.753044
[epoch3, step2948]: loss 0.695806
[epoch3, step2949]: loss 0.776242
[epoch3, step2950]: loss 0.822426
[epoch3, step2951]: loss 0.791149
[epoch3, step2952]: loss 0.742407
[epoch3, step2953]: loss 0.906672
[epoch3, step2954]: loss 0.576819
[epoch3, step2955]: loss 1.069828
[epoch3, step2956]: loss 0.798470
[epoch3, step2957]: loss 0.890330
[epoch3, step2958]: loss 1.020749
[epoch3, step2959]: loss 0.724222
[epoch3, step2960]: loss 0.785138
[epoch3, step2961]: loss 1.031737
[epoch3, step2962]: loss 0.752421
[epoch3, step2963]: loss 0.967834
[epoch3, step2964]: loss 0.815694
[epoch3, step2965]: loss 0.828228
[epoch3, step2966]: loss 0.555280
[epoch3, step2967]: loss 0.804018
[epoch3, step2968]: loss 0.767516
[epoch3, step2969]: loss 0.794684
[epoch3, step2970]: loss 0.750022
[epoch3, step2971]: loss 0.897653
[epoch3, step2972]: loss 0.925997
[epoch3, step2973]: loss 0.814704
[epoch3, step2974]: loss 0.937879
[epoch3, step2975]: loss 0.765518
[epoch3, step2976]: loss 0.906120
[epoch3, step2977]: loss 0.899041
[epoch3, step2978]: loss 0.711744
[epoch3, step2979]: loss 0.850063
[epoch3, step2980]: loss 0.765272
[epoch3, step2981]: loss 0.791957
[epoch3, step2982]: loss 0.463050
[epoch3, step2983]: loss 0.864924
[epoch3, step2984]: loss 1.083604
[epoch3, step2985]: loss 0.903038
[epoch3, step2986]: loss 1.201222
[epoch3, step2987]: loss 0.939392
[epoch3, step2988]: loss 0.879338
[epoch3, step2989]: loss 0.758713
[epoch3, step2990]: loss 1.146863
[epoch3, step2991]: loss 0.890428
[epoch3, step2992]: loss 0.945939
[epoch3, step2993]: loss 0.784926
[epoch3, step2994]: loss 0.897462
[epoch3, step2995]: loss 0.920516
[epoch3, step2996]: loss 0.472889
[epoch3, step2997]: loss 0.774356
[epoch3, step2998]: loss 0.763549
[epoch3, step2999]: loss 0.787512
[epoch3, step3000]: loss 0.886151
[epoch3, step3001]: loss 0.876328
[epoch3, step3002]: loss 0.919568
[epoch3, step3003]: loss 0.582884
[epoch3, step3004]: loss 0.839875
[epoch3, step3005]: loss 0.958964
[epoch3, step3006]: loss 0.720954
[epoch3, step3007]: loss 0.474186
[epoch3, step3008]: loss 0.916693
[epoch3, step3009]: loss 0.507248
[epoch3, step3010]: loss 0.686734
[epoch3, step3011]: loss 1.049669
[epoch3, step3012]: loss 0.890872
[epoch3, step3013]: loss 0.964557
[epoch3, step3014]: loss 1.048116
[epoch3, step3015]: loss 0.549204
[epoch3, step3016]: loss 0.818349
[epoch3, step3017]: loss 0.850682
[epoch3, step3018]: loss 1.159633
[epoch3, step3019]: loss 1.030532
[epoch3, step3020]: loss 0.971774
[epoch3, step3021]: loss 0.904656
[epoch3, step3022]: loss 0.986986
[epoch3, step3023]: loss 0.656380
[epoch3, step3024]: loss 0.777409
[epoch3, step3025]: loss 0.874864
[epoch3, step3026]: loss 0.757127
[epoch3, step3027]: loss 0.618864
[epoch3, step3028]: loss 0.750330
[epoch3, step3029]: loss 1.052887
[epoch3, step3030]: loss 0.744830
[epoch3, step3031]: loss 0.578445
[epoch3, step3032]: loss 0.807774
[epoch3, step3033]: loss 1.073536
[epoch3, step3034]: loss 0.836508
[epoch3, step3035]: loss 0.781690
[epoch3, step3036]: loss 0.661022
[epoch3, step3037]: loss 1.007135
[epoch3, step3038]: loss 0.738042
[epoch3, step3039]: loss 0.679677
[epoch3, step3040]: loss 0.763026
[epoch3, step3041]: loss 0.852706
[epoch3, step3042]: loss 0.882597
[epoch3, step3043]: loss 0.965113
[epoch3, step3044]: loss 1.018788
[epoch3, step3045]: loss 0.650276
[epoch3, step3046]: loss 0.899950
[epoch3, step3047]: loss 0.661564
[epoch3, step3048]: loss 0.851370
[epoch3, step3049]: loss 1.046735
[epoch3, step3050]: loss 0.772584
[epoch3, step3051]: loss 0.977430
[epoch3, step3052]: loss 0.599435
[epoch3, step3053]: loss 0.833444
[epoch3, step3054]: loss 0.876589
[epoch3, step3055]: loss 0.390580
[epoch3, step3056]: loss 0.763723
[epoch3, step3057]: loss 1.036561
[epoch3, step3058]: loss 0.620560
[epoch3, step3059]: loss 0.662585
[epoch3, step3060]: loss 0.915303
[epoch3, step3061]: loss 0.805165
[epoch3, step3062]: loss 0.972014
[epoch3, step3063]: loss 0.700840
[epoch3, step3064]: loss 1.128789
[epoch3, step3065]: loss 1.114286
[epoch3, step3066]: loss 0.784223
[epoch3, step3067]: loss 0.821159
[epoch3, step3068]: loss 1.006900
[epoch3, step3069]: loss 0.783274
[epoch3, step3070]: loss 0.807110
[epoch3, step3071]: loss 0.955991
[epoch3, step3072]: loss 0.613507
[epoch3, step3073]: loss 0.837659
[epoch3, step3074]: loss 1.070953
[epoch3, step3075]: loss 0.697089
[epoch3, step3076]: loss 0.919077

[epoch3]: avg loss 0.919077

[epoch4, step1]: loss 0.684007
[epoch4, step2]: loss 0.700979
[epoch4, step3]: loss 0.780663
[epoch4, step4]: loss 0.698590
[epoch4, step5]: loss 0.689508
[epoch4, step6]: loss 0.981011
[epoch4, step7]: loss 1.018575
[epoch4, step8]: loss 0.771391
[epoch4, step9]: loss 0.831238
[epoch4, step10]: loss 1.108020
[epoch4, step11]: loss 0.772055
[epoch4, step12]: loss 0.665056
[epoch4, step13]: loss 0.663648
[epoch4, step14]: loss 0.922966
[epoch4, step15]: loss 0.900499
[epoch4, step16]: loss 0.945531
[epoch4, step17]: loss 0.695238
[epoch4, step18]: loss 0.812459
[epoch4, step19]: loss 0.822622
[epoch4, step20]: loss 0.965730
[epoch4, step21]: loss 0.808394
[epoch4, step22]: loss 0.953165
[epoch4, step23]: loss 0.806164
[epoch4, step24]: loss 0.723563
[epoch4, step25]: loss 0.841991
[epoch4, step26]: loss 0.834881
[epoch4, step27]: loss 0.906686
[epoch4, step28]: loss 0.813440
[epoch4, step29]: loss 1.009074
[epoch4, step30]: loss 0.992003
[epoch4, step31]: loss 0.999340
[epoch4, step32]: loss 0.933667
[epoch4, step33]: loss 1.030543
[epoch4, step34]: loss 0.940948
[epoch4, step35]: loss 0.670770
[epoch4, step36]: loss 1.059793
[epoch4, step37]: loss 0.830359
[epoch4, step38]: loss 1.031522
[epoch4, step39]: loss 0.924869
[epoch4, step40]: loss 0.537876
[epoch4, step41]: loss 1.025087
[epoch4, step42]: loss 1.049434
[epoch4, step43]: loss 0.825002
[epoch4, step44]: loss 0.935110
[epoch4, step45]: loss 0.641181
[epoch4, step46]: loss 0.858643
[epoch4, step47]: loss 1.015143
[epoch4, step48]: loss 0.937281
[epoch4, step49]: loss 0.869407
[epoch4, step50]: loss 0.494887
[epoch4, step51]: loss 0.788064
[epoch4, step52]: loss 0.444022
[epoch4, step53]: loss 0.783125
[epoch4, step54]: loss 0.884208
[epoch4, step55]: loss 0.882531
[epoch4, step56]: loss 0.972790
[epoch4, step57]: loss 0.948816
[epoch4, step58]: loss 0.903005
[epoch4, step59]: loss 0.675037
[epoch4, step60]: loss 0.851321
[epoch4, step61]: loss 0.809007
[epoch4, step62]: loss 0.508982
[epoch4, step63]: loss 1.144587
[epoch4, step64]: loss 1.063022
[epoch4, step65]: loss 0.728798
[epoch4, step66]: loss 0.731124
[epoch4, step67]: loss 0.756141
[epoch4, step68]: loss 0.842194
[epoch4, step69]: loss 0.706394
[epoch4, step70]: loss 0.961960
[epoch4, step71]: loss 1.016011
[epoch4, step72]: loss 0.854481
[epoch4, step73]: loss 0.891516
[epoch4, step74]: loss 0.663321
[epoch4, step75]: loss 0.809931
[epoch4, step76]: loss 1.029939
[epoch4, step77]: loss 0.805590
[epoch4, step78]: loss 0.816229
[epoch4, step79]: loss 0.982324
[epoch4, step80]: loss 0.925395
[epoch4, step81]: loss 0.957270
[epoch4, step82]: loss 0.474344
[epoch4, step83]: loss 0.936955
[epoch4, step84]: loss 0.623455
[epoch4, step85]: loss 0.753596
[epoch4, step86]: loss 0.721761
[epoch4, step87]: loss 0.963729
[epoch4, step88]: loss 0.842233
[epoch4, step89]: loss 0.947305
[epoch4, step90]: loss 0.414020
[epoch4, step91]: loss 0.727914
[epoch4, step92]: loss 0.575984
[epoch4, step93]: loss 1.007995
[epoch4, step94]: loss 0.836466
[epoch4, step95]: loss 0.947041
[epoch4, step96]: loss 0.812994
[epoch4, step97]: loss 0.970582
[epoch4, step98]: loss 0.839630
[epoch4, step99]: loss 0.520926
[epoch4, step100]: loss 0.837293
[epoch4, step101]: loss 0.921493
[epoch4, step102]: loss 0.875610
[epoch4, step103]: loss 0.803136
[epoch4, step104]: loss 0.623032
[epoch4, step105]: loss 0.745518
[epoch4, step106]: loss 0.785538
[epoch4, step107]: loss 0.791421
[epoch4, step108]: loss 0.820391
[epoch4, step109]: loss 0.871193
[epoch4, step110]: loss 0.694378
[epoch4, step111]: loss 0.789545
[epoch4, step112]: loss 0.774644
[epoch4, step113]: loss 1.072477
[epoch4, step114]: loss 0.531868
[epoch4, step115]: loss 0.790193
[epoch4, step116]: loss 0.761972
[epoch4, step117]: loss 0.946781
[epoch4, step118]: loss 0.986186
[epoch4, step119]: loss 0.954911
[epoch4, step120]: loss 0.533036
[epoch4, step121]: loss 0.723077
[epoch4, step122]: loss 0.853255
[epoch4, step123]: loss 0.841730
[epoch4, step124]: loss 0.666148
[epoch4, step125]: loss 0.632589
[epoch4, step126]: loss 0.595458
[epoch4, step127]: loss 0.960418
[epoch4, step128]: loss 0.785284
[epoch4, step129]: loss 0.990918
[epoch4, step130]: loss 0.816836
[epoch4, step131]: loss 0.382313
[epoch4, step132]: loss 0.905621
[epoch4, step133]: loss 0.797540
[epoch4, step134]: loss 0.614065
[epoch4, step135]: loss 0.949304
[epoch4, step136]: loss 0.697998
[epoch4, step137]: loss 0.961232
[epoch4, step138]: loss 0.887305
[epoch4, step139]: loss 0.736699
[epoch4, step140]: loss 0.739619
[epoch4, step141]: loss 0.850594
[epoch4, step142]: loss 0.763860
[epoch4, step143]: loss 1.067624
[epoch4, step144]: loss 0.899717
[epoch4, step145]: loss 0.545515
[epoch4, step146]: loss 0.783472
[epoch4, step147]: loss 0.908805
[epoch4, step148]: loss 0.996268
[epoch4, step149]: loss 0.832349
[epoch4, step150]: loss 0.938653
[epoch4, step151]: loss 0.593784
[epoch4, step152]: loss 0.877429
[epoch4, step153]: loss 0.737501
[epoch4, step154]: loss 0.950579
[epoch4, step155]: loss 0.862325
[epoch4, step156]: loss 0.764275
[epoch4, step157]: loss 0.813210
[epoch4, step158]: loss 0.561088
[epoch4, step159]: loss 0.908647
[epoch4, step160]: loss 1.121315
[epoch4, step161]: loss 0.932586
[epoch4, step162]: loss 0.502285
[epoch4, step163]: loss 1.019229
[epoch4, step164]: loss 0.948327
[epoch4, step165]: loss 0.806034
[epoch4, step166]: loss 1.067592
[epoch4, step167]: loss 1.018285
[epoch4, step168]: loss 0.814605
[epoch4, step169]: loss 0.658516
[epoch4, step170]: loss 0.557655
[epoch4, step171]: loss 0.762136
[epoch4, step172]: loss 0.841068
[epoch4, step173]: loss 0.544361
[epoch4, step174]: loss 0.999883
[epoch4, step175]: loss 0.626704
[epoch4, step176]: loss 0.855370
[epoch4, step177]: loss 0.796999
[epoch4, step178]: loss 0.848092
[epoch4, step179]: loss 0.683996
[epoch4, step180]: loss 1.029737
[epoch4, step181]: loss 0.931469
[epoch4, step182]: loss 1.003081
[epoch4, step183]: loss 1.106390
[epoch4, step184]: loss 1.050068
[epoch4, step185]: loss 0.999731
[epoch4, step186]: loss 0.954723
[epoch4, step187]: loss 0.532706
[epoch4, step188]: loss 0.785152
[epoch4, step189]: loss 0.664029
[epoch4, step190]: loss 0.821068
[epoch4, step191]: loss 0.742116
[epoch4, step192]: loss 0.452372
[epoch4, step193]: loss 0.917920
[epoch4, step194]: loss 0.826695
[epoch4, step195]: loss 0.865170
[epoch4, step196]: loss 0.742646
[epoch4, step197]: loss 0.916208
[epoch4, step198]: loss 0.709683
[epoch4, step199]: loss 0.692099
[epoch4, step200]: loss 1.051562
[epoch4, step201]: loss 0.887465
[epoch4, step202]: loss 1.048491
[epoch4, step203]: loss 0.654726
[epoch4, step204]: loss 0.896076
[epoch4, step205]: loss 0.726423
[epoch4, step206]: loss 1.084692
[epoch4, step207]: loss 1.103798
[epoch4, step208]: loss 0.586350
[epoch4, step209]: loss 1.102571
[epoch4, step210]: loss 0.824754
[epoch4, step211]: loss 1.144722
[epoch4, step212]: loss 0.938347
[epoch4, step213]: loss 0.877091
[epoch4, step214]: loss 0.956248
[epoch4, step215]: loss 0.738212
[epoch4, step216]: loss 0.961222
[epoch4, step217]: loss 0.958755
[epoch4, step218]: loss 1.093851
[epoch4, step219]: loss 0.845602
[epoch4, step220]: loss 0.996440
[epoch4, step221]: loss 0.870319
[epoch4, step222]: loss 0.948674
[epoch4, step223]: loss 0.778816
[epoch4, step224]: loss 0.682936
[epoch4, step225]: loss 0.652334
[epoch4, step226]: loss 0.754342
[epoch4, step227]: loss 0.586466
[epoch4, step228]: loss 0.909269
[epoch4, step229]: loss 0.801899
[epoch4, step230]: loss 0.813510
[epoch4, step231]: loss 0.564145
[epoch4, step232]: loss 0.787608
[epoch4, step233]: loss 1.110062
[epoch4, step234]: loss 0.706838
[epoch4, step235]: loss 0.884411
[epoch4, step236]: loss 0.792312
[epoch4, step237]: loss 0.852597
[epoch4, step238]: loss 0.811750
[epoch4, step239]: loss 1.092816
[epoch4, step240]: loss 0.496863
[epoch4, step241]: loss 0.962912
[epoch4, step242]: loss 0.828711
[epoch4, step243]: loss 0.984905
[epoch4, step244]: loss 1.180762
[epoch4, step245]: loss 0.916044
[epoch4, step246]: loss 0.922807
[epoch4, step247]: loss 0.895307
[epoch4, step248]: loss 0.653126
[epoch4, step249]: loss 0.741401
[epoch4, step250]: loss 0.634955
[epoch4, step251]: loss 0.691991
[epoch4, step252]: loss 0.698829
[epoch4, step253]: loss 0.647581
[epoch4, step254]: loss 1.170219
[epoch4, step255]: loss 1.041561
[epoch4, step256]: loss 0.964418
[epoch4, step257]: loss 0.748375
[epoch4, step258]: loss 0.668042
[epoch4, step259]: loss 0.704600
[epoch4, step260]: loss 0.871336
[epoch4, step261]: loss 0.838009
[epoch4, step262]: loss 0.812991
[epoch4, step263]: loss 0.589493
[epoch4, step264]: loss 0.705272
[epoch4, step265]: loss 1.014148
[epoch4, step266]: loss 0.906838
[epoch4, step267]: loss 0.946048
[epoch4, step268]: loss 0.868659
[epoch4, step269]: loss 0.674636
[epoch4, step270]: loss 0.677892
[epoch4, step271]: loss 1.131702
[epoch4, step272]: loss 0.699192
[epoch4, step273]: loss 0.857851
[epoch4, step274]: loss 0.839337
[epoch4, step275]: loss 0.836268
[epoch4, step276]: loss 0.888894
[epoch4, step277]: loss 0.739570
[epoch4, step278]: loss 0.760459
[epoch4, step279]: loss 1.075188
[epoch4, step280]: loss 0.702809
[epoch4, step281]: loss 1.102288
[epoch4, step282]: loss 0.934457
[epoch4, step283]: loss 0.935434
[epoch4, step284]: loss 0.479055
[epoch4, step285]: loss 1.162684
[epoch4, step286]: loss 0.910122
[epoch4, step287]: loss 0.829671
[epoch4, step288]: loss 0.927657
[epoch4, step289]: loss 0.910379
[epoch4, step290]: loss 0.800775
[epoch4, step291]: loss 1.116768
[epoch4, step292]: loss 0.645080
[epoch4, step293]: loss 0.851681
[epoch4, step294]: loss 0.607405
[epoch4, step295]: loss 0.883338
[epoch4, step296]: loss 0.937722
[epoch4, step297]: loss 0.847721
[epoch4, step298]: loss 0.967045
[epoch4, step299]: loss 0.603222
[epoch4, step300]: loss 0.714242
[epoch4, step301]: loss 0.570470
[epoch4, step302]: loss 0.986178
[epoch4, step303]: loss 1.056426
[epoch4, step304]: loss 0.803694
[epoch4, step305]: loss 0.910107
[epoch4, step306]: loss 0.688180
[epoch4, step307]: loss 1.052302
[epoch4, step308]: loss 0.613294
[epoch4, step309]: loss 0.737712
[epoch4, step310]: loss 0.713026
[epoch4, step311]: loss 0.864032
[epoch4, step312]: loss 0.685261
[epoch4, step313]: loss 0.843731
[epoch4, step314]: loss 0.821791
[epoch4, step315]: loss 1.029056
[epoch4, step316]: loss 0.861919
[epoch4, step317]: loss 0.876728
[epoch4, step318]: loss 0.776651
[epoch4, step319]: loss 0.828553
[epoch4, step320]: loss 1.003140
[epoch4, step321]: loss 0.880136
[epoch4, step322]: loss 0.768596
[epoch4, step323]: loss 0.865089
[epoch4, step324]: loss 0.710856
[epoch4, step325]: loss 1.170919
[epoch4, step326]: loss 0.583800
[epoch4, step327]: loss 0.882984
[epoch4, step328]: loss 0.944156
[epoch4, step329]: loss 0.789447
[epoch4, step330]: loss 0.944833
[epoch4, step331]: loss 0.974886
[epoch4, step332]: loss 0.695133
[epoch4, step333]: loss 1.060976
[epoch4, step334]: loss 0.703887
[epoch4, step335]: loss 0.844293
[epoch4, step336]: loss 0.846011
[epoch4, step337]: loss 1.063047
[epoch4, step338]: loss 0.913292
[epoch4, step339]: loss 0.855718
[epoch4, step340]: loss 0.860623
[epoch4, step341]: loss 1.145296
[epoch4, step342]: loss 0.834698
[epoch4, step343]: loss 0.860433
[epoch4, step344]: loss 0.586859
[epoch4, step345]: loss 1.043988
[epoch4, step346]: loss 0.726995
[epoch4, step347]: loss 0.630100
[epoch4, step348]: loss 1.045102
[epoch4, step349]: loss 0.965534
[epoch4, step350]: loss 0.945311
[epoch4, step351]: loss 0.544396
[epoch4, step352]: loss 0.691005
[epoch4, step353]: loss 0.843579
[epoch4, step354]: loss 0.619456
[epoch4, step355]: loss 0.959893
[epoch4, step356]: loss 0.665809
[epoch4, step357]: loss 1.081346
[epoch4, step358]: loss 0.832817
[epoch4, step359]: loss 0.802973
[epoch4, step360]: loss 0.705417
[epoch4, step361]: loss 0.680259
[epoch4, step362]: loss 0.880074
[epoch4, step363]: loss 0.658039
[epoch4, step364]: loss 0.671673
[epoch4, step365]: loss 0.721859
[epoch4, step366]: loss 0.877497
[epoch4, step367]: loss 0.869453
[epoch4, step368]: loss 0.583398
[epoch4, step369]: loss 0.961851
[epoch4, step370]: loss 0.713880
[epoch4, step371]: loss 0.975419
[epoch4, step372]: loss 0.924711
[epoch4, step373]: loss 0.635409
[epoch4, step374]: loss 0.871513
[epoch4, step375]: loss 0.828812
[epoch4, step376]: loss 0.699495
[epoch4, step377]: loss 0.681968
[epoch4, step378]: loss 0.981435
[epoch4, step379]: loss 0.815420
[epoch4, step380]: loss 0.824361
[epoch4, step381]: loss 1.057117
[epoch4, step382]: loss 1.167366
[epoch4, step383]: loss 0.915880
[epoch4, step384]: loss 0.998896
[epoch4, step385]: loss 0.997912
[epoch4, step386]: loss 0.719127
[epoch4, step387]: loss 0.620094
[epoch4, step388]: loss 0.912802
[epoch4, step389]: loss 0.952483
[epoch4, step390]: loss 0.739125
[epoch4, step391]: loss 1.032798
[epoch4, step392]: loss 0.823821
[epoch4, step393]: loss 0.775076
[epoch4, step394]: loss 0.580815
[epoch4, step395]: loss 0.456849
[epoch4, step396]: loss 0.657633
[epoch4, step397]: loss 0.744829
[epoch4, step398]: loss 0.794694
[epoch4, step399]: loss 0.668423
[epoch4, step400]: loss 0.857407
[epoch4, step401]: loss 0.873014
[epoch4, step402]: loss 0.572132
[epoch4, step403]: loss 0.658253
[epoch4, step404]: loss 0.927129
[epoch4, step405]: loss 0.609665
[epoch4, step406]: loss 0.856365
[epoch4, step407]: loss 0.843692
[epoch4, step408]: loss 0.772766
[epoch4, step409]: loss 0.679065
[epoch4, step410]: loss 0.930566
[epoch4, step411]: loss 1.001142
[epoch4, step412]: loss 1.056022
[epoch4, step413]: loss 0.780405
[epoch4, step414]: loss 1.001209
[epoch4, step415]: loss 0.709788
[epoch4, step416]: loss 0.753877
[epoch4, step417]: loss 0.387353
[epoch4, step418]: loss 0.764874
[epoch4, step419]: loss 0.865281
[epoch4, step420]: loss 0.847711
[epoch4, step421]: loss 0.802934
[epoch4, step422]: loss 0.869069
[epoch4, step423]: loss 0.746032
[epoch4, step424]: loss 0.861374
[epoch4, step425]: loss 1.129513
[epoch4, step426]: loss 0.815120
[epoch4, step427]: loss 0.911448
[epoch4, step428]: loss 1.101388
[epoch4, step429]: loss 0.758829
[epoch4, step430]: loss 0.733239
[epoch4, step431]: loss 0.772297
[epoch4, step432]: loss 1.012591
[epoch4, step433]: loss 0.997262
[epoch4, step434]: loss 1.057729
[epoch4, step435]: loss 0.665744
[epoch4, step436]: loss 1.072280
[epoch4, step437]: loss 0.837873
[epoch4, step438]: loss 0.931463
[epoch4, step439]: loss 0.680385
[epoch4, step440]: loss 0.813309
[epoch4, step441]: loss 0.619115
[epoch4, step442]: loss 0.700186
[epoch4, step443]: loss 0.651663
[epoch4, step444]: loss 0.954741
[epoch4, step445]: loss 1.044244
[epoch4, step446]: loss 0.646318
[epoch4, step447]: loss 0.881772
[epoch4, step448]: loss 0.902173
[epoch4, step449]: loss 0.712344
[epoch4, step450]: loss 0.929411
[epoch4, step451]: loss 0.621792
[epoch4, step452]: loss 0.797052
[epoch4, step453]: loss 0.654124
[epoch4, step454]: loss 1.017276
[epoch4, step455]: loss 0.720653
[epoch4, step456]: loss 0.811689
[epoch4, step457]: loss 0.883810
[epoch4, step458]: loss 0.877747
[epoch4, step459]: loss 0.882815
[epoch4, step460]: loss 0.603069
[epoch4, step461]: loss 0.757301
[epoch4, step462]: loss 0.995597
[epoch4, step463]: loss 0.600868
[epoch4, step464]: loss 0.864085
[epoch4, step465]: loss 0.718074
[epoch4, step466]: loss 0.782695
[epoch4, step467]: loss 0.874057
[epoch4, step468]: loss 0.934074
[epoch4, step469]: loss 0.887936
[epoch4, step470]: loss 0.901180
[epoch4, step471]: loss 0.648503
[epoch4, step472]: loss 0.819280
[epoch4, step473]: loss 0.950626
[epoch4, step474]: loss 0.920928
[epoch4, step475]: loss 0.946342
[epoch4, step476]: loss 0.780174
[epoch4, step477]: loss 1.029840
[epoch4, step478]: loss 0.980858
[epoch4, step479]: loss 0.785400
[epoch4, step480]: loss 0.710225
[epoch4, step481]: loss 0.685597
[epoch4, step482]: loss 0.859492
[epoch4, step483]: loss 0.810188
[epoch4, step484]: loss 0.893638
[epoch4, step485]: loss 0.572907
[epoch4, step486]: loss 0.997680
[epoch4, step487]: loss 0.550254
[epoch4, step488]: loss 0.854443
[epoch4, step489]: loss 0.859555
[epoch4, step490]: loss 0.740450
[epoch4, step491]: loss 0.567099
[epoch4, step492]: loss 0.623439
[epoch4, step493]: loss 0.735866
[epoch4, step494]: loss 0.547628
[epoch4, step495]: loss 0.475088
[epoch4, step496]: loss 0.642823
[epoch4, step497]: loss 0.865207
[epoch4, step498]: loss 1.009271
[epoch4, step499]: loss 0.875020
[epoch4, step500]: loss 0.761497
[epoch4, step501]: loss 0.859714
[epoch4, step502]: loss 0.439624
[epoch4, step503]: loss 0.858529
[epoch4, step504]: loss 1.134605
[epoch4, step505]: loss 0.953396
[epoch4, step506]: loss 1.027959
[epoch4, step507]: loss 0.943934
[epoch4, step508]: loss 0.827509
[epoch4, step509]: loss 0.744263
[epoch4, step510]: loss 0.351012
[epoch4, step511]: loss 0.680985
[epoch4, step512]: loss 0.890197
[epoch4, step513]: loss 0.777034
[epoch4, step514]: loss 0.776632
[epoch4, step515]: loss 0.700363
[epoch4, step516]: loss 0.769021
[epoch4, step517]: loss 0.601806
[epoch4, step518]: loss 1.080272
[epoch4, step519]: loss 0.605842
[epoch4, step520]: loss 0.970690
[epoch4, step521]: loss 0.863498
[epoch4, step522]: loss 0.473242
[epoch4, step523]: loss 0.902056
[epoch4, step524]: loss 0.936727
[epoch4, step525]: loss 0.888941
[epoch4, step526]: loss 0.848134
[epoch4, step527]: loss 0.771418
[epoch4, step528]: loss 0.717977
[epoch4, step529]: loss 0.928949
[epoch4, step530]: loss 0.696817
[epoch4, step531]: loss 0.885857
[epoch4, step532]: loss 1.017929
[epoch4, step533]: loss 0.655627
[epoch4, step534]: loss 0.943503
[epoch4, step535]: loss 0.983167
[epoch4, step536]: loss 0.621055
[epoch4, step537]: loss 1.045979
[epoch4, step538]: loss 0.784513
[epoch4, step539]: loss 0.868677
[epoch4, step540]: loss 0.622352
[epoch4, step541]: loss 0.629410
[epoch4, step542]: loss 0.504110
[epoch4, step543]: loss 1.120090
[epoch4, step544]: loss 0.853240
[epoch4, step545]: loss 0.845226
[epoch4, step546]: loss 0.811954
[epoch4, step547]: loss 0.526555
[epoch4, step548]: loss 0.841847
[epoch4, step549]: loss 0.839745
[epoch4, step550]: loss 0.772394
[epoch4, step551]: loss 0.650636
[epoch4, step552]: loss 0.744096
[epoch4, step553]: loss 0.824305
[epoch4, step554]: loss 0.912193
[epoch4, step555]: loss 0.814188
[epoch4, step556]: loss 0.982747
[epoch4, step557]: loss 0.819254
[epoch4, step558]: loss 1.130501
[epoch4, step559]: loss 0.625400
[epoch4, step560]: loss 0.913653
[epoch4, step561]: loss 0.823040
[epoch4, step562]: loss 0.835897
[epoch4, step563]: loss 0.885710
[epoch4, step564]: loss 0.814701
[epoch4, step565]: loss 0.580929
[epoch4, step566]: loss 0.933422
[epoch4, step567]: loss 0.716193
[epoch4, step568]: loss 0.888735
[epoch4, step569]: loss 0.537883
[epoch4, step570]: loss 0.677795
[epoch4, step571]: loss 0.793401
[epoch4, step572]: loss 0.941030
[epoch4, step573]: loss 0.834911
[epoch4, step574]: loss 0.867152
[epoch4, step575]: loss 0.757513
[epoch4, step576]: loss 0.406563
[epoch4, step577]: loss 0.822681
[epoch4, step578]: loss 0.773042
[epoch4, step579]: loss 0.828358
[epoch4, step580]: loss 0.891035
[epoch4, step581]: loss 0.506245
[epoch4, step582]: loss 0.851761
[epoch4, step583]: loss 0.770243
[epoch4, step584]: loss 0.786805
[epoch4, step585]: loss 1.088360
[epoch4, step586]: loss 0.917072
[epoch4, step587]: loss 0.824965
[epoch4, step588]: loss 0.795619
[epoch4, step589]: loss 0.963807
[epoch4, step590]: loss 0.766788
[epoch4, step591]: loss 1.004538
[epoch4, step592]: loss 0.799439
[epoch4, step593]: loss 0.792525
[epoch4, step594]: loss 1.047039
[epoch4, step595]: loss 0.606978
[epoch4, step596]: loss 0.634310
[epoch4, step597]: loss 0.687816
[epoch4, step598]: loss 0.774473
[epoch4, step599]: loss 0.874985
[epoch4, step600]: loss 1.029373
[epoch4, step601]: loss 0.796482
[epoch4, step602]: loss 0.729057
[epoch4, step603]: loss 0.805063
[epoch4, step604]: loss 0.815929
[epoch4, step605]: loss 0.765899
[epoch4, step606]: loss 0.997184
[epoch4, step607]: loss 0.845160
[epoch4, step608]: loss 0.591173
[epoch4, step609]: loss 0.808082
[epoch4, step610]: loss 0.747744
[epoch4, step611]: loss 0.846122
[epoch4, step612]: loss 0.898773
[epoch4, step613]: loss 1.137114
[epoch4, step614]: loss 1.066859
[epoch4, step615]: loss 1.135810
[epoch4, step616]: loss 0.775720
[epoch4, step617]: loss 0.845268
[epoch4, step618]: loss 0.835213
[epoch4, step619]: loss 0.933566
[epoch4, step620]: loss 0.911830
[epoch4, step621]: loss 0.771320
[epoch4, step622]: loss 0.866658
[epoch4, step623]: loss 0.612579
[epoch4, step624]: loss 1.217891
[epoch4, step625]: loss 0.706222
[epoch4, step626]: loss 1.048816
[epoch4, step627]: loss 0.909249
[epoch4, step628]: loss 0.657615
[epoch4, step629]: loss 0.484172
[epoch4, step630]: loss 0.874995
[epoch4, step631]: loss 0.946039
[epoch4, step632]: loss 0.702585
[epoch4, step633]: loss 0.716832
[epoch4, step634]: loss 0.886864
[epoch4, step635]: loss 0.695022
[epoch4, step636]: loss 0.903342
[epoch4, step637]: loss 1.047247
[epoch4, step638]: loss 0.905945
[epoch4, step639]: loss 0.987347
[epoch4, step640]: loss 0.856564
[epoch4, step641]: loss 0.817051
[epoch4, step642]: loss 0.872317
[epoch4, step643]: loss 0.596631
[epoch4, step644]: loss 0.954187
[epoch4, step645]: loss 0.912371
[epoch4, step646]: loss 0.698524
[epoch4, step647]: loss 0.998855
[epoch4, step648]: loss 0.835404
[epoch4, step649]: loss 0.917993
[epoch4, step650]: loss 0.759351
[epoch4, step651]: loss 0.707972
[epoch4, step652]: loss 0.764030
[epoch4, step653]: loss 1.023158
[epoch4, step654]: loss 0.717450
[epoch4, step655]: loss 0.799002
[epoch4, step656]: loss 0.992575
[epoch4, step657]: loss 0.810837
[epoch4, step658]: loss 0.787594
[epoch4, step659]: loss 0.813784
[epoch4, step660]: loss 0.734057
[epoch4, step661]: loss 0.865575
[epoch4, step662]: loss 1.058598
[epoch4, step663]: loss 0.964264
[epoch4, step664]: loss 0.742868
[epoch4, step665]: loss 0.749289
[epoch4, step666]: loss 0.790842
[epoch4, step667]: loss 0.791238
[epoch4, step668]: loss 0.926892
[epoch4, step669]: loss 1.102676
[epoch4, step670]: loss 0.751006
[epoch4, step671]: loss 1.026571
[epoch4, step672]: loss 1.173125
[epoch4, step673]: loss 0.581200
[epoch4, step674]: loss 0.725668
[epoch4, step675]: loss 0.914360
[epoch4, step676]: loss 0.856985
[epoch4, step677]: loss 0.798907
[epoch4, step678]: loss 0.888529
[epoch4, step679]: loss 0.843084
[epoch4, step680]: loss 0.828092
[epoch4, step681]: loss 0.738947
[epoch4, step682]: loss 0.812591
[epoch4, step683]: loss 1.166616
[epoch4, step684]: loss 0.369446
[epoch4, step685]: loss 0.997703
[epoch4, step686]: loss 0.449772
[epoch4, step687]: loss 0.522657
[epoch4, step688]: loss 0.883168
[epoch4, step689]: loss 0.757214
[epoch4, step690]: loss 0.883634
[epoch4, step691]: loss 0.759146
[epoch4, step692]: loss 0.882067
[epoch4, step693]: loss 0.927790
[epoch4, step694]: loss 0.941345
[epoch4, step695]: loss 1.070684
[epoch4, step696]: loss 0.398333
[epoch4, step697]: loss 0.937920
[epoch4, step698]: loss 0.789299
[epoch4, step699]: loss 1.075513
[epoch4, step700]: loss 1.077458
[epoch4, step701]: loss 0.884913
[epoch4, step702]: loss 0.506733
[epoch4, step703]: loss 1.169724
[epoch4, step704]: loss 0.616291
[epoch4, step705]: loss 0.918681
[epoch4, step706]: loss 0.741714
[epoch4, step707]: loss 0.680855
[epoch4, step708]: loss 0.906117
[epoch4, step709]: loss 0.904700
[epoch4, step710]: loss 1.028077
[epoch4, step711]: loss 0.704093
[epoch4, step712]: loss 1.004825
[epoch4, step713]: loss 0.947509
[epoch4, step714]: loss 0.863098
[epoch4, step715]: loss 0.744841
[epoch4, step716]: loss 0.691435
[epoch4, step717]: loss 0.931779
[epoch4, step718]: loss 0.528143
[epoch4, step719]: loss 0.795672
[epoch4, step720]: loss 0.946684
[epoch4, step721]: loss 0.866493
[epoch4, step722]: loss 0.848424
[epoch4, step723]: loss 0.698688
[epoch4, step724]: loss 0.870042
[epoch4, step725]: loss 0.880748
[epoch4, step726]: loss 0.971923
[epoch4, step727]: loss 0.947349
[epoch4, step728]: loss 0.787241
[epoch4, step729]: loss 0.960895
[epoch4, step730]: loss 0.936582
[epoch4, step731]: loss 0.808739
[epoch4, step732]: loss 0.860676
[epoch4, step733]: loss 0.621376
[epoch4, step734]: loss 0.994670
[epoch4, step735]: loss 0.825012
[epoch4, step736]: loss 1.090981
[epoch4, step737]: loss 0.947957
[epoch4, step738]: loss 0.914525
[epoch4, step739]: loss 0.847330
[epoch4, step740]: loss 0.779933
[epoch4, step741]: loss 0.670486
[epoch4, step742]: loss 0.975642
[epoch4, step743]: loss 0.784525
[epoch4, step744]: loss 0.707248
[epoch4, step745]: loss 0.713290
[epoch4, step746]: loss 0.973514
[epoch4, step747]: loss 0.824050
[epoch4, step748]: loss 0.568645
[epoch4, step749]: loss 1.090232
[epoch4, step750]: loss 0.792710
[epoch4, step751]: loss 0.896754
[epoch4, step752]: loss 0.735809
[epoch4, step753]: loss 0.549888
[epoch4, step754]: loss 0.820243
[epoch4, step755]: loss 0.798698
[epoch4, step756]: loss 0.602388
[epoch4, step757]: loss 0.853239
[epoch4, step758]: loss 0.817301
[epoch4, step759]: loss 0.833646
[epoch4, step760]: loss 0.833589
[epoch4, step761]: loss 0.868999
[epoch4, step762]: loss 0.721808
[epoch4, step763]: loss 0.929947
[epoch4, step764]: loss 0.679233
[epoch4, step765]: loss 0.763827
[epoch4, step766]: loss 1.130428
[epoch4, step767]: loss 0.607590
[epoch4, step768]: loss 0.903258
[epoch4, step769]: loss 0.716806
[epoch4, step770]: loss 0.879160
[epoch4, step771]: loss 1.112440
[epoch4, step772]: loss 0.813613
[epoch4, step773]: loss 0.687794
[epoch4, step774]: loss 0.853292
[epoch4, step775]: loss 0.604867
[epoch4, step776]: loss 0.801557
[epoch4, step777]: loss 1.021200
[epoch4, step778]: loss 0.743350
[epoch4, step779]: loss 0.631405
[epoch4, step780]: loss 0.608458
[epoch4, step781]: loss 0.814161
[epoch4, step782]: loss 0.711457
[epoch4, step783]: loss 0.803427
[epoch4, step784]: loss 0.941316
[epoch4, step785]: loss 0.693478
[epoch4, step786]: loss 0.866281
[epoch4, step787]: loss 0.739816
[epoch4, step788]: loss 0.965333
[epoch4, step789]: loss 1.105394
[epoch4, step790]: loss 1.023915
[epoch4, step791]: loss 0.758186
[epoch4, step792]: loss 0.783676
[epoch4, step793]: loss 0.844808
[epoch4, step794]: loss 0.465489
[epoch4, step795]: loss 1.046983
[epoch4, step796]: loss 1.011351
[epoch4, step797]: loss 1.017004
[epoch4, step798]: loss 0.715845
[epoch4, step799]: loss 0.942464
[epoch4, step800]: loss 0.643936
[epoch4, step801]: loss 0.839956
[epoch4, step802]: loss 0.813632
[epoch4, step803]: loss 0.721241
[epoch4, step804]: loss 0.536966
[epoch4, step805]: loss 0.808582
[epoch4, step806]: loss 0.796042
[epoch4, step807]: loss 0.615237
[epoch4, step808]: loss 0.897612
[epoch4, step809]: loss 0.925812
[epoch4, step810]: loss 0.709573
[epoch4, step811]: loss 0.680129
[epoch4, step812]: loss 0.786411
[epoch4, step813]: loss 1.059009
[epoch4, step814]: loss 0.860000
[epoch4, step815]: loss 0.893532
[epoch4, step816]: loss 0.767882
[epoch4, step817]: loss 0.698921
[epoch4, step818]: loss 0.483651
[epoch4, step819]: loss 0.824639
[epoch4, step820]: loss 0.958585
[epoch4, step821]: loss 0.887891
[epoch4, step822]: loss 1.088705
[epoch4, step823]: loss 1.111267
[epoch4, step824]: loss 0.735572
[epoch4, step825]: loss 0.863172
[epoch4, step826]: loss 0.711078
[epoch4, step827]: loss 0.707771
[epoch4, step828]: loss 0.725887
[epoch4, step829]: loss 1.015864
[epoch4, step830]: loss 0.755904
[epoch4, step831]: loss 0.772104
[epoch4, step832]: loss 0.638411
[epoch4, step833]: loss 0.817993
[epoch4, step834]: loss 0.825327
[epoch4, step835]: loss 0.894909
[epoch4, step836]: loss 0.516441
[epoch4, step837]: loss 0.986478
[epoch4, step838]: loss 0.590481
[epoch4, step839]: loss 0.914532
[epoch4, step840]: loss 0.803047
[epoch4, step841]: loss 0.839316
[epoch4, step842]: loss 0.779318
[epoch4, step843]: loss 0.597992
[epoch4, step844]: loss 0.912356
[epoch4, step845]: loss 0.655616
[epoch4, step846]: loss 0.840648
[epoch4, step847]: loss 0.937073
[epoch4, step848]: loss 0.833612
[epoch4, step849]: loss 0.819657
[epoch4, step850]: loss 0.854277
[epoch4, step851]: loss 0.791455
[epoch4, step852]: loss 0.605516
[epoch4, step853]: loss 0.702161
[epoch4, step854]: loss 0.696340
[epoch4, step855]: loss 0.454185
[epoch4, step856]: loss 0.935938
[epoch4, step857]: loss 0.965875
[epoch4, step858]: loss 0.785500
[epoch4, step859]: loss 0.595293
[epoch4, step860]: loss 0.683486
[epoch4, step861]: loss 0.373052
[epoch4, step862]: loss 0.956842
[epoch4, step863]: loss 1.006510
[epoch4, step864]: loss 0.825458
[epoch4, step865]: loss 0.944661
[epoch4, step866]: loss 1.011047
[epoch4, step867]: loss 0.295231
[epoch4, step868]: loss 0.745475
[epoch4, step869]: loss 0.892229
[epoch4, step870]: loss 0.702097
[epoch4, step871]: loss 0.777332
[epoch4, step872]: loss 0.544243
[epoch4, step873]: loss 0.891768
[epoch4, step874]: loss 0.614755
[epoch4, step875]: loss 0.887402
[epoch4, step876]: loss 0.932432
[epoch4, step877]: loss 0.690518
[epoch4, step878]: loss 0.453917
[epoch4, step879]: loss 0.986384
[epoch4, step880]: loss 0.729959
[epoch4, step881]: loss 0.964003
[epoch4, step882]: loss 0.777502
[epoch4, step883]: loss 0.883055
[epoch4, step884]: loss 0.757790
[epoch4, step885]: loss 0.416436
[epoch4, step886]: loss 0.828634
[epoch4, step887]: loss 0.792312
[epoch4, step888]: loss 0.752537
[epoch4, step889]: loss 0.837202
[epoch4, step890]: loss 0.989361
[epoch4, step891]: loss 0.908858
[epoch4, step892]: loss 0.822662
[epoch4, step893]: loss 0.623091
[epoch4, step894]: loss 0.803539
[epoch4, step895]: loss 0.841657
[epoch4, step896]: loss 0.942266
[epoch4, step897]: loss 0.705750
[epoch4, step898]: loss 0.859611
[epoch4, step899]: loss 0.800560
[epoch4, step900]: loss 0.852100
[epoch4, step901]: loss 0.749397
[epoch4, step902]: loss 0.821350
[epoch4, step903]: loss 0.947915
[epoch4, step904]: loss 0.454102
[epoch4, step905]: loss 0.905613
[epoch4, step906]: loss 0.825267
[epoch4, step907]: loss 0.734547
[epoch4, step908]: loss 0.575418
[epoch4, step909]: loss 0.846466
[epoch4, step910]: loss 0.729676
[epoch4, step911]: loss 0.670704
[epoch4, step912]: loss 0.942990
[epoch4, step913]: loss 0.893493
[epoch4, step914]: loss 0.783190
[epoch4, step915]: loss 0.928531
[epoch4, step916]: loss 0.535472
[epoch4, step917]: loss 0.905641
[epoch4, step918]: loss 0.888269
[epoch4, step919]: loss 0.780632
[epoch4, step920]: loss 0.954536
[epoch4, step921]: loss 0.964579
[epoch4, step922]: loss 0.862229
[epoch4, step923]: loss 0.899112
[epoch4, step924]: loss 0.602456
[epoch4, step925]: loss 1.006497
[epoch4, step926]: loss 0.791369
[epoch4, step927]: loss 0.853990
[epoch4, step928]: loss 1.008653
[epoch4, step929]: loss 0.505382
[epoch4, step930]: loss 0.637909
[epoch4, step931]: loss 1.086697
[epoch4, step932]: loss 0.631469
[epoch4, step933]: loss 0.834910
[epoch4, step934]: loss 0.756170
[epoch4, step935]: loss 0.897611
[epoch4, step936]: loss 0.643843
[epoch4, step937]: loss 0.640559
[epoch4, step938]: loss 0.645119
[epoch4, step939]: loss 0.841353
[epoch4, step940]: loss 0.880191
[epoch4, step941]: loss 1.121521
[epoch4, step942]: loss 1.109883
[epoch4, step943]: loss 0.889407
[epoch4, step944]: loss 0.908371
[epoch4, step945]: loss 1.053089
[epoch4, step946]: loss 0.412592
[epoch4, step947]: loss 0.720334
[epoch4, step948]: loss 0.941077
[epoch4, step949]: loss 0.863609
[epoch4, step950]: loss 0.804921
[epoch4, step951]: loss 0.576195
[epoch4, step952]: loss 0.512222
[epoch4, step953]: loss 0.690269
[epoch4, step954]: loss 0.802791
[epoch4, step955]: loss 0.699845
[epoch4, step956]: loss 0.621088
[epoch4, step957]: loss 1.022007
[epoch4, step958]: loss 1.020903
[epoch4, step959]: loss 0.857438
[epoch4, step960]: loss 0.988781
[epoch4, step961]: loss 0.711120
[epoch4, step962]: loss 0.895394
[epoch4, step963]: loss 0.653634
[epoch4, step964]: loss 1.045188
[epoch4, step965]: loss 0.988791
[epoch4, step966]: loss 0.755717
[epoch4, step967]: loss 0.991368
[epoch4, step968]: loss 0.686467
[epoch4, step969]: loss 0.930238
[epoch4, step970]: loss 0.948660
[epoch4, step971]: loss 0.686865
[epoch4, step972]: loss 0.625162
[epoch4, step973]: loss 0.978295
[epoch4, step974]: loss 0.943064
[epoch4, step975]: loss 0.809698
[epoch4, step976]: loss 0.753812
[epoch4, step977]: loss 0.830923
[epoch4, step978]: loss 0.927393
[epoch4, step979]: loss 1.132298
[epoch4, step980]: loss 0.882902
[epoch4, step981]: loss 0.940175
[epoch4, step982]: loss 1.101606
[epoch4, step983]: loss 0.939189
[epoch4, step984]: loss 0.743542
[epoch4, step985]: loss 0.608435
[epoch4, step986]: loss 0.678621
[epoch4, step987]: loss 0.869942
[epoch4, step988]: loss 0.734548
[epoch4, step989]: loss 0.707787
[epoch4, step990]: loss 0.740981
[epoch4, step991]: loss 0.963836
[epoch4, step992]: loss 0.617181
[epoch4, step993]: loss 0.715321
[epoch4, step994]: loss 0.500959
[epoch4, step995]: loss 1.060939
[epoch4, step996]: loss 0.729282
[epoch4, step997]: loss 0.790082
[epoch4, step998]: loss 0.809667
[epoch4, step999]: loss 0.719191
[epoch4, step1000]: loss 0.876984
[epoch4, step1001]: loss 0.791375
[epoch4, step1002]: loss 0.714137
[epoch4, step1003]: loss 0.799109
[epoch4, step1004]: loss 1.085425
[epoch4, step1005]: loss 0.378224
[epoch4, step1006]: loss 0.763343
[epoch4, step1007]: loss 0.796525
[epoch4, step1008]: loss 0.700521
[epoch4, step1009]: loss 0.646895
[epoch4, step1010]: loss 0.884299
[epoch4, step1011]: loss 0.890406
[epoch4, step1012]: loss 1.013597
[epoch4, step1013]: loss 0.695284
[epoch4, step1014]: loss 0.799032
[epoch4, step1015]: loss 0.974895
[epoch4, step1016]: loss 0.750930
[epoch4, step1017]: loss 0.906444
[epoch4, step1018]: loss 0.976728
[epoch4, step1019]: loss 0.778038
[epoch4, step1020]: loss 0.835221
[epoch4, step1021]: loss 1.047028
[epoch4, step1022]: loss 0.837978
[epoch4, step1023]: loss 0.736142
[epoch4, step1024]: loss 1.087714
[epoch4, step1025]: loss 0.945166
[epoch4, step1026]: loss 0.765169
[epoch4, step1027]: loss 0.569368
[epoch4, step1028]: loss 0.882477
[epoch4, step1029]: loss 0.971440
[epoch4, step1030]: loss 0.647777
[epoch4, step1031]: loss 0.689613
[epoch4, step1032]: loss 0.718736
[epoch4, step1033]: loss 0.925393
[epoch4, step1034]: loss 0.540267
[epoch4, step1035]: loss 0.935824
[epoch4, step1036]: loss 1.099776
[epoch4, step1037]: loss 1.089121
[epoch4, step1038]: loss 0.855434
[epoch4, step1039]: loss 0.770989
[epoch4, step1040]: loss 0.637741
[epoch4, step1041]: loss 0.931386
[epoch4, step1042]: loss 0.663244
[epoch4, step1043]: loss 0.880979
[epoch4, step1044]: loss 0.530970
[epoch4, step1045]: loss 0.464265
[epoch4, step1046]: loss 0.922119
[epoch4, step1047]: loss 0.744463
[epoch4, step1048]: loss 0.772143
[epoch4, step1049]: loss 0.827552
[epoch4, step1050]: loss 0.729994
[epoch4, step1051]: loss 1.045177
[epoch4, step1052]: loss 0.844148
[epoch4, step1053]: loss 0.952820
[epoch4, step1054]: loss 0.714583
[epoch4, step1055]: loss 0.977064
[epoch4, step1056]: loss 0.972860
[epoch4, step1057]: loss 0.708129
[epoch4, step1058]: loss 0.820161
[epoch4, step1059]: loss 0.846867
[epoch4, step1060]: loss 0.907697
[epoch4, step1061]: loss 0.819006
[epoch4, step1062]: loss 0.658664
[epoch4, step1063]: loss 0.980169
[epoch4, step1064]: loss 1.057490
[epoch4, step1065]: loss 0.837859
[epoch4, step1066]: loss 0.935045
[epoch4, step1067]: loss 0.660686
[epoch4, step1068]: loss 1.018888
[epoch4, step1069]: loss 0.876634
[epoch4, step1070]: loss 0.809973
[epoch4, step1071]: loss 0.607776
[epoch4, step1072]: loss 0.706312
[epoch4, step1073]: loss 0.878847
[epoch4, step1074]: loss 0.985063
[epoch4, step1075]: loss 1.026183
[epoch4, step1076]: loss 0.624535
[epoch4, step1077]: loss 0.835960
[epoch4, step1078]: loss 0.904862
[epoch4, step1079]: loss 0.769705
[epoch4, step1080]: loss 1.059286
[epoch4, step1081]: loss 0.463240
[epoch4, step1082]: loss 0.816247
[epoch4, step1083]: loss 0.756463
[epoch4, step1084]: loss 0.664432
[epoch4, step1085]: loss 0.632133
[epoch4, step1086]: loss 0.559830
[epoch4, step1087]: loss 0.792091
[epoch4, step1088]: loss 0.829596
[epoch4, step1089]: loss 0.892185
[epoch4, step1090]: loss 0.742231
[epoch4, step1091]: loss 0.819080
[epoch4, step1092]: loss 0.772140
[epoch4, step1093]: loss 0.749021
[epoch4, step1094]: loss 0.773793
[epoch4, step1095]: loss 0.762330
[epoch4, step1096]: loss 1.007777
[epoch4, step1097]: loss 0.828768
[epoch4, step1098]: loss 0.993985
[epoch4, step1099]: loss 1.038518
[epoch4, step1100]: loss 0.652663
[epoch4, step1101]: loss 0.683757
[epoch4, step1102]: loss 0.596503
[epoch4, step1103]: loss 0.854600
[epoch4, step1104]: loss 0.744650
[epoch4, step1105]: loss 0.720034
[epoch4, step1106]: loss 0.941052
[epoch4, step1107]: loss 0.727892
[epoch4, step1108]: loss 0.695340
[epoch4, step1109]: loss 0.863348
[epoch4, step1110]: loss 0.896744
[epoch4, step1111]: loss 0.429722
[epoch4, step1112]: loss 0.715512
[epoch4, step1113]: loss 0.946888
[epoch4, step1114]: loss 0.741226
[epoch4, step1115]: loss 0.892779
[epoch4, step1116]: loss 0.995056
[epoch4, step1117]: loss 0.943666
[epoch4, step1118]: loss 0.938527
[epoch4, step1119]: loss 0.830577
[epoch4, step1120]: loss 0.820035
[epoch4, step1121]: loss 0.783478
[epoch4, step1122]: loss 0.529226
[epoch4, step1123]: loss 0.867572
[epoch4, step1124]: loss 0.923483
[epoch4, step1125]: loss 0.850322
[epoch4, step1126]: loss 0.871893
[epoch4, step1127]: loss 0.620476
[epoch4, step1128]: loss 0.679960
[epoch4, step1129]: loss 0.765245
[epoch4, step1130]: loss 0.703746
[epoch4, step1131]: loss 0.728666
[epoch4, step1132]: loss 0.392026
[epoch4, step1133]: loss 0.962719
[epoch4, step1134]: loss 0.946823
[epoch4, step1135]: loss 0.770938
[epoch4, step1136]: loss 0.563632
[epoch4, step1137]: loss 0.949261
[epoch4, step1138]: loss 0.893896
[epoch4, step1139]: loss 0.702797
[epoch4, step1140]: loss 0.621696
[epoch4, step1141]: loss 0.723550
[epoch4, step1142]: loss 0.908666
[epoch4, step1143]: loss 0.894778
[epoch4, step1144]: loss 0.630205
[epoch4, step1145]: loss 0.935797
[epoch4, step1146]: loss 0.881552
[epoch4, step1147]: loss 0.950128
[epoch4, step1148]: loss 0.885487
[epoch4, step1149]: loss 0.798979
[epoch4, step1150]: loss 0.716816
[epoch4, step1151]: loss 0.641490
[epoch4, step1152]: loss 0.717569
[epoch4, step1153]: loss 0.826207
[epoch4, step1154]: loss 0.755135
[epoch4, step1155]: loss 0.650352
[epoch4, step1156]: loss 0.746718
[epoch4, step1157]: loss 0.571557
[epoch4, step1158]: loss 0.863224
[epoch4, step1159]: loss 0.771589
[epoch4, step1160]: loss 0.874174
[epoch4, step1161]: loss 0.774072
[epoch4, step1162]: loss 0.827502
[epoch4, step1163]: loss 0.740583
[epoch4, step1164]: loss 0.829616
[epoch4, step1165]: loss 0.789163
[epoch4, step1166]: loss 0.939964
[epoch4, step1167]: loss 0.602549
[epoch4, step1168]: loss 0.659974
[epoch4, step1169]: loss 0.823857
[epoch4, step1170]: loss 0.937894
[epoch4, step1171]: loss 0.767915
[epoch4, step1172]: loss 0.956705
[epoch4, step1173]: loss 0.899986
[epoch4, step1174]: loss 0.822108
[epoch4, step1175]: loss 0.535692
[epoch4, step1176]: loss 0.798976
[epoch4, step1177]: loss 0.820377
[epoch4, step1178]: loss 0.714442
[epoch4, step1179]: loss 0.939919
[epoch4, step1180]: loss 0.742510
[epoch4, step1181]: loss 0.725551
[epoch4, step1182]: loss 0.937966
[epoch4, step1183]: loss 0.651987
[epoch4, step1184]: loss 0.637329
[epoch4, step1185]: loss 0.949753
[epoch4, step1186]: loss 0.899020
[epoch4, step1187]: loss 1.025521
[epoch4, step1188]: loss 0.793746
[epoch4, step1189]: loss 0.936281
[epoch4, step1190]: loss 1.066982
[epoch4, step1191]: loss 0.726896
[epoch4, step1192]: loss 0.883674
[epoch4, step1193]: loss 0.600005
[epoch4, step1194]: loss 0.714143
[epoch4, step1195]: loss 0.758232
[epoch4, step1196]: loss 0.706546
[epoch4, step1197]: loss 0.962060
[epoch4, step1198]: loss 0.764051
[epoch4, step1199]: loss 0.545133
[epoch4, step1200]: loss 0.966730
[epoch4, step1201]: loss 0.816438
[epoch4, step1202]: loss 0.717629
[epoch4, step1203]: loss 0.638862
[epoch4, step1204]: loss 0.875114
[epoch4, step1205]: loss 0.643904
[epoch4, step1206]: loss 0.989515
[epoch4, step1207]: loss 1.057726
[epoch4, step1208]: loss 0.751926
[epoch4, step1209]: loss 0.643731
[epoch4, step1210]: loss 0.538292
[epoch4, step1211]: loss 0.811248
[epoch4, step1212]: loss 0.730316
[epoch4, step1213]: loss 0.833188
[epoch4, step1214]: loss 1.046392
[epoch4, step1215]: loss 0.859880
[epoch4, step1216]: loss 0.715986
[epoch4, step1217]: loss 0.978589
[epoch4, step1218]: loss 0.841701
[epoch4, step1219]: loss 0.763048
[epoch4, step1220]: loss 0.864705
[epoch4, step1221]: loss 0.816674
[epoch4, step1222]: loss 1.047032
[epoch4, step1223]: loss 0.800677
[epoch4, step1224]: loss 0.431037
[epoch4, step1225]: loss 0.875958
[epoch4, step1226]: loss 0.594204
[epoch4, step1227]: loss 0.931520
[epoch4, step1228]: loss 0.951610
[epoch4, step1229]: loss 1.013272
[epoch4, step1230]: loss 0.885581
[epoch4, step1231]: loss 0.785324
[epoch4, step1232]: loss 0.722508
[epoch4, step1233]: loss 0.755086
[epoch4, step1234]: loss 0.623857
[epoch4, step1235]: loss 0.678125
[epoch4, step1236]: loss 0.846986
[epoch4, step1237]: loss 0.634093
[epoch4, step1238]: loss 0.974775
[epoch4, step1239]: loss 1.052295
[epoch4, step1240]: loss 1.101746
[epoch4, step1241]: loss 0.715590
[epoch4, step1242]: loss 0.832651
[epoch4, step1243]: loss 0.891703
[epoch4, step1244]: loss 0.899493
[epoch4, step1245]: loss 0.825588
[epoch4, step1246]: loss 0.567268
[epoch4, step1247]: loss 0.877524
[epoch4, step1248]: loss 0.891666
[epoch4, step1249]: loss 0.697178
[epoch4, step1250]: loss 0.821498
[epoch4, step1251]: loss 0.727975
[epoch4, step1252]: loss 1.017049
[epoch4, step1253]: loss 0.667829
[epoch4, step1254]: loss 0.742417
[epoch4, step1255]: loss 0.643533
[epoch4, step1256]: loss 0.697472
[epoch4, step1257]: loss 0.697877
[epoch4, step1258]: loss 0.641572
[epoch4, step1259]: loss 0.916668
[epoch4, step1260]: loss 0.659274
[epoch4, step1261]: loss 0.843298
[epoch4, step1262]: loss 0.921935
[epoch4, step1263]: loss 0.834244
[epoch4, step1264]: loss 0.848477
[epoch4, step1265]: loss 0.841625
[epoch4, step1266]: loss 0.851063
[epoch4, step1267]: loss 0.693862
[epoch4, step1268]: loss 1.152391
[epoch4, step1269]: loss 0.894391
[epoch4, step1270]: loss 0.751137
[epoch4, step1271]: loss 0.632390
[epoch4, step1272]: loss 0.666885
[epoch4, step1273]: loss 0.713336
[epoch4, step1274]: loss 0.736996
[epoch4, step1275]: loss 0.822780
[epoch4, step1276]: loss 0.663124
[epoch4, step1277]: loss 0.829162
[epoch4, step1278]: loss 0.747814
[epoch4, step1279]: loss 0.827458
[epoch4, step1280]: loss 1.004348
[epoch4, step1281]: loss 0.824926
[epoch4, step1282]: loss 0.853309
[epoch4, step1283]: loss 0.716328
[epoch4, step1284]: loss 0.668575
[epoch4, step1285]: loss 0.877965
[epoch4, step1286]: loss 0.783504
[epoch4, step1287]: loss 0.418283
[epoch4, step1288]: loss 0.817034
[epoch4, step1289]: loss 0.652716
[epoch4, step1290]: loss 0.730571
[epoch4, step1291]: loss 0.875162
[epoch4, step1292]: loss 0.959256
[epoch4, step1293]: loss 0.566560
[epoch4, step1294]: loss 0.587371
[epoch4, step1295]: loss 0.983089
[epoch4, step1296]: loss 0.668840
[epoch4, step1297]: loss 0.894007
[epoch4, step1298]: loss 0.602620
[epoch4, step1299]: loss 0.438365
[epoch4, step1300]: loss 0.736802
[epoch4, step1301]: loss 1.135356
[epoch4, step1302]: loss 0.659162
[epoch4, step1303]: loss 0.765320
[epoch4, step1304]: loss 0.847469
[epoch4, step1305]: loss 0.734440
[epoch4, step1306]: loss 0.722434
[epoch4, step1307]: loss 0.798399
[epoch4, step1308]: loss 0.927699
[epoch4, step1309]: loss 0.845442
[epoch4, step1310]: loss 0.631306
[epoch4, step1311]: loss 0.965156
[epoch4, step1312]: loss 0.322839
[epoch4, step1313]: loss 0.360802
[epoch4, step1314]: loss 0.968341
[epoch4, step1315]: loss 0.914359
[epoch4, step1316]: loss 0.551667
[epoch4, step1317]: loss 0.423936
[epoch4, step1318]: loss 0.956047
[epoch4, step1319]: loss 0.830027
[epoch4, step1320]: loss 0.924983
[epoch4, step1321]: loss 0.603595
[epoch4, step1322]: loss 0.843768
[epoch4, step1323]: loss 0.878306
[epoch4, step1324]: loss 1.100532
[epoch4, step1325]: loss 0.797125
[epoch4, step1326]: loss 0.705961
[epoch4, step1327]: loss 0.881396
[epoch4, step1328]: loss 0.627824
[epoch4, step1329]: loss 0.602563
[epoch4, step1330]: loss 1.106360
[epoch4, step1331]: loss 0.834795
[epoch4, step1332]: loss 0.705184
[epoch4, step1333]: loss 0.822053
[epoch4, step1334]: loss 0.769405
[epoch4, step1335]: loss 0.856216
[epoch4, step1336]: loss 0.756455
[epoch4, step1337]: loss 0.777363
[epoch4, step1338]: loss 0.951791
[epoch4, step1339]: loss 0.646469
[epoch4, step1340]: loss 0.994730
[epoch4, step1341]: loss 0.816349
[epoch4, step1342]: loss 0.969705
[epoch4, step1343]: loss 0.999203
[epoch4, step1344]: loss 0.782129
[epoch4, step1345]: loss 0.943295
[epoch4, step1346]: loss 0.749881
[epoch4, step1347]: loss 0.546038
[epoch4, step1348]: loss 0.717993
[epoch4, step1349]: loss 0.712293
[epoch4, step1350]: loss 0.737380
[epoch4, step1351]: loss 0.867564
[epoch4, step1352]: loss 0.694441
[epoch4, step1353]: loss 0.690955
[epoch4, step1354]: loss 0.990521
[epoch4, step1355]: loss 0.757592
[epoch4, step1356]: loss 0.507941
[epoch4, step1357]: loss 1.023702
[epoch4, step1358]: loss 0.665194
[epoch4, step1359]: loss 0.950926
[epoch4, step1360]: loss 0.789443
[epoch4, step1361]: loss 0.972750
[epoch4, step1362]: loss 0.803468
[epoch4, step1363]: loss 0.762735
[epoch4, step1364]: loss 0.753381
[epoch4, step1365]: loss 0.910869
[epoch4, step1366]: loss 0.892779
[epoch4, step1367]: loss 0.682526
[epoch4, step1368]: loss 0.879389
[epoch4, step1369]: loss 0.915663
[epoch4, step1370]: loss 0.617939
[epoch4, step1371]: loss 0.872455
[epoch4, step1372]: loss 0.661815
[epoch4, step1373]: loss 0.739001
[epoch4, step1374]: loss 0.913760
[epoch4, step1375]: loss 0.917645
[epoch4, step1376]: loss 0.931434
[epoch4, step1377]: loss 0.785895
[epoch4, step1378]: loss 0.999447
[epoch4, step1379]: loss 0.758341
[epoch4, step1380]: loss 0.802103
[epoch4, step1381]: loss 0.784832
[epoch4, step1382]: loss 0.707268
[epoch4, step1383]: loss 0.774401
[epoch4, step1384]: loss 0.904785
[epoch4, step1385]: loss 0.642864
[epoch4, step1386]: loss 0.855863
[epoch4, step1387]: loss 0.727519
[epoch4, step1388]: loss 0.870883
[epoch4, step1389]: loss 0.939943
[epoch4, step1390]: loss 0.671322
[epoch4, step1391]: loss 0.921694
[epoch4, step1392]: loss 0.638379
[epoch4, step1393]: loss 0.683776
[epoch4, step1394]: loss 0.545241
[epoch4, step1395]: loss 0.685312
[epoch4, step1396]: loss 0.907125
[epoch4, step1397]: loss 0.881918
[epoch4, step1398]: loss 0.322573
[epoch4, step1399]: loss 0.937872
[epoch4, step1400]: loss 0.565443
[epoch4, step1401]: loss 0.728002
[epoch4, step1402]: loss 0.844162
[epoch4, step1403]: loss 0.682691
[epoch4, step1404]: loss 0.749613
[epoch4, step1405]: loss 0.721753
[epoch4, step1406]: loss 0.424383
[epoch4, step1407]: loss 1.049734
[epoch4, step1408]: loss 0.732499
[epoch4, step1409]: loss 0.460808
[epoch4, step1410]: loss 0.732484
[epoch4, step1411]: loss 0.904348
[epoch4, step1412]: loss 0.892718
[epoch4, step1413]: loss 0.861960
[epoch4, step1414]: loss 0.703226
[epoch4, step1415]: loss 0.962510
[epoch4, step1416]: loss 0.886984
[epoch4, step1417]: loss 0.538186
[epoch4, step1418]: loss 0.924655
[epoch4, step1419]: loss 0.892720
[epoch4, step1420]: loss 0.479215
[epoch4, step1421]: loss 0.866444
[epoch4, step1422]: loss 0.584525
[epoch4, step1423]: loss 0.795426
[epoch4, step1424]: loss 0.643115
[epoch4, step1425]: loss 1.033759
[epoch4, step1426]: loss 0.821401
[epoch4, step1427]: loss 0.773110
[epoch4, step1428]: loss 0.797708
[epoch4, step1429]: loss 0.901010
[epoch4, step1430]: loss 0.823589
[epoch4, step1431]: loss 1.005862
[epoch4, step1432]: loss 0.706780
[epoch4, step1433]: loss 0.681660
[epoch4, step1434]: loss 0.736814
[epoch4, step1435]: loss 0.225133
[epoch4, step1436]: loss 0.814868
[epoch4, step1437]: loss 0.917109
[epoch4, step1438]: loss 0.910675
[epoch4, step1439]: loss 0.676795
[epoch4, step1440]: loss 0.977837
[epoch4, step1441]: loss 0.760460
[epoch4, step1442]: loss 0.666279
[epoch4, step1443]: loss 0.788505
[epoch4, step1444]: loss 0.324681
[epoch4, step1445]: loss 0.725184
[epoch4, step1446]: loss 0.950373
[epoch4, step1447]: loss 0.514717
[epoch4, step1448]: loss 0.701846
[epoch4, step1449]: loss 1.104787
[epoch4, step1450]: loss 0.940827
[epoch4, step1451]: loss 0.766393
[epoch4, step1452]: loss 0.880253
[epoch4, step1453]: loss 0.792954
[epoch4, step1454]: loss 0.807875
[epoch4, step1455]: loss 0.563770
[epoch4, step1456]: loss 0.564203
[epoch4, step1457]: loss 0.693496
[epoch4, step1458]: loss 0.803726
[epoch4, step1459]: loss 0.639884
[epoch4, step1460]: loss 0.629888
[epoch4, step1461]: loss 0.748889
[epoch4, step1462]: loss 0.918717
[epoch4, step1463]: loss 0.974124
[epoch4, step1464]: loss 0.743541
[epoch4, step1465]: loss 0.728635
[epoch4, step1466]: loss 0.396108
[epoch4, step1467]: loss 0.625567
[epoch4, step1468]: loss 0.870291
[epoch4, step1469]: loss 0.797540
[epoch4, step1470]: loss 0.775817
[epoch4, step1471]: loss 0.896470
[epoch4, step1472]: loss 0.427988
[epoch4, step1473]: loss 0.481435
[epoch4, step1474]: loss 0.871806
[epoch4, step1475]: loss 0.486535
[epoch4, step1476]: loss 0.682413
[epoch4, step1477]: loss 0.875233
[epoch4, step1478]: loss 0.892205
[epoch4, step1479]: loss 0.733366
[epoch4, step1480]: loss 0.737302
[epoch4, step1481]: loss 0.726055
[epoch4, step1482]: loss 0.799485
[epoch4, step1483]: loss 0.676615
[epoch4, step1484]: loss 0.858364
[epoch4, step1485]: loss 0.861980
[epoch4, step1486]: loss 0.775725
[epoch4, step1487]: loss 0.518143
[epoch4, step1488]: loss 0.677577
[epoch4, step1489]: loss 0.792985
[epoch4, step1490]: loss 0.863488
[epoch4, step1491]: loss 0.697842
[epoch4, step1492]: loss 0.520736
[epoch4, step1493]: loss 0.861218
[epoch4, step1494]: loss 0.643723
[epoch4, step1495]: loss 0.575275
[epoch4, step1496]: loss 0.712595
[epoch4, step1497]: loss 0.935978
[epoch4, step1498]: loss 0.994336
[epoch4, step1499]: loss 0.597186
[epoch4, step1500]: loss 0.982776
[epoch4, step1501]: loss 0.836730
[epoch4, step1502]: loss 0.667763
[epoch4, step1503]: loss 0.858713
[epoch4, step1504]: loss 0.687155
[epoch4, step1505]: loss 0.549343
[epoch4, step1506]: loss 0.787014
[epoch4, step1507]: loss 0.819566
[epoch4, step1508]: loss 0.873505
[epoch4, step1509]: loss 0.758461
[epoch4, step1510]: loss 0.989066
[epoch4, step1511]: loss 0.629650
[epoch4, step1512]: loss 0.823840
[epoch4, step1513]: loss 0.718977
[epoch4, step1514]: loss 0.702765
[epoch4, step1515]: loss 0.971446
[epoch4, step1516]: loss 0.933151
[epoch4, step1517]: loss 1.000573
[epoch4, step1518]: loss 0.920330
[epoch4, step1519]: loss 0.609513
[epoch4, step1520]: loss 0.758899
[epoch4, step1521]: loss 0.924478
[epoch4, step1522]: loss 0.879759
[epoch4, step1523]: loss 0.633496
[epoch4, step1524]: loss 1.031754
[epoch4, step1525]: loss 0.867389
[epoch4, step1526]: loss 0.968388
[epoch4, step1527]: loss 0.632569
[epoch4, step1528]: loss 0.631417
[epoch4, step1529]: loss 0.354881
[epoch4, step1530]: loss 0.964984
[epoch4, step1531]: loss 0.712461
[epoch4, step1532]: loss 0.737237
[epoch4, step1533]: loss 0.898591
[epoch4, step1534]: loss 0.634822
[epoch4, step1535]: loss 0.681124
[epoch4, step1536]: loss 0.634465
[epoch4, step1537]: loss 0.528257
[epoch4, step1538]: loss 0.590706
[epoch4, step1539]: loss 0.476649
[epoch4, step1540]: loss 1.020834
[epoch4, step1541]: loss 0.889950
[epoch4, step1542]: loss 0.774550
[epoch4, step1543]: loss 0.809393
[epoch4, step1544]: loss 0.991824
[epoch4, step1545]: loss 0.568907
[epoch4, step1546]: loss 1.094689
[epoch4, step1547]: loss 0.963626
[epoch4, step1548]: loss 0.885622
[epoch4, step1549]: loss 0.773548
[epoch4, step1550]: loss 0.946956
[epoch4, step1551]: loss 0.800545
[epoch4, step1552]: loss 0.789832
[epoch4, step1553]: loss 0.669230
[epoch4, step1554]: loss 0.651068
[epoch4, step1555]: loss 0.917349
[epoch4, step1556]: loss 1.133704
[epoch4, step1557]: loss 0.970887
[epoch4, step1558]: loss 0.827285
[epoch4, step1559]: loss 0.684720
[epoch4, step1560]: loss 0.778536
[epoch4, step1561]: loss 0.863913
[epoch4, step1562]: loss 0.755577
[epoch4, step1563]: loss 0.682718
[epoch4, step1564]: loss 0.763766
[epoch4, step1565]: loss 0.663617
[epoch4, step1566]: loss 1.037214
[epoch4, step1567]: loss 1.012991
[epoch4, step1568]: loss 0.980783
[epoch4, step1569]: loss 0.698606
[epoch4, step1570]: loss 0.656762
[epoch4, step1571]: loss 0.816714
[epoch4, step1572]: loss 0.775320
[epoch4, step1573]: loss 0.557897
[epoch4, step1574]: loss 0.933932
[epoch4, step1575]: loss 0.896226
[epoch4, step1576]: loss 0.891367
[epoch4, step1577]: loss 1.146940
[epoch4, step1578]: loss 0.756173
[epoch4, step1579]: loss 0.922555
[epoch4, step1580]: loss 0.828570
[epoch4, step1581]: loss 0.387386
[epoch4, step1582]: loss 0.798283
[epoch4, step1583]: loss 0.617680
[epoch4, step1584]: loss 0.896860
[epoch4, step1585]: loss 0.906953
[epoch4, step1586]: loss 0.566185
[epoch4, step1587]: loss 0.988670
[epoch4, step1588]: loss 0.702586
[epoch4, step1589]: loss 0.462048
[epoch4, step1590]: loss 0.629461
[epoch4, step1591]: loss 0.867428
[epoch4, step1592]: loss 0.910077
[epoch4, step1593]: loss 0.915909
[epoch4, step1594]: loss 0.421227
[epoch4, step1595]: loss 0.829970
[epoch4, step1596]: loss 0.838683
[epoch4, step1597]: loss 0.751518
[epoch4, step1598]: loss 0.723664
[epoch4, step1599]: loss 0.856242
[epoch4, step1600]: loss 0.830100
[epoch4, step1601]: loss 1.002705
[epoch4, step1602]: loss 0.470009
[epoch4, step1603]: loss 0.763761
[epoch4, step1604]: loss 0.733280
[epoch4, step1605]: loss 0.906647
[epoch4, step1606]: loss 0.552626
[epoch4, step1607]: loss 0.983558
[epoch4, step1608]: loss 0.874364
[epoch4, step1609]: loss 0.659869
[epoch4, step1610]: loss 0.688039
[epoch4, step1611]: loss 0.892541
[epoch4, step1612]: loss 0.824366
[epoch4, step1613]: loss 0.940792
[epoch4, step1614]: loss 0.892270
[epoch4, step1615]: loss 0.717905
[epoch4, step1616]: loss 0.913305
[epoch4, step1617]: loss 0.672659
[epoch4, step1618]: loss 0.618233
[epoch4, step1619]: loss 0.832596
[epoch4, step1620]: loss 0.780241
[epoch4, step1621]: loss 0.604737
[epoch4, step1622]: loss 0.659162
[epoch4, step1623]: loss 0.833108
[epoch4, step1624]: loss 0.699867
[epoch4, step1625]: loss 0.906201
[epoch4, step1626]: loss 0.875162
[epoch4, step1627]: loss 0.684310
[epoch4, step1628]: loss 1.050958
[epoch4, step1629]: loss 0.827577
[epoch4, step1630]: loss 1.010286
[epoch4, step1631]: loss 0.663874
[epoch4, step1632]: loss 0.679660
[epoch4, step1633]: loss 0.888470
[epoch4, step1634]: loss 0.958772
[epoch4, step1635]: loss 1.011492
[epoch4, step1636]: loss 0.796574
[epoch4, step1637]: loss 0.502062
[epoch4, step1638]: loss 0.630192
[epoch4, step1639]: loss 0.627854
[epoch4, step1640]: loss 0.584263
[epoch4, step1641]: loss 0.565729
[epoch4, step1642]: loss 0.818389
[epoch4, step1643]: loss 0.850857
[epoch4, step1644]: loss 0.477312
[epoch4, step1645]: loss 0.854996
[epoch4, step1646]: loss 0.809794
[epoch4, step1647]: loss 0.935307
[epoch4, step1648]: loss 0.846793
[epoch4, step1649]: loss 0.674717
[epoch4, step1650]: loss 0.644225
[epoch4, step1651]: loss 0.922911
[epoch4, step1652]: loss 0.908037
[epoch4, step1653]: loss 0.620934
[epoch4, step1654]: loss 0.991579
[epoch4, step1655]: loss 0.888523
[epoch4, step1656]: loss 0.620883
[epoch4, step1657]: loss 1.127618
[epoch4, step1658]: loss 0.884635
[epoch4, step1659]: loss 0.629461
[epoch4, step1660]: loss 1.109262
[epoch4, step1661]: loss 0.808155
[epoch4, step1662]: loss 0.722059
[epoch4, step1663]: loss 0.675634
[epoch4, step1664]: loss 0.766513
[epoch4, step1665]: loss 0.627413
[epoch4, step1666]: loss 0.572554
[epoch4, step1667]: loss 0.717487
[epoch4, step1668]: loss 0.435095
[epoch4, step1669]: loss 0.929267
[epoch4, step1670]: loss 0.771102
[epoch4, step1671]: loss 0.921247
[epoch4, step1672]: loss 0.746557
[epoch4, step1673]: loss 0.884948
[epoch4, step1674]: loss 0.806788
[epoch4, step1675]: loss 0.560650
[epoch4, step1676]: loss 0.787833
[epoch4, step1677]: loss 0.884338
[epoch4, step1678]: loss 0.558032
[epoch4, step1679]: loss 0.617821
[epoch4, step1680]: loss 0.620660
[epoch4, step1681]: loss 0.815388
[epoch4, step1682]: loss 0.773460
[epoch4, step1683]: loss 0.637557
[epoch4, step1684]: loss 0.703046
[epoch4, step1685]: loss 0.630189
[epoch4, step1686]: loss 0.906656
[epoch4, step1687]: loss 0.929401
[epoch4, step1688]: loss 0.780360
[epoch4, step1689]: loss 0.869144
[epoch4, step1690]: loss 0.780594
[epoch4, step1691]: loss 0.584885
[epoch4, step1692]: loss 0.870214
[epoch4, step1693]: loss 0.692740
[epoch4, step1694]: loss 0.905993
[epoch4, step1695]: loss 0.392081
[epoch4, step1696]: loss 0.725468
[epoch4, step1697]: loss 0.751348
[epoch4, step1698]: loss 0.593557
[epoch4, step1699]: loss 0.822790
[epoch4, step1700]: loss 0.827273
[epoch4, step1701]: loss 0.547792
[epoch4, step1702]: loss 0.437809
[epoch4, step1703]: loss 0.798092
[epoch4, step1704]: loss 0.881578
[epoch4, step1705]: loss 0.663167
[epoch4, step1706]: loss 0.861291
[epoch4, step1707]: loss 0.870337
[epoch4, step1708]: loss 0.757823
[epoch4, step1709]: loss 0.882607
[epoch4, step1710]: loss 0.885343
[epoch4, step1711]: loss 0.805245
[epoch4, step1712]: loss 0.681863
[epoch4, step1713]: loss 0.873502
[epoch4, step1714]: loss 1.039872
[epoch4, step1715]: loss 0.734356
[epoch4, step1716]: loss 0.826443
[epoch4, step1717]: loss 0.919396
[epoch4, step1718]: loss 0.638976
[epoch4, step1719]: loss 0.570706
[epoch4, step1720]: loss 0.696827
[epoch4, step1721]: loss 0.842495
[epoch4, step1722]: loss 0.720729
[epoch4, step1723]: loss 0.912752
[epoch4, step1724]: loss 0.737564
[epoch4, step1725]: loss 0.692124
[epoch4, step1726]: loss 0.995115
[epoch4, step1727]: loss 0.881089
[epoch4, step1728]: loss 1.176805
[epoch4, step1729]: loss 0.822803
[epoch4, step1730]: loss 0.638537
[epoch4, step1731]: loss 0.837020
[epoch4, step1732]: loss 0.926479
[epoch4, step1733]: loss 0.774415
[epoch4, step1734]: loss 0.751017
[epoch4, step1735]: loss 0.648114
[epoch4, step1736]: loss 0.636216
[epoch4, step1737]: loss 0.696509
[epoch4, step1738]: loss 0.832402
[epoch4, step1739]: loss 0.727766
[epoch4, step1740]: loss 0.970651
[epoch4, step1741]: loss 0.859899
[epoch4, step1742]: loss 0.653236
[epoch4, step1743]: loss 0.818586
[epoch4, step1744]: loss 0.640679
[epoch4, step1745]: loss 0.780289
[epoch4, step1746]: loss 0.670979
[epoch4, step1747]: loss 0.823093
[epoch4, step1748]: loss 0.669019
[epoch4, step1749]: loss 0.875393
[epoch4, step1750]: loss 0.906157
[epoch4, step1751]: loss 0.605193
[epoch4, step1752]: loss 0.722378
[epoch4, step1753]: loss 0.735767
[epoch4, step1754]: loss 0.506433
[epoch4, step1755]: loss 0.815973
[epoch4, step1756]: loss 0.737029
[epoch4, step1757]: loss 0.613173
[epoch4, step1758]: loss 0.735679
[epoch4, step1759]: loss 0.435847
[epoch4, step1760]: loss 0.819906
[epoch4, step1761]: loss 0.921539
[epoch4, step1762]: loss 0.735148
[epoch4, step1763]: loss 0.613190
[epoch4, step1764]: loss 0.861789
[epoch4, step1765]: loss 0.954543
[epoch4, step1766]: loss 0.800989
[epoch4, step1767]: loss 0.796183
[epoch4, step1768]: loss 0.638547
[epoch4, step1769]: loss 0.758578
[epoch4, step1770]: loss 0.954539
[epoch4, step1771]: loss 0.255472
[epoch4, step1772]: loss 0.903448
[epoch4, step1773]: loss 0.733176
[epoch4, step1774]: loss 0.790040
[epoch4, step1775]: loss 0.510916
[epoch4, step1776]: loss 0.768050
[epoch4, step1777]: loss 0.991707
[epoch4, step1778]: loss 0.685249
[epoch4, step1779]: loss 0.625616
[epoch4, step1780]: loss 0.800055
[epoch4, step1781]: loss 0.842123
[epoch4, step1782]: loss 0.909206
[epoch4, step1783]: loss 0.832279
[epoch4, step1784]: loss 0.788737
[epoch4, step1785]: loss 0.590794
[epoch4, step1786]: loss 0.676181
[epoch4, step1787]: loss 0.809687
[epoch4, step1788]: loss 0.986075
[epoch4, step1789]: loss 0.788253
[epoch4, step1790]: loss 1.065306
[epoch4, step1791]: loss 0.928251
[epoch4, step1792]: loss 0.517424
[epoch4, step1793]: loss 0.329141
[epoch4, step1794]: loss 0.564547
[epoch4, step1795]: loss 0.944892
[epoch4, step1796]: loss 0.824683
[epoch4, step1797]: loss 0.690294
[epoch4, step1798]: loss 0.642940
[epoch4, step1799]: loss 0.843579
[epoch4, step1800]: loss 0.734567
[epoch4, step1801]: loss 0.751442
[epoch4, step1802]: loss 0.606435
[epoch4, step1803]: loss 0.703783
[epoch4, step1804]: loss 0.755914
[epoch4, step1805]: loss 0.802904
[epoch4, step1806]: loss 0.852467
[epoch4, step1807]: loss 0.934115
[epoch4, step1808]: loss 0.775822
[epoch4, step1809]: loss 0.587426
[epoch4, step1810]: loss 0.503868
[epoch4, step1811]: loss 1.068079
[epoch4, step1812]: loss 0.940730
[epoch4, step1813]: loss 0.655433
[epoch4, step1814]: loss 0.266463
[epoch4, step1815]: loss 0.505223
[epoch4, step1816]: loss 0.739014
[epoch4, step1817]: loss 0.856639
[epoch4, step1818]: loss 0.790042
[epoch4, step1819]: loss 0.789301
[epoch4, step1820]: loss 0.950196
[epoch4, step1821]: loss 0.661282
[epoch4, step1822]: loss 0.941225
[epoch4, step1823]: loss 0.870528
[epoch4, step1824]: loss 0.938487
[epoch4, step1825]: loss 0.515447
[epoch4, step1826]: loss 0.737471
[epoch4, step1827]: loss 0.953915
[epoch4, step1828]: loss 1.009882
[epoch4, step1829]: loss 0.446006
[epoch4, step1830]: loss 0.655651
[epoch4, step1831]: loss 0.894658
[epoch4, step1832]: loss 0.949858
[epoch4, step1833]: loss 0.719970
[epoch4, step1834]: loss 0.938396
[epoch4, step1835]: loss 0.976301
[epoch4, step1836]: loss 0.711529
[epoch4, step1837]: loss 0.667385
[epoch4, step1838]: loss 0.901363
[epoch4, step1839]: loss 0.848035
[epoch4, step1840]: loss 0.624910
[epoch4, step1841]: loss 0.818072
[epoch4, step1842]: loss 0.798079
[epoch4, step1843]: loss 0.496180
[epoch4, step1844]: loss 0.850325
[epoch4, step1845]: loss 0.789543
[epoch4, step1846]: loss 0.847316
[epoch4, step1847]: loss 0.516493
[epoch4, step1848]: loss 0.886679
[epoch4, step1849]: loss 0.828450
[epoch4, step1850]: loss 0.912634
[epoch4, step1851]: loss 0.957880
[epoch4, step1852]: loss 0.893713
[epoch4, step1853]: loss 0.951202
[epoch4, step1854]: loss 0.838387
[epoch4, step1855]: loss 0.628201
[epoch4, step1856]: loss 0.923254
[epoch4, step1857]: loss 0.706456
[epoch4, step1858]: loss 0.750494
[epoch4, step1859]: loss 0.522024
[epoch4, step1860]: loss 0.865847
[epoch4, step1861]: loss 0.949940
[epoch4, step1862]: loss 0.802273
[epoch4, step1863]: loss 0.879164
[epoch4, step1864]: loss 0.473204
[epoch4, step1865]: loss 0.582818
[epoch4, step1866]: loss 0.911253
[epoch4, step1867]: loss 0.590563
[epoch4, step1868]: loss 0.528420
[epoch4, step1869]: loss 0.775578
[epoch4, step1870]: loss 0.898540
[epoch4, step1871]: loss 0.807148
[epoch4, step1872]: loss 0.712611
[epoch4, step1873]: loss 0.862103
[epoch4, step1874]: loss 0.774659
[epoch4, step1875]: loss 0.882078
[epoch4, step1876]: loss 0.921212
[epoch4, step1877]: loss 1.044938
[epoch4, step1878]: loss 0.680951
[epoch4, step1879]: loss 0.791955
[epoch4, step1880]: loss 0.903522
[epoch4, step1881]: loss 0.672716
[epoch4, step1882]: loss 0.774375
[epoch4, step1883]: loss 0.799390
[epoch4, step1884]: loss 0.514392
[epoch4, step1885]: loss 0.785156
[epoch4, step1886]: loss 0.809133
[epoch4, step1887]: loss 0.872409
[epoch4, step1888]: loss 0.535725
[epoch4, step1889]: loss 0.643917
[epoch4, step1890]: loss 0.918750
[epoch4, step1891]: loss 0.642708
[epoch4, step1892]: loss 0.650315
[epoch4, step1893]: loss 0.654148
[epoch4, step1894]: loss 0.779173
[epoch4, step1895]: loss 0.851498
[epoch4, step1896]: loss 0.697535
[epoch4, step1897]: loss 1.066718
[epoch4, step1898]: loss 0.910988
[epoch4, step1899]: loss 1.015748
[epoch4, step1900]: loss 0.992612
[epoch4, step1901]: loss 0.873288
[epoch4, step1902]: loss 0.750294
[epoch4, step1903]: loss 0.953725
[epoch4, step1904]: loss 0.796186
[epoch4, step1905]: loss 0.702418
[epoch4, step1906]: loss 0.616178
[epoch4, step1907]: loss 0.767952
[epoch4, step1908]: loss 1.001782
[epoch4, step1909]: loss 0.536058
[epoch4, step1910]: loss 0.484425
[epoch4, step1911]: loss 0.979282
[epoch4, step1912]: loss 0.760857
[epoch4, step1913]: loss 0.708939
[epoch4, step1914]: loss 0.358790
[epoch4, step1915]: loss 0.823659
[epoch4, step1916]: loss 0.654709
[epoch4, step1917]: loss 1.121532
[epoch4, step1918]: loss 0.909857
[epoch4, step1919]: loss 0.744434
[epoch4, step1920]: loss 0.797859
[epoch4, step1921]: loss 0.550082
[epoch4, step1922]: loss 0.792690
[epoch4, step1923]: loss 0.680266
[epoch4, step1924]: loss 0.573521
[epoch4, step1925]: loss 0.533203
[epoch4, step1926]: loss 0.870165
[epoch4, step1927]: loss 0.741211
[epoch4, step1928]: loss 0.856394
[epoch4, step1929]: loss 0.431728
[epoch4, step1930]: loss 0.761102
[epoch4, step1931]: loss 0.566031
[epoch4, step1932]: loss 0.695968
[epoch4, step1933]: loss 0.849502
[epoch4, step1934]: loss 0.639099
[epoch4, step1935]: loss 0.911938
[epoch4, step1936]: loss 0.999624
[epoch4, step1937]: loss 0.626487
[epoch4, step1938]: loss 0.688926
[epoch4, step1939]: loss 0.809163
[epoch4, step1940]: loss 0.874071
[epoch4, step1941]: loss 0.643770
[epoch4, step1942]: loss 0.914234
[epoch4, step1943]: loss 0.677237
[epoch4, step1944]: loss 0.825159
[epoch4, step1945]: loss 0.755951
[epoch4, step1946]: loss 0.948364
[epoch4, step1947]: loss 0.830527
[epoch4, step1948]: loss 1.146875
[epoch4, step1949]: loss 0.931644
[epoch4, step1950]: loss 0.428812
[epoch4, step1951]: loss 0.909247
[epoch4, step1952]: loss 0.827684
[epoch4, step1953]: loss 0.768614
[epoch4, step1954]: loss 0.954798
[epoch4, step1955]: loss 0.546629
[epoch4, step1956]: loss 0.612794
[epoch4, step1957]: loss 0.812521
[epoch4, step1958]: loss 0.709650
[epoch4, step1959]: loss 0.360412
[epoch4, step1960]: loss 0.740969
[epoch4, step1961]: loss 0.569036
[epoch4, step1962]: loss 1.118801
[epoch4, step1963]: loss 0.661496
[epoch4, step1964]: loss 0.753567
[epoch4, step1965]: loss 0.792295
[epoch4, step1966]: loss 1.100566
[epoch4, step1967]: loss 0.823462
[epoch4, step1968]: loss 0.753345
[epoch4, step1969]: loss 0.919449
[epoch4, step1970]: loss 0.922626
[epoch4, step1971]: loss 0.582583
[epoch4, step1972]: loss 0.918641
[epoch4, step1973]: loss 0.783528
[epoch4, step1974]: loss 0.843507
[epoch4, step1975]: loss 0.557612
[epoch4, step1976]: loss 0.992709
[epoch4, step1977]: loss 0.580157
[epoch4, step1978]: loss 0.907996
[epoch4, step1979]: loss 0.733812
[epoch4, step1980]: loss 0.409105
[epoch4, step1981]: loss 0.748603
[epoch4, step1982]: loss 0.635571
[epoch4, step1983]: loss 0.444808
[epoch4, step1984]: loss 0.941962
[epoch4, step1985]: loss 0.992205
[epoch4, step1986]: loss 0.799746
[epoch4, step1987]: loss 0.763581
[epoch4, step1988]: loss 0.636938
[epoch4, step1989]: loss 0.605941
[epoch4, step1990]: loss 0.759102
[epoch4, step1991]: loss 0.535586
[epoch4, step1992]: loss 0.692038
[epoch4, step1993]: loss 0.718894
[epoch4, step1994]: loss 0.816823
[epoch4, step1995]: loss 0.832004
[epoch4, step1996]: loss 0.403272
[epoch4, step1997]: loss 0.859971
[epoch4, step1998]: loss 1.038674
[epoch4, step1999]: loss 0.433571
[epoch4, step2000]: loss 0.296827
[epoch4, step2001]: loss 0.798941
[epoch4, step2002]: loss 0.562252
[epoch4, step2003]: loss 0.765789
[epoch4, step2004]: loss 0.582842
[epoch4, step2005]: loss 0.841474
[epoch4, step2006]: loss 0.832344
[epoch4, step2007]: loss 0.990321
[epoch4, step2008]: loss 0.797791
[epoch4, step2009]: loss 0.354953
[epoch4, step2010]: loss 1.057562
[epoch4, step2011]: loss 0.653339
[epoch4, step2012]: loss 0.790858
[epoch4, step2013]: loss 0.762062
[epoch4, step2014]: loss 0.652500
[epoch4, step2015]: loss 0.618104
[epoch4, step2016]: loss 1.047862
[epoch4, step2017]: loss 0.767725
[epoch4, step2018]: loss 0.469579
[epoch4, step2019]: loss 0.939084
[epoch4, step2020]: loss 0.728735
[epoch4, step2021]: loss 0.277219
[epoch4, step2022]: loss 0.791190
[epoch4, step2023]: loss 0.572843
[epoch4, step2024]: loss 0.682756
[epoch4, step2025]: loss 0.726061
[epoch4, step2026]: loss 0.813731
[epoch4, step2027]: loss 0.793939
[epoch4, step2028]: loss 0.920538
[epoch4, step2029]: loss 0.454997
[epoch4, step2030]: loss 0.843579
[epoch4, step2031]: loss 0.984153
[epoch4, step2032]: loss 0.876798
[epoch4, step2033]: loss 0.885489
[epoch4, step2034]: loss 0.741482
[epoch4, step2035]: loss 0.847853
[epoch4, step2036]: loss 0.496021
[epoch4, step2037]: loss 0.731532
[epoch4, step2038]: loss 0.810375
[epoch4, step2039]: loss 1.159110
[epoch4, step2040]: loss 0.654085
[epoch4, step2041]: loss 0.612544
[epoch4, step2042]: loss 0.881143
[epoch4, step2043]: loss 0.495248
[epoch4, step2044]: loss 0.612225
[epoch4, step2045]: loss 0.665865
[epoch4, step2046]: loss 0.945119
[epoch4, step2047]: loss 0.953014
[epoch4, step2048]: loss 0.450242
[epoch4, step2049]: loss 0.994588
[epoch4, step2050]: loss 0.661787
[epoch4, step2051]: loss 0.718278
[epoch4, step2052]: loss 0.877503
[epoch4, step2053]: loss 0.554773
[epoch4, step2054]: loss 0.841206
[epoch4, step2055]: loss 1.098354
[epoch4, step2056]: loss 0.790962
[epoch4, step2057]: loss 0.821961
[epoch4, step2058]: loss 0.776283
[epoch4, step2059]: loss 0.700240
[epoch4, step2060]: loss 0.891655
[epoch4, step2061]: loss 0.923663
[epoch4, step2062]: loss 0.572315
[epoch4, step2063]: loss 0.782523
[epoch4, step2064]: loss 0.790965
[epoch4, step2065]: loss 0.450771
[epoch4, step2066]: loss 0.617755
[epoch4, step2067]: loss 0.762936
[epoch4, step2068]: loss 0.967390
[epoch4, step2069]: loss 0.637332
[epoch4, step2070]: loss 0.771446
[epoch4, step2071]: loss 0.440557
[epoch4, step2072]: loss 0.845711
[epoch4, step2073]: loss 0.680253
[epoch4, step2074]: loss 0.801732
[epoch4, step2075]: loss 0.824063
[epoch4, step2076]: loss 0.571968
[epoch4, step2077]: loss 0.721692
[epoch4, step2078]: loss 0.618341
[epoch4, step2079]: loss 0.747941
[epoch4, step2080]: loss 0.823440
[epoch4, step2081]: loss 1.044494
[epoch4, step2082]: loss 0.847788
[epoch4, step2083]: loss 0.876513
[epoch4, step2084]: loss 0.721538
[epoch4, step2085]: loss 0.640531
[epoch4, step2086]: loss 0.672384
[epoch4, step2087]: loss 0.412748
[epoch4, step2088]: loss 0.786481
[epoch4, step2089]: loss 0.760618
[epoch4, step2090]: loss 0.636617
[epoch4, step2091]: loss 0.802149
[epoch4, step2092]: loss 0.844772
[epoch4, step2093]: loss 0.867359
[epoch4, step2094]: loss 0.578979
[epoch4, step2095]: loss 0.731292
[epoch4, step2096]: loss 0.831734
[epoch4, step2097]: loss 0.778808
[epoch4, step2098]: loss 1.032894
[epoch4, step2099]: loss 0.747239
[epoch4, step2100]: loss 0.556733
[epoch4, step2101]: loss 0.808276
[epoch4, step2102]: loss 0.631971
[epoch4, step2103]: loss 0.693625
[epoch4, step2104]: loss 0.947788
[epoch4, step2105]: loss 0.553548
[epoch4, step2106]: loss 0.701593
[epoch4, step2107]: loss 0.572266
[epoch4, step2108]: loss 0.653375
[epoch4, step2109]: loss 0.883982
[epoch4, step2110]: loss 0.850716
[epoch4, step2111]: loss 0.595200
[epoch4, step2112]: loss 0.807530
[epoch4, step2113]: loss 0.597082
[epoch4, step2114]: loss 0.781490
[epoch4, step2115]: loss 0.701016
[epoch4, step2116]: loss 0.663173
[epoch4, step2117]: loss 0.677513
[epoch4, step2118]: loss 0.943109
[epoch4, step2119]: loss 0.874714
[epoch4, step2120]: loss 1.069368
[epoch4, step2121]: loss 0.783228
[epoch4, step2122]: loss 0.597121
[epoch4, step2123]: loss 0.951586
[epoch4, step2124]: loss 0.944583
[epoch4, step2125]: loss 0.961436
[epoch4, step2126]: loss 0.561675
[epoch4, step2127]: loss 0.622444
[epoch4, step2128]: loss 0.890721
[epoch4, step2129]: loss 0.670369
[epoch4, step2130]: loss 0.661234
[epoch4, step2131]: loss 0.632461
[epoch4, step2132]: loss 0.874256
[epoch4, step2133]: loss 0.420579
[epoch4, step2134]: loss 0.549081
[epoch4, step2135]: loss 0.572093
[epoch4, step2136]: loss 0.951950
[epoch4, step2137]: loss 0.670149
[epoch4, step2138]: loss 0.724193
[epoch4, step2139]: loss 0.525840
[epoch4, step2140]: loss 0.972388
[epoch4, step2141]: loss 0.908661
[epoch4, step2142]: loss 0.824915
[epoch4, step2143]: loss 0.283633
[epoch4, step2144]: loss 0.951095
[epoch4, step2145]: loss 0.862553
[epoch4, step2146]: loss 0.732572
[epoch4, step2147]: loss 0.788492
[epoch4, step2148]: loss 0.553692
[epoch4, step2149]: loss 0.951874
[epoch4, step2150]: loss 0.749571
[epoch4, step2151]: loss 0.890489
[epoch4, step2152]: loss 0.711530
[epoch4, step2153]: loss 0.525878
[epoch4, step2154]: loss 0.503649
[epoch4, step2155]: loss 0.775156
[epoch4, step2156]: loss 0.754882
[epoch4, step2157]: loss 0.799815
[epoch4, step2158]: loss 0.598960
[epoch4, step2159]: loss 0.803734
[epoch4, step2160]: loss 0.670572
[epoch4, step2161]: loss 0.813604
[epoch4, step2162]: loss 0.702606
[epoch4, step2163]: loss 0.824697
[epoch4, step2164]: loss 0.865068
[epoch4, step2165]: loss 0.796521
[epoch4, step2166]: loss 0.685179
[epoch4, step2167]: loss 0.735798
[epoch4, step2168]: loss 0.963725
[epoch4, step2169]: loss 0.850540
[epoch4, step2170]: loss 0.696802
[epoch4, step2171]: loss 0.871255
[epoch4, step2172]: loss 0.791130
[epoch4, step2173]: loss 0.719540
[epoch4, step2174]: loss 0.916814
[epoch4, step2175]: loss 0.685410
[epoch4, step2176]: loss 0.480566
[epoch4, step2177]: loss 0.719018
[epoch4, step2178]: loss 0.763412
[epoch4, step2179]: loss 0.697162
[epoch4, step2180]: loss 0.490553
[epoch4, step2181]: loss 0.826767
[epoch4, step2182]: loss 0.762947
[epoch4, step2183]: loss 0.582807
[epoch4, step2184]: loss 0.817163
[epoch4, step2185]: loss 0.931259
[epoch4, step2186]: loss 0.412162
[epoch4, step2187]: loss 0.909089
[epoch4, step2188]: loss 0.843384
[epoch4, step2189]: loss 0.815166
[epoch4, step2190]: loss 0.308528
[epoch4, step2191]: loss 0.878502
[epoch4, step2192]: loss 0.843674
[epoch4, step2193]: loss 0.782724
[epoch4, step2194]: loss 0.716201
[epoch4, step2195]: loss 0.967311
[epoch4, step2196]: loss 0.593445
[epoch4, step2197]: loss 0.712860
[epoch4, step2198]: loss 0.567627
[epoch4, step2199]: loss 0.576846
[epoch4, step2200]: loss 0.740099
[epoch4, step2201]: loss 0.611995
[epoch4, step2202]: loss 0.715481
[epoch4, step2203]: loss 0.894183
[epoch4, step2204]: loss 0.805169
[epoch4, step2205]: loss 0.883524
[epoch4, step2206]: loss 0.845217
[epoch4, step2207]: loss 0.849929
[epoch4, step2208]: loss 0.768423
[epoch4, step2209]: loss 0.802572
[epoch4, step2210]: loss 0.793520
[epoch4, step2211]: loss 0.939700
[epoch4, step2212]: loss 1.015266
[epoch4, step2213]: loss 0.838184
[epoch4, step2214]: loss 0.779283
[epoch4, step2215]: loss 0.771241
[epoch4, step2216]: loss 0.494403
[epoch4, step2217]: loss 0.586455
[epoch4, step2218]: loss 0.836664
[epoch4, step2219]: loss 0.935247
[epoch4, step2220]: loss 0.684072
[epoch4, step2221]: loss 0.528222
[epoch4, step2222]: loss 0.754120
[epoch4, step2223]: loss 0.899671
[epoch4, step2224]: loss 0.727398
[epoch4, step2225]: loss 0.782361
[epoch4, step2226]: loss 0.675168
[epoch4, step2227]: loss 0.821367
[epoch4, step2228]: loss 0.798080
[epoch4, step2229]: loss 0.757698
[epoch4, step2230]: loss 0.881427
[epoch4, step2231]: loss 0.655470
[epoch4, step2232]: loss 0.941798
[epoch4, step2233]: loss 0.807007
[epoch4, step2234]: loss 0.518823
[epoch4, step2235]: loss 0.952407
[epoch4, step2236]: loss 0.320720
[epoch4, step2237]: loss 0.694170
[epoch4, step2238]: loss 0.864333
[epoch4, step2239]: loss 0.680667
[epoch4, step2240]: loss 0.725767
[epoch4, step2241]: loss 0.933293
[epoch4, step2242]: loss 0.371711
[epoch4, step2243]: loss 0.936132
[epoch4, step2244]: loss 0.921014
[epoch4, step2245]: loss 0.737934
[epoch4, step2246]: loss 0.754126
[epoch4, step2247]: loss 0.468880
[epoch4, step2248]: loss 0.830961
[epoch4, step2249]: loss 0.602937
[epoch4, step2250]: loss 0.712772
[epoch4, step2251]: loss 0.798088
[epoch4, step2252]: loss 0.879086
[epoch4, step2253]: loss 0.920456
[epoch4, step2254]: loss 0.779040
[epoch4, step2255]: loss 0.768775
[epoch4, step2256]: loss 0.837075
[epoch4, step2257]: loss 0.891142
[epoch4, step2258]: loss 0.782449
[epoch4, step2259]: loss 1.007645
[epoch4, step2260]: loss 1.006647
[epoch4, step2261]: loss 0.634583
[epoch4, step2262]: loss 0.737375
[epoch4, step2263]: loss 0.907491
[epoch4, step2264]: loss 1.112812
[epoch4, step2265]: loss 0.637637
[epoch4, step2266]: loss 0.733390
[epoch4, step2267]: loss 0.805382
[epoch4, step2268]: loss 0.593445
[epoch4, step2269]: loss 0.955828
[epoch4, step2270]: loss 0.960885
[epoch4, step2271]: loss 0.823276
[epoch4, step2272]: loss 0.853413
[epoch4, step2273]: loss 0.776317
[epoch4, step2274]: loss 0.303491
[epoch4, step2275]: loss 1.033308
[epoch4, step2276]: loss 0.761701
[epoch4, step2277]: loss 1.057359
[epoch4, step2278]: loss 0.825476
[epoch4, step2279]: loss 0.736718
[epoch4, step2280]: loss 0.314168
[epoch4, step2281]: loss 1.044116
[epoch4, step2282]: loss 0.619350
[epoch4, step2283]: loss 0.819423
[epoch4, step2284]: loss 0.560148
[epoch4, step2285]: loss 1.073052
[epoch4, step2286]: loss 0.827797
[epoch4, step2287]: loss 0.656893
[epoch4, step2288]: loss 0.561071
[epoch4, step2289]: loss 0.745934
[epoch4, step2290]: loss 0.748309
[epoch4, step2291]: loss 0.378013
[epoch4, step2292]: loss 0.707929
[epoch4, step2293]: loss 0.960260
[epoch4, step2294]: loss 0.516968
[epoch4, step2295]: loss 0.649175
[epoch4, step2296]: loss 0.882740
[epoch4, step2297]: loss 0.889336
[epoch4, step2298]: loss 0.760993
[epoch4, step2299]: loss 0.536008
[epoch4, step2300]: loss 0.685157
[epoch4, step2301]: loss 0.736927
[epoch4, step2302]: loss 0.633981
[epoch4, step2303]: loss 0.869043
[epoch4, step2304]: loss 0.673585
[epoch4, step2305]: loss 0.758747
[epoch4, step2306]: loss 0.831723
[epoch4, step2307]: loss 0.745491
[epoch4, step2308]: loss 0.689694
[epoch4, step2309]: loss 0.327823
[epoch4, step2310]: loss 0.808275
[epoch4, step2311]: loss 0.788929
[epoch4, step2312]: loss 0.778416
[epoch4, step2313]: loss 0.744036
[epoch4, step2314]: loss 0.661336
[epoch4, step2315]: loss 0.838877
[epoch4, step2316]: loss 1.060533
[epoch4, step2317]: loss 0.555140
[epoch4, step2318]: loss 0.754835
[epoch4, step2319]: loss 0.923674
[epoch4, step2320]: loss 0.755510
[epoch4, step2321]: loss 0.741005
[epoch4, step2322]: loss 0.796302
[epoch4, step2323]: loss 0.690301
[epoch4, step2324]: loss 0.837216
[epoch4, step2325]: loss 0.651513
[epoch4, step2326]: loss 1.087987
[epoch4, step2327]: loss 0.909304
[epoch4, step2328]: loss 0.776386
[epoch4, step2329]: loss 0.769059
[epoch4, step2330]: loss 0.781854
[epoch4, step2331]: loss 0.988099
[epoch4, step2332]: loss 0.753574
[epoch4, step2333]: loss 0.934324
[epoch4, step2334]: loss 0.855035
[epoch4, step2335]: loss 0.744543
[epoch4, step2336]: loss 0.641632
[epoch4, step2337]: loss 0.735525
[epoch4, step2338]: loss 0.734330
[epoch4, step2339]: loss 1.001616
[epoch4, step2340]: loss 0.905914
[epoch4, step2341]: loss 0.784674
[epoch4, step2342]: loss 0.778663
[epoch4, step2343]: loss 1.040462
[epoch4, step2344]: loss 0.722379
[epoch4, step2345]: loss 0.621249
[epoch4, step2346]: loss 0.880356
[epoch4, step2347]: loss 0.626415
[epoch4, step2348]: loss 0.585681
[epoch4, step2349]: loss 0.810200
[epoch4, step2350]: loss 0.466693
[epoch4, step2351]: loss 0.745837
[epoch4, step2352]: loss 0.971373
[epoch4, step2353]: loss 1.061441
[epoch4, step2354]: loss 0.434205
[epoch4, step2355]: loss 0.874647
[epoch4, step2356]: loss 0.460332
[epoch4, step2357]: loss 0.670487
[epoch4, step2358]: loss 0.751044
[epoch4, step2359]: loss 0.859034
[epoch4, step2360]: loss 0.959783
[epoch4, step2361]: loss 0.705362
[epoch4, step2362]: loss 0.689459
[epoch4, step2363]: loss 0.853362
[epoch4, step2364]: loss 0.320221
[epoch4, step2365]: loss 1.068446
[epoch4, step2366]: loss 0.701882
[epoch4, step2367]: loss 0.769646
[epoch4, step2368]: loss 0.862596
[epoch4, step2369]: loss 0.526280
[epoch4, step2370]: loss 0.645233
[epoch4, step2371]: loss 0.574616
[epoch4, step2372]: loss 0.926621
[epoch4, step2373]: loss 0.365722
[epoch4, step2374]: loss 0.507877
[epoch4, step2375]: loss 0.872811
[epoch4, step2376]: loss 0.722784
[epoch4, step2377]: loss 0.846038
[epoch4, step2378]: loss 0.765191
[epoch4, step2379]: loss 0.819653
[epoch4, step2380]: loss 0.856499
[epoch4, step2381]: loss 0.771072
[epoch4, step2382]: loss 0.960470
[epoch4, step2383]: loss 0.652767
[epoch4, step2384]: loss 0.817097
[epoch4, step2385]: loss 0.909826
[epoch4, step2386]: loss 0.863923
[epoch4, step2387]: loss 0.750620
[epoch4, step2388]: loss 0.802280
[epoch4, step2389]: loss 0.560077
[epoch4, step2390]: loss 0.758091
[epoch4, step2391]: loss 0.730746
[epoch4, step2392]: loss 0.905151
[epoch4, step2393]: loss 0.700426
[epoch4, step2394]: loss 0.648353
[epoch4, step2395]: loss 0.832256
[epoch4, step2396]: loss 0.857523
[epoch4, step2397]: loss 0.790777
[epoch4, step2398]: loss 0.815144
[epoch4, step2399]: loss 0.735311
[epoch4, step2400]: loss 1.107370
[epoch4, step2401]: loss 0.856059
[epoch4, step2402]: loss 0.548745
[epoch4, step2403]: loss 0.595211
[epoch4, step2404]: loss 0.712239
[epoch4, step2405]: loss 0.902761
[epoch4, step2406]: loss 0.692997
[epoch4, step2407]: loss 0.435131
[epoch4, step2408]: loss 0.757356
[epoch4, step2409]: loss 1.083442
[epoch4, step2410]: loss 0.818709
[epoch4, step2411]: loss 0.803832
[epoch4, step2412]: loss 0.856609
[epoch4, step2413]: loss 0.723876
[epoch4, step2414]: loss 0.888958
[epoch4, step2415]: loss 0.791676
[epoch4, step2416]: loss 0.885404
[epoch4, step2417]: loss 0.998160
[epoch4, step2418]: loss 0.597623
[epoch4, step2419]: loss 0.755418
[epoch4, step2420]: loss 0.907741
[epoch4, step2421]: loss 0.747933
[epoch4, step2422]: loss 0.930383
[epoch4, step2423]: loss 0.651202
[epoch4, step2424]: loss 0.689744
[epoch4, step2425]: loss 0.798854
[epoch4, step2426]: loss 0.752644
[epoch4, step2427]: loss 0.984152
[epoch4, step2428]: loss 0.876856
[epoch4, step2429]: loss 0.854312
[epoch4, step2430]: loss 0.732083
[epoch4, step2431]: loss 0.808201
[epoch4, step2432]: loss 0.760227
[epoch4, step2433]: loss 0.736435
[epoch4, step2434]: loss 0.863696
[epoch4, step2435]: loss 0.524198
[epoch4, step2436]: loss 0.636420
[epoch4, step2437]: loss 0.755476
[epoch4, step2438]: loss 0.669355
[epoch4, step2439]: loss 0.753967
[epoch4, step2440]: loss 0.897167
[epoch4, step2441]: loss 0.776046
[epoch4, step2442]: loss 0.547439
[epoch4, step2443]: loss 0.866700
[epoch4, step2444]: loss 0.894928
[epoch4, step2445]: loss 0.746023
[epoch4, step2446]: loss 0.418002
[epoch4, step2447]: loss 0.836981
[epoch4, step2448]: loss 0.750235
[epoch4, step2449]: loss 0.703951
[epoch4, step2450]: loss 0.834808
[epoch4, step2451]: loss 0.710399
[epoch4, step2452]: loss 0.871368
[epoch4, step2453]: loss 0.560458
[epoch4, step2454]: loss 0.621783
[epoch4, step2455]: loss 1.012378
[epoch4, step2456]: loss 0.661532
[epoch4, step2457]: loss 0.881156
[epoch4, step2458]: loss 0.961567
[epoch4, step2459]: loss 0.939871
[epoch4, step2460]: loss 0.830742
[epoch4, step2461]: loss 0.694676
[epoch4, step2462]: loss 0.765867
[epoch4, step2463]: loss 0.736086
[epoch4, step2464]: loss 0.539368
[epoch4, step2465]: loss 0.686728
[epoch4, step2466]: loss 0.633203
[epoch4, step2467]: loss 0.778314
[epoch4, step2468]: loss 0.810684
[epoch4, step2469]: loss 0.736031
[epoch4, step2470]: loss 0.864233
[epoch4, step2471]: loss 0.696399
[epoch4, step2472]: loss 0.980806
[epoch4, step2473]: loss 0.894654
[epoch4, step2474]: loss 0.772458
[epoch4, step2475]: loss 0.791665
[epoch4, step2476]: loss 1.113835
[epoch4, step2477]: loss 0.882209
[epoch4, step2478]: loss 0.599944
[epoch4, step2479]: loss 0.901713
[epoch4, step2480]: loss 0.681340
[epoch4, step2481]: loss 0.868039
[epoch4, step2482]: loss 0.686582
[epoch4, step2483]: loss 0.822310
[epoch4, step2484]: loss 0.792258
[epoch4, step2485]: loss 0.768692
[epoch4, step2486]: loss 0.696277
[epoch4, step2487]: loss 0.874596
[epoch4, step2488]: loss 0.532499
[epoch4, step2489]: loss 0.781051
[epoch4, step2490]: loss 0.586468
[epoch4, step2491]: loss 0.792586
[epoch4, step2492]: loss 0.524731
[epoch4, step2493]: loss 0.781223
[epoch4, step2494]: loss 0.930090
[epoch4, step2495]: loss 0.756904
[epoch4, step2496]: loss 0.804032
[epoch4, step2497]: loss 0.454120
[epoch4, step2498]: loss 0.702777
[epoch4, step2499]: loss 0.704248
[epoch4, step2500]: loss 0.621902
[epoch4, step2501]: loss 0.681214
[epoch4, step2502]: loss 1.004910
[epoch4, step2503]: loss 0.581369
[epoch4, step2504]: loss 0.694635
[epoch4, step2505]: loss 0.792890
[epoch4, step2506]: loss 0.834929
[epoch4, step2507]: loss 0.748819
[epoch4, step2508]: loss 0.971651
[epoch4, step2509]: loss 0.604985
[epoch4, step2510]: loss 0.730176
[epoch4, step2511]: loss 0.981810
[epoch4, step2512]: loss 0.697621
[epoch4, step2513]: loss 0.784985
[epoch4, step2514]: loss 1.037577
[epoch4, step2515]: loss 0.598327
[epoch4, step2516]: loss 0.424207
[epoch4, step2517]: loss 0.650816
[epoch4, step2518]: loss 0.883197
[epoch4, step2519]: loss 0.673810
[epoch4, step2520]: loss 0.682528
[epoch4, step2521]: loss 0.713509
[epoch4, step2522]: loss 0.648567
[epoch4, step2523]: loss 0.889764
[epoch4, step2524]: loss 0.785858
[epoch4, step2525]: loss 0.764194
[epoch4, step2526]: loss 1.050384
[epoch4, step2527]: loss 0.872700
[epoch4, step2528]: loss 0.824144
[epoch4, step2529]: loss 0.829151
[epoch4, step2530]: loss 0.536264
[epoch4, step2531]: loss 0.640971
[epoch4, step2532]: loss 0.611302
[epoch4, step2533]: loss 0.808810
[epoch4, step2534]: loss 0.573903
[epoch4, step2535]: loss 0.672446
[epoch4, step2536]: loss 0.712989
[epoch4, step2537]: loss 0.948907
[epoch4, step2538]: loss 0.854765
[epoch4, step2539]: loss 0.846213
[epoch4, step2540]: loss 0.776947
[epoch4, step2541]: loss 0.644108
[epoch4, step2542]: loss 0.868830
[epoch4, step2543]: loss 0.463918
[epoch4, step2544]: loss 0.711159
[epoch4, step2545]: loss 0.838764
[epoch4, step2546]: loss 0.780114
[epoch4, step2547]: loss 0.732054
[epoch4, step2548]: loss 0.538978
[epoch4, step2549]: loss 0.341805
[epoch4, step2550]: loss 0.651805
[epoch4, step2551]: loss 0.870306
[epoch4, step2552]: loss 0.632866
[epoch4, step2553]: loss 0.909384
[epoch4, step2554]: loss 0.919003
[epoch4, step2555]: loss 0.975469
[epoch4, step2556]: loss 0.706978
[epoch4, step2557]: loss 0.999141
[epoch4, step2558]: loss 0.882234
[epoch4, step2559]: loss 0.995788
[epoch4, step2560]: loss 0.607753
[epoch4, step2561]: loss 0.985424
[epoch4, step2562]: loss 0.633168
[epoch4, step2563]: loss 0.928015
[epoch4, step2564]: loss 0.821839
[epoch4, step2565]: loss 0.717233
[epoch4, step2566]: loss 0.786582
[epoch4, step2567]: loss 0.653511
[epoch4, step2568]: loss 0.700689
[epoch4, step2569]: loss 0.926842
[epoch4, step2570]: loss 0.809650
[epoch4, step2571]: loss 0.922971
[epoch4, step2572]: loss 0.720539
[epoch4, step2573]: loss 0.904743
[epoch4, step2574]: loss 0.602549
[epoch4, step2575]: loss 0.856787
[epoch4, step2576]: loss 0.908815
[epoch4, step2577]: loss 0.710052
[epoch4, step2578]: loss 0.761319
[epoch4, step2579]: loss 0.817582
[epoch4, step2580]: loss 0.172541
[epoch4, step2581]: loss 0.661438
[epoch4, step2582]: loss 0.737431
[epoch4, step2583]: loss 0.803777
[epoch4, step2584]: loss 0.723299
[epoch4, step2585]: loss 0.932069
[epoch4, step2586]: loss 0.884216
[epoch4, step2587]: loss 0.501755
[epoch4, step2588]: loss 0.740789
[epoch4, step2589]: loss 0.729582
[epoch4, step2590]: loss 0.861026
[epoch4, step2591]: loss 0.766040
[epoch4, step2592]: loss 0.545413
[epoch4, step2593]: loss 0.817477
[epoch4, step2594]: loss 0.947077
[epoch4, step2595]: loss 0.948680
[epoch4, step2596]: loss 0.257080
[epoch4, step2597]: loss 0.804141
[epoch4, step2598]: loss 1.016780
[epoch4, step2599]: loss 0.631289
[epoch4, step2600]: loss 0.784660
[epoch4, step2601]: loss 0.732680
[epoch4, step2602]: loss 0.684430
[epoch4, step2603]: loss 0.638169
[epoch4, step2604]: loss 0.859508
[epoch4, step2605]: loss 0.951663
[epoch4, step2606]: loss 0.927746
[epoch4, step2607]: loss 0.648108
[epoch4, step2608]: loss 0.893403
[epoch4, step2609]: loss 0.926110
[epoch4, step2610]: loss 0.824655
[epoch4, step2611]: loss 1.006244
[epoch4, step2612]: loss 0.737391
[epoch4, step2613]: loss 0.936114
[epoch4, step2614]: loss 0.679103
[epoch4, step2615]: loss 0.667047
[epoch4, step2616]: loss 0.758241
[epoch4, step2617]: loss 0.678246
[epoch4, step2618]: loss 0.688786
[epoch4, step2619]: loss 0.629597
[epoch4, step2620]: loss 0.820642
[epoch4, step2621]: loss 0.769176
[epoch4, step2622]: loss 0.481150
[epoch4, step2623]: loss 0.783130
[epoch4, step2624]: loss 0.888139
[epoch4, step2625]: loss 0.852025
[epoch4, step2626]: loss 0.933685
[epoch4, step2627]: loss 0.726830
[epoch4, step2628]: loss 0.878697
[epoch4, step2629]: loss 0.905252
[epoch4, step2630]: loss 1.026488
[epoch4, step2631]: loss 0.674962
[epoch4, step2632]: loss 0.294773
[epoch4, step2633]: loss 0.895119
[epoch4, step2634]: loss 0.636214
[epoch4, step2635]: loss 0.732819
[epoch4, step2636]: loss 0.986363
[epoch4, step2637]: loss 0.588869
[epoch4, step2638]: loss 0.833136
[epoch4, step2639]: loss 1.001066
[epoch4, step2640]: loss 0.826284
[epoch4, step2641]: loss 0.565794
[epoch4, step2642]: loss 0.733966
[epoch4, step2643]: loss 0.949426
[epoch4, step2644]: loss 0.482139
[epoch4, step2645]: loss 0.414838
[epoch4, step2646]: loss 0.777777
[epoch4, step2647]: loss 0.601534
[epoch4, step2648]: loss 0.834727
[epoch4, step2649]: loss 0.956374
[epoch4, step2650]: loss 0.968644
[epoch4, step2651]: loss 0.537768
[epoch4, step2652]: loss 0.842480
[epoch4, step2653]: loss 0.592511
[epoch4, step2654]: loss 0.678059
[epoch4, step2655]: loss 0.629447
[epoch4, step2656]: loss 0.797586
[epoch4, step2657]: loss 0.766311
[epoch4, step2658]: loss 0.896202
[epoch4, step2659]: loss 0.668363
[epoch4, step2660]: loss 0.749508
[epoch4, step2661]: loss 0.482532
[epoch4, step2662]: loss 0.804491
[epoch4, step2663]: loss 0.704502
[epoch4, step2664]: loss 0.755781
[epoch4, step2665]: loss 0.520146
[epoch4, step2666]: loss 0.872703
[epoch4, step2667]: loss 0.824850
[epoch4, step2668]: loss 0.741102
[epoch4, step2669]: loss 0.845890
[epoch4, step2670]: loss 0.590508
[epoch4, step2671]: loss 0.733794
[epoch4, step2672]: loss 0.812215
[epoch4, step2673]: loss 0.799473
[epoch4, step2674]: loss 1.055426
[epoch4, step2675]: loss 0.630469
[epoch4, step2676]: loss 0.765960
[epoch4, step2677]: loss 0.674328
[epoch4, step2678]: loss 0.863016
[epoch4, step2679]: loss 0.624631
[epoch4, step2680]: loss 0.487642
[epoch4, step2681]: loss 0.735071
[epoch4, step2682]: loss 0.916156
[epoch4, step2683]: loss 0.660138
[epoch4, step2684]: loss 0.954513
[epoch4, step2685]: loss 0.653882
[epoch4, step2686]: loss 0.674260
[epoch4, step2687]: loss 0.705062
[epoch4, step2688]: loss 0.732201
[epoch4, step2689]: loss 0.691817
[epoch4, step2690]: loss 0.940054
[epoch4, step2691]: loss 0.843770
[epoch4, step2692]: loss 0.703846
[epoch4, step2693]: loss 0.619656
[epoch4, step2694]: loss 0.935381
[epoch4, step2695]: loss 0.806845
[epoch4, step2696]: loss 0.860250
[epoch4, step2697]: loss 0.622753
[epoch4, step2698]: loss 0.825700
[epoch4, step2699]: loss 0.724618
[epoch4, step2700]: loss 0.625454
[epoch4, step2701]: loss 0.834647
[epoch4, step2702]: loss 0.477659
[epoch4, step2703]: loss 0.925306
[epoch4, step2704]: loss 0.570775
[epoch4, step2705]: loss 0.918865
[epoch4, step2706]: loss 0.701553
[epoch4, step2707]: loss 0.714522
[epoch4, step2708]: loss 0.691654
[epoch4, step2709]: loss 0.609605
[epoch4, step2710]: loss 0.845853
[epoch4, step2711]: loss 0.508896
[epoch4, step2712]: loss 0.802094
[epoch4, step2713]: loss 0.723793
[epoch4, step2714]: loss 0.825606
[epoch4, step2715]: loss 0.575668
[epoch4, step2716]: loss 0.797746
[epoch4, step2717]: loss 0.627390
[epoch4, step2718]: loss 0.739625
[epoch4, step2719]: loss 0.746202
[epoch4, step2720]: loss 0.631125
[epoch4, step2721]: loss 0.651084
[epoch4, step2722]: loss 0.566301
[epoch4, step2723]: loss 0.569882
[epoch4, step2724]: loss 0.701560
[epoch4, step2725]: loss 0.764645
[epoch4, step2726]: loss 0.528951
[epoch4, step2727]: loss 0.931785
[epoch4, step2728]: loss 0.814703
[epoch4, step2729]: loss 0.571027
[epoch4, step2730]: loss 0.595054
[epoch4, step2731]: loss 1.050848
[epoch4, step2732]: loss 0.536604
[epoch4, step2733]: loss 0.800826
[epoch4, step2734]: loss 0.945228
[epoch4, step2735]: loss 0.941974
[epoch4, step2736]: loss 0.784827
[epoch4, step2737]: loss 0.867401
[epoch4, step2738]: loss 0.872601
[epoch4, step2739]: loss 0.859189
[epoch4, step2740]: loss 0.707712
[epoch4, step2741]: loss 0.710698
[epoch4, step2742]: loss 0.739149
[epoch4, step2743]: loss 0.729822
[epoch4, step2744]: loss 0.872573
[epoch4, step2745]: loss 0.872788
[epoch4, step2746]: loss 0.748925
[epoch4, step2747]: loss 0.785132
[epoch4, step2748]: loss 0.649205
[epoch4, step2749]: loss 0.922671
[epoch4, step2750]: loss 0.609809
[epoch4, step2751]: loss 0.741508
[epoch4, step2752]: loss 0.737252
[epoch4, step2753]: loss 0.870218
[epoch4, step2754]: loss 0.881910
[epoch4, step2755]: loss 0.987236
[epoch4, step2756]: loss 0.654294
[epoch4, step2757]: loss 0.737688
[epoch4, step2758]: loss 0.938979
[epoch4, step2759]: loss 0.688706
[epoch4, step2760]: loss 0.738985
[epoch4, step2761]: loss 0.456541
[epoch4, step2762]: loss 0.915831
[epoch4, step2763]: loss 0.636082
[epoch4, step2764]: loss 0.755961
[epoch4, step2765]: loss 0.845187
[epoch4, step2766]: loss 0.482274
[epoch4, step2767]: loss 1.031622
[epoch4, step2768]: loss 0.541387
[epoch4, step2769]: loss 0.666957
[epoch4, step2770]: loss 0.741569
[epoch4, step2771]: loss 0.496239
[epoch4, step2772]: loss 0.791686
[epoch4, step2773]: loss 0.631085
[epoch4, step2774]: loss 0.909479
[epoch4, step2775]: loss 0.702228
[epoch4, step2776]: loss 0.794776
[epoch4, step2777]: loss 0.880684
[epoch4, step2778]: loss 0.582035
[epoch4, step2779]: loss 0.801507
[epoch4, step2780]: loss 0.761874
[epoch4, step2781]: loss 0.868084
[epoch4, step2782]: loss 0.685311
[epoch4, step2783]: loss 0.803437
[epoch4, step2784]: loss 1.013695
[epoch4, step2785]: loss 0.668936
[epoch4, step2786]: loss 0.687305
[epoch4, step2787]: loss 0.474762
[epoch4, step2788]: loss 0.783137
[epoch4, step2789]: loss 0.789786
[epoch4, step2790]: loss 0.661768
[epoch4, step2791]: loss 0.590905
[epoch4, step2792]: loss 0.879302
[epoch4, step2793]: loss 0.886995
[epoch4, step2794]: loss 0.749785
[epoch4, step2795]: loss 0.749840
[epoch4, step2796]: loss 0.722428
[epoch4, step2797]: loss 1.026006
[epoch4, step2798]: loss 0.472765
[epoch4, step2799]: loss 0.543443
[epoch4, step2800]: loss 0.730165
[epoch4, step2801]: loss 0.891043
[epoch4, step2802]: loss 0.862548
[epoch4, step2803]: loss 0.804437
[epoch4, step2804]: loss 0.690004
[epoch4, step2805]: loss 0.669726
[epoch4, step2806]: loss 0.690674
[epoch4, step2807]: loss 0.562280
[epoch4, step2808]: loss 0.630517
[epoch4, step2809]: loss 0.602592
[epoch4, step2810]: loss 0.651848
[epoch4, step2811]: loss 0.842168
[epoch4, step2812]: loss 0.939318
[epoch4, step2813]: loss 0.687387
[epoch4, step2814]: loss 0.655298
[epoch4, step2815]: loss 0.607042
[epoch4, step2816]: loss 0.554260
[epoch4, step2817]: loss 0.733126
[epoch4, step2818]: loss 0.707880
[epoch4, step2819]: loss 0.588804
[epoch4, step2820]: loss 0.696441
[epoch4, step2821]: loss 0.794121
[epoch4, step2822]: loss 0.806496
[epoch4, step2823]: loss 0.664234
[epoch4, step2824]: loss 0.526479
[epoch4, step2825]: loss 0.601870
[epoch4, step2826]: loss 0.479494
[epoch4, step2827]: loss 0.562913
[epoch4, step2828]: loss 0.930440
[epoch4, step2829]: loss 0.697920
[epoch4, step2830]: loss 0.772699
[epoch4, step2831]: loss 0.836157
[epoch4, step2832]: loss 0.849867
[epoch4, step2833]: loss 0.402977
[epoch4, step2834]: loss 0.922694
[epoch4, step2835]: loss 0.702699
[epoch4, step2836]: loss 0.946752
[epoch4, step2837]: loss 0.509332
[epoch4, step2838]: loss 0.609334
[epoch4, step2839]: loss 0.869881
[epoch4, step2840]: loss 0.907312
[epoch4, step2841]: loss 0.575386
[epoch4, step2842]: loss 0.514911
[epoch4, step2843]: loss 0.856671
[epoch4, step2844]: loss 0.981736
[epoch4, step2845]: loss 0.564264
[epoch4, step2846]: loss 0.855777
[epoch4, step2847]: loss 0.995555
[epoch4, step2848]: loss 0.784998
[epoch4, step2849]: loss 0.761654
[epoch4, step2850]: loss 0.961814
[epoch4, step2851]: loss 0.640221
[epoch4, step2852]: loss 0.717285
[epoch4, step2853]: loss 0.458087
[epoch4, step2854]: loss 0.988677
[epoch4, step2855]: loss 0.859331
[epoch4, step2856]: loss 0.902705
[epoch4, step2857]: loss 0.631777
[epoch4, step2858]: loss 0.910484
[epoch4, step2859]: loss 0.729748
[epoch4, step2860]: loss 0.571682
[epoch4, step2861]: loss 0.748777
[epoch4, step2862]: loss 0.786408
[epoch4, step2863]: loss 0.710063
[epoch4, step2864]: loss 0.644770
[epoch4, step2865]: loss 0.625377
[epoch4, step2866]: loss 0.671365
[epoch4, step2867]: loss 0.841896
[epoch4, step2868]: loss 0.778773
[epoch4, step2869]: loss 0.774682
[epoch4, step2870]: loss 0.814750
[epoch4, step2871]: loss 0.710240
[epoch4, step2872]: loss 0.738442
[epoch4, step2873]: loss 0.780074
[epoch4, step2874]: loss 0.675578
[epoch4, step2875]: loss 0.806535
[epoch4, step2876]: loss 0.597470
[epoch4, step2877]: loss 0.745784
[epoch4, step2878]: loss 0.719289
[epoch4, step2879]: loss 0.659284
[epoch4, step2880]: loss 0.807482
[epoch4, step2881]: loss 0.800167
[epoch4, step2882]: loss 0.409914
[epoch4, step2883]: loss 0.704878
[epoch4, step2884]: loss 0.383152
[epoch4, step2885]: loss 0.822227
[epoch4, step2886]: loss 0.661107
[epoch4, step2887]: loss 0.533045
[epoch4, step2888]: loss 0.656846
[epoch4, step2889]: loss 0.896841
[epoch4, step2890]: loss 0.603405
[epoch4, step2891]: loss 0.786446
[epoch4, step2892]: loss 0.490536
[epoch4, step2893]: loss 0.665985
[epoch4, step2894]: loss 0.334134
[epoch4, step2895]: loss 0.894811
[epoch4, step2896]: loss 0.984736
[epoch4, step2897]: loss 0.854706
[epoch4, step2898]: loss 1.009767
[epoch4, step2899]: loss 0.659003
[epoch4, step2900]: loss 0.894550
[epoch4, step2901]: loss 0.642246
[epoch4, step2902]: loss 0.600300
[epoch4, step2903]: loss 0.802728
[epoch4, step2904]: loss 0.757330
[epoch4, step2905]: loss 1.073206
[epoch4, step2906]: loss 0.725550
[epoch4, step2907]: loss 0.777720
[epoch4, step2908]: loss 0.564955
[epoch4, step2909]: loss 0.433855
[epoch4, step2910]: loss 0.969077
[epoch4, step2911]: loss 0.779939
[epoch4, step2912]: loss 0.757985
[epoch4, step2913]: loss 0.931494
[epoch4, step2914]: loss 0.718643
[epoch4, step2915]: loss 0.582929
[epoch4, step2916]: loss 0.774086
[epoch4, step2917]: loss 0.574905
[epoch4, step2918]: loss 0.878833
[epoch4, step2919]: loss 0.692633
[epoch4, step2920]: loss 0.670334
[epoch4, step2921]: loss 0.404836
[epoch4, step2922]: loss 0.733788
[epoch4, step2923]: loss 0.741751
[epoch4, step2924]: loss 0.640404
[epoch4, step2925]: loss 0.944270
[epoch4, step2926]: loss 0.715178
[epoch4, step2927]: loss 0.699055
[epoch4, step2928]: loss 0.491684
[epoch4, step2929]: loss 1.127475
[epoch4, step2930]: loss 0.704251
[epoch4, step2931]: loss 0.994355
[epoch4, step2932]: loss 0.571083
[epoch4, step2933]: loss 0.738075
[epoch4, step2934]: loss 0.788314
[epoch4, step2935]: loss 0.728625
[epoch4, step2936]: loss 0.839047
[epoch4, step2937]: loss 0.816400
[epoch4, step2938]: loss 0.614423
[epoch4, step2939]: loss 0.619473
[epoch4, step2940]: loss 0.789731
[epoch4, step2941]: loss 0.497452
[epoch4, step2942]: loss 0.714974
[epoch4, step2943]: loss 0.861589
[epoch4, step2944]: loss 0.765027
[epoch4, step2945]: loss 0.680606
[epoch4, step2946]: loss 0.529353
[epoch4, step2947]: loss 0.889019
[epoch4, step2948]: loss 0.665222
[epoch4, step2949]: loss 0.817840
[epoch4, step2950]: loss 1.085534
[epoch4, step2951]: loss 0.818024
[epoch4, step2952]: loss 0.796588
[epoch4, step2953]: loss 0.692507
[epoch4, step2954]: loss 0.883906
[epoch4, step2955]: loss 0.735697
[epoch4, step2956]: loss 0.752266
[epoch4, step2957]: loss 0.606788
[epoch4, step2958]: loss 0.749944
[epoch4, step2959]: loss 0.848801
[epoch4, step2960]: loss 0.800456
[epoch4, step2961]: loss 0.660500
[epoch4, step2962]: loss 0.781597
[epoch4, step2963]: loss 0.891853
[epoch4, step2964]: loss 0.906037
[epoch4, step2965]: loss 0.819843
[epoch4, step2966]: loss 0.826324
[epoch4, step2967]: loss 0.940497
[epoch4, step2968]: loss 0.502776
[epoch4, step2969]: loss 0.911509
[epoch4, step2970]: loss 0.662728
[epoch4, step2971]: loss 0.632045
[epoch4, step2972]: loss 0.827956
[epoch4, step2973]: loss 0.533644
[epoch4, step2974]: loss 0.578265
[epoch4, step2975]: loss 0.914077
[epoch4, step2976]: loss 0.922863
[epoch4, step2977]: loss 0.815864
[epoch4, step2978]: loss 0.821697
[epoch4, step2979]: loss 0.492065
[epoch4, step2980]: loss 0.614511
[epoch4, step2981]: loss 0.552216
[epoch4, step2982]: loss 0.789202
[epoch4, step2983]: loss 0.689132
[epoch4, step2984]: loss 0.738842
[epoch4, step2985]: loss 0.866863
[epoch4, step2986]: loss 0.801400
[epoch4, step2987]: loss 0.815254
[epoch4, step2988]: loss 0.880476
[epoch4, step2989]: loss 0.739749
[epoch4, step2990]: loss 0.740274
[epoch4, step2991]: loss 0.696635
[epoch4, step2992]: loss 0.695549
[epoch4, step2993]: loss 0.465354
[epoch4, step2994]: loss 0.827580
[epoch4, step2995]: loss 1.053332
[epoch4, step2996]: loss 0.586166
[epoch4, step2997]: loss 0.599245
[epoch4, step2998]: loss 0.523609
[epoch4, step2999]: loss 0.707199
[epoch4, step3000]: loss 0.868688
[epoch4, step3001]: loss 0.873328
[epoch4, step3002]: loss 0.764115
[epoch4, step3003]: loss 0.577695
[epoch4, step3004]: loss 0.655031
[epoch4, step3005]: loss 0.592545
[epoch4, step3006]: loss 0.821165
[epoch4, step3007]: loss 0.845799
[epoch4, step3008]: loss 0.821852
[epoch4, step3009]: loss 0.540892
[epoch4, step3010]: loss 0.875439
[epoch4, step3011]: loss 0.685993
[epoch4, step3012]: loss 0.590206
[epoch4, step3013]: loss 0.785140
[epoch4, step3014]: loss 0.798200
[epoch4, step3015]: loss 0.535370
[epoch4, step3016]: loss 0.645380
[epoch4, step3017]: loss 0.697566
[epoch4, step3018]: loss 0.460312
[epoch4, step3019]: loss 0.868005
[epoch4, step3020]: loss 0.545848
[epoch4, step3021]: loss 0.762603
[epoch4, step3022]: loss 0.894028
[epoch4, step3023]: loss 0.584072
[epoch4, step3024]: loss 0.990525
[epoch4, step3025]: loss 0.810423
[epoch4, step3026]: loss 0.924655
[epoch4, step3027]: loss 0.777201
[epoch4, step3028]: loss 0.797815
[epoch4, step3029]: loss 0.619778
[epoch4, step3030]: loss 0.838942
[epoch4, step3031]: loss 0.819065
[epoch4, step3032]: loss 0.600955
[epoch4, step3033]: loss 0.880049
[epoch4, step3034]: loss 0.768292
[epoch4, step3035]: loss 0.878798
[epoch4, step3036]: loss 0.788781
[epoch4, step3037]: loss 0.838363
[epoch4, step3038]: loss 0.612917
[epoch4, step3039]: loss 0.724629
[epoch4, step3040]: loss 0.694448
[epoch4, step3041]: loss 0.761650
[epoch4, step3042]: loss 0.724687
[epoch4, step3043]: loss 0.700275
[epoch4, step3044]: loss 0.611700
[epoch4, step3045]: loss 0.966647
[epoch4, step3046]: loss 0.513755
[epoch4, step3047]: loss 0.424677
[epoch4, step3048]: loss 0.792898
[epoch4, step3049]: loss 0.793052
[epoch4, step3050]: loss 0.798890
[epoch4, step3051]: loss 0.855617
[epoch4, step3052]: loss 0.680832
[epoch4, step3053]: loss 0.599077
[epoch4, step3054]: loss 0.310114
[epoch4, step3055]: loss 0.573205
[epoch4, step3056]: loss 0.557482
[epoch4, step3057]: loss 0.919930
[epoch4, step3058]: loss 0.827448
[epoch4, step3059]: loss 0.723228
[epoch4, step3060]: loss 0.922472
[epoch4, step3061]: loss 0.513983
[epoch4, step3062]: loss 0.560514
[epoch4, step3063]: loss 0.784096
[epoch4, step3064]: loss 0.697565
[epoch4, step3065]: loss 0.852910
[epoch4, step3066]: loss 0.766654
[epoch4, step3067]: loss 0.607788
[epoch4, step3068]: loss 0.789728
[epoch4, step3069]: loss 0.864567
[epoch4, step3070]: loss 0.658931
[epoch4, step3071]: loss 0.694611
[epoch4, step3072]: loss 0.701576
[epoch4, step3073]: loss 0.475239
[epoch4, step3074]: loss 0.658473
[epoch4, step3075]: loss 0.482246
[epoch4, step3076]: loss 0.500113

[epoch4]: avg loss 0.500113

[epoch5, step1]: loss 0.565487
[epoch5, step2]: loss 0.813038
[epoch5, step3]: loss 0.449789
[epoch5, step4]: loss 0.862605
[epoch5, step5]: loss 0.619860
[epoch5, step6]: loss 0.611891
[epoch5, step7]: loss 0.995817
[epoch5, step8]: loss 0.853620
[epoch5, step9]: loss 0.749627
[epoch5, step10]: loss 0.908559
[epoch5, step11]: loss 0.780232
[epoch5, step12]: loss 0.940871
[epoch5, step13]: loss 0.669038
[epoch5, step14]: loss 0.836487
[epoch5, step15]: loss 0.761850
[epoch5, step16]: loss 0.682013
[epoch5, step17]: loss 0.746797
[epoch5, step18]: loss 0.734135
[epoch5, step19]: loss 0.775560
[epoch5, step20]: loss 0.804593
[epoch5, step21]: loss 0.655065
[epoch5, step22]: loss 0.524901
[epoch5, step23]: loss 0.513446
[epoch5, step24]: loss 0.859905
[epoch5, step25]: loss 0.694182
[epoch5, step26]: loss 1.010438
[epoch5, step27]: loss 0.415896
[epoch5, step28]: loss 0.770194
[epoch5, step29]: loss 0.606714
[epoch5, step30]: loss 0.324471
[epoch5, step31]: loss 0.899681
[epoch5, step32]: loss 0.772837
[epoch5, step33]: loss 0.598669
[epoch5, step34]: loss 0.902097
[epoch5, step35]: loss 0.592037
[epoch5, step36]: loss 0.932679
[epoch5, step37]: loss 0.867458
[epoch5, step38]: loss 0.649926
[epoch5, step39]: loss 0.701957
[epoch5, step40]: loss 0.779228
[epoch5, step41]: loss 0.676213
[epoch5, step42]: loss 0.695167
[epoch5, step43]: loss 0.873699
[epoch5, step44]: loss 0.232296
[epoch5, step45]: loss 0.506498
[epoch5, step46]: loss 0.765478
[epoch5, step47]: loss 0.950952
[epoch5, step48]: loss 0.889530
[epoch5, step49]: loss 0.661774
[epoch5, step50]: loss 0.441819
[epoch5, step51]: loss 0.755280
[epoch5, step52]: loss 0.988017
[epoch5, step53]: loss 0.646394
[epoch5, step54]: loss 0.957028
[epoch5, step55]: loss 1.089152
[epoch5, step56]: loss 0.657257
[epoch5, step57]: loss 0.617647
[epoch5, step58]: loss 0.706951
[epoch5, step59]: loss 0.851623
[epoch5, step60]: loss 0.608794
[epoch5, step61]: loss 0.938851
[epoch5, step62]: loss 0.462533
[epoch5, step63]: loss 0.849465
[epoch5, step64]: loss 0.875837
[epoch5, step65]: loss 0.900168
[epoch5, step66]: loss 0.926297
[epoch5, step67]: loss 0.730991
[epoch5, step68]: loss 0.737184
[epoch5, step69]: loss 0.668029
[epoch5, step70]: loss 0.686580
[epoch5, step71]: loss 0.712615
[epoch5, step72]: loss 0.577081
[epoch5, step73]: loss 0.709593
[epoch5, step74]: loss 0.611612
[epoch5, step75]: loss 0.564740
[epoch5, step76]: loss 0.960834
[epoch5, step77]: loss 0.979419
[epoch5, step78]: loss 0.816667
[epoch5, step79]: loss 0.658213
[epoch5, step80]: loss 0.781530
[epoch5, step81]: loss 0.761037
[epoch5, step82]: loss 0.416299
[epoch5, step83]: loss 0.558041
[epoch5, step84]: loss 0.795378
[epoch5, step85]: loss 0.595724
[epoch5, step86]: loss 0.612595
[epoch5, step87]: loss 0.818513
[epoch5, step88]: loss 0.877511
[epoch5, step89]: loss 0.399567
[epoch5, step90]: loss 0.532918
[epoch5, step91]: loss 1.153991
[epoch5, step92]: loss 1.024945
[epoch5, step93]: loss 0.713268
[epoch5, step94]: loss 0.601893
[epoch5, step95]: loss 0.889457
[epoch5, step96]: loss 0.643703
[epoch5, step97]: loss 1.019119
[epoch5, step98]: loss 0.922549
[epoch5, step99]: loss 1.038915
[epoch5, step100]: loss 0.587023
[epoch5, step101]: loss 0.753036
[epoch5, step102]: loss 0.770691
[epoch5, step103]: loss 0.953755
[epoch5, step104]: loss 0.783420
[epoch5, step105]: loss 0.426586
[epoch5, step106]: loss 0.678598
[epoch5, step107]: loss 0.603487
[epoch5, step108]: loss 0.716033
[epoch5, step109]: loss 0.764981
[epoch5, step110]: loss 0.940123
[epoch5, step111]: loss 0.702941
[epoch5, step112]: loss 0.609260
[epoch5, step113]: loss 0.710661
[epoch5, step114]: loss 0.666713
[epoch5, step115]: loss 0.800355
[epoch5, step116]: loss 0.945470
[epoch5, step117]: loss 0.905691
[epoch5, step118]: loss 0.929375
[epoch5, step119]: loss 0.781978
[epoch5, step120]: loss 0.978774
[epoch5, step121]: loss 0.507034
[epoch5, step122]: loss 0.637926
[epoch5, step123]: loss 0.579589
[epoch5, step124]: loss 0.801778
[epoch5, step125]: loss 0.496756
[epoch5, step126]: loss 1.080736
[epoch5, step127]: loss 0.788897
[epoch5, step128]: loss 0.914829
[epoch5, step129]: loss 0.421737
[epoch5, step130]: loss 0.376717
[epoch5, step131]: loss 0.963912
[epoch5, step132]: loss 1.014404
[epoch5, step133]: loss 0.656160
[epoch5, step134]: loss 0.689326
[epoch5, step135]: loss 0.743199
[epoch5, step136]: loss 0.613510
[epoch5, step137]: loss 0.570094
[epoch5, step138]: loss 0.604448
[epoch5, step139]: loss 0.883155
[epoch5, step140]: loss 0.869116
[epoch5, step141]: loss 0.555564
[epoch5, step142]: loss 0.905341
[epoch5, step143]: loss 0.824701
[epoch5, step144]: loss 0.680873
[epoch5, step145]: loss 0.630438
[epoch5, step146]: loss 0.871363
[epoch5, step147]: loss 0.845476
[epoch5, step148]: loss 0.820800
[epoch5, step149]: loss 0.799085
[epoch5, step150]: loss 0.839122
[epoch5, step151]: loss 1.063138
[epoch5, step152]: loss 0.559528
[epoch5, step153]: loss 0.615518
[epoch5, step154]: loss 0.808338
[epoch5, step155]: loss 0.432339
[epoch5, step156]: loss 0.482535
[epoch5, step157]: loss 0.670864
[epoch5, step158]: loss 0.730272
[epoch5, step159]: loss 1.038953
[epoch5, step160]: loss 0.730015
[epoch5, step161]: loss 0.504693
[epoch5, step162]: loss 0.776279
[epoch5, step163]: loss 0.819549
[epoch5, step164]: loss 0.885620
[epoch5, step165]: loss 0.778138
[epoch5, step166]: loss 0.577991
[epoch5, step167]: loss 0.949400
[epoch5, step168]: loss 0.992592
[epoch5, step169]: loss 0.623082
[epoch5, step170]: loss 0.765500
[epoch5, step171]: loss 0.790683
[epoch5, step172]: loss 0.527476
[epoch5, step173]: loss 0.724002
[epoch5, step174]: loss 0.880510
[epoch5, step175]: loss 0.844989
[epoch5, step176]: loss 0.805653
[epoch5, step177]: loss 0.718805
[epoch5, step178]: loss 0.864535
[epoch5, step179]: loss 0.797368
[epoch5, step180]: loss 0.704118
[epoch5, step181]: loss 0.740597
[epoch5, step182]: loss 0.719825
[epoch5, step183]: loss 0.714777
[epoch5, step184]: loss 1.053017
[epoch5, step185]: loss 0.664956
[epoch5, step186]: loss 0.729787
[epoch5, step187]: loss 0.896920
[epoch5, step188]: loss 0.741097
[epoch5, step189]: loss 0.589603
[epoch5, step190]: loss 0.773142
[epoch5, step191]: loss 0.968802
[epoch5, step192]: loss 0.905795
[epoch5, step193]: loss 0.899270
[epoch5, step194]: loss 0.862930
[epoch5, step195]: loss 0.714246
[epoch5, step196]: loss 0.698525
[epoch5, step197]: loss 0.680328
[epoch5, step198]: loss 0.451633
[epoch5, step199]: loss 0.727526
[epoch5, step200]: loss 0.609747
[epoch5, step201]: loss 0.791005
[epoch5, step202]: loss 0.568033
[epoch5, step203]: loss 0.729118
[epoch5, step204]: loss 0.976306
[epoch5, step205]: loss 0.822411
[epoch5, step206]: loss 0.695332
[epoch5, step207]: loss 0.913002
[epoch5, step208]: loss 0.611429
[epoch5, step209]: loss 1.003399
[epoch5, step210]: loss 0.766315
[epoch5, step211]: loss 0.794828
[epoch5, step212]: loss 0.748129
[epoch5, step213]: loss 0.838558
[epoch5, step214]: loss 0.694101
[epoch5, step215]: loss 0.569715
[epoch5, step216]: loss 0.935989
[epoch5, step217]: loss 0.888064
[epoch5, step218]: loss 0.934152
[epoch5, step219]: loss 0.437252
[epoch5, step220]: loss 0.526178
[epoch5, step221]: loss 0.693597
[epoch5, step222]: loss 0.634755
[epoch5, step223]: loss 0.829268
[epoch5, step224]: loss 0.667173
[epoch5, step225]: loss 0.779523
[epoch5, step226]: loss 0.685912
[epoch5, step227]: loss 0.787969
[epoch5, step228]: loss 0.549658
[epoch5, step229]: loss 0.646173
[epoch5, step230]: loss 0.509366
[epoch5, step231]: loss 0.796235
[epoch5, step232]: loss 0.820778
[epoch5, step233]: loss 0.819866
[epoch5, step234]: loss 1.007068
[epoch5, step235]: loss 0.721158
[epoch5, step236]: loss 0.851908
[epoch5, step237]: loss 0.932658
[epoch5, step238]: loss 0.580065
[epoch5, step239]: loss 0.605695
[epoch5, step240]: loss 0.732481
[epoch5, step241]: loss 0.706054
[epoch5, step242]: loss 0.878262
[epoch5, step243]: loss 0.851486
[epoch5, step244]: loss 1.012754
[epoch5, step245]: loss 0.668566
[epoch5, step246]: loss 0.652588
[epoch5, step247]: loss 0.605046
[epoch5, step248]: loss 0.692528
[epoch5, step249]: loss 0.730817
[epoch5, step250]: loss 0.741114
[epoch5, step251]: loss 0.560654
[epoch5, step252]: loss 0.870860
[epoch5, step253]: loss 0.800125
[epoch5, step254]: loss 0.965188
[epoch5, step255]: loss 0.870597
[epoch5, step256]: loss 0.949586
[epoch5, step257]: loss 0.602734
[epoch5, step258]: loss 0.666044
[epoch5, step259]: loss 0.499665
[epoch5, step260]: loss 0.641107
[epoch5, step261]: loss 0.623261
[epoch5, step262]: loss 0.532230
[epoch5, step263]: loss 0.622286
[epoch5, step264]: loss 0.609334
[epoch5, step265]: loss 0.582231
[epoch5, step266]: loss 0.769673
[epoch5, step267]: loss 0.849846
[epoch5, step268]: loss 0.676610
[epoch5, step269]: loss 0.573404
[epoch5, step270]: loss 0.796530
[epoch5, step271]: loss 0.744508
[epoch5, step272]: loss 0.608745
[epoch5, step273]: loss 0.739254
[epoch5, step274]: loss 0.721278
[epoch5, step275]: loss 0.897371
[epoch5, step276]: loss 0.668087
[epoch5, step277]: loss 0.850784
[epoch5, step278]: loss 0.753468
[epoch5, step279]: loss 0.873620
[epoch5, step280]: loss 0.957260
[epoch5, step281]: loss 0.918026
[epoch5, step282]: loss 0.653802
[epoch5, step283]: loss 0.996405
[epoch5, step284]: loss 0.662278
[epoch5, step285]: loss 0.635924
[epoch5, step286]: loss 0.991926
[epoch5, step287]: loss 0.706066
[epoch5, step288]: loss 0.925134
[epoch5, step289]: loss 0.804403
[epoch5, step290]: loss 0.993312
[epoch5, step291]: loss 0.920227
[epoch5, step292]: loss 0.798326
[epoch5, step293]: loss 0.855926
[epoch5, step294]: loss 0.632870
[epoch5, step295]: loss 0.668495
[epoch5, step296]: loss 0.394566
[epoch5, step297]: loss 0.838473
[epoch5, step298]: loss 0.672260
[epoch5, step299]: loss 0.899992
[epoch5, step300]: loss 0.839446
[epoch5, step301]: loss 0.792213
[epoch5, step302]: loss 0.770859
[epoch5, step303]: loss 0.666691
[epoch5, step304]: loss 0.648713
[epoch5, step305]: loss 0.849835
[epoch5, step306]: loss 1.020973
[epoch5, step307]: loss 0.628574
[epoch5, step308]: loss 0.712423
[epoch5, step309]: loss 0.853107
[epoch5, step310]: loss 0.531175
[epoch5, step311]: loss 0.565366
[epoch5, step312]: loss 0.795902
[epoch5, step313]: loss 0.682111
[epoch5, step314]: loss 0.603670
[epoch5, step315]: loss 0.846693
[epoch5, step316]: loss 0.207552
[epoch5, step317]: loss 0.601607
[epoch5, step318]: loss 0.926361
[epoch5, step319]: loss 0.877856
[epoch5, step320]: loss 0.666604
[epoch5, step321]: loss 0.686918
[epoch5, step322]: loss 0.973170
[epoch5, step323]: loss 0.788458
[epoch5, step324]: loss 0.727779
[epoch5, step325]: loss 0.673515
[epoch5, step326]: loss 0.873426
[epoch5, step327]: loss 0.993407
[epoch5, step328]: loss 0.745632
[epoch5, step329]: loss 0.726200
[epoch5, step330]: loss 0.627871
[epoch5, step331]: loss 0.707799
[epoch5, step332]: loss 0.615519
[epoch5, step333]: loss 0.751276
[epoch5, step334]: loss 0.635716
[epoch5, step335]: loss 0.641801
[epoch5, step336]: loss 0.905301
[epoch5, step337]: loss 0.733952
[epoch5, step338]: loss 0.608067
[epoch5, step339]: loss 0.615321
[epoch5, step340]: loss 0.724546
[epoch5, step341]: loss 0.520444
[epoch5, step342]: loss 0.748240
[epoch5, step343]: loss 0.373498
[epoch5, step344]: loss 0.713013
[epoch5, step345]: loss 0.609199
[epoch5, step346]: loss 0.877627
[epoch5, step347]: loss 0.769880
[epoch5, step348]: loss 0.789398
[epoch5, step349]: loss 0.842669
[epoch5, step350]: loss 0.839159
[epoch5, step351]: loss 0.680687
[epoch5, step352]: loss 0.644233
[epoch5, step353]: loss 0.727414
[epoch5, step354]: loss 0.987326
[epoch5, step355]: loss 0.445216
[epoch5, step356]: loss 0.813302
[epoch5, step357]: loss 0.662962
[epoch5, step358]: loss 0.700708
[epoch5, step359]: loss 0.830913
[epoch5, step360]: loss 0.716460
[epoch5, step361]: loss 0.631202
[epoch5, step362]: loss 0.452840
[epoch5, step363]: loss 0.471569
[epoch5, step364]: loss 0.974388
[epoch5, step365]: loss 0.726183
[epoch5, step366]: loss 0.689909
[epoch5, step367]: loss 1.062905
[epoch5, step368]: loss 0.794390
[epoch5, step369]: loss 0.686283
[epoch5, step370]: loss 0.706686
[epoch5, step371]: loss 0.871935
[epoch5, step372]: loss 0.527199
[epoch5, step373]: loss 0.672084
[epoch5, step374]: loss 0.254187
[epoch5, step375]: loss 0.805710
[epoch5, step376]: loss 0.914371
[epoch5, step377]: loss 0.879401
[epoch5, step378]: loss 0.833569
[epoch5, step379]: loss 0.640030
[epoch5, step380]: loss 0.783162
[epoch5, step381]: loss 0.720828
[epoch5, step382]: loss 0.855661
[epoch5, step383]: loss 0.838681
[epoch5, step384]: loss 0.806596
[epoch5, step385]: loss 0.628456
[epoch5, step386]: loss 0.736926
[epoch5, step387]: loss 0.728352
[epoch5, step388]: loss 0.938664
[epoch5, step389]: loss 1.056428
[epoch5, step390]: loss 0.578721
[epoch5, step391]: loss 0.760912
[epoch5, step392]: loss 0.785903
[epoch5, step393]: loss 0.834800
[epoch5, step394]: loss 0.810999
[epoch5, step395]: loss 0.737103
[epoch5, step396]: loss 0.718865
[epoch5, step397]: loss 0.894128
[epoch5, step398]: loss 0.647978
[epoch5, step399]: loss 0.616316
[epoch5, step400]: loss 0.551992
[epoch5, step401]: loss 0.604407
[epoch5, step402]: loss 0.637787
[epoch5, step403]: loss 0.883246
[epoch5, step404]: loss 0.782178
[epoch5, step405]: loss 0.895193
[epoch5, step406]: loss 0.993055
[epoch5, step407]: loss 0.733132
[epoch5, step408]: loss 0.730331
[epoch5, step409]: loss 0.775907
[epoch5, step410]: loss 0.801667
[epoch5, step411]: loss 0.739118
[epoch5, step412]: loss 0.869675
[epoch5, step413]: loss 0.605834
[epoch5, step414]: loss 0.919608
[epoch5, step415]: loss 0.913288
[epoch5, step416]: loss 0.580702
[epoch5, step417]: loss 0.662755
[epoch5, step418]: loss 0.871144
[epoch5, step419]: loss 0.681486
[epoch5, step420]: loss 0.627303
[epoch5, step421]: loss 0.619045
[epoch5, step422]: loss 0.849633
[epoch5, step423]: loss 0.965143
[epoch5, step424]: loss 0.606798
[epoch5, step425]: loss 0.536244
[epoch5, step426]: loss 0.738528
[epoch5, step427]: loss 0.564518
[epoch5, step428]: loss 0.598564
[epoch5, step429]: loss 0.549197
[epoch5, step430]: loss 0.700139
[epoch5, step431]: loss 1.085568
[epoch5, step432]: loss 0.499512
[epoch5, step433]: loss 0.743570
[epoch5, step434]: loss 0.752469
[epoch5, step435]: loss 0.669125
[epoch5, step436]: loss 0.942553
[epoch5, step437]: loss 0.772961
[epoch5, step438]: loss 0.842636
[epoch5, step439]: loss 0.672929
[epoch5, step440]: loss 0.762173
[epoch5, step441]: loss 0.487979
[epoch5, step442]: loss 0.639870
[epoch5, step443]: loss 0.561717
[epoch5, step444]: loss 0.822460
[epoch5, step445]: loss 0.584664
[epoch5, step446]: loss 0.410167
[epoch5, step447]: loss 0.811200
[epoch5, step448]: loss 0.905455
[epoch5, step449]: loss 0.647470
[epoch5, step450]: loss 0.833809
[epoch5, step451]: loss 0.779146
[epoch5, step452]: loss 0.793994
[epoch5, step453]: loss 0.746208
[epoch5, step454]: loss 0.741410
[epoch5, step455]: loss 0.805529
[epoch5, step456]: loss 0.564950
[epoch5, step457]: loss 0.466303
[epoch5, step458]: loss 0.687313
[epoch5, step459]: loss 0.876084
[epoch5, step460]: loss 0.696478
[epoch5, step461]: loss 0.490584
[epoch5, step462]: loss 0.897714
[epoch5, step463]: loss 0.649255
[epoch5, step464]: loss 0.795779
[epoch5, step465]: loss 0.786706
[epoch5, step466]: loss 0.411700
[epoch5, step467]: loss 0.839331
[epoch5, step468]: loss 0.707187
[epoch5, step469]: loss 0.813506
[epoch5, step470]: loss 0.921223
[epoch5, step471]: loss 0.778014
[epoch5, step472]: loss 0.824570
[epoch5, step473]: loss 0.673946
[epoch5, step474]: loss 0.474598
[epoch5, step475]: loss 0.692185
[epoch5, step476]: loss 0.917604
[epoch5, step477]: loss 0.989844
[epoch5, step478]: loss 0.728476
[epoch5, step479]: loss 0.865343
[epoch5, step480]: loss 0.944913
[epoch5, step481]: loss 0.847883
[epoch5, step482]: loss 0.774445
[epoch5, step483]: loss 0.587992
[epoch5, step484]: loss 0.417648
[epoch5, step485]: loss 0.917206
[epoch5, step486]: loss 0.809953
[epoch5, step487]: loss 0.909539
[epoch5, step488]: loss 0.698962
[epoch5, step489]: loss 0.899840
[epoch5, step490]: loss 0.736265
[epoch5, step491]: loss 0.667068
[epoch5, step492]: loss 0.887097
[epoch5, step493]: loss 0.300828
[epoch5, step494]: loss 0.583472
[epoch5, step495]: loss 0.660925
[epoch5, step496]: loss 0.736081
[epoch5, step497]: loss 0.656903
[epoch5, step498]: loss 0.536648
[epoch5, step499]: loss 0.876182
[epoch5, step500]: loss 0.681633
[epoch5, step501]: loss 0.671592
[epoch5, step502]: loss 0.636039
[epoch5, step503]: loss 0.613248
[epoch5, step504]: loss 0.843756
[epoch5, step505]: loss 0.491421
[epoch5, step506]: loss 0.849853
[epoch5, step507]: loss 0.661946
[epoch5, step508]: loss 0.519719
[epoch5, step509]: loss 0.794233
[epoch5, step510]: loss 1.030640
[epoch5, step511]: loss 0.741281
[epoch5, step512]: loss 0.654345
[epoch5, step513]: loss 0.889463
[epoch5, step514]: loss 0.862139
[epoch5, step515]: loss 0.729165
[epoch5, step516]: loss 0.583681
[epoch5, step517]: loss 0.529878
[epoch5, step518]: loss 0.825043
[epoch5, step519]: loss 0.468455
[epoch5, step520]: loss 0.727365
[epoch5, step521]: loss 0.465020
[epoch5, step522]: loss 0.685876
[epoch5, step523]: loss 0.550899
[epoch5, step524]: loss 0.547926
[epoch5, step525]: loss 0.670389
[epoch5, step526]: loss 0.787634
[epoch5, step527]: loss 0.642991
[epoch5, step528]: loss 0.457282
[epoch5, step529]: loss 0.813408
[epoch5, step530]: loss 0.931377
[epoch5, step531]: loss 0.829746
[epoch5, step532]: loss 0.861464
[epoch5, step533]: loss 0.563157
[epoch5, step534]: loss 0.577816
[epoch5, step535]: loss 0.799148
[epoch5, step536]: loss 0.674142
[epoch5, step537]: loss 0.679655
[epoch5, step538]: loss 0.776305
[epoch5, step539]: loss 0.753821
[epoch5, step540]: loss 0.393371
[epoch5, step541]: loss 0.780427
[epoch5, step542]: loss 0.746331
[epoch5, step543]: loss 0.727983
[epoch5, step544]: loss 0.733806
[epoch5, step545]: loss 0.995734
[epoch5, step546]: loss 0.673437
[epoch5, step547]: loss 0.748769
[epoch5, step548]: loss 0.577725
[epoch5, step549]: loss 0.676606
[epoch5, step550]: loss 0.715708
[epoch5, step551]: loss 0.658219
[epoch5, step552]: loss 0.625042
[epoch5, step553]: loss 0.712596
[epoch5, step554]: loss 0.833262
[epoch5, step555]: loss 0.883366
[epoch5, step556]: loss 0.499425
[epoch5, step557]: loss 0.810536
[epoch5, step558]: loss 0.713327
[epoch5, step559]: loss 0.657081
[epoch5, step560]: loss 0.761889
[epoch5, step561]: loss 0.702295
[epoch5, step562]: loss 0.864025
[epoch5, step563]: loss 0.460504
[epoch5, step564]: loss 0.725239
[epoch5, step565]: loss 0.803906
[epoch5, step566]: loss 0.710194
[epoch5, step567]: loss 0.844525
[epoch5, step568]: loss 0.570011
[epoch5, step569]: loss 0.604739
[epoch5, step570]: loss 0.658232
[epoch5, step571]: loss 0.815498
[epoch5, step572]: loss 0.865130
[epoch5, step573]: loss 0.854332
[epoch5, step574]: loss 0.811604
[epoch5, step575]: loss 0.853010
[epoch5, step576]: loss 0.883020
[epoch5, step577]: loss 0.601147
[epoch5, step578]: loss 0.638221
[epoch5, step579]: loss 0.402745
[epoch5, step580]: loss 0.746260
[epoch5, step581]: loss 0.813097
[epoch5, step582]: loss 0.835137
[epoch5, step583]: loss 0.691522
[epoch5, step584]: loss 0.679312
[epoch5, step585]: loss 0.792719
[epoch5, step586]: loss 0.729158
[epoch5, step587]: loss 0.454052
[epoch5, step588]: loss 0.777215
[epoch5, step589]: loss 0.811887
[epoch5, step590]: loss 0.730278
[epoch5, step591]: loss 0.761922
[epoch5, step592]: loss 0.758806
[epoch5, step593]: loss 0.599666
[epoch5, step594]: loss 0.936627
[epoch5, step595]: loss 0.683035
[epoch5, step596]: loss 0.856094
[epoch5, step597]: loss 0.778890
[epoch5, step598]: loss 0.575714
[epoch5, step599]: loss 0.677172
[epoch5, step600]: loss 0.729980
[epoch5, step601]: loss 0.975801
[epoch5, step602]: loss 0.618262
[epoch5, step603]: loss 0.547590
[epoch5, step604]: loss 0.850729
[epoch5, step605]: loss 0.834214
[epoch5, step606]: loss 0.942881
[epoch5, step607]: loss 0.653788
[epoch5, step608]: loss 0.991029
[epoch5, step609]: loss 0.698407
[epoch5, step610]: loss 0.918176
[epoch5, step611]: loss 0.819837
[epoch5, step612]: loss 0.634194
[epoch5, step613]: loss 0.651966
[epoch5, step614]: loss 0.849607
[epoch5, step615]: loss 0.686042
[epoch5, step616]: loss 0.658191
[epoch5, step617]: loss 0.845640
[epoch5, step618]: loss 0.618771
[epoch5, step619]: loss 0.798392
[epoch5, step620]: loss 0.642323
[epoch5, step621]: loss 0.573548
[epoch5, step622]: loss 0.681225
[epoch5, step623]: loss 0.820653
[epoch5, step624]: loss 0.673837
[epoch5, step625]: loss 0.498150
[epoch5, step626]: loss 0.565031
[epoch5, step627]: loss 0.933162
[epoch5, step628]: loss 0.517887
[epoch5, step629]: loss 0.687768
[epoch5, step630]: loss 0.782210
[epoch5, step631]: loss 0.730917
[epoch5, step632]: loss 0.791065
[epoch5, step633]: loss 0.755699
[epoch5, step634]: loss 0.668365
[epoch5, step635]: loss 0.736708
[epoch5, step636]: loss 0.631927
[epoch5, step637]: loss 0.682243
[epoch5, step638]: loss 0.654058
[epoch5, step639]: loss 0.739527
[epoch5, step640]: loss 0.651004
[epoch5, step641]: loss 0.810020
[epoch5, step642]: loss 0.686946
[epoch5, step643]: loss 0.746916
[epoch5, step644]: loss 0.576330
[epoch5, step645]: loss 0.583666
[epoch5, step646]: loss 0.406945
[epoch5, step647]: loss 0.340157
[epoch5, step648]: loss 0.614949
[epoch5, step649]: loss 0.967776
[epoch5, step650]: loss 0.980286
[epoch5, step651]: loss 0.697450
[epoch5, step652]: loss 0.889729
[epoch5, step653]: loss 0.660540
[epoch5, step654]: loss 0.909855
[epoch5, step655]: loss 1.053624
[epoch5, step656]: loss 0.592870
[epoch5, step657]: loss 0.519216
[epoch5, step658]: loss 0.602846
[epoch5, step659]: loss 0.542780
[epoch5, step660]: loss 0.629894
[epoch5, step661]: loss 0.661517
[epoch5, step662]: loss 0.491918
[epoch5, step663]: loss 0.687833
[epoch5, step664]: loss 0.504811
[epoch5, step665]: loss 0.762472
[epoch5, step666]: loss 0.223900
[epoch5, step667]: loss 1.057142
[epoch5, step668]: loss 0.789966
[epoch5, step669]: loss 0.932481
[epoch5, step670]: loss 0.703840
[epoch5, step671]: loss 0.678191
[epoch5, step672]: loss 0.808433
[epoch5, step673]: loss 0.403012
[epoch5, step674]: loss 0.839392
[epoch5, step675]: loss 0.785956
[epoch5, step676]: loss 0.763827
[epoch5, step677]: loss 0.836309
[epoch5, step678]: loss 0.676928
[epoch5, step679]: loss 0.771838
[epoch5, step680]: loss 0.840206
[epoch5, step681]: loss 0.415213
[epoch5, step682]: loss 0.657004
[epoch5, step683]: loss 0.777780
[epoch5, step684]: loss 0.568087
[epoch5, step685]: loss 0.781750
[epoch5, step686]: loss 0.713274
[epoch5, step687]: loss 0.708206
[epoch5, step688]: loss 0.881487
[epoch5, step689]: loss 0.325857
[epoch5, step690]: loss 0.436873
[epoch5, step691]: loss 1.073540
[epoch5, step692]: loss 0.949962
[epoch5, step693]: loss 0.715467
[epoch5, step694]: loss 0.810376
[epoch5, step695]: loss 0.541075
[epoch5, step696]: loss 0.713601
[epoch5, step697]: loss 0.777428
[epoch5, step698]: loss 0.394910
[epoch5, step699]: loss 0.803568
[epoch5, step700]: loss 0.999026
[epoch5, step701]: loss 0.686409
[epoch5, step702]: loss 0.894297
[epoch5, step703]: loss 0.818313
[epoch5, step704]: loss 0.801463
[epoch5, step705]: loss 1.010111
[epoch5, step706]: loss 0.757691
[epoch5, step707]: loss 1.032021
[epoch5, step708]: loss 0.748208
[epoch5, step709]: loss 0.533542
[epoch5, step710]: loss 0.891155
[epoch5, step711]: loss 0.866364
[epoch5, step712]: loss 0.549748
[epoch5, step713]: loss 0.736418
[epoch5, step714]: loss 0.645258
[epoch5, step715]: loss 0.500650
[epoch5, step716]: loss 0.641385
[epoch5, step717]: loss 0.525433
[epoch5, step718]: loss 0.565790
[epoch5, step719]: loss 0.896566
[epoch5, step720]: loss 0.782525
[epoch5, step721]: loss 0.541807
[epoch5, step722]: loss 0.607782
[epoch5, step723]: loss 0.386516
[epoch5, step724]: loss 0.687852
[epoch5, step725]: loss 0.545332
[epoch5, step726]: loss 0.575604
[epoch5, step727]: loss 1.078473
[epoch5, step728]: loss 1.006095
[epoch5, step729]: loss 0.421762
[epoch5, step730]: loss 0.946065
[epoch5, step731]: loss 0.927348
[epoch5, step732]: loss 0.687884
[epoch5, step733]: loss 0.841695
[epoch5, step734]: loss 0.222994
[epoch5, step735]: loss 0.603623
[epoch5, step736]: loss 0.726879
[epoch5, step737]: loss 0.972653
[epoch5, step738]: loss 0.724169
[epoch5, step739]: loss 0.885970
[epoch5, step740]: loss 0.887109
[epoch5, step741]: loss 0.934171
[epoch5, step742]: loss 0.867691
[epoch5, step743]: loss 0.674639
[epoch5, step744]: loss 0.665720
[epoch5, step745]: loss 0.712692
[epoch5, step746]: loss 0.725342
[epoch5, step747]: loss 0.791329
[epoch5, step748]: loss 0.684146
[epoch5, step749]: loss 0.920031
[epoch5, step750]: loss 0.567270
[epoch5, step751]: loss 0.815557
[epoch5, step752]: loss 0.772001
[epoch5, step753]: loss 0.757292
[epoch5, step754]: loss 0.849998
[epoch5, step755]: loss 0.839645
[epoch5, step756]: loss 0.763696
[epoch5, step757]: loss 0.951084
[epoch5, step758]: loss 0.656333
[epoch5, step759]: loss 0.640752
[epoch5, step760]: loss 0.969041
[epoch5, step761]: loss 0.450240
[epoch5, step762]: loss 0.854668
[epoch5, step763]: loss 0.843610
[epoch5, step764]: loss 0.643086
[epoch5, step765]: loss 0.739089
[epoch5, step766]: loss 0.538434
[epoch5, step767]: loss 0.977381
[epoch5, step768]: loss 0.936036
[epoch5, step769]: loss 0.818887
[epoch5, step770]: loss 1.072956
[epoch5, step771]: loss 0.856180
[epoch5, step772]: loss 0.642132
[epoch5, step773]: loss 0.912545
[epoch5, step774]: loss 0.680888
[epoch5, step775]: loss 0.645309
[epoch5, step776]: loss 1.104916
[epoch5, step777]: loss 0.849058
[epoch5, step778]: loss 0.887178
[epoch5, step779]: loss 0.717041
[epoch5, step780]: loss 0.951419
[epoch5, step781]: loss 0.881123
[epoch5, step782]: loss 0.944845
[epoch5, step783]: loss 0.754046
[epoch5, step784]: loss 0.562712
[epoch5, step785]: loss 0.660951
[epoch5, step786]: loss 0.528319
[epoch5, step787]: loss 0.732834
[epoch5, step788]: loss 0.731076
[epoch5, step789]: loss 0.813515
[epoch5, step790]: loss 0.564676
[epoch5, step791]: loss 0.880531
[epoch5, step792]: loss 0.955265
[epoch5, step793]: loss 0.777358
[epoch5, step794]: loss 0.714699
[epoch5, step795]: loss 0.834331
[epoch5, step796]: loss 0.688565
[epoch5, step797]: loss 0.812130
[epoch5, step798]: loss 0.740742
[epoch5, step799]: loss 0.775974
[epoch5, step800]: loss 0.724679
[epoch5, step801]: loss 0.787160
[epoch5, step802]: loss 0.880102
[epoch5, step803]: loss 0.714930
[epoch5, step804]: loss 0.416506
[epoch5, step805]: loss 0.886299
[epoch5, step806]: loss 0.693736
[epoch5, step807]: loss 0.552719
[epoch5, step808]: loss 0.941906
[epoch5, step809]: loss 0.567101
[epoch5, step810]: loss 0.693169
[epoch5, step811]: loss 0.836953
[epoch5, step812]: loss 0.674874
[epoch5, step813]: loss 0.497044
[epoch5, step814]: loss 0.539717
[epoch5, step815]: loss 0.775013
[epoch5, step816]: loss 0.910740
[epoch5, step817]: loss 0.559667
[epoch5, step818]: loss 0.561950
[epoch5, step819]: loss 0.862895
[epoch5, step820]: loss 0.776101
[epoch5, step821]: loss 0.712604
[epoch5, step822]: loss 0.685692
[epoch5, step823]: loss 0.699825
[epoch5, step824]: loss 0.719819
[epoch5, step825]: loss 0.902394
[epoch5, step826]: loss 0.638089
[epoch5, step827]: loss 0.602398
[epoch5, step828]: loss 0.714005
[epoch5, step829]: loss 0.614690
[epoch5, step830]: loss 0.866638
[epoch5, step831]: loss 0.631639
[epoch5, step832]: loss 0.751385
[epoch5, step833]: loss 0.914440
[epoch5, step834]: loss 0.497579
[epoch5, step835]: loss 0.625990
[epoch5, step836]: loss 0.875066
[epoch5, step837]: loss 0.720143
[epoch5, step838]: loss 0.860958
[epoch5, step839]: loss 0.718433
[epoch5, step840]: loss 0.844285
[epoch5, step841]: loss 0.667062
[epoch5, step842]: loss 0.880530
[epoch5, step843]: loss 0.799065
[epoch5, step844]: loss 0.575492
[epoch5, step845]: loss 0.614642
[epoch5, step846]: loss 1.042214
[epoch5, step847]: loss 0.659673
[epoch5, step848]: loss 0.599341
[epoch5, step849]: loss 0.899280
[epoch5, step850]: loss 0.777132
[epoch5, step851]: loss 0.663989
[epoch5, step852]: loss 0.717808
[epoch5, step853]: loss 0.735602
[epoch5, step854]: loss 0.966092
[epoch5, step855]: loss 0.653268
[epoch5, step856]: loss 0.916851
[epoch5, step857]: loss 0.443692
[epoch5, step858]: loss 0.960875
[epoch5, step859]: loss 0.887920
[epoch5, step860]: loss 0.693196
[epoch5, step861]: loss 0.905694
[epoch5, step862]: loss 0.664834
[epoch5, step863]: loss 1.022073
[epoch5, step864]: loss 0.633784
[epoch5, step865]: loss 0.707172
[epoch5, step866]: loss 0.882178
[epoch5, step867]: loss 0.898708
[epoch5, step868]: loss 0.903183
[epoch5, step869]: loss 0.839689
[epoch5, step870]: loss 0.649752
[epoch5, step871]: loss 0.887052
[epoch5, step872]: loss 0.566559
[epoch5, step873]: loss 0.563053
[epoch5, step874]: loss 0.734401
[epoch5, step875]: loss 0.456991
[epoch5, step876]: loss 0.568827
[epoch5, step877]: loss 0.765418
[epoch5, step878]: loss 0.590863
[epoch5, step879]: loss 1.020793
[epoch5, step880]: loss 0.446398
[epoch5, step881]: loss 0.834064
[epoch5, step882]: loss 0.609014
[epoch5, step883]: loss 0.813804
[epoch5, step884]: loss 0.880257
[epoch5, step885]: loss 0.697184
[epoch5, step886]: loss 0.849679
[epoch5, step887]: loss 0.641991
[epoch5, step888]: loss 0.760971
[epoch5, step889]: loss 0.788990
[epoch5, step890]: loss 0.645258
[epoch5, step891]: loss 0.734704
[epoch5, step892]: loss 0.734738
[epoch5, step893]: loss 0.740871
[epoch5, step894]: loss 0.728771
[epoch5, step895]: loss 0.762870
[epoch5, step896]: loss 0.928624
[epoch5, step897]: loss 0.590930
[epoch5, step898]: loss 0.546462
[epoch5, step899]: loss 0.720925
[epoch5, step900]: loss 0.709916
[epoch5, step901]: loss 0.691619
[epoch5, step902]: loss 0.578080
[epoch5, step903]: loss 0.833598
[epoch5, step904]: loss 0.625437
[epoch5, step905]: loss 0.859923
[epoch5, step906]: loss 0.722960
[epoch5, step907]: loss 0.943244
[epoch5, step908]: loss 0.978559
[epoch5, step909]: loss 0.793362
[epoch5, step910]: loss 0.566653
[epoch5, step911]: loss 0.775596
[epoch5, step912]: loss 0.855170
[epoch5, step913]: loss 0.543401
[epoch5, step914]: loss 0.668692
[epoch5, step915]: loss 0.877697
[epoch5, step916]: loss 0.823805
[epoch5, step917]: loss 0.695327
[epoch5, step918]: loss 0.663585
[epoch5, step919]: loss 0.638192
[epoch5, step920]: loss 0.675761
[epoch5, step921]: loss 0.615787
[epoch5, step922]: loss 0.838579
[epoch5, step923]: loss 0.842423
[epoch5, step924]: loss 0.799878
[epoch5, step925]: loss 0.731422
[epoch5, step926]: loss 0.800164
[epoch5, step927]: loss 0.480903
[epoch5, step928]: loss 0.834213
[epoch5, step929]: loss 0.745136
[epoch5, step930]: loss 0.568194
[epoch5, step931]: loss 0.750947
[epoch5, step932]: loss 0.772843
[epoch5, step933]: loss 0.506657
[epoch5, step934]: loss 0.776461
[epoch5, step935]: loss 0.967923
[epoch5, step936]: loss 0.656794
[epoch5, step937]: loss 0.402161
[epoch5, step938]: loss 0.826730
[epoch5, step939]: loss 0.731098
[epoch5, step940]: loss 0.894179
[epoch5, step941]: loss 0.851133
[epoch5, step942]: loss 0.568855
[epoch5, step943]: loss 0.661265
[epoch5, step944]: loss 0.725117
[epoch5, step945]: loss 0.757315
[epoch5, step946]: loss 0.729052
[epoch5, step947]: loss 0.487722
[epoch5, step948]: loss 0.697912
[epoch5, step949]: loss 0.560106
[epoch5, step950]: loss 0.603101
[epoch5, step951]: loss 0.780719
[epoch5, step952]: loss 0.945733
[epoch5, step953]: loss 0.907708
[epoch5, step954]: loss 0.744087
[epoch5, step955]: loss 0.816009
[epoch5, step956]: loss 0.455131
[epoch5, step957]: loss 0.841994
[epoch5, step958]: loss 0.755950
[epoch5, step959]: loss 0.686248
[epoch5, step960]: loss 0.564019
[epoch5, step961]: loss 0.693750
[epoch5, step962]: loss 0.555867
[epoch5, step963]: loss 0.765151
[epoch5, step964]: loss 1.023772
[epoch5, step965]: loss 0.636964
[epoch5, step966]: loss 0.888169
[epoch5, step967]: loss 0.763333
[epoch5, step968]: loss 0.501722
[epoch5, step969]: loss 0.439932
[epoch5, step970]: loss 0.707066
[epoch5, step971]: loss 0.824598
[epoch5, step972]: loss 0.787620
[epoch5, step973]: loss 0.730224
[epoch5, step974]: loss 0.626580
[epoch5, step975]: loss 0.784422
[epoch5, step976]: loss 0.931264
[epoch5, step977]: loss 0.703366
[epoch5, step978]: loss 0.764220
[epoch5, step979]: loss 0.913444
[epoch5, step980]: loss 0.600007
[epoch5, step981]: loss 0.777610
[epoch5, step982]: loss 0.680271
[epoch5, step983]: loss 0.689970
[epoch5, step984]: loss 0.778547
[epoch5, step985]: loss 0.524927
[epoch5, step986]: loss 0.759509
[epoch5, step987]: loss 0.571069
[epoch5, step988]: loss 0.862183
[epoch5, step989]: loss 0.706889
[epoch5, step990]: loss 0.804552
[epoch5, step991]: loss 1.006516
[epoch5, step992]: loss 0.777340
[epoch5, step993]: loss 0.759321
[epoch5, step994]: loss 0.690200
[epoch5, step995]: loss 0.578589
[epoch5, step996]: loss 0.748305
[epoch5, step997]: loss 0.710669
[epoch5, step998]: loss 0.632909
[epoch5, step999]: loss 0.769177
[epoch5, step1000]: loss 0.790139
[epoch5, step1001]: loss 0.496711
[epoch5, step1002]: loss 0.314334
[epoch5, step1003]: loss 0.768979
[epoch5, step1004]: loss 0.692739
[epoch5, step1005]: loss 0.872414
[epoch5, step1006]: loss 0.631086
[epoch5, step1007]: loss 0.722173
[epoch5, step1008]: loss 0.742582
[epoch5, step1009]: loss 0.834718
[epoch5, step1010]: loss 0.657026
[epoch5, step1011]: loss 0.853621
[epoch5, step1012]: loss 0.589067
[epoch5, step1013]: loss 0.819886
[epoch5, step1014]: loss 0.512392
[epoch5, step1015]: loss 0.710562
[epoch5, step1016]: loss 0.760569
[epoch5, step1017]: loss 0.773665
[epoch5, step1018]: loss 0.757721
[epoch5, step1019]: loss 0.606405
[epoch5, step1020]: loss 0.869435
[epoch5, step1021]: loss 1.022122
[epoch5, step1022]: loss 0.792018
[epoch5, step1023]: loss 0.731839
[epoch5, step1024]: loss 0.614341
[epoch5, step1025]: loss 0.599206
[epoch5, step1026]: loss 0.692757
[epoch5, step1027]: loss 0.501727
[epoch5, step1028]: loss 0.657492
[epoch5, step1029]: loss 0.703267
[epoch5, step1030]: loss 0.603264
[epoch5, step1031]: loss 0.927239
[epoch5, step1032]: loss 0.643776
[epoch5, step1033]: loss 0.763133
[epoch5, step1034]: loss 0.877364
[epoch5, step1035]: loss 0.886627
[epoch5, step1036]: loss 0.540516
[epoch5, step1037]: loss 0.837879
[epoch5, step1038]: loss 0.847464
[epoch5, step1039]: loss 0.693284
[epoch5, step1040]: loss 0.530459
[epoch5, step1041]: loss 0.537704
[epoch5, step1042]: loss 0.708444
[epoch5, step1043]: loss 0.588985
[epoch5, step1044]: loss 0.573409
[epoch5, step1045]: loss 0.752477
[epoch5, step1046]: loss 0.832162
[epoch5, step1047]: loss 0.822990
[epoch5, step1048]: loss 0.390952
[epoch5, step1049]: loss 0.695379
[epoch5, step1050]: loss 0.798744
[epoch5, step1051]: loss 0.693760
[epoch5, step1052]: loss 0.922515
[epoch5, step1053]: loss 0.723840
[epoch5, step1054]: loss 0.910581
[epoch5, step1055]: loss 0.381908
[epoch5, step1056]: loss 0.811612
[epoch5, step1057]: loss 0.624143
[epoch5, step1058]: loss 0.451036
[epoch5, step1059]: loss 0.824072
[epoch5, step1060]: loss 0.897782
[epoch5, step1061]: loss 0.714346
[epoch5, step1062]: loss 0.767333
[epoch5, step1063]: loss 0.430113
[epoch5, step1064]: loss 0.839464
[epoch5, step1065]: loss 0.844962
[epoch5, step1066]: loss 0.701167
[epoch5, step1067]: loss 0.664607
[epoch5, step1068]: loss 0.539086
[epoch5, step1069]: loss 0.861876
[epoch5, step1070]: loss 0.669692
[epoch5, step1071]: loss 0.723260
[epoch5, step1072]: loss 0.837935
[epoch5, step1073]: loss 0.747491
[epoch5, step1074]: loss 0.869913
[epoch5, step1075]: loss 0.810631
[epoch5, step1076]: loss 0.547360
[epoch5, step1077]: loss 0.555699
[epoch5, step1078]: loss 0.745442
[epoch5, step1079]: loss 0.797718
[epoch5, step1080]: loss 0.798637
[epoch5, step1081]: loss 0.471132
[epoch5, step1082]: loss 0.735452
[epoch5, step1083]: loss 0.781591
[epoch5, step1084]: loss 0.541863
[epoch5, step1085]: loss 0.566929
[epoch5, step1086]: loss 0.876923
[epoch5, step1087]: loss 0.809803
[epoch5, step1088]: loss 0.728095
[epoch5, step1089]: loss 0.562440
[epoch5, step1090]: loss 0.696159
[epoch5, step1091]: loss 0.701429
[epoch5, step1092]: loss 0.640625
[epoch5, step1093]: loss 0.577590
[epoch5, step1094]: loss 0.886792
[epoch5, step1095]: loss 0.846384
[epoch5, step1096]: loss 0.677152
[epoch5, step1097]: loss 0.814738
[epoch5, step1098]: loss 0.780949
[epoch5, step1099]: loss 0.606901
[epoch5, step1100]: loss 0.763812
[epoch5, step1101]: loss 0.627486
[epoch5, step1102]: loss 0.722606
[epoch5, step1103]: loss 0.810214
[epoch5, step1104]: loss 0.798998
[epoch5, step1105]: loss 0.578495
[epoch5, step1106]: loss 0.655985
[epoch5, step1107]: loss 0.894073
[epoch5, step1108]: loss 0.818200
[epoch5, step1109]: loss 0.672018
[epoch5, step1110]: loss 0.591940
[epoch5, step1111]: loss 0.725147
[epoch5, step1112]: loss 0.837861
[epoch5, step1113]: loss 0.642874
[epoch5, step1114]: loss 0.896369
[epoch5, step1115]: loss 0.708180
[epoch5, step1116]: loss 0.647026
[epoch5, step1117]: loss 0.595122
[epoch5, step1118]: loss 0.867333
[epoch5, step1119]: loss 0.993958
[epoch5, step1120]: loss 0.970212
[epoch5, step1121]: loss 0.725316
[epoch5, step1122]: loss 0.485275
[epoch5, step1123]: loss 0.428145
[epoch5, step1124]: loss 0.873540
[epoch5, step1125]: loss 0.683775
[epoch5, step1126]: loss 0.769884
[epoch5, step1127]: loss 0.997282
[epoch5, step1128]: loss 0.944490
[epoch5, step1129]: loss 0.609649
[epoch5, step1130]: loss 0.531974
[epoch5, step1131]: loss 0.930838
[epoch5, step1132]: loss 0.585789
[epoch5, step1133]: loss 0.658751
[epoch5, step1134]: loss 0.949555
[epoch5, step1135]: loss 0.563525
[epoch5, step1136]: loss 0.873241
[epoch5, step1137]: loss 0.537581
[epoch5, step1138]: loss 0.920294
[epoch5, step1139]: loss 0.859329
[epoch5, step1140]: loss 0.630777
[epoch5, step1141]: loss 0.592497
[epoch5, step1142]: loss 0.783148
[epoch5, step1143]: loss 0.582276
[epoch5, step1144]: loss 0.525408
[epoch5, step1145]: loss 0.622256
[epoch5, step1146]: loss 0.587709
[epoch5, step1147]: loss 0.811157
[epoch5, step1148]: loss 0.818409
[epoch5, step1149]: loss 0.390349
[epoch5, step1150]: loss 0.874105
[epoch5, step1151]: loss 0.938452
[epoch5, step1152]: loss 0.774078
[epoch5, step1153]: loss 0.312239
[epoch5, step1154]: loss 0.833062
[epoch5, step1155]: loss 0.754194
[epoch5, step1156]: loss 0.530845
[epoch5, step1157]: loss 0.855375
[epoch5, step1158]: loss 0.513091
[epoch5, step1159]: loss 0.568777
[epoch5, step1160]: loss 0.655464
[epoch5, step1161]: loss 0.829221
[epoch5, step1162]: loss 0.195239
[epoch5, step1163]: loss 0.744579
[epoch5, step1164]: loss 0.628000
[epoch5, step1165]: loss 0.599667
[epoch5, step1166]: loss 0.523907
[epoch5, step1167]: loss 0.840793
[epoch5, step1168]: loss 0.670670
[epoch5, step1169]: loss 0.537782
[epoch5, step1170]: loss 0.676393
[epoch5, step1171]: loss 0.969991
[epoch5, step1172]: loss 0.663193
[epoch5, step1173]: loss 0.720154
[epoch5, step1174]: loss 0.715095
[epoch5, step1175]: loss 0.816448
[epoch5, step1176]: loss 0.689509
[epoch5, step1177]: loss 0.585357
[epoch5, step1178]: loss 0.737520
[epoch5, step1179]: loss 0.784965
[epoch5, step1180]: loss 0.637115
[epoch5, step1181]: loss 0.872594
[epoch5, step1182]: loss 0.730034
[epoch5, step1183]: loss 0.924913
[epoch5, step1184]: loss 0.495622
[epoch5, step1185]: loss 0.666066
[epoch5, step1186]: loss 0.284806
[epoch5, step1187]: loss 0.696433
[epoch5, step1188]: loss 0.568274
[epoch5, step1189]: loss 0.544586
[epoch5, step1190]: loss 0.673404
[epoch5, step1191]: loss 0.816858
[epoch5, step1192]: loss 0.811324
[epoch5, step1193]: loss 0.670912
[epoch5, step1194]: loss 0.902715
[epoch5, step1195]: loss 0.736302
[epoch5, step1196]: loss 0.631412
[epoch5, step1197]: loss 0.843524
[epoch5, step1198]: loss 0.723862
[epoch5, step1199]: loss 0.899715
[epoch5, step1200]: loss 0.705733
[epoch5, step1201]: loss 0.515764
[epoch5, step1202]: loss 0.761998
[epoch5, step1203]: loss 0.729171
[epoch5, step1204]: loss 0.507913
[epoch5, step1205]: loss 0.514288
[epoch5, step1206]: loss 0.560758
[epoch5, step1207]: loss 0.659634
[epoch5, step1208]: loss 0.535319
[epoch5, step1209]: loss 0.682602
[epoch5, step1210]: loss 0.799688
[epoch5, step1211]: loss 0.966179
[epoch5, step1212]: loss 0.637242
[epoch5, step1213]: loss 0.510371
[epoch5, step1214]: loss 0.534998
[epoch5, step1215]: loss 0.727171
[epoch5, step1216]: loss 0.703046
[epoch5, step1217]: loss 0.559509
[epoch5, step1218]: loss 0.563931
[epoch5, step1219]: loss 0.828518
[epoch5, step1220]: loss 0.793394
[epoch5, step1221]: loss 0.604701
[epoch5, step1222]: loss 0.650481
[epoch5, step1223]: loss 0.670836
[epoch5, step1224]: loss 0.917773
[epoch5, step1225]: loss 0.823815
[epoch5, step1226]: loss 0.712054
[epoch5, step1227]: loss 0.595769
[epoch5, step1228]: loss 0.486638
[epoch5, step1229]: loss 0.831332
[epoch5, step1230]: loss 0.680708
[epoch5, step1231]: loss 0.535895
[epoch5, step1232]: loss 0.549595
[epoch5, step1233]: loss 0.504183
[epoch5, step1234]: loss 0.710780
[epoch5, step1235]: loss 0.704832
[epoch5, step1236]: loss 0.287978
[epoch5, step1237]: loss 0.674772
[epoch5, step1238]: loss 0.619281
[epoch5, step1239]: loss 0.710999
[epoch5, step1240]: loss 0.777044
[epoch5, step1241]: loss 0.840474
[epoch5, step1242]: loss 0.838605
[epoch5, step1243]: loss 0.891567
[epoch5, step1244]: loss 0.457517
[epoch5, step1245]: loss 0.875602
[epoch5, step1246]: loss 0.826055
[epoch5, step1247]: loss 0.859675
[epoch5, step1248]: loss 0.790337
[epoch5, step1249]: loss 0.523552
[epoch5, step1250]: loss 0.908173
[epoch5, step1251]: loss 0.339855
[epoch5, step1252]: loss 0.667555
[epoch5, step1253]: loss 0.850205
[epoch5, step1254]: loss 1.019323
[epoch5, step1255]: loss 0.633095
[epoch5, step1256]: loss 0.625331
[epoch5, step1257]: loss 0.576911
[epoch5, step1258]: loss 0.558755
[epoch5, step1259]: loss 0.997936
[epoch5, step1260]: loss 0.650024
[epoch5, step1261]: loss 0.732600
[epoch5, step1262]: loss 0.706225
[epoch5, step1263]: loss 0.580087
[epoch5, step1264]: loss 0.582887
[epoch5, step1265]: loss 0.587575
[epoch5, step1266]: loss 0.495031
[epoch5, step1267]: loss 0.884129
[epoch5, step1268]: loss 0.605576
[epoch5, step1269]: loss 0.612805
[epoch5, step1270]: loss 0.738480
[epoch5, step1271]: loss 0.845598
[epoch5, step1272]: loss 0.823736
[epoch5, step1273]: loss 0.688809
[epoch5, step1274]: loss 0.723849
[epoch5, step1275]: loss 0.730129
[epoch5, step1276]: loss 0.641638
[epoch5, step1277]: loss 0.626587
[epoch5, step1278]: loss 0.770609
[epoch5, step1279]: loss 0.599248
[epoch5, step1280]: loss 0.620335
[epoch5, step1281]: loss 0.531905
[epoch5, step1282]: loss 0.437820
[epoch5, step1283]: loss 0.835520
[epoch5, step1284]: loss 0.776856
[epoch5, step1285]: loss 0.824996
[epoch5, step1286]: loss 0.607932
[epoch5, step1287]: loss 0.689707
[epoch5, step1288]: loss 0.923931
[epoch5, step1289]: loss 0.484722
[epoch5, step1290]: loss 0.657545
[epoch5, step1291]: loss 0.858059
[epoch5, step1292]: loss 0.556035
[epoch5, step1293]: loss 0.777946
[epoch5, step1294]: loss 0.455508
[epoch5, step1295]: loss 0.919620
[epoch5, step1296]: loss 0.777326
[epoch5, step1297]: loss 0.924938
[epoch5, step1298]: loss 0.701545
[epoch5, step1299]: loss 0.714350
[epoch5, step1300]: loss 0.514261
[epoch5, step1301]: loss 0.669097
[epoch5, step1302]: loss 0.669044
[epoch5, step1303]: loss 0.784571
[epoch5, step1304]: loss 0.966031
[epoch5, step1305]: loss 0.894033
[epoch5, step1306]: loss 0.839681
[epoch5, step1307]: loss 1.063696
[epoch5, step1308]: loss 0.261818
[epoch5, step1309]: loss 0.639575
[epoch5, step1310]: loss 0.698036
[epoch5, step1311]: loss 0.800030
[epoch5, step1312]: loss 0.955619
[epoch5, step1313]: loss 0.812998
[epoch5, step1314]: loss 0.786843
[epoch5, step1315]: loss 0.802677
[epoch5, step1316]: loss 0.756633
[epoch5, step1317]: loss 0.811613
[epoch5, step1318]: loss 0.733456
[epoch5, step1319]: loss 0.891866
[epoch5, step1320]: loss 0.595057
[epoch5, step1321]: loss 0.604287
[epoch5, step1322]: loss 0.818003
[epoch5, step1323]: loss 0.563528
[epoch5, step1324]: loss 0.886203
[epoch5, step1325]: loss 0.688980
[epoch5, step1326]: loss 0.956887
[epoch5, step1327]: loss 0.518581
[epoch5, step1328]: loss 0.699358
[epoch5, step1329]: loss 0.767629
[epoch5, step1330]: loss 0.890719
[epoch5, step1331]: loss 0.716120
[epoch5, step1332]: loss 0.718092
[epoch5, step1333]: loss 0.655005
[epoch5, step1334]: loss 0.740312
[epoch5, step1335]: loss 0.662343
[epoch5, step1336]: loss 0.436268
[epoch5, step1337]: loss 0.837253
[epoch5, step1338]: loss 0.389531
[epoch5, step1339]: loss 0.648157
[epoch5, step1340]: loss 0.740586
[epoch5, step1341]: loss 0.851581
[epoch5, step1342]: loss 0.661948
[epoch5, step1343]: loss 0.751864
[epoch5, step1344]: loss 0.677787
[epoch5, step1345]: loss 0.705963
[epoch5, step1346]: loss 0.845426
[epoch5, step1347]: loss 0.746315
[epoch5, step1348]: loss 0.742974
[epoch5, step1349]: loss 0.543578
[epoch5, step1350]: loss 0.852410
[epoch5, step1351]: loss 0.894837
[epoch5, step1352]: loss 0.920515
[epoch5, step1353]: loss 0.845343
[epoch5, step1354]: loss 0.813862
[epoch5, step1355]: loss 0.596185
[epoch5, step1356]: loss 0.893570
[epoch5, step1357]: loss 0.647474
[epoch5, step1358]: loss 0.912511
[epoch5, step1359]: loss 0.490440
[epoch5, step1360]: loss 0.862924
[epoch5, step1361]: loss 0.541906
[epoch5, step1362]: loss 0.535719
[epoch5, step1363]: loss 0.630629
[epoch5, step1364]: loss 0.473728
[epoch5, step1365]: loss 0.661634
[epoch5, step1366]: loss 0.743663
[epoch5, step1367]: loss 0.569652
[epoch5, step1368]: loss 0.773452
[epoch5, step1369]: loss 0.514989
[epoch5, step1370]: loss 0.673054
[epoch5, step1371]: loss 0.842900
[epoch5, step1372]: loss 0.721619
[epoch5, step1373]: loss 0.523360
[epoch5, step1374]: loss 0.344939
[epoch5, step1375]: loss 0.746165
[epoch5, step1376]: loss 0.588886
[epoch5, step1377]: loss 0.730775
[epoch5, step1378]: loss 0.750199
[epoch5, step1379]: loss 0.986268
[epoch5, step1380]: loss 0.827666
[epoch5, step1381]: loss 0.758295
[epoch5, step1382]: loss 0.715380
[epoch5, step1383]: loss 0.772224
[epoch5, step1384]: loss 0.644218
[epoch5, step1385]: loss 0.628382
[epoch5, step1386]: loss 0.587168
[epoch5, step1387]: loss 0.739306
[epoch5, step1388]: loss 0.740805
[epoch5, step1389]: loss 0.587606
[epoch5, step1390]: loss 0.748703
[epoch5, step1391]: loss 0.852508
[epoch5, step1392]: loss 0.608289
[epoch5, step1393]: loss 0.811428
[epoch5, step1394]: loss 0.618282
[epoch5, step1395]: loss 0.574718
[epoch5, step1396]: loss 0.710942
[epoch5, step1397]: loss 0.851058
[epoch5, step1398]: loss 0.704237
[epoch5, step1399]: loss 0.678266
[epoch5, step1400]: loss 0.551391
[epoch5, step1401]: loss 0.707502
[epoch5, step1402]: loss 0.653811
[epoch5, step1403]: loss 0.823114
[epoch5, step1404]: loss 0.261973
[epoch5, step1405]: loss 0.572690
[epoch5, step1406]: loss 0.844864
[epoch5, step1407]: loss 0.531323
[epoch5, step1408]: loss 0.719607
[epoch5, step1409]: loss 0.827762
[epoch5, step1410]: loss 0.716591
[epoch5, step1411]: loss 0.942579
[epoch5, step1412]: loss 0.912138
[epoch5, step1413]: loss 0.856436
[epoch5, step1414]: loss 0.707493
[epoch5, step1415]: loss 0.998407
[epoch5, step1416]: loss 0.694726
[epoch5, step1417]: loss 0.873529
[epoch5, step1418]: loss 0.808501
[epoch5, step1419]: loss 0.573929
[epoch5, step1420]: loss 0.783777
[epoch5, step1421]: loss 0.774783
[epoch5, step1422]: loss 0.571038
[epoch5, step1423]: loss 0.714659
[epoch5, step1424]: loss 0.608578
[epoch5, step1425]: loss 1.081229
[epoch5, step1426]: loss 0.668361
[epoch5, step1427]: loss 0.828736
[epoch5, step1428]: loss 0.818907
[epoch5, step1429]: loss 0.750625
[epoch5, step1430]: loss 0.995680
[epoch5, step1431]: loss 0.843564
[epoch5, step1432]: loss 0.796893
[epoch5, step1433]: loss 0.799689
[epoch5, step1434]: loss 0.741312
[epoch5, step1435]: loss 0.740425
[epoch5, step1436]: loss 0.640761
[epoch5, step1437]: loss 0.716713
[epoch5, step1438]: loss 0.698468
[epoch5, step1439]: loss 0.633537
[epoch5, step1440]: loss 0.542725
[epoch5, step1441]: loss 0.937340
[epoch5, step1442]: loss 0.824031
[epoch5, step1443]: loss 0.442800
[epoch5, step1444]: loss 0.366478
[epoch5, step1445]: loss 0.587001
[epoch5, step1446]: loss 0.413244
[epoch5, step1447]: loss 0.674822
[epoch5, step1448]: loss 0.731153
[epoch5, step1449]: loss 0.521069
[epoch5, step1450]: loss 0.656947
[epoch5, step1451]: loss 0.859451
[epoch5, step1452]: loss 0.537843
[epoch5, step1453]: loss 0.445000
[epoch5, step1454]: loss 0.544879
[epoch5, step1455]: loss 0.365876
[epoch5, step1456]: loss 0.603820
[epoch5, step1457]: loss 0.551687
[epoch5, step1458]: loss 0.599048
[epoch5, step1459]: loss 0.886140
[epoch5, step1460]: loss 0.852041
[epoch5, step1461]: loss 0.458438
[epoch5, step1462]: loss 0.658242
[epoch5, step1463]: loss 0.463784
[epoch5, step1464]: loss 0.455932
[epoch5, step1465]: loss 0.844423
[epoch5, step1466]: loss 0.632495
[epoch5, step1467]: loss 0.702342
[epoch5, step1468]: loss 0.550755
[epoch5, step1469]: loss 0.794246
[epoch5, step1470]: loss 0.726900
[epoch5, step1471]: loss 0.844526
[epoch5, step1472]: loss 0.562655
[epoch5, step1473]: loss 0.604170
[epoch5, step1474]: loss 0.745637
[epoch5, step1475]: loss 0.778094
[epoch5, step1476]: loss 0.869861
[epoch5, step1477]: loss 0.683168
[epoch5, step1478]: loss 0.612516
[epoch5, step1479]: loss 0.752644
[epoch5, step1480]: loss 0.695666
[epoch5, step1481]: loss 0.490575
[epoch5, step1482]: loss 0.657590
[epoch5, step1483]: loss 0.190968
[epoch5, step1484]: loss 0.722067
[epoch5, step1485]: loss 0.686293
[epoch5, step1486]: loss 0.910710
[epoch5, step1487]: loss 0.419664
[epoch5, step1488]: loss 0.483713
[epoch5, step1489]: loss 0.803617
[epoch5, step1490]: loss 0.512223
[epoch5, step1491]: loss 0.782237
[epoch5, step1492]: loss 0.956629
[epoch5, step1493]: loss 0.859457
[epoch5, step1494]: loss 0.852077
[epoch5, step1495]: loss 0.734086
[epoch5, step1496]: loss 0.643891
[epoch5, step1497]: loss 0.620748
[epoch5, step1498]: loss 0.872409
[epoch5, step1499]: loss 0.644836
[epoch5, step1500]: loss 0.614015
[epoch5, step1501]: loss 0.894222
[epoch5, step1502]: loss 0.636603
[epoch5, step1503]: loss 0.752204
[epoch5, step1504]: loss 0.706388
[epoch5, step1505]: loss 0.565192
[epoch5, step1506]: loss 0.301502
[epoch5, step1507]: loss 0.833493
[epoch5, step1508]: loss 0.684109
[epoch5, step1509]: loss 0.922042
[epoch5, step1510]: loss 0.809178
[epoch5, step1511]: loss 0.852007
[epoch5, step1512]: loss 0.664304
[epoch5, step1513]: loss 0.743905
[epoch5, step1514]: loss 0.726680
[epoch5, step1515]: loss 0.664058
[epoch5, step1516]: loss 0.788767
[epoch5, step1517]: loss 0.602682
[epoch5, step1518]: loss 0.782889
[epoch5, step1519]: loss 0.670154
[epoch5, step1520]: loss 0.892944
[epoch5, step1521]: loss 0.565057
[epoch5, step1522]: loss 0.838093
[epoch5, step1523]: loss 0.677193
[epoch5, step1524]: loss 0.728519
[epoch5, step1525]: loss 0.726961
[epoch5, step1526]: loss 0.654046
[epoch5, step1527]: loss 0.639184
[epoch5, step1528]: loss 0.847918
[epoch5, step1529]: loss 0.695579
[epoch5, step1530]: loss 0.877864
[epoch5, step1531]: loss 0.434429
[epoch5, step1532]: loss 0.358575
[epoch5, step1533]: loss 0.795771
[epoch5, step1534]: loss 0.876530
[epoch5, step1535]: loss 0.993391
[epoch5, step1536]: loss 0.406672
[epoch5, step1537]: loss 0.903361
[epoch5, step1538]: loss 0.854448
[epoch5, step1539]: loss 0.763131
[epoch5, step1540]: loss 0.316145
[epoch5, step1541]: loss 0.705194
[epoch5, step1542]: loss 0.827987
[epoch5, step1543]: loss 0.730383
[epoch5, step1544]: loss 0.665576
[epoch5, step1545]: loss 0.812078
[epoch5, step1546]: loss 0.849483
[epoch5, step1547]: loss 0.652384
[epoch5, step1548]: loss 0.857770
[epoch5, step1549]: loss 0.854965
[epoch5, step1550]: loss 0.931543
[epoch5, step1551]: loss 0.491581
[epoch5, step1552]: loss 0.667304
[epoch5, step1553]: loss 0.635517
[epoch5, step1554]: loss 0.552714
[epoch5, step1555]: loss 0.839261
[epoch5, step1556]: loss 0.629659
[epoch5, step1557]: loss 0.785004
[epoch5, step1558]: loss 0.689752
[epoch5, step1559]: loss 0.687878
[epoch5, step1560]: loss 0.925573
[epoch5, step1561]: loss 0.474920
[epoch5, step1562]: loss 0.669960
[epoch5, step1563]: loss 0.733517
[epoch5, step1564]: loss 0.739539
[epoch5, step1565]: loss 0.716354
[epoch5, step1566]: loss 0.910179
[epoch5, step1567]: loss 0.756064
[epoch5, step1568]: loss 0.722711
[epoch5, step1569]: loss 0.846015
[epoch5, step1570]: loss 0.648649
[epoch5, step1571]: loss 0.599561
[epoch5, step1572]: loss 0.616412
[epoch5, step1573]: loss 0.473930
[epoch5, step1574]: loss 0.920877
[epoch5, step1575]: loss 0.768451
[epoch5, step1576]: loss 0.695736
[epoch5, step1577]: loss 0.408072
[epoch5, step1578]: loss 0.716592
[epoch5, step1579]: loss 0.574116
[epoch5, step1580]: loss 0.559839
[epoch5, step1581]: loss 0.488447
[epoch5, step1582]: loss 0.915981
[epoch5, step1583]: loss 0.530290
[epoch5, step1584]: loss 0.421651
[epoch5, step1585]: loss 0.616947
[epoch5, step1586]: loss 0.465550
[epoch5, step1587]: loss 0.884831
[epoch5, step1588]: loss 0.708959
[epoch5, step1589]: loss 0.529152
[epoch5, step1590]: loss 0.922467
[epoch5, step1591]: loss 0.850276
[epoch5, step1592]: loss 0.618952
[epoch5, step1593]: loss 0.591347
[epoch5, step1594]: loss 0.825956
[epoch5, step1595]: loss 0.598581
[epoch5, step1596]: loss 0.701689
[epoch5, step1597]: loss 0.574921
[epoch5, step1598]: loss 0.651022
[epoch5, step1599]: loss 0.502789
[epoch5, step1600]: loss 0.751587
[epoch5, step1601]: loss 0.507311
[epoch5, step1602]: loss 0.603776
[epoch5, step1603]: loss 0.667782
[epoch5, step1604]: loss 0.641441
[epoch5, step1605]: loss 0.463423
[epoch5, step1606]: loss 0.548569
[epoch5, step1607]: loss 0.750978
[epoch5, step1608]: loss 0.518380
[epoch5, step1609]: loss 0.788794
[epoch5, step1610]: loss 0.676967
[epoch5, step1611]: loss 0.811711
[epoch5, step1612]: loss 0.683375
[epoch5, step1613]: loss 0.488346
[epoch5, step1614]: loss 0.858630
[epoch5, step1615]: loss 0.611313
[epoch5, step1616]: loss 0.426466
[epoch5, step1617]: loss 0.526373
[epoch5, step1618]: loss 0.646229
[epoch5, step1619]: loss 0.553436
[epoch5, step1620]: loss 0.724373
[epoch5, step1621]: loss 0.860636
[epoch5, step1622]: loss 0.356356
[epoch5, step1623]: loss 0.825473
[epoch5, step1624]: loss 0.588354
[epoch5, step1625]: loss 0.646864
[epoch5, step1626]: loss 1.037442
[epoch5, step1627]: loss 0.665744
[epoch5, step1628]: loss 0.832368
[epoch5, step1629]: loss 0.657132
[epoch5, step1630]: loss 0.776004
[epoch5, step1631]: loss 0.764486
[epoch5, step1632]: loss 0.775122
[epoch5, step1633]: loss 0.768993
[epoch5, step1634]: loss 0.640497
[epoch5, step1635]: loss 0.550202
[epoch5, step1636]: loss 0.242880
[epoch5, step1637]: loss 0.757662
[epoch5, step1638]: loss 0.587233
[epoch5, step1639]: loss 0.679119
[epoch5, step1640]: loss 0.719226
[epoch5, step1641]: loss 0.767012
[epoch5, step1642]: loss 0.859769
[epoch5, step1643]: loss 0.730995
[epoch5, step1644]: loss 0.718284
[epoch5, step1645]: loss 0.846330
[epoch5, step1646]: loss 0.487609
[epoch5, step1647]: loss 0.940716
[epoch5, step1648]: loss 0.638286
[epoch5, step1649]: loss 0.760457
[epoch5, step1650]: loss 0.616705
[epoch5, step1651]: loss 0.691882
[epoch5, step1652]: loss 0.619065
[epoch5, step1653]: loss 0.576878
[epoch5, step1654]: loss 0.793258
[epoch5, step1655]: loss 0.712267
[epoch5, step1656]: loss 0.843515
[epoch5, step1657]: loss 0.979770
[epoch5, step1658]: loss 0.531393
[epoch5, step1659]: loss 0.924541
[epoch5, step1660]: loss 0.437921
[epoch5, step1661]: loss 0.532978
[epoch5, step1662]: loss 0.578932
[epoch5, step1663]: loss 0.807691
[epoch5, step1664]: loss 0.762971
[epoch5, step1665]: loss 0.849014
[epoch5, step1666]: loss 0.566423
[epoch5, step1667]: loss 0.414491
[epoch5, step1668]: loss 0.795670
[epoch5, step1669]: loss 0.642578
[epoch5, step1670]: loss 0.752976
[epoch5, step1671]: loss 0.775800
[epoch5, step1672]: loss 0.613316
[epoch5, step1673]: loss 0.852644
[epoch5, step1674]: loss 0.640509
[epoch5, step1675]: loss 0.990876
[epoch5, step1676]: loss 0.632128
[epoch5, step1677]: loss 0.805669
[epoch5, step1678]: loss 0.883515
[epoch5, step1679]: loss 0.733042
[epoch5, step1680]: loss 0.686629
[epoch5, step1681]: loss 0.733715
[epoch5, step1682]: loss 0.711051
[epoch5, step1683]: loss 0.893230
[epoch5, step1684]: loss 0.609928
[epoch5, step1685]: loss 0.692365
[epoch5, step1686]: loss 0.635524
[epoch5, step1687]: loss 0.712089
[epoch5, step1688]: loss 0.765335
[epoch5, step1689]: loss 0.738661
[epoch5, step1690]: loss 0.823720
[epoch5, step1691]: loss 0.805705
[epoch5, step1692]: loss 0.802715
[epoch5, step1693]: loss 0.956638
[epoch5, step1694]: loss 0.654326
[epoch5, step1695]: loss 0.670798
[epoch5, step1696]: loss 0.557214
[epoch5, step1697]: loss 0.962091
[epoch5, step1698]: loss 0.892540
[epoch5, step1699]: loss 0.725060
[epoch5, step1700]: loss 0.301085
[epoch5, step1701]: loss 0.889404
[epoch5, step1702]: loss 0.850265
[epoch5, step1703]: loss 0.633509
[epoch5, step1704]: loss 0.425308
[epoch5, step1705]: loss 1.072032
[epoch5, step1706]: loss 0.468979
[epoch5, step1707]: loss 0.675995
[epoch5, step1708]: loss 0.681916
[epoch5, step1709]: loss 0.820513
[epoch5, step1710]: loss 0.862188
[epoch5, step1711]: loss 0.669323
[epoch5, step1712]: loss 0.538647
[epoch5, step1713]: loss 0.745489
[epoch5, step1714]: loss 0.782636
[epoch5, step1715]: loss 0.460387
[epoch5, step1716]: loss 0.745509
[epoch5, step1717]: loss 0.408515
[epoch5, step1718]: loss 0.736532
[epoch5, step1719]: loss 0.654107
[epoch5, step1720]: loss 0.816551
[epoch5, step1721]: loss 0.736389
[epoch5, step1722]: loss 0.563077
[epoch5, step1723]: loss 0.573066
[epoch5, step1724]: loss 0.764674
[epoch5, step1725]: loss 0.763086
[epoch5, step1726]: loss 0.344499
[epoch5, step1727]: loss 0.900079
[epoch5, step1728]: loss 0.883401
[epoch5, step1729]: loss 0.493080
[epoch5, step1730]: loss 0.870581
[epoch5, step1731]: loss 0.348314
[epoch5, step1732]: loss 0.540525
[epoch5, step1733]: loss 0.614496
[epoch5, step1734]: loss 0.630094
[epoch5, step1735]: loss 0.631220
[epoch5, step1736]: loss 0.682871
[epoch5, step1737]: loss 0.674895
[epoch5, step1738]: loss 0.914224
[epoch5, step1739]: loss 0.670298
[epoch5, step1740]: loss 0.437231
[epoch5, step1741]: loss 0.401669
[epoch5, step1742]: loss 0.634721
[epoch5, step1743]: loss 0.776049
[epoch5, step1744]: loss 0.786125
[epoch5, step1745]: loss 0.556616
[epoch5, step1746]: loss 0.671667
[epoch5, step1747]: loss 0.818319
[epoch5, step1748]: loss 0.417392
[epoch5, step1749]: loss 0.702947
[epoch5, step1750]: loss 0.723519
[epoch5, step1751]: loss 0.918611
[epoch5, step1752]: loss 0.644048
[epoch5, step1753]: loss 0.578143
[epoch5, step1754]: loss 0.723113
[epoch5, step1755]: loss 0.991814
[epoch5, step1756]: loss 0.459311
[epoch5, step1757]: loss 0.643189
[epoch5, step1758]: loss 0.956484
[epoch5, step1759]: loss 0.630117
[epoch5, step1760]: loss 0.631202
[epoch5, step1761]: loss 0.849442
[epoch5, step1762]: loss 0.884763
[epoch5, step1763]: loss 0.692425
[epoch5, step1764]: loss 0.883862
[epoch5, step1765]: loss 0.826315
[epoch5, step1766]: loss 0.877186
[epoch5, step1767]: loss 0.976331
[epoch5, step1768]: loss 0.587213
[epoch5, step1769]: loss 0.643247
[epoch5, step1770]: loss 0.890906
[epoch5, step1771]: loss 0.874437
[epoch5, step1772]: loss 0.764942
[epoch5, step1773]: loss 0.568387
[epoch5, step1774]: loss 0.491614
[epoch5, step1775]: loss 0.557094
[epoch5, step1776]: loss 0.874953
[epoch5, step1777]: loss 0.697335
[epoch5, step1778]: loss 0.781002
[epoch5, step1779]: loss 0.746775
[epoch5, step1780]: loss 0.529435
[epoch5, step1781]: loss 0.567929
[epoch5, step1782]: loss 0.575202
[epoch5, step1783]: loss 0.808948
[epoch5, step1784]: loss 0.504243
[epoch5, step1785]: loss 0.871085
[epoch5, step1786]: loss 0.628564
[epoch5, step1787]: loss 0.879991
[epoch5, step1788]: loss 0.672543
[epoch5, step1789]: loss 0.894932
[epoch5, step1790]: loss 0.596626
[epoch5, step1791]: loss 0.809783
[epoch5, step1792]: loss 0.696630
[epoch5, step1793]: loss 0.673498
[epoch5, step1794]: loss 0.961418
[epoch5, step1795]: loss 0.706757
[epoch5, step1796]: loss 0.678834
[epoch5, step1797]: loss 0.603095
[epoch5, step1798]: loss 0.831887
[epoch5, step1799]: loss 0.720708
[epoch5, step1800]: loss 0.553445
[epoch5, step1801]: loss 0.861419
[epoch5, step1802]: loss 0.631074
[epoch5, step1803]: loss 0.813770
[epoch5, step1804]: loss 0.551453
[epoch5, step1805]: loss 0.559179
[epoch5, step1806]: loss 0.707068
[epoch5, step1807]: loss 0.902853
[epoch5, step1808]: loss 0.481726
[epoch5, step1809]: loss 0.881014
[epoch5, step1810]: loss 0.808100
[epoch5, step1811]: loss 0.828497
[epoch5, step1812]: loss 0.773145
[epoch5, step1813]: loss 0.729630
[epoch5, step1814]: loss 0.786077
[epoch5, step1815]: loss 0.899563
[epoch5, step1816]: loss 0.710556
[epoch5, step1817]: loss 0.869116
[epoch5, step1818]: loss 0.691507
[epoch5, step1819]: loss 0.528543
[epoch5, step1820]: loss 0.900350
[epoch5, step1821]: loss 0.679777
[epoch5, step1822]: loss 0.306372
[epoch5, step1823]: loss 0.975330
[epoch5, step1824]: loss 0.471875
[epoch5, step1825]: loss 0.534889
[epoch5, step1826]: loss 0.562870
[epoch5, step1827]: loss 0.700683
[epoch5, step1828]: loss 0.808341
[epoch5, step1829]: loss 0.611702
[epoch5, step1830]: loss 0.747965
[epoch5, step1831]: loss 0.451299
[epoch5, step1832]: loss 0.840706
[epoch5, step1833]: loss 0.796649
[epoch5, step1834]: loss 0.726494
[epoch5, step1835]: loss 0.636009
[epoch5, step1836]: loss 0.789064
[epoch5, step1837]: loss 0.610200
[epoch5, step1838]: loss 0.746365
[epoch5, step1839]: loss 0.599901
[epoch5, step1840]: loss 0.548782
[epoch5, step1841]: loss 0.541310
[epoch5, step1842]: loss 0.744008
[epoch5, step1843]: loss 0.693593
[epoch5, step1844]: loss 0.835093
[epoch5, step1845]: loss 0.629925
[epoch5, step1846]: loss 0.586841
[epoch5, step1847]: loss 0.744216
[epoch5, step1848]: loss 0.942261
[epoch5, step1849]: loss 0.518682
[epoch5, step1850]: loss 0.461231
[epoch5, step1851]: loss 0.736730
[epoch5, step1852]: loss 0.707865
[epoch5, step1853]: loss 0.807091
[epoch5, step1854]: loss 0.567440
[epoch5, step1855]: loss 0.402723
[epoch5, step1856]: loss 0.456718
[epoch5, step1857]: loss 0.900600
[epoch5, step1858]: loss 0.588804
[epoch5, step1859]: loss 0.674290
[epoch5, step1860]: loss 0.731339
[epoch5, step1861]: loss 0.642577
[epoch5, step1862]: loss 0.863512
[epoch5, step1863]: loss 0.838103
[epoch5, step1864]: loss 0.509571
[epoch5, step1865]: loss 0.769244
[epoch5, step1866]: loss 0.731893
[epoch5, step1867]: loss 0.548382
[epoch5, step1868]: loss 0.554697
[epoch5, step1869]: loss 0.545599
[epoch5, step1870]: loss 0.661172
[epoch5, step1871]: loss 1.011611
[epoch5, step1872]: loss 0.655337
[epoch5, step1873]: loss 0.770011
[epoch5, step1874]: loss 0.616664
[epoch5, step1875]: loss 0.632704
[epoch5, step1876]: loss 0.775114
[epoch5, step1877]: loss 0.653146
[epoch5, step1878]: loss 0.820496
[epoch5, step1879]: loss 0.590169
[epoch5, step1880]: loss 0.683075
[epoch5, step1881]: loss 0.672453
[epoch5, step1882]: loss 0.729100
[epoch5, step1883]: loss 0.672864
[epoch5, step1884]: loss 0.539839
[epoch5, step1885]: loss 0.509637
[epoch5, step1886]: loss 0.618696
[epoch5, step1887]: loss 0.441211
[epoch5, step1888]: loss 0.557455
[epoch5, step1889]: loss 0.616039
[epoch5, step1890]: loss 0.769186
[epoch5, step1891]: loss 0.430608
[epoch5, step1892]: loss 0.804887
[epoch5, step1893]: loss 0.607551
[epoch5, step1894]: loss 0.631304
[epoch5, step1895]: loss 0.583944
[epoch5, step1896]: loss 0.669212
[epoch5, step1897]: loss 0.961412
[epoch5, step1898]: loss 0.745504
[epoch5, step1899]: loss 0.748945
[epoch5, step1900]: loss 1.052485
[epoch5, step1901]: loss 0.515354
[epoch5, step1902]: loss 0.717875
[epoch5, step1903]: loss 0.607736
[epoch5, step1904]: loss 0.715841
[epoch5, step1905]: loss 0.604894
[epoch5, step1906]: loss 0.710911
[epoch5, step1907]: loss 0.482235
[epoch5, step1908]: loss 0.899517
[epoch5, step1909]: loss 0.645275
[epoch5, step1910]: loss 0.592714
[epoch5, step1911]: loss 0.458841
[epoch5, step1912]: loss 0.409690
[epoch5, step1913]: loss 0.616189
[epoch5, step1914]: loss 0.819293
[epoch5, step1915]: loss 0.663533
[epoch5, step1916]: loss 0.826385
[epoch5, step1917]: loss 0.556879
[epoch5, step1918]: loss 0.760809
[epoch5, step1919]: loss 0.752119
[epoch5, step1920]: loss 0.856746
[epoch5, step1921]: loss 0.871937
[epoch5, step1922]: loss 0.685680
[epoch5, step1923]: loss 0.813841
[epoch5, step1924]: loss 0.874617
[epoch5, step1925]: loss 0.601688
[epoch5, step1926]: loss 0.386205
[epoch5, step1927]: loss 0.913085
[epoch5, step1928]: loss 0.954916
[epoch5, step1929]: loss 0.706357
[epoch5, step1930]: loss 0.817100
[epoch5, step1931]: loss 0.613200
[epoch5, step1932]: loss 0.792612
[epoch5, step1933]: loss 0.373463
[epoch5, step1934]: loss 0.610195
[epoch5, step1935]: loss 0.720293
[epoch5, step1936]: loss 0.874912
[epoch5, step1937]: loss 0.590969
[epoch5, step1938]: loss 0.536467
[epoch5, step1939]: loss 0.783863
[epoch5, step1940]: loss 0.575279
[epoch5, step1941]: loss 0.664344
[epoch5, step1942]: loss 0.675742
[epoch5, step1943]: loss 0.645725
[epoch5, step1944]: loss 0.812956
[epoch5, step1945]: loss 0.715747
[epoch5, step1946]: loss 0.876438
[epoch5, step1947]: loss 0.653850
[epoch5, step1948]: loss 0.803867
[epoch5, step1949]: loss 0.607477
[epoch5, step1950]: loss 0.752380
[epoch5, step1951]: loss 0.710127
[epoch5, step1952]: loss 0.767519
[epoch5, step1953]: loss 0.826512
[epoch5, step1954]: loss 0.797303
[epoch5, step1955]: loss 0.652632
[epoch5, step1956]: loss 0.932997
[epoch5, step1957]: loss 0.510718
[epoch5, step1958]: loss 0.602399
[epoch5, step1959]: loss 0.695427
[epoch5, step1960]: loss 0.967221
[epoch5, step1961]: loss 0.928367
[epoch5, step1962]: loss 0.730331
[epoch5, step1963]: loss 0.629456
[epoch5, step1964]: loss 0.838561
[epoch5, step1965]: loss 0.658772
[epoch5, step1966]: loss 0.761638
[epoch5, step1967]: loss 0.680140
[epoch5, step1968]: loss 0.857287
[epoch5, step1969]: loss 0.680633
[epoch5, step1970]: loss 0.809107
[epoch5, step1971]: loss 0.896633
[epoch5, step1972]: loss 0.794182
[epoch5, step1973]: loss 0.500601
[epoch5, step1974]: loss 0.970853
[epoch5, step1975]: loss 0.644049
[epoch5, step1976]: loss 0.629542
[epoch5, step1977]: loss 0.513746
[epoch5, step1978]: loss 0.714383
[epoch5, step1979]: loss 0.696766
[epoch5, step1980]: loss 0.383719
[epoch5, step1981]: loss 0.610568
[epoch5, step1982]: loss 0.914937
[epoch5, step1983]: loss 0.744118
[epoch5, step1984]: loss 0.517861
[epoch5, step1985]: loss 0.860000
[epoch5, step1986]: loss 0.797658
[epoch5, step1987]: loss 0.835895
[epoch5, step1988]: loss 0.514672
[epoch5, step1989]: loss 0.844826
[epoch5, step1990]: loss 0.689850
[epoch5, step1991]: loss 0.746961
[epoch5, step1992]: loss 0.677015
[epoch5, step1993]: loss 0.928481
[epoch5, step1994]: loss 0.679940
[epoch5, step1995]: loss 0.604673
[epoch5, step1996]: loss 0.811629
[epoch5, step1997]: loss 0.789373
[epoch5, step1998]: loss 0.915411
[epoch5, step1999]: loss 0.767367
[epoch5, step2000]: loss 0.878924
[epoch5, step2001]: loss 0.513385
[epoch5, step2002]: loss 0.532740
[epoch5, step2003]: loss 0.833526
[epoch5, step2004]: loss 0.582724
[epoch5, step2005]: loss 0.687968
[epoch5, step2006]: loss 0.681815
[epoch5, step2007]: loss 0.695820
[epoch5, step2008]: loss 0.842840
[epoch5, step2009]: loss 0.750501
[epoch5, step2010]: loss 0.275630
[epoch5, step2011]: loss 1.018353
[epoch5, step2012]: loss 0.689865
[epoch5, step2013]: loss 0.731093
[epoch5, step2014]: loss 0.958338
[epoch5, step2015]: loss 0.948011
[epoch5, step2016]: loss 0.878925
[epoch5, step2017]: loss 0.716337
[epoch5, step2018]: loss 0.713736
[epoch5, step2019]: loss 0.843533
[epoch5, step2020]: loss 0.502762
[epoch5, step2021]: loss 0.730146
[epoch5, step2022]: loss 0.868056
[epoch5, step2023]: loss 0.465131
[epoch5, step2024]: loss 0.742370
[epoch5, step2025]: loss 0.671415
[epoch5, step2026]: loss 0.593412
[epoch5, step2027]: loss 0.720677
[epoch5, step2028]: loss 0.753782
[epoch5, step2029]: loss 0.675082
[epoch5, step2030]: loss 0.548342
[epoch5, step2031]: loss 0.719783
[epoch5, step2032]: loss 0.771451
[epoch5, step2033]: loss 0.849010
[epoch5, step2034]: loss 0.655040
[epoch5, step2035]: loss 0.719929
[epoch5, step2036]: loss 0.454127
[epoch5, step2037]: loss 0.849835
[epoch5, step2038]: loss 0.774192
[epoch5, step2039]: loss 0.643645
[epoch5, step2040]: loss 0.628355
[epoch5, step2041]: loss 0.637686
[epoch5, step2042]: loss 0.821568
[epoch5, step2043]: loss 0.594495
[epoch5, step2044]: loss 0.719589
[epoch5, step2045]: loss 0.670860
[epoch5, step2046]: loss 0.472671
[epoch5, step2047]: loss 0.492487
[epoch5, step2048]: loss 1.040266
[epoch5, step2049]: loss 0.725210
[epoch5, step2050]: loss 0.670583
[epoch5, step2051]: loss 0.860822
[epoch5, step2052]: loss 0.664568
[epoch5, step2053]: loss 0.701894
[epoch5, step2054]: loss 0.907254
[epoch5, step2055]: loss 0.556443
[epoch5, step2056]: loss 0.573389
[epoch5, step2057]: loss 0.982104
[epoch5, step2058]: loss 0.541727
[epoch5, step2059]: loss 0.692014
[epoch5, step2060]: loss 0.688261
[epoch5, step2061]: loss 0.789740
[epoch5, step2062]: loss 0.574617
[epoch5, step2063]: loss 0.635147
[epoch5, step2064]: loss 0.764819
[epoch5, step2065]: loss 0.574534
[epoch5, step2066]: loss 0.510465
[epoch5, step2067]: loss 0.794330
[epoch5, step2068]: loss 0.622561
[epoch5, step2069]: loss 0.725264
[epoch5, step2070]: loss 0.593281
[epoch5, step2071]: loss 0.572617
[epoch5, step2072]: loss 0.840088
[epoch5, step2073]: loss 0.915574
[epoch5, step2074]: loss 0.392776
[epoch5, step2075]: loss 0.594943
[epoch5, step2076]: loss 0.523437
[epoch5, step2077]: loss 0.609209
[epoch5, step2078]: loss 0.654337
[epoch5, step2079]: loss 0.890776
[epoch5, step2080]: loss 0.764368
[epoch5, step2081]: loss 0.710410
[epoch5, step2082]: loss 0.775859
[epoch5, step2083]: loss 0.864119
[epoch5, step2084]: loss 0.719899
[epoch5, step2085]: loss 0.499661
[epoch5, step2086]: loss 0.651743
[epoch5, step2087]: loss 0.704265
[epoch5, step2088]: loss 0.666475
[epoch5, step2089]: loss 0.531322
[epoch5, step2090]: loss 0.514497
[epoch5, step2091]: loss 0.803075
[epoch5, step2092]: loss 0.436691
[epoch5, step2093]: loss 0.355161
[epoch5, step2094]: loss 0.773283
[epoch5, step2095]: loss 0.622498
[epoch5, step2096]: loss 0.702944
[epoch5, step2097]: loss 0.735583
[epoch5, step2098]: loss 0.596395
[epoch5, step2099]: loss 0.781698
[epoch5, step2100]: loss 0.770746
[epoch5, step2101]: loss 0.424168
[epoch5, step2102]: loss 0.732277
[epoch5, step2103]: loss 0.896134
[epoch5, step2104]: loss 0.786059
[epoch5, step2105]: loss 0.803522
[epoch5, step2106]: loss 0.628583
[epoch5, step2107]: loss 0.852246
[epoch5, step2108]: loss 0.774722
[epoch5, step2109]: loss 0.622746
[epoch5, step2110]: loss 0.827219
[epoch5, step2111]: loss 0.570158
[epoch5, step2112]: loss 0.848573
[epoch5, step2113]: loss 0.963962
[epoch5, step2114]: loss 0.912622
[epoch5, step2115]: loss 0.526534
[epoch5, step2116]: loss 0.754146
[epoch5, step2117]: loss 0.704694
[epoch5, step2118]: loss 0.879754
[epoch5, step2119]: loss 0.788171
[epoch5, step2120]: loss 0.637794
[epoch5, step2121]: loss 0.482137
[epoch5, step2122]: loss 0.595587
[epoch5, step2123]: loss 0.623241
[epoch5, step2124]: loss 0.708221
[epoch5, step2125]: loss 0.895953
[epoch5, step2126]: loss 1.059008
[epoch5, step2127]: loss 0.857687
[epoch5, step2128]: loss 0.602955
[epoch5, step2129]: loss 0.664610
[epoch5, step2130]: loss 0.655765
[epoch5, step2131]: loss 0.856615
[epoch5, step2132]: loss 0.840251
[epoch5, step2133]: loss 0.873891
[epoch5, step2134]: loss 0.700801
[epoch5, step2135]: loss 0.633265
[epoch5, step2136]: loss 0.665493
[epoch5, step2137]: loss 0.631481
[epoch5, step2138]: loss 0.593483
[epoch5, step2139]: loss 0.825151
[epoch5, step2140]: loss 0.652165
[epoch5, step2141]: loss 0.842009
[epoch5, step2142]: loss 0.778497
[epoch5, step2143]: loss 0.545785
[epoch5, step2144]: loss 0.423653
[epoch5, step2145]: loss 0.605697
[epoch5, step2146]: loss 0.709895
[epoch5, step2147]: loss 0.656016
[epoch5, step2148]: loss 0.532111
[epoch5, step2149]: loss 0.999823
[epoch5, step2150]: loss 0.633523
[epoch5, step2151]: loss 0.377109
[epoch5, step2152]: loss 1.014873
[epoch5, step2153]: loss 0.756943
[epoch5, step2154]: loss 0.876432
[epoch5, step2155]: loss 0.819034
[epoch5, step2156]: loss 0.942972
[epoch5, step2157]: loss 0.354281
[epoch5, step2158]: loss 0.618056
[epoch5, step2159]: loss 0.826732
[epoch5, step2160]: loss 0.723621
[epoch5, step2161]: loss 0.657977
[epoch5, step2162]: loss 0.493740
[epoch5, step2163]: loss 0.719926
[epoch5, step2164]: loss 0.724100
[epoch5, step2165]: loss 0.574531
[epoch5, step2166]: loss 0.479912
[epoch5, step2167]: loss 0.634303
[epoch5, step2168]: loss 0.775063
[epoch5, step2169]: loss 0.760564
[epoch5, step2170]: loss 0.961859
[epoch5, step2171]: loss 0.737166
[epoch5, step2172]: loss 0.652804
[epoch5, step2173]: loss 1.045369
[epoch5, step2174]: loss 0.767800
[epoch5, step2175]: loss 0.755208
[epoch5, step2176]: loss 0.720062
[epoch5, step2177]: loss 0.558815
[epoch5, step2178]: loss 0.719071
[epoch5, step2179]: loss 0.600579
[epoch5, step2180]: loss 0.466064
[epoch5, step2181]: loss 0.734508
[epoch5, step2182]: loss 0.652637
[epoch5, step2183]: loss 0.569791
[epoch5, step2184]: loss 1.013545
[epoch5, step2185]: loss 0.808689
[epoch5, step2186]: loss 0.690425
[epoch5, step2187]: loss 0.986876
[epoch5, step2188]: loss 0.953770
[epoch5, step2189]: loss 0.758652
[epoch5, step2190]: loss 0.607652
[epoch5, step2191]: loss 0.959343
[epoch5, step2192]: loss 0.795840
[epoch5, step2193]: loss 0.886226
[epoch5, step2194]: loss 0.694224
[epoch5, step2195]: loss 0.224212
[epoch5, step2196]: loss 0.802982
[epoch5, step2197]: loss 0.782029
[epoch5, step2198]: loss 0.895817
[epoch5, step2199]: loss 0.740534
[epoch5, step2200]: loss 0.402091
[epoch5, step2201]: loss 0.714141
[epoch5, step2202]: loss 0.678550
[epoch5, step2203]: loss 0.885421
[epoch5, step2204]: loss 0.431184
[epoch5, step2205]: loss 0.756034
[epoch5, step2206]: loss 0.615115
[epoch5, step2207]: loss 0.710321
[epoch5, step2208]: loss 0.664769
[epoch5, step2209]: loss 0.607536
[epoch5, step2210]: loss 0.835240
[epoch5, step2211]: loss 0.790422
[epoch5, step2212]: loss 0.542487
[epoch5, step2213]: loss 0.762729
[epoch5, step2214]: loss 0.348748
[epoch5, step2215]: loss 0.492893
[epoch5, step2216]: loss 0.574160
[epoch5, step2217]: loss 0.786841
[epoch5, step2218]: loss 0.960886
[epoch5, step2219]: loss 0.566836
[epoch5, step2220]: loss 0.801552
[epoch5, step2221]: loss 0.554466
[epoch5, step2222]: loss 0.730812
[epoch5, step2223]: loss 0.579230
[epoch5, step2224]: loss 0.874838
[epoch5, step2225]: loss 0.788265
[epoch5, step2226]: loss 0.801172
[epoch5, step2227]: loss 0.690631
[epoch5, step2228]: loss 0.731420
[epoch5, step2229]: loss 0.620352
[epoch5, step2230]: loss 0.646968
[epoch5, step2231]: loss 0.687574
[epoch5, step2232]: loss 0.576554
[epoch5, step2233]: loss 0.728912
[epoch5, step2234]: loss 0.796751
[epoch5, step2235]: loss 0.892226
[epoch5, step2236]: loss 0.867476
[epoch5, step2237]: loss 0.769057
[epoch5, step2238]: loss 0.856680
[epoch5, step2239]: loss 0.632192
[epoch5, step2240]: loss 0.545332
[epoch5, step2241]: loss 0.783119
[epoch5, step2242]: loss 0.660497
[epoch5, step2243]: loss 0.677761
[epoch5, step2244]: loss 0.792731
[epoch5, step2245]: loss 0.754006
[epoch5, step2246]: loss 0.811245
[epoch5, step2247]: loss 0.678422
[epoch5, step2248]: loss 0.873538
[epoch5, step2249]: loss 0.761124
[epoch5, step2250]: loss 0.693937
[epoch5, step2251]: loss 0.726884
[epoch5, step2252]: loss 0.299358
[epoch5, step2253]: loss 0.816910
[epoch5, step2254]: loss 0.883971
[epoch5, step2255]: loss 0.645906
[epoch5, step2256]: loss 0.735055
[epoch5, step2257]: loss 0.719705
[epoch5, step2258]: loss 0.763795
[epoch5, step2259]: loss 0.625711
[epoch5, step2260]: loss 0.599155
[epoch5, step2261]: loss 0.701418
[epoch5, step2262]: loss 0.768231
[epoch5, step2263]: loss 0.626594
[epoch5, step2264]: loss 0.525302
[epoch5, step2265]: loss 0.891127
[epoch5, step2266]: loss 0.841512
[epoch5, step2267]: loss 0.629413
[epoch5, step2268]: loss 0.728986
[epoch5, step2269]: loss 0.824113
[epoch5, step2270]: loss 0.667387
[epoch5, step2271]: loss 0.737008
[epoch5, step2272]: loss 0.984129
[epoch5, step2273]: loss 0.584128
[epoch5, step2274]: loss 0.565244
[epoch5, step2275]: loss 0.568076
[epoch5, step2276]: loss 0.639058
[epoch5, step2277]: loss 0.817834
[epoch5, step2278]: loss 0.800212
[epoch5, step2279]: loss 0.794171
[epoch5, step2280]: loss 0.630859
[epoch5, step2281]: loss 0.811749
[epoch5, step2282]: loss 0.674402
[epoch5, step2283]: loss 0.513702
[epoch5, step2284]: loss 0.482789
[epoch5, step2285]: loss 0.444967
[epoch5, step2286]: loss 0.731705
[epoch5, step2287]: loss 0.718587
[epoch5, step2288]: loss 0.748362
[epoch5, step2289]: loss 0.660802
[epoch5, step2290]: loss 0.529280
[epoch5, step2291]: loss 0.671869
[epoch5, step2292]: loss 0.824683
[epoch5, step2293]: loss 0.702053
[epoch5, step2294]: loss 0.790216
[epoch5, step2295]: loss 0.772603
[epoch5, step2296]: loss 0.584398
[epoch5, step2297]: loss 0.705312
[epoch5, step2298]: loss 0.897337
[epoch5, step2299]: loss 0.590060
[epoch5, step2300]: loss 0.990168
[epoch5, step2301]: loss 0.955963
[epoch5, step2302]: loss 0.460948
[epoch5, step2303]: loss 0.507108
[epoch5, step2304]: loss 0.733784
[epoch5, step2305]: loss 0.612226
[epoch5, step2306]: loss 0.805141
[epoch5, step2307]: loss 0.709007
[epoch5, step2308]: loss 0.690907
[epoch5, step2309]: loss 0.534461
[epoch5, step2310]: loss 0.549757
[epoch5, step2311]: loss 0.878579
[epoch5, step2312]: loss 0.823902
[epoch5, step2313]: loss 0.732055
[epoch5, step2314]: loss 0.396528
[epoch5, step2315]: loss 0.518644
[epoch5, step2316]: loss 0.278508
[epoch5, step2317]: loss 0.488445
[epoch5, step2318]: loss 0.725004
[epoch5, step2319]: loss 0.241835
[epoch5, step2320]: loss 0.675390
[epoch5, step2321]: loss 0.584199
[epoch5, step2322]: loss 0.643008
[epoch5, step2323]: loss 0.686542
[epoch5, step2324]: loss 0.530708
[epoch5, step2325]: loss 0.593388
[epoch5, step2326]: loss 0.769758
[epoch5, step2327]: loss 0.879326
[epoch5, step2328]: loss 0.614198
[epoch5, step2329]: loss 0.596822
[epoch5, step2330]: loss 0.868318
[epoch5, step2331]: loss 0.770901
[epoch5, step2332]: loss 0.781774
[epoch5, step2333]: loss 0.311939
[epoch5, step2334]: loss 0.894161
[epoch5, step2335]: loss 0.685048
[epoch5, step2336]: loss 0.827682
[epoch5, step2337]: loss 0.552359
[epoch5, step2338]: loss 0.823594
[epoch5, step2339]: loss 0.805777
[epoch5, step2340]: loss 0.703521
[epoch5, step2341]: loss 0.674724
[epoch5, step2342]: loss 0.730001
[epoch5, step2343]: loss 0.748712
[epoch5, step2344]: loss 0.720859
[epoch5, step2345]: loss 0.880060
[epoch5, step2346]: loss 0.761281
[epoch5, step2347]: loss 0.680758
[epoch5, step2348]: loss 0.830681
[epoch5, step2349]: loss 0.837305
[epoch5, step2350]: loss 0.640054
[epoch5, step2351]: loss 0.824832
[epoch5, step2352]: loss 0.566954
[epoch5, step2353]: loss 0.692022
[epoch5, step2354]: loss 0.835800
[epoch5, step2355]: loss 0.719188
[epoch5, step2356]: loss 0.649577
[epoch5, step2357]: loss 0.548990
[epoch5, step2358]: loss 0.928352
[epoch5, step2359]: loss 0.721700
[epoch5, step2360]: loss 0.857916
[epoch5, step2361]: loss 0.795512
[epoch5, step2362]: loss 0.769312
[epoch5, step2363]: loss 0.644427
[epoch5, step2364]: loss 0.476174
[epoch5, step2365]: loss 0.569914
[epoch5, step2366]: loss 0.804625
[epoch5, step2367]: loss 0.580600
[epoch5, step2368]: loss 0.717120
[epoch5, step2369]: loss 0.549460
[epoch5, step2370]: loss 0.798569
[epoch5, step2371]: loss 0.617698
[epoch5, step2372]: loss 0.895571
[epoch5, step2373]: loss 0.646253
[epoch5, step2374]: loss 0.487595
[epoch5, step2375]: loss 0.735519
[epoch5, step2376]: loss 0.571333
[epoch5, step2377]: loss 0.936513
[epoch5, step2378]: loss 0.812745
[epoch5, step2379]: loss 0.582326
[epoch5, step2380]: loss 0.725167
[epoch5, step2381]: loss 0.867909
[epoch5, step2382]: loss 0.766623
[epoch5, step2383]: loss 0.466189
[epoch5, step2384]: loss 0.797953
[epoch5, step2385]: loss 0.510438
[epoch5, step2386]: loss 0.684372
[epoch5, step2387]: loss 0.490593
[epoch5, step2388]: loss 0.644261
[epoch5, step2389]: loss 0.459882
[epoch5, step2390]: loss 0.652359
[epoch5, step2391]: loss 0.953284
[epoch5, step2392]: loss 0.927472
[epoch5, step2393]: loss 0.862567
[epoch5, step2394]: loss 0.408085
[epoch5, step2395]: loss 0.767703
[epoch5, step2396]: loss 0.737420
[epoch5, step2397]: loss 0.997988
[epoch5, step2398]: loss 0.429761
[epoch5, step2399]: loss 0.902644
[epoch5, step2400]: loss 0.557713
[epoch5, step2401]: loss 0.792860
[epoch5, step2402]: loss 0.262286
[epoch5, step2403]: loss 0.642875
[epoch5, step2404]: loss 0.550630
[epoch5, step2405]: loss 0.607507
[epoch5, step2406]: loss 0.576841
[epoch5, step2407]: loss 0.843103
[epoch5, step2408]: loss 0.783972
[epoch5, step2409]: loss 0.756349
[epoch5, step2410]: loss 0.814532
[epoch5, step2411]: loss 0.825885
[epoch5, step2412]: loss 0.766780
[epoch5, step2413]: loss 0.781711
[epoch5, step2414]: loss 0.778731
[epoch5, step2415]: loss 0.615735
[epoch5, step2416]: loss 0.732243
[epoch5, step2417]: loss 0.575141
[epoch5, step2418]: loss 0.613614
[epoch5, step2419]: loss 0.550123
[epoch5, step2420]: loss 0.809512
[epoch5, step2421]: loss 0.614258
[epoch5, step2422]: loss 0.377131
[epoch5, step2423]: loss 0.775255
[epoch5, step2424]: loss 0.824617
[epoch5, step2425]: loss 0.567011
[epoch5, step2426]: loss 0.482393
[epoch5, step2427]: loss 0.928886
[epoch5, step2428]: loss 0.821325
[epoch5, step2429]: loss 0.604489
[epoch5, step2430]: loss 0.783027
[epoch5, step2431]: loss 0.524619
[epoch5, step2432]: loss 0.777057
[epoch5, step2433]: loss 0.678411
[epoch5, step2434]: loss 0.806430
[epoch5, step2435]: loss 0.930004
[epoch5, step2436]: loss 0.642717
[epoch5, step2437]: loss 0.508653
[epoch5, step2438]: loss 0.682459
[epoch5, step2439]: loss 0.423980
[epoch5, step2440]: loss 0.480580
[epoch5, step2441]: loss 0.680705
[epoch5, step2442]: loss 0.629885
[epoch5, step2443]: loss 0.849252
[epoch5, step2444]: loss 0.558575
[epoch5, step2445]: loss 0.641423
[epoch5, step2446]: loss 0.714432
[epoch5, step2447]: loss 0.496544
[epoch5, step2448]: loss 0.486255
[epoch5, step2449]: loss 1.008866
[epoch5, step2450]: loss 0.701584
[epoch5, step2451]: loss 0.434328
[epoch5, step2452]: loss 0.656445
[epoch5, step2453]: loss 0.683857
[epoch5, step2454]: loss 0.791508
[epoch5, step2455]: loss 0.830483
[epoch5, step2456]: loss 0.593329
[epoch5, step2457]: loss 0.966972
[epoch5, step2458]: loss 0.939370
[epoch5, step2459]: loss 0.545590
[epoch5, step2460]: loss 0.550941
[epoch5, step2461]: loss 0.778653
[epoch5, step2462]: loss 0.869558
[epoch5, step2463]: loss 0.400490
[epoch5, step2464]: loss 0.666369
[epoch5, step2465]: loss 0.574974
[epoch5, step2466]: loss 0.726261
[epoch5, step2467]: loss 0.577546
[epoch5, step2468]: loss 0.471912
[epoch5, step2469]: loss 0.577552
[epoch5, step2470]: loss 0.817717
[epoch5, step2471]: loss 0.662627
[epoch5, step2472]: loss 0.772647
[epoch5, step2473]: loss 0.714642
[epoch5, step2474]: loss 0.657450
[epoch5, step2475]: loss 0.573296
[epoch5, step2476]: loss 0.861862
[epoch5, step2477]: loss 0.714513
[epoch5, step2478]: loss 0.735491
[epoch5, step2479]: loss 0.559688
[epoch5, step2480]: loss 0.650928
[epoch5, step2481]: loss 0.779595
[epoch5, step2482]: loss 0.550360
[epoch5, step2483]: loss 0.683636
[epoch5, step2484]: loss 0.713131
[epoch5, step2485]: loss 0.598679
[epoch5, step2486]: loss 0.796603
[epoch5, step2487]: loss 0.711871
[epoch5, step2488]: loss 0.630397
[epoch5, step2489]: loss 0.623487
[epoch5, step2490]: loss 0.963655
[epoch5, step2491]: loss 0.534767
[epoch5, step2492]: loss 0.886564
[epoch5, step2493]: loss 0.865765
[epoch5, step2494]: loss 0.804104
[epoch5, step2495]: loss 0.556120
[epoch5, step2496]: loss 0.762515
[epoch5, step2497]: loss 0.765257
[epoch5, step2498]: loss 0.873787
[epoch5, step2499]: loss 0.703471
[epoch5, step2500]: loss 0.672833
[epoch5, step2501]: loss 1.004576
[epoch5, step2502]: loss 0.759538
[epoch5, step2503]: loss 0.767795
[epoch5, step2504]: loss 0.790134
[epoch5, step2505]: loss 0.596092
[epoch5, step2506]: loss 0.498596
[epoch5, step2507]: loss 0.642839
[epoch5, step2508]: loss 0.749981
[epoch5, step2509]: loss 0.887704
[epoch5, step2510]: loss 0.459485
[epoch5, step2511]: loss 0.621422
[epoch5, step2512]: loss 0.667129
[epoch5, step2513]: loss 0.765909
[epoch5, step2514]: loss 0.493598
[epoch5, step2515]: loss 0.812534
[epoch5, step2516]: loss 0.874402
[epoch5, step2517]: loss 0.768698
[epoch5, step2518]: loss 0.630792
[epoch5, step2519]: loss 0.675369
[epoch5, step2520]: loss 0.590490
[epoch5, step2521]: loss 0.612879
[epoch5, step2522]: loss 0.568110
[epoch5, step2523]: loss 0.702862
[epoch5, step2524]: loss 0.434065
[epoch5, step2525]: loss 0.784920
[epoch5, step2526]: loss 0.647927
[epoch5, step2527]: loss 0.611842
[epoch5, step2528]: loss 0.891759
[epoch5, step2529]: loss 0.674628
[epoch5, step2530]: loss 0.807059
[epoch5, step2531]: loss 0.839541
[epoch5, step2532]: loss 0.725698
[epoch5, step2533]: loss 0.815096
[epoch5, step2534]: loss 0.546481
[epoch5, step2535]: loss 0.603851
[epoch5, step2536]: loss 0.808531
[epoch5, step2537]: loss 0.872434
[epoch5, step2538]: loss 0.780470
[epoch5, step2539]: loss 0.628601
[epoch5, step2540]: loss 0.573192
[epoch5, step2541]: loss 0.801927
[epoch5, step2542]: loss 0.814628
[epoch5, step2543]: loss 0.498294
[epoch5, step2544]: loss 0.657915
[epoch5, step2545]: loss 0.482494
[epoch5, step2546]: loss 0.870779
[epoch5, step2547]: loss 0.500568
[epoch5, step2548]: loss 0.891516
[epoch5, step2549]: loss 0.683786
[epoch5, step2550]: loss 0.636343
[epoch5, step2551]: loss 0.714327
[epoch5, step2552]: loss 0.371631
[epoch5, step2553]: loss 0.893153
[epoch5, step2554]: loss 0.778982
[epoch5, step2555]: loss 0.815656
[epoch5, step2556]: loss 0.648921
[epoch5, step2557]: loss 0.827101
[epoch5, step2558]: loss 0.748583
[epoch5, step2559]: loss 0.638652
[epoch5, step2560]: loss 0.884257
[epoch5, step2561]: loss 0.443610
[epoch5, step2562]: loss 0.529051
[epoch5, step2563]: loss 0.586307
[epoch5, step2564]: loss 0.787802
[epoch5, step2565]: loss 0.730860
[epoch5, step2566]: loss 0.698427
[epoch5, step2567]: loss 0.835611
[epoch5, step2568]: loss 0.778549
[epoch5, step2569]: loss 0.826546
[epoch5, step2570]: loss 0.643369
[epoch5, step2571]: loss 0.982321
[epoch5, step2572]: loss 0.641201
[epoch5, step2573]: loss 0.780158
[epoch5, step2574]: loss 0.674491
[epoch5, step2575]: loss 0.836367
[epoch5, step2576]: loss 0.856320
[epoch5, step2577]: loss 0.725585
[epoch5, step2578]: loss 0.643311
[epoch5, step2579]: loss 0.963221
[epoch5, step2580]: loss 0.763391
[epoch5, step2581]: loss 0.518722
[epoch5, step2582]: loss 0.692876
[epoch5, step2583]: loss 0.275994
[epoch5, step2584]: loss 0.894485
[epoch5, step2585]: loss 0.893780
[epoch5, step2586]: loss 0.893301
[epoch5, step2587]: loss 0.683215
[epoch5, step2588]: loss 0.674866
[epoch5, step2589]: loss 0.797557
[epoch5, step2590]: loss 0.598085
[epoch5, step2591]: loss 0.690088
[epoch5, step2592]: loss 0.835280
[epoch5, step2593]: loss 0.865999
[epoch5, step2594]: loss 0.511097
[epoch5, step2595]: loss 0.411940
[epoch5, step2596]: loss 0.684023
[epoch5, step2597]: loss 0.837365
[epoch5, step2598]: loss 0.689965
[epoch5, step2599]: loss 0.872354
[epoch5, step2600]: loss 0.751414
[epoch5, step2601]: loss 0.649191
[epoch5, step2602]: loss 0.967617
[epoch5, step2603]: loss 0.715815
[epoch5, step2604]: loss 0.902442
[epoch5, step2605]: loss 0.479953
[epoch5, step2606]: loss 0.604458
[epoch5, step2607]: loss 0.654227
[epoch5, step2608]: loss 0.743759
[epoch5, step2609]: loss 0.733325
[epoch5, step2610]: loss 0.398175
[epoch5, step2611]: loss 0.502649
[epoch5, step2612]: loss 0.774268
[epoch5, step2613]: loss 0.706003
[epoch5, step2614]: loss 0.966705
[epoch5, step2615]: loss 0.785282
[epoch5, step2616]: loss 0.623376
[epoch5, step2617]: loss 0.537989
[epoch5, step2618]: loss 0.677229
[epoch5, step2619]: loss 0.596589
[epoch5, step2620]: loss 0.467814
[epoch5, step2621]: loss 0.628496
[epoch5, step2622]: loss 0.515593
[epoch5, step2623]: loss 0.829910
[epoch5, step2624]: loss 0.723697
[epoch5, step2625]: loss 0.858978
[epoch5, step2626]: loss 0.613982
[epoch5, step2627]: loss 0.558492
[epoch5, step2628]: loss 0.629956
[epoch5, step2629]: loss 0.742663
[epoch5, step2630]: loss 0.870736
[epoch5, step2631]: loss 0.800526
[epoch5, step2632]: loss 0.599315
[epoch5, step2633]: loss 0.847607
[epoch5, step2634]: loss 0.609436
[epoch5, step2635]: loss 0.768939
[epoch5, step2636]: loss 0.332977
[epoch5, step2637]: loss 0.461917
[epoch5, step2638]: loss 0.631501
[epoch5, step2639]: loss 0.649413
[epoch5, step2640]: loss 0.639454
[epoch5, step2641]: loss 0.594730
[epoch5, step2642]: loss 0.704083
[epoch5, step2643]: loss 0.775041
[epoch5, step2644]: loss 0.762402
[epoch5, step2645]: loss 0.758043
[epoch5, step2646]: loss 0.633302
[epoch5, step2647]: loss 0.759259
[epoch5, step2648]: loss 0.701182
[epoch5, step2649]: loss 0.817471
[epoch5, step2650]: loss 0.430522
[epoch5, step2651]: loss 0.280702
[epoch5, step2652]: loss 0.875083
[epoch5, step2653]: loss 0.685066
[epoch5, step2654]: loss 0.422943
[epoch5, step2655]: loss 0.666748
[epoch5, step2656]: loss 0.256427
[epoch5, step2657]: loss 0.710522
[epoch5, step2658]: loss 0.746471
[epoch5, step2659]: loss 0.615475
[epoch5, step2660]: loss 0.605034
[epoch5, step2661]: loss 0.791554
[epoch5, step2662]: loss 0.923141
[epoch5, step2663]: loss 0.499261
[epoch5, step2664]: loss 0.616906
[epoch5, step2665]: loss 0.686513
[epoch5, step2666]: loss 0.440772
[epoch5, step2667]: loss 0.913074
[epoch5, step2668]: loss 0.743713
[epoch5, step2669]: loss 0.892497
[epoch5, step2670]: loss 0.734571
[epoch5, step2671]: loss 0.489962
[epoch5, step2672]: loss 0.451566
[epoch5, step2673]: loss 0.803471
[epoch5, step2674]: loss 0.869720
[epoch5, step2675]: loss 0.821959
[epoch5, step2676]: loss 0.666140
[epoch5, step2677]: loss 0.927585
[epoch5, step2678]: loss 0.909987
[epoch5, step2679]: loss 0.796447
[epoch5, step2680]: loss 0.609454
[epoch5, step2681]: loss 0.417999
[epoch5, step2682]: loss 0.302797
[epoch5, step2683]: loss 0.699998
[epoch5, step2684]: loss 0.699004
[epoch5, step2685]: loss 0.769259
[epoch5, step2686]: loss 0.903072
[epoch5, step2687]: loss 0.579478
[epoch5, step2688]: loss 0.504240
[epoch5, step2689]: loss 0.628850
[epoch5, step2690]: loss 0.783277
[epoch5, step2691]: loss 0.730194
[epoch5, step2692]: loss 0.616741
[epoch5, step2693]: loss 0.909853
[epoch5, step2694]: loss 0.684850
[epoch5, step2695]: loss 0.694934
[epoch5, step2696]: loss 0.659675
[epoch5, step2697]: loss 0.745854
[epoch5, step2698]: loss 0.900489
[epoch5, step2699]: loss 0.809743
[epoch5, step2700]: loss 0.633847
[epoch5, step2701]: loss 0.568607
[epoch5, step2702]: loss 0.695716
[epoch5, step2703]: loss 0.764758
[epoch5, step2704]: loss 0.822316
[epoch5, step2705]: loss 0.654045
[epoch5, step2706]: loss 0.636164
[epoch5, step2707]: loss 0.906128
[epoch5, step2708]: loss 0.768648
[epoch5, step2709]: loss 0.821941
[epoch5, step2710]: loss 0.769699
[epoch5, step2711]: loss 0.578091
[epoch5, step2712]: loss 1.000742
[epoch5, step2713]: loss 0.697122
[epoch5, step2714]: loss 0.746710
[epoch5, step2715]: loss 0.259259
[epoch5, step2716]: loss 0.820775
[epoch5, step2717]: loss 0.793482
[epoch5, step2718]: loss 0.728126
[epoch5, step2719]: loss 0.810042
[epoch5, step2720]: loss 0.558504
[epoch5, step2721]: loss 0.527302
[epoch5, step2722]: loss 0.813018
[epoch5, step2723]: loss 0.778888
[epoch5, step2724]: loss 0.817399
[epoch5, step2725]: loss 0.606660
[epoch5, step2726]: loss 0.788352
[epoch5, step2727]: loss 0.770479
[epoch5, step2728]: loss 0.383278
[epoch5, step2729]: loss 0.702714
[epoch5, step2730]: loss 0.681825
[epoch5, step2731]: loss 0.754923
[epoch5, step2732]: loss 0.758944
[epoch5, step2733]: loss 1.092032
[epoch5, step2734]: loss 0.542802
[epoch5, step2735]: loss 0.731004
[epoch5, step2736]: loss 0.351824
[epoch5, step2737]: loss 0.792508
[epoch5, step2738]: loss 0.762062
[epoch5, step2739]: loss 0.681410
[epoch5, step2740]: loss 0.657417
[epoch5, step2741]: loss 0.639412
[epoch5, step2742]: loss 0.784327
[epoch5, step2743]: loss 0.711504
[epoch5, step2744]: loss 0.783074
[epoch5, step2745]: loss 0.609555
[epoch5, step2746]: loss 0.488018
[epoch5, step2747]: loss 0.685577
[epoch5, step2748]: loss 0.492643
[epoch5, step2749]: loss 0.662138
[epoch5, step2750]: loss 0.522687
[epoch5, step2751]: loss 0.791967
[epoch5, step2752]: loss 0.595774
[epoch5, step2753]: loss 0.469969
[epoch5, step2754]: loss 0.758416
[epoch5, step2755]: loss 0.628275
[epoch5, step2756]: loss 0.870078
[epoch5, step2757]: loss 0.689460
[epoch5, step2758]: loss 0.654176
[epoch5, step2759]: loss 0.903943
[epoch5, step2760]: loss 0.472910
[epoch5, step2761]: loss 0.738175
[epoch5, step2762]: loss 0.752293
[epoch5, step2763]: loss 0.698280
[epoch5, step2764]: loss 0.371873
[epoch5, step2765]: loss 0.670378
[epoch5, step2766]: loss 0.532445
[epoch5, step2767]: loss 0.767413
[epoch5, step2768]: loss 0.745663
[epoch5, step2769]: loss 0.696065
[epoch5, step2770]: loss 0.857758
[epoch5, step2771]: loss 0.884031
[epoch5, step2772]: loss 1.032213
[epoch5, step2773]: loss 0.550957
[epoch5, step2774]: loss 0.709539
[epoch5, step2775]: loss 0.676595
[epoch5, step2776]: loss 0.782429
[epoch5, step2777]: loss 0.636699
[epoch5, step2778]: loss 0.505199
[epoch5, step2779]: loss 0.721539
[epoch5, step2780]: loss 0.911986
[epoch5, step2781]: loss 0.612567
[epoch5, step2782]: loss 0.682449
[epoch5, step2783]: loss 0.493231
[epoch5, step2784]: loss 0.723417
[epoch5, step2785]: loss 0.837421
[epoch5, step2786]: loss 0.873385
[epoch5, step2787]: loss 0.295695
[epoch5, step2788]: loss 0.792138
[epoch5, step2789]: loss 0.909646
[epoch5, step2790]: loss 0.810918
[epoch5, step2791]: loss 0.354473
[epoch5, step2792]: loss 0.627997
[epoch5, step2793]: loss 0.779415
[epoch5, step2794]: loss 0.995066
[epoch5, step2795]: loss 0.924263
[epoch5, step2796]: loss 0.829801
[epoch5, step2797]: loss 0.799224
[epoch5, step2798]: loss 0.731772
[epoch5, step2799]: loss 0.693705
[epoch5, step2800]: loss 0.640889
[epoch5, step2801]: loss 0.681103
[epoch5, step2802]: loss 0.632187
[epoch5, step2803]: loss 0.786420
[epoch5, step2804]: loss 0.526900
[epoch5, step2805]: loss 0.882420
[epoch5, step2806]: loss 0.660249
[epoch5, step2807]: loss 0.786804
[epoch5, step2808]: loss 0.825753
[epoch5, step2809]: loss 0.701740
[epoch5, step2810]: loss 0.433992
[epoch5, step2811]: loss 0.628037
[epoch5, step2812]: loss 0.597673
[epoch5, step2813]: loss 0.737173
[epoch5, step2814]: loss 0.655435
[epoch5, step2815]: loss 0.654497
[epoch5, step2816]: loss 0.757055
[epoch5, step2817]: loss 0.749710
[epoch5, step2818]: loss 0.760540
[epoch5, step2819]: loss 0.641171
[epoch5, step2820]: loss 0.695127
[epoch5, step2821]: loss 0.535473
[epoch5, step2822]: loss 0.686002
[epoch5, step2823]: loss 0.557596
[epoch5, step2824]: loss 0.749411
[epoch5, step2825]: loss 0.820268
[epoch5, step2826]: loss 0.751891
[epoch5, step2827]: loss 0.678263
[epoch5, step2828]: loss 0.734865
[epoch5, step2829]: loss 0.585694
[epoch5, step2830]: loss 0.218405
[epoch5, step2831]: loss 0.876087
[epoch5, step2832]: loss 0.618338
[epoch5, step2833]: loss 0.873043
[epoch5, step2834]: loss 0.881346
[epoch5, step2835]: loss 0.838117
[epoch5, step2836]: loss 0.677663
[epoch5, step2837]: loss 0.617604
[epoch5, step2838]: loss 0.660846
[epoch5, step2839]: loss 0.586933
[epoch5, step2840]: loss 0.438826
[epoch5, step2841]: loss 0.664622
[epoch5, step2842]: loss 0.625404
[epoch5, step2843]: loss 0.973375
[epoch5, step2844]: loss 0.866315
[epoch5, step2845]: loss 0.578915
[epoch5, step2846]: loss 0.785041
[epoch5, step2847]: loss 0.442711
[epoch5, step2848]: loss 0.790423
[epoch5, step2849]: loss 0.484660
[epoch5, step2850]: loss 0.975281
[epoch5, step2851]: loss 0.532367
[epoch5, step2852]: loss 0.674804
[epoch5, step2853]: loss 0.601295
[epoch5, step2854]: loss 0.670721
[epoch5, step2855]: loss 0.641696
[epoch5, step2856]: loss 0.466584
[epoch5, step2857]: loss 0.481078
[epoch5, step2858]: loss 0.728287
[epoch5, step2859]: loss 0.640315
[epoch5, step2860]: loss 0.517283
[epoch5, step2861]: loss 0.855342
[epoch5, step2862]: loss 0.668799
[epoch5, step2863]: loss 0.714527
[epoch5, step2864]: loss 0.533418
[epoch5, step2865]: loss 0.388572
[epoch5, step2866]: loss 0.670536
[epoch5, step2867]: loss 0.791110
[epoch5, step2868]: loss 0.607368
[epoch5, step2869]: loss 0.779041
[epoch5, step2870]: loss 0.555817
[epoch5, step2871]: loss 0.677034
[epoch5, step2872]: loss 0.787127
[epoch5, step2873]: loss 0.701845
[epoch5, step2874]: loss 0.965762
[epoch5, step2875]: loss 0.553182
[epoch5, step2876]: loss 0.865847
[epoch5, step2877]: loss 0.449918
[epoch5, step2878]: loss 0.693153
[epoch5, step2879]: loss 0.676994
[epoch5, step2880]: loss 0.622647
[epoch5, step2881]: loss 0.803586
[epoch5, step2882]: loss 0.731496
[epoch5, step2883]: loss 0.800061
[epoch5, step2884]: loss 0.685830
[epoch5, step2885]: loss 0.764201
[epoch5, step2886]: loss 0.769732
[epoch5, step2887]: loss 0.723632
[epoch5, step2888]: loss 0.639544
[epoch5, step2889]: loss 0.731153
[epoch5, step2890]: loss 0.806259
[epoch5, step2891]: loss 0.690784
[epoch5, step2892]: loss 0.683033
[epoch5, step2893]: loss 0.914342
[epoch5, step2894]: loss 0.835369
[epoch5, step2895]: loss 0.907704
[epoch5, step2896]: loss 0.756978
[epoch5, step2897]: loss 0.589472
[epoch5, step2898]: loss 0.763732
[epoch5, step2899]: loss 0.679282
[epoch5, step2900]: loss 0.663533
[epoch5, step2901]: loss 0.678174
[epoch5, step2902]: loss 0.687922
[epoch5, step2903]: loss 0.719340
[epoch5, step2904]: loss 0.544127
[epoch5, step2905]: loss 0.832570
[epoch5, step2906]: loss 0.596695
[epoch5, step2907]: loss 0.723982
[epoch5, step2908]: loss 0.670082
[epoch5, step2909]: loss 0.561799
[epoch5, step2910]: loss 0.771922
[epoch5, step2911]: loss 0.699210
[epoch5, step2912]: loss 0.620357
[epoch5, step2913]: loss 0.872617
[epoch5, step2914]: loss 0.625584
[epoch5, step2915]: loss 0.862972
[epoch5, step2916]: loss 0.790732
[epoch5, step2917]: loss 0.685111
[epoch5, step2918]: loss 0.853781
[epoch5, step2919]: loss 0.693025
[epoch5, step2920]: loss 0.799314
[epoch5, step2921]: loss 0.912741
[epoch5, step2922]: loss 0.658198
[epoch5, step2923]: loss 0.963974
[epoch5, step2924]: loss 0.686241
[epoch5, step2925]: loss 0.967688
[epoch5, step2926]: loss 0.743914
[epoch5, step2927]: loss 0.668138
[epoch5, step2928]: loss 0.860983
[epoch5, step2929]: loss 0.755816
[epoch5, step2930]: loss 0.848445
[epoch5, step2931]: loss 0.673669
[epoch5, step2932]: loss 0.925917
[epoch5, step2933]: loss 0.702217
[epoch5, step2934]: loss 0.715850
[epoch5, step2935]: loss 0.791457
[epoch5, step2936]: loss 0.680205
[epoch5, step2937]: loss 0.622313
[epoch5, step2938]: loss 0.958856
[epoch5, step2939]: loss 0.705761
[epoch5, step2940]: loss 0.696184
[epoch5, step2941]: loss 0.695382
[epoch5, step2942]: loss 0.534635
[epoch5, step2943]: loss 0.524516
[epoch5, step2944]: loss 0.494215
[epoch5, step2945]: loss 0.808200
[epoch5, step2946]: loss 0.468058
[epoch5, step2947]: loss 0.664571
[epoch5, step2948]: loss 0.784823
[epoch5, step2949]: loss 0.787536
[epoch5, step2950]: loss 0.943605
[epoch5, step2951]: loss 0.704870
[epoch5, step2952]: loss 0.419540
[epoch5, step2953]: loss 0.826120
[epoch5, step2954]: loss 0.727202
[epoch5, step2955]: loss 0.796768
[epoch5, step2956]: loss 0.632139
[epoch5, step2957]: loss 0.785759
[epoch5, step2958]: loss 0.579501
[epoch5, step2959]: loss 0.986052
[epoch5, step2960]: loss 0.714583
[epoch5, step2961]: loss 0.551582
[epoch5, step2962]: loss 0.745470
[epoch5, step2963]: loss 0.791549
[epoch5, step2964]: loss 0.529500
[epoch5, step2965]: loss 0.861677
[epoch5, step2966]: loss 0.658664
[epoch5, step2967]: loss 0.638681
[epoch5, step2968]: loss 0.850355
[epoch5, step2969]: loss 0.636636
[epoch5, step2970]: loss 0.634152
[epoch5, step2971]: loss 0.780358
[epoch5, step2972]: loss 0.797069
[epoch5, step2973]: loss 0.669263
[epoch5, step2974]: loss 0.913278
[epoch5, step2975]: loss 0.542400
[epoch5, step2976]: loss 0.912786
[epoch5, step2977]: loss 0.478764
[epoch5, step2978]: loss 0.544578
[epoch5, step2979]: loss 0.615102
[epoch5, step2980]: loss 0.813594
[epoch5, step2981]: loss 0.737459
[epoch5, step2982]: loss 0.665436
[epoch5, step2983]: loss 0.509493
[epoch5, step2984]: loss 0.815458
[epoch5, step2985]: loss 0.696558
[epoch5, step2986]: loss 0.250093
[epoch5, step2987]: loss 0.742778
[epoch5, step2988]: loss 0.760756
[epoch5, step2989]: loss 0.404772
[epoch5, step2990]: loss 0.853069
[epoch5, step2991]: loss 0.347629
[epoch5, step2992]: loss 0.532282
[epoch5, step2993]: loss 0.239851
[epoch5, step2994]: loss 0.789230
[epoch5, step2995]: loss 1.004463
[epoch5, step2996]: loss 0.575813
[epoch5, step2997]: loss 1.002078
[epoch5, step2998]: loss 0.540719
[epoch5, step2999]: loss 0.699573
[epoch5, step3000]: loss 0.765759
[epoch5, step3001]: loss 0.574177
[epoch5, step3002]: loss 0.740201
[epoch5, step3003]: loss 0.956078
[epoch5, step3004]: loss 0.813825
[epoch5, step3005]: loss 0.801095
[epoch5, step3006]: loss 0.526053
[epoch5, step3007]: loss 0.852449
[epoch5, step3008]: loss 0.738872
[epoch5, step3009]: loss 0.592176
[epoch5, step3010]: loss 0.908466
[epoch5, step3011]: loss 0.501578
[epoch5, step3012]: loss 0.384782
[epoch5, step3013]: loss 0.941291
[epoch5, step3014]: loss 0.579714
[epoch5, step3015]: loss 0.517416
[epoch5, step3016]: loss 0.786236
[epoch5, step3017]: loss 0.665024
[epoch5, step3018]: loss 0.803927
[epoch5, step3019]: loss 0.666432
[epoch5, step3020]: loss 0.995890
[epoch5, step3021]: loss 0.585940
[epoch5, step3022]: loss 0.697652
[epoch5, step3023]: loss 0.764906
[epoch5, step3024]: loss 0.697266
[epoch5, step3025]: loss 0.642694
[epoch5, step3026]: loss 0.738176
[epoch5, step3027]: loss 0.738602
[epoch5, step3028]: loss 0.442890
[epoch5, step3029]: loss 0.912947
[epoch5, step3030]: loss 0.768234
[epoch5, step3031]: loss 0.362760
[epoch5, step3032]: loss 0.702059
[epoch5, step3033]: loss 0.702760
[epoch5, step3034]: loss 0.519341
[epoch5, step3035]: loss 0.917001
[epoch5, step3036]: loss 0.894286
[epoch5, step3037]: loss 1.003301
[epoch5, step3038]: loss 0.710634
[epoch5, step3039]: loss 0.785353
[epoch5, step3040]: loss 0.814574
[epoch5, step3041]: loss 0.553560
[epoch5, step3042]: loss 0.839213
[epoch5, step3043]: loss 0.683517
[epoch5, step3044]: loss 0.849540
[epoch5, step3045]: loss 0.535267
[epoch5, step3046]: loss 0.501623
[epoch5, step3047]: loss 0.716636
[epoch5, step3048]: loss 0.648958
[epoch5, step3049]: loss 0.971706
[epoch5, step3050]: loss 0.670458
[epoch5, step3051]: loss 0.614578
[epoch5, step3052]: loss 0.708671
[epoch5, step3053]: loss 0.596182
[epoch5, step3054]: loss 0.713825
[epoch5, step3055]: loss 0.494762
[epoch5, step3056]: loss 0.536267
[epoch5, step3057]: loss 0.694031
[epoch5, step3058]: loss 0.982119
[epoch5, step3059]: loss 0.507331
[epoch5, step3060]: loss 0.643084
[epoch5, step3061]: loss 0.843689
[epoch5, step3062]: loss 0.481689
[epoch5, step3063]: loss 0.718893
[epoch5, step3064]: loss 0.295752
[epoch5, step3065]: loss 0.715034
[epoch5, step3066]: loss 0.689535
[epoch5, step3067]: loss 0.661985
[epoch5, step3068]: loss 0.684175
[epoch5, step3069]: loss 1.001807
[epoch5, step3070]: loss 0.838719
[epoch5, step3071]: loss 0.831398
[epoch5, step3072]: loss 0.738938
[epoch5, step3073]: loss 0.614598
[epoch5, step3074]: loss 0.662448
[epoch5, step3075]: loss 0.887105
[epoch5, step3076]: loss 0.707219

[epoch5]: avg loss 0.707219

[epoch6, step1]: loss 0.571600
[epoch6, step2]: loss 0.733184
[epoch6, step3]: loss 0.555186
[epoch6, step4]: loss 0.534276
[epoch6, step5]: loss 0.411019
[epoch6, step6]: loss 0.876860
[epoch6, step7]: loss 0.766818
[epoch6, step8]: loss 0.373716
[epoch6, step9]: loss 0.764603
[epoch6, step10]: loss 0.364125
[epoch6, step11]: loss 0.737969
[epoch6, step12]: loss 0.536286
[epoch6, step13]: loss 0.548303
[epoch6, step14]: loss 0.552646
[epoch6, step15]: loss 0.525260
[epoch6, step16]: loss 0.848418
[epoch6, step17]: loss 0.476580
[epoch6, step18]: loss 0.810882
[epoch6, step19]: loss 0.716330
[epoch6, step20]: loss 0.668305
[epoch6, step21]: loss 0.765031
[epoch6, step22]: loss 0.864746
[epoch6, step23]: loss 0.570594
[epoch6, step24]: loss 0.550114
[epoch6, step25]: loss 0.735706
[epoch6, step26]: loss 0.634064
[epoch6, step27]: loss 0.728713
[epoch6, step28]: loss 0.704718
[epoch6, step29]: loss 0.587615
[epoch6, step30]: loss 0.841148
[epoch6, step31]: loss 0.700485
[epoch6, step32]: loss 0.561983
[epoch6, step33]: loss 0.503817
[epoch6, step34]: loss 0.645309
[epoch6, step35]: loss 0.662214
[epoch6, step36]: loss 0.656042
[epoch6, step37]: loss 0.647639
[epoch6, step38]: loss 0.846076
[epoch6, step39]: loss 0.834239
[epoch6, step40]: loss 0.913005
[epoch6, step41]: loss 0.707471
[epoch6, step42]: loss 0.460868
[epoch6, step43]: loss 0.756943
[epoch6, step44]: loss 0.913286
[epoch6, step45]: loss 0.945688
[epoch6, step46]: loss 1.003443
[epoch6, step47]: loss 0.585413
[epoch6, step48]: loss 0.895037
[epoch6, step49]: loss 0.648211
[epoch6, step50]: loss 0.514159
[epoch6, step51]: loss 0.422942
[epoch6, step52]: loss 0.415211
[epoch6, step53]: loss 0.786487
[epoch6, step54]: loss 0.687431
[epoch6, step55]: loss 0.201723
[epoch6, step56]: loss 0.786961
[epoch6, step57]: loss 0.473923
[epoch6, step58]: loss 0.624124
[epoch6, step59]: loss 0.836731
[epoch6, step60]: loss 0.888916
[epoch6, step61]: loss 0.796244
[epoch6, step62]: loss 0.912897
[epoch6, step63]: loss 0.371601
[epoch6, step64]: loss 0.610404
[epoch6, step65]: loss 0.545317
[epoch6, step66]: loss 0.831424
[epoch6, step67]: loss 0.707471
[epoch6, step68]: loss 0.582872
[epoch6, step69]: loss 0.639701
[epoch6, step70]: loss 0.650371
[epoch6, step71]: loss 0.653022
[epoch6, step72]: loss 0.691505
[epoch6, step73]: loss 0.738584
[epoch6, step74]: loss 0.454115
[epoch6, step75]: loss 0.702821
[epoch6, step76]: loss 0.824114
[epoch6, step77]: loss 0.851800
[epoch6, step78]: loss 0.647481
[epoch6, step79]: loss 0.805304
[epoch6, step80]: loss 0.668238
[epoch6, step81]: loss 0.710461
[epoch6, step82]: loss 0.772225
[epoch6, step83]: loss 0.419818
[epoch6, step84]: loss 0.896116
[epoch6, step85]: loss 0.987269
[epoch6, step86]: loss 0.400795
[epoch6, step87]: loss 0.416007
[epoch6, step88]: loss 0.571900
[epoch6, step89]: loss 0.696066
[epoch6, step90]: loss 0.735322
[epoch6, step91]: loss 0.472177
[epoch6, step92]: loss 0.913889
[epoch6, step93]: loss 0.190622
[epoch6, step94]: loss 0.876708
[epoch6, step95]: loss 0.872498
[epoch6, step96]: loss 0.964567
[epoch6, step97]: loss 0.893766
[epoch6, step98]: loss 0.841710
[epoch6, step99]: loss 0.428219
[epoch6, step100]: loss 0.660983
[epoch6, step101]: loss 0.539677
[epoch6, step102]: loss 0.826995
[epoch6, step103]: loss 0.684537
[epoch6, step104]: loss 0.962309
[epoch6, step105]: loss 0.416506
[epoch6, step106]: loss 0.761604
[epoch6, step107]: loss 0.482590
[epoch6, step108]: loss 0.778498
[epoch6, step109]: loss 0.619244
[epoch6, step110]: loss 0.525403
[epoch6, step111]: loss 0.919682
[epoch6, step112]: loss 0.734519
[epoch6, step113]: loss 0.626661
[epoch6, step114]: loss 0.544176
[epoch6, step115]: loss 0.790177
[epoch6, step116]: loss 0.743108
[epoch6, step117]: loss 0.503274
[epoch6, step118]: loss 0.999076
[epoch6, step119]: loss 0.660091
[epoch6, step120]: loss 0.790103
[epoch6, step121]: loss 0.713355
[epoch6, step122]: loss 0.593533
[epoch6, step123]: loss 0.424034
[epoch6, step124]: loss 0.539469
[epoch6, step125]: loss 0.869280
[epoch6, step126]: loss 0.630491
[epoch6, step127]: loss 0.430391
[epoch6, step128]: loss 0.601992
[epoch6, step129]: loss 0.581230
[epoch6, step130]: loss 0.805476
[epoch6, step131]: loss 0.661525
[epoch6, step132]: loss 0.633519
[epoch6, step133]: loss 0.704177
[epoch6, step134]: loss 0.502980
[epoch6, step135]: loss 0.428972
[epoch6, step136]: loss 0.569119
[epoch6, step137]: loss 0.838850
[epoch6, step138]: loss 0.915874
[epoch6, step139]: loss 0.698949
[epoch6, step140]: loss 0.679607
[epoch6, step141]: loss 0.725034
[epoch6, step142]: loss 0.805902
[epoch6, step143]: loss 0.571720
[epoch6, step144]: loss 0.563944
[epoch6, step145]: loss 0.459348
[epoch6, step146]: loss 0.554978
[epoch6, step147]: loss 0.631288
[epoch6, step148]: loss 0.577647
[epoch6, step149]: loss 0.898942
[epoch6, step150]: loss 0.424718
[epoch6, step151]: loss 0.640568
[epoch6, step152]: loss 0.489345
[epoch6, step153]: loss 0.795464
[epoch6, step154]: loss 0.640746
[epoch6, step155]: loss 0.530448
[epoch6, step156]: loss 1.046401
[epoch6, step157]: loss 0.536519
[epoch6, step158]: loss 0.618733
[epoch6, step159]: loss 0.664936
[epoch6, step160]: loss 0.739553
[epoch6, step161]: loss 0.670061
[epoch6, step162]: loss 0.684934
[epoch6, step163]: loss 0.407125
[epoch6, step164]: loss 0.844689
[epoch6, step165]: loss 0.697839
[epoch6, step166]: loss 0.785227
[epoch6, step167]: loss 0.799396
[epoch6, step168]: loss 0.681360
[epoch6, step169]: loss 0.793546
[epoch6, step170]: loss 0.906484
[epoch6, step171]: loss 0.673402
[epoch6, step172]: loss 0.764486
[epoch6, step173]: loss 0.753406
[epoch6, step174]: loss 0.634890
[epoch6, step175]: loss 0.544329
[epoch6, step176]: loss 0.849638
[epoch6, step177]: loss 0.579542
[epoch6, step178]: loss 0.871733
[epoch6, step179]: loss 0.229834
[epoch6, step180]: loss 0.382555
[epoch6, step181]: loss 0.454315
[epoch6, step182]: loss 0.570690
[epoch6, step183]: loss 0.705087
[epoch6, step184]: loss 0.943732
[epoch6, step185]: loss 0.562067
[epoch6, step186]: loss 0.741290
[epoch6, step187]: loss 0.827533
[epoch6, step188]: loss 0.526388
[epoch6, step189]: loss 0.714772
[epoch6, step190]: loss 0.707999
[epoch6, step191]: loss 0.737198
[epoch6, step192]: loss 0.710882
[epoch6, step193]: loss 0.861265
[epoch6, step194]: loss 0.760936
[epoch6, step195]: loss 0.611979
[epoch6, step196]: loss 0.846319
[epoch6, step197]: loss 0.698737
[epoch6, step198]: loss 0.771840
[epoch6, step199]: loss 0.708247
[epoch6, step200]: loss 0.642626
[epoch6, step201]: loss 0.566849
[epoch6, step202]: loss 0.557223
[epoch6, step203]: loss 0.536695
[epoch6, step204]: loss 0.670928
[epoch6, step205]: loss 0.414658
[epoch6, step206]: loss 0.394066
[epoch6, step207]: loss 0.419629
[epoch6, step208]: loss 0.830091
[epoch6, step209]: loss 0.818756
[epoch6, step210]: loss 0.451214
[epoch6, step211]: loss 0.678302
[epoch6, step212]: loss 0.731838
[epoch6, step213]: loss 0.843510
[epoch6, step214]: loss 0.741843
[epoch6, step215]: loss 0.739101
[epoch6, step216]: loss 0.958709
[epoch6, step217]: loss 0.648004
[epoch6, step218]: loss 0.709740
[epoch6, step219]: loss 0.389345
[epoch6, step220]: loss 0.945652
[epoch6, step221]: loss 0.692532
[epoch6, step222]: loss 0.741489
[epoch6, step223]: loss 0.862938
[epoch6, step224]: loss 0.559077
[epoch6, step225]: loss 0.835724
[epoch6, step226]: loss 0.856905
[epoch6, step227]: loss 0.454740
[epoch6, step228]: loss 0.733668
[epoch6, step229]: loss 0.540969
[epoch6, step230]: loss 0.532333
[epoch6, step231]: loss 0.671156
[epoch6, step232]: loss 0.684829
[epoch6, step233]: loss 0.315194
[epoch6, step234]: loss 0.776080
[epoch6, step235]: loss 0.607261
[epoch6, step236]: loss 0.568735
[epoch6, step237]: loss 0.717449
[epoch6, step238]: loss 0.705149
[epoch6, step239]: loss 0.762648
[epoch6, step240]: loss 0.674132
[epoch6, step241]: loss 0.610016
[epoch6, step242]: loss 0.656304
[epoch6, step243]: loss 0.356368
[epoch6, step244]: loss 0.668466
[epoch6, step245]: loss 0.474703
[epoch6, step246]: loss 0.803815
[epoch6, step247]: loss 0.684498
[epoch6, step248]: loss 0.834012
[epoch6, step249]: loss 0.797358
[epoch6, step250]: loss 0.601481
[epoch6, step251]: loss 0.937102
[epoch6, step252]: loss 0.869595
[epoch6, step253]: loss 0.881601
[epoch6, step254]: loss 0.612972
[epoch6, step255]: loss 0.683773
[epoch6, step256]: loss 0.559597
[epoch6, step257]: loss 0.843897
[epoch6, step258]: loss 0.750727
[epoch6, step259]: loss 0.833925
[epoch6, step260]: loss 0.528354
[epoch6, step261]: loss 0.821185
[epoch6, step262]: loss 0.702328
[epoch6, step263]: loss 0.775481
[epoch6, step264]: loss 0.630807
[epoch6, step265]: loss 0.484095
[epoch6, step266]: loss 0.610825
[epoch6, step267]: loss 0.374255
[epoch6, step268]: loss 0.753302
[epoch6, step269]: loss 0.686429
[epoch6, step270]: loss 0.535213
[epoch6, step271]: loss 0.694154
[epoch6, step272]: loss 0.827693
[epoch6, step273]: loss 0.499556
[epoch6, step274]: loss 0.703846
[epoch6, step275]: loss 1.049354
[epoch6, step276]: loss 0.572369
[epoch6, step277]: loss 0.420606
[epoch6, step278]: loss 0.648310
[epoch6, step279]: loss 0.586760
[epoch6, step280]: loss 0.819108
[epoch6, step281]: loss 0.786217
[epoch6, step282]: loss 0.778064
[epoch6, step283]: loss 0.669413
[epoch6, step284]: loss 0.904437
[epoch6, step285]: loss 0.435884
[epoch6, step286]: loss 0.473912
[epoch6, step287]: loss 0.982457
[epoch6, step288]: loss 0.545135
[epoch6, step289]: loss 0.816730
[epoch6, step290]: loss 0.564707
[epoch6, step291]: loss 0.456099
[epoch6, step292]: loss 0.634047
[epoch6, step293]: loss 0.438431
[epoch6, step294]: loss 0.746685
[epoch6, step295]: loss 0.693507
[epoch6, step296]: loss 0.765132
[epoch6, step297]: loss 0.640165
[epoch6, step298]: loss 0.675007
[epoch6, step299]: loss 0.718742
[epoch6, step300]: loss 0.541323
[epoch6, step301]: loss 0.870585
[epoch6, step302]: loss 0.497716
[epoch6, step303]: loss 0.679146
[epoch6, step304]: loss 0.689508
[epoch6, step305]: loss 0.609725
[epoch6, step306]: loss 0.781407
[epoch6, step307]: loss 0.704679
[epoch6, step308]: loss 0.851056
[epoch6, step309]: loss 0.430228
[epoch6, step310]: loss 0.906060
[epoch6, step311]: loss 0.701641
[epoch6, step312]: loss 0.823677
[epoch6, step313]: loss 0.736431
[epoch6, step314]: loss 0.787941
[epoch6, step315]: loss 0.676857
[epoch6, step316]: loss 0.788316
[epoch6, step317]: loss 0.495018
[epoch6, step318]: loss 0.592339
[epoch6, step319]: loss 0.713750
[epoch6, step320]: loss 0.720806
[epoch6, step321]: loss 0.797067
[epoch6, step322]: loss 0.861590
[epoch6, step323]: loss 0.498831
[epoch6, step324]: loss 0.662386
[epoch6, step325]: loss 0.629461
[epoch6, step326]: loss 0.786930
[epoch6, step327]: loss 0.461797
[epoch6, step328]: loss 0.873148
[epoch6, step329]: loss 0.540690
[epoch6, step330]: loss 0.677612
[epoch6, step331]: loss 0.539262
[epoch6, step332]: loss 0.577854
[epoch6, step333]: loss 0.799414
[epoch6, step334]: loss 0.853785
[epoch6, step335]: loss 0.780089
[epoch6, step336]: loss 0.851824
[epoch6, step337]: loss 0.772275
[epoch6, step338]: loss 0.562614
[epoch6, step339]: loss 0.717090
[epoch6, step340]: loss 0.639330
[epoch6, step341]: loss 0.408584
[epoch6, step342]: loss 0.379262
[epoch6, step343]: loss 0.708126
[epoch6, step344]: loss 0.804221
[epoch6, step345]: loss 0.783631
[epoch6, step346]: loss 0.831292
[epoch6, step347]: loss 0.785871
[epoch6, step348]: loss 0.631442
[epoch6, step349]: loss 0.748106
[epoch6, step350]: loss 0.551633
[epoch6, step351]: loss 0.454556
[epoch6, step352]: loss 0.773661
[epoch6, step353]: loss 0.719704
[epoch6, step354]: loss 0.652833
[epoch6, step355]: loss 0.631035
[epoch6, step356]: loss 0.612953
[epoch6, step357]: loss 1.104403
[epoch6, step358]: loss 0.444751
[epoch6, step359]: loss 0.661438
[epoch6, step360]: loss 0.561321
[epoch6, step361]: loss 0.573910
[epoch6, step362]: loss 0.477847
[epoch6, step363]: loss 0.814062
[epoch6, step364]: loss 0.671451
[epoch6, step365]: loss 0.719131
[epoch6, step366]: loss 0.744724
[epoch6, step367]: loss 0.632128
[epoch6, step368]: loss 0.772634
[epoch6, step369]: loss 0.697880
[epoch6, step370]: loss 0.810078
[epoch6, step371]: loss 0.601954
[epoch6, step372]: loss 0.790215
[epoch6, step373]: loss 0.734571
[epoch6, step374]: loss 0.664455
[epoch6, step375]: loss 0.669309
[epoch6, step376]: loss 0.601291
[epoch6, step377]: loss 0.541051
[epoch6, step378]: loss 0.603502
[epoch6, step379]: loss 0.781605
[epoch6, step380]: loss 0.711448
[epoch6, step381]: loss 0.748552
[epoch6, step382]: loss 0.446112
[epoch6, step383]: loss 0.882115
[epoch6, step384]: loss 0.967318
[epoch6, step385]: loss 0.673684
[epoch6, step386]: loss 1.072243
[epoch6, step387]: loss 0.447680
[epoch6, step388]: loss 0.414654
[epoch6, step389]: loss 0.583830
[epoch6, step390]: loss 0.738469
[epoch6, step391]: loss 0.523030
[epoch6, step392]: loss 0.725257
[epoch6, step393]: loss 0.715140
[epoch6, step394]: loss 0.905389
[epoch6, step395]: loss 0.887446
[epoch6, step396]: loss 0.702455
[epoch6, step397]: loss 0.279680
[epoch6, step398]: loss 0.731510
[epoch6, step399]: loss 0.682086
[epoch6, step400]: loss 0.443598
[epoch6, step401]: loss 0.631241
[epoch6, step402]: loss 0.954702
[epoch6, step403]: loss 0.772944
[epoch6, step404]: loss 0.654599
[epoch6, step405]: loss 0.530359
[epoch6, step406]: loss 0.954956
[epoch6, step407]: loss 0.534878
[epoch6, step408]: loss 0.413874
[epoch6, step409]: loss 0.874185
[epoch6, step410]: loss 0.688135
[epoch6, step411]: loss 0.755449
[epoch6, step412]: loss 0.631313
[epoch6, step413]: loss 0.781385
[epoch6, step414]: loss 0.986731
[epoch6, step415]: loss 0.480516
[epoch6, step416]: loss 0.738005
[epoch6, step417]: loss 0.566499
[epoch6, step418]: loss 0.478744
[epoch6, step419]: loss 0.721175
[epoch6, step420]: loss 0.829356
[epoch6, step421]: loss 0.532820
[epoch6, step422]: loss 0.280763
[epoch6, step423]: loss 0.643836
[epoch6, step424]: loss 0.573967
[epoch6, step425]: loss 0.839233
[epoch6, step426]: loss 0.757517
[epoch6, step427]: loss 0.756283
[epoch6, step428]: loss 0.906394
[epoch6, step429]: loss 0.630065
[epoch6, step430]: loss 0.885345
[epoch6, step431]: loss 0.885710
[epoch6, step432]: loss 0.573092
[epoch6, step433]: loss 0.823609
[epoch6, step434]: loss 0.709484
[epoch6, step435]: loss 0.656340
[epoch6, step436]: loss 0.641367
[epoch6, step437]: loss 0.663442
[epoch6, step438]: loss 1.043576
[epoch6, step439]: loss 0.632630
[epoch6, step440]: loss 0.497208
[epoch6, step441]: loss 0.875271
[epoch6, step442]: loss 0.708439
[epoch6, step443]: loss 0.748533
[epoch6, step444]: loss 0.845383
[epoch6, step445]: loss 0.779606
[epoch6, step446]: loss 0.732310
[epoch6, step447]: loss 0.650769
[epoch6, step448]: loss 0.822265
[epoch6, step449]: loss 0.498158
[epoch6, step450]: loss 1.063109
[epoch6, step451]: loss 0.855465
[epoch6, step452]: loss 0.759070
[epoch6, step453]: loss 0.865427
[epoch6, step454]: loss 0.734825
[epoch6, step455]: loss 0.858305
[epoch6, step456]: loss 0.339368
[epoch6, step457]: loss 0.693064
[epoch6, step458]: loss 0.457915
[epoch6, step459]: loss 0.769655
[epoch6, step460]: loss 0.717756
[epoch6, step461]: loss 0.738052
[epoch6, step462]: loss 0.792416
[epoch6, step463]: loss 0.542109
[epoch6, step464]: loss 0.611131
[epoch6, step465]: loss 0.745848
[epoch6, step466]: loss 0.652523
[epoch6, step467]: loss 0.884416
[epoch6, step468]: loss 0.543570
[epoch6, step469]: loss 0.461201
[epoch6, step470]: loss 0.282296
[epoch6, step471]: loss 0.621162
[epoch6, step472]: loss 0.958130
[epoch6, step473]: loss 0.882065
[epoch6, step474]: loss 0.714011
[epoch6, step475]: loss 0.741004
[epoch6, step476]: loss 0.940591
[epoch6, step477]: loss 0.944166
[epoch6, step478]: loss 0.705969
[epoch6, step479]: loss 0.559756
[epoch6, step480]: loss 0.621560
[epoch6, step481]: loss 0.737521
[epoch6, step482]: loss 0.394883
[epoch6, step483]: loss 0.820632
[epoch6, step484]: loss 0.722197
[epoch6, step485]: loss 0.517601
[epoch6, step486]: loss 0.842906
[epoch6, step487]: loss 0.437185
[epoch6, step488]: loss 0.609905
[epoch6, step489]: loss 0.572544
[epoch6, step490]: loss 0.580309
[epoch6, step491]: loss 0.574115
[epoch6, step492]: loss 0.736340
[epoch6, step493]: loss 0.680059
[epoch6, step494]: loss 0.555472
[epoch6, step495]: loss 1.027824
[epoch6, step496]: loss 0.799984
[epoch6, step497]: loss 0.443625
[epoch6, step498]: loss 0.828508
[epoch6, step499]: loss 0.409208
[epoch6, step500]: loss 0.795739
[epoch6, step501]: loss 0.931112
[epoch6, step502]: loss 0.839649
[epoch6, step503]: loss 0.573146
[epoch6, step504]: loss 0.729453
[epoch6, step505]: loss 0.720947
[epoch6, step506]: loss 0.610380
[epoch6, step507]: loss 0.608616
[epoch6, step508]: loss 0.480636
[epoch6, step509]: loss 0.734493
[epoch6, step510]: loss 0.696111
[epoch6, step511]: loss 0.412447
[epoch6, step512]: loss 0.464637
[epoch6, step513]: loss 0.776598
[epoch6, step514]: loss 0.586335
[epoch6, step515]: loss 0.829689
[epoch6, step516]: loss 0.883421
[epoch6, step517]: loss 1.017771
[epoch6, step518]: loss 0.647971
[epoch6, step519]: loss 0.934219
[epoch6, step520]: loss 0.686455
[epoch6, step521]: loss 0.542907
[epoch6, step522]: loss 0.563102
[epoch6, step523]: loss 0.398730
[epoch6, step524]: loss 0.590928
[epoch6, step525]: loss 0.758742
[epoch6, step526]: loss 0.623174
[epoch6, step527]: loss 0.659214
[epoch6, step528]: loss 0.808977
[epoch6, step529]: loss 0.827594
[epoch6, step530]: loss 0.750091
[epoch6, step531]: loss 0.728109
[epoch6, step532]: loss 0.360716
[epoch6, step533]: loss 0.358754
[epoch6, step534]: loss 0.746328
[epoch6, step535]: loss 0.784535
[epoch6, step536]: loss 0.790408
[epoch6, step537]: loss 0.762281
[epoch6, step538]: loss 0.583618
[epoch6, step539]: loss 0.781300
[epoch6, step540]: loss 0.889141
[epoch6, step541]: loss 0.657047
[epoch6, step542]: loss 0.655345
[epoch6, step543]: loss 0.763013
[epoch6, step544]: loss 0.555987
[epoch6, step545]: loss 0.681046
[epoch6, step546]: loss 0.716312
[epoch6, step547]: loss 0.800349
[epoch6, step548]: loss 0.657784
[epoch6, step549]: loss 0.662655
[epoch6, step550]: loss 0.607654
[epoch6, step551]: loss 0.760290
[epoch6, step552]: loss 0.672834
[epoch6, step553]: loss 0.669559
[epoch6, step554]: loss 0.865661
[epoch6, step555]: loss 0.755223
[epoch6, step556]: loss 0.797427
[epoch6, step557]: loss 0.496292
[epoch6, step558]: loss 0.713834
[epoch6, step559]: loss 0.412606
[epoch6, step560]: loss 0.759207
[epoch6, step561]: loss 0.620187
[epoch6, step562]: loss 0.644757
[epoch6, step563]: loss 0.821697
[epoch6, step564]: loss 0.731723
[epoch6, step565]: loss 0.551974
[epoch6, step566]: loss 0.727342
[epoch6, step567]: loss 0.508874
[epoch6, step568]: loss 0.848118
[epoch6, step569]: loss 0.753841
[epoch6, step570]: loss 0.535435
[epoch6, step571]: loss 0.565074
[epoch6, step572]: loss 0.868626
[epoch6, step573]: loss 0.837567
[epoch6, step574]: loss 0.515319
[epoch6, step575]: loss 0.529214
[epoch6, step576]: loss 0.835559
[epoch6, step577]: loss 0.613651
[epoch6, step578]: loss 0.575063
[epoch6, step579]: loss 0.521100
[epoch6, step580]: loss 0.884911
[epoch6, step581]: loss 0.864782
[epoch6, step582]: loss 0.758005
[epoch6, step583]: loss 0.990630
[epoch6, step584]: loss 0.908059
[epoch6, step585]: loss 0.614074
[epoch6, step586]: loss 0.637941
[epoch6, step587]: loss 0.797345
[epoch6, step588]: loss 0.623623
[epoch6, step589]: loss 0.781672
[epoch6, step590]: loss 0.469977
[epoch6, step591]: loss 0.654759
[epoch6, step592]: loss 0.330369
[epoch6, step593]: loss 0.729821
[epoch6, step594]: loss 0.610665
[epoch6, step595]: loss 0.730750
[epoch6, step596]: loss 0.643234
[epoch6, step597]: loss 0.815692
[epoch6, step598]: loss 0.644946
[epoch6, step599]: loss 0.618876
[epoch6, step600]: loss 0.658038
[epoch6, step601]: loss 0.733685
[epoch6, step602]: loss 0.733867
[epoch6, step603]: loss 0.912524
[epoch6, step604]: loss 0.664294
[epoch6, step605]: loss 0.992513
[epoch6, step606]: loss 0.522517
[epoch6, step607]: loss 0.573855
[epoch6, step608]: loss 0.516633
[epoch6, step609]: loss 0.784315
[epoch6, step610]: loss 0.760057
[epoch6, step611]: loss 0.655297
[epoch6, step612]: loss 0.395490
[epoch6, step613]: loss 0.757217
[epoch6, step614]: loss 0.511215
[epoch6, step615]: loss 0.661927
[epoch6, step616]: loss 0.541184
[epoch6, step617]: loss 0.857592
[epoch6, step618]: loss 0.654132
[epoch6, step619]: loss 0.674243
[epoch6, step620]: loss 0.791797
[epoch6, step621]: loss 0.843631
[epoch6, step622]: loss 0.830595
[epoch6, step623]: loss 0.670866
[epoch6, step624]: loss 0.724736
[epoch6, step625]: loss 0.650388
[epoch6, step626]: loss 0.514682
[epoch6, step627]: loss 0.594872
[epoch6, step628]: loss 0.621235
[epoch6, step629]: loss 0.614210
[epoch6, step630]: loss 0.647150
[epoch6, step631]: loss 0.870611
[epoch6, step632]: loss 0.510621
[epoch6, step633]: loss 0.866195
[epoch6, step634]: loss 0.645394
[epoch6, step635]: loss 0.712220
[epoch6, step636]: loss 0.549356
[epoch6, step637]: loss 0.857474
[epoch6, step638]: loss 0.511879
[epoch6, step639]: loss 0.546237
[epoch6, step640]: loss 0.495849
[epoch6, step641]: loss 0.466618
[epoch6, step642]: loss 0.865566
[epoch6, step643]: loss 0.480634
[epoch6, step644]: loss 0.709727
[epoch6, step645]: loss 0.708816
[epoch6, step646]: loss 0.960616
[epoch6, step647]: loss 0.652262
[epoch6, step648]: loss 0.607987
[epoch6, step649]: loss 0.504131
[epoch6, step650]: loss 0.617872
[epoch6, step651]: loss 0.509405
[epoch6, step652]: loss 0.693559
[epoch6, step653]: loss 0.742776
[epoch6, step654]: loss 0.685921
[epoch6, step655]: loss 0.746198
[epoch6, step656]: loss 0.668551
[epoch6, step657]: loss 0.480190
[epoch6, step658]: loss 0.567896
[epoch6, step659]: loss 0.614701
[epoch6, step660]: loss 0.552223
[epoch6, step661]: loss 0.417570
[epoch6, step662]: loss 0.909253
[epoch6, step663]: loss 0.584778
[epoch6, step664]: loss 0.655385
[epoch6, step665]: loss 0.500394
[epoch6, step666]: loss 0.537933
[epoch6, step667]: loss 0.479757
[epoch6, step668]: loss 0.799065
[epoch6, step669]: loss 0.681991
[epoch6, step670]: loss 0.595341
[epoch6, step671]: loss 0.654034
[epoch6, step672]: loss 0.522731
[epoch6, step673]: loss 0.850032
[epoch6, step674]: loss 0.763872
[epoch6, step675]: loss 0.863243
[epoch6, step676]: loss 0.860644
[epoch6, step677]: loss 0.751752
[epoch6, step678]: loss 0.796023
[epoch6, step679]: loss 0.183058
[epoch6, step680]: loss 0.892625
[epoch6, step681]: loss 0.868011
[epoch6, step682]: loss 0.736001
[epoch6, step683]: loss 0.638321
[epoch6, step684]: loss 0.837776
[epoch6, step685]: loss 0.900585
[epoch6, step686]: loss 0.538003
[epoch6, step687]: loss 0.697059
[epoch6, step688]: loss 0.650523
[epoch6, step689]: loss 0.487567
[epoch6, step690]: loss 0.388831
[epoch6, step691]: loss 0.588252
[epoch6, step692]: loss 0.740423
[epoch6, step693]: loss 0.534634
[epoch6, step694]: loss 0.765839
[epoch6, step695]: loss 0.738317
[epoch6, step696]: loss 0.495893
[epoch6, step697]: loss 0.607264
[epoch6, step698]: loss 0.618959
[epoch6, step699]: loss 0.793239
[epoch6, step700]: loss 0.776565
[epoch6, step701]: loss 0.840765
[epoch6, step702]: loss 0.626712
[epoch6, step703]: loss 0.672844
[epoch6, step704]: loss 0.545531
[epoch6, step705]: loss 0.832349
[epoch6, step706]: loss 0.664830
[epoch6, step707]: loss 0.762905
[epoch6, step708]: loss 0.811575
[epoch6, step709]: loss 0.824077
[epoch6, step710]: loss 0.766056
[epoch6, step711]: loss 0.869535
[epoch6, step712]: loss 0.863620
[epoch6, step713]: loss 0.739205
[epoch6, step714]: loss 0.801177
[epoch6, step715]: loss 0.739833
[epoch6, step716]: loss 0.932480
[epoch6, step717]: loss 0.715001
[epoch6, step718]: loss 0.800271
[epoch6, step719]: loss 0.780441
[epoch6, step720]: loss 0.509384
[epoch6, step721]: loss 0.694668
[epoch6, step722]: loss 0.665223
[epoch6, step723]: loss 0.574199
[epoch6, step724]: loss 0.610757
[epoch6, step725]: loss 0.755322
[epoch6, step726]: loss 0.896482
[epoch6, step727]: loss 0.629197
[epoch6, step728]: loss 0.696886
[epoch6, step729]: loss 0.654927
[epoch6, step730]: loss 0.830633
[epoch6, step731]: loss 0.906627
[epoch6, step732]: loss 0.599876
[epoch6, step733]: loss 0.508055
[epoch6, step734]: loss 0.761220
[epoch6, step735]: loss 0.794500
[epoch6, step736]: loss 0.713653
[epoch6, step737]: loss 0.821145
[epoch6, step738]: loss 0.647865
[epoch6, step739]: loss 0.682523
[epoch6, step740]: loss 0.834439
[epoch6, step741]: loss 0.646516
[epoch6, step742]: loss 0.621127
[epoch6, step743]: loss 0.653559
[epoch6, step744]: loss 0.319956
[epoch6, step745]: loss 0.677755
[epoch6, step746]: loss 0.536759
[epoch6, step747]: loss 0.911957
[epoch6, step748]: loss 0.896870
[epoch6, step749]: loss 0.663958
[epoch6, step750]: loss 0.369157
[epoch6, step751]: loss 0.820321
[epoch6, step752]: loss 0.540679
[epoch6, step753]: loss 0.926930
[epoch6, step754]: loss 0.834660
[epoch6, step755]: loss 0.814586
[epoch6, step756]: loss 0.505129
[epoch6, step757]: loss 0.616799
[epoch6, step758]: loss 0.593494
[epoch6, step759]: loss 0.324505
[epoch6, step760]: loss 0.758027
[epoch6, step761]: loss 0.691257
[epoch6, step762]: loss 0.602340
[epoch6, step763]: loss 0.823383
[epoch6, step764]: loss 0.594722
[epoch6, step765]: loss 0.738634
[epoch6, step766]: loss 0.552230
[epoch6, step767]: loss 0.827944
[epoch6, step768]: loss 0.814942
[epoch6, step769]: loss 0.248905
[epoch6, step770]: loss 0.474573
[epoch6, step771]: loss 0.803878
[epoch6, step772]: loss 0.568688
[epoch6, step773]: loss 0.482138
[epoch6, step774]: loss 0.656882
[epoch6, step775]: loss 0.538246
[epoch6, step776]: loss 0.516930
[epoch6, step777]: loss 0.557560
[epoch6, step778]: loss 0.425986
[epoch6, step779]: loss 0.673614
[epoch6, step780]: loss 0.849182
[epoch6, step781]: loss 0.582748
[epoch6, step782]: loss 1.023427
[epoch6, step783]: loss 0.673541
[epoch6, step784]: loss 0.635882
[epoch6, step785]: loss 0.680254
[epoch6, step786]: loss 0.589197
[epoch6, step787]: loss 0.790144
[epoch6, step788]: loss 0.806710
[epoch6, step789]: loss 0.754754
[epoch6, step790]: loss 0.781893
[epoch6, step791]: loss 0.898737
[epoch6, step792]: loss 0.764916
[epoch6, step793]: loss 0.717455
[epoch6, step794]: loss 0.777989
[epoch6, step795]: loss 0.928273
[epoch6, step796]: loss 0.576330
[epoch6, step797]: loss 0.730247
[epoch6, step798]: loss 0.565090
[epoch6, step799]: loss 0.569306
[epoch6, step800]: loss 0.672797
[epoch6, step801]: loss 0.861719
[epoch6, step802]: loss 0.662285
[epoch6, step803]: loss 0.739367
[epoch6, step804]: loss 0.834197
[epoch6, step805]: loss 0.789061
[epoch6, step806]: loss 0.651477
[epoch6, step807]: loss 0.626527
[epoch6, step808]: loss 0.634052
[epoch6, step809]: loss 0.603583
[epoch6, step810]: loss 0.549030
[epoch6, step811]: loss 0.380426
[epoch6, step812]: loss 0.714443
[epoch6, step813]: loss 0.510157
[epoch6, step814]: loss 0.324935
[epoch6, step815]: loss 0.366767
[epoch6, step816]: loss 0.740252
[epoch6, step817]: loss 0.719837
[epoch6, step818]: loss 0.698109
[epoch6, step819]: loss 0.724971
[epoch6, step820]: loss 0.600169
[epoch6, step821]: loss 0.624571
[epoch6, step822]: loss 0.746683
[epoch6, step823]: loss 0.570928
[epoch6, step824]: loss 0.726450
[epoch6, step825]: loss 0.659936
[epoch6, step826]: loss 0.417617
[epoch6, step827]: loss 0.679164
[epoch6, step828]: loss 0.675116
[epoch6, step829]: loss 0.614348
[epoch6, step830]: loss 0.587417
[epoch6, step831]: loss 0.962568
[epoch6, step832]: loss 0.763905
[epoch6, step833]: loss 0.853182
[epoch6, step834]: loss 0.847114
[epoch6, step835]: loss 0.704816
[epoch6, step836]: loss 0.786284
[epoch6, step837]: loss 0.604358
[epoch6, step838]: loss 0.744313
[epoch6, step839]: loss 0.568090
[epoch6, step840]: loss 0.795692
[epoch6, step841]: loss 0.635161
[epoch6, step842]: loss 0.747931
[epoch6, step843]: loss 0.661204
[epoch6, step844]: loss 0.532596
[epoch6, step845]: loss 0.852140
[epoch6, step846]: loss 0.900113
[epoch6, step847]: loss 0.583770
[epoch6, step848]: loss 1.030938
[epoch6, step849]: loss 0.652048
[epoch6, step850]: loss 0.849199
[epoch6, step851]: loss 0.353892
[epoch6, step852]: loss 0.823660
[epoch6, step853]: loss 0.198192
[epoch6, step854]: loss 0.688786
[epoch6, step855]: loss 0.839865
[epoch6, step856]: loss 0.680674
[epoch6, step857]: loss 0.536277
[epoch6, step858]: loss 0.724919
[epoch6, step859]: loss 0.781894
[epoch6, step860]: loss 0.866353
[epoch6, step861]: loss 0.443189
[epoch6, step862]: loss 0.572103
[epoch6, step863]: loss 0.842239
[epoch6, step864]: loss 0.691593
[epoch6, step865]: loss 0.682972
[epoch6, step866]: loss 0.552093
[epoch6, step867]: loss 0.694675
[epoch6, step868]: loss 0.625614
[epoch6, step869]: loss 0.635356
[epoch6, step870]: loss 0.349643
[epoch6, step871]: loss 0.562955
[epoch6, step872]: loss 0.806029
[epoch6, step873]: loss 0.734620
[epoch6, step874]: loss 0.605773
[epoch6, step875]: loss 0.509784
[epoch6, step876]: loss 0.805495
[epoch6, step877]: loss 0.661485
[epoch6, step878]: loss 0.756894
[epoch6, step879]: loss 0.427158
[epoch6, step880]: loss 0.648328
[epoch6, step881]: loss 0.703043
[epoch6, step882]: loss 0.532378
[epoch6, step883]: loss 0.763063
[epoch6, step884]: loss 0.732338
[epoch6, step885]: loss 0.442528
[epoch6, step886]: loss 0.659026
[epoch6, step887]: loss 0.734726
[epoch6, step888]: loss 0.434266
[epoch6, step889]: loss 0.641810
[epoch6, step890]: loss 0.882060
[epoch6, step891]: loss 0.275351
[epoch6, step892]: loss 0.895753
[epoch6, step893]: loss 0.549641
[epoch6, step894]: loss 0.464464
[epoch6, step895]: loss 0.889992
[epoch6, step896]: loss 0.745877
[epoch6, step897]: loss 0.883240
[epoch6, step898]: loss 0.719866
[epoch6, step899]: loss 0.469958
[epoch6, step900]: loss 0.437743
[epoch6, step901]: loss 0.909921
[epoch6, step902]: loss 0.785599
[epoch6, step903]: loss 0.673715
[epoch6, step904]: loss 0.732473
[epoch6, step905]: loss 0.506802
[epoch6, step906]: loss 0.577005
[epoch6, step907]: loss 0.792006
[epoch6, step908]: loss 0.886571
[epoch6, step909]: loss 0.786560
[epoch6, step910]: loss 0.514132
[epoch6, step911]: loss 0.830206
[epoch6, step912]: loss 0.801557
[epoch6, step913]: loss 0.681048
[epoch6, step914]: loss 0.547223
[epoch6, step915]: loss 0.804821
[epoch6, step916]: loss 0.727999
[epoch6, step917]: loss 0.856813
[epoch6, step918]: loss 0.622056
[epoch6, step919]: loss 0.770846
[epoch6, step920]: loss 0.638561
[epoch6, step921]: loss 0.677360
[epoch6, step922]: loss 0.449431
[epoch6, step923]: loss 0.541373
[epoch6, step924]: loss 0.573917
[epoch6, step925]: loss 0.894858
[epoch6, step926]: loss 0.651981
[epoch6, step927]: loss 0.712564
[epoch6, step928]: loss 0.544967
[epoch6, step929]: loss 0.588328
[epoch6, step930]: loss 0.626213
[epoch6, step931]: loss 0.756204
[epoch6, step932]: loss 0.786202
[epoch6, step933]: loss 0.674583
[epoch6, step934]: loss 0.464257
[epoch6, step935]: loss 0.864488
[epoch6, step936]: loss 0.687560
[epoch6, step937]: loss 0.607960
[epoch6, step938]: loss 0.749595
[epoch6, step939]: loss 0.714029
[epoch6, step940]: loss 0.562607
[epoch6, step941]: loss 0.621628
[epoch6, step942]: loss 0.837986
[epoch6, step943]: loss 0.683080
[epoch6, step944]: loss 0.749116
[epoch6, step945]: loss 0.575537
[epoch6, step946]: loss 0.858147
[epoch6, step947]: loss 0.685971
[epoch6, step948]: loss 0.661299
[epoch6, step949]: loss 0.401235
[epoch6, step950]: loss 0.701814
[epoch6, step951]: loss 0.542464
[epoch6, step952]: loss 0.610129
[epoch6, step953]: loss 0.830471
[epoch6, step954]: loss 0.679611
[epoch6, step955]: loss 0.795553
[epoch6, step956]: loss 0.824250
[epoch6, step957]: loss 0.664949
[epoch6, step958]: loss 0.850242
[epoch6, step959]: loss 0.711626
[epoch6, step960]: loss 0.713921
[epoch6, step961]: loss 0.527805
[epoch6, step962]: loss 0.611962
[epoch6, step963]: loss 0.826056
[epoch6, step964]: loss 0.697231
[epoch6, step965]: loss 0.850670
[epoch6, step966]: loss 0.842414
[epoch6, step967]: loss 0.587362
[epoch6, step968]: loss 0.741621
[epoch6, step969]: loss 0.709764
[epoch6, step970]: loss 0.603738
[epoch6, step971]: loss 0.703422
[epoch6, step972]: loss 0.346907
[epoch6, step973]: loss 0.698140
[epoch6, step974]: loss 0.713435
[epoch6, step975]: loss 0.399500
[epoch6, step976]: loss 0.691804
[epoch6, step977]: loss 0.623466
[epoch6, step978]: loss 0.479487
[epoch6, step979]: loss 0.691933
[epoch6, step980]: loss 0.542122
[epoch6, step981]: loss 0.657832
[epoch6, step982]: loss 0.880072
[epoch6, step983]: loss 0.760369
[epoch6, step984]: loss 0.577334
[epoch6, step985]: loss 0.734574
[epoch6, step986]: loss 0.764323
[epoch6, step987]: loss 0.442182
[epoch6, step988]: loss 0.624010
[epoch6, step989]: loss 0.760140
[epoch6, step990]: loss 0.419922
[epoch6, step991]: loss 0.877269
[epoch6, step992]: loss 0.946500
[epoch6, step993]: loss 0.774307
[epoch6, step994]: loss 0.962018
[epoch6, step995]: loss 0.441238
[epoch6, step996]: loss 0.581969
[epoch6, step997]: loss 0.193503
[epoch6, step998]: loss 0.775050
[epoch6, step999]: loss 0.803405
[epoch6, step1000]: loss 0.385213
[epoch6, step1001]: loss 0.501001
[epoch6, step1002]: loss 0.671182
[epoch6, step1003]: loss 0.240870
[epoch6, step1004]: loss 0.775523
[epoch6, step1005]: loss 0.744284
[epoch6, step1006]: loss 0.550196
[epoch6, step1007]: loss 0.528109
[epoch6, step1008]: loss 0.709065
[epoch6, step1009]: loss 0.540547
[epoch6, step1010]: loss 0.550476
[epoch6, step1011]: loss 0.686894
[epoch6, step1012]: loss 0.618719
[epoch6, step1013]: loss 0.715711
[epoch6, step1014]: loss 0.527689
[epoch6, step1015]: loss 0.665391
[epoch6, step1016]: loss 0.489009
[epoch6, step1017]: loss 0.473662
[epoch6, step1018]: loss 0.579267
[epoch6, step1019]: loss 0.316419
[epoch6, step1020]: loss 0.599412
[epoch6, step1021]: loss 0.566804
[epoch6, step1022]: loss 0.734945
[epoch6, step1023]: loss 0.628101
[epoch6, step1024]: loss 0.570074
[epoch6, step1025]: loss 0.763694
[epoch6, step1026]: loss 0.961477
[epoch6, step1027]: loss 0.686481
[epoch6, step1028]: loss 0.751433
[epoch6, step1029]: loss 0.739668
[epoch6, step1030]: loss 0.867282
[epoch6, step1031]: loss 0.616180
[epoch6, step1032]: loss 0.638462
[epoch6, step1033]: loss 0.676533
[epoch6, step1034]: loss 0.597962
[epoch6, step1035]: loss 0.827675
[epoch6, step1036]: loss 0.653768
[epoch6, step1037]: loss 0.730776
[epoch6, step1038]: loss 0.526769
[epoch6, step1039]: loss 0.685954
[epoch6, step1040]: loss 0.417658
[epoch6, step1041]: loss 0.732247
[epoch6, step1042]: loss 0.489090
[epoch6, step1043]: loss 0.544989
[epoch6, step1044]: loss 0.611662
[epoch6, step1045]: loss 0.801517
[epoch6, step1046]: loss 0.705926
[epoch6, step1047]: loss 0.745878
[epoch6, step1048]: loss 0.682272
[epoch6, step1049]: loss 0.733018
[epoch6, step1050]: loss 0.754117
[epoch6, step1051]: loss 0.603620
[epoch6, step1052]: loss 0.749819
[epoch6, step1053]: loss 0.523985
[epoch6, step1054]: loss 0.704282
[epoch6, step1055]: loss 0.514663
[epoch6, step1056]: loss 0.586560
[epoch6, step1057]: loss 0.567775
[epoch6, step1058]: loss 0.527968
[epoch6, step1059]: loss 0.433210
[epoch6, step1060]: loss 0.682324
[epoch6, step1061]: loss 0.610352
[epoch6, step1062]: loss 0.876025
[epoch6, step1063]: loss 0.365325
[epoch6, step1064]: loss 0.705619
[epoch6, step1065]: loss 0.568410
[epoch6, step1066]: loss 1.030714
[epoch6, step1067]: loss 0.401055
[epoch6, step1068]: loss 0.855215
[epoch6, step1069]: loss 0.390802
[epoch6, step1070]: loss 0.808024
[epoch6, step1071]: loss 0.770515
[epoch6, step1072]: loss 0.657395
[epoch6, step1073]: loss 0.748147
[epoch6, step1074]: loss 0.633389
[epoch6, step1075]: loss 0.638451
[epoch6, step1076]: loss 0.839821
[epoch6, step1077]: loss 0.597978
[epoch6, step1078]: loss 0.759393
[epoch6, step1079]: loss 0.674166
[epoch6, step1080]: loss 0.667696
[epoch6, step1081]: loss 0.534051
[epoch6, step1082]: loss 0.650592
[epoch6, step1083]: loss 0.485632
[epoch6, step1084]: loss 0.910979
[epoch6, step1085]: loss 0.523502
[epoch6, step1086]: loss 0.610807
[epoch6, step1087]: loss 0.554699
[epoch6, step1088]: loss 0.813824
[epoch6, step1089]: loss 0.680630
[epoch6, step1090]: loss 0.538972
[epoch6, step1091]: loss 0.674708
[epoch6, step1092]: loss 0.311748
[epoch6, step1093]: loss 0.966871
[epoch6, step1094]: loss 0.577079
[epoch6, step1095]: loss 0.385853
[epoch6, step1096]: loss 0.503287
[epoch6, step1097]: loss 0.930446
[epoch6, step1098]: loss 0.603203
[epoch6, step1099]: loss 0.794768
[epoch6, step1100]: loss 0.797330
[epoch6, step1101]: loss 0.725688
[epoch6, step1102]: loss 0.431654
[epoch6, step1103]: loss 0.675167
[epoch6, step1104]: loss 0.733164
[epoch6, step1105]: loss 0.948910
[epoch6, step1106]: loss 0.468662
[epoch6, step1107]: loss 0.890971
[epoch6, step1108]: loss 0.358638
[epoch6, step1109]: loss 0.789456
[epoch6, step1110]: loss 0.715814
[epoch6, step1111]: loss 0.689709
[epoch6, step1112]: loss 0.657465
[epoch6, step1113]: loss 0.499287
[epoch6, step1114]: loss 0.536554
[epoch6, step1115]: loss 0.527365
[epoch6, step1116]: loss 0.587913
[epoch6, step1117]: loss 0.784190
[epoch6, step1118]: loss 0.787566
[epoch6, step1119]: loss 0.803007
[epoch6, step1120]: loss 0.537390
[epoch6, step1121]: loss 0.443878
[epoch6, step1122]: loss 0.484641
[epoch6, step1123]: loss 0.218119
[epoch6, step1124]: loss 0.611453
[epoch6, step1125]: loss 0.761583
[epoch6, step1126]: loss 0.528610
[epoch6, step1127]: loss 0.904553
[epoch6, step1128]: loss 0.637952
[epoch6, step1129]: loss 0.413964
[epoch6, step1130]: loss 0.368519
[epoch6, step1131]: loss 0.588104
[epoch6, step1132]: loss 0.895111
[epoch6, step1133]: loss 0.932242
[epoch6, step1134]: loss 0.384178
[epoch6, step1135]: loss 0.492955
[epoch6, step1136]: loss 0.873862
[epoch6, step1137]: loss 0.892913
[epoch6, step1138]: loss 0.877292
[epoch6, step1139]: loss 0.655750
[epoch6, step1140]: loss 0.611746
[epoch6, step1141]: loss 0.581609
[epoch6, step1142]: loss 0.659588
[epoch6, step1143]: loss 0.828364
[epoch6, step1144]: loss 0.870094
[epoch6, step1145]: loss 0.744464
[epoch6, step1146]: loss 0.943868
[epoch6, step1147]: loss 0.440996
[epoch6, step1148]: loss 0.512835
[epoch6, step1149]: loss 0.785089
[epoch6, step1150]: loss 0.775650
[epoch6, step1151]: loss 0.621735
[epoch6, step1152]: loss 0.840222
[epoch6, step1153]: loss 0.782902
[epoch6, step1154]: loss 0.813268
[epoch6, step1155]: loss 0.303930
[epoch6, step1156]: loss 0.638930
[epoch6, step1157]: loss 0.430309
[epoch6, step1158]: loss 0.385418
[epoch6, step1159]: loss 0.622318
[epoch6, step1160]: loss 0.450870
[epoch6, step1161]: loss 0.484481
[epoch6, step1162]: loss 0.825785
[epoch6, step1163]: loss 0.737569
[epoch6, step1164]: loss 0.321459
[epoch6, step1165]: loss 0.748437
[epoch6, step1166]: loss 0.631392
[epoch6, step1167]: loss 0.729307
[epoch6, step1168]: loss 0.665055
[epoch6, step1169]: loss 0.814903
[epoch6, step1170]: loss 0.564271
[epoch6, step1171]: loss 0.470230
[epoch6, step1172]: loss 0.666148
[epoch6, step1173]: loss 0.571951
[epoch6, step1174]: loss 0.801691
[epoch6, step1175]: loss 0.662248
[epoch6, step1176]: loss 0.689863
[epoch6, step1177]: loss 0.641100
[epoch6, step1178]: loss 0.647703
[epoch6, step1179]: loss 0.576652
[epoch6, step1180]: loss 0.920903
[epoch6, step1181]: loss 0.862523
[epoch6, step1182]: loss 0.777985
[epoch6, step1183]: loss 0.644750
[epoch6, step1184]: loss 0.592729
[epoch6, step1185]: loss 0.451253
[epoch6, step1186]: loss 0.691421
[epoch6, step1187]: loss 0.433204
[epoch6, step1188]: loss 0.563219
[epoch6, step1189]: loss 0.499575
[epoch6, step1190]: loss 0.629408
[epoch6, step1191]: loss 0.743187
[epoch6, step1192]: loss 0.761798
[epoch6, step1193]: loss 0.240124
[epoch6, step1194]: loss 0.734311
[epoch6, step1195]: loss 0.674839
[epoch6, step1196]: loss 0.616752
[epoch6, step1197]: loss 0.662433
[epoch6, step1198]: loss 0.795992
[epoch6, step1199]: loss 0.643484
[epoch6, step1200]: loss 0.744523
[epoch6, step1201]: loss 0.447198
[epoch6, step1202]: loss 0.666859
[epoch6, step1203]: loss 0.760181
[epoch6, step1204]: loss 0.629748
[epoch6, step1205]: loss 0.290190
[epoch6, step1206]: loss 0.431236
[epoch6, step1207]: loss 0.557174
[epoch6, step1208]: loss 0.965850
[epoch6, step1209]: loss 0.639913
[epoch6, step1210]: loss 0.510278
[epoch6, step1211]: loss 0.881925
[epoch6, step1212]: loss 0.555594
[epoch6, step1213]: loss 0.666853
[epoch6, step1214]: loss 0.534600
[epoch6, step1215]: loss 0.433545
[epoch6, step1216]: loss 0.506339
[epoch6, step1217]: loss 0.523305
[epoch6, step1218]: loss 0.690661
[epoch6, step1219]: loss 0.620303
[epoch6, step1220]: loss 0.806726
[epoch6, step1221]: loss 0.767596
[epoch6, step1222]: loss 0.736087
[epoch6, step1223]: loss 0.548152
[epoch6, step1224]: loss 0.604290
[epoch6, step1225]: loss 0.666631
[epoch6, step1226]: loss 0.688902
[epoch6, step1227]: loss 0.753182
[epoch6, step1228]: loss 0.327278
[epoch6, step1229]: loss 0.542491
[epoch6, step1230]: loss 0.943738
[epoch6, step1231]: loss 0.699784
[epoch6, step1232]: loss 0.654867
[epoch6, step1233]: loss 0.835816
[epoch6, step1234]: loss 1.143623
[epoch6, step1235]: loss 0.513872
[epoch6, step1236]: loss 0.826712
[epoch6, step1237]: loss 0.888908
[epoch6, step1238]: loss 0.780551
[epoch6, step1239]: loss 0.472838
[epoch6, step1240]: loss 0.592475
[epoch6, step1241]: loss 0.707021
[epoch6, step1242]: loss 0.742897
[epoch6, step1243]: loss 0.517117
[epoch6, step1244]: loss 0.641374
[epoch6, step1245]: loss 0.818037
[epoch6, step1246]: loss 0.665766
[epoch6, step1247]: loss 0.856050
[epoch6, step1248]: loss 0.775184
[epoch6, step1249]: loss 0.479302
[epoch6, step1250]: loss 0.529955
[epoch6, step1251]: loss 0.377750
[epoch6, step1252]: loss 0.377854
[epoch6, step1253]: loss 0.637034
[epoch6, step1254]: loss 0.704327
[epoch6, step1255]: loss 0.590581
[epoch6, step1256]: loss 0.798005
[epoch6, step1257]: loss 0.609769
[epoch6, step1258]: loss 0.591740
[epoch6, step1259]: loss 0.452414
[epoch6, step1260]: loss 0.611513
[epoch6, step1261]: loss 0.739602
[epoch6, step1262]: loss 0.359998
[epoch6, step1263]: loss 0.354008
[epoch6, step1264]: loss 0.909916
[epoch6, step1265]: loss 0.693206
[epoch6, step1266]: loss 0.604220
[epoch6, step1267]: loss 0.670831
[epoch6, step1268]: loss 0.771932
[epoch6, step1269]: loss 0.733768
[epoch6, step1270]: loss 0.800460
[epoch6, step1271]: loss 0.705995
[epoch6, step1272]: loss 0.669844
[epoch6, step1273]: loss 0.557985
[epoch6, step1274]: loss 0.699413
[epoch6, step1275]: loss 0.878880
[epoch6, step1276]: loss 0.312643
[epoch6, step1277]: loss 0.155917
[epoch6, step1278]: loss 0.569767
[epoch6, step1279]: loss 0.696616
[epoch6, step1280]: loss 0.903006
[epoch6, step1281]: loss 0.600350
[epoch6, step1282]: loss 0.882259
[epoch6, step1283]: loss 0.453064
[epoch6, step1284]: loss 0.915941
[epoch6, step1285]: loss 0.684611
[epoch6, step1286]: loss 0.539979
[epoch6, step1287]: loss 0.378420
[epoch6, step1288]: loss 0.701331
[epoch6, step1289]: loss 0.575433
[epoch6, step1290]: loss 0.500198
[epoch6, step1291]: loss 0.476373
[epoch6, step1292]: loss 0.415810
[epoch6, step1293]: loss 0.628732
[epoch6, step1294]: loss 0.750762
[epoch6, step1295]: loss 0.535695
[epoch6, step1296]: loss 0.561278
[epoch6, step1297]: loss 0.810751
[epoch6, step1298]: loss 0.473273
[epoch6, step1299]: loss 0.835912
[epoch6, step1300]: loss 0.857522
[epoch6, step1301]: loss 0.508872
[epoch6, step1302]: loss 0.767880
[epoch6, step1303]: loss 0.806953
[epoch6, step1304]: loss 0.772194
[epoch6, step1305]: loss 0.679738
[epoch6, step1306]: loss 0.732371
[epoch6, step1307]: loss 0.557471
[epoch6, step1308]: loss 0.813368
[epoch6, step1309]: loss 0.612821
[epoch6, step1310]: loss 0.572158
[epoch6, step1311]: loss 0.846734
[epoch6, step1312]: loss 0.668277
[epoch6, step1313]: loss 0.737504
[epoch6, step1314]: loss 0.764625
[epoch6, step1315]: loss 0.930269
[epoch6, step1316]: loss 0.540973
[epoch6, step1317]: loss 0.679896
[epoch6, step1318]: loss 0.744937
[epoch6, step1319]: loss 0.645778
[epoch6, step1320]: loss 0.659378
[epoch6, step1321]: loss 0.532735
[epoch6, step1322]: loss 0.940884
[epoch6, step1323]: loss 0.778808
[epoch6, step1324]: loss 0.713439
[epoch6, step1325]: loss 0.676492
[epoch6, step1326]: loss 0.666170
[epoch6, step1327]: loss 0.668920
[epoch6, step1328]: loss 0.692761
[epoch6, step1329]: loss 0.868625
[epoch6, step1330]: loss 0.775498
[epoch6, step1331]: loss 0.773669
[epoch6, step1332]: loss 0.758376
[epoch6, step1333]: loss 0.589560
[epoch6, step1334]: loss 0.665822
[epoch6, step1335]: loss 0.763148
[epoch6, step1336]: loss 0.723658
[epoch6, step1337]: loss 0.473877
[epoch6, step1338]: loss 0.490415
[epoch6, step1339]: loss 0.861776
[epoch6, step1340]: loss 0.632294
[epoch6, step1341]: loss 0.873451
[epoch6, step1342]: loss 0.644596
[epoch6, step1343]: loss 0.882019
[epoch6, step1344]: loss 0.791400
[epoch6, step1345]: loss 0.745739
[epoch6, step1346]: loss 0.442696
[epoch6, step1347]: loss 0.685943
[epoch6, step1348]: loss 0.564599
[epoch6, step1349]: loss 0.690050
[epoch6, step1350]: loss 0.578226
[epoch6, step1351]: loss 0.462852
[epoch6, step1352]: loss 0.616525
[epoch6, step1353]: loss 0.858158
[epoch6, step1354]: loss 0.732586
[epoch6, step1355]: loss 0.816999
[epoch6, step1356]: loss 0.395619
[epoch6, step1357]: loss 0.798123
[epoch6, step1358]: loss 0.806955
[epoch6, step1359]: loss 0.539006
[epoch6, step1360]: loss 0.516934
[epoch6, step1361]: loss 0.657357
[epoch6, step1362]: loss 0.566241
[epoch6, step1363]: loss 0.668890
[epoch6, step1364]: loss 0.702716
[epoch6, step1365]: loss 0.582080
[epoch6, step1366]: loss 0.568213
[epoch6, step1367]: loss 0.531283
[epoch6, step1368]: loss 0.860614
[epoch6, step1369]: loss 0.580890
[epoch6, step1370]: loss 0.583929
[epoch6, step1371]: loss 0.386807
[epoch6, step1372]: loss 0.389231
[epoch6, step1373]: loss 0.701997
[epoch6, step1374]: loss 0.882136
[epoch6, step1375]: loss 0.888871
[epoch6, step1376]: loss 0.667695
[epoch6, step1377]: loss 0.600406
[epoch6, step1378]: loss 0.719178
[epoch6, step1379]: loss 0.717854
[epoch6, step1380]: loss 0.520933
[epoch6, step1381]: loss 0.621965
[epoch6, step1382]: loss 0.531214
[epoch6, step1383]: loss 0.653300
[epoch6, step1384]: loss 0.743825
[epoch6, step1385]: loss 0.778131
[epoch6, step1386]: loss 0.561372
[epoch6, step1387]: loss 0.519787
[epoch6, step1388]: loss 0.687951
[epoch6, step1389]: loss 0.557662
[epoch6, step1390]: loss 0.718024
[epoch6, step1391]: loss 0.612410
[epoch6, step1392]: loss 0.893527
[epoch6, step1393]: loss 0.652982
[epoch6, step1394]: loss 0.741027
[epoch6, step1395]: loss 0.607218
[epoch6, step1396]: loss 0.484380
[epoch6, step1397]: loss 0.557760
[epoch6, step1398]: loss 0.490136
[epoch6, step1399]: loss 0.539581
[epoch6, step1400]: loss 0.919422
[epoch6, step1401]: loss 0.714157
[epoch6, step1402]: loss 0.505685
[epoch6, step1403]: loss 0.574923
[epoch6, step1404]: loss 0.888527
[epoch6, step1405]: loss 0.745938
[epoch6, step1406]: loss 0.674710
[epoch6, step1407]: loss 0.702735
[epoch6, step1408]: loss 0.942969
[epoch6, step1409]: loss 0.284474
[epoch6, step1410]: loss 0.786068
[epoch6, step1411]: loss 0.839877
[epoch6, step1412]: loss 0.687070
[epoch6, step1413]: loss 0.737616
[epoch6, step1414]: loss 0.780672
[epoch6, step1415]: loss 0.772059
[epoch6, step1416]: loss 0.735857
[epoch6, step1417]: loss 0.667718
[epoch6, step1418]: loss 0.735729
[epoch6, step1419]: loss 0.627932
[epoch6, step1420]: loss 0.796431
[epoch6, step1421]: loss 0.965317
[epoch6, step1422]: loss 0.930789
[epoch6, step1423]: loss 0.627897
[epoch6, step1424]: loss 0.704557
[epoch6, step1425]: loss 0.564225
[epoch6, step1426]: loss 0.637640
[epoch6, step1427]: loss 0.621925
[epoch6, step1428]: loss 0.644070
[epoch6, step1429]: loss 0.716494
[epoch6, step1430]: loss 0.543950
[epoch6, step1431]: loss 0.969425
[epoch6, step1432]: loss 0.835193
[epoch6, step1433]: loss 0.579962
[epoch6, step1434]: loss 0.517096
[epoch6, step1435]: loss 0.667075
[epoch6, step1436]: loss 0.523859
[epoch6, step1437]: loss 0.784655
[epoch6, step1438]: loss 0.458549
[epoch6, step1439]: loss 0.614814
[epoch6, step1440]: loss 0.527923
[epoch6, step1441]: loss 0.919145
[epoch6, step1442]: loss 0.627609
[epoch6, step1443]: loss 0.653974
[epoch6, step1444]: loss 0.515603
[epoch6, step1445]: loss 0.622496
[epoch6, step1446]: loss 0.698782
[epoch6, step1447]: loss 0.759314
[epoch6, step1448]: loss 0.631932
[epoch6, step1449]: loss 0.338351
[epoch6, step1450]: loss 0.525053
[epoch6, step1451]: loss 0.744429
[epoch6, step1452]: loss 0.545220
[epoch6, step1453]: loss 0.827535
[epoch6, step1454]: loss 0.843302
[epoch6, step1455]: loss 0.619921
[epoch6, step1456]: loss 0.658358
[epoch6, step1457]: loss 0.590003
[epoch6, step1458]: loss 0.863803
[epoch6, step1459]: loss 0.626200
[epoch6, step1460]: loss 0.671774
[epoch6, step1461]: loss 0.554933
[epoch6, step1462]: loss 0.699026
[epoch6, step1463]: loss 0.978286
[epoch6, step1464]: loss 0.673552
[epoch6, step1465]: loss 0.875796
[epoch6, step1466]: loss 0.742373
[epoch6, step1467]: loss 0.732098
[epoch6, step1468]: loss 0.441695
[epoch6, step1469]: loss 0.282112
[epoch6, step1470]: loss 0.783023
[epoch6, step1471]: loss 0.552262
[epoch6, step1472]: loss 0.766904
[epoch6, step1473]: loss 0.576923
[epoch6, step1474]: loss 0.763936
[epoch6, step1475]: loss 0.635180
[epoch6, step1476]: loss 0.385331
[epoch6, step1477]: loss 0.785312
[epoch6, step1478]: loss 0.776802
[epoch6, step1479]: loss 0.501563
[epoch6, step1480]: loss 0.406853
[epoch6, step1481]: loss 0.625198
[epoch6, step1482]: loss 0.799068
[epoch6, step1483]: loss 0.702587
[epoch6, step1484]: loss 0.676794
[epoch6, step1485]: loss 0.671001
[epoch6, step1486]: loss 0.708026
[epoch6, step1487]: loss 0.667540
[epoch6, step1488]: loss 0.579860
[epoch6, step1489]: loss 0.552091
[epoch6, step1490]: loss 0.762469
[epoch6, step1491]: loss 0.800910
[epoch6, step1492]: loss 0.501785
[epoch6, step1493]: loss 0.624582
[epoch6, step1494]: loss 0.723578
[epoch6, step1495]: loss 0.671914
[epoch6, step1496]: loss 0.758163
[epoch6, step1497]: loss 0.593008
[epoch6, step1498]: loss 0.763060
[epoch6, step1499]: loss 0.489156
[epoch6, step1500]: loss 0.608081
[epoch6, step1501]: loss 0.813435
[epoch6, step1502]: loss 0.774073
[epoch6, step1503]: loss 0.851291
[epoch6, step1504]: loss 0.614766
[epoch6, step1505]: loss 0.696137
[epoch6, step1506]: loss 0.794667
[epoch6, step1507]: loss 0.820052
[epoch6, step1508]: loss 0.538672
[epoch6, step1509]: loss 0.301714
[epoch6, step1510]: loss 0.725193
[epoch6, step1511]: loss 0.735602
[epoch6, step1512]: loss 0.748441
[epoch6, step1513]: loss 0.441528
[epoch6, step1514]: loss 0.544996
[epoch6, step1515]: loss 0.705327
[epoch6, step1516]: loss 0.612047
[epoch6, step1517]: loss 0.659338
[epoch6, step1518]: loss 0.678531
[epoch6, step1519]: loss 0.589984
[epoch6, step1520]: loss 0.716547
[epoch6, step1521]: loss 0.855282
[epoch6, step1522]: loss 0.803538
[epoch6, step1523]: loss 0.800039
[epoch6, step1524]: loss 0.765192
[epoch6, step1525]: loss 0.802656
[epoch6, step1526]: loss 0.744004
[epoch6, step1527]: loss 0.492518
[epoch6, step1528]: loss 0.667138
[epoch6, step1529]: loss 0.836478
[epoch6, step1530]: loss 0.740490
[epoch6, step1531]: loss 0.335376
[epoch6, step1532]: loss 0.639192
[epoch6, step1533]: loss 0.858002
[epoch6, step1534]: loss 0.771950
[epoch6, step1535]: loss 0.649018
[epoch6, step1536]: loss 0.458922
[epoch6, step1537]: loss 0.477119
[epoch6, step1538]: loss 0.511961
[epoch6, step1539]: loss 0.547693
[epoch6, step1540]: loss 0.481458
[epoch6, step1541]: loss 0.733068
[epoch6, step1542]: loss 0.510263
[epoch6, step1543]: loss 0.820744
[epoch6, step1544]: loss 0.349972
[epoch6, step1545]: loss 0.573804
[epoch6, step1546]: loss 0.454827
[epoch6, step1547]: loss 0.522687
[epoch6, step1548]: loss 0.729829
[epoch6, step1549]: loss 0.900301
[epoch6, step1550]: loss 0.698583
[epoch6, step1551]: loss 0.716506
[epoch6, step1552]: loss 0.615059
[epoch6, step1553]: loss 0.842117
[epoch6, step1554]: loss 0.756379
[epoch6, step1555]: loss 0.457658
[epoch6, step1556]: loss 0.522719
[epoch6, step1557]: loss 0.707291
[epoch6, step1558]: loss 0.656578
[epoch6, step1559]: loss 0.708217
[epoch6, step1560]: loss 0.342022
[epoch6, step1561]: loss 0.591526
[epoch6, step1562]: loss 0.476345
[epoch6, step1563]: loss 0.875727
[epoch6, step1564]: loss 0.722685
[epoch6, step1565]: loss 0.522027
[epoch6, step1566]: loss 0.713511
[epoch6, step1567]: loss 0.606198
[epoch6, step1568]: loss 0.834783
[epoch6, step1569]: loss 0.590307
[epoch6, step1570]: loss 0.603549
[epoch6, step1571]: loss 0.786663
[epoch6, step1572]: loss 0.704784
[epoch6, step1573]: loss 0.676664
[epoch6, step1574]: loss 0.686432
[epoch6, step1575]: loss 0.400977
[epoch6, step1576]: loss 0.958230
[epoch6, step1577]: loss 0.810056
[epoch6, step1578]: loss 0.645129
[epoch6, step1579]: loss 0.511585
[epoch6, step1580]: loss 0.706440
[epoch6, step1581]: loss 0.507282
[epoch6, step1582]: loss 0.777794
[epoch6, step1583]: loss 0.608863
[epoch6, step1584]: loss 0.502873
[epoch6, step1585]: loss 0.785415
[epoch6, step1586]: loss 0.695722
[epoch6, step1587]: loss 0.670769
[epoch6, step1588]: loss 0.783851
[epoch6, step1589]: loss 0.715404
[epoch6, step1590]: loss 0.724046
[epoch6, step1591]: loss 0.695150
[epoch6, step1592]: loss 0.548895
[epoch6, step1593]: loss 0.652074
[epoch6, step1594]: loss 0.489231
[epoch6, step1595]: loss 0.679893
[epoch6, step1596]: loss 0.726209
[epoch6, step1597]: loss 0.796486
[epoch6, step1598]: loss 0.412800
[epoch6, step1599]: loss 0.707044
[epoch6, step1600]: loss 0.789123
[epoch6, step1601]: loss 0.910863
[epoch6, step1602]: loss 0.445270
[epoch6, step1603]: loss 0.896773
[epoch6, step1604]: loss 0.873351
[epoch6, step1605]: loss 0.744008
[epoch6, step1606]: loss 0.568698
[epoch6, step1607]: loss 0.549526
[epoch6, step1608]: loss 0.630945
[epoch6, step1609]: loss 0.624810
[epoch6, step1610]: loss 0.648187
[epoch6, step1611]: loss 0.861790
[epoch6, step1612]: loss 0.783527
[epoch6, step1613]: loss 0.775756
[epoch6, step1614]: loss 0.621428
[epoch6, step1615]: loss 0.704489
[epoch6, step1616]: loss 0.680769
[epoch6, step1617]: loss 0.424872
[epoch6, step1618]: loss 0.667742
[epoch6, step1619]: loss 0.533713
[epoch6, step1620]: loss 0.761966
[epoch6, step1621]: loss 0.940012
[epoch6, step1622]: loss 0.700132
[epoch6, step1623]: loss 0.868868
[epoch6, step1624]: loss 0.685111
[epoch6, step1625]: loss 0.783503
[epoch6, step1626]: loss 0.672033
[epoch6, step1627]: loss 0.590929
[epoch6, step1628]: loss 0.455784
[epoch6, step1629]: loss 0.413239
[epoch6, step1630]: loss 0.690478
[epoch6, step1631]: loss 0.720330
[epoch6, step1632]: loss 0.874960
[epoch6, step1633]: loss 0.686157
[epoch6, step1634]: loss 0.555366
[epoch6, step1635]: loss 0.526672
[epoch6, step1636]: loss 0.763741
[epoch6, step1637]: loss 0.941394
[epoch6, step1638]: loss 0.773702
[epoch6, step1639]: loss 0.560927
[epoch6, step1640]: loss 0.936561
[epoch6, step1641]: loss 0.553585
[epoch6, step1642]: loss 0.711705
[epoch6, step1643]: loss 0.513791
[epoch6, step1644]: loss 0.858894
[epoch6, step1645]: loss 0.779218
[epoch6, step1646]: loss 0.477724
[epoch6, step1647]: loss 0.935459
[epoch6, step1648]: loss 0.675759
[epoch6, step1649]: loss 0.670212
[epoch6, step1650]: loss 0.723710
[epoch6, step1651]: loss 0.609186
[epoch6, step1652]: loss 0.873871
[epoch6, step1653]: loss 0.758836
[epoch6, step1654]: loss 0.377101
[epoch6, step1655]: loss 0.623064
[epoch6, step1656]: loss 0.574664
[epoch6, step1657]: loss 0.709035
[epoch6, step1658]: loss 0.821554
[epoch6, step1659]: loss 0.482168
[epoch6, step1660]: loss 0.644707
[epoch6, step1661]: loss 0.645261
[epoch6, step1662]: loss 0.599731
[epoch6, step1663]: loss 0.563477
[epoch6, step1664]: loss 0.645214
[epoch6, step1665]: loss 0.791279
[epoch6, step1666]: loss 0.476704
[epoch6, step1667]: loss 0.673264
[epoch6, step1668]: loss 0.706995
[epoch6, step1669]: loss 0.653545
[epoch6, step1670]: loss 0.887988
[epoch6, step1671]: loss 0.832017
[epoch6, step1672]: loss 0.667901
[epoch6, step1673]: loss 0.471853
[epoch6, step1674]: loss 1.014071
[epoch6, step1675]: loss 0.519090
[epoch6, step1676]: loss 0.750832
[epoch6, step1677]: loss 0.807101
[epoch6, step1678]: loss 0.763902
[epoch6, step1679]: loss 0.775634
[epoch6, step1680]: loss 0.741485
[epoch6, step1681]: loss 0.719885
[epoch6, step1682]: loss 0.698103
[epoch6, step1683]: loss 0.679360
[epoch6, step1684]: loss 0.473106
[epoch6, step1685]: loss 0.764023
[epoch6, step1686]: loss 0.621396
[epoch6, step1687]: loss 0.433561
[epoch6, step1688]: loss 0.638504
[epoch6, step1689]: loss 0.922499
[epoch6, step1690]: loss 0.736802
[epoch6, step1691]: loss 0.635296
[epoch6, step1692]: loss 0.819178
[epoch6, step1693]: loss 0.510705
[epoch6, step1694]: loss 0.668098
[epoch6, step1695]: loss 0.493824
[epoch6, step1696]: loss 0.735511
[epoch6, step1697]: loss 0.479225
[epoch6, step1698]: loss 0.662168
[epoch6, step1699]: loss 0.795158
[epoch6, step1700]: loss 0.742341
[epoch6, step1701]: loss 0.741698
[epoch6, step1702]: loss 0.757758
[epoch6, step1703]: loss 0.476321
[epoch6, step1704]: loss 0.594072
[epoch6, step1705]: loss 0.601073
[epoch6, step1706]: loss 0.714305
[epoch6, step1707]: loss 0.907112
[epoch6, step1708]: loss 0.505409
[epoch6, step1709]: loss 0.880227
[epoch6, step1710]: loss 0.481510
[epoch6, step1711]: loss 0.624209
[epoch6, step1712]: loss 0.767518
[epoch6, step1713]: loss 0.374600
[epoch6, step1714]: loss 0.570148
[epoch6, step1715]: loss 0.893854
[epoch6, step1716]: loss 0.592978
[epoch6, step1717]: loss 0.746736
[epoch6, step1718]: loss 0.943130
[epoch6, step1719]: loss 0.755925
[epoch6, step1720]: loss 0.591267
[epoch6, step1721]: loss 0.641409
[epoch6, step1722]: loss 0.652716
[epoch6, step1723]: loss 0.794320
[epoch6, step1724]: loss 0.370748
[epoch6, step1725]: loss 0.627935
[epoch6, step1726]: loss 0.738461
[epoch6, step1727]: loss 0.412409
[epoch6, step1728]: loss 0.622900
[epoch6, step1729]: loss 0.772850
[epoch6, step1730]: loss 0.538049
[epoch6, step1731]: loss 0.583515
[epoch6, step1732]: loss 0.701792
[epoch6, step1733]: loss 0.809426
[epoch6, step1734]: loss 0.627221
[epoch6, step1735]: loss 0.752387
[epoch6, step1736]: loss 0.707825
[epoch6, step1737]: loss 0.673327
[epoch6, step1738]: loss 0.680822
[epoch6, step1739]: loss 0.910560
[epoch6, step1740]: loss 0.619141
[epoch6, step1741]: loss 0.835701
[epoch6, step1742]: loss 0.892861
[epoch6, step1743]: loss 0.559998
[epoch6, step1744]: loss 0.437594
[epoch6, step1745]: loss 0.885133
[epoch6, step1746]: loss 0.515139
[epoch6, step1747]: loss 0.604676
[epoch6, step1748]: loss 0.504475
[epoch6, step1749]: loss 0.748492
[epoch6, step1750]: loss 0.552657
[epoch6, step1751]: loss 0.553105
[epoch6, step1752]: loss 0.640513
[epoch6, step1753]: loss 0.497562
[epoch6, step1754]: loss 0.668149
[epoch6, step1755]: loss 0.899957
[epoch6, step1756]: loss 0.868054
[epoch6, step1757]: loss 0.752641
[epoch6, step1758]: loss 0.312100
[epoch6, step1759]: loss 0.440499
[epoch6, step1760]: loss 0.724678
[epoch6, step1761]: loss 0.415734
[epoch6, step1762]: loss 0.711268
[epoch6, step1763]: loss 0.354875
[epoch6, step1764]: loss 0.729108
[epoch6, step1765]: loss 0.567733
[epoch6, step1766]: loss 0.705152
[epoch6, step1767]: loss 0.611616
[epoch6, step1768]: loss 0.433126
[epoch6, step1769]: loss 0.863514
[epoch6, step1770]: loss 0.826048
[epoch6, step1771]: loss 0.737752
[epoch6, step1772]: loss 0.574815
[epoch6, step1773]: loss 0.707482
[epoch6, step1774]: loss 0.642323
[epoch6, step1775]: loss 0.912067
[epoch6, step1776]: loss 0.775790
[epoch6, step1777]: loss 0.483209
[epoch6, step1778]: loss 0.888142
[epoch6, step1779]: loss 0.684227
[epoch6, step1780]: loss 0.594960
[epoch6, step1781]: loss 0.920010
[epoch6, step1782]: loss 0.740839
[epoch6, step1783]: loss 0.774145
[epoch6, step1784]: loss 0.759070
[epoch6, step1785]: loss 0.778044
[epoch6, step1786]: loss 0.790855
[epoch6, step1787]: loss 0.888259
[epoch6, step1788]: loss 0.748158
[epoch6, step1789]: loss 0.670545
[epoch6, step1790]: loss 0.566215
[epoch6, step1791]: loss 0.770192
[epoch6, step1792]: loss 0.578358
[epoch6, step1793]: loss 0.831119
[epoch6, step1794]: loss 0.877833
[epoch6, step1795]: loss 0.770145
[epoch6, step1796]: loss 0.631948
[epoch6, step1797]: loss 0.783785
[epoch6, step1798]: loss 0.803779
[epoch6, step1799]: loss 0.701181
[epoch6, step1800]: loss 0.915273
[epoch6, step1801]: loss 0.884711
[epoch6, step1802]: loss 0.735512
[epoch6, step1803]: loss 0.790416
[epoch6, step1804]: loss 0.518697
[epoch6, step1805]: loss 0.832538
[epoch6, step1806]: loss 0.629500
[epoch6, step1807]: loss 0.440902
[epoch6, step1808]: loss 0.654645
[epoch6, step1809]: loss 0.536483
[epoch6, step1810]: loss 0.567454
[epoch6, step1811]: loss 0.733923
[epoch6, step1812]: loss 0.389297
[epoch6, step1813]: loss 0.825845
[epoch6, step1814]: loss 1.005641
[epoch6, step1815]: loss 0.566253
[epoch6, step1816]: loss 0.812082
[epoch6, step1817]: loss 0.747216
[epoch6, step1818]: loss 0.745632
[epoch6, step1819]: loss 0.805412
[epoch6, step1820]: loss 0.571250
[epoch6, step1821]: loss 0.474882
[epoch6, step1822]: loss 0.822354
[epoch6, step1823]: loss 0.719918
[epoch6, step1824]: loss 0.583906
[epoch6, step1825]: loss 0.694053
[epoch6, step1826]: loss 0.536584
[epoch6, step1827]: loss 0.611076
[epoch6, step1828]: loss 0.379665
[epoch6, step1829]: loss 0.681246
[epoch6, step1830]: loss 0.342229
[epoch6, step1831]: loss 0.629119
[epoch6, step1832]: loss 0.622231
[epoch6, step1833]: loss 0.980674
[epoch6, step1834]: loss 0.586635
[epoch6, step1835]: loss 0.354598
[epoch6, step1836]: loss 0.878806
[epoch6, step1837]: loss 0.408452
[epoch6, step1838]: loss 0.829316
[epoch6, step1839]: loss 0.840858
[epoch6, step1840]: loss 0.607977
[epoch6, step1841]: loss 0.650987
[epoch6, step1842]: loss 0.752560
[epoch6, step1843]: loss 0.400573
[epoch6, step1844]: loss 0.777409
[epoch6, step1845]: loss 0.572861
[epoch6, step1846]: loss 0.705596
[epoch6, step1847]: loss 0.647915
[epoch6, step1848]: loss 0.702109
[epoch6, step1849]: loss 0.813064
[epoch6, step1850]: loss 0.704737
[epoch6, step1851]: loss 0.600953
[epoch6, step1852]: loss 0.716178
[epoch6, step1853]: loss 0.725903
[epoch6, step1854]: loss 0.361610
[epoch6, step1855]: loss 0.739526
[epoch6, step1856]: loss 0.234835
[epoch6, step1857]: loss 0.750996
[epoch6, step1858]: loss 0.233223
[epoch6, step1859]: loss 0.706188
[epoch6, step1860]: loss 0.938878
[epoch6, step1861]: loss 0.576155
[epoch6, step1862]: loss 0.791581
[epoch6, step1863]: loss 0.757985
[epoch6, step1864]: loss 0.432142
[epoch6, step1865]: loss 1.022851
[epoch6, step1866]: loss 0.684828
[epoch6, step1867]: loss 0.575327
[epoch6, step1868]: loss 0.603432
[epoch6, step1869]: loss 0.669662
[epoch6, step1870]: loss 0.826826
[epoch6, step1871]: loss 0.417779
[epoch6, step1872]: loss 0.649932
[epoch6, step1873]: loss 0.753512
[epoch6, step1874]: loss 0.655962
[epoch6, step1875]: loss 0.800854
[epoch6, step1876]: loss 0.574454
[epoch6, step1877]: loss 0.408358
[epoch6, step1878]: loss 0.623826
[epoch6, step1879]: loss 0.974802
[epoch6, step1880]: loss 0.800489
[epoch6, step1881]: loss 0.579112
[epoch6, step1882]: loss 0.704971
[epoch6, step1883]: loss 0.914026
[epoch6, step1884]: loss 0.444102
[epoch6, step1885]: loss 0.587006
[epoch6, step1886]: loss 0.735665
[epoch6, step1887]: loss 0.479847
[epoch6, step1888]: loss 0.728211
[epoch6, step1889]: loss 0.684544
[epoch6, step1890]: loss 0.697107
[epoch6, step1891]: loss 0.842282
[epoch6, step1892]: loss 0.322757
[epoch6, step1893]: loss 0.722230
[epoch6, step1894]: loss 0.904347
[epoch6, step1895]: loss 0.624167
[epoch6, step1896]: loss 0.575884
[epoch6, step1897]: loss 0.461627
[epoch6, step1898]: loss 0.601310
[epoch6, step1899]: loss 0.671425
[epoch6, step1900]: loss 0.691701
[epoch6, step1901]: loss 0.550077
[epoch6, step1902]: loss 0.961248
[epoch6, step1903]: loss 0.442596
[epoch6, step1904]: loss 0.656485
[epoch6, step1905]: loss 0.673515
[epoch6, step1906]: loss 0.857803
[epoch6, step1907]: loss 0.605122
[epoch6, step1908]: loss 0.778317
[epoch6, step1909]: loss 0.516921
[epoch6, step1910]: loss 0.558661
[epoch6, step1911]: loss 0.688069
[epoch6, step1912]: loss 0.658411
[epoch6, step1913]: loss 0.870223
[epoch6, step1914]: loss 1.003995
[epoch6, step1915]: loss 0.678476
[epoch6, step1916]: loss 0.327758
[epoch6, step1917]: loss 0.608456
[epoch6, step1918]: loss 0.762142
[epoch6, step1919]: loss 0.522422
[epoch6, step1920]: loss 0.556121
[epoch6, step1921]: loss 0.739122
[epoch6, step1922]: loss 0.775106
[epoch6, step1923]: loss 0.696244
[epoch6, step1924]: loss 0.542601
[epoch6, step1925]: loss 0.703741
[epoch6, step1926]: loss 0.707529
[epoch6, step1927]: loss 0.962668
[epoch6, step1928]: loss 0.733089
[epoch6, step1929]: loss 0.987622
[epoch6, step1930]: loss 0.836852
[epoch6, step1931]: loss 0.603356
[epoch6, step1932]: loss 0.591101
[epoch6, step1933]: loss 0.780104
[epoch6, step1934]: loss 0.870807
[epoch6, step1935]: loss 0.868986
[epoch6, step1936]: loss 0.628860
[epoch6, step1937]: loss 0.767665
[epoch6, step1938]: loss 0.762697
[epoch6, step1939]: loss 0.661941
[epoch6, step1940]: loss 0.817216
[epoch6, step1941]: loss 0.654663
[epoch6, step1942]: loss 0.638588
[epoch6, step1943]: loss 0.616797
[epoch6, step1944]: loss 0.689987
[epoch6, step1945]: loss 0.431780
[epoch6, step1946]: loss 0.697748
[epoch6, step1947]: loss 0.789338
[epoch6, step1948]: loss 0.677696
[epoch6, step1949]: loss 0.309820
[epoch6, step1950]: loss 0.699932
[epoch6, step1951]: loss 0.806388
[epoch6, step1952]: loss 0.887465
[epoch6, step1953]: loss 0.541543
[epoch6, step1954]: loss 0.479639
[epoch6, step1955]: loss 0.671365
[epoch6, step1956]: loss 0.866794
[epoch6, step1957]: loss 0.811631
[epoch6, step1958]: loss 0.698405
[epoch6, step1959]: loss 0.811864
[epoch6, step1960]: loss 0.825992
[epoch6, step1961]: loss 0.870041
[epoch6, step1962]: loss 0.607540
[epoch6, step1963]: loss 0.789189
[epoch6, step1964]: loss 0.566250
[epoch6, step1965]: loss 0.851485
[epoch6, step1966]: loss 0.775890
[epoch6, step1967]: loss 0.846796
[epoch6, step1968]: loss 1.041275
[epoch6, step1969]: loss 0.517066
[epoch6, step1970]: loss 0.681175
[epoch6, step1971]: loss 0.674802
[epoch6, step1972]: loss 0.768391
[epoch6, step1973]: loss 0.725172
[epoch6, step1974]: loss 0.658797
[epoch6, step1975]: loss 0.565988
[epoch6, step1976]: loss 0.249269
[epoch6, step1977]: loss 0.551955
[epoch6, step1978]: loss 0.908139
[epoch6, step1979]: loss 0.630797
[epoch6, step1980]: loss 0.635827
[epoch6, step1981]: loss 0.673415
[epoch6, step1982]: loss 0.744280
[epoch6, step1983]: loss 0.958189
[epoch6, step1984]: loss 0.527322
[epoch6, step1985]: loss 0.855706
[epoch6, step1986]: loss 0.639453
[epoch6, step1987]: loss 0.778456
[epoch6, step1988]: loss 0.511737
[epoch6, step1989]: loss 0.557072
[epoch6, step1990]: loss 0.844402
[epoch6, step1991]: loss 0.695915
[epoch6, step1992]: loss 0.661572
[epoch6, step1993]: loss 0.771144
[epoch6, step1994]: loss 0.502384
[epoch6, step1995]: loss 0.408330
[epoch6, step1996]: loss 0.689626
[epoch6, step1997]: loss 0.999033
[epoch6, step1998]: loss 0.649076
[epoch6, step1999]: loss 0.386997
[epoch6, step2000]: loss 0.689603
[epoch6, step2001]: loss 0.640242
[epoch6, step2002]: loss 1.009538
[epoch6, step2003]: loss 0.709105
[epoch6, step2004]: loss 0.762346
[epoch6, step2005]: loss 0.578104
[epoch6, step2006]: loss 0.672587
[epoch6, step2007]: loss 0.545536
[epoch6, step2008]: loss 0.916756
[epoch6, step2009]: loss 0.919698
[epoch6, step2010]: loss 0.696336
[epoch6, step2011]: loss 0.619322
[epoch6, step2012]: loss 0.344406
[epoch6, step2013]: loss 0.604800
[epoch6, step2014]: loss 0.375329
[epoch6, step2015]: loss 0.512555
[epoch6, step2016]: loss 0.793001
[epoch6, step2017]: loss 0.439801
[epoch6, step2018]: loss 0.606061
[epoch6, step2019]: loss 0.624237
[epoch6, step2020]: loss 0.309016
[epoch6, step2021]: loss 0.649158
[epoch6, step2022]: loss 0.577872
[epoch6, step2023]: loss 0.413724
[epoch6, step2024]: loss 0.633980
[epoch6, step2025]: loss 0.704276
[epoch6, step2026]: loss 0.545605
[epoch6, step2027]: loss 0.633359
[epoch6, step2028]: loss 0.783597
[epoch6, step2029]: loss 0.855437
[epoch6, step2030]: loss 0.555301
[epoch6, step2031]: loss 0.813115
[epoch6, step2032]: loss 0.704479
[epoch6, step2033]: loss 0.783186
[epoch6, step2034]: loss 0.353544
[epoch6, step2035]: loss 0.596937
[epoch6, step2036]: loss 0.623842
[epoch6, step2037]: loss 0.840415
[epoch6, step2038]: loss 0.298378
[epoch6, step2039]: loss 0.839015
[epoch6, step2040]: loss 0.617689
[epoch6, step2041]: loss 1.045539
[epoch6, step2042]: loss 0.598309
[epoch6, step2043]: loss 0.727017
[epoch6, step2044]: loss 0.519494
[epoch6, step2045]: loss 0.502352
[epoch6, step2046]: loss 0.662001
[epoch6, step2047]: loss 0.816548
[epoch6, step2048]: loss 0.819964
[epoch6, step2049]: loss 0.770846
[epoch6, step2050]: loss 0.510453
[epoch6, step2051]: loss 0.519888
[epoch6, step2052]: loss 0.762902
[epoch6, step2053]: loss 0.681632
[epoch6, step2054]: loss 0.776912
[epoch6, step2055]: loss 0.992919
[epoch6, step2056]: loss 0.817374
[epoch6, step2057]: loss 0.784381
[epoch6, step2058]: loss 0.564623
[epoch6, step2059]: loss 0.335327
[epoch6, step2060]: loss 0.660226
[epoch6, step2061]: loss 0.665058
[epoch6, step2062]: loss 0.594367
[epoch6, step2063]: loss 0.604080
[epoch6, step2064]: loss 0.593640
[epoch6, step2065]: loss 0.755020
[epoch6, step2066]: loss 0.875637
[epoch6, step2067]: loss 0.637356
[epoch6, step2068]: loss 0.634476
[epoch6, step2069]: loss 0.821446
[epoch6, step2070]: loss 0.630815
[epoch6, step2071]: loss 0.556025
[epoch6, step2072]: loss 0.966058
[epoch6, step2073]: loss 0.772755
[epoch6, step2074]: loss 0.897489
[epoch6, step2075]: loss 0.618928
[epoch6, step2076]: loss 0.467578
[epoch6, step2077]: loss 0.829857
[epoch6, step2078]: loss 0.415942
[epoch6, step2079]: loss 0.307244
[epoch6, step2080]: loss 0.645511
[epoch6, step2081]: loss 0.806713
[epoch6, step2082]: loss 0.675301
[epoch6, step2083]: loss 0.659975
[epoch6, step2084]: loss 0.606242
[epoch6, step2085]: loss 0.743950
[epoch6, step2086]: loss 0.811820
[epoch6, step2087]: loss 0.739908
[epoch6, step2088]: loss 0.603189
[epoch6, step2089]: loss 0.522358
[epoch6, step2090]: loss 0.845322
[epoch6, step2091]: loss 0.547952
[epoch6, step2092]: loss 0.563208
[epoch6, step2093]: loss 0.646855
[epoch6, step2094]: loss 0.555158
[epoch6, step2095]: loss 0.771674
[epoch6, step2096]: loss 0.685153
[epoch6, step2097]: loss 0.842034
[epoch6, step2098]: loss 0.521923
[epoch6, step2099]: loss 0.551825
[epoch6, step2100]: loss 0.354022
[epoch6, step2101]: loss 0.933518
[epoch6, step2102]: loss 0.721975
[epoch6, step2103]: loss 0.593679
[epoch6, step2104]: loss 0.694249
[epoch6, step2105]: loss 0.761240
[epoch6, step2106]: loss 0.874729
[epoch6, step2107]: loss 0.681855
[epoch6, step2108]: loss 0.781871
[epoch6, step2109]: loss 0.687864
[epoch6, step2110]: loss 0.777513
[epoch6, step2111]: loss 0.642292
[epoch6, step2112]: loss 0.287412
[epoch6, step2113]: loss 0.308939
[epoch6, step2114]: loss 0.478249
[epoch6, step2115]: loss 0.769690
[epoch6, step2116]: loss 0.569919
[epoch6, step2117]: loss 0.627009
[epoch6, step2118]: loss 0.598998
[epoch6, step2119]: loss 0.809922
[epoch6, step2120]: loss 0.547118
[epoch6, step2121]: loss 0.638182
[epoch6, step2122]: loss 0.669277
[epoch6, step2123]: loss 0.754308
[epoch6, step2124]: loss 0.611157
[epoch6, step2125]: loss 0.891285
[epoch6, step2126]: loss 0.728439
[epoch6, step2127]: loss 0.577832
[epoch6, step2128]: loss 0.948090
[epoch6, step2129]: loss 0.471687
[epoch6, step2130]: loss 0.361089
[epoch6, step2131]: loss 0.404039
[epoch6, step2132]: loss 0.633314
[epoch6, step2133]: loss 0.664032
[epoch6, step2134]: loss 0.490176
[epoch6, step2135]: loss 0.903088
[epoch6, step2136]: loss 0.656339
[epoch6, step2137]: loss 0.767760
[epoch6, step2138]: loss 0.445977
[epoch6, step2139]: loss 0.589964
[epoch6, step2140]: loss 0.516187
[epoch6, step2141]: loss 0.623093
[epoch6, step2142]: loss 0.277290
[epoch6, step2143]: loss 0.590132
[epoch6, step2144]: loss 0.573722
[epoch6, step2145]: loss 0.416336
[epoch6, step2146]: loss 0.502466
[epoch6, step2147]: loss 0.839538
[epoch6, step2148]: loss 0.680945
[epoch6, step2149]: loss 0.677881
[epoch6, step2150]: loss 0.680998
[epoch6, step2151]: loss 0.547816
[epoch6, step2152]: loss 0.237031
[epoch6, step2153]: loss 0.739231
[epoch6, step2154]: loss 0.720513
[epoch6, step2155]: loss 0.638637
[epoch6, step2156]: loss 0.797055
[epoch6, step2157]: loss 0.696987
[epoch6, step2158]: loss 0.726090
[epoch6, step2159]: loss 0.741244
[epoch6, step2160]: loss 0.783638
[epoch6, step2161]: loss 0.779111
[epoch6, step2162]: loss 0.762424
[epoch6, step2163]: loss 0.456328
[epoch6, step2164]: loss 0.694986
[epoch6, step2165]: loss 0.699444
[epoch6, step2166]: loss 0.731716
[epoch6, step2167]: loss 0.559963
[epoch6, step2168]: loss 0.733742
[epoch6, step2169]: loss 0.594063
[epoch6, step2170]: loss 0.717217
[epoch6, step2171]: loss 0.621876
[epoch6, step2172]: loss 0.777389
[epoch6, step2173]: loss 0.363890
[epoch6, step2174]: loss 0.874373
[epoch6, step2175]: loss 0.240989
[epoch6, step2176]: loss 0.699471
[epoch6, step2177]: loss 0.588540
[epoch6, step2178]: loss 0.504953
[epoch6, step2179]: loss 0.630712
[epoch6, step2180]: loss 0.696468
[epoch6, step2181]: loss 0.340788
[epoch6, step2182]: loss 0.761683
[epoch6, step2183]: loss 0.750233
[epoch6, step2184]: loss 0.765504
[epoch6, step2185]: loss 0.857731
[epoch6, step2186]: loss 0.599887
[epoch6, step2187]: loss 0.700576
[epoch6, step2188]: loss 0.868990
[epoch6, step2189]: loss 0.599792
[epoch6, step2190]: loss 0.788761
[epoch6, step2191]: loss 0.627746
[epoch6, step2192]: loss 0.438521
[epoch6, step2193]: loss 0.710119
[epoch6, step2194]: loss 0.874831
[epoch6, step2195]: loss 0.755728
[epoch6, step2196]: loss 0.553879
[epoch6, step2197]: loss 0.574917
[epoch6, step2198]: loss 0.526910
[epoch6, step2199]: loss 0.651836
[epoch6, step2200]: loss 0.525417
[epoch6, step2201]: loss 0.468030
[epoch6, step2202]: loss 0.473933
[epoch6, step2203]: loss 0.724062
[epoch6, step2204]: loss 0.646428
[epoch6, step2205]: loss 0.498721
[epoch6, step2206]: loss 0.582054
[epoch6, step2207]: loss 0.773002
[epoch6, step2208]: loss 0.808856
[epoch6, step2209]: loss 0.688146
[epoch6, step2210]: loss 0.545709
[epoch6, step2211]: loss 0.576740
[epoch6, step2212]: loss 0.682805
[epoch6, step2213]: loss 0.395696
[epoch6, step2214]: loss 0.466900
[epoch6, step2215]: loss 0.580118
[epoch6, step2216]: loss 0.609472
[epoch6, step2217]: loss 0.483389
[epoch6, step2218]: loss 0.828910
[epoch6, step2219]: loss 0.729263
[epoch6, step2220]: loss 0.506407
[epoch6, step2221]: loss 0.559901
[epoch6, step2222]: loss 0.769115
[epoch6, step2223]: loss 0.537417
[epoch6, step2224]: loss 0.314911
[epoch6, step2225]: loss 0.538664
[epoch6, step2226]: loss 0.862616
[epoch6, step2227]: loss 0.525914
[epoch6, step2228]: loss 0.861229
[epoch6, step2229]: loss 0.512535
[epoch6, step2230]: loss 0.644645
[epoch6, step2231]: loss 0.457078
[epoch6, step2232]: loss 0.973243
[epoch6, step2233]: loss 0.702491
[epoch6, step2234]: loss 0.747746
[epoch6, step2235]: loss 0.805327
[epoch6, step2236]: loss 0.691216
[epoch6, step2237]: loss 0.800536
[epoch6, step2238]: loss 0.705725
[epoch6, step2239]: loss 0.638726
[epoch6, step2240]: loss 0.601982
[epoch6, step2241]: loss 0.834872
[epoch6, step2242]: loss 0.968649
[epoch6, step2243]: loss 0.315429
[epoch6, step2244]: loss 0.628814
[epoch6, step2245]: loss 0.370444
[epoch6, step2246]: loss 0.782851
[epoch6, step2247]: loss 0.668374
[epoch6, step2248]: loss 0.817478
[epoch6, step2249]: loss 0.887378
[epoch6, step2250]: loss 0.728404
[epoch6, step2251]: loss 0.558517
[epoch6, step2252]: loss 0.356296
[epoch6, step2253]: loss 0.814316
[epoch6, step2254]: loss 0.662930
[epoch6, step2255]: loss 0.794989
[epoch6, step2256]: loss 0.651353
[epoch6, step2257]: loss 0.854767
[epoch6, step2258]: loss 0.824195
[epoch6, step2259]: loss 0.798671
[epoch6, step2260]: loss 0.575693
[epoch6, step2261]: loss 0.579336
[epoch6, step2262]: loss 0.793986
[epoch6, step2263]: loss 0.700199
[epoch6, step2264]: loss 0.809567
[epoch6, step2265]: loss 0.735907
[epoch6, step2266]: loss 0.889947
[epoch6, step2267]: loss 0.665032
[epoch6, step2268]: loss 0.599933
[epoch6, step2269]: loss 0.589053
[epoch6, step2270]: loss 0.458271
[epoch6, step2271]: loss 0.534220
[epoch6, step2272]: loss 0.702428
[epoch6, step2273]: loss 0.613512
[epoch6, step2274]: loss 0.550799
[epoch6, step2275]: loss 0.714640
[epoch6, step2276]: loss 0.679851
[epoch6, step2277]: loss 0.443683
[epoch6, step2278]: loss 0.791291
[epoch6, step2279]: loss 0.597457
[epoch6, step2280]: loss 0.475449
[epoch6, step2281]: loss 0.748296
[epoch6, step2282]: loss 0.927081
[epoch6, step2283]: loss 0.539565
[epoch6, step2284]: loss 0.607421
[epoch6, step2285]: loss 0.559490
[epoch6, step2286]: loss 0.795156
[epoch6, step2287]: loss 0.746046
[epoch6, step2288]: loss 0.783243
[epoch6, step2289]: loss 0.848977
[epoch6, step2290]: loss 0.514574
[epoch6, step2291]: loss 0.726216
[epoch6, step2292]: loss 0.613463
[epoch6, step2293]: loss 0.338942
[epoch6, step2294]: loss 0.803924
[epoch6, step2295]: loss 0.451444
[epoch6, step2296]: loss 0.850389
[epoch6, step2297]: loss 0.732209
[epoch6, step2298]: loss 0.499923
[epoch6, step2299]: loss 0.935724
[epoch6, step2300]: loss 0.530883
[epoch6, step2301]: loss 0.729611
[epoch6, step2302]: loss 0.342261
[epoch6, step2303]: loss 0.707388
[epoch6, step2304]: loss 0.722943
[epoch6, step2305]: loss 0.670411
[epoch6, step2306]: loss 0.666733
[epoch6, step2307]: loss 0.749180
[epoch6, step2308]: loss 0.672822
[epoch6, step2309]: loss 0.798313
[epoch6, step2310]: loss 0.778665
[epoch6, step2311]: loss 0.588333
[epoch6, step2312]: loss 0.536039
[epoch6, step2313]: loss 0.729430
[epoch6, step2314]: loss 0.717976
[epoch6, step2315]: loss 0.605246
[epoch6, step2316]: loss 0.578282
[epoch6, step2317]: loss 0.575523
[epoch6, step2318]: loss 0.723150
[epoch6, step2319]: loss 0.727154
[epoch6, step2320]: loss 0.692552
[epoch6, step2321]: loss 0.665471
[epoch6, step2322]: loss 0.754740
[epoch6, step2323]: loss 0.672886
[epoch6, step2324]: loss 0.625936
[epoch6, step2325]: loss 0.600240
[epoch6, step2326]: loss 0.656521
[epoch6, step2327]: loss 1.028268
[epoch6, step2328]: loss 0.408166
[epoch6, step2329]: loss 0.492130
[epoch6, step2330]: loss 0.754069
[epoch6, step2331]: loss 0.582440
[epoch6, step2332]: loss 0.787076
[epoch6, step2333]: loss 0.707722
[epoch6, step2334]: loss 0.579099
[epoch6, step2335]: loss 0.783825
[epoch6, step2336]: loss 0.594422
[epoch6, step2337]: loss 0.753086
[epoch6, step2338]: loss 0.885135
[epoch6, step2339]: loss 0.702058
[epoch6, step2340]: loss 0.644104
[epoch6, step2341]: loss 0.547254
[epoch6, step2342]: loss 0.734884
[epoch6, step2343]: loss 0.595199
[epoch6, step2344]: loss 0.350805
[epoch6, step2345]: loss 0.458884
[epoch6, step2346]: loss 0.650600
[epoch6, step2347]: loss 0.655536
[epoch6, step2348]: loss 0.871328
[epoch6, step2349]: loss 0.692590
[epoch6, step2350]: loss 0.748044
[epoch6, step2351]: loss 0.524935
[epoch6, step2352]: loss 0.617976
[epoch6, step2353]: loss 0.705698
[epoch6, step2354]: loss 0.588604
[epoch6, step2355]: loss 0.553014
[epoch6, step2356]: loss 0.736428
[epoch6, step2357]: loss 0.649419
[epoch6, step2358]: loss 0.590035
[epoch6, step2359]: loss 0.449221
[epoch6, step2360]: loss 0.506500
[epoch6, step2361]: loss 0.626179
[epoch6, step2362]: loss 0.375969
[epoch6, step2363]: loss 0.819501
[epoch6, step2364]: loss 0.722435
[epoch6, step2365]: loss 0.383510
[epoch6, step2366]: loss 0.549478
[epoch6, step2367]: loss 0.920399
[epoch6, step2368]: loss 0.345377
[epoch6, step2369]: loss 0.586507
[epoch6, step2370]: loss 0.553032
[epoch6, step2371]: loss 0.773422
[epoch6, step2372]: loss 0.767343
[epoch6, step2373]: loss 0.739311
[epoch6, step2374]: loss 0.477975
[epoch6, step2375]: loss 0.473476
[epoch6, step2376]: loss 0.530944
[epoch6, step2377]: loss 0.654863
[epoch6, step2378]: loss 0.686507
[epoch6, step2379]: loss 0.475662
[epoch6, step2380]: loss 0.511555
[epoch6, step2381]: loss 0.776543
[epoch6, step2382]: loss 0.644764
[epoch6, step2383]: loss 0.691843
[epoch6, step2384]: loss 0.802023
[epoch6, step2385]: loss 0.828516
[epoch6, step2386]: loss 0.336911
[epoch6, step2387]: loss 0.760501
[epoch6, step2388]: loss 0.845422
[epoch6, step2389]: loss 0.313881
[epoch6, step2390]: loss 0.701487
[epoch6, step2391]: loss 0.800081
[epoch6, step2392]: loss 0.324344
[epoch6, step2393]: loss 0.743358
[epoch6, step2394]: loss 0.699354
[epoch6, step2395]: loss 0.768267
[epoch6, step2396]: loss 0.642699
[epoch6, step2397]: loss 0.834158
[epoch6, step2398]: loss 0.766762
[epoch6, step2399]: loss 0.716863
[epoch6, step2400]: loss 0.659666
[epoch6, step2401]: loss 0.895114
[epoch6, step2402]: loss 0.542524
[epoch6, step2403]: loss 0.324405
[epoch6, step2404]: loss 0.512528
[epoch6, step2405]: loss 0.561026
[epoch6, step2406]: loss 0.758108
[epoch6, step2407]: loss 0.677419
[epoch6, step2408]: loss 0.707494
[epoch6, step2409]: loss 0.861688
[epoch6, step2410]: loss 0.734955
[epoch6, step2411]: loss 0.949741
[epoch6, step2412]: loss 0.633252
[epoch6, step2413]: loss 0.616818
[epoch6, step2414]: loss 0.826715
[epoch6, step2415]: loss 0.864173
[epoch6, step2416]: loss 0.263866
[epoch6, step2417]: loss 0.561506
[epoch6, step2418]: loss 0.498167
[epoch6, step2419]: loss 0.829157
[epoch6, step2420]: loss 0.450477
[epoch6, step2421]: loss 0.531695
[epoch6, step2422]: loss 0.685783
[epoch6, step2423]: loss 0.610676
[epoch6, step2424]: loss 0.618188
[epoch6, step2425]: loss 0.779253
[epoch6, step2426]: loss 0.621149
[epoch6, step2427]: loss 0.601029
[epoch6, step2428]: loss 0.668414
[epoch6, step2429]: loss 0.638600
[epoch6, step2430]: loss 0.665523
[epoch6, step2431]: loss 0.599871
[epoch6, step2432]: loss 0.839183
[epoch6, step2433]: loss 0.642141
[epoch6, step2434]: loss 0.647804
[epoch6, step2435]: loss 0.661442
[epoch6, step2436]: loss 0.503786
[epoch6, step2437]: loss 0.729274
[epoch6, step2438]: loss 0.542359
[epoch6, step2439]: loss 0.636497
[epoch6, step2440]: loss 0.500962
[epoch6, step2441]: loss 0.806951
[epoch6, step2442]: loss 0.554670
[epoch6, step2443]: loss 0.654550
[epoch6, step2444]: loss 0.629992
[epoch6, step2445]: loss 0.695147
[epoch6, step2446]: loss 0.682346
[epoch6, step2447]: loss 0.806614
[epoch6, step2448]: loss 0.537861
[epoch6, step2449]: loss 0.590591
[epoch6, step2450]: loss 0.657201
[epoch6, step2451]: loss 0.711504
[epoch6, step2452]: loss 0.541198
[epoch6, step2453]: loss 0.500602
[epoch6, step2454]: loss 0.876103
[epoch6, step2455]: loss 0.593341
[epoch6, step2456]: loss 0.928033
[epoch6, step2457]: loss 0.747924
[epoch6, step2458]: loss 0.418651
[epoch6, step2459]: loss 0.619681
[epoch6, step2460]: loss 0.803549
[epoch6, step2461]: loss 0.765069
[epoch6, step2462]: loss 0.695067
[epoch6, step2463]: loss 0.638525
[epoch6, step2464]: loss 0.640630
[epoch6, step2465]: loss 0.464762
[epoch6, step2466]: loss 0.787006
[epoch6, step2467]: loss 0.840513
[epoch6, step2468]: loss 0.433818
[epoch6, step2469]: loss 0.681722
[epoch6, step2470]: loss 0.341446
[epoch6, step2471]: loss 0.806406
[epoch6, step2472]: loss 0.612232
[epoch6, step2473]: loss 0.690834
[epoch6, step2474]: loss 0.709495
[epoch6, step2475]: loss 0.713213
[epoch6, step2476]: loss 0.687708
[epoch6, step2477]: loss 0.745785
[epoch6, step2478]: loss 0.395340
[epoch6, step2479]: loss 0.815829
[epoch6, step2480]: loss 0.545352
[epoch6, step2481]: loss 0.686937
[epoch6, step2482]: loss 0.799128
[epoch6, step2483]: loss 0.360022
[epoch6, step2484]: loss 0.634654
[epoch6, step2485]: loss 0.625188
[epoch6, step2486]: loss 0.409086
[epoch6, step2487]: loss 0.965576
[epoch6, step2488]: loss 0.571847
[epoch6, step2489]: loss 0.504058
[epoch6, step2490]: loss 0.648693
[epoch6, step2491]: loss 0.496960
[epoch6, step2492]: loss 0.683813
[epoch6, step2493]: loss 0.646179
[epoch6, step2494]: loss 0.954326
[epoch6, step2495]: loss 0.866380
[epoch6, step2496]: loss 0.232018
[epoch6, step2497]: loss 0.616019
[epoch6, step2498]: loss 0.781502
[epoch6, step2499]: loss 0.622396
[epoch6, step2500]: loss 0.378715
[epoch6, step2501]: loss 0.636542
[epoch6, step2502]: loss 0.715586
[epoch6, step2503]: loss 0.728151
[epoch6, step2504]: loss 0.533570
[epoch6, step2505]: loss 0.762455
[epoch6, step2506]: loss 0.555257
[epoch6, step2507]: loss 0.698568
[epoch6, step2508]: loss 0.447640
[epoch6, step2509]: loss 0.383786
[epoch6, step2510]: loss 0.851954
[epoch6, step2511]: loss 0.835214
[epoch6, step2512]: loss 0.767211
[epoch6, step2513]: loss 0.680006
[epoch6, step2514]: loss 0.589664
[epoch6, step2515]: loss 0.806665
[epoch6, step2516]: loss 0.554209
[epoch6, step2517]: loss 0.825002
[epoch6, step2518]: loss 0.921338
[epoch6, step2519]: loss 0.575555
[epoch6, step2520]: loss 0.932317
[epoch6, step2521]: loss 0.440451
[epoch6, step2522]: loss 0.663211
[epoch6, step2523]: loss 0.909703
[epoch6, step2524]: loss 0.495605
[epoch6, step2525]: loss 0.638209
[epoch6, step2526]: loss 0.562884
[epoch6, step2527]: loss 0.543393
[epoch6, step2528]: loss 0.930292
[epoch6, step2529]: loss 0.743229
[epoch6, step2530]: loss 0.587287
[epoch6, step2531]: loss 0.741179
[epoch6, step2532]: loss 0.701928
[epoch6, step2533]: loss 0.379501
[epoch6, step2534]: loss 0.761634
[epoch6, step2535]: loss 0.765367
[epoch6, step2536]: loss 0.766699
[epoch6, step2537]: loss 0.567188
[epoch6, step2538]: loss 0.403569
[epoch6, step2539]: loss 0.508910
[epoch6, step2540]: loss 0.878123
[epoch6, step2541]: loss 0.822086
[epoch6, step2542]: loss 0.621312
[epoch6, step2543]: loss 0.496000
[epoch6, step2544]: loss 0.694420
[epoch6, step2545]: loss 0.734389
[epoch6, step2546]: loss 0.761000
[epoch6, step2547]: loss 0.615448
[epoch6, step2548]: loss 0.521286
[epoch6, step2549]: loss 0.616727
[epoch6, step2550]: loss 0.760767
[epoch6, step2551]: loss 0.843061
[epoch6, step2552]: loss 0.846834
[epoch6, step2553]: loss 0.620658
[epoch6, step2554]: loss 0.603725
[epoch6, step2555]: loss 0.712111
[epoch6, step2556]: loss 0.476766
[epoch6, step2557]: loss 0.558189
[epoch6, step2558]: loss 0.721834
[epoch6, step2559]: loss 0.836383
[epoch6, step2560]: loss 0.561928
[epoch6, step2561]: loss 0.743063
[epoch6, step2562]: loss 0.313757
[epoch6, step2563]: loss 0.659595
[epoch6, step2564]: loss 0.620378
[epoch6, step2565]: loss 0.403919
[epoch6, step2566]: loss 0.700926
[epoch6, step2567]: loss 0.474342
[epoch6, step2568]: loss 0.548764
[epoch6, step2569]: loss 0.958980
[epoch6, step2570]: loss 0.885785
[epoch6, step2571]: loss 0.545795
[epoch6, step2572]: loss 0.568756
[epoch6, step2573]: loss 0.457420
[epoch6, step2574]: loss 0.584660
[epoch6, step2575]: loss 0.758277
[epoch6, step2576]: loss 0.780747
[epoch6, step2577]: loss 0.767320
[epoch6, step2578]: loss 0.867432
[epoch6, step2579]: loss 0.791432
[epoch6, step2580]: loss 0.494175
[epoch6, step2581]: loss 0.690774
[epoch6, step2582]: loss 0.497105
[epoch6, step2583]: loss 0.686791
[epoch6, step2584]: loss 0.503913
[epoch6, step2585]: loss 0.406714
[epoch6, step2586]: loss 0.849163
[epoch6, step2587]: loss 0.508025
[epoch6, step2588]: loss 0.614213
[epoch6, step2589]: loss 0.704778
[epoch6, step2590]: loss 0.714005
[epoch6, step2591]: loss 0.546063
[epoch6, step2592]: loss 0.891034
[epoch6, step2593]: loss 0.802762
[epoch6, step2594]: loss 0.522407
[epoch6, step2595]: loss 0.660650
[epoch6, step2596]: loss 0.748103
[epoch6, step2597]: loss 0.528283
[epoch6, step2598]: loss 0.586283
[epoch6, step2599]: loss 0.734873
[epoch6, step2600]: loss 0.650499
[epoch6, step2601]: loss 0.323353
[epoch6, step2602]: loss 0.599194
[epoch6, step2603]: loss 0.586767
[epoch6, step2604]: loss 0.659586
[epoch6, step2605]: loss 0.844014
[epoch6, step2606]: loss 0.605971
[epoch6, step2607]: loss 0.769705
[epoch6, step2608]: loss 0.620701
[epoch6, step2609]: loss 0.726030
[epoch6, step2610]: loss 0.615217
[epoch6, step2611]: loss 0.692539
[epoch6, step2612]: loss 0.761067
[epoch6, step2613]: loss 0.526088
[epoch6, step2614]: loss 0.432972
[epoch6, step2615]: loss 0.473080
[epoch6, step2616]: loss 0.611097
[epoch6, step2617]: loss 0.749439
[epoch6, step2618]: loss 0.746142
[epoch6, step2619]: loss 0.760775
[epoch6, step2620]: loss 0.398968
[epoch6, step2621]: loss 0.420086
[epoch6, step2622]: loss 0.506880
[epoch6, step2623]: loss 0.632124
[epoch6, step2624]: loss 0.583824
[epoch6, step2625]: loss 0.951931
[epoch6, step2626]: loss 0.768506
[epoch6, step2627]: loss 0.640333
[epoch6, step2628]: loss 0.603676
[epoch6, step2629]: loss 0.352689
[epoch6, step2630]: loss 0.657078
[epoch6, step2631]: loss 0.624862
[epoch6, step2632]: loss 0.798485
[epoch6, step2633]: loss 0.574812
[epoch6, step2634]: loss 0.696243
[epoch6, step2635]: loss 0.848565
[epoch6, step2636]: loss 0.737116
[epoch6, step2637]: loss 0.773172
[epoch6, step2638]: loss 0.484186
[epoch6, step2639]: loss 0.685176
[epoch6, step2640]: loss 0.775675
[epoch6, step2641]: loss 0.855480
[epoch6, step2642]: loss 0.335177
[epoch6, step2643]: loss 0.925282
[epoch6, step2644]: loss 0.648318
[epoch6, step2645]: loss 0.900107
[epoch6, step2646]: loss 0.951834
[epoch6, step2647]: loss 0.637915
[epoch6, step2648]: loss 0.803730
[epoch6, step2649]: loss 0.791291
[epoch6, step2650]: loss 0.614399
[epoch6, step2651]: loss 0.665615
[epoch6, step2652]: loss 0.831093
[epoch6, step2653]: loss 0.865386
[epoch6, step2654]: loss 0.710473
[epoch6, step2655]: loss 0.732427
[epoch6, step2656]: loss 0.669974
[epoch6, step2657]: loss 0.360681
[epoch6, step2658]: loss 0.780757
[epoch6, step2659]: loss 0.758135
[epoch6, step2660]: loss 0.345929
[epoch6, step2661]: loss 0.704823
[epoch6, step2662]: loss 0.519313
[epoch6, step2663]: loss 0.746162
[epoch6, step2664]: loss 0.597602
[epoch6, step2665]: loss 0.735619
[epoch6, step2666]: loss 0.719481
[epoch6, step2667]: loss 0.840012
[epoch6, step2668]: loss 0.709260
[epoch6, step2669]: loss 0.679752
[epoch6, step2670]: loss 0.917885
[epoch6, step2671]: loss 0.793652
[epoch6, step2672]: loss 0.238514
[epoch6, step2673]: loss 0.742830
[epoch6, step2674]: loss 0.696812
[epoch6, step2675]: loss 0.621695
[epoch6, step2676]: loss 0.674477
[epoch6, step2677]: loss 0.744528
[epoch6, step2678]: loss 0.943130
[epoch6, step2679]: loss 0.891705
[epoch6, step2680]: loss 0.567491
[epoch6, step2681]: loss 0.710367
[epoch6, step2682]: loss 0.697279
[epoch6, step2683]: loss 0.306290
[epoch6, step2684]: loss 0.764318
[epoch6, step2685]: loss 0.506716
[epoch6, step2686]: loss 0.808238
[epoch6, step2687]: loss 0.548693
[epoch6, step2688]: loss 0.866391
[epoch6, step2689]: loss 0.635480
[epoch6, step2690]: loss 0.631402
[epoch6, step2691]: loss 0.774156
[epoch6, step2692]: loss 0.687959
[epoch6, step2693]: loss 0.802363
[epoch6, step2694]: loss 0.772719
[epoch6, step2695]: loss 0.829728
[epoch6, step2696]: loss 0.448275
[epoch6, step2697]: loss 0.601168
[epoch6, step2698]: loss 0.629707
[epoch6, step2699]: loss 0.691423
[epoch6, step2700]: loss 0.644276
[epoch6, step2701]: loss 0.454103
[epoch6, step2702]: loss 0.675899
[epoch6, step2703]: loss 0.710477
[epoch6, step2704]: loss 0.640744
[epoch6, step2705]: loss 0.665030
[epoch6, step2706]: loss 0.446972
[epoch6, step2707]: loss 0.511197
[epoch6, step2708]: loss 0.577763
[epoch6, step2709]: loss 0.691376
[epoch6, step2710]: loss 0.718144
[epoch6, step2711]: loss 0.207555
[epoch6, step2712]: loss 0.726400
[epoch6, step2713]: loss 0.543196
[epoch6, step2714]: loss 0.592779
[epoch6, step2715]: loss 0.776219
[epoch6, step2716]: loss 0.693281
[epoch6, step2717]: loss 0.756404
[epoch6, step2718]: loss 0.526189
[epoch6, step2719]: loss 0.346974
[epoch6, step2720]: loss 0.715018
[epoch6, step2721]: loss 0.664244
[epoch6, step2722]: loss 0.464500
[epoch6, step2723]: loss 0.666963
[epoch6, step2724]: loss 0.796065
[epoch6, step2725]: loss 0.678459
[epoch6, step2726]: loss 0.425525
[epoch6, step2727]: loss 0.475841
[epoch6, step2728]: loss 0.655712
[epoch6, step2729]: loss 0.896453
[epoch6, step2730]: loss 0.659818
[epoch6, step2731]: loss 0.577297
[epoch6, step2732]: loss 0.633362
[epoch6, step2733]: loss 1.068251
[epoch6, step2734]: loss 0.691149
[epoch6, step2735]: loss 0.638984
[epoch6, step2736]: loss 0.595048
[epoch6, step2737]: loss 0.637589
[epoch6, step2738]: loss 0.410197
[epoch6, step2739]: loss 0.366900
[epoch6, step2740]: loss 0.697818
[epoch6, step2741]: loss 0.623067
[epoch6, step2742]: loss 0.627632
[epoch6, step2743]: loss 0.592023
[epoch6, step2744]: loss 0.462726
[epoch6, step2745]: loss 0.869874
[epoch6, step2746]: loss 0.793191
[epoch6, step2747]: loss 0.590641
[epoch6, step2748]: loss 0.584099
[epoch6, step2749]: loss 0.732498
[epoch6, step2750]: loss 0.607994
[epoch6, step2751]: loss 0.604645
[epoch6, step2752]: loss 0.344279
[epoch6, step2753]: loss 0.913744
[epoch6, step2754]: loss 0.703569
[epoch6, step2755]: loss 0.843500
[epoch6, step2756]: loss 0.520721
[epoch6, step2757]: loss 0.787516
[epoch6, step2758]: loss 0.827930
[epoch6, step2759]: loss 0.721195
[epoch6, step2760]: loss 0.815794
[epoch6, step2761]: loss 0.637319
[epoch6, step2762]: loss 0.818171
[epoch6, step2763]: loss 0.536547
[epoch6, step2764]: loss 0.757631
[epoch6, step2765]: loss 0.826074
[epoch6, step2766]: loss 0.635083
[epoch6, step2767]: loss 0.580092
[epoch6, step2768]: loss 0.770429
[epoch6, step2769]: loss 0.724850
[epoch6, step2770]: loss 0.870777
[epoch6, step2771]: loss 0.442948
[epoch6, step2772]: loss 0.650234
[epoch6, step2773]: loss 0.548546
[epoch6, step2774]: loss 0.294792
[epoch6, step2775]: loss 0.651015
[epoch6, step2776]: loss 0.623258
[epoch6, step2777]: loss 0.368487
[epoch6, step2778]: loss 0.714170
[epoch6, step2779]: loss 0.688562
[epoch6, step2780]: loss 0.848948
[epoch6, step2781]: loss 0.683618
[epoch6, step2782]: loss 0.667716
[epoch6, step2783]: loss 0.683071
[epoch6, step2784]: loss 0.647463
[epoch6, step2785]: loss 0.453809
[epoch6, step2786]: loss 0.952148
[epoch6, step2787]: loss 0.864277
[epoch6, step2788]: loss 0.661564
[epoch6, step2789]: loss 0.793322
[epoch6, step2790]: loss 0.480280
[epoch6, step2791]: loss 0.636730
[epoch6, step2792]: loss 0.485048
[epoch6, step2793]: loss 0.496177
[epoch6, step2794]: loss 0.607907
[epoch6, step2795]: loss 0.748757
[epoch6, step2796]: loss 0.878963
[epoch6, step2797]: loss 0.744543
[epoch6, step2798]: loss 0.652489
[epoch6, step2799]: loss 0.633015
[epoch6, step2800]: loss 0.593021
[epoch6, step2801]: loss 0.760190
[epoch6, step2802]: loss 0.813691
[epoch6, step2803]: loss 0.360561
[epoch6, step2804]: loss 0.699093
[epoch6, step2805]: loss 0.702487
[epoch6, step2806]: loss 0.708686
[epoch6, step2807]: loss 0.689975
[epoch6, step2808]: loss 0.635425
[epoch6, step2809]: loss 0.995818
[epoch6, step2810]: loss 0.777098
[epoch6, step2811]: loss 0.853212
[epoch6, step2812]: loss 0.537482
[epoch6, step2813]: loss 0.387802
[epoch6, step2814]: loss 0.600517
[epoch6, step2815]: loss 0.680390
[epoch6, step2816]: loss 0.640990
[epoch6, step2817]: loss 0.808957
[epoch6, step2818]: loss 0.780575
[epoch6, step2819]: loss 0.558390
[epoch6, step2820]: loss 0.677175
[epoch6, step2821]: loss 0.830168
[epoch6, step2822]: loss 0.701175
[epoch6, step2823]: loss 0.701974
[epoch6, step2824]: loss 0.787231
[epoch6, step2825]: loss 0.832680
[epoch6, step2826]: loss 0.405866
[epoch6, step2827]: loss 0.444554
[epoch6, step2828]: loss 0.888267
[epoch6, step2829]: loss 0.736941
[epoch6, step2830]: loss 0.879273
[epoch6, step2831]: loss 0.616461
[epoch6, step2832]: loss 0.701180
[epoch6, step2833]: loss 0.661815
[epoch6, step2834]: loss 0.567555
[epoch6, step2835]: loss 0.784896
[epoch6, step2836]: loss 0.821262
[epoch6, step2837]: loss 0.606347
[epoch6, step2838]: loss 0.523618
[epoch6, step2839]: loss 0.548104
[epoch6, step2840]: loss 0.698831
[epoch6, step2841]: loss 0.426376
[epoch6, step2842]: loss 0.843617
[epoch6, step2843]: loss 0.738906
[epoch6, step2844]: loss 0.582524
[epoch6, step2845]: loss 0.635826
[epoch6, step2846]: loss 0.563140
[epoch6, step2847]: loss 0.901346
[epoch6, step2848]: loss 0.537979
[epoch6, step2849]: loss 0.742119
[epoch6, step2850]: loss 0.701818
[epoch6, step2851]: loss 0.692661
[epoch6, step2852]: loss 0.431898
[epoch6, step2853]: loss 0.877308
[epoch6, step2854]: loss 0.827668
[epoch6, step2855]: loss 0.637558
[epoch6, step2856]: loss 0.703131
[epoch6, step2857]: loss 0.711805
[epoch6, step2858]: loss 0.811221
[epoch6, step2859]: loss 0.565705
[epoch6, step2860]: loss 0.585528
[epoch6, step2861]: loss 0.699449
[epoch6, step2862]: loss 0.803164
[epoch6, step2863]: loss 0.285192
[epoch6, step2864]: loss 0.871567
[epoch6, step2865]: loss 0.416386
[epoch6, step2866]: loss 0.533391
[epoch6, step2867]: loss 0.515114
[epoch6, step2868]: loss 0.519163
[epoch6, step2869]: loss 0.548964
[epoch6, step2870]: loss 0.715900
[epoch6, step2871]: loss 0.509618
[epoch6, step2872]: loss 0.599670
[epoch6, step2873]: loss 0.613206
[epoch6, step2874]: loss 0.546054
[epoch6, step2875]: loss 0.565808
[epoch6, step2876]: loss 0.832358
[epoch6, step2877]: loss 0.485754
[epoch6, step2878]: loss 0.692575
[epoch6, step2879]: loss 0.647348
[epoch6, step2880]: loss 0.683371
[epoch6, step2881]: loss 0.658828
[epoch6, step2882]: loss 0.457655
[epoch6, step2883]: loss 0.657934
[epoch6, step2884]: loss 0.728932
[epoch6, step2885]: loss 0.609389
[epoch6, step2886]: loss 0.519479
[epoch6, step2887]: loss 0.502152
[epoch6, step2888]: loss 0.497796
[epoch6, step2889]: loss 0.529804
[epoch6, step2890]: loss 0.742653
[epoch6, step2891]: loss 0.667312
[epoch6, step2892]: loss 0.870166
[epoch6, step2893]: loss 0.465073
[epoch6, step2894]: loss 0.561978
[epoch6, step2895]: loss 0.471198
[epoch6, step2896]: loss 0.522207
[epoch6, step2897]: loss 0.746728
[epoch6, step2898]: loss 0.778352
[epoch6, step2899]: loss 0.352948
[epoch6, step2900]: loss 0.822871
[epoch6, step2901]: loss 0.395000
[epoch6, step2902]: loss 0.311547
[epoch6, step2903]: loss 0.608017
[epoch6, step2904]: loss 0.570187
[epoch6, step2905]: loss 0.615280
[epoch6, step2906]: loss 0.482035
[epoch6, step2907]: loss 0.811117
[epoch6, step2908]: loss 0.645297
[epoch6, step2909]: loss 0.606506
[epoch6, step2910]: loss 0.658517
[epoch6, step2911]: loss 0.732576
[epoch6, step2912]: loss 0.757860
[epoch6, step2913]: loss 0.582131
[epoch6, step2914]: loss 0.666950
[epoch6, step2915]: loss 0.565184
[epoch6, step2916]: loss 0.531821
[epoch6, step2917]: loss 0.687383
[epoch6, step2918]: loss 0.446408
[epoch6, step2919]: loss 0.821255
[epoch6, step2920]: loss 0.710912
[epoch6, step2921]: loss 0.481824
[epoch6, step2922]: loss 0.629991
[epoch6, step2923]: loss 0.857976
[epoch6, step2924]: loss 0.742104
[epoch6, step2925]: loss 0.751773
[epoch6, step2926]: loss 0.553019
[epoch6, step2927]: loss 0.354156
[epoch6, step2928]: loss 0.392102
[epoch6, step2929]: loss 0.592975
[epoch6, step2930]: loss 0.768912
[epoch6, step2931]: loss 0.632625
[epoch6, step2932]: loss 0.551817
[epoch6, step2933]: loss 0.265913
[epoch6, step2934]: loss 0.793766
[epoch6, step2935]: loss 0.694565
[epoch6, step2936]: loss 0.538195
[epoch6, step2937]: loss 0.594496
[epoch6, step2938]: loss 0.692276
[epoch6, step2939]: loss 0.761980
[epoch6, step2940]: loss 0.675795
[epoch6, step2941]: loss 0.485250
[epoch6, step2942]: loss 0.614872
[epoch6, step2943]: loss 0.460053
[epoch6, step2944]: loss 0.706543
[epoch6, step2945]: loss 0.267823
[epoch6, step2946]: loss 0.524358
[epoch6, step2947]: loss 0.696563
[epoch6, step2948]: loss 0.567940
[epoch6, step2949]: loss 0.556228
[epoch6, step2950]: loss 1.083569
[epoch6, step2951]: loss 0.553125
[epoch6, step2952]: loss 0.784709
[epoch6, step2953]: loss 0.780053
[epoch6, step2954]: loss 0.786819
[epoch6, step2955]: loss 0.536157
[epoch6, step2956]: loss 0.651822
[epoch6, step2957]: loss 0.732109
[epoch6, step2958]: loss 0.385661
[epoch6, step2959]: loss 0.769711
[epoch6, step2960]: loss 0.934101
[epoch6, step2961]: loss 0.468948
[epoch6, step2962]: loss 0.907745
[epoch6, step2963]: loss 0.890294
[epoch6, step2964]: loss 0.448160
[epoch6, step2965]: loss 0.311999
[epoch6, step2966]: loss 0.672228
[epoch6, step2967]: loss 0.282199
[epoch6, step2968]: loss 0.751336
[epoch6, step2969]: loss 0.808556
[epoch6, step2970]: loss 0.681737
[epoch6, step2971]: loss 0.193998
[epoch6, step2972]: loss 0.661406
[epoch6, step2973]: loss 0.800295
[epoch6, step2974]: loss 0.851847
[epoch6, step2975]: loss 0.647953
[epoch6, step2976]: loss 0.722254
[epoch6, step2977]: loss 0.763279
[epoch6, step2978]: loss 0.860420
[epoch6, step2979]: loss 0.913645
[epoch6, step2980]: loss 0.774943
[epoch6, step2981]: loss 0.575639
[epoch6, step2982]: loss 0.789891
[epoch6, step2983]: loss 0.801339
[epoch6, step2984]: loss 0.594340
[epoch6, step2985]: loss 0.539996
[epoch6, step2986]: loss 0.476426
[epoch6, step2987]: loss 0.482483
[epoch6, step2988]: loss 0.539607
[epoch6, step2989]: loss 0.580151
[epoch6, step2990]: loss 0.888956
[epoch6, step2991]: loss 0.685818
[epoch6, step2992]: loss 0.743851
[epoch6, step2993]: loss 0.734376
[epoch6, step2994]: loss 0.734865
[epoch6, step2995]: loss 0.316508
[epoch6, step2996]: loss 0.878658
[epoch6, step2997]: loss 0.619230
[epoch6, step2998]: loss 0.641926
[epoch6, step2999]: loss 0.497048
[epoch6, step3000]: loss 0.659797
[epoch6, step3001]: loss 0.530893
[epoch6, step3002]: loss 0.737372
[epoch6, step3003]: loss 0.540700
[epoch6, step3004]: loss 0.495941
[epoch6, step3005]: loss 0.773529
[epoch6, step3006]: loss 0.573578
[epoch6, step3007]: loss 0.501928
[epoch6, step3008]: loss 0.622818
[epoch6, step3009]: loss 0.601870
[epoch6, step3010]: loss 0.578819
[epoch6, step3011]: loss 0.648531
[epoch6, step3012]: loss 0.801935
[epoch6, step3013]: loss 0.658972
[epoch6, step3014]: loss 0.684071
[epoch6, step3015]: loss 0.623320
[epoch6, step3016]: loss 0.730353
[epoch6, step3017]: loss 0.647005
[epoch6, step3018]: loss 0.825597
[epoch6, step3019]: loss 0.732731
[epoch6, step3020]: loss 0.576597
[epoch6, step3021]: loss 0.577683
[epoch6, step3022]: loss 0.552878
[epoch6, step3023]: loss 0.752749
[epoch6, step3024]: loss 0.621254
[epoch6, step3025]: loss 0.597633
[epoch6, step3026]: loss 0.563274
[epoch6, step3027]: loss 0.655532
[epoch6, step3028]: loss 0.598204
[epoch6, step3029]: loss 0.822860
[epoch6, step3030]: loss 0.847379
[epoch6, step3031]: loss 0.654099
[epoch6, step3032]: loss 0.708634
[epoch6, step3033]: loss 0.798863
[epoch6, step3034]: loss 0.761048
[epoch6, step3035]: loss 0.938488
[epoch6, step3036]: loss 0.683962
[epoch6, step3037]: loss 0.896847
[epoch6, step3038]: loss 0.580794
[epoch6, step3039]: loss 0.800158
[epoch6, step3040]: loss 0.553568
[epoch6, step3041]: loss 0.778166
[epoch6, step3042]: loss 0.611226
[epoch6, step3043]: loss 0.739258
[epoch6, step3044]: loss 0.483164
[epoch6, step3045]: loss 0.814062
[epoch6, step3046]: loss 0.442218
[epoch6, step3047]: loss 0.426024
[epoch6, step3048]: loss 0.692578
[epoch6, step3049]: loss 0.687563
[epoch6, step3050]: loss 0.793774
[epoch6, step3051]: loss 0.569520
[epoch6, step3052]: loss 0.743461
[epoch6, step3053]: loss 0.389979
[epoch6, step3054]: loss 0.626398
[epoch6, step3055]: loss 0.688280
[epoch6, step3056]: loss 0.733805
[epoch6, step3057]: loss 0.833108
[epoch6, step3058]: loss 0.514652
[epoch6, step3059]: loss 0.549692
[epoch6, step3060]: loss 0.590190
[epoch6, step3061]: loss 0.762913
[epoch6, step3062]: loss 0.808641
[epoch6, step3063]: loss 0.727378
[epoch6, step3064]: loss 0.484307
[epoch6, step3065]: loss 0.621107
[epoch6, step3066]: loss 0.705251
[epoch6, step3067]: loss 0.580504
[epoch6, step3068]: loss 0.437581
[epoch6, step3069]: loss 0.678900
[epoch6, step3070]: loss 0.639715
[epoch6, step3071]: loss 1.011946
[epoch6, step3072]: loss 0.525874
[epoch6, step3073]: loss 0.785744
[epoch6, step3074]: loss 0.518123
[epoch6, step3075]: loss 0.417863
[epoch6, step3076]: loss 0.616717

[epoch6]: avg loss 0.616717

[epoch7, step1]: loss 0.930344
[epoch7, step2]: loss 0.785534
[epoch7, step3]: loss 0.606461
[epoch7, step4]: loss 0.815827
[epoch7, step5]: loss 0.550418
[epoch7, step6]: loss 0.527654
[epoch7, step7]: loss 0.727665
[epoch7, step8]: loss 0.503705
[epoch7, step9]: loss 0.327481
[epoch7, step10]: loss 0.763819
[epoch7, step11]: loss 0.940592
[epoch7, step12]: loss 0.771851
[epoch7, step13]: loss 0.462282
[epoch7, step14]: loss 0.435913
[epoch7, step15]: loss 0.720132
[epoch7, step16]: loss 0.706988
[epoch7, step17]: loss 0.570106
[epoch7, step18]: loss 0.853659
[epoch7, step19]: loss 0.328746
[epoch7, step20]: loss 0.712045
[epoch7, step21]: loss 0.633800
[epoch7, step22]: loss 0.629520
[epoch7, step23]: loss 0.805200
[epoch7, step24]: loss 0.542175
[epoch7, step25]: loss 0.588516
[epoch7, step26]: loss 0.761947
[epoch7, step27]: loss 0.636021
[epoch7, step28]: loss 0.800085
[epoch7, step29]: loss 0.648079
[epoch7, step30]: loss 0.488684
[epoch7, step31]: loss 0.715370
[epoch7, step32]: loss 0.744456
[epoch7, step33]: loss 0.696405
[epoch7, step34]: loss 0.676117
[epoch7, step35]: loss 0.745357
[epoch7, step36]: loss 0.843323
[epoch7, step37]: loss 0.710911
[epoch7, step38]: loss 0.659310
[epoch7, step39]: loss 0.774561
[epoch7, step40]: loss 0.864047
[epoch7, step41]: loss 0.632999
[epoch7, step42]: loss 0.674858
[epoch7, step43]: loss 0.450423
[epoch7, step44]: loss 0.587384
[epoch7, step45]: loss 0.668492
[epoch7, step46]: loss 0.689573
[epoch7, step47]: loss 0.799211
[epoch7, step48]: loss 0.458617
[epoch7, step49]: loss 0.758368
[epoch7, step50]: loss 0.640304
[epoch7, step51]: loss 0.739089
[epoch7, step52]: loss 0.714541
[epoch7, step53]: loss 0.629102
[epoch7, step54]: loss 0.731194
[epoch7, step55]: loss 0.466003
[epoch7, step56]: loss 0.700442
[epoch7, step57]: loss 0.659466
[epoch7, step58]: loss 0.600482
[epoch7, step59]: loss 0.606510
[epoch7, step60]: loss 0.573986
[epoch7, step61]: loss 0.643161
[epoch7, step62]: loss 0.769401
[epoch7, step63]: loss 0.654232
[epoch7, step64]: loss 0.524971
[epoch7, step65]: loss 0.271089
[epoch7, step66]: loss 0.480155
[epoch7, step67]: loss 0.622399
[epoch7, step68]: loss 0.772765
[epoch7, step69]: loss 0.540532
[epoch7, step70]: loss 0.492044
[epoch7, step71]: loss 0.600126
[epoch7, step72]: loss 0.322916
[epoch7, step73]: loss 0.580992
[epoch7, step74]: loss 0.644466
[epoch7, step75]: loss 0.723688
[epoch7, step76]: loss 0.688018
[epoch7, step77]: loss 0.485405
[epoch7, step78]: loss 0.643131
[epoch7, step79]: loss 0.903972
[epoch7, step80]: loss 0.834335
[epoch7, step81]: loss 0.691106
[epoch7, step82]: loss 0.622808
[epoch7, step83]: loss 0.577107
[epoch7, step84]: loss 0.325261
[epoch7, step85]: loss 0.899367
[epoch7, step86]: loss 0.524230
[epoch7, step87]: loss 0.724374
[epoch7, step88]: loss 0.653688
[epoch7, step89]: loss 0.404640
[epoch7, step90]: loss 0.925269
[epoch7, step91]: loss 0.364876
[epoch7, step92]: loss 0.370388
[epoch7, step93]: loss 0.813672
[epoch7, step94]: loss 0.557103
[epoch7, step95]: loss 0.788441
[epoch7, step96]: loss 0.781085
[epoch7, step97]: loss 0.756573
[epoch7, step98]: loss 0.664224
[epoch7, step99]: loss 0.944872
[epoch7, step100]: loss 0.773353
[epoch7, step101]: loss 0.406730
[epoch7, step102]: loss 0.454774
[epoch7, step103]: loss 0.520639
[epoch7, step104]: loss 0.416929
[epoch7, step105]: loss 0.557162
[epoch7, step106]: loss 0.648659
[epoch7, step107]: loss 0.669429
[epoch7, step108]: loss 0.534246
[epoch7, step109]: loss 0.768604
[epoch7, step110]: loss 0.698104
[epoch7, step111]: loss 0.610115
[epoch7, step112]: loss 0.581309
[epoch7, step113]: loss 0.613187
[epoch7, step114]: loss 0.758572
[epoch7, step115]: loss 0.793810
[epoch7, step116]: loss 0.694670
[epoch7, step117]: loss 0.668383
[epoch7, step118]: loss 0.460390
[epoch7, step119]: loss 0.896552
[epoch7, step120]: loss 0.622879
[epoch7, step121]: loss 0.689061
[epoch7, step122]: loss 0.693187
[epoch7, step123]: loss 0.660642
[epoch7, step124]: loss 0.696150
[epoch7, step125]: loss 0.494191
[epoch7, step126]: loss 0.963553
[epoch7, step127]: loss 0.826918
[epoch7, step128]: loss 0.709817
[epoch7, step129]: loss 0.743476
[epoch7, step130]: loss 0.472696
[epoch7, step131]: loss 0.685898
[epoch7, step132]: loss 0.716943
[epoch7, step133]: loss 0.976533
[epoch7, step134]: loss 0.797146
[epoch7, step135]: loss 0.749609
[epoch7, step136]: loss 0.898756
[epoch7, step137]: loss 0.720091
[epoch7, step138]: loss 0.676853
[epoch7, step139]: loss 0.732009
[epoch7, step140]: loss 0.814754
[epoch7, step141]: loss 0.400593
[epoch7, step142]: loss 0.502846
[epoch7, step143]: loss 0.676609
[epoch7, step144]: loss 0.983520
[epoch7, step145]: loss 0.603429
[epoch7, step146]: loss 0.468561
[epoch7, step147]: loss 0.573233
[epoch7, step148]: loss 0.760851
[epoch7, step149]: loss 0.474182
[epoch7, step150]: loss 0.650841
[epoch7, step151]: loss 0.783948
[epoch7, step152]: loss 0.610879
[epoch7, step153]: loss 0.622942
[epoch7, step154]: loss 0.586410
[epoch7, step155]: loss 0.595655
[epoch7, step156]: loss 0.578567
[epoch7, step157]: loss 0.850598
[epoch7, step158]: loss 0.830644
[epoch7, step159]: loss 0.391168
[epoch7, step160]: loss 0.956778
[epoch7, step161]: loss 0.764417
[epoch7, step162]: loss 0.724605
[epoch7, step163]: loss 0.735398
[epoch7, step164]: loss 0.691540
[epoch7, step165]: loss 0.279331
[epoch7, step166]: loss 0.876941
[epoch7, step167]: loss 0.699441
[epoch7, step168]: loss 0.866070
[epoch7, step169]: loss 0.779463
[epoch7, step170]: loss 0.503871
[epoch7, step171]: loss 0.577922
[epoch7, step172]: loss 0.398350
[epoch7, step173]: loss 0.763685
[epoch7, step174]: loss 0.542969
[epoch7, step175]: loss 0.535775
[epoch7, step176]: loss 0.590679
[epoch7, step177]: loss 0.494086
[epoch7, step178]: loss 0.502429
[epoch7, step179]: loss 0.815356
[epoch7, step180]: loss 0.643401
[epoch7, step181]: loss 0.723481
[epoch7, step182]: loss 0.783313
[epoch7, step183]: loss 0.670711
[epoch7, step184]: loss 0.590897
[epoch7, step185]: loss 0.575571
[epoch7, step186]: loss 1.016388
[epoch7, step187]: loss 0.819210
[epoch7, step188]: loss 0.528873
[epoch7, step189]: loss 0.563562
[epoch7, step190]: loss 0.670909
[epoch7, step191]: loss 0.529310
[epoch7, step192]: loss 0.679152
[epoch7, step193]: loss 0.171244
[epoch7, step194]: loss 0.543797
[epoch7, step195]: loss 0.773514
[epoch7, step196]: loss 0.790661
[epoch7, step197]: loss 0.721375
[epoch7, step198]: loss 0.869417
[epoch7, step199]: loss 0.825045
[epoch7, step200]: loss 0.642811
[epoch7, step201]: loss 0.748134
[epoch7, step202]: loss 0.733508
[epoch7, step203]: loss 0.583200
[epoch7, step204]: loss 0.727825
[epoch7, step205]: loss 0.721814
[epoch7, step206]: loss 0.664857
[epoch7, step207]: loss 0.550349
[epoch7, step208]: loss 0.649827
[epoch7, step209]: loss 0.473081
[epoch7, step210]: loss 0.598002
[epoch7, step211]: loss 0.597829
[epoch7, step212]: loss 0.706765
[epoch7, step213]: loss 0.646923
[epoch7, step214]: loss 0.589434
[epoch7, step215]: loss 0.690528
[epoch7, step216]: loss 0.719363
[epoch7, step217]: loss 0.750775
[epoch7, step218]: loss 0.741587
[epoch7, step219]: loss 0.671731
[epoch7, step220]: loss 0.691170
[epoch7, step221]: loss 0.362284
[epoch7, step222]: loss 0.439645
[epoch7, step223]: loss 0.782598
[epoch7, step224]: loss 0.935807
[epoch7, step225]: loss 0.644115
[epoch7, step226]: loss 0.840284
[epoch7, step227]: loss 0.685426
[epoch7, step228]: loss 0.941465
[epoch7, step229]: loss 0.839628
[epoch7, step230]: loss 0.847539
[epoch7, step231]: loss 0.619451
[epoch7, step232]: loss 0.648595
[epoch7, step233]: loss 0.764326
[epoch7, step234]: loss 0.648176
[epoch7, step235]: loss 0.828476
[epoch7, step236]: loss 0.415811
[epoch7, step237]: loss 0.869793
[epoch7, step238]: loss 0.497898
[epoch7, step239]: loss 0.893057
[epoch7, step240]: loss 0.642589
[epoch7, step241]: loss 0.390625
[epoch7, step242]: loss 0.737925
[epoch7, step243]: loss 0.527335
[epoch7, step244]: loss 0.656281
[epoch7, step245]: loss 0.641282
[epoch7, step246]: loss 0.665364
[epoch7, step247]: loss 0.796793
[epoch7, step248]: loss 0.713789
[epoch7, step249]: loss 0.550961
[epoch7, step250]: loss 0.586100
[epoch7, step251]: loss 0.899458
[epoch7, step252]: loss 0.780935
[epoch7, step253]: loss 0.769226
[epoch7, step254]: loss 0.781851
[epoch7, step255]: loss 0.662974
[epoch7, step256]: loss 0.746992
[epoch7, step257]: loss 0.653554
[epoch7, step258]: loss 0.807812
[epoch7, step259]: loss 0.694920
[epoch7, step260]: loss 0.717009
[epoch7, step261]: loss 0.971136
[epoch7, step262]: loss 0.523160
[epoch7, step263]: loss 0.838399
[epoch7, step264]: loss 0.538073
[epoch7, step265]: loss 0.725776
[epoch7, step266]: loss 0.726600
[epoch7, step267]: loss 0.657189
[epoch7, step268]: loss 0.664958
[epoch7, step269]: loss 0.705963
[epoch7, step270]: loss 0.528082
[epoch7, step271]: loss 0.813505
[epoch7, step272]: loss 0.464756
[epoch7, step273]: loss 0.702099
[epoch7, step274]: loss 0.664450
[epoch7, step275]: loss 0.843616
[epoch7, step276]: loss 0.515328
[epoch7, step277]: loss 0.889106
[epoch7, step278]: loss 0.691229
[epoch7, step279]: loss 0.684783
[epoch7, step280]: loss 0.704127
[epoch7, step281]: loss 0.692954
[epoch7, step282]: loss 0.676717
[epoch7, step283]: loss 0.648674
[epoch7, step284]: loss 0.874047
[epoch7, step285]: loss 0.447170
[epoch7, step286]: loss 0.484558
[epoch7, step287]: loss 0.671967
[epoch7, step288]: loss 0.779846
[epoch7, step289]: loss 0.613102
[epoch7, step290]: loss 0.421648
[epoch7, step291]: loss 0.813276
[epoch7, step292]: loss 0.576657
[epoch7, step293]: loss 0.711390
[epoch7, step294]: loss 0.575072
[epoch7, step295]: loss 0.622654
[epoch7, step296]: loss 0.681573
[epoch7, step297]: loss 0.833130
[epoch7, step298]: loss 0.616558
[epoch7, step299]: loss 0.794307
[epoch7, step300]: loss 0.660813
[epoch7, step301]: loss 0.655314
[epoch7, step302]: loss 0.580136
[epoch7, step303]: loss 0.388521
[epoch7, step304]: loss 0.553152
[epoch7, step305]: loss 0.524184
[epoch7, step306]: loss 0.564504
[epoch7, step307]: loss 0.552412
[epoch7, step308]: loss 0.915509
[epoch7, step309]: loss 0.726189
[epoch7, step310]: loss 0.867131
[epoch7, step311]: loss 0.467556
[epoch7, step312]: loss 0.722674
[epoch7, step313]: loss 0.737771
[epoch7, step314]: loss 0.505151
[epoch7, step315]: loss 0.877948
[epoch7, step316]: loss 0.736282
[epoch7, step317]: loss 0.650691
[epoch7, step318]: loss 0.607971
[epoch7, step319]: loss 0.577188
[epoch7, step320]: loss 0.477380
[epoch7, step321]: loss 0.814235
[epoch7, step322]: loss 0.285060
[epoch7, step323]: loss 0.348941
[epoch7, step324]: loss 0.478737
[epoch7, step325]: loss 0.540636
[epoch7, step326]: loss 0.843924
[epoch7, step327]: loss 0.539363
[epoch7, step328]: loss 0.601428
[epoch7, step329]: loss 0.528960
[epoch7, step330]: loss 0.753874
[epoch7, step331]: loss 0.872059
[epoch7, step332]: loss 0.599238
[epoch7, step333]: loss 0.624450
[epoch7, step334]: loss 0.758274
[epoch7, step335]: loss 0.633491
[epoch7, step336]: loss 0.994239
[epoch7, step337]: loss 0.910532
[epoch7, step338]: loss 0.798528
[epoch7, step339]: loss 0.487781
[epoch7, step340]: loss 0.541616
[epoch7, step341]: loss 0.792818
[epoch7, step342]: loss 0.684089
[epoch7, step343]: loss 0.783740
[epoch7, step344]: loss 0.389108
[epoch7, step345]: loss 0.592263
[epoch7, step346]: loss 0.655710
[epoch7, step347]: loss 0.615916
[epoch7, step348]: loss 0.774770
[epoch7, step349]: loss 0.551717
[epoch7, step350]: loss 0.568951
[epoch7, step351]: loss 0.607445
[epoch7, step352]: loss 0.397208
[epoch7, step353]: loss 0.278673
[epoch7, step354]: loss 0.586289
[epoch7, step355]: loss 0.300051
[epoch7, step356]: loss 0.479988
[epoch7, step357]: loss 0.847114
[epoch7, step358]: loss 0.824354
[epoch7, step359]: loss 0.394218
[epoch7, step360]: loss 0.570309
[epoch7, step361]: loss 0.595321
[epoch7, step362]: loss 0.592763
[epoch7, step363]: loss 0.713123
[epoch7, step364]: loss 0.779741
[epoch7, step365]: loss 0.707700
[epoch7, step366]: loss 0.698922
[epoch7, step367]: loss 0.529891
[epoch7, step368]: loss 0.683347
[epoch7, step369]: loss 0.955934
[epoch7, step370]: loss 0.519321
[epoch7, step371]: loss 0.832501
[epoch7, step372]: loss 0.762682
[epoch7, step373]: loss 0.750086
[epoch7, step374]: loss 0.826388
[epoch7, step375]: loss 0.496887
[epoch7, step376]: loss 0.377457
[epoch7, step377]: loss 0.782847
[epoch7, step378]: loss 0.529499
[epoch7, step379]: loss 0.609248
[epoch7, step380]: loss 0.658100
[epoch7, step381]: loss 0.702342
[epoch7, step382]: loss 0.744137
[epoch7, step383]: loss 0.482896
[epoch7, step384]: loss 0.527841
[epoch7, step385]: loss 0.497803
[epoch7, step386]: loss 0.733255
[epoch7, step387]: loss 0.569273
[epoch7, step388]: loss 0.437190
[epoch7, step389]: loss 0.781315
[epoch7, step390]: loss 0.523005
[epoch7, step391]: loss 0.653866
[epoch7, step392]: loss 0.624546
[epoch7, step393]: loss 0.624256
[epoch7, step394]: loss 0.478725
[epoch7, step395]: loss 0.692561
[epoch7, step396]: loss 0.771412
[epoch7, step397]: loss 0.574792
[epoch7, step398]: loss 0.527123
[epoch7, step399]: loss 0.552184
[epoch7, step400]: loss 0.548624
[epoch7, step401]: loss 0.473558
[epoch7, step402]: loss 0.844583
[epoch7, step403]: loss 0.576970
[epoch7, step404]: loss 0.531015
[epoch7, step405]: loss 0.717886
[epoch7, step406]: loss 0.718251
[epoch7, step407]: loss 0.761833
[epoch7, step408]: loss 0.791678
[epoch7, step409]: loss 0.746529
[epoch7, step410]: loss 0.383739
[epoch7, step411]: loss 0.651560
[epoch7, step412]: loss 0.352851
[epoch7, step413]: loss 0.732486
[epoch7, step414]: loss 0.677607
[epoch7, step415]: loss 0.415898
[epoch7, step416]: loss 0.969035
[epoch7, step417]: loss 0.342274
[epoch7, step418]: loss 0.597198
[epoch7, step419]: loss 0.828468
[epoch7, step420]: loss 0.501275
[epoch7, step421]: loss 0.853714
[epoch7, step422]: loss 0.343219
[epoch7, step423]: loss 0.550489
[epoch7, step424]: loss 0.653701
[epoch7, step425]: loss 0.674529
[epoch7, step426]: loss 0.689699
[epoch7, step427]: loss 0.906158
[epoch7, step428]: loss 0.717291
[epoch7, step429]: loss 0.425773
[epoch7, step430]: loss 0.893226
[epoch7, step431]: loss 0.146994
[epoch7, step432]: loss 0.767623
[epoch7, step433]: loss 0.521219
[epoch7, step434]: loss 0.816257
[epoch7, step435]: loss 0.250230
[epoch7, step436]: loss 0.755069
[epoch7, step437]: loss 0.786226
[epoch7, step438]: loss 0.479269
[epoch7, step439]: loss 0.788913
[epoch7, step440]: loss 0.790708
[epoch7, step441]: loss 0.667081
[epoch7, step442]: loss 0.586110
[epoch7, step443]: loss 0.367479
[epoch7, step444]: loss 0.556551
[epoch7, step445]: loss 0.509886
[epoch7, step446]: loss 0.828451
[epoch7, step447]: loss 0.716707
[epoch7, step448]: loss 0.686344
[epoch7, step449]: loss 0.754842
[epoch7, step450]: loss 0.554785
[epoch7, step451]: loss 0.608280
[epoch7, step452]: loss 0.724169
[epoch7, step453]: loss 0.340771
[epoch7, step454]: loss 0.614391
[epoch7, step455]: loss 0.368005
[epoch7, step456]: loss 0.427061
[epoch7, step457]: loss 0.700572
[epoch7, step458]: loss 0.527697
[epoch7, step459]: loss 0.700181
[epoch7, step460]: loss 0.447571
[epoch7, step461]: loss 0.663900
[epoch7, step462]: loss 0.786647
[epoch7, step463]: loss 0.825336
[epoch7, step464]: loss 0.811432
[epoch7, step465]: loss 0.597187
[epoch7, step466]: loss 0.524183
[epoch7, step467]: loss 0.551734
[epoch7, step468]: loss 0.678457
[epoch7, step469]: loss 0.518077
[epoch7, step470]: loss 0.506784
[epoch7, step471]: loss 0.681250
[epoch7, step472]: loss 0.779830
[epoch7, step473]: loss 0.733457
[epoch7, step474]: loss 0.923794
[epoch7, step475]: loss 0.701825
[epoch7, step476]: loss 0.747612
[epoch7, step477]: loss 0.761402
[epoch7, step478]: loss 0.650009
[epoch7, step479]: loss 0.735704
[epoch7, step480]: loss 0.714503
[epoch7, step481]: loss 0.665269
[epoch7, step482]: loss 0.443760
[epoch7, step483]: loss 0.606981
[epoch7, step484]: loss 0.554062
[epoch7, step485]: loss 0.668741
[epoch7, step486]: loss 0.638783
[epoch7, step487]: loss 0.741600
[epoch7, step488]: loss 0.627927
[epoch7, step489]: loss 0.532597
[epoch7, step490]: loss 0.896326
[epoch7, step491]: loss 0.621180
[epoch7, step492]: loss 0.830899
[epoch7, step493]: loss 0.656026
[epoch7, step494]: loss 0.628432
[epoch7, step495]: loss 0.548481
[epoch7, step496]: loss 0.742162
[epoch7, step497]: loss 0.605297
[epoch7, step498]: loss 0.316797
[epoch7, step499]: loss 0.474176
[epoch7, step500]: loss 0.731931
[epoch7, step501]: loss 0.611337
[epoch7, step502]: loss 0.261925
[epoch7, step503]: loss 0.546769
[epoch7, step504]: loss 0.533409
[epoch7, step505]: loss 0.885393
[epoch7, step506]: loss 0.791323
[epoch7, step507]: loss 0.510662
[epoch7, step508]: loss 0.569000
[epoch7, step509]: loss 0.929184
[epoch7, step510]: loss 0.570389
[epoch7, step511]: loss 0.621107
[epoch7, step512]: loss 0.725531
[epoch7, step513]: loss 0.843361
[epoch7, step514]: loss 0.481927
[epoch7, step515]: loss 0.447878
[epoch7, step516]: loss 0.660138
[epoch7, step517]: loss 0.858357
[epoch7, step518]: loss 0.362337
[epoch7, step519]: loss 0.763426
[epoch7, step520]: loss 0.344833
[epoch7, step521]: loss 0.666728
[epoch7, step522]: loss 0.626291
[epoch7, step523]: loss 0.682381
[epoch7, step524]: loss 0.504289
[epoch7, step525]: loss 0.666217
[epoch7, step526]: loss 0.520387
[epoch7, step527]: loss 0.777907
[epoch7, step528]: loss 0.597517
[epoch7, step529]: loss 0.610231
[epoch7, step530]: loss 0.734735
[epoch7, step531]: loss 0.671161
[epoch7, step532]: loss 0.584393
[epoch7, step533]: loss 0.344535
[epoch7, step534]: loss 0.297047
[epoch7, step535]: loss 0.495391
[epoch7, step536]: loss 0.896727
[epoch7, step537]: loss 0.449687
[epoch7, step538]: loss 0.719524
[epoch7, step539]: loss 0.664576
[epoch7, step540]: loss 0.609740
[epoch7, step541]: loss 0.560779
[epoch7, step542]: loss 0.470028
[epoch7, step543]: loss 0.927184
[epoch7, step544]: loss 0.679998
[epoch7, step545]: loss 0.621995
[epoch7, step546]: loss 0.538086
[epoch7, step547]: loss 0.591394
[epoch7, step548]: loss 0.605712
[epoch7, step549]: loss 0.526461
[epoch7, step550]: loss 0.631148
[epoch7, step551]: loss 0.638003
[epoch7, step552]: loss 0.726595
[epoch7, step553]: loss 0.715480
[epoch7, step554]: loss 0.689514
[epoch7, step555]: loss 0.468334
[epoch7, step556]: loss 0.852223
[epoch7, step557]: loss 0.837234
[epoch7, step558]: loss 0.499213
[epoch7, step559]: loss 0.738858
[epoch7, step560]: loss 0.634694
[epoch7, step561]: loss 0.644900
[epoch7, step562]: loss 0.416759
[epoch7, step563]: loss 0.242619
[epoch7, step564]: loss 0.615686
[epoch7, step565]: loss 0.603537
[epoch7, step566]: loss 0.657231
[epoch7, step567]: loss 0.641815
[epoch7, step568]: loss 0.930728
[epoch7, step569]: loss 0.727751
[epoch7, step570]: loss 0.137362
[epoch7, step571]: loss 0.547788
[epoch7, step572]: loss 0.550714
[epoch7, step573]: loss 0.643032
[epoch7, step574]: loss 0.610743
[epoch7, step575]: loss 0.643179
[epoch7, step576]: loss 0.746315
[epoch7, step577]: loss 0.718213
[epoch7, step578]: loss 0.634914
[epoch7, step579]: loss 0.594425
[epoch7, step580]: loss 0.694148
[epoch7, step581]: loss 0.369340
[epoch7, step582]: loss 0.723239
[epoch7, step583]: loss 0.437401
[epoch7, step584]: loss 0.700105
[epoch7, step585]: loss 0.676120
[epoch7, step586]: loss 0.709954
[epoch7, step587]: loss 0.665467
[epoch7, step588]: loss 0.829828
[epoch7, step589]: loss 0.704354
[epoch7, step590]: loss 0.703454
[epoch7, step591]: loss 0.851948
[epoch7, step592]: loss 0.580871
[epoch7, step593]: loss 0.797680
[epoch7, step594]: loss 0.671463
[epoch7, step595]: loss 0.537614
[epoch7, step596]: loss 0.662988
[epoch7, step597]: loss 0.728523
[epoch7, step598]: loss 0.664808
[epoch7, step599]: loss 0.398760
[epoch7, step600]: loss 0.693119
[epoch7, step601]: loss 0.485301
[epoch7, step602]: loss 0.679233
[epoch7, step603]: loss 0.742112
[epoch7, step604]: loss 0.741696
[epoch7, step605]: loss 0.608404
[epoch7, step606]: loss 0.596931
[epoch7, step607]: loss 0.479241
[epoch7, step608]: loss 0.754570
[epoch7, step609]: loss 0.504335
[epoch7, step610]: loss 0.694206
[epoch7, step611]: loss 0.654936
[epoch7, step612]: loss 0.508352
[epoch7, step613]: loss 0.468332
[epoch7, step614]: loss 0.731802
[epoch7, step615]: loss 0.695604
[epoch7, step616]: loss 0.503613
[epoch7, step617]: loss 0.703365
[epoch7, step618]: loss 0.918342
[epoch7, step619]: loss 0.483816
[epoch7, step620]: loss 0.621230
[epoch7, step621]: loss 0.332310
[epoch7, step622]: loss 0.355105
[epoch7, step623]: loss 0.498206
[epoch7, step624]: loss 0.465483
[epoch7, step625]: loss 0.510846
[epoch7, step626]: loss 0.655680
[epoch7, step627]: loss 0.890576
[epoch7, step628]: loss 0.337583
[epoch7, step629]: loss 0.720232
[epoch7, step630]: loss 0.789004
[epoch7, step631]: loss 0.513605
[epoch7, step632]: loss 0.729449
[epoch7, step633]: loss 0.569962
[epoch7, step634]: loss 0.641204
[epoch7, step635]: loss 0.933220
[epoch7, step636]: loss 0.681748
[epoch7, step637]: loss 0.589264
[epoch7, step638]: loss 0.774690
[epoch7, step639]: loss 0.742863
[epoch7, step640]: loss 0.707510
[epoch7, step641]: loss 0.830368
[epoch7, step642]: loss 0.888979
[epoch7, step643]: loss 0.958650
[epoch7, step644]: loss 0.762621
[epoch7, step645]: loss 0.773327
[epoch7, step646]: loss 0.713275
[epoch7, step647]: loss 0.666390
[epoch7, step648]: loss 0.813394
[epoch7, step649]: loss 0.783712
[epoch7, step650]: loss 0.686978
[epoch7, step651]: loss 0.892702
[epoch7, step652]: loss 0.573969
[epoch7, step653]: loss 0.324596
[epoch7, step654]: loss 0.476113
[epoch7, step655]: loss 0.751419
[epoch7, step656]: loss 0.436065
[epoch7, step657]: loss 0.599783
[epoch7, step658]: loss 0.683577
[epoch7, step659]: loss 0.432438
[epoch7, step660]: loss 0.714825
[epoch7, step661]: loss 0.396792
[epoch7, step662]: loss 0.757268
[epoch7, step663]: loss 0.605242
[epoch7, step664]: loss 0.206856
[epoch7, step665]: loss 0.901654
[epoch7, step666]: loss 0.279917
[epoch7, step667]: loss 0.558194
[epoch7, step668]: loss 0.225391
[epoch7, step669]: loss 0.575427
[epoch7, step670]: loss 0.758123
[epoch7, step671]: loss 0.760815
[epoch7, step672]: loss 0.643690
[epoch7, step673]: loss 0.649336
[epoch7, step674]: loss 0.550936
[epoch7, step675]: loss 0.645392
[epoch7, step676]: loss 0.544878
[epoch7, step677]: loss 0.605348
[epoch7, step678]: loss 0.752131
[epoch7, step679]: loss 0.607666
[epoch7, step680]: loss 0.414567
[epoch7, step681]: loss 0.859809
[epoch7, step682]: loss 0.427953
[epoch7, step683]: loss 0.795510
[epoch7, step684]: loss 0.679863
[epoch7, step685]: loss 0.284087
[epoch7, step686]: loss 0.849336
[epoch7, step687]: loss 0.575820
[epoch7, step688]: loss 0.734818
[epoch7, step689]: loss 0.696701
[epoch7, step690]: loss 0.641257
[epoch7, step691]: loss 0.406447
[epoch7, step692]: loss 0.899925
[epoch7, step693]: loss 0.770580
[epoch7, step694]: loss 0.777814
[epoch7, step695]: loss 0.827737
[epoch7, step696]: loss 0.576164
[epoch7, step697]: loss 0.403030
[epoch7, step698]: loss 0.628064
[epoch7, step699]: loss 0.626452
[epoch7, step700]: loss 0.704129
[epoch7, step701]: loss 0.615700
[epoch7, step702]: loss 0.771458
[epoch7, step703]: loss 0.711521
[epoch7, step704]: loss 0.682664
[epoch7, step705]: loss 0.889155
[epoch7, step706]: loss 0.764839
[epoch7, step707]: loss 0.542205
[epoch7, step708]: loss 0.595666
[epoch7, step709]: loss 0.734602
[epoch7, step710]: loss 0.611905
[epoch7, step711]: loss 0.683645
[epoch7, step712]: loss 0.701166
[epoch7, step713]: loss 0.885450
[epoch7, step714]: loss 0.630873
[epoch7, step715]: loss 0.558955
[epoch7, step716]: loss 0.641428
[epoch7, step717]: loss 0.768579
[epoch7, step718]: loss 0.547801
[epoch7, step719]: loss 0.515730
[epoch7, step720]: loss 0.743046
[epoch7, step721]: loss 0.816546
[epoch7, step722]: loss 0.615110
[epoch7, step723]: loss 0.527637
[epoch7, step724]: loss 0.859263
[epoch7, step725]: loss 0.563990
[epoch7, step726]: loss 0.543575
[epoch7, step727]: loss 0.705030
[epoch7, step728]: loss 0.650183
[epoch7, step729]: loss 0.757873
[epoch7, step730]: loss 0.456681
[epoch7, step731]: loss 0.666997
[epoch7, step732]: loss 0.728388
[epoch7, step733]: loss 0.546674
[epoch7, step734]: loss 0.621122
[epoch7, step735]: loss 0.636951
[epoch7, step736]: loss 0.703406
[epoch7, step737]: loss 0.870866
[epoch7, step738]: loss 0.640642
[epoch7, step739]: loss 0.652538
[epoch7, step740]: loss 0.594877
[epoch7, step741]: loss 0.670159
[epoch7, step742]: loss 0.734222
[epoch7, step743]: loss 0.502323
[epoch7, step744]: loss 0.394512
[epoch7, step745]: loss 0.562225
[epoch7, step746]: loss 0.579626
[epoch7, step747]: loss 0.598834
[epoch7, step748]: loss 0.693481
[epoch7, step749]: loss 0.658822
[epoch7, step750]: loss 0.744788
[epoch7, step751]: loss 0.734559
[epoch7, step752]: loss 0.884003
[epoch7, step753]: loss 0.671371
[epoch7, step754]: loss 0.634504
[epoch7, step755]: loss 0.700056
[epoch7, step756]: loss 0.603809
[epoch7, step757]: loss 0.627389
[epoch7, step758]: loss 0.574411
[epoch7, step759]: loss 0.859895
[epoch7, step760]: loss 0.684302
[epoch7, step761]: loss 0.739098
[epoch7, step762]: loss 0.675347
[epoch7, step763]: loss 0.708229
[epoch7, step764]: loss 0.698450
[epoch7, step765]: loss 0.631298
[epoch7, step766]: loss 0.617490
[epoch7, step767]: loss 0.700083
[epoch7, step768]: loss 0.397584
[epoch7, step769]: loss 0.432667
[epoch7, step770]: loss 0.908016
[epoch7, step771]: loss 0.639419
[epoch7, step772]: loss 0.515615
[epoch7, step773]: loss 0.836827
[epoch7, step774]: loss 0.556664
[epoch7, step775]: loss 0.718980
[epoch7, step776]: loss 0.557205
[epoch7, step777]: loss 0.690844
[epoch7, step778]: loss 0.500610
[epoch7, step779]: loss 0.790574
[epoch7, step780]: loss 0.803731
[epoch7, step781]: loss 0.390595
[epoch7, step782]: loss 0.822104
[epoch7, step783]: loss 0.693466
[epoch7, step784]: loss 0.834945
[epoch7, step785]: loss 0.665470
[epoch7, step786]: loss 1.044456
[epoch7, step787]: loss 0.881376
[epoch7, step788]: loss 0.457066
[epoch7, step789]: loss 0.543537
[epoch7, step790]: loss 1.006476
[epoch7, step791]: loss 0.362839
[epoch7, step792]: loss 0.834241
[epoch7, step793]: loss 0.820592
[epoch7, step794]: loss 0.483520
[epoch7, step795]: loss 0.623168
[epoch7, step796]: loss 0.750038
[epoch7, step797]: loss 0.480066
[epoch7, step798]: loss 0.655842
[epoch7, step799]: loss 0.790196
[epoch7, step800]: loss 0.703495
[epoch7, step801]: loss 0.836366
[epoch7, step802]: loss 0.409806
[epoch7, step803]: loss 0.649417
[epoch7, step804]: loss 0.629249
[epoch7, step805]: loss 0.832163
[epoch7, step806]: loss 0.738251
[epoch7, step807]: loss 0.742790
[epoch7, step808]: loss 0.591405
[epoch7, step809]: loss 0.766135
[epoch7, step810]: loss 0.489779
[epoch7, step811]: loss 0.713513
[epoch7, step812]: loss 0.709089
[epoch7, step813]: loss 0.741207
[epoch7, step814]: loss 0.609510
[epoch7, step815]: loss 0.654473
[epoch7, step816]: loss 0.799052
[epoch7, step817]: loss 0.552807
[epoch7, step818]: loss 0.592737
[epoch7, step819]: loss 0.571695
[epoch7, step820]: loss 0.725345
[epoch7, step821]: loss 0.428529
[epoch7, step822]: loss 0.630681
[epoch7, step823]: loss 0.689376
[epoch7, step824]: loss 0.762507
[epoch7, step825]: loss 0.582694
[epoch7, step826]: loss 0.703838
[epoch7, step827]: loss 0.479538
[epoch7, step828]: loss 0.726613
[epoch7, step829]: loss 0.733786
[epoch7, step830]: loss 0.833107
[epoch7, step831]: loss 0.708691
[epoch7, step832]: loss 0.638507
[epoch7, step833]: loss 0.494040
[epoch7, step834]: loss 0.766474
[epoch7, step835]: loss 0.742928
[epoch7, step836]: loss 0.878251
[epoch7, step837]: loss 0.556413
[epoch7, step838]: loss 0.778202
[epoch7, step839]: loss 0.152352
[epoch7, step840]: loss 0.669566
[epoch7, step841]: loss 0.783397
[epoch7, step842]: loss 0.726224
[epoch7, step843]: loss 0.499782
[epoch7, step844]: loss 0.549741
[epoch7, step845]: loss 0.623763
[epoch7, step846]: loss 0.618785
[epoch7, step847]: loss 0.705408
[epoch7, step848]: loss 0.713577
[epoch7, step849]: loss 0.803342
[epoch7, step850]: loss 0.886447
[epoch7, step851]: loss 0.590132
[epoch7, step852]: loss 0.628954
[epoch7, step853]: loss 0.711514
[epoch7, step854]: loss 0.573799
[epoch7, step855]: loss 0.656562
[epoch7, step856]: loss 0.638804
[epoch7, step857]: loss 0.693407
[epoch7, step858]: loss 0.355319
[epoch7, step859]: loss 0.523218
[epoch7, step860]: loss 0.652270
[epoch7, step861]: loss 0.809460
[epoch7, step862]: loss 0.878014
[epoch7, step863]: loss 0.701119
[epoch7, step864]: loss 0.445990
[epoch7, step865]: loss 0.555549
[epoch7, step866]: loss 0.746691
[epoch7, step867]: loss 0.755161
[epoch7, step868]: loss 0.752555
[epoch7, step869]: loss 0.328295
[epoch7, step870]: loss 0.822121
[epoch7, step871]: loss 0.833907
[epoch7, step872]: loss 0.817244
[epoch7, step873]: loss 0.550720
[epoch7, step874]: loss 0.572078
[epoch7, step875]: loss 0.710563
[epoch7, step876]: loss 0.572540
[epoch7, step877]: loss 0.671199
[epoch7, step878]: loss 0.390813
[epoch7, step879]: loss 0.752212
[epoch7, step880]: loss 0.742702
[epoch7, step881]: loss 0.718065
[epoch7, step882]: loss 0.636897
[epoch7, step883]: loss 0.794713
[epoch7, step884]: loss 0.600364
[epoch7, step885]: loss 0.602658
[epoch7, step886]: loss 0.788648
[epoch7, step887]: loss 0.474018
[epoch7, step888]: loss 0.638068
[epoch7, step889]: loss 0.761620
[epoch7, step890]: loss 0.972372
[epoch7, step891]: loss 0.678267
[epoch7, step892]: loss 0.401718
[epoch7, step893]: loss 0.801635
[epoch7, step894]: loss 0.652747
[epoch7, step895]: loss 0.752325
[epoch7, step896]: loss 0.495623
[epoch7, step897]: loss 0.669528
[epoch7, step898]: loss 0.723998
[epoch7, step899]: loss 0.580593
[epoch7, step900]: loss 0.622031
[epoch7, step901]: loss 0.758198
[epoch7, step902]: loss 0.740898
[epoch7, step903]: loss 0.635827
[epoch7, step904]: loss 0.578393
[epoch7, step905]: loss 0.767004
[epoch7, step906]: loss 0.445738
[epoch7, step907]: loss 0.390733
[epoch7, step908]: loss 0.593712
[epoch7, step909]: loss 0.520080
[epoch7, step910]: loss 0.426240
[epoch7, step911]: loss 0.787246
[epoch7, step912]: loss 0.520634
[epoch7, step913]: loss 0.681613
[epoch7, step914]: loss 0.699481
[epoch7, step915]: loss 0.998020
[epoch7, step916]: loss 0.552762
[epoch7, step917]: loss 0.765706
[epoch7, step918]: loss 0.692914
[epoch7, step919]: loss 0.631965
[epoch7, step920]: loss 0.426655
[epoch7, step921]: loss 0.789155
[epoch7, step922]: loss 0.714036
[epoch7, step923]: loss 0.591176
[epoch7, step924]: loss 0.512464
[epoch7, step925]: loss 0.861604
[epoch7, step926]: loss 0.841370
[epoch7, step927]: loss 0.637051
[epoch7, step928]: loss 0.714949
[epoch7, step929]: loss 0.817101
[epoch7, step930]: loss 0.724651
[epoch7, step931]: loss 0.557265
[epoch7, step932]: loss 0.435704
[epoch7, step933]: loss 0.544185
[epoch7, step934]: loss 0.853032
[epoch7, step935]: loss 0.778956
[epoch7, step936]: loss 0.816050
[epoch7, step937]: loss 0.846086
[epoch7, step938]: loss 0.754377
[epoch7, step939]: loss 0.669152
[epoch7, step940]: loss 0.459160
[epoch7, step941]: loss 0.362152
[epoch7, step942]: loss 0.406381
[epoch7, step943]: loss 0.596114
[epoch7, step944]: loss 0.738856
[epoch7, step945]: loss 0.765005
[epoch7, step946]: loss 0.748115
[epoch7, step947]: loss 0.805241
[epoch7, step948]: loss 0.620055
[epoch7, step949]: loss 0.574325
[epoch7, step950]: loss 0.595538
[epoch7, step951]: loss 0.599951
[epoch7, step952]: loss 0.434997
[epoch7, step953]: loss 0.492031
[epoch7, step954]: loss 0.784019
[epoch7, step955]: loss 0.764142
[epoch7, step956]: loss 0.758544
[epoch7, step957]: loss 0.605530
[epoch7, step958]: loss 0.595376
[epoch7, step959]: loss 0.805029
[epoch7, step960]: loss 0.608259
[epoch7, step961]: loss 0.580517
[epoch7, step962]: loss 0.574193
[epoch7, step963]: loss 0.503561
[epoch7, step964]: loss 0.309673
[epoch7, step965]: loss 0.740987
[epoch7, step966]: loss 0.859389
[epoch7, step967]: loss 0.783028
[epoch7, step968]: loss 0.785335
[epoch7, step969]: loss 0.652199
[epoch7, step970]: loss 0.615839
[epoch7, step971]: loss 0.636198
[epoch7, step972]: loss 0.726507
[epoch7, step973]: loss 0.731859
[epoch7, step974]: loss 0.832649
[epoch7, step975]: loss 0.559383
[epoch7, step976]: loss 0.697311
[epoch7, step977]: loss 0.514436
[epoch7, step978]: loss 0.562684
[epoch7, step979]: loss 0.684811
[epoch7, step980]: loss 0.650737
[epoch7, step981]: loss 0.710179
[epoch7, step982]: loss 0.809481
[epoch7, step983]: loss 0.678691
[epoch7, step984]: loss 0.588046
[epoch7, step985]: loss 0.406667
[epoch7, step986]: loss 0.605001
[epoch7, step987]: loss 0.842885
[epoch7, step988]: loss 0.738743
[epoch7, step989]: loss 0.662867
[epoch7, step990]: loss 0.392757
[epoch7, step991]: loss 0.921044
[epoch7, step992]: loss 0.508640
[epoch7, step993]: loss 0.698316
[epoch7, step994]: loss 0.816589
[epoch7, step995]: loss 0.673242
[epoch7, step996]: loss 0.641073
[epoch7, step997]: loss 0.428050
[epoch7, step998]: loss 0.661594
[epoch7, step999]: loss 0.548568
[epoch7, step1000]: loss 0.210447
[epoch7, step1001]: loss 0.846283
[epoch7, step1002]: loss 0.780342
[epoch7, step1003]: loss 0.812088
[epoch7, step1004]: loss 0.870404
[epoch7, step1005]: loss 0.695122
[epoch7, step1006]: loss 0.431430
[epoch7, step1007]: loss 0.694935
[epoch7, step1008]: loss 0.773865
[epoch7, step1009]: loss 0.401255
[epoch7, step1010]: loss 0.269174
[epoch7, step1011]: loss 0.548019
[epoch7, step1012]: loss 0.557323
[epoch7, step1013]: loss 0.844601
[epoch7, step1014]: loss 0.595366
[epoch7, step1015]: loss 0.541547
[epoch7, step1016]: loss 0.562944
[epoch7, step1017]: loss 0.489975
[epoch7, step1018]: loss 0.418334
[epoch7, step1019]: loss 0.705149
[epoch7, step1020]: loss 0.434010
[epoch7, step1021]: loss 0.510269
[epoch7, step1022]: loss 0.730296
[epoch7, step1023]: loss 0.922881
[epoch7, step1024]: loss 0.668142
[epoch7, step1025]: loss 0.634444
[epoch7, step1026]: loss 0.826229
[epoch7, step1027]: loss 0.674431
[epoch7, step1028]: loss 0.462573
[epoch7, step1029]: loss 0.568862
[epoch7, step1030]: loss 0.756030
[epoch7, step1031]: loss 0.435851
[epoch7, step1032]: loss 0.425548
[epoch7, step1033]: loss 0.337601
[epoch7, step1034]: loss 0.699423
[epoch7, step1035]: loss 0.794523
[epoch7, step1036]: loss 0.655607
[epoch7, step1037]: loss 0.745042
[epoch7, step1038]: loss 0.728656
[epoch7, step1039]: loss 0.461402
[epoch7, step1040]: loss 0.739211
[epoch7, step1041]: loss 0.897776
[epoch7, step1042]: loss 0.824854
[epoch7, step1043]: loss 0.626640
[epoch7, step1044]: loss 0.573286
[epoch7, step1045]: loss 0.832946
[epoch7, step1046]: loss 0.522562
[epoch7, step1047]: loss 0.712947
[epoch7, step1048]: loss 0.812745
[epoch7, step1049]: loss 0.550178
[epoch7, step1050]: loss 0.686255
[epoch7, step1051]: loss 0.581856
[epoch7, step1052]: loss 0.364771
[epoch7, step1053]: loss 0.893011
[epoch7, step1054]: loss 0.679306
[epoch7, step1055]: loss 0.512731
[epoch7, step1056]: loss 0.359927
[epoch7, step1057]: loss 0.644017
[epoch7, step1058]: loss 0.847695
[epoch7, step1059]: loss 0.568359
[epoch7, step1060]: loss 0.640874
[epoch7, step1061]: loss 0.651363
[epoch7, step1062]: loss 0.917306
[epoch7, step1063]: loss 0.685700
[epoch7, step1064]: loss 0.577224
[epoch7, step1065]: loss 0.519634
[epoch7, step1066]: loss 0.659786
[epoch7, step1067]: loss 0.606785
[epoch7, step1068]: loss 0.803688
[epoch7, step1069]: loss 0.619322
[epoch7, step1070]: loss 0.456340
[epoch7, step1071]: loss 0.729916
[epoch7, step1072]: loss 0.754397
[epoch7, step1073]: loss 0.682521
[epoch7, step1074]: loss 0.590846
[epoch7, step1075]: loss 0.789506
[epoch7, step1076]: loss 0.533411
[epoch7, step1077]: loss 0.642555
[epoch7, step1078]: loss 0.558474
[epoch7, step1079]: loss 0.599729
[epoch7, step1080]: loss 0.672939
[epoch7, step1081]: loss 0.804156
[epoch7, step1082]: loss 0.769962
[epoch7, step1083]: loss 0.705387
[epoch7, step1084]: loss 0.806796
[epoch7, step1085]: loss 0.712315
[epoch7, step1086]: loss 0.429330
[epoch7, step1087]: loss 0.506750
[epoch7, step1088]: loss 0.696523
[epoch7, step1089]: loss 0.700837
[epoch7, step1090]: loss 0.782895
[epoch7, step1091]: loss 0.647463
[epoch7, step1092]: loss 0.738850
[epoch7, step1093]: loss 0.748791
[epoch7, step1094]: loss 0.918809
[epoch7, step1095]: loss 0.691280
[epoch7, step1096]: loss 0.485499
[epoch7, step1097]: loss 0.609556
[epoch7, step1098]: loss 0.501155
[epoch7, step1099]: loss 0.823618
[epoch7, step1100]: loss 0.786271
[epoch7, step1101]: loss 0.661190
[epoch7, step1102]: loss 0.898484
[epoch7, step1103]: loss 0.577933
[epoch7, step1104]: loss 0.681859
[epoch7, step1105]: loss 0.254267
[epoch7, step1106]: loss 0.571397
[epoch7, step1107]: loss 0.488130
[epoch7, step1108]: loss 0.881985
[epoch7, step1109]: loss 0.541653
[epoch7, step1110]: loss 0.716039
[epoch7, step1111]: loss 0.796212
[epoch7, step1112]: loss 0.404099
[epoch7, step1113]: loss 0.772175
[epoch7, step1114]: loss 0.559200
[epoch7, step1115]: loss 0.729726
[epoch7, step1116]: loss 0.496674
[epoch7, step1117]: loss 0.356347
[epoch7, step1118]: loss 0.762510
[epoch7, step1119]: loss 0.470961
[epoch7, step1120]: loss 0.746744
[epoch7, step1121]: loss 0.478400
[epoch7, step1122]: loss 0.648278
[epoch7, step1123]: loss 0.796324
[epoch7, step1124]: loss 0.722851
[epoch7, step1125]: loss 0.601496
[epoch7, step1126]: loss 0.493189
[epoch7, step1127]: loss 0.654242
[epoch7, step1128]: loss 0.653341
[epoch7, step1129]: loss 0.801477
[epoch7, step1130]: loss 0.799212
[epoch7, step1131]: loss 0.517071
[epoch7, step1132]: loss 0.643286
[epoch7, step1133]: loss 0.574425
[epoch7, step1134]: loss 0.770567
[epoch7, step1135]: loss 0.643985
[epoch7, step1136]: loss 0.705562
[epoch7, step1137]: loss 0.339836
[epoch7, step1138]: loss 0.571560
[epoch7, step1139]: loss 0.297632
[epoch7, step1140]: loss 0.546232
[epoch7, step1141]: loss 0.625049
[epoch7, step1142]: loss 0.549328
[epoch7, step1143]: loss 0.583109
[epoch7, step1144]: loss 0.829386
[epoch7, step1145]: loss 0.574282
[epoch7, step1146]: loss 0.479077
[epoch7, step1147]: loss 0.798865
[epoch7, step1148]: loss 0.502299
[epoch7, step1149]: loss 0.821017
[epoch7, step1150]: loss 0.726551
[epoch7, step1151]: loss 0.635910
[epoch7, step1152]: loss 0.546251
[epoch7, step1153]: loss 0.603185
[epoch7, step1154]: loss 0.566240
[epoch7, step1155]: loss 0.598094
[epoch7, step1156]: loss 0.591450
[epoch7, step1157]: loss 0.654978
[epoch7, step1158]: loss 0.614249
[epoch7, step1159]: loss 0.521903
[epoch7, step1160]: loss 0.555988
[epoch7, step1161]: loss 0.516751
[epoch7, step1162]: loss 0.448389
[epoch7, step1163]: loss 0.595764
[epoch7, step1164]: loss 0.591670
[epoch7, step1165]: loss 0.614089
[epoch7, step1166]: loss 0.811831
[epoch7, step1167]: loss 0.709835
[epoch7, step1168]: loss 0.404723
[epoch7, step1169]: loss 0.690255
[epoch7, step1170]: loss 0.475852
[epoch7, step1171]: loss 0.616882
[epoch7, step1172]: loss 0.564601
[epoch7, step1173]: loss 0.807611
[epoch7, step1174]: loss 0.637497
[epoch7, step1175]: loss 0.592559
[epoch7, step1176]: loss 0.652319
[epoch7, step1177]: loss 0.650128
[epoch7, step1178]: loss 0.370577
[epoch7, step1179]: loss 0.491551
[epoch7, step1180]: loss 0.594662
[epoch7, step1181]: loss 0.837937
[epoch7, step1182]: loss 0.548197
[epoch7, step1183]: loss 0.862341
[epoch7, step1184]: loss 0.480625
[epoch7, step1185]: loss 0.569296
[epoch7, step1186]: loss 0.857966
[epoch7, step1187]: loss 0.800494
[epoch7, step1188]: loss 0.550994
[epoch7, step1189]: loss 0.542115
[epoch7, step1190]: loss 0.439700
[epoch7, step1191]: loss 0.575975
[epoch7, step1192]: loss 0.675634
[epoch7, step1193]: loss 0.763679
[epoch7, step1194]: loss 0.561220
[epoch7, step1195]: loss 0.768369
[epoch7, step1196]: loss 0.223563
[epoch7, step1197]: loss 0.550251
[epoch7, step1198]: loss 0.576784
[epoch7, step1199]: loss 0.854557
[epoch7, step1200]: loss 0.470431
[epoch7, step1201]: loss 0.728292
[epoch7, step1202]: loss 0.191071
[epoch7, step1203]: loss 0.709065
[epoch7, step1204]: loss 0.696590
[epoch7, step1205]: loss 0.820363
[epoch7, step1206]: loss 0.546622
[epoch7, step1207]: loss 0.767030
[epoch7, step1208]: loss 0.549863
[epoch7, step1209]: loss 0.846098
[epoch7, step1210]: loss 0.756358
[epoch7, step1211]: loss 0.449659
[epoch7, step1212]: loss 0.867298
[epoch7, step1213]: loss 0.864551
[epoch7, step1214]: loss 0.783298
[epoch7, step1215]: loss 0.588050
[epoch7, step1216]: loss 0.731118
[epoch7, step1217]: loss 0.319555
[epoch7, step1218]: loss 0.618569
[epoch7, step1219]: loss 0.541629
[epoch7, step1220]: loss 0.644138
[epoch7, step1221]: loss 0.714425
[epoch7, step1222]: loss 0.711880
[epoch7, step1223]: loss 0.815558
[epoch7, step1224]: loss 0.591487
[epoch7, step1225]: loss 0.712245
[epoch7, step1226]: loss 0.741510
[epoch7, step1227]: loss 0.574119
[epoch7, step1228]: loss 0.541244
[epoch7, step1229]: loss 0.525841
[epoch7, step1230]: loss 0.634924
[epoch7, step1231]: loss 0.617527
[epoch7, step1232]: loss 0.811560
[epoch7, step1233]: loss 0.754651
[epoch7, step1234]: loss 0.608281
[epoch7, step1235]: loss 0.830728
[epoch7, step1236]: loss 0.576836
[epoch7, step1237]: loss 0.460920
[epoch7, step1238]: loss 0.511710
[epoch7, step1239]: loss 0.356464
[epoch7, step1240]: loss 0.779032
[epoch7, step1241]: loss 0.710975
[epoch7, step1242]: loss 0.460026
[epoch7, step1243]: loss 0.757985
[epoch7, step1244]: loss 0.924159
[epoch7, step1245]: loss 0.555095
[epoch7, step1246]: loss 0.724039
[epoch7, step1247]: loss 0.715038
[epoch7, step1248]: loss 0.608991
[epoch7, step1249]: loss 0.403297
[epoch7, step1250]: loss 0.838180
[epoch7, step1251]: loss 0.641492
[epoch7, step1252]: loss 0.805834
[epoch7, step1253]: loss 0.344282
[epoch7, step1254]: loss 0.814071
[epoch7, step1255]: loss 0.606102
[epoch7, step1256]: loss 0.734680
[epoch7, step1257]: loss 0.582945
[epoch7, step1258]: loss 0.809735
[epoch7, step1259]: loss 0.717168
[epoch7, step1260]: loss 0.681286
[epoch7, step1261]: loss 0.695157
[epoch7, step1262]: loss 0.913830
[epoch7, step1263]: loss 0.641003
[epoch7, step1264]: loss 0.584340
[epoch7, step1265]: loss 0.448725
[epoch7, step1266]: loss 0.686959
[epoch7, step1267]: loss 0.902677
[epoch7, step1268]: loss 0.677972
[epoch7, step1269]: loss 0.619264
[epoch7, step1270]: loss 0.592559
[epoch7, step1271]: loss 0.776677
[epoch7, step1272]: loss 0.694273
[epoch7, step1273]: loss 0.659238
[epoch7, step1274]: loss 0.457798
[epoch7, step1275]: loss 0.778540
[epoch7, step1276]: loss 0.789041
[epoch7, step1277]: loss 0.294073
[epoch7, step1278]: loss 0.811663
[epoch7, step1279]: loss 0.680844
[epoch7, step1280]: loss 0.454614
[epoch7, step1281]: loss 0.597818
[epoch7, step1282]: loss 0.600702
[epoch7, step1283]: loss 0.623035
[epoch7, step1284]: loss 0.671430
[epoch7, step1285]: loss 0.809453
[epoch7, step1286]: loss 0.594295
[epoch7, step1287]: loss 0.504583
[epoch7, step1288]: loss 0.676344
[epoch7, step1289]: loss 0.376621
[epoch7, step1290]: loss 0.705766
[epoch7, step1291]: loss 0.567040
[epoch7, step1292]: loss 0.594919
[epoch7, step1293]: loss 0.620950
[epoch7, step1294]: loss 0.584244
[epoch7, step1295]: loss 0.530120
[epoch7, step1296]: loss 0.409043
[epoch7, step1297]: loss 0.725841
[epoch7, step1298]: loss 0.923074
[epoch7, step1299]: loss 0.649076
[epoch7, step1300]: loss 0.702171
[epoch7, step1301]: loss 0.811125
[epoch7, step1302]: loss 0.774339
[epoch7, step1303]: loss 0.866369
[epoch7, step1304]: loss 0.354581
[epoch7, step1305]: loss 0.797717
[epoch7, step1306]: loss 0.580647
[epoch7, step1307]: loss 0.931370
[epoch7, step1308]: loss 0.685562
[epoch7, step1309]: loss 0.580851
[epoch7, step1310]: loss 0.757540
[epoch7, step1311]: loss 0.721337
[epoch7, step1312]: loss 0.455863
[epoch7, step1313]: loss 0.804967
[epoch7, step1314]: loss 0.777047
[epoch7, step1315]: loss 0.596764
[epoch7, step1316]: loss 0.771532
[epoch7, step1317]: loss 0.896074
[epoch7, step1318]: loss 0.619237
[epoch7, step1319]: loss 0.661554
[epoch7, step1320]: loss 0.620124
[epoch7, step1321]: loss 0.734156
[epoch7, step1322]: loss 0.493680
[epoch7, step1323]: loss 0.600528
[epoch7, step1324]: loss 0.535662
[epoch7, step1325]: loss 0.782763
[epoch7, step1326]: loss 0.657252
[epoch7, step1327]: loss 0.712008
[epoch7, step1328]: loss 0.545161
[epoch7, step1329]: loss 0.718341
[epoch7, step1330]: loss 0.536453
[epoch7, step1331]: loss 0.682835
[epoch7, step1332]: loss 0.680946
[epoch7, step1333]: loss 0.625487
[epoch7, step1334]: loss 0.797808
[epoch7, step1335]: loss 0.587909
[epoch7, step1336]: loss 0.674891
[epoch7, step1337]: loss 0.646880
[epoch7, step1338]: loss 0.755913
[epoch7, step1339]: loss 0.762799
[epoch7, step1340]: loss 0.620066
[epoch7, step1341]: loss 0.677058
[epoch7, step1342]: loss 0.808093
[epoch7, step1343]: loss 0.705528
[epoch7, step1344]: loss 0.768819
[epoch7, step1345]: loss 0.749198
[epoch7, step1346]: loss 0.780986
[epoch7, step1347]: loss 0.755256
[epoch7, step1348]: loss 0.591518
[epoch7, step1349]: loss 0.736946
[epoch7, step1350]: loss 0.381606
[epoch7, step1351]: loss 0.704851
[epoch7, step1352]: loss 0.765399
[epoch7, step1353]: loss 0.655543
[epoch7, step1354]: loss 0.556466
[epoch7, step1355]: loss 0.785968
[epoch7, step1356]: loss 0.507174
[epoch7, step1357]: loss 0.676842
[epoch7, step1358]: loss 0.892971
[epoch7, step1359]: loss 0.846647
[epoch7, step1360]: loss 0.933772
[epoch7, step1361]: loss 0.607720
[epoch7, step1362]: loss 0.649386
[epoch7, step1363]: loss 0.813133
[epoch7, step1364]: loss 0.774969
[epoch7, step1365]: loss 0.320795
[epoch7, step1366]: loss 0.627241
[epoch7, step1367]: loss 0.865723
[epoch7, step1368]: loss 0.371647
[epoch7, step1369]: loss 0.667887
[epoch7, step1370]: loss 0.631057
[epoch7, step1371]: loss 0.829727
[epoch7, step1372]: loss 0.542320
[epoch7, step1373]: loss 0.848277
[epoch7, step1374]: loss 0.525593
[epoch7, step1375]: loss 0.374150
[epoch7, step1376]: loss 0.717929
[epoch7, step1377]: loss 0.560046
[epoch7, step1378]: loss 0.548907
[epoch7, step1379]: loss 0.617128
[epoch7, step1380]: loss 0.633579
[epoch7, step1381]: loss 0.646923
[epoch7, step1382]: loss 0.414087
[epoch7, step1383]: loss 0.447688
[epoch7, step1384]: loss 0.705412
[epoch7, step1385]: loss 0.527538
[epoch7, step1386]: loss 0.598874
[epoch7, step1387]: loss 0.464021
[epoch7, step1388]: loss 0.567278
[epoch7, step1389]: loss 0.716862
[epoch7, step1390]: loss 0.389311
[epoch7, step1391]: loss 0.542551
[epoch7, step1392]: loss 0.476530
[epoch7, step1393]: loss 0.526029
[epoch7, step1394]: loss 0.513637
[epoch7, step1395]: loss 0.591415
[epoch7, step1396]: loss 0.694993
[epoch7, step1397]: loss 0.416602
[epoch7, step1398]: loss 0.931713
[epoch7, step1399]: loss 0.548161
[epoch7, step1400]: loss 0.461290
[epoch7, step1401]: loss 0.372300
[epoch7, step1402]: loss 0.658366
[epoch7, step1403]: loss 0.475623
[epoch7, step1404]: loss 0.424178
[epoch7, step1405]: loss 0.804404
[epoch7, step1406]: loss 0.426637
[epoch7, step1407]: loss 0.748806
[epoch7, step1408]: loss 0.578294
[epoch7, step1409]: loss 0.664870
[epoch7, step1410]: loss 0.645575
[epoch7, step1411]: loss 0.652746
[epoch7, step1412]: loss 0.708793
[epoch7, step1413]: loss 0.867216
[epoch7, step1414]: loss 0.751309
[epoch7, step1415]: loss 0.729892
[epoch7, step1416]: loss 0.607221
[epoch7, step1417]: loss 0.903723
[epoch7, step1418]: loss 0.819118
[epoch7, step1419]: loss 0.814315
[epoch7, step1420]: loss 0.774345
[epoch7, step1421]: loss 0.494323
[epoch7, step1422]: loss 0.646574
[epoch7, step1423]: loss 0.742324
[epoch7, step1424]: loss 0.758166
[epoch7, step1425]: loss 0.505977
[epoch7, step1426]: loss 0.310000
[epoch7, step1427]: loss 0.485690
[epoch7, step1428]: loss 0.317369
[epoch7, step1429]: loss 0.746255
[epoch7, step1430]: loss 0.710082
[epoch7, step1431]: loss 0.669001
[epoch7, step1432]: loss 0.622063
[epoch7, step1433]: loss 0.691867
[epoch7, step1434]: loss 0.811862
[epoch7, step1435]: loss 0.457473
[epoch7, step1436]: loss 0.327376
[epoch7, step1437]: loss 0.545738
[epoch7, step1438]: loss 0.898950
[epoch7, step1439]: loss 0.591088
[epoch7, step1440]: loss 0.619675
[epoch7, step1441]: loss 0.612493
[epoch7, step1442]: loss 0.648654
[epoch7, step1443]: loss 0.757416
[epoch7, step1444]: loss 0.723345
[epoch7, step1445]: loss 0.589635
[epoch7, step1446]: loss 0.509740
[epoch7, step1447]: loss 0.591984
[epoch7, step1448]: loss 0.743428
[epoch7, step1449]: loss 0.625613
[epoch7, step1450]: loss 0.576639
[epoch7, step1451]: loss 0.373378
[epoch7, step1452]: loss 0.899117
[epoch7, step1453]: loss 0.733064
[epoch7, step1454]: loss 0.371783
[epoch7, step1455]: loss 0.465991
[epoch7, step1456]: loss 0.506566
[epoch7, step1457]: loss 0.677932
[epoch7, step1458]: loss 0.720912
[epoch7, step1459]: loss 0.835631
[epoch7, step1460]: loss 0.513087
[epoch7, step1461]: loss 0.869198
[epoch7, step1462]: loss 0.573652
[epoch7, step1463]: loss 0.685724
[epoch7, step1464]: loss 0.766319
[epoch7, step1465]: loss 0.688287
[epoch7, step1466]: loss 0.599400
[epoch7, step1467]: loss 0.383631
[epoch7, step1468]: loss 0.671001
[epoch7, step1469]: loss 0.224898
[epoch7, step1470]: loss 0.733222
[epoch7, step1471]: loss 0.853185
[epoch7, step1472]: loss 0.601832
[epoch7, step1473]: loss 0.622507
[epoch7, step1474]: loss 0.636585
[epoch7, step1475]: loss 0.616111
[epoch7, step1476]: loss 0.554073
[epoch7, step1477]: loss 0.602422
[epoch7, step1478]: loss 0.217854
[epoch7, step1479]: loss 0.675581
[epoch7, step1480]: loss 0.425452
[epoch7, step1481]: loss 0.448416
[epoch7, step1482]: loss 0.385042
[epoch7, step1483]: loss 0.540129
[epoch7, step1484]: loss 0.602129
[epoch7, step1485]: loss 0.923974
[epoch7, step1486]: loss 0.728649
[epoch7, step1487]: loss 0.776225
[epoch7, step1488]: loss 0.559963
[epoch7, step1489]: loss 0.798009
[epoch7, step1490]: loss 0.498466
[epoch7, step1491]: loss 0.536493
[epoch7, step1492]: loss 0.410645
[epoch7, step1493]: loss 0.507934
[epoch7, step1494]: loss 0.742936
[epoch7, step1495]: loss 0.669320
[epoch7, step1496]: loss 0.560119
[epoch7, step1497]: loss 0.686279
[epoch7, step1498]: loss 0.517067
[epoch7, step1499]: loss 0.581508
[epoch7, step1500]: loss 0.612474
[epoch7, step1501]: loss 0.690663
[epoch7, step1502]: loss 0.868953
[epoch7, step1503]: loss 0.769219
[epoch7, step1504]: loss 0.579323
[epoch7, step1505]: loss 0.175286
[epoch7, step1506]: loss 0.311030
[epoch7, step1507]: loss 0.657309
[epoch7, step1508]: loss 0.760355
[epoch7, step1509]: loss 0.685394
[epoch7, step1510]: loss 0.857080
[epoch7, step1511]: loss 0.708212
[epoch7, step1512]: loss 0.718936
[epoch7, step1513]: loss 0.503264
[epoch7, step1514]: loss 0.984612
[epoch7, step1515]: loss 0.675201
[epoch7, step1516]: loss 0.671726
[epoch7, step1517]: loss 0.752980
[epoch7, step1518]: loss 0.823422
[epoch7, step1519]: loss 0.783525
[epoch7, step1520]: loss 0.864072
[epoch7, step1521]: loss 0.433594
[epoch7, step1522]: loss 0.712589
[epoch7, step1523]: loss 0.798325
[epoch7, step1524]: loss 0.540627
[epoch7, step1525]: loss 0.816643
[epoch7, step1526]: loss 0.496783
[epoch7, step1527]: loss 0.563683
[epoch7, step1528]: loss 0.579188
[epoch7, step1529]: loss 0.604044
[epoch7, step1530]: loss 0.612112
[epoch7, step1531]: loss 0.282895
[epoch7, step1532]: loss 0.414420
[epoch7, step1533]: loss 0.639448
[epoch7, step1534]: loss 0.625647
[epoch7, step1535]: loss 0.831351
[epoch7, step1536]: loss 0.628171
[epoch7, step1537]: loss 0.516798
[epoch7, step1538]: loss 0.594739
[epoch7, step1539]: loss 0.657338
[epoch7, step1540]: loss 0.561325
[epoch7, step1541]: loss 0.589587
[epoch7, step1542]: loss 0.649912
[epoch7, step1543]: loss 0.723148
[epoch7, step1544]: loss 0.690038
[epoch7, step1545]: loss 0.727754
[epoch7, step1546]: loss 0.604434
[epoch7, step1547]: loss 0.702753
[epoch7, step1548]: loss 0.742442
[epoch7, step1549]: loss 0.346823
[epoch7, step1550]: loss 0.709048
[epoch7, step1551]: loss 0.636815
[epoch7, step1552]: loss 0.432659
[epoch7, step1553]: loss 0.806867
[epoch7, step1554]: loss 0.605367
[epoch7, step1555]: loss 0.719327
[epoch7, step1556]: loss 0.382641
[epoch7, step1557]: loss 0.791990
[epoch7, step1558]: loss 0.660834
[epoch7, step1559]: loss 0.671974
[epoch7, step1560]: loss 0.672564
[epoch7, step1561]: loss 0.717685
[epoch7, step1562]: loss 0.424087
[epoch7, step1563]: loss 0.213939
[epoch7, step1564]: loss 0.670164
[epoch7, step1565]: loss 0.707579
[epoch7, step1566]: loss 0.864254
[epoch7, step1567]: loss 0.676138
[epoch7, step1568]: loss 0.708370
[epoch7, step1569]: loss 0.377063
[epoch7, step1570]: loss 0.529450
[epoch7, step1571]: loss 0.576761
[epoch7, step1572]: loss 0.536368
[epoch7, step1573]: loss 0.673019
[epoch7, step1574]: loss 0.923172
[epoch7, step1575]: loss 0.730721
[epoch7, step1576]: loss 0.929646
[epoch7, step1577]: loss 0.551449
[epoch7, step1578]: loss 0.488744
[epoch7, step1579]: loss 0.483438
[epoch7, step1580]: loss 0.604182
[epoch7, step1581]: loss 0.564864
[epoch7, step1582]: loss 0.671130
[epoch7, step1583]: loss 0.838413
[epoch7, step1584]: loss 0.529526
[epoch7, step1585]: loss 0.430507
[epoch7, step1586]: loss 0.832839
[epoch7, step1587]: loss 0.820338
[epoch7, step1588]: loss 0.756806
[epoch7, step1589]: loss 0.753491
[epoch7, step1590]: loss 0.660455
[epoch7, step1591]: loss 0.348289
[epoch7, step1592]: loss 0.799962
[epoch7, step1593]: loss 0.608773
[epoch7, step1594]: loss 0.663926
[epoch7, step1595]: loss 0.571103
[epoch7, step1596]: loss 0.655317
[epoch7, step1597]: loss 0.690579
[epoch7, step1598]: loss 0.665269
[epoch7, step1599]: loss 0.761354
[epoch7, step1600]: loss 0.774672
[epoch7, step1601]: loss 0.535783
[epoch7, step1602]: loss 0.638103
[epoch7, step1603]: loss 0.746347
[epoch7, step1604]: loss 0.534008
[epoch7, step1605]: loss 0.423318
[epoch7, step1606]: loss 0.515148
[epoch7, step1607]: loss 0.543306
[epoch7, step1608]: loss 0.783166
[epoch7, step1609]: loss 0.447521
[epoch7, step1610]: loss 0.631626
[epoch7, step1611]: loss 0.744056
[epoch7, step1612]: loss 0.565277
[epoch7, step1613]: loss 0.541646
[epoch7, step1614]: loss 0.378854
[epoch7, step1615]: loss 0.639111
[epoch7, step1616]: loss 0.515466
[epoch7, step1617]: loss 0.621468
[epoch7, step1618]: loss 0.782906
[epoch7, step1619]: loss 0.720073
[epoch7, step1620]: loss 0.725441
[epoch7, step1621]: loss 0.610585
[epoch7, step1622]: loss 0.702458
[epoch7, step1623]: loss 0.810388
[epoch7, step1624]: loss 0.687990
[epoch7, step1625]: loss 0.790144
[epoch7, step1626]: loss 0.720746
[epoch7, step1627]: loss 0.780518
[epoch7, step1628]: loss 0.821353
[epoch7, step1629]: loss 0.633443
[epoch7, step1630]: loss 0.884822
[epoch7, step1631]: loss 0.604205
[epoch7, step1632]: loss 0.706158
[epoch7, step1633]: loss 0.331658
[epoch7, step1634]: loss 0.750824
[epoch7, step1635]: loss 0.676051
[epoch7, step1636]: loss 0.671173
[epoch7, step1637]: loss 0.676045
[epoch7, step1638]: loss 0.847340
[epoch7, step1639]: loss 0.479428
[epoch7, step1640]: loss 0.643107
[epoch7, step1641]: loss 0.676037
[epoch7, step1642]: loss 0.852156
[epoch7, step1643]: loss 0.659918
[epoch7, step1644]: loss 0.580190
[epoch7, step1645]: loss 0.569877
[epoch7, step1646]: loss 0.627602
[epoch7, step1647]: loss 0.633185
[epoch7, step1648]: loss 0.623429
[epoch7, step1649]: loss 0.381483
[epoch7, step1650]: loss 0.392315
[epoch7, step1651]: loss 0.633352
[epoch7, step1652]: loss 0.665562
[epoch7, step1653]: loss 0.784320
[epoch7, step1654]: loss 0.506155
[epoch7, step1655]: loss 0.561811
[epoch7, step1656]: loss 0.493847
[epoch7, step1657]: loss 0.545401
[epoch7, step1658]: loss 0.823032
[epoch7, step1659]: loss 0.621803
[epoch7, step1660]: loss 0.532062
[epoch7, step1661]: loss 0.811349
[epoch7, step1662]: loss 0.632342
[epoch7, step1663]: loss 0.542981
[epoch7, step1664]: loss 0.368176
[epoch7, step1665]: loss 0.692245
[epoch7, step1666]: loss 0.638674
[epoch7, step1667]: loss 0.774968
[epoch7, step1668]: loss 0.747563
[epoch7, step1669]: loss 0.566938
[epoch7, step1670]: loss 0.556763
[epoch7, step1671]: loss 0.430259
[epoch7, step1672]: loss 0.794602
[epoch7, step1673]: loss 0.580006
[epoch7, step1674]: loss 0.782179
[epoch7, step1675]: loss 0.645197
[epoch7, step1676]: loss 0.635436
[epoch7, step1677]: loss 0.669273
[epoch7, step1678]: loss 0.704225
[epoch7, step1679]: loss 0.294847
[epoch7, step1680]: loss 0.481944
[epoch7, step1681]: loss 0.613206
[epoch7, step1682]: loss 0.719996
[epoch7, step1683]: loss 0.775640
[epoch7, step1684]: loss 0.870481
[epoch7, step1685]: loss 0.471806
[epoch7, step1686]: loss 0.781488
[epoch7, step1687]: loss 0.440216
[epoch7, step1688]: loss 0.650189
[epoch7, step1689]: loss 0.589320
[epoch7, step1690]: loss 0.852409
[epoch7, step1691]: loss 0.790447
[epoch7, step1692]: loss 0.329416
[epoch7, step1693]: loss 0.680912
[epoch7, step1694]: loss 0.493083
[epoch7, step1695]: loss 0.610743
[epoch7, step1696]: loss 0.672771
[epoch7, step1697]: loss 0.753593
[epoch7, step1698]: loss 0.605087
[epoch7, step1699]: loss 0.944791
[epoch7, step1700]: loss 0.805127
[epoch7, step1701]: loss 0.830340
[epoch7, step1702]: loss 0.795491
[epoch7, step1703]: loss 0.580187
[epoch7, step1704]: loss 0.704108
[epoch7, step1705]: loss 0.494038
[epoch7, step1706]: loss 0.458793
[epoch7, step1707]: loss 0.696682
[epoch7, step1708]: loss 0.558775
[epoch7, step1709]: loss 0.619268
[epoch7, step1710]: loss 0.831143
[epoch7, step1711]: loss 0.823056
[epoch7, step1712]: loss 0.519928
[epoch7, step1713]: loss 0.645435
[epoch7, step1714]: loss 0.788968
[epoch7, step1715]: loss 0.685914
[epoch7, step1716]: loss 0.484159
[epoch7, step1717]: loss 0.684199
[epoch7, step1718]: loss 0.559598
[epoch7, step1719]: loss 0.639739
[epoch7, step1720]: loss 0.495212
[epoch7, step1721]: loss 0.533659
[epoch7, step1722]: loss 0.574252
[epoch7, step1723]: loss 0.619298
[epoch7, step1724]: loss 0.602633
[epoch7, step1725]: loss 0.422351
[epoch7, step1726]: loss 0.555122
[epoch7, step1727]: loss 0.841117
[epoch7, step1728]: loss 0.736985
[epoch7, step1729]: loss 0.704759
[epoch7, step1730]: loss 0.724958
[epoch7, step1731]: loss 0.466172
[epoch7, step1732]: loss 0.668632
[epoch7, step1733]: loss 0.683058
[epoch7, step1734]: loss 0.673347
[epoch7, step1735]: loss 0.622786
[epoch7, step1736]: loss 0.535515
[epoch7, step1737]: loss 0.709556
[epoch7, step1738]: loss 0.467665
[epoch7, step1739]: loss 0.734333
[epoch7, step1740]: loss 0.727126
[epoch7, step1741]: loss 0.533830
[epoch7, step1742]: loss 0.298435
[epoch7, step1743]: loss 0.912011
[epoch7, step1744]: loss 0.526739
[epoch7, step1745]: loss 0.818102
[epoch7, step1746]: loss 0.296527
[epoch7, step1747]: loss 0.541341
[epoch7, step1748]: loss 0.701990
[epoch7, step1749]: loss 0.384339
[epoch7, step1750]: loss 0.440842
[epoch7, step1751]: loss 0.352141
[epoch7, step1752]: loss 0.803800
[epoch7, step1753]: loss 0.773782
[epoch7, step1754]: loss 0.379595
[epoch7, step1755]: loss 0.211310
[epoch7, step1756]: loss 0.594917
[epoch7, step1757]: loss 0.565014
[epoch7, step1758]: loss 0.634224
[epoch7, step1759]: loss 0.895301
[epoch7, step1760]: loss 0.341042
[epoch7, step1761]: loss 0.509413
[epoch7, step1762]: loss 0.795871
[epoch7, step1763]: loss 0.666679
[epoch7, step1764]: loss 0.449543
[epoch7, step1765]: loss 0.671224
[epoch7, step1766]: loss 0.841228
[epoch7, step1767]: loss 0.469312
[epoch7, step1768]: loss 0.709279
[epoch7, step1769]: loss 0.810643
[epoch7, step1770]: loss 0.785904
[epoch7, step1771]: loss 0.866224
[epoch7, step1772]: loss 0.374049
[epoch7, step1773]: loss 0.664806
[epoch7, step1774]: loss 0.647115
[epoch7, step1775]: loss 0.784389
[epoch7, step1776]: loss 0.635010
[epoch7, step1777]: loss 0.663535
[epoch7, step1778]: loss 0.573022
[epoch7, step1779]: loss 0.743211
[epoch7, step1780]: loss 0.703585
[epoch7, step1781]: loss 0.900980
[epoch7, step1782]: loss 0.373868
[epoch7, step1783]: loss 0.689760
[epoch7, step1784]: loss 0.549188
[epoch7, step1785]: loss 0.626231
[epoch7, step1786]: loss 0.686923
[epoch7, step1787]: loss 0.554656
[epoch7, step1788]: loss 0.713899
[epoch7, step1789]: loss 0.758695
[epoch7, step1790]: loss 0.585045
[epoch7, step1791]: loss 0.751759
[epoch7, step1792]: loss 0.470880
[epoch7, step1793]: loss 0.566647
[epoch7, step1794]: loss 0.623616
[epoch7, step1795]: loss 0.404466
[epoch7, step1796]: loss 0.872509
[epoch7, step1797]: loss 0.816785
[epoch7, step1798]: loss 0.628531
[epoch7, step1799]: loss 0.657014
[epoch7, step1800]: loss 0.689362
[epoch7, step1801]: loss 0.743437
[epoch7, step1802]: loss 0.650206
[epoch7, step1803]: loss 0.472907
[epoch7, step1804]: loss 0.646662
[epoch7, step1805]: loss 0.340608
[epoch7, step1806]: loss 0.716527
[epoch7, step1807]: loss 0.912707
[epoch7, step1808]: loss 0.671234
[epoch7, step1809]: loss 0.551048
[epoch7, step1810]: loss 0.617261
[epoch7, step1811]: loss 0.764459
[epoch7, step1812]: loss 0.904276
[epoch7, step1813]: loss 0.729267
[epoch7, step1814]: loss 0.782887
[epoch7, step1815]: loss 0.249840
[epoch7, step1816]: loss 0.470950
[epoch7, step1817]: loss 0.702732
[epoch7, step1818]: loss 0.728180
[epoch7, step1819]: loss 0.702070
[epoch7, step1820]: loss 0.764792
[epoch7, step1821]: loss 0.700728
[epoch7, step1822]: loss 0.640224
[epoch7, step1823]: loss 0.677683
[epoch7, step1824]: loss 0.618501
[epoch7, step1825]: loss 0.668025
[epoch7, step1826]: loss 0.584553
[epoch7, step1827]: loss 0.669709
[epoch7, step1828]: loss 0.503795
[epoch7, step1829]: loss 0.472900
[epoch7, step1830]: loss 0.701997
[epoch7, step1831]: loss 0.396999
[epoch7, step1832]: loss 0.861552
[epoch7, step1833]: loss 0.569946
[epoch7, step1834]: loss 0.722502
[epoch7, step1835]: loss 0.850284
[epoch7, step1836]: loss 0.748232
[epoch7, step1837]: loss 0.756124
[epoch7, step1838]: loss 0.866494
[epoch7, step1839]: loss 0.753437
[epoch7, step1840]: loss 0.838681
[epoch7, step1841]: loss 0.460549
[epoch7, step1842]: loss 0.647340
[epoch7, step1843]: loss 0.907958
[epoch7, step1844]: loss 0.645861
[epoch7, step1845]: loss 0.677050
[epoch7, step1846]: loss 0.443216
[epoch7, step1847]: loss 0.597726
[epoch7, step1848]: loss 0.833994
[epoch7, step1849]: loss 0.507538
[epoch7, step1850]: loss 0.333635
[epoch7, step1851]: loss 0.668373
[epoch7, step1852]: loss 0.490663
[epoch7, step1853]: loss 0.572750
[epoch7, step1854]: loss 0.559245
[epoch7, step1855]: loss 0.671063
[epoch7, step1856]: loss 0.783272
[epoch7, step1857]: loss 0.597715
[epoch7, step1858]: loss 0.701650
[epoch7, step1859]: loss 0.456924
[epoch7, step1860]: loss 0.637333
[epoch7, step1861]: loss 0.353547
[epoch7, step1862]: loss 0.455423
[epoch7, step1863]: loss 0.485470
[epoch7, step1864]: loss 0.561872
[epoch7, step1865]: loss 0.677188
[epoch7, step1866]: loss 0.468951
[epoch7, step1867]: loss 0.474340
[epoch7, step1868]: loss 0.455870
[epoch7, step1869]: loss 0.609840
[epoch7, step1870]: loss 0.473171
[epoch7, step1871]: loss 0.717719
[epoch7, step1872]: loss 0.632135
[epoch7, step1873]: loss 0.566724
[epoch7, step1874]: loss 0.694699
[epoch7, step1875]: loss 0.838263
[epoch7, step1876]: loss 0.667049
[epoch7, step1877]: loss 0.587187
[epoch7, step1878]: loss 0.449698
[epoch7, step1879]: loss 0.920794
[epoch7, step1880]: loss 0.685036
[epoch7, step1881]: loss 0.492694
[epoch7, step1882]: loss 0.605244
[epoch7, step1883]: loss 0.519953
[epoch7, step1884]: loss 0.399358
[epoch7, step1885]: loss 0.487779
[epoch7, step1886]: loss 0.628744
[epoch7, step1887]: loss 0.646253
[epoch7, step1888]: loss 0.618800
[epoch7, step1889]: loss 0.738148
[epoch7, step1890]: loss 0.764269
[epoch7, step1891]: loss 0.556259
[epoch7, step1892]: loss 0.477717
[epoch7, step1893]: loss 0.631233
[epoch7, step1894]: loss 0.817984
[epoch7, step1895]: loss 0.841132
[epoch7, step1896]: loss 0.830448
[epoch7, step1897]: loss 0.295911
[epoch7, step1898]: loss 0.762915
[epoch7, step1899]: loss 0.496751
[epoch7, step1900]: loss 0.674610
[epoch7, step1901]: loss 0.343177
[epoch7, step1902]: loss 0.625654
[epoch7, step1903]: loss 0.420719
[epoch7, step1904]: loss 0.554623
[epoch7, step1905]: loss 0.470760
[epoch7, step1906]: loss 0.558504
[epoch7, step1907]: loss 0.707249
[epoch7, step1908]: loss 0.595700
[epoch7, step1909]: loss 0.658393
[epoch7, step1910]: loss 0.767229
[epoch7, step1911]: loss 0.761122
[epoch7, step1912]: loss 0.505476
[epoch7, step1913]: loss 0.505793
[epoch7, step1914]: loss 0.745593
[epoch7, step1915]: loss 0.445098
[epoch7, step1916]: loss 0.822626
[epoch7, step1917]: loss 0.762405
[epoch7, step1918]: loss 0.485615
[epoch7, step1919]: loss 0.531780
[epoch7, step1920]: loss 0.767975
[epoch7, step1921]: loss 0.573923
[epoch7, step1922]: loss 0.684546
[epoch7, step1923]: loss 0.970014
[epoch7, step1924]: loss 0.494594
[epoch7, step1925]: loss 0.650272
[epoch7, step1926]: loss 0.814299
[epoch7, step1927]: loss 0.490780
[epoch7, step1928]: loss 0.600257
[epoch7, step1929]: loss 0.702745
[epoch7, step1930]: loss 0.530197
[epoch7, step1931]: loss 0.527868
[epoch7, step1932]: loss 0.579097
[epoch7, step1933]: loss 0.541604
[epoch7, step1934]: loss 0.703571
[epoch7, step1935]: loss 0.724329
[epoch7, step1936]: loss 0.657212
[epoch7, step1937]: loss 0.693477
[epoch7, step1938]: loss 0.552754
[epoch7, step1939]: loss 0.473530
[epoch7, step1940]: loss 0.809817
[epoch7, step1941]: loss 0.747873
[epoch7, step1942]: loss 0.509693
[epoch7, step1943]: loss 0.355424
[epoch7, step1944]: loss 0.579406
[epoch7, step1945]: loss 0.679557
[epoch7, step1946]: loss 0.683719
[epoch7, step1947]: loss 0.852358
[epoch7, step1948]: loss 0.663974
[epoch7, step1949]: loss 0.638749
[epoch7, step1950]: loss 0.420783
[epoch7, step1951]: loss 0.542424
[epoch7, step1952]: loss 0.401834
[epoch7, step1953]: loss 0.519293
[epoch7, step1954]: loss 0.714670
[epoch7, step1955]: loss 0.351059
[epoch7, step1956]: loss 0.438020
[epoch7, step1957]: loss 0.432769
[epoch7, step1958]: loss 0.843651
[epoch7, step1959]: loss 0.669175
[epoch7, step1960]: loss 0.948316
[epoch7, step1961]: loss 0.477625
[epoch7, step1962]: loss 0.652645
[epoch7, step1963]: loss 0.735574
[epoch7, step1964]: loss 0.871986
[epoch7, step1965]: loss 0.346170
[epoch7, step1966]: loss 0.516886
[epoch7, step1967]: loss 0.673701
[epoch7, step1968]: loss 0.901980
[epoch7, step1969]: loss 0.650462
[epoch7, step1970]: loss 0.877190
[epoch7, step1971]: loss 0.854744
[epoch7, step1972]: loss 0.637251
[epoch7, step1973]: loss 0.549896
[epoch7, step1974]: loss 0.714780
[epoch7, step1975]: loss 0.548832
[epoch7, step1976]: loss 0.692715
[epoch7, step1977]: loss 0.404256
[epoch7, step1978]: loss 0.676884
[epoch7, step1979]: loss 0.675128
[epoch7, step1980]: loss 0.732548
[epoch7, step1981]: loss 0.797832
[epoch7, step1982]: loss 0.619405
[epoch7, step1983]: loss 0.559913
[epoch7, step1984]: loss 0.526469
[epoch7, step1985]: loss 0.895054
[epoch7, step1986]: loss 0.219752
[epoch7, step1987]: loss 0.700925
[epoch7, step1988]: loss 0.478862
[epoch7, step1989]: loss 0.573326
[epoch7, step1990]: loss 0.612036
[epoch7, step1991]: loss 0.723127
[epoch7, step1992]: loss 0.563716
[epoch7, step1993]: loss 0.707181
[epoch7, step1994]: loss 0.625115
[epoch7, step1995]: loss 0.722091
[epoch7, step1996]: loss 0.696941
[epoch7, step1997]: loss 0.573709
[epoch7, step1998]: loss 0.508624
[epoch7, step1999]: loss 0.557927
[epoch7, step2000]: loss 0.720441
[epoch7, step2001]: loss 0.592742
[epoch7, step2002]: loss 0.674092
[epoch7, step2003]: loss 0.698095
[epoch7, step2004]: loss 0.432400
[epoch7, step2005]: loss 0.545125
[epoch7, step2006]: loss 0.622337
[epoch7, step2007]: loss 0.697900
[epoch7, step2008]: loss 0.692800
[epoch7, step2009]: loss 0.397671
[epoch7, step2010]: loss 0.768844
[epoch7, step2011]: loss 0.462335
[epoch7, step2012]: loss 0.572118
[epoch7, step2013]: loss 0.543494
[epoch7, step2014]: loss 0.795419
[epoch7, step2015]: loss 0.747381
[epoch7, step2016]: loss 0.862964
[epoch7, step2017]: loss 0.687255
[epoch7, step2018]: loss 0.264561
[epoch7, step2019]: loss 0.755350
[epoch7, step2020]: loss 0.632443
[epoch7, step2021]: loss 0.588213
[epoch7, step2022]: loss 0.636507
[epoch7, step2023]: loss 0.774519
[epoch7, step2024]: loss 0.583454
[epoch7, step2025]: loss 0.515138
[epoch7, step2026]: loss 0.662584
[epoch7, step2027]: loss 0.721638
[epoch7, step2028]: loss 0.630942
[epoch7, step2029]: loss 0.911526
[epoch7, step2030]: loss 0.411860
[epoch7, step2031]: loss 0.285134
[epoch7, step2032]: loss 0.851249
[epoch7, step2033]: loss 0.745046
[epoch7, step2034]: loss 0.679994
[epoch7, step2035]: loss 0.710886
[epoch7, step2036]: loss 0.575735
[epoch7, step2037]: loss 0.467821
[epoch7, step2038]: loss 0.716415
[epoch7, step2039]: loss 0.687664
[epoch7, step2040]: loss 0.716690
[epoch7, step2041]: loss 0.567385
[epoch7, step2042]: loss 0.688488
[epoch7, step2043]: loss 0.704894
[epoch7, step2044]: loss 0.821130
[epoch7, step2045]: loss 0.463070
[epoch7, step2046]: loss 0.557793
[epoch7, step2047]: loss 0.540892
[epoch7, step2048]: loss 0.397133
[epoch7, step2049]: loss 0.504581
[epoch7, step2050]: loss 0.507613
[epoch7, step2051]: loss 0.607504
[epoch7, step2052]: loss 0.663148
[epoch7, step2053]: loss 0.745484
[epoch7, step2054]: loss 0.730016
[epoch7, step2055]: loss 0.766060
[epoch7, step2056]: loss 0.684669
[epoch7, step2057]: loss 0.797234
[epoch7, step2058]: loss 0.624358
[epoch7, step2059]: loss 0.443496
[epoch7, step2060]: loss 0.756258
[epoch7, step2061]: loss 0.565772
[epoch7, step2062]: loss 0.526656
[epoch7, step2063]: loss 0.785689
[epoch7, step2064]: loss 0.846870
[epoch7, step2065]: loss 0.628392
[epoch7, step2066]: loss 0.541919
[epoch7, step2067]: loss 0.478248
[epoch7, step2068]: loss 0.706751
[epoch7, step2069]: loss 0.577951
[epoch7, step2070]: loss 0.675438
[epoch7, step2071]: loss 0.717164
[epoch7, step2072]: loss 0.432404
[epoch7, step2073]: loss 0.675852
[epoch7, step2074]: loss 0.763603
[epoch7, step2075]: loss 0.859626
[epoch7, step2076]: loss 0.752274
[epoch7, step2077]: loss 0.620790
[epoch7, step2078]: loss 0.820613
[epoch7, step2079]: loss 0.735624
[epoch7, step2080]: loss 0.668526
[epoch7, step2081]: loss 0.322345
[epoch7, step2082]: loss 0.322929
[epoch7, step2083]: loss 0.489422
[epoch7, step2084]: loss 0.447301
[epoch7, step2085]: loss 0.738061
[epoch7, step2086]: loss 0.722321
[epoch7, step2087]: loss 0.583675
[epoch7, step2088]: loss 0.837367
[epoch7, step2089]: loss 0.513996
[epoch7, step2090]: loss 0.727830
[epoch7, step2091]: loss 0.659460
[epoch7, step2092]: loss 0.572346
[epoch7, step2093]: loss 0.551323
[epoch7, step2094]: loss 0.605342
[epoch7, step2095]: loss 0.695165
[epoch7, step2096]: loss 0.686035
[epoch7, step2097]: loss 0.536725
[epoch7, step2098]: loss 0.334047
[epoch7, step2099]: loss 0.636099
[epoch7, step2100]: loss 0.480022
[epoch7, step2101]: loss 0.608046
[epoch7, step2102]: loss 0.598237
[epoch7, step2103]: loss 0.888125
[epoch7, step2104]: loss 0.619810
[epoch7, step2105]: loss 0.736303
[epoch7, step2106]: loss 0.432103
[epoch7, step2107]: loss 0.873539
[epoch7, step2108]: loss 0.731859
[epoch7, step2109]: loss 0.846406
[epoch7, step2110]: loss 0.596131
[epoch7, step2111]: loss 0.536442
[epoch7, step2112]: loss 0.799572
[epoch7, step2113]: loss 0.549686
[epoch7, step2114]: loss 0.655943
[epoch7, step2115]: loss 0.694766
[epoch7, step2116]: loss 0.707465
[epoch7, step2117]: loss 0.509681
[epoch7, step2118]: loss 0.861466
[epoch7, step2119]: loss 0.482733
[epoch7, step2120]: loss 0.729659
[epoch7, step2121]: loss 0.681629
[epoch7, step2122]: loss 0.665257
[epoch7, step2123]: loss 0.678343
[epoch7, step2124]: loss 0.676493
[epoch7, step2125]: loss 0.578220
[epoch7, step2126]: loss 0.525225
[epoch7, step2127]: loss 0.690117
[epoch7, step2128]: loss 0.696407
[epoch7, step2129]: loss 0.515583
[epoch7, step2130]: loss 0.671525
[epoch7, step2131]: loss 0.744034
[epoch7, step2132]: loss 0.854411
[epoch7, step2133]: loss 0.335161
[epoch7, step2134]: loss 0.673685
[epoch7, step2135]: loss 0.683804
[epoch7, step2136]: loss 0.740926
[epoch7, step2137]: loss 0.621077
[epoch7, step2138]: loss 0.321829
[epoch7, step2139]: loss 0.560786
[epoch7, step2140]: loss 0.596781
[epoch7, step2141]: loss 0.425149
[epoch7, step2142]: loss 0.748992
[epoch7, step2143]: loss 0.457441
[epoch7, step2144]: loss 0.388394
[epoch7, step2145]: loss 0.616755
[epoch7, step2146]: loss 0.696475
[epoch7, step2147]: loss 0.551382
[epoch7, step2148]: loss 0.830744
[epoch7, step2149]: loss 0.557499
[epoch7, step2150]: loss 0.624692
[epoch7, step2151]: loss 0.601837
[epoch7, step2152]: loss 0.661214
[epoch7, step2153]: loss 0.597461
[epoch7, step2154]: loss 0.629332
[epoch7, step2155]: loss 0.261263
[epoch7, step2156]: loss 0.547585
[epoch7, step2157]: loss 0.689178
[epoch7, step2158]: loss 0.838693
[epoch7, step2159]: loss 0.726038
[epoch7, step2160]: loss 0.485171
[epoch7, step2161]: loss 0.844408
[epoch7, step2162]: loss 0.533013
[epoch7, step2163]: loss 0.663616
[epoch7, step2164]: loss 0.675906
[epoch7, step2165]: loss 0.590178
[epoch7, step2166]: loss 0.866316
[epoch7, step2167]: loss 0.586520
[epoch7, step2168]: loss 0.972298
[epoch7, step2169]: loss 0.662634
[epoch7, step2170]: loss 0.692548
[epoch7, step2171]: loss 0.592247
[epoch7, step2172]: loss 0.402096
[epoch7, step2173]: loss 0.574612
[epoch7, step2174]: loss 0.659243
[epoch7, step2175]: loss 0.655509
[epoch7, step2176]: loss 0.872462
[epoch7, step2177]: loss 0.764431
[epoch7, step2178]: loss 0.615628
[epoch7, step2179]: loss 0.570881
[epoch7, step2180]: loss 0.638490
[epoch7, step2181]: loss 0.841078
[epoch7, step2182]: loss 0.671789
[epoch7, step2183]: loss 0.502479
[epoch7, step2184]: loss 0.664055
[epoch7, step2185]: loss 0.619435
[epoch7, step2186]: loss 0.521442
[epoch7, step2187]: loss 0.696636
[epoch7, step2188]: loss 0.387631
[epoch7, step2189]: loss 0.664751
[epoch7, step2190]: loss 0.525814
[epoch7, step2191]: loss 0.489859
[epoch7, step2192]: loss 0.903720
[epoch7, step2193]: loss 0.863820
[epoch7, step2194]: loss 0.707581
[epoch7, step2195]: loss 0.890243
[epoch7, step2196]: loss 0.725379
[epoch7, step2197]: loss 0.691515
[epoch7, step2198]: loss 0.816784
[epoch7, step2199]: loss 0.428171
[epoch7, step2200]: loss 0.805280
[epoch7, step2201]: loss 0.684848
[epoch7, step2202]: loss 0.590220
[epoch7, step2203]: loss 0.505773
[epoch7, step2204]: loss 0.395460
[epoch7, step2205]: loss 0.619066
[epoch7, step2206]: loss 0.586026
[epoch7, step2207]: loss 0.712658
[epoch7, step2208]: loss 0.497683
[epoch7, step2209]: loss 0.872044
[epoch7, step2210]: loss 0.533901
[epoch7, step2211]: loss 0.505481
[epoch7, step2212]: loss 0.506088
[epoch7, step2213]: loss 0.751407
[epoch7, step2214]: loss 0.574225
[epoch7, step2215]: loss 0.483826
[epoch7, step2216]: loss 0.592169
[epoch7, step2217]: loss 0.518092
[epoch7, step2218]: loss 0.811330
[epoch7, step2219]: loss 0.685336
[epoch7, step2220]: loss 0.778526
[epoch7, step2221]: loss 0.954092
[epoch7, step2222]: loss 0.743412
[epoch7, step2223]: loss 0.572309
[epoch7, step2224]: loss 0.785729
[epoch7, step2225]: loss 0.384734
[epoch7, step2226]: loss 0.293674
[epoch7, step2227]: loss 0.674112
[epoch7, step2228]: loss 0.691638
[epoch7, step2229]: loss 0.873373
[epoch7, step2230]: loss 0.623987
[epoch7, step2231]: loss 0.501490
[epoch7, step2232]: loss 0.850544
[epoch7, step2233]: loss 0.594899
[epoch7, step2234]: loss 0.856894
[epoch7, step2235]: loss 0.660537
[epoch7, step2236]: loss 0.717576
[epoch7, step2237]: loss 0.292559
[epoch7, step2238]: loss 0.732004
[epoch7, step2239]: loss 0.470651
[epoch7, step2240]: loss 0.799649
[epoch7, step2241]: loss 0.715911
[epoch7, step2242]: loss 0.716558
[epoch7, step2243]: loss 0.589870
[epoch7, step2244]: loss 0.726098
[epoch7, step2245]: loss 0.691800
[epoch7, step2246]: loss 0.666714
[epoch7, step2247]: loss 0.942485
[epoch7, step2248]: loss 0.653216
[epoch7, step2249]: loss 0.666722
[epoch7, step2250]: loss 0.650586
[epoch7, step2251]: loss 0.498046
[epoch7, step2252]: loss 0.580001
[epoch7, step2253]: loss 0.564314
[epoch7, step2254]: loss 0.779299
[epoch7, step2255]: loss 0.732518
[epoch7, step2256]: loss 0.704031
[epoch7, step2257]: loss 0.610170
[epoch7, step2258]: loss 0.617663
[epoch7, step2259]: loss 0.592014
[epoch7, step2260]: loss 0.333816
[epoch7, step2261]: loss 0.270034
[epoch7, step2262]: loss 0.552556
[epoch7, step2263]: loss 0.576737
[epoch7, step2264]: loss 0.852092
[epoch7, step2265]: loss 0.680642
[epoch7, step2266]: loss 0.346716
[epoch7, step2267]: loss 0.739744
[epoch7, step2268]: loss 0.614374
[epoch7, step2269]: loss 0.585412
[epoch7, step2270]: loss 0.749150
[epoch7, step2271]: loss 0.749236
[epoch7, step2272]: loss 0.491322
[epoch7, step2273]: loss 0.546218
[epoch7, step2274]: loss 0.507450
[epoch7, step2275]: loss 0.600680
[epoch7, step2276]: loss 0.796101
[epoch7, step2277]: loss 0.836059
[epoch7, step2278]: loss 0.737230
[epoch7, step2279]: loss 0.746466
[epoch7, step2280]: loss 0.626090
[epoch7, step2281]: loss 0.267157
[epoch7, step2282]: loss 0.862535
[epoch7, step2283]: loss 0.598341
[epoch7, step2284]: loss 0.323514
[epoch7, step2285]: loss 0.705359
[epoch7, step2286]: loss 0.886196
[epoch7, step2287]: loss 0.524427
[epoch7, step2288]: loss 0.728499
[epoch7, step2289]: loss 0.483108
[epoch7, step2290]: loss 0.433201
[epoch7, step2291]: loss 0.504062
[epoch7, step2292]: loss 0.522000
[epoch7, step2293]: loss 0.473947
[epoch7, step2294]: loss 0.724485
[epoch7, step2295]: loss 0.866830
[epoch7, step2296]: loss 0.658915
[epoch7, step2297]: loss 0.757751
[epoch7, step2298]: loss 0.720567
[epoch7, step2299]: loss 0.152980
[epoch7, step2300]: loss 0.394510
[epoch7, step2301]: loss 0.496979
[epoch7, step2302]: loss 0.515992
[epoch7, step2303]: loss 0.385046
[epoch7, step2304]: loss 0.560491
[epoch7, step2305]: loss 0.625418
[epoch7, step2306]: loss 0.429812
[epoch7, step2307]: loss 0.571082
[epoch7, step2308]: loss 0.628766
[epoch7, step2309]: loss 0.340545
[epoch7, step2310]: loss 0.903899
[epoch7, step2311]: loss 0.375036
[epoch7, step2312]: loss 0.499851
[epoch7, step2313]: loss 0.663097
[epoch7, step2314]: loss 0.679233
[epoch7, step2315]: loss 0.643204
[epoch7, step2316]: loss 0.755855
[epoch7, step2317]: loss 0.645679
[epoch7, step2318]: loss 0.671941
[epoch7, step2319]: loss 0.567283
[epoch7, step2320]: loss 0.573695
[epoch7, step2321]: loss 0.684646
[epoch7, step2322]: loss 0.887495
[epoch7, step2323]: loss 0.381552
[epoch7, step2324]: loss 0.771834
[epoch7, step2325]: loss 0.613832
[epoch7, step2326]: loss 0.326148
[epoch7, step2327]: loss 0.516414
[epoch7, step2328]: loss 0.656640
[epoch7, step2329]: loss 0.588484
[epoch7, step2330]: loss 0.369611
[epoch7, step2331]: loss 0.593504
[epoch7, step2332]: loss 0.480436
[epoch7, step2333]: loss 0.749003
[epoch7, step2334]: loss 0.733045
[epoch7, step2335]: loss 0.585574
[epoch7, step2336]: loss 0.754497
[epoch7, step2337]: loss 0.623422
[epoch7, step2338]: loss 0.775932
[epoch7, step2339]: loss 0.647749
[epoch7, step2340]: loss 0.667244
[epoch7, step2341]: loss 0.711986
[epoch7, step2342]: loss 0.431839
[epoch7, step2343]: loss 0.561957
[epoch7, step2344]: loss 0.409726
[epoch7, step2345]: loss 0.469034
[epoch7, step2346]: loss 0.675105
[epoch7, step2347]: loss 0.695596
[epoch7, step2348]: loss 0.695129
[epoch7, step2349]: loss 0.584731
[epoch7, step2350]: loss 0.630549
[epoch7, step2351]: loss 0.781871
[epoch7, step2352]: loss 0.670823
[epoch7, step2353]: loss 0.543350
[epoch7, step2354]: loss 0.842049
[epoch7, step2355]: loss 0.725235
[epoch7, step2356]: loss 0.855001
[epoch7, step2357]: loss 0.650454
[epoch7, step2358]: loss 0.829597
[epoch7, step2359]: loss 0.706795
[epoch7, step2360]: loss 0.761648
[epoch7, step2361]: loss 0.704921
[epoch7, step2362]: loss 0.570123
[epoch7, step2363]: loss 0.661587
[epoch7, step2364]: loss 0.524756
[epoch7, step2365]: loss 0.563968
[epoch7, step2366]: loss 0.525258
[epoch7, step2367]: loss 0.513587
[epoch7, step2368]: loss 0.524019
[epoch7, step2369]: loss 0.663867
[epoch7, step2370]: loss 0.781438
[epoch7, step2371]: loss 0.719459
[epoch7, step2372]: loss 0.399350
[epoch7, step2373]: loss 0.775157
[epoch7, step2374]: loss 0.403454
[epoch7, step2375]: loss 0.476205
[epoch7, step2376]: loss 0.575027
[epoch7, step2377]: loss 0.489656
[epoch7, step2378]: loss 0.690250
[epoch7, step2379]: loss 0.776663
[epoch7, step2380]: loss 0.492061
[epoch7, step2381]: loss 0.620820
[epoch7, step2382]: loss 0.658789
[epoch7, step2383]: loss 0.743502
[epoch7, step2384]: loss 0.419465
[epoch7, step2385]: loss 0.672030
[epoch7, step2386]: loss 0.366770
[epoch7, step2387]: loss 0.441443
[epoch7, step2388]: loss 0.632316
[epoch7, step2389]: loss 0.686792
[epoch7, step2390]: loss 0.658523
[epoch7, step2391]: loss 0.691085
[epoch7, step2392]: loss 0.701735
[epoch7, step2393]: loss 0.577218
[epoch7, step2394]: loss 0.554793
[epoch7, step2395]: loss 0.424587
[epoch7, step2396]: loss 0.798508
[epoch7, step2397]: loss 0.785530
[epoch7, step2398]: loss 0.455940
[epoch7, step2399]: loss 0.641271
[epoch7, step2400]: loss 0.802247
[epoch7, step2401]: loss 0.873864
[epoch7, step2402]: loss 0.722603
[epoch7, step2403]: loss 0.235867
[epoch7, step2404]: loss 0.604387
[epoch7, step2405]: loss 0.600518
[epoch7, step2406]: loss 0.819911
[epoch7, step2407]: loss 0.624044
[epoch7, step2408]: loss 0.711355
[epoch7, step2409]: loss 0.654418
[epoch7, step2410]: loss 0.671263
[epoch7, step2411]: loss 0.917975
[epoch7, step2412]: loss 0.706893
[epoch7, step2413]: loss 0.520540
[epoch7, step2414]: loss 0.564224
[epoch7, step2415]: loss 0.683310
[epoch7, step2416]: loss 0.447493
[epoch7, step2417]: loss 0.854696
[epoch7, step2418]: loss 0.572323
[epoch7, step2419]: loss 0.379181
[epoch7, step2420]: loss 0.502194
[epoch7, step2421]: loss 0.567483
[epoch7, step2422]: loss 0.833637
[epoch7, step2423]: loss 0.713583
[epoch7, step2424]: loss 0.892014
[epoch7, step2425]: loss 0.790793
[epoch7, step2426]: loss 0.587085
[epoch7, step2427]: loss 0.514868
[epoch7, step2428]: loss 0.844709
[epoch7, step2429]: loss 0.550688
[epoch7, step2430]: loss 0.642751
[epoch7, step2431]: loss 0.692178
[epoch7, step2432]: loss 0.599646
[epoch7, step2433]: loss 0.359405
[epoch7, step2434]: loss 0.406890
[epoch7, step2435]: loss 0.571297
[epoch7, step2436]: loss 0.605823
[epoch7, step2437]: loss 0.474562
[epoch7, step2438]: loss 0.864378
[epoch7, step2439]: loss 0.772449
[epoch7, step2440]: loss 0.470325
[epoch7, step2441]: loss 0.635301
[epoch7, step2442]: loss 0.547724
[epoch7, step2443]: loss 0.711632
[epoch7, step2444]: loss 0.566614
[epoch7, step2445]: loss 0.393842
[epoch7, step2446]: loss 0.294166
[epoch7, step2447]: loss 0.667978
[epoch7, step2448]: loss 0.601646
[epoch7, step2449]: loss 0.643597
[epoch7, step2450]: loss 0.505301
[epoch7, step2451]: loss 0.565778
[epoch7, step2452]: loss 0.411719
[epoch7, step2453]: loss 0.397067
[epoch7, step2454]: loss 0.641814
[epoch7, step2455]: loss 0.857949
[epoch7, step2456]: loss 0.487601
[epoch7, step2457]: loss 0.580621
[epoch7, step2458]: loss 0.851727
[epoch7, step2459]: loss 0.454154
[epoch7, step2460]: loss 0.678705
[epoch7, step2461]: loss 0.601347
[epoch7, step2462]: loss 0.686572
[epoch7, step2463]: loss 0.395654
[epoch7, step2464]: loss 0.525536
[epoch7, step2465]: loss 0.441429
[epoch7, step2466]: loss 0.453801
[epoch7, step2467]: loss 0.657803
[epoch7, step2468]: loss 0.596314
[epoch7, step2469]: loss 0.391693
[epoch7, step2470]: loss 0.473912
[epoch7, step2471]: loss 0.561385
[epoch7, step2472]: loss 0.746104
[epoch7, step2473]: loss 0.593157
[epoch7, step2474]: loss 0.554693
[epoch7, step2475]: loss 0.508845
[epoch7, step2476]: loss 0.850196
[epoch7, step2477]: loss 0.509687
[epoch7, step2478]: loss 0.592436
[epoch7, step2479]: loss 0.706359
[epoch7, step2480]: loss 0.641224
[epoch7, step2481]: loss 0.743258
[epoch7, step2482]: loss 0.797322
[epoch7, step2483]: loss 0.782738
[epoch7, step2484]: loss 0.447738
[epoch7, step2485]: loss 0.506094
[epoch7, step2486]: loss 0.502672
[epoch7, step2487]: loss 0.532478
[epoch7, step2488]: loss 0.793197
[epoch7, step2489]: loss 0.894235
[epoch7, step2490]: loss 0.576100
[epoch7, step2491]: loss 0.675825
[epoch7, step2492]: loss 0.678860
[epoch7, step2493]: loss 0.683376
[epoch7, step2494]: loss 0.634689
[epoch7, step2495]: loss 0.559767
[epoch7, step2496]: loss 0.421095
[epoch7, step2497]: loss 0.536637
[epoch7, step2498]: loss 0.564241
[epoch7, step2499]: loss 0.937886
[epoch7, step2500]: loss 0.622282
[epoch7, step2501]: loss 0.795664
[epoch7, step2502]: loss 0.505778
[epoch7, step2503]: loss 0.584314
[epoch7, step2504]: loss 0.662780
[epoch7, step2505]: loss 0.484863
[epoch7, step2506]: loss 0.509903
[epoch7, step2507]: loss 0.768016
[epoch7, step2508]: loss 0.696437
[epoch7, step2509]: loss 0.728943
[epoch7, step2510]: loss 0.746640
[epoch7, step2511]: loss 0.628188
[epoch7, step2512]: loss 0.677796
[epoch7, step2513]: loss 0.397688
[epoch7, step2514]: loss 0.465975
[epoch7, step2515]: loss 0.732718
[epoch7, step2516]: loss 0.607737
[epoch7, step2517]: loss 0.742653
[epoch7, step2518]: loss 0.635136
[epoch7, step2519]: loss 0.698944
[epoch7, step2520]: loss 0.659076
[epoch7, step2521]: loss 0.544311
[epoch7, step2522]: loss 0.636347
[epoch7, step2523]: loss 0.783475
[epoch7, step2524]: loss 0.486717
[epoch7, step2525]: loss 0.842582
[epoch7, step2526]: loss 0.473099
[epoch7, step2527]: loss 0.500300
[epoch7, step2528]: loss 0.735912
[epoch7, step2529]: loss 0.640394
[epoch7, step2530]: loss 0.684376
[epoch7, step2531]: loss 0.702165
[epoch7, step2532]: loss 0.595148
[epoch7, step2533]: loss 0.428717
[epoch7, step2534]: loss 0.438803
[epoch7, step2535]: loss 0.277313
[epoch7, step2536]: loss 0.705954
[epoch7, step2537]: loss 0.638366
[epoch7, step2538]: loss 0.717479
[epoch7, step2539]: loss 0.581757
[epoch7, step2540]: loss 0.554360
[epoch7, step2541]: loss 0.575731
[epoch7, step2542]: loss 0.532081
[epoch7, step2543]: loss 0.704826
[epoch7, step2544]: loss 0.569882
[epoch7, step2545]: loss 0.259697
[epoch7, step2546]: loss 0.608293
[epoch7, step2547]: loss 0.620655
[epoch7, step2548]: loss 0.850698
[epoch7, step2549]: loss 0.452375
[epoch7, step2550]: loss 0.670990
[epoch7, step2551]: loss 0.813715
[epoch7, step2552]: loss 0.551685
[epoch7, step2553]: loss 0.766806
[epoch7, step2554]: loss 0.601708
[epoch7, step2555]: loss 0.820823
[epoch7, step2556]: loss 0.570633
[epoch7, step2557]: loss 0.665360
[epoch7, step2558]: loss 0.949555
[epoch7, step2559]: loss 0.667231
[epoch7, step2560]: loss 0.613608
[epoch7, step2561]: loss 0.290339
[epoch7, step2562]: loss 0.879688
[epoch7, step2563]: loss 0.562609
[epoch7, step2564]: loss 0.196169
[epoch7, step2565]: loss 0.678111
[epoch7, step2566]: loss 0.571491
[epoch7, step2567]: loss 0.827403
[epoch7, step2568]: loss 0.289103
[epoch7, step2569]: loss 0.616255
[epoch7, step2570]: loss 0.875027
[epoch7, step2571]: loss 0.521432
[epoch7, step2572]: loss 0.401430
[epoch7, step2573]: loss 0.671500
[epoch7, step2574]: loss 0.568381
[epoch7, step2575]: loss 0.833672
[epoch7, step2576]: loss 0.744020
[epoch7, step2577]: loss 0.742658
[epoch7, step2578]: loss 0.602575
[epoch7, step2579]: loss 0.766625
[epoch7, step2580]: loss 0.636649
[epoch7, step2581]: loss 0.750851
[epoch7, step2582]: loss 0.697220
[epoch7, step2583]: loss 0.550860
[epoch7, step2584]: loss 0.736496
[epoch7, step2585]: loss 0.396797
[epoch7, step2586]: loss 0.632683
[epoch7, step2587]: loss 0.741912
[epoch7, step2588]: loss 0.540481
[epoch7, step2589]: loss 0.731458
[epoch7, step2590]: loss 0.814536
[epoch7, step2591]: loss 0.893124
[epoch7, step2592]: loss 0.782433
[epoch7, step2593]: loss 0.639558
[epoch7, step2594]: loss 0.401153
[epoch7, step2595]: loss 0.636890
[epoch7, step2596]: loss 0.644610
[epoch7, step2597]: loss 0.733498
[epoch7, step2598]: loss 0.788500
[epoch7, step2599]: loss 0.543180
[epoch7, step2600]: loss 0.840386
[epoch7, step2601]: loss 0.539464
[epoch7, step2602]: loss 0.384152
[epoch7, step2603]: loss 0.641407
[epoch7, step2604]: loss 0.748902
[epoch7, step2605]: loss 0.834529
[epoch7, step2606]: loss 0.628458
[epoch7, step2607]: loss 0.808932
[epoch7, step2608]: loss 0.700358
[epoch7, step2609]: loss 0.618339
[epoch7, step2610]: loss 0.580022
[epoch7, step2611]: loss 0.717945
[epoch7, step2612]: loss 0.514522
[epoch7, step2613]: loss 0.493442
[epoch7, step2614]: loss 0.725475
[epoch7, step2615]: loss 0.499869
[epoch7, step2616]: loss 0.761498
[epoch7, step2617]: loss 0.603891
[epoch7, step2618]: loss 0.810716
[epoch7, step2619]: loss 0.809283
[epoch7, step2620]: loss 0.706924
[epoch7, step2621]: loss 0.771982
[epoch7, step2622]: loss 0.740563
[epoch7, step2623]: loss 0.513472
[epoch7, step2624]: loss 0.675812
[epoch7, step2625]: loss 0.555439
[epoch7, step2626]: loss 0.467002
[epoch7, step2627]: loss 0.742390
[epoch7, step2628]: loss 0.598351
[epoch7, step2629]: loss 0.652998
[epoch7, step2630]: loss 0.528573
[epoch7, step2631]: loss 0.706478
[epoch7, step2632]: loss 0.578744
[epoch7, step2633]: loss 0.297628
[epoch7, step2634]: loss 0.695892
[epoch7, step2635]: loss 0.803642
[epoch7, step2636]: loss 0.560915
[epoch7, step2637]: loss 0.744204
[epoch7, step2638]: loss 0.610293
[epoch7, step2639]: loss 0.686966
[epoch7, step2640]: loss 0.652938
[epoch7, step2641]: loss 0.621148
[epoch7, step2642]: loss 0.719224
[epoch7, step2643]: loss 0.591358
[epoch7, step2644]: loss 0.721957
[epoch7, step2645]: loss 0.738444
[epoch7, step2646]: loss 0.698262
[epoch7, step2647]: loss 0.784473
[epoch7, step2648]: loss 0.473977
[epoch7, step2649]: loss 0.537389
[epoch7, step2650]: loss 0.817875
[epoch7, step2651]: loss 0.527763
[epoch7, step2652]: loss 0.600312
[epoch7, step2653]: loss 0.491172
[epoch7, step2654]: loss 0.542925
[epoch7, step2655]: loss 0.579347
[epoch7, step2656]: loss 0.193901
[epoch7, step2657]: loss 0.777083
[epoch7, step2658]: loss 0.889269
[epoch7, step2659]: loss 0.632004
[epoch7, step2660]: loss 0.504921
[epoch7, step2661]: loss 0.562967
[epoch7, step2662]: loss 0.496118
[epoch7, step2663]: loss 0.813963
[epoch7, step2664]: loss 0.647840
[epoch7, step2665]: loss 0.855100
[epoch7, step2666]: loss 0.632264
[epoch7, step2667]: loss 0.696625
[epoch7, step2668]: loss 0.588183
[epoch7, step2669]: loss 0.816901
[epoch7, step2670]: loss 0.372266
[epoch7, step2671]: loss 0.272492
[epoch7, step2672]: loss 0.743404
[epoch7, step2673]: loss 0.590144
[epoch7, step2674]: loss 0.788290
[epoch7, step2675]: loss 0.799511
[epoch7, step2676]: loss 0.761004
[epoch7, step2677]: loss 0.682391
[epoch7, step2678]: loss 0.593912
[epoch7, step2679]: loss 0.683055
[epoch7, step2680]: loss 0.609109
[epoch7, step2681]: loss 0.753953
[epoch7, step2682]: loss 0.640585
[epoch7, step2683]: loss 0.609330
[epoch7, step2684]: loss 0.465369
[epoch7, step2685]: loss 0.569688
[epoch7, step2686]: loss 0.693729
[epoch7, step2687]: loss 0.713956
[epoch7, step2688]: loss 0.635905
[epoch7, step2689]: loss 0.679889
[epoch7, step2690]: loss 0.721820
[epoch7, step2691]: loss 0.856811
[epoch7, step2692]: loss 0.333752
[epoch7, step2693]: loss 0.602341
[epoch7, step2694]: loss 0.777424
[epoch7, step2695]: loss 0.780920
[epoch7, step2696]: loss 0.524764
[epoch7, step2697]: loss 0.753648
[epoch7, step2698]: loss 0.718360
[epoch7, step2699]: loss 0.559688
[epoch7, step2700]: loss 0.706718
[epoch7, step2701]: loss 0.451606
[epoch7, step2702]: loss 1.010018
[epoch7, step2703]: loss 0.761976
[epoch7, step2704]: loss 0.622169
[epoch7, step2705]: loss 0.289103
[epoch7, step2706]: loss 0.699498
[epoch7, step2707]: loss 0.545921
[epoch7, step2708]: loss 0.501646
[epoch7, step2709]: loss 0.332647
[epoch7, step2710]: loss 0.476770
[epoch7, step2711]: loss 0.689116
[epoch7, step2712]: loss 0.498541
[epoch7, step2713]: loss 0.825286
[epoch7, step2714]: loss 0.492208
[epoch7, step2715]: loss 0.807416
[epoch7, step2716]: loss 0.789025
[epoch7, step2717]: loss 0.859437
[epoch7, step2718]: loss 0.752492
[epoch7, step2719]: loss 0.421340
[epoch7, step2720]: loss 0.706513
[epoch7, step2721]: loss 0.444879
[epoch7, step2722]: loss 0.449533
[epoch7, step2723]: loss 0.778466
[epoch7, step2724]: loss 0.319421
[epoch7, step2725]: loss 0.732972
[epoch7, step2726]: loss 0.742387
[epoch7, step2727]: loss 0.836469
[epoch7, step2728]: loss 0.573745
[epoch7, step2729]: loss 0.852107
[epoch7, step2730]: loss 0.821262
[epoch7, step2731]: loss 0.314152
[epoch7, step2732]: loss 0.640609
[epoch7, step2733]: loss 0.489677
[epoch7, step2734]: loss 0.513127
[epoch7, step2735]: loss 0.531963
[epoch7, step2736]: loss 0.342349
[epoch7, step2737]: loss 0.636924
[epoch7, step2738]: loss 0.760834
[epoch7, step2739]: loss 0.464723
[epoch7, step2740]: loss 0.400533
[epoch7, step2741]: loss 0.418664
[epoch7, step2742]: loss 0.606986
[epoch7, step2743]: loss 0.533062
[epoch7, step2744]: loss 0.689623
[epoch7, step2745]: loss 0.771882
[epoch7, step2746]: loss 0.777877
[epoch7, step2747]: loss 0.489054
[epoch7, step2748]: loss 0.590067
[epoch7, step2749]: loss 0.583099
[epoch7, step2750]: loss 0.706386
[epoch7, step2751]: loss 0.631518
[epoch7, step2752]: loss 0.726411
[epoch7, step2753]: loss 0.505218
[epoch7, step2754]: loss 0.725857
[epoch7, step2755]: loss 0.832490
[epoch7, step2756]: loss 0.490257
[epoch7, step2757]: loss 0.466055
[epoch7, step2758]: loss 0.599877
[epoch7, step2759]: loss 0.863922
[epoch7, step2760]: loss 0.583222
[epoch7, step2761]: loss 0.793094
[epoch7, step2762]: loss 0.345825
[epoch7, step2763]: loss 0.349847
[epoch7, step2764]: loss 0.690685
[epoch7, step2765]: loss 0.753808
[epoch7, step2766]: loss 0.616200
[epoch7, step2767]: loss 0.505124
[epoch7, step2768]: loss 0.537623
[epoch7, step2769]: loss 0.682190
[epoch7, step2770]: loss 0.622679
[epoch7, step2771]: loss 0.841102
[epoch7, step2772]: loss 0.503548
[epoch7, step2773]: loss 0.706856
[epoch7, step2774]: loss 0.636661
[epoch7, step2775]: loss 0.666261
[epoch7, step2776]: loss 0.397649
[epoch7, step2777]: loss 0.483375
[epoch7, step2778]: loss 0.532379
[epoch7, step2779]: loss 0.543383
[epoch7, step2780]: loss 0.817532
[epoch7, step2781]: loss 0.601166
[epoch7, step2782]: loss 0.403551
[epoch7, step2783]: loss 0.636572
[epoch7, step2784]: loss 0.470513
[epoch7, step2785]: loss 0.562141
[epoch7, step2786]: loss 0.321369
[epoch7, step2787]: loss 0.635563
[epoch7, step2788]: loss 0.319971
[epoch7, step2789]: loss 0.588888
[epoch7, step2790]: loss 0.649736
[epoch7, step2791]: loss 0.712974
[epoch7, step2792]: loss 0.729578
[epoch7, step2793]: loss 0.682872
[epoch7, step2794]: loss 0.529102
[epoch7, step2795]: loss 0.370492
[epoch7, step2796]: loss 0.661909
[epoch7, step2797]: loss 0.889343
[epoch7, step2798]: loss 0.730017
[epoch7, step2799]: loss 0.350174
[epoch7, step2800]: loss 0.327723
[epoch7, step2801]: loss 0.604941
[epoch7, step2802]: loss 0.810043
[epoch7, step2803]: loss 0.795385
[epoch7, step2804]: loss 0.704140
[epoch7, step2805]: loss 0.538693
[epoch7, step2806]: loss 0.691962
[epoch7, step2807]: loss 0.489375
[epoch7, step2808]: loss 0.602959
[epoch7, step2809]: loss 0.991499
[epoch7, step2810]: loss 0.591920
[epoch7, step2811]: loss 0.674894
[epoch7, step2812]: loss 0.462194
[epoch7, step2813]: loss 0.544184
[epoch7, step2814]: loss 0.657204
[epoch7, step2815]: loss 0.508864
[epoch7, step2816]: loss 0.713762
[epoch7, step2817]: loss 0.425411
[epoch7, step2818]: loss 0.806141
[epoch7, step2819]: loss 0.748338
[epoch7, step2820]: loss 0.715662
[epoch7, step2821]: loss 0.609274
[epoch7, step2822]: loss 0.764194
[epoch7, step2823]: loss 0.923380
[epoch7, step2824]: loss 0.321092
[epoch7, step2825]: loss 0.548505
[epoch7, step2826]: loss 0.480872
[epoch7, step2827]: loss 0.679780
[epoch7, step2828]: loss 0.538949
[epoch7, step2829]: loss 0.687616
[epoch7, step2830]: loss 0.692810
[epoch7, step2831]: loss 0.808876
[epoch7, step2832]: loss 0.596245
[epoch7, step2833]: loss 0.920684
[epoch7, step2834]: loss 0.575540
[epoch7, step2835]: loss 0.691854
[epoch7, step2836]: loss 0.458304
[epoch7, step2837]: loss 0.438362
[epoch7, step2838]: loss 0.803677
[epoch7, step2839]: loss 0.862225
[epoch7, step2840]: loss 0.522744
[epoch7, step2841]: loss 0.545472
[epoch7, step2842]: loss 0.540717
[epoch7, step2843]: loss 0.826938
[epoch7, step2844]: loss 0.721785
[epoch7, step2845]: loss 0.726472
[epoch7, step2846]: loss 0.373229
[epoch7, step2847]: loss 0.708107
[epoch7, step2848]: loss 0.581040
[epoch7, step2849]: loss 0.502538
[epoch7, step2850]: loss 0.797075
[epoch7, step2851]: loss 0.716022
[epoch7, step2852]: loss 0.540011
[epoch7, step2853]: loss 0.775575
[epoch7, step2854]: loss 0.583973
[epoch7, step2855]: loss 0.686644
[epoch7, step2856]: loss 0.474871
[epoch7, step2857]: loss 0.491257
[epoch7, step2858]: loss 0.778801
[epoch7, step2859]: loss 0.639244
[epoch7, step2860]: loss 0.638337
[epoch7, step2861]: loss 0.720592
[epoch7, step2862]: loss 0.657477
[epoch7, step2863]: loss 0.555735
[epoch7, step2864]: loss 0.554757
[epoch7, step2865]: loss 0.494659
[epoch7, step2866]: loss 0.858353
[epoch7, step2867]: loss 0.553468
[epoch7, step2868]: loss 0.698389
[epoch7, step2869]: loss 0.632671
[epoch7, step2870]: loss 0.340369
[epoch7, step2871]: loss 0.732954
[epoch7, step2872]: loss 0.577375
[epoch7, step2873]: loss 0.753711
[epoch7, step2874]: loss 0.465025
[epoch7, step2875]: loss 0.460450
[epoch7, step2876]: loss 0.721727
[epoch7, step2877]: loss 0.468186
[epoch7, step2878]: loss 0.510315
[epoch7, step2879]: loss 0.563383
[epoch7, step2880]: loss 0.538172
[epoch7, step2881]: loss 0.363511
[epoch7, step2882]: loss 0.810715
[epoch7, step2883]: loss 0.365605
[epoch7, step2884]: loss 0.463326
[epoch7, step2885]: loss 0.762110
[epoch7, step2886]: loss 0.473019
[epoch7, step2887]: loss 0.641604
[epoch7, step2888]: loss 0.753163
[epoch7, step2889]: loss 0.609251
[epoch7, step2890]: loss 0.605722
[epoch7, step2891]: loss 0.637816
[epoch7, step2892]: loss 0.731805
[epoch7, step2893]: loss 0.668072
[epoch7, step2894]: loss 0.741634
[epoch7, step2895]: loss 0.404652
[epoch7, step2896]: loss 0.853070
[epoch7, step2897]: loss 0.533981
[epoch7, step2898]: loss 0.677784
[epoch7, step2899]: loss 0.612767
[epoch7, step2900]: loss 0.420966
[epoch7, step2901]: loss 0.636431
[epoch7, step2902]: loss 0.665136
[epoch7, step2903]: loss 0.690981
[epoch7, step2904]: loss 0.621739
[epoch7, step2905]: loss 0.418187
[epoch7, step2906]: loss 0.618878
[epoch7, step2907]: loss 0.706296
[epoch7, step2908]: loss 0.813255
[epoch7, step2909]: loss 0.807956
[epoch7, step2910]: loss 0.518798
[epoch7, step2911]: loss 0.749345
[epoch7, step2912]: loss 0.777892
[epoch7, step2913]: loss 0.709095
[epoch7, step2914]: loss 0.917067
[epoch7, step2915]: loss 0.539431
[epoch7, step2916]: loss 0.700277
[epoch7, step2917]: loss 0.726472
[epoch7, step2918]: loss 0.828114
[epoch7, step2919]: loss 0.627003
[epoch7, step2920]: loss 0.540455
[epoch7, step2921]: loss 0.681011
[epoch7, step2922]: loss 0.830696
[epoch7, step2923]: loss 0.657854
[epoch7, step2924]: loss 0.511989
[epoch7, step2925]: loss 0.636071
[epoch7, step2926]: loss 0.527554
[epoch7, step2927]: loss 0.639515
[epoch7, step2928]: loss 0.742574
[epoch7, step2929]: loss 0.618657
[epoch7, step2930]: loss 0.516182
[epoch7, step2931]: loss 0.423474
[epoch7, step2932]: loss 0.484486
[epoch7, step2933]: loss 0.589033
[epoch7, step2934]: loss 0.669169
[epoch7, step2935]: loss 0.800815
[epoch7, step2936]: loss 0.686093
[epoch7, step2937]: loss 0.624515
[epoch7, step2938]: loss 0.471875
[epoch7, step2939]: loss 0.588903
[epoch7, step2940]: loss 0.621466
[epoch7, step2941]: loss 0.644521
[epoch7, step2942]: loss 0.684465
[epoch7, step2943]: loss 0.432227
[epoch7, step2944]: loss 0.423862
[epoch7, step2945]: loss 0.718633
[epoch7, step2946]: loss 0.376051
[epoch7, step2947]: loss 0.769444
[epoch7, step2948]: loss 0.758666
[epoch7, step2949]: loss 0.514377
[epoch7, step2950]: loss 0.689016
[epoch7, step2951]: loss 0.524424
[epoch7, step2952]: loss 0.625129
[epoch7, step2953]: loss 0.280866
[epoch7, step2954]: loss 0.841141
[epoch7, step2955]: loss 0.741702
[epoch7, step2956]: loss 0.582378
[epoch7, step2957]: loss 0.676121
[epoch7, step2958]: loss 0.385991
[epoch7, step2959]: loss 0.635667
[epoch7, step2960]: loss 0.561079
[epoch7, step2961]: loss 0.641855
[epoch7, step2962]: loss 0.673508
[epoch7, step2963]: loss 0.523632
[epoch7, step2964]: loss 0.577342
[epoch7, step2965]: loss 0.616765
[epoch7, step2966]: loss 0.429724
[epoch7, step2967]: loss 0.615221
[epoch7, step2968]: loss 0.803272
[epoch7, step2969]: loss 0.506238
[epoch7, step2970]: loss 0.539573
[epoch7, step2971]: loss 0.879529
[epoch7, step2972]: loss 0.635424
[epoch7, step2973]: loss 0.838274
[epoch7, step2974]: loss 0.510571
[epoch7, step2975]: loss 0.607121
[epoch7, step2976]: loss 0.568327
[epoch7, step2977]: loss 0.712985
[epoch7, step2978]: loss 0.751257
[epoch7, step2979]: loss 0.425447
[epoch7, step2980]: loss 0.862620
[epoch7, step2981]: loss 0.718567
[epoch7, step2982]: loss 0.793008
[epoch7, step2983]: loss 0.506284
[epoch7, step2984]: loss 0.532110
[epoch7, step2985]: loss 0.412568
[epoch7, step2986]: loss 0.511291
[epoch7, step2987]: loss 0.491390
[epoch7, step2988]: loss 0.501547
[epoch7, step2989]: loss 0.581080
[epoch7, step2990]: loss 0.211157
[epoch7, step2991]: loss 0.849755
[epoch7, step2992]: loss 0.628643
[epoch7, step2993]: loss 0.607217
[epoch7, step2994]: loss 0.758177
[epoch7, step2995]: loss 0.352616
[epoch7, step2996]: loss 0.360322
[epoch7, step2997]: loss 0.791325
[epoch7, step2998]: loss 0.590716
[epoch7, step2999]: loss 0.799534
[epoch7, step3000]: loss 0.531009
[epoch7, step3001]: loss 0.719889
[epoch7, step3002]: loss 0.668982
[epoch7, step3003]: loss 0.434680
[epoch7, step3004]: loss 0.686795
[epoch7, step3005]: loss 0.617977
[epoch7, step3006]: loss 0.762989
[epoch7, step3007]: loss 0.904771
[epoch7, step3008]: loss 0.670833
[epoch7, step3009]: loss 0.634158
[epoch7, step3010]: loss 0.473160
[epoch7, step3011]: loss 0.854116
[epoch7, step3012]: loss 0.628011
[epoch7, step3013]: loss 0.723943
[epoch7, step3014]: loss 0.619491
[epoch7, step3015]: loss 0.738572
[epoch7, step3016]: loss 0.770168
[epoch7, step3017]: loss 0.734562
[epoch7, step3018]: loss 0.574412
[epoch7, step3019]: loss 0.380919
[epoch7, step3020]: loss 0.682888
[epoch7, step3021]: loss 0.613647
[epoch7, step3022]: loss 0.883654
[epoch7, step3023]: loss 0.517563
[epoch7, step3024]: loss 0.206019
[epoch7, step3025]: loss 0.540642
[epoch7, step3026]: loss 0.658263
[epoch7, step3027]: loss 0.448780
[epoch7, step3028]: loss 0.529362
[epoch7, step3029]: loss 0.480784
[epoch7, step3030]: loss 0.769323
[epoch7, step3031]: loss 0.706650
[epoch7, step3032]: loss 0.644153
[epoch7, step3033]: loss 0.370029
[epoch7, step3034]: loss 0.583487
[epoch7, step3035]: loss 0.680739
[epoch7, step3036]: loss 0.477843
[epoch7, step3037]: loss 0.741116
[epoch7, step3038]: loss 0.710435
[epoch7, step3039]: loss 0.746480
[epoch7, step3040]: loss 0.457652
[epoch7, step3041]: loss 0.788914
[epoch7, step3042]: loss 0.583316
[epoch7, step3043]: loss 0.521182
[epoch7, step3044]: loss 0.469194
[epoch7, step3045]: loss 0.638020
[epoch7, step3046]: loss 0.634704
[epoch7, step3047]: loss 0.588655
[epoch7, step3048]: loss 0.700259
[epoch7, step3049]: loss 0.790814
[epoch7, step3050]: loss 0.792819
[epoch7, step3051]: loss 0.427859
[epoch7, step3052]: loss 0.609130
[epoch7, step3053]: loss 0.608539
[epoch7, step3054]: loss 0.693754
[epoch7, step3055]: loss 0.502874
[epoch7, step3056]: loss 0.529193
[epoch7, step3057]: loss 0.428876
[epoch7, step3058]: loss 0.851211
[epoch7, step3059]: loss 0.831649
[epoch7, step3060]: loss 0.624873
[epoch7, step3061]: loss 0.392263
[epoch7, step3062]: loss 0.730807
[epoch7, step3063]: loss 0.594468
[epoch7, step3064]: loss 0.378533
[epoch7, step3065]: loss 0.600351
[epoch7, step3066]: loss 0.405821
[epoch7, step3067]: loss 0.443439
[epoch7, step3068]: loss 0.478729
[epoch7, step3069]: loss 0.856826
[epoch7, step3070]: loss 0.798394
[epoch7, step3071]: loss 0.540915
[epoch7, step3072]: loss 0.644243
[epoch7, step3073]: loss 0.867020
[epoch7, step3074]: loss 0.776101
[epoch7, step3075]: loss 0.573008
[epoch7, step3076]: loss 0.780792

[epoch7]: avg loss 0.780792

[epoch8, step1]: loss 0.594002
[epoch8, step2]: loss 0.701751
[epoch8, step3]: loss 0.745317
[epoch8, step4]: loss 0.788468
[epoch8, step5]: loss 0.793590
[epoch8, step6]: loss 0.533848
[epoch8, step7]: loss 0.743184
[epoch8, step8]: loss 0.716855
[epoch8, step9]: loss 0.576904
[epoch8, step10]: loss 0.836993
[epoch8, step11]: loss 0.643998
[epoch8, step12]: loss 0.658922
[epoch8, step13]: loss 0.544361
[epoch8, step14]: loss 0.771240
[epoch8, step15]: loss 0.617820
[epoch8, step16]: loss 0.454673
[epoch8, step17]: loss 0.511805
[epoch8, step18]: loss 0.499359
[epoch8, step19]: loss 0.683755
[epoch8, step20]: loss 0.680220
[epoch8, step21]: loss 0.704442
[epoch8, step22]: loss 0.741839
[epoch8, step23]: loss 0.620559
[epoch8, step24]: loss 0.593470
[epoch8, step25]: loss 0.897406
[epoch8, step26]: loss 0.760292
[epoch8, step27]: loss 0.645483
[epoch8, step28]: loss 0.573529
[epoch8, step29]: loss 0.761265
[epoch8, step30]: loss 0.591360
[epoch8, step31]: loss 0.697390
[epoch8, step32]: loss 0.540573
[epoch8, step33]: loss 0.521964
[epoch8, step34]: loss 0.434031
[epoch8, step35]: loss 0.844038
[epoch8, step36]: loss 0.743386
[epoch8, step37]: loss 0.995226
[epoch8, step38]: loss 0.719620
[epoch8, step39]: loss 0.749932
[epoch8, step40]: loss 0.724329
[epoch8, step41]: loss 0.510826
[epoch8, step42]: loss 0.523839
[epoch8, step43]: loss 0.768195
[epoch8, step44]: loss 0.576943
[epoch8, step45]: loss 0.561780
[epoch8, step46]: loss 0.488329
[epoch8, step47]: loss 0.564621
[epoch8, step48]: loss 0.763126
[epoch8, step49]: loss 0.689778
[epoch8, step50]: loss 0.653499
[epoch8, step51]: loss 0.455407
[epoch8, step52]: loss 0.427874
[epoch8, step53]: loss 0.399812
[epoch8, step54]: loss 0.646631
[epoch8, step55]: loss 0.714249
[epoch8, step56]: loss 0.553409
[epoch8, step57]: loss 0.597187
[epoch8, step58]: loss 0.680631
[epoch8, step59]: loss 0.689293
[epoch8, step60]: loss 0.508555
[epoch8, step61]: loss 0.937298
[epoch8, step62]: loss 0.395365
[epoch8, step63]: loss 0.706812
[epoch8, step64]: loss 0.689788
[epoch8, step65]: loss 0.599872
[epoch8, step66]: loss 0.741614
[epoch8, step67]: loss 0.664903
[epoch8, step68]: loss 0.634686
[epoch8, step69]: loss 0.477502
[epoch8, step70]: loss 0.622469
[epoch8, step71]: loss 0.831952
[epoch8, step72]: loss 0.707162
[epoch8, step73]: loss 0.895608
[epoch8, step74]: loss 0.618770
[epoch8, step75]: loss 0.449932
[epoch8, step76]: loss 0.453063
[epoch8, step77]: loss 0.356796
[epoch8, step78]: loss 0.587914
[epoch8, step79]: loss 0.504089
[epoch8, step80]: loss 0.401613
[epoch8, step81]: loss 0.671643
[epoch8, step82]: loss 0.742801
[epoch8, step83]: loss 0.894249
[epoch8, step84]: loss 0.779606
[epoch8, step85]: loss 0.337790
[epoch8, step86]: loss 0.604498
[epoch8, step87]: loss 0.692265
[epoch8, step88]: loss 0.290403
[epoch8, step89]: loss 0.598516
[epoch8, step90]: loss 0.665885
[epoch8, step91]: loss 0.612738
[epoch8, step92]: loss 0.874595
[epoch8, step93]: loss 0.530962
[epoch8, step94]: loss 0.309530
[epoch8, step95]: loss 0.479961
[epoch8, step96]: loss 0.597154
[epoch8, step97]: loss 0.626981
[epoch8, step98]: loss 0.791700
[epoch8, step99]: loss 0.772053
[epoch8, step100]: loss 0.545577
[epoch8, step101]: loss 0.785569
[epoch8, step102]: loss 0.624356
[epoch8, step103]: loss 0.731831
[epoch8, step104]: loss 0.642335
[epoch8, step105]: loss 0.876229
[epoch8, step106]: loss 0.658347
[epoch8, step107]: loss 0.735166
[epoch8, step108]: loss 0.629536
[epoch8, step109]: loss 0.780524
[epoch8, step110]: loss 0.716253
[epoch8, step111]: loss 0.606575
[epoch8, step112]: loss 0.821433
[epoch8, step113]: loss 0.670309
[epoch8, step114]: loss 0.744265
[epoch8, step115]: loss 0.725595
[epoch8, step116]: loss 0.507373
[epoch8, step117]: loss 0.505541
[epoch8, step118]: loss 0.684708
[epoch8, step119]: loss 0.470149
[epoch8, step120]: loss 0.632915
[epoch8, step121]: loss 0.581508
[epoch8, step122]: loss 0.648784
[epoch8, step123]: loss 0.839870
[epoch8, step124]: loss 0.657928
[epoch8, step125]: loss 0.687634
[epoch8, step126]: loss 0.708174
[epoch8, step127]: loss 0.709606
[epoch8, step128]: loss 0.503575
[epoch8, step129]: loss 0.765937
[epoch8, step130]: loss 0.449811
[epoch8, step131]: loss 0.842814
[epoch8, step132]: loss 0.639270
[epoch8, step133]: loss 0.759325
[epoch8, step134]: loss 0.572404
[epoch8, step135]: loss 0.462091
[epoch8, step136]: loss 0.500047
[epoch8, step137]: loss 0.577955
[epoch8, step138]: loss 0.502534
[epoch8, step139]: loss 0.803306
[epoch8, step140]: loss 0.904880
[epoch8, step141]: loss 0.739636
[epoch8, step142]: loss 0.648605
[epoch8, step143]: loss 0.819841
[epoch8, step144]: loss 0.768509
[epoch8, step145]: loss 0.457681
[epoch8, step146]: loss 0.568725
[epoch8, step147]: loss 0.490312
[epoch8, step148]: loss 0.711792
[epoch8, step149]: loss 0.474113
[epoch8, step150]: loss 0.389323
[epoch8, step151]: loss 0.642560
[epoch8, step152]: loss 0.808347
[epoch8, step153]: loss 0.743790
[epoch8, step154]: loss 0.676686
[epoch8, step155]: loss 0.589817
[epoch8, step156]: loss 0.378437
[epoch8, step157]: loss 0.795518
[epoch8, step158]: loss 0.546138
[epoch8, step159]: loss 0.459621
[epoch8, step160]: loss 0.444274
[epoch8, step161]: loss 0.766753
[epoch8, step162]: loss 0.677147
[epoch8, step163]: loss 0.587780
[epoch8, step164]: loss 0.783278
[epoch8, step165]: loss 0.793881
[epoch8, step166]: loss 0.597371
[epoch8, step167]: loss 0.758525
[epoch8, step168]: loss 0.732686
[epoch8, step169]: loss 0.634695
[epoch8, step170]: loss 0.312361
[epoch8, step171]: loss 0.700643
[epoch8, step172]: loss 0.560525
[epoch8, step173]: loss 0.605653
[epoch8, step174]: loss 0.909592
[epoch8, step175]: loss 0.659058
[epoch8, step176]: loss 0.594238
[epoch8, step177]: loss 0.703522
[epoch8, step178]: loss 0.799389
[epoch8, step179]: loss 0.679257
[epoch8, step180]: loss 0.395185
[epoch8, step181]: loss 0.465315
[epoch8, step182]: loss 0.592277
[epoch8, step183]: loss 0.683453
[epoch8, step184]: loss 0.531681
[epoch8, step185]: loss 0.789478
[epoch8, step186]: loss 0.667799
[epoch8, step187]: loss 0.509687
[epoch8, step188]: loss 0.706456
[epoch8, step189]: loss 0.622004
[epoch8, step190]: loss 0.461558
[epoch8, step191]: loss 0.507717
[epoch8, step192]: loss 0.494632
[epoch8, step193]: loss 0.719528
[epoch8, step194]: loss 0.531676
[epoch8, step195]: loss 0.724438
[epoch8, step196]: loss 0.557790
[epoch8, step197]: loss 0.881696
[epoch8, step198]: loss 0.508189
[epoch8, step199]: loss 0.665215
[epoch8, step200]: loss 0.665566
[epoch8, step201]: loss 0.963329
[epoch8, step202]: loss 0.597765
[epoch8, step203]: loss 0.664088
[epoch8, step204]: loss 0.569648
[epoch8, step205]: loss 0.505533
[epoch8, step206]: loss 0.474922
[epoch8, step207]: loss 0.579591
[epoch8, step208]: loss 0.639434
[epoch8, step209]: loss 0.659946
[epoch8, step210]: loss 0.562161
[epoch8, step211]: loss 0.537238
[epoch8, step212]: loss 0.822853
[epoch8, step213]: loss 0.645253
[epoch8, step214]: loss 0.250618
[epoch8, step215]: loss 0.656243
[epoch8, step216]: loss 0.469888
[epoch8, step217]: loss 0.448329
[epoch8, step218]: loss 0.221595
[epoch8, step219]: loss 0.704832
[epoch8, step220]: loss 0.551997
[epoch8, step221]: loss 0.385697
[epoch8, step222]: loss 0.981725
[epoch8, step223]: loss 0.579133
[epoch8, step224]: loss 0.528063
[epoch8, step225]: loss 0.878939
[epoch8, step226]: loss 0.733697
[epoch8, step227]: loss 0.584391
[epoch8, step228]: loss 0.565973
[epoch8, step229]: loss 0.571892
[epoch8, step230]: loss 0.627536
[epoch8, step231]: loss 0.972879
[epoch8, step232]: loss 0.458488
[epoch8, step233]: loss 0.420118
[epoch8, step234]: loss 0.530279
[epoch8, step235]: loss 0.389859
[epoch8, step236]: loss 0.520203
[epoch8, step237]: loss 0.615251
[epoch8, step238]: loss 0.650236
[epoch8, step239]: loss 0.708628
[epoch8, step240]: loss 0.530464
[epoch8, step241]: loss 0.691539
[epoch8, step242]: loss 0.804379
[epoch8, step243]: loss 0.665823
[epoch8, step244]: loss 0.779048
[epoch8, step245]: loss 0.591402
[epoch8, step246]: loss 0.528558
[epoch8, step247]: loss 0.702206
[epoch8, step248]: loss 0.505007
[epoch8, step249]: loss 0.556675
[epoch8, step250]: loss 0.591706
[epoch8, step251]: loss 0.452451
[epoch8, step252]: loss 0.512365
[epoch8, step253]: loss 0.858339
[epoch8, step254]: loss 0.686062
[epoch8, step255]: loss 0.799579
[epoch8, step256]: loss 0.674142
[epoch8, step257]: loss 0.835753
[epoch8, step258]: loss 0.684025
[epoch8, step259]: loss 0.647755
[epoch8, step260]: loss 0.696352
[epoch8, step261]: loss 0.683849
[epoch8, step262]: loss 0.681032
[epoch8, step263]: loss 0.528245
[epoch8, step264]: loss 0.780755
[epoch8, step265]: loss 0.233172
[epoch8, step266]: loss 0.769964
[epoch8, step267]: loss 0.650014
[epoch8, step268]: loss 0.802774
[epoch8, step269]: loss 0.693573
[epoch8, step270]: loss 0.642416
[epoch8, step271]: loss 0.743499
[epoch8, step272]: loss 0.694103
[epoch8, step273]: loss 0.586203
[epoch8, step274]: loss 0.459213
[epoch8, step275]: loss 0.455288
[epoch8, step276]: loss 0.588996
[epoch8, step277]: loss 0.562326
[epoch8, step278]: loss 0.576418
[epoch8, step279]: loss 0.547314
[epoch8, step280]: loss 0.632216
[epoch8, step281]: loss 0.628985
[epoch8, step282]: loss 0.583956
[epoch8, step283]: loss 0.430584
[epoch8, step284]: loss 0.616571
[epoch8, step285]: loss 0.777367
[epoch8, step286]: loss 0.710174
[epoch8, step287]: loss 0.632963
[epoch8, step288]: loss 0.664003
[epoch8, step289]: loss 0.716533
[epoch8, step290]: loss 0.598119
[epoch8, step291]: loss 0.348336
[epoch8, step292]: loss 0.854340
[epoch8, step293]: loss 0.526477
[epoch8, step294]: loss 0.815053
[epoch8, step295]: loss 0.539533
[epoch8, step296]: loss 0.686236
[epoch8, step297]: loss 0.683733
[epoch8, step298]: loss 0.799930
[epoch8, step299]: loss 0.412339
[epoch8, step300]: loss 0.847094
[epoch8, step301]: loss 0.518672
[epoch8, step302]: loss 0.608112
[epoch8, step303]: loss 0.470676
[epoch8, step304]: loss 0.596397
[epoch8, step305]: loss 0.630823
[epoch8, step306]: loss 0.516596
[epoch8, step307]: loss 0.710711
[epoch8, step308]: loss 0.649229
[epoch8, step309]: loss 0.710724
[epoch8, step310]: loss 0.298654
[epoch8, step311]: loss 0.786623
[epoch8, step312]: loss 0.840710
[epoch8, step313]: loss 0.518001
[epoch8, step314]: loss 0.620155
[epoch8, step315]: loss 0.699349
[epoch8, step316]: loss 0.698027
[epoch8, step317]: loss 0.805671
[epoch8, step318]: loss 0.432970
[epoch8, step319]: loss 0.674643
[epoch8, step320]: loss 0.558630
[epoch8, step321]: loss 0.776375
[epoch8, step322]: loss 0.600368
[epoch8, step323]: loss 0.757128
[epoch8, step324]: loss 0.696765
[epoch8, step325]: loss 0.684220
[epoch8, step326]: loss 0.708656
[epoch8, step327]: loss 0.272923
[epoch8, step328]: loss 0.755429
[epoch8, step329]: loss 0.429193
[epoch8, step330]: loss 0.884165
[epoch8, step331]: loss 0.612126
[epoch8, step332]: loss 0.536618
[epoch8, step333]: loss 0.829962
[epoch8, step334]: loss 0.622367
[epoch8, step335]: loss 0.748086
[epoch8, step336]: loss 0.774863
[epoch8, step337]: loss 0.690712
[epoch8, step338]: loss 0.621389
[epoch8, step339]: loss 0.314142
[epoch8, step340]: loss 0.891421
[epoch8, step341]: loss 0.594969
[epoch8, step342]: loss 0.855945
[epoch8, step343]: loss 0.733927
[epoch8, step344]: loss 0.874364
[epoch8, step345]: loss 0.505711
[epoch8, step346]: loss 0.701414
[epoch8, step347]: loss 0.741581
[epoch8, step348]: loss 0.292103
[epoch8, step349]: loss 0.855057
[epoch8, step350]: loss 0.220920
[epoch8, step351]: loss 0.421849
[epoch8, step352]: loss 0.519157
[epoch8, step353]: loss 0.534207
[epoch8, step354]: loss 0.633890
[epoch8, step355]: loss 0.603000
[epoch8, step356]: loss 0.702091
[epoch8, step357]: loss 0.763144
[epoch8, step358]: loss 0.789074
[epoch8, step359]: loss 0.351336
[epoch8, step360]: loss 0.641269
[epoch8, step361]: loss 0.356578
[epoch8, step362]: loss 0.623156
[epoch8, step363]: loss 0.495944
[epoch8, step364]: loss 0.668703
[epoch8, step365]: loss 0.752948
[epoch8, step366]: loss 0.318181
[epoch8, step367]: loss 0.601414
[epoch8, step368]: loss 0.776683
[epoch8, step369]: loss 0.392951
[epoch8, step370]: loss 0.573099
[epoch8, step371]: loss 0.682153
[epoch8, step372]: loss 0.440132
[epoch8, step373]: loss 0.438586
[epoch8, step374]: loss 0.441890
[epoch8, step375]: loss 0.739464
[epoch8, step376]: loss 0.721514
[epoch8, step377]: loss 0.301371
[epoch8, step378]: loss 0.644200
[epoch8, step379]: loss 0.668674
[epoch8, step380]: loss 0.698682
[epoch8, step381]: loss 0.550300
[epoch8, step382]: loss 0.709335
[epoch8, step383]: loss 0.817583
[epoch8, step384]: loss 0.684404
[epoch8, step385]: loss 0.865489
[epoch8, step386]: loss 0.689921
[epoch8, step387]: loss 0.584814
[epoch8, step388]: loss 0.660511
[epoch8, step389]: loss 0.757884
[epoch8, step390]: loss 0.507908
[epoch8, step391]: loss 0.455150
[epoch8, step392]: loss 0.308382
[epoch8, step393]: loss 0.796731
[epoch8, step394]: loss 0.510197
[epoch8, step395]: loss 0.427947
[epoch8, step396]: loss 0.443324
[epoch8, step397]: loss 0.478657
[epoch8, step398]: loss 0.679363
[epoch8, step399]: loss 0.687066
[epoch8, step400]: loss 0.394641
[epoch8, step401]: loss 0.585371
[epoch8, step402]: loss 0.793239
[epoch8, step403]: loss 0.533437
[epoch8, step404]: loss 0.458780
[epoch8, step405]: loss 0.419185
[epoch8, step406]: loss 0.358875
[epoch8, step407]: loss 0.469932
[epoch8, step408]: loss 0.482620
[epoch8, step409]: loss 0.439413
[epoch8, step410]: loss 0.702894
[epoch8, step411]: loss 0.593586
[epoch8, step412]: loss 0.660478
[epoch8, step413]: loss 0.717024
[epoch8, step414]: loss 0.600060
[epoch8, step415]: loss 0.674499
[epoch8, step416]: loss 0.688704
[epoch8, step417]: loss 0.270447
[epoch8, step418]: loss 0.812571
[epoch8, step419]: loss 0.552204
[epoch8, step420]: loss 0.634720
[epoch8, step421]: loss 0.751509
[epoch8, step422]: loss 0.333823
[epoch8, step423]: loss 0.663982
[epoch8, step424]: loss 0.675093
[epoch8, step425]: loss 0.624978
[epoch8, step426]: loss 0.870678
[epoch8, step427]: loss 0.569355
[epoch8, step428]: loss 0.767348
[epoch8, step429]: loss 0.851868
[epoch8, step430]: loss 0.597254
[epoch8, step431]: loss 0.520766
[epoch8, step432]: loss 0.662284
[epoch8, step433]: loss 0.659991
[epoch8, step434]: loss 0.541132
[epoch8, step435]: loss 0.523201
[epoch8, step436]: loss 0.536108
[epoch8, step437]: loss 0.737851
[epoch8, step438]: loss 0.790080
[epoch8, step439]: loss 0.600853
[epoch8, step440]: loss 0.639582
[epoch8, step441]: loss 0.619386
[epoch8, step442]: loss 0.618820
[epoch8, step443]: loss 0.525552
[epoch8, step444]: loss 0.910455
[epoch8, step445]: loss 0.885884
[epoch8, step446]: loss 0.747021
[epoch8, step447]: loss 0.740825
[epoch8, step448]: loss 0.746588
[epoch8, step449]: loss 0.846246
[epoch8, step450]: loss 0.601122
[epoch8, step451]: loss 0.667351
[epoch8, step452]: loss 0.434016
[epoch8, step453]: loss 0.721060
[epoch8, step454]: loss 0.814016
[epoch8, step455]: loss 0.822121
[epoch8, step456]: loss 0.634294
[epoch8, step457]: loss 0.599035
[epoch8, step458]: loss 0.464451
[epoch8, step459]: loss 0.915473
[epoch8, step460]: loss 0.492544
[epoch8, step461]: loss 0.508990
[epoch8, step462]: loss 0.715818
[epoch8, step463]: loss 0.677261
[epoch8, step464]: loss 0.657208
[epoch8, step465]: loss 0.644375
[epoch8, step466]: loss 0.797089
[epoch8, step467]: loss 0.484640
[epoch8, step468]: loss 0.506770
[epoch8, step469]: loss 0.584988
[epoch8, step470]: loss 0.560413
[epoch8, step471]: loss 0.487806
[epoch8, step472]: loss 0.482729
[epoch8, step473]: loss 0.571005
[epoch8, step474]: loss 0.641763
[epoch8, step475]: loss 0.668016
[epoch8, step476]: loss 0.576077
[epoch8, step477]: loss 0.507996
[epoch8, step478]: loss 0.487771
[epoch8, step479]: loss 0.422439
[epoch8, step480]: loss 0.517421
[epoch8, step481]: loss 0.685511
[epoch8, step482]: loss 0.759268
[epoch8, step483]: loss 0.498908
[epoch8, step484]: loss 0.681780
[epoch8, step485]: loss 0.565147
[epoch8, step486]: loss 0.561675
[epoch8, step487]: loss 0.761503
[epoch8, step488]: loss 0.798287
[epoch8, step489]: loss 0.421488
[epoch8, step490]: loss 0.446034
[epoch8, step491]: loss 0.453045
[epoch8, step492]: loss 0.521001
[epoch8, step493]: loss 0.696434
[epoch8, step494]: loss 0.678508
[epoch8, step495]: loss 0.619839
[epoch8, step496]: loss 0.673587
[epoch8, step497]: loss 0.554451
[epoch8, step498]: loss 0.299521
[epoch8, step499]: loss 0.485438
[epoch8, step500]: loss 0.900928
[epoch8, step501]: loss 0.675488
[epoch8, step502]: loss 0.658630
[epoch8, step503]: loss 0.771599
[epoch8, step504]: loss 0.611332
[epoch8, step505]: loss 0.546103
[epoch8, step506]: loss 0.400214
[epoch8, step507]: loss 0.220296
[epoch8, step508]: loss 0.529991
[epoch8, step509]: loss 0.467194
[epoch8, step510]: loss 0.371965
[epoch8, step511]: loss 0.638514
[epoch8, step512]: loss 0.859615
[epoch8, step513]: loss 0.464589
[epoch8, step514]: loss 0.815719
[epoch8, step515]: loss 0.566484
[epoch8, step516]: loss 0.685113
[epoch8, step517]: loss 0.529558
[epoch8, step518]: loss 0.651886
[epoch8, step519]: loss 0.368819
[epoch8, step520]: loss 0.670914
[epoch8, step521]: loss 0.517857
[epoch8, step522]: loss 0.626472
[epoch8, step523]: loss 0.500282
[epoch8, step524]: loss 0.831381
[epoch8, step525]: loss 0.563447
[epoch8, step526]: loss 0.636884
[epoch8, step527]: loss 0.546296
[epoch8, step528]: loss 0.482636
[epoch8, step529]: loss 0.785325
[epoch8, step530]: loss 0.598959
[epoch8, step531]: loss 0.722926
[epoch8, step532]: loss 0.775215
[epoch8, step533]: loss 0.668509
[epoch8, step534]: loss 0.930292
[epoch8, step535]: loss 0.711750
[epoch8, step536]: loss 0.740468
[epoch8, step537]: loss 0.536639
[epoch8, step538]: loss 0.708584
[epoch8, step539]: loss 0.448405
[epoch8, step540]: loss 0.894366
[epoch8, step541]: loss 0.706753
[epoch8, step542]: loss 0.189282
[epoch8, step543]: loss 0.531380
[epoch8, step544]: loss 0.327006
[epoch8, step545]: loss 0.524489
[epoch8, step546]: loss 0.741440
[epoch8, step547]: loss 0.504361
[epoch8, step548]: loss 0.444569
[epoch8, step549]: loss 0.306344
[epoch8, step550]: loss 0.631555
[epoch8, step551]: loss 0.865884
[epoch8, step552]: loss 0.808095
[epoch8, step553]: loss 0.502296
[epoch8, step554]: loss 0.615209
[epoch8, step555]: loss 0.715729
[epoch8, step556]: loss 0.519594
[epoch8, step557]: loss 0.611663
[epoch8, step558]: loss 0.778641
[epoch8, step559]: loss 0.560628
[epoch8, step560]: loss 0.643777
[epoch8, step561]: loss 0.648561
[epoch8, step562]: loss 0.526441
[epoch8, step563]: loss 0.573205
[epoch8, step564]: loss 0.499354
[epoch8, step565]: loss 0.559811
[epoch8, step566]: loss 0.729925
[epoch8, step567]: loss 0.876974
[epoch8, step568]: loss 0.656699
[epoch8, step569]: loss 0.418463
[epoch8, step570]: loss 0.422903
[epoch8, step571]: loss 0.484610
[epoch8, step572]: loss 0.697067
[epoch8, step573]: loss 0.423272
[epoch8, step574]: loss 0.512313
[epoch8, step575]: loss 0.349161
[epoch8, step576]: loss 0.566848
[epoch8, step577]: loss 0.382602
[epoch8, step578]: loss 0.298118
[epoch8, step579]: loss 0.454089
[epoch8, step580]: loss 0.691245
[epoch8, step581]: loss 0.530774
[epoch8, step582]: loss 0.680744
[epoch8, step583]: loss 0.684196
[epoch8, step584]: loss 0.775801
[epoch8, step585]: loss 0.501409
[epoch8, step586]: loss 0.587969
[epoch8, step587]: loss 0.620330
[epoch8, step588]: loss 0.673684
[epoch8, step589]: loss 0.555317
[epoch8, step590]: loss 0.449926
[epoch8, step591]: loss 0.477252
[epoch8, step592]: loss 0.696452
[epoch8, step593]: loss 0.212022
[epoch8, step594]: loss 0.706578
[epoch8, step595]: loss 0.406187
[epoch8, step596]: loss 0.806125
[epoch8, step597]: loss 0.792045
[epoch8, step598]: loss 0.627952
[epoch8, step599]: loss 0.763053
[epoch8, step600]: loss 0.450802
[epoch8, step601]: loss 0.613761
[epoch8, step602]: loss 0.817127
[epoch8, step603]: loss 0.454293
[epoch8, step604]: loss 0.752547
[epoch8, step605]: loss 0.770711
[epoch8, step606]: loss 0.604144
[epoch8, step607]: loss 0.889113
[epoch8, step608]: loss 0.310778
[epoch8, step609]: loss 0.604524
[epoch8, step610]: loss 0.798949
[epoch8, step611]: loss 0.598188
[epoch8, step612]: loss 0.742118
[epoch8, step613]: loss 0.761696
[epoch8, step614]: loss 0.647421
[epoch8, step615]: loss 0.519279
[epoch8, step616]: loss 0.895734
[epoch8, step617]: loss 0.785511
[epoch8, step618]: loss 0.739215
[epoch8, step619]: loss 0.763891
[epoch8, step620]: loss 0.565224
[epoch8, step621]: loss 0.828932
[epoch8, step622]: loss 0.428134
[epoch8, step623]: loss 0.651408
[epoch8, step624]: loss 0.230083
[epoch8, step625]: loss 0.667308
[epoch8, step626]: loss 0.670184
[epoch8, step627]: loss 0.763351
[epoch8, step628]: loss 0.724777
[epoch8, step629]: loss 0.729314
[epoch8, step630]: loss 0.411035
[epoch8, step631]: loss 0.537536
[epoch8, step632]: loss 0.799268
[epoch8, step633]: loss 0.669130
[epoch8, step634]: loss 0.652237
[epoch8, step635]: loss 0.736214
[epoch8, step636]: loss 0.655267
[epoch8, step637]: loss 0.677988
[epoch8, step638]: loss 0.829464
[epoch8, step639]: loss 0.679018
[epoch8, step640]: loss 0.721937
[epoch8, step641]: loss 0.406990
[epoch8, step642]: loss 0.555565
[epoch8, step643]: loss 0.657915
[epoch8, step644]: loss 0.573932
[epoch8, step645]: loss 0.358465
[epoch8, step646]: loss 0.761972
[epoch8, step647]: loss 0.640564
[epoch8, step648]: loss 0.552051
[epoch8, step649]: loss 0.719851
[epoch8, step650]: loss 0.257924
[epoch8, step651]: loss 0.537974
[epoch8, step652]: loss 0.627160
[epoch8, step653]: loss 0.758750
[epoch8, step654]: loss 0.619244
[epoch8, step655]: loss 0.293010
[epoch8, step656]: loss 0.573811
[epoch8, step657]: loss 0.840569
[epoch8, step658]: loss 0.670274
[epoch8, step659]: loss 0.418878
[epoch8, step660]: loss 0.777580
[epoch8, step661]: loss 0.718820
[epoch8, step662]: loss 0.747161
[epoch8, step663]: loss 0.623684
[epoch8, step664]: loss 0.352985
[epoch8, step665]: loss 0.732595
[epoch8, step666]: loss 0.807606
[epoch8, step667]: loss 0.551431
[epoch8, step668]: loss 0.448492
[epoch8, step669]: loss 0.539002
[epoch8, step670]: loss 0.562724
[epoch8, step671]: loss 0.758445
[epoch8, step672]: loss 0.530778
[epoch8, step673]: loss 0.593114
[epoch8, step674]: loss 0.675858
[epoch8, step675]: loss 0.595053
[epoch8, step676]: loss 0.592275
[epoch8, step677]: loss 0.502762
[epoch8, step678]: loss 0.916235
[epoch8, step679]: loss 0.748772
[epoch8, step680]: loss 0.234635
[epoch8, step681]: loss 0.571690
[epoch8, step682]: loss 0.811698
[epoch8, step683]: loss 0.688462
[epoch8, step684]: loss 0.612058
[epoch8, step685]: loss 0.600715
[epoch8, step686]: loss 0.755179
[epoch8, step687]: loss 0.696245
[epoch8, step688]: loss 0.627531
[epoch8, step689]: loss 0.630527
[epoch8, step690]: loss 0.763050
[epoch8, step691]: loss 0.667568
[epoch8, step692]: loss 0.511946
[epoch8, step693]: loss 0.607455
[epoch8, step694]: loss 0.701665
[epoch8, step695]: loss 0.701179
[epoch8, step696]: loss 0.638353
[epoch8, step697]: loss 0.391654
[epoch8, step698]: loss 0.330166
[epoch8, step699]: loss 0.637536
[epoch8, step700]: loss 0.697263
[epoch8, step701]: loss 0.595527
[epoch8, step702]: loss 0.447580
[epoch8, step703]: loss 0.789512
[epoch8, step704]: loss 0.576796
[epoch8, step705]: loss 0.331680
[epoch8, step706]: loss 0.574991
[epoch8, step707]: loss 0.665280
[epoch8, step708]: loss 0.596808
[epoch8, step709]: loss 0.730244
[epoch8, step710]: loss 0.719502
[epoch8, step711]: loss 0.558879
[epoch8, step712]: loss 0.692609
[epoch8, step713]: loss 0.690716
[epoch8, step714]: loss 0.555099
[epoch8, step715]: loss 0.606438
[epoch8, step716]: loss 0.650841
[epoch8, step717]: loss 0.793906
[epoch8, step718]: loss 0.545605
[epoch8, step719]: loss 0.632018
[epoch8, step720]: loss 0.472232
[epoch8, step721]: loss 0.718539
[epoch8, step722]: loss 0.825265
[epoch8, step723]: loss 0.481148
[epoch8, step724]: loss 0.703722
[epoch8, step725]: loss 0.752982
[epoch8, step726]: loss 0.830529
[epoch8, step727]: loss 0.256613
[epoch8, step728]: loss 0.647626
[epoch8, step729]: loss 0.629046
[epoch8, step730]: loss 0.602540
[epoch8, step731]: loss 0.238253
[epoch8, step732]: loss 0.595528
[epoch8, step733]: loss 0.651464
[epoch8, step734]: loss 0.639196
[epoch8, step735]: loss 0.638579
[epoch8, step736]: loss 0.363419
[epoch8, step737]: loss 0.606688
[epoch8, step738]: loss 0.781669
[epoch8, step739]: loss 0.655352
[epoch8, step740]: loss 0.584528
[epoch8, step741]: loss 0.725838
[epoch8, step742]: loss 0.672104
[epoch8, step743]: loss 0.379854
[epoch8, step744]: loss 0.739087
[epoch8, step745]: loss 0.548417
[epoch8, step746]: loss 0.421855
[epoch8, step747]: loss 0.682315
[epoch8, step748]: loss 0.732443
[epoch8, step749]: loss 0.724223
[epoch8, step750]: loss 0.569013
[epoch8, step751]: loss 0.426984
[epoch8, step752]: loss 0.435746
[epoch8, step753]: loss 0.777676
[epoch8, step754]: loss 0.463778
[epoch8, step755]: loss 0.790989
[epoch8, step756]: loss 0.588743
[epoch8, step757]: loss 0.452353
[epoch8, step758]: loss 0.729538
[epoch8, step759]: loss 0.687798
[epoch8, step760]: loss 0.517107
[epoch8, step761]: loss 0.544246
[epoch8, step762]: loss 0.760747
[epoch8, step763]: loss 0.710634
[epoch8, step764]: loss 0.618201
[epoch8, step765]: loss 0.548761
[epoch8, step766]: loss 0.839467
[epoch8, step767]: loss 0.765394
[epoch8, step768]: loss 0.565205
[epoch8, step769]: loss 0.560702
[epoch8, step770]: loss 0.735521
[epoch8, step771]: loss 0.624699
[epoch8, step772]: loss 0.778571
[epoch8, step773]: loss 0.694476
[epoch8, step774]: loss 0.568956
[epoch8, step775]: loss 0.310929
[epoch8, step776]: loss 0.629247
[epoch8, step777]: loss 0.769732
[epoch8, step778]: loss 0.429425
[epoch8, step779]: loss 0.582537
[epoch8, step780]: loss 0.569429
[epoch8, step781]: loss 0.562313
[epoch8, step782]: loss 0.727083
[epoch8, step783]: loss 0.776954
[epoch8, step784]: loss 0.746116
[epoch8, step785]: loss 0.664905
[epoch8, step786]: loss 0.655723
[epoch8, step787]: loss 0.550032
[epoch8, step788]: loss 0.608175
[epoch8, step789]: loss 0.560297
[epoch8, step790]: loss 0.417243
[epoch8, step791]: loss 0.714282
[epoch8, step792]: loss 0.369811
[epoch8, step793]: loss 0.600987
[epoch8, step794]: loss 0.702415
[epoch8, step795]: loss 0.549472
[epoch8, step796]: loss 0.579438
[epoch8, step797]: loss 0.741622
[epoch8, step798]: loss 0.673508
[epoch8, step799]: loss 0.578067
[epoch8, step800]: loss 0.588549
[epoch8, step801]: loss 0.756171
[epoch8, step802]: loss 0.494210
[epoch8, step803]: loss 0.560415
[epoch8, step804]: loss 0.799102
[epoch8, step805]: loss 0.572020
[epoch8, step806]: loss 0.596855
[epoch8, step807]: loss 0.727018
[epoch8, step808]: loss 0.562986
[epoch8, step809]: loss 0.635555
[epoch8, step810]: loss 0.615735
[epoch8, step811]: loss 0.559714
[epoch8, step812]: loss 0.611696
[epoch8, step813]: loss 0.594818
[epoch8, step814]: loss 0.447293
[epoch8, step815]: loss 0.639423
[epoch8, step816]: loss 0.839141
[epoch8, step817]: loss 0.548962
[epoch8, step818]: loss 0.702349
[epoch8, step819]: loss 0.358236
[epoch8, step820]: loss 0.721900
[epoch8, step821]: loss 0.465227
[epoch8, step822]: loss 0.491548
[epoch8, step823]: loss 0.603200
[epoch8, step824]: loss 0.504864
[epoch8, step825]: loss 0.463422
[epoch8, step826]: loss 0.668958
[epoch8, step827]: loss 0.703197
[epoch8, step828]: loss 0.399533
[epoch8, step829]: loss 0.714603
[epoch8, step830]: loss 0.828228
[epoch8, step831]: loss 0.206246
[epoch8, step832]: loss 0.610419
[epoch8, step833]: loss 0.811862
[epoch8, step834]: loss 0.691664
[epoch8, step835]: loss 0.680456
[epoch8, step836]: loss 0.444536
[epoch8, step837]: loss 0.621486
[epoch8, step838]: loss 0.857346
[epoch8, step839]: loss 0.681345
[epoch8, step840]: loss 0.494629
[epoch8, step841]: loss 0.537723
[epoch8, step842]: loss 0.791685
[epoch8, step843]: loss 0.881664
[epoch8, step844]: loss 0.918308
[epoch8, step845]: loss 0.575012
[epoch8, step846]: loss 0.580247
[epoch8, step847]: loss 0.779475
[epoch8, step848]: loss 0.654325
[epoch8, step849]: loss 0.467049
[epoch8, step850]: loss 0.679154
[epoch8, step851]: loss 0.610177
[epoch8, step852]: loss 0.562995
[epoch8, step853]: loss 0.765693
[epoch8, step854]: loss 0.467125
[epoch8, step855]: loss 0.636267
[epoch8, step856]: loss 0.582824
[epoch8, step857]: loss 0.527036
[epoch8, step858]: loss 0.697560
[epoch8, step859]: loss 0.614375
[epoch8, step860]: loss 0.462609
[epoch8, step861]: loss 0.820641
[epoch8, step862]: loss 0.625168
[epoch8, step863]: loss 0.719623
[epoch8, step864]: loss 0.531475
[epoch8, step865]: loss 0.538432
[epoch8, step866]: loss 0.386324
[epoch8, step867]: loss 0.422523
[epoch8, step868]: loss 0.567071
[epoch8, step869]: loss 0.825648
[epoch8, step870]: loss 0.674046
[epoch8, step871]: loss 0.355216
[epoch8, step872]: loss 0.402243
[epoch8, step873]: loss 0.695480
[epoch8, step874]: loss 0.479379
[epoch8, step875]: loss 0.581378
[epoch8, step876]: loss 0.722397
[epoch8, step877]: loss 0.702348
[epoch8, step878]: loss 0.723499
[epoch8, step879]: loss 0.686732
[epoch8, step880]: loss 0.865971
[epoch8, step881]: loss 0.788893
[epoch8, step882]: loss 0.326387
[epoch8, step883]: loss 0.738438
[epoch8, step884]: loss 0.494540
[epoch8, step885]: loss 0.489481
[epoch8, step886]: loss 0.841385
[epoch8, step887]: loss 0.784925
[epoch8, step888]: loss 0.755582
[epoch8, step889]: loss 0.822920
[epoch8, step890]: loss 0.700116
[epoch8, step891]: loss 0.459190
[epoch8, step892]: loss 0.556405
[epoch8, step893]: loss 0.324750
[epoch8, step894]: loss 0.529910
[epoch8, step895]: loss 0.384409
[epoch8, step896]: loss 0.744129
[epoch8, step897]: loss 0.499413
[epoch8, step898]: loss 0.654267
[epoch8, step899]: loss 0.787841
[epoch8, step900]: loss 0.534217
[epoch8, step901]: loss 0.526681
[epoch8, step902]: loss 0.530749
[epoch8, step903]: loss 0.694830
[epoch8, step904]: loss 0.455289
[epoch8, step905]: loss 0.622704
[epoch8, step906]: loss 0.416084
[epoch8, step907]: loss 0.489463
[epoch8, step908]: loss 0.755287
[epoch8, step909]: loss 0.856040
[epoch8, step910]: loss 0.694036
[epoch8, step911]: loss 0.679736
[epoch8, step912]: loss 0.821557
[epoch8, step913]: loss 0.354129
[epoch8, step914]: loss 0.697573
[epoch8, step915]: loss 0.817404
[epoch8, step916]: loss 0.506685
[epoch8, step917]: loss 0.731574
[epoch8, step918]: loss 0.343260
[epoch8, step919]: loss 0.757937
[epoch8, step920]: loss 0.555187
[epoch8, step921]: loss 0.703034
[epoch8, step922]: loss 0.340826
[epoch8, step923]: loss 0.507308
[epoch8, step924]: loss 0.570219
[epoch8, step925]: loss 0.805053
[epoch8, step926]: loss 0.645369
[epoch8, step927]: loss 0.521785
[epoch8, step928]: loss 0.688263
[epoch8, step929]: loss 0.719224
[epoch8, step930]: loss 0.522484
[epoch8, step931]: loss 0.468733
[epoch8, step932]: loss 0.834996
[epoch8, step933]: loss 0.610018
[epoch8, step934]: loss 0.499456
[epoch8, step935]: loss 0.813251
[epoch8, step936]: loss 0.418418
[epoch8, step937]: loss 0.483272
[epoch8, step938]: loss 0.696060
[epoch8, step939]: loss 0.576823
[epoch8, step940]: loss 0.542856
[epoch8, step941]: loss 0.678883
[epoch8, step942]: loss 0.226280
[epoch8, step943]: loss 0.418936
[epoch8, step944]: loss 0.430968
[epoch8, step945]: loss 0.686028
[epoch8, step946]: loss 0.736630
[epoch8, step947]: loss 0.812115
[epoch8, step948]: loss 0.719414
[epoch8, step949]: loss 0.209880
[epoch8, step950]: loss 0.917272
[epoch8, step951]: loss 0.591353
[epoch8, step952]: loss 0.616107
[epoch8, step953]: loss 0.879139
[epoch8, step954]: loss 0.733024
[epoch8, step955]: loss 0.668122
[epoch8, step956]: loss 0.799289
[epoch8, step957]: loss 0.560125
[epoch8, step958]: loss 0.515361
[epoch8, step959]: loss 0.445756
[epoch8, step960]: loss 0.531927
[epoch8, step961]: loss 0.443542
[epoch8, step962]: loss 0.808048
[epoch8, step963]: loss 0.742674
[epoch8, step964]: loss 0.669017
[epoch8, step965]: loss 0.754046
[epoch8, step966]: loss 0.644186
[epoch8, step967]: loss 0.740392
[epoch8, step968]: loss 0.672345
[epoch8, step969]: loss 0.655730
[epoch8, step970]: loss 0.498740
[epoch8, step971]: loss 0.424610
[epoch8, step972]: loss 0.638324
[epoch8, step973]: loss 0.560239
[epoch8, step974]: loss 0.791180
[epoch8, step975]: loss 0.509514
[epoch8, step976]: loss 0.270052
[epoch8, step977]: loss 0.667327
[epoch8, step978]: loss 0.730262
[epoch8, step979]: loss 0.622545
[epoch8, step980]: loss 0.530505
[epoch8, step981]: loss 0.600170
[epoch8, step982]: loss 0.646752
[epoch8, step983]: loss 0.666256
[epoch8, step984]: loss 0.736957
[epoch8, step985]: loss 0.557755
[epoch8, step986]: loss 0.584396
[epoch8, step987]: loss 0.780312
[epoch8, step988]: loss 0.494772
[epoch8, step989]: loss 0.577251
[epoch8, step990]: loss 0.641086
[epoch8, step991]: loss 0.256099
[epoch8, step992]: loss 0.450070
[epoch8, step993]: loss 0.622389
[epoch8, step994]: loss 0.634275
[epoch8, step995]: loss 0.765600
[epoch8, step996]: loss 0.719922
[epoch8, step997]: loss 0.477182
[epoch8, step998]: loss 0.790575
[epoch8, step999]: loss 0.250088
[epoch8, step1000]: loss 0.575073
[epoch8, step1001]: loss 0.814121
[epoch8, step1002]: loss 0.502119
[epoch8, step1003]: loss 0.681347
[epoch8, step1004]: loss 0.367358
[epoch8, step1005]: loss 0.369852
[epoch8, step1006]: loss 0.591513
[epoch8, step1007]: loss 0.605377
[epoch8, step1008]: loss 0.559317
[epoch8, step1009]: loss 0.624048
[epoch8, step1010]: loss 0.752381
[epoch8, step1011]: loss 0.670359
[epoch8, step1012]: loss 0.222559
[epoch8, step1013]: loss 0.429659
[epoch8, step1014]: loss 0.509160
[epoch8, step1015]: loss 0.638628
[epoch8, step1016]: loss 0.668462
[epoch8, step1017]: loss 0.389229
[epoch8, step1018]: loss 0.739710
[epoch8, step1019]: loss 0.528973
[epoch8, step1020]: loss 0.864258
[epoch8, step1021]: loss 0.614537
[epoch8, step1022]: loss 0.761082
[epoch8, step1023]: loss 0.604141
[epoch8, step1024]: loss 0.526460
[epoch8, step1025]: loss 0.501193
[epoch8, step1026]: loss 0.693931
[epoch8, step1027]: loss 0.334684
[epoch8, step1028]: loss 0.808609
[epoch8, step1029]: loss 0.721773
[epoch8, step1030]: loss 0.615796
[epoch8, step1031]: loss 0.656765
[epoch8, step1032]: loss 0.647332
[epoch8, step1033]: loss 0.654485
[epoch8, step1034]: loss 0.738127
[epoch8, step1035]: loss 0.612068
[epoch8, step1036]: loss 0.601715
[epoch8, step1037]: loss 0.805795
[epoch8, step1038]: loss 0.679827
[epoch8, step1039]: loss 0.401952
[epoch8, step1040]: loss 0.760522
[epoch8, step1041]: loss 0.478227
[epoch8, step1042]: loss 0.764919
[epoch8, step1043]: loss 0.727840
[epoch8, step1044]: loss 0.734997
[epoch8, step1045]: loss 0.591876
[epoch8, step1046]: loss 0.598440
[epoch8, step1047]: loss 0.541980
[epoch8, step1048]: loss 0.618279
[epoch8, step1049]: loss 0.652465
[epoch8, step1050]: loss 0.607091
[epoch8, step1051]: loss 0.554448
[epoch8, step1052]: loss 0.371647
[epoch8, step1053]: loss 0.826220
[epoch8, step1054]: loss 0.880490
[epoch8, step1055]: loss 0.791843
[epoch8, step1056]: loss 0.714190
[epoch8, step1057]: loss 0.811812
[epoch8, step1058]: loss 0.548647
[epoch8, step1059]: loss 0.711845
[epoch8, step1060]: loss 0.521747
[epoch8, step1061]: loss 0.748809
[epoch8, step1062]: loss 0.474939
[epoch8, step1063]: loss 0.471707
[epoch8, step1064]: loss 0.649350
[epoch8, step1065]: loss 0.549282
[epoch8, step1066]: loss 0.449271
[epoch8, step1067]: loss 0.806217
[epoch8, step1068]: loss 0.677242
[epoch8, step1069]: loss 0.690133
[epoch8, step1070]: loss 0.540910
[epoch8, step1071]: loss 0.641426
[epoch8, step1072]: loss 0.574800
[epoch8, step1073]: loss 0.739190
[epoch8, step1074]: loss 0.438449
[epoch8, step1075]: loss 0.720171
[epoch8, step1076]: loss 0.696038
[epoch8, step1077]: loss 0.648627
[epoch8, step1078]: loss 0.836814
[epoch8, step1079]: loss 0.499754
[epoch8, step1080]: loss 0.739799
[epoch8, step1081]: loss 0.646639
[epoch8, step1082]: loss 0.891077
[epoch8, step1083]: loss 0.731303
[epoch8, step1084]: loss 0.619942
[epoch8, step1085]: loss 0.660316
[epoch8, step1086]: loss 0.536566
[epoch8, step1087]: loss 0.664625
[epoch8, step1088]: loss 0.382626
[epoch8, step1089]: loss 0.795412
[epoch8, step1090]: loss 0.770198
[epoch8, step1091]: loss 0.709582
[epoch8, step1092]: loss 0.590067
[epoch8, step1093]: loss 0.760069
[epoch8, step1094]: loss 0.456672
[epoch8, step1095]: loss 0.796213
[epoch8, step1096]: loss 0.723164
[epoch8, step1097]: loss 0.329935
[epoch8, step1098]: loss 0.762625
[epoch8, step1099]: loss 0.657790
[epoch8, step1100]: loss 0.756516
[epoch8, step1101]: loss 0.666174
[epoch8, step1102]: loss 0.847553
[epoch8, step1103]: loss 0.654117
[epoch8, step1104]: loss 0.692200
[epoch8, step1105]: loss 0.663745
[epoch8, step1106]: loss 0.683756
[epoch8, step1107]: loss 0.676099
[epoch8, step1108]: loss 0.705037
[epoch8, step1109]: loss 0.651679
[epoch8, step1110]: loss 0.644432
[epoch8, step1111]: loss 0.726807
[epoch8, step1112]: loss 0.603535
[epoch8, step1113]: loss 0.714709
[epoch8, step1114]: loss 0.627982
[epoch8, step1115]: loss 0.674297
[epoch8, step1116]: loss 0.770491
[epoch8, step1117]: loss 0.709062
[epoch8, step1118]: loss 0.630869
[epoch8, step1119]: loss 0.641520
[epoch8, step1120]: loss 0.541830
[epoch8, step1121]: loss 0.558306
[epoch8, step1122]: loss 0.680650
[epoch8, step1123]: loss 0.383695
[epoch8, step1124]: loss 0.978632
[epoch8, step1125]: loss 0.616811
[epoch8, step1126]: loss 0.547874
[epoch8, step1127]: loss 0.678348
[epoch8, step1128]: loss 0.735312
[epoch8, step1129]: loss 0.943294
[epoch8, step1130]: loss 0.562016
[epoch8, step1131]: loss 0.866204
[epoch8, step1132]: loss 0.653266
[epoch8, step1133]: loss 0.839080
[epoch8, step1134]: loss 0.711374
[epoch8, step1135]: loss 0.803073
[epoch8, step1136]: loss 0.749952
[epoch8, step1137]: loss 0.737549
[epoch8, step1138]: loss 0.782484
[epoch8, step1139]: loss 0.464273
[epoch8, step1140]: loss 0.536854
[epoch8, step1141]: loss 0.656899
[epoch8, step1142]: loss 0.807048
[epoch8, step1143]: loss 0.590870
[epoch8, step1144]: loss 0.400573
[epoch8, step1145]: loss 0.609974
[epoch8, step1146]: loss 0.651010
[epoch8, step1147]: loss 0.686672
[epoch8, step1148]: loss 0.899509
[epoch8, step1149]: loss 0.414370
[epoch8, step1150]: loss 0.633767
[epoch8, step1151]: loss 0.604301
[epoch8, step1152]: loss 0.767432
[epoch8, step1153]: loss 0.912063
[epoch8, step1154]: loss 0.479782
[epoch8, step1155]: loss 0.740020
[epoch8, step1156]: loss 0.822461
[epoch8, step1157]: loss 0.837538
[epoch8, step1158]: loss 0.479017
[epoch8, step1159]: loss 0.847241
[epoch8, step1160]: loss 0.649441
[epoch8, step1161]: loss 0.499990
[epoch8, step1162]: loss 0.687652
[epoch8, step1163]: loss 0.718226
[epoch8, step1164]: loss 0.583379
[epoch8, step1165]: loss 0.668364
[epoch8, step1166]: loss 0.840664
[epoch8, step1167]: loss 0.545661
[epoch8, step1168]: loss 0.566540
[epoch8, step1169]: loss 0.750481
[epoch8, step1170]: loss 0.694761
[epoch8, step1171]: loss 0.493156
[epoch8, step1172]: loss 0.647553
[epoch8, step1173]: loss 0.575536
[epoch8, step1174]: loss 0.664719
[epoch8, step1175]: loss 0.705938
[epoch8, step1176]: loss 0.754624
[epoch8, step1177]: loss 0.671004
[epoch8, step1178]: loss 0.537153
[epoch8, step1179]: loss 0.402486
[epoch8, step1180]: loss 0.619802
[epoch8, step1181]: loss 0.484859
[epoch8, step1182]: loss 0.645119
[epoch8, step1183]: loss 0.676586
[epoch8, step1184]: loss 0.653476
[epoch8, step1185]: loss 0.494944
[epoch8, step1186]: loss 0.517223
[epoch8, step1187]: loss 0.621100
[epoch8, step1188]: loss 0.525027
[epoch8, step1189]: loss 0.778750
[epoch8, step1190]: loss 0.578031
[epoch8, step1191]: loss 0.619109
[epoch8, step1192]: loss 0.472448
[epoch8, step1193]: loss 0.865867
[epoch8, step1194]: loss 0.698630
[epoch8, step1195]: loss 0.626500
[epoch8, step1196]: loss 0.694966
[epoch8, step1197]: loss 0.762364
[epoch8, step1198]: loss 0.759414
[epoch8, step1199]: loss 0.385660
[epoch8, step1200]: loss 0.519422
[epoch8, step1201]: loss 0.541325
[epoch8, step1202]: loss 0.552126
[epoch8, step1203]: loss 0.731296
[epoch8, step1204]: loss 0.404613
[epoch8, step1205]: loss 0.605502
[epoch8, step1206]: loss 0.470719
[epoch8, step1207]: loss 0.647338
[epoch8, step1208]: loss 0.418072
[epoch8, step1209]: loss 0.506463
[epoch8, step1210]: loss 0.626372
[epoch8, step1211]: loss 0.792700
[epoch8, step1212]: loss 0.730749
[epoch8, step1213]: loss 0.617336
[epoch8, step1214]: loss 0.814635
[epoch8, step1215]: loss 0.590617
[epoch8, step1216]: loss 0.493171
[epoch8, step1217]: loss 0.644850
[epoch8, step1218]: loss 0.570297
[epoch8, step1219]: loss 0.579862
[epoch8, step1220]: loss 0.608645
[epoch8, step1221]: loss 0.858575
[epoch8, step1222]: loss 0.607516
[epoch8, step1223]: loss 0.756234
[epoch8, step1224]: loss 0.501249
[epoch8, step1225]: loss 0.328251
[epoch8, step1226]: loss 0.806961
[epoch8, step1227]: loss 0.552379
[epoch8, step1228]: loss 0.501792
[epoch8, step1229]: loss 0.532658
[epoch8, step1230]: loss 0.530681
[epoch8, step1231]: loss 0.535310
[epoch8, step1232]: loss 0.361242
[epoch8, step1233]: loss 0.721796
[epoch8, step1234]: loss 0.431393
[epoch8, step1235]: loss 0.635743
[epoch8, step1236]: loss 0.373367
[epoch8, step1237]: loss 0.679894
[epoch8, step1238]: loss 0.755735
[epoch8, step1239]: loss 0.521825
[epoch8, step1240]: loss 0.512691
[epoch8, step1241]: loss 0.575347
[epoch8, step1242]: loss 0.337731
[epoch8, step1243]: loss 0.689909
[epoch8, step1244]: loss 0.734111
[epoch8, step1245]: loss 0.370144
[epoch8, step1246]: loss 0.575443
[epoch8, step1247]: loss 0.426655
[epoch8, step1248]: loss 0.354635
[epoch8, step1249]: loss 0.630980
[epoch8, step1250]: loss 0.422475
[epoch8, step1251]: loss 0.302275
[epoch8, step1252]: loss 0.466546
[epoch8, step1253]: loss 0.689484
[epoch8, step1254]: loss 0.805669
[epoch8, step1255]: loss 0.664443
[epoch8, step1256]: loss 0.986472
[epoch8, step1257]: loss 0.456145
[epoch8, step1258]: loss 0.630266
[epoch8, step1259]: loss 0.875190
[epoch8, step1260]: loss 0.618702
[epoch8, step1261]: loss 0.614700
[epoch8, step1262]: loss 0.712224
[epoch8, step1263]: loss 0.625105
[epoch8, step1264]: loss 0.697952
[epoch8, step1265]: loss 0.501680
[epoch8, step1266]: loss 0.725233
[epoch8, step1267]: loss 0.805801
[epoch8, step1268]: loss 0.650307
[epoch8, step1269]: loss 0.530856
[epoch8, step1270]: loss 0.597661
[epoch8, step1271]: loss 0.675339
[epoch8, step1272]: loss 0.566554
[epoch8, step1273]: loss 0.656634
[epoch8, step1274]: loss 0.747279
[epoch8, step1275]: loss 0.749523
[epoch8, step1276]: loss 0.633217
[epoch8, step1277]: loss 0.296610
[epoch8, step1278]: loss 0.581610
[epoch8, step1279]: loss 0.526447
[epoch8, step1280]: loss 0.516356
[epoch8, step1281]: loss 0.649401
[epoch8, step1282]: loss 0.663000
[epoch8, step1283]: loss 0.462774
[epoch8, step1284]: loss 0.601307
[epoch8, step1285]: loss 0.687810
[epoch8, step1286]: loss 0.603763
[epoch8, step1287]: loss 0.637306
[epoch8, step1288]: loss 0.771909
[epoch8, step1289]: loss 0.326203
[epoch8, step1290]: loss 0.533501
[epoch8, step1291]: loss 0.664155
[epoch8, step1292]: loss 0.451656
[epoch8, step1293]: loss 0.437747
[epoch8, step1294]: loss 0.337931
[epoch8, step1295]: loss 0.649778
[epoch8, step1296]: loss 0.543386
[epoch8, step1297]: loss 0.501526
[epoch8, step1298]: loss 0.831045
[epoch8, step1299]: loss 0.537158
[epoch8, step1300]: loss 0.588810
[epoch8, step1301]: loss 0.370858
[epoch8, step1302]: loss 0.633324
[epoch8, step1303]: loss 0.607208
[epoch8, step1304]: loss 0.566298
[epoch8, step1305]: loss 0.775706
[epoch8, step1306]: loss 0.865674
[epoch8, step1307]: loss 0.765595
[epoch8, step1308]: loss 0.683939
[epoch8, step1309]: loss 0.279441
[epoch8, step1310]: loss 0.635283
[epoch8, step1311]: loss 0.388861
[epoch8, step1312]: loss 0.803332
[epoch8, step1313]: loss 0.809415
[epoch8, step1314]: loss 0.405273
[epoch8, step1315]: loss 0.435723
[epoch8, step1316]: loss 0.677481
[epoch8, step1317]: loss 0.575519
[epoch8, step1318]: loss 0.342856
[epoch8, step1319]: loss 0.529866
[epoch8, step1320]: loss 0.753430
[epoch8, step1321]: loss 0.604189
[epoch8, step1322]: loss 0.734519
[epoch8, step1323]: loss 0.580322
[epoch8, step1324]: loss 0.533532
[epoch8, step1325]: loss 0.674154
[epoch8, step1326]: loss 0.645523
[epoch8, step1327]: loss 0.798935
[epoch8, step1328]: loss 0.523903
[epoch8, step1329]: loss 0.712277
[epoch8, step1330]: loss 0.560059
[epoch8, step1331]: loss 0.700329
[epoch8, step1332]: loss 0.824656
[epoch8, step1333]: loss 0.577448
[epoch8, step1334]: loss 0.670999
[epoch8, step1335]: loss 0.633277
[epoch8, step1336]: loss 0.502628
[epoch8, step1337]: loss 0.432338
[epoch8, step1338]: loss 0.321900
[epoch8, step1339]: loss 0.456555
[epoch8, step1340]: loss 0.710202
[epoch8, step1341]: loss 0.457675
[epoch8, step1342]: loss 0.721093
[epoch8, step1343]: loss 0.784246
[epoch8, step1344]: loss 0.816792
[epoch8, step1345]: loss 0.613650
[epoch8, step1346]: loss 0.664864
[epoch8, step1347]: loss 0.806160
[epoch8, step1348]: loss 0.586945
[epoch8, step1349]: loss 0.754263
[epoch8, step1350]: loss 0.605490
[epoch8, step1351]: loss 0.883445
[epoch8, step1352]: loss 0.215361
[epoch8, step1353]: loss 0.745829
[epoch8, step1354]: loss 0.747362
[epoch8, step1355]: loss 0.916377
[epoch8, step1356]: loss 0.731533
[epoch8, step1357]: loss 0.362931
[epoch8, step1358]: loss 0.847696
[epoch8, step1359]: loss 0.764904
[epoch8, step1360]: loss 0.720087
[epoch8, step1361]: loss 0.668312
[epoch8, step1362]: loss 0.304212
[epoch8, step1363]: loss 0.687645
[epoch8, step1364]: loss 0.608447
[epoch8, step1365]: loss 0.681761
[epoch8, step1366]: loss 0.508401
[epoch8, step1367]: loss 0.607261
[epoch8, step1368]: loss 0.672498
[epoch8, step1369]: loss 0.515196
[epoch8, step1370]: loss 0.793934
[epoch8, step1371]: loss 0.700219
[epoch8, step1372]: loss 0.834514
[epoch8, step1373]: loss 0.409432
[epoch8, step1374]: loss 0.829108
[epoch8, step1375]: loss 0.399327
[epoch8, step1376]: loss 0.718880
[epoch8, step1377]: loss 0.380386
[epoch8, step1378]: loss 0.609783
[epoch8, step1379]: loss 0.316210
[epoch8, step1380]: loss 0.396278
[epoch8, step1381]: loss 0.540149
[epoch8, step1382]: loss 0.823484
[epoch8, step1383]: loss 0.689477
[epoch8, step1384]: loss 0.487291
[epoch8, step1385]: loss 0.451627
[epoch8, step1386]: loss 0.594175
[epoch8, step1387]: loss 0.740985
[epoch8, step1388]: loss 0.418972
[epoch8, step1389]: loss 0.658839
[epoch8, step1390]: loss 0.722337
[epoch8, step1391]: loss 0.619113
[epoch8, step1392]: loss 0.718451
[epoch8, step1393]: loss 0.632740
[epoch8, step1394]: loss 0.492002
[epoch8, step1395]: loss 0.664327
[epoch8, step1396]: loss 0.524760
[epoch8, step1397]: loss 0.549589
[epoch8, step1398]: loss 0.749202
[epoch8, step1399]: loss 0.719790
[epoch8, step1400]: loss 0.746134
[epoch8, step1401]: loss 0.594340
[epoch8, step1402]: loss 0.635927
[epoch8, step1403]: loss 0.323192
[epoch8, step1404]: loss 0.252032
[epoch8, step1405]: loss 0.474193
[epoch8, step1406]: loss 0.634698
[epoch8, step1407]: loss 0.559331
[epoch8, step1408]: loss 0.773338
[epoch8, step1409]: loss 0.669210
[epoch8, step1410]: loss 0.658550
[epoch8, step1411]: loss 0.550878
[epoch8, step1412]: loss 0.715743
[epoch8, step1413]: loss 0.668548
[epoch8, step1414]: loss 0.577604
[epoch8, step1415]: loss 0.567339
[epoch8, step1416]: loss 0.675364
[epoch8, step1417]: loss 0.683080
[epoch8, step1418]: loss 0.540473
[epoch8, step1419]: loss 0.824477
[epoch8, step1420]: loss 0.655153
[epoch8, step1421]: loss 0.492294
[epoch8, step1422]: loss 0.559523
[epoch8, step1423]: loss 0.713782
[epoch8, step1424]: loss 0.619625
[epoch8, step1425]: loss 0.488394
[epoch8, step1426]: loss 0.636482
[epoch8, step1427]: loss 0.491854
[epoch8, step1428]: loss 0.468085
[epoch8, step1429]: loss 0.381924
[epoch8, step1430]: loss 0.809740
[epoch8, step1431]: loss 0.723010
[epoch8, step1432]: loss 0.532748
[epoch8, step1433]: loss 0.568607
[epoch8, step1434]: loss 0.587564
[epoch8, step1435]: loss 0.587462
[epoch8, step1436]: loss 0.390231
[epoch8, step1437]: loss 0.681054
[epoch8, step1438]: loss 0.789968
[epoch8, step1439]: loss 0.738175
[epoch8, step1440]: loss 0.561282
[epoch8, step1441]: loss 0.614185
[epoch8, step1442]: loss 0.395160
[epoch8, step1443]: loss 0.798981
[epoch8, step1444]: loss 0.706591
[epoch8, step1445]: loss 0.567138
[epoch8, step1446]: loss 0.429218
[epoch8, step1447]: loss 0.891722
[epoch8, step1448]: loss 0.304888
[epoch8, step1449]: loss 0.723848
[epoch8, step1450]: loss 0.731954
[epoch8, step1451]: loss 0.422363
[epoch8, step1452]: loss 0.569334
[epoch8, step1453]: loss 0.422769
[epoch8, step1454]: loss 0.569384
[epoch8, step1455]: loss 0.553189
[epoch8, step1456]: loss 0.741998
[epoch8, step1457]: loss 0.435307
[epoch8, step1458]: loss 0.613141
[epoch8, step1459]: loss 0.758916
[epoch8, step1460]: loss 0.794555
[epoch8, step1461]: loss 0.723346
[epoch8, step1462]: loss 0.743056
[epoch8, step1463]: loss 0.724740
[epoch8, step1464]: loss 0.813907
[epoch8, step1465]: loss 0.810975
[epoch8, step1466]: loss 0.768356
[epoch8, step1467]: loss 0.602325
[epoch8, step1468]: loss 0.628758
[epoch8, step1469]: loss 0.691134
[epoch8, step1470]: loss 0.653708
[epoch8, step1471]: loss 0.551113
[epoch8, step1472]: loss 0.496891
[epoch8, step1473]: loss 0.838834
[epoch8, step1474]: loss 0.448603
[epoch8, step1475]: loss 0.840842
[epoch8, step1476]: loss 0.555168
[epoch8, step1477]: loss 0.759841
[epoch8, step1478]: loss 0.737252
[epoch8, step1479]: loss 0.810116
[epoch8, step1480]: loss 0.616129
[epoch8, step1481]: loss 0.709945
[epoch8, step1482]: loss 0.383277
[epoch8, step1483]: loss 0.537840
[epoch8, step1484]: loss 0.672758
[epoch8, step1485]: loss 0.790925
[epoch8, step1486]: loss 0.593134
[epoch8, step1487]: loss 0.548356
[epoch8, step1488]: loss 0.750026
[epoch8, step1489]: loss 0.810160
[epoch8, step1490]: loss 0.398624
[epoch8, step1491]: loss 0.428406
[epoch8, step1492]: loss 0.797147
[epoch8, step1493]: loss 0.704106
[epoch8, step1494]: loss 0.411664
[epoch8, step1495]: loss 0.796334
[epoch8, step1496]: loss 0.751113
[epoch8, step1497]: loss 0.761852
[epoch8, step1498]: loss 0.638706
[epoch8, step1499]: loss 0.498573
[epoch8, step1500]: loss 0.615732
[epoch8, step1501]: loss 0.688003
[epoch8, step1502]: loss 0.748913
[epoch8, step1503]: loss 0.592813
[epoch8, step1504]: loss 0.702281
[epoch8, step1505]: loss 0.603784
[epoch8, step1506]: loss 0.806049
[epoch8, step1507]: loss 0.652354
[epoch8, step1508]: loss 0.664872
[epoch8, step1509]: loss 0.383284
[epoch8, step1510]: loss 0.922368
[epoch8, step1511]: loss 0.651303
[epoch8, step1512]: loss 0.709283
[epoch8, step1513]: loss 0.402515
[epoch8, step1514]: loss 0.675174
[epoch8, step1515]: loss 0.556485
[epoch8, step1516]: loss 0.660196
[epoch8, step1517]: loss 0.753727
[epoch8, step1518]: loss 0.723841
[epoch8, step1519]: loss 0.725288
[epoch8, step1520]: loss 0.762833
[epoch8, step1521]: loss 0.824580
[epoch8, step1522]: loss 0.757614
[epoch8, step1523]: loss 0.869553
[epoch8, step1524]: loss 0.670525
[epoch8, step1525]: loss 0.919206
[epoch8, step1526]: loss 0.649092
[epoch8, step1527]: loss 0.511161
[epoch8, step1528]: loss 0.463153
[epoch8, step1529]: loss 0.710062
[epoch8, step1530]: loss 0.561822
[epoch8, step1531]: loss 0.670228
[epoch8, step1532]: loss 0.617938
[epoch8, step1533]: loss 0.821841
[epoch8, step1534]: loss 0.765238
[epoch8, step1535]: loss 0.726516
[epoch8, step1536]: loss 0.561751
[epoch8, step1537]: loss 0.585263
[epoch8, step1538]: loss 0.329684
[epoch8, step1539]: loss 0.544561
[epoch8, step1540]: loss 0.754644
[epoch8, step1541]: loss 0.689313
[epoch8, step1542]: loss 0.549226
[epoch8, step1543]: loss 0.491437
[epoch8, step1544]: loss 0.352617
[epoch8, step1545]: loss 0.516916
[epoch8, step1546]: loss 0.894481
[epoch8, step1547]: loss 0.884050
[epoch8, step1548]: loss 0.540956
[epoch8, step1549]: loss 0.671963
[epoch8, step1550]: loss 0.706626
[epoch8, step1551]: loss 0.718356
[epoch8, step1552]: loss 0.671580
[epoch8, step1553]: loss 0.574905
[epoch8, step1554]: loss 0.623584
[epoch8, step1555]: loss 0.607101
[epoch8, step1556]: loss 0.931236
[epoch8, step1557]: loss 0.822264
[epoch8, step1558]: loss 0.541396
[epoch8, step1559]: loss 0.577226
[epoch8, step1560]: loss 0.516783
[epoch8, step1561]: loss 0.702263
[epoch8, step1562]: loss 0.676769
[epoch8, step1563]: loss 0.831066
[epoch8, step1564]: loss 0.624912
[epoch8, step1565]: loss 0.665219
[epoch8, step1566]: loss 0.502178
[epoch8, step1567]: loss 0.552627
[epoch8, step1568]: loss 0.211924
[epoch8, step1569]: loss 0.613072
[epoch8, step1570]: loss 0.650798
[epoch8, step1571]: loss 0.682730
[epoch8, step1572]: loss 0.933322
[epoch8, step1573]: loss 0.683493
[epoch8, step1574]: loss 0.757124
[epoch8, step1575]: loss 0.809705
[epoch8, step1576]: loss 0.706394
[epoch8, step1577]: loss 0.769534
[epoch8, step1578]: loss 0.626083
[epoch8, step1579]: loss 0.716215
[epoch8, step1580]: loss 0.614953
[epoch8, step1581]: loss 0.424184
[epoch8, step1582]: loss 0.857982
[epoch8, step1583]: loss 0.445917
[epoch8, step1584]: loss 0.639285
[epoch8, step1585]: loss 0.474725
[epoch8, step1586]: loss 0.449876
[epoch8, step1587]: loss 0.566244
[epoch8, step1588]: loss 0.702441
[epoch8, step1589]: loss 0.559765
[epoch8, step1590]: loss 0.616459
[epoch8, step1591]: loss 0.684496
[epoch8, step1592]: loss 0.278056
[epoch8, step1593]: loss 0.287113
[epoch8, step1594]: loss 0.727127
[epoch8, step1595]: loss 0.775970
[epoch8, step1596]: loss 0.646429
[epoch8, step1597]: loss 0.561219
[epoch8, step1598]: loss 0.785911
[epoch8, step1599]: loss 0.499262
[epoch8, step1600]: loss 0.393931
[epoch8, step1601]: loss 0.565193
[epoch8, step1602]: loss 0.733854
[epoch8, step1603]: loss 0.649127
[epoch8, step1604]: loss 0.586496
[epoch8, step1605]: loss 0.619385
[epoch8, step1606]: loss 0.717139
[epoch8, step1607]: loss 0.605562
[epoch8, step1608]: loss 0.599000
[epoch8, step1609]: loss 0.524069
[epoch8, step1610]: loss 0.721392
[epoch8, step1611]: loss 0.551033
[epoch8, step1612]: loss 0.513995
[epoch8, step1613]: loss 0.797808
[epoch8, step1614]: loss 0.656341
[epoch8, step1615]: loss 0.853633
[epoch8, step1616]: loss 0.561590
[epoch8, step1617]: loss 0.415886
[epoch8, step1618]: loss 0.704827
[epoch8, step1619]: loss 0.634780
[epoch8, step1620]: loss 0.555378
[epoch8, step1621]: loss 0.420148
[epoch8, step1622]: loss 0.513713
[epoch8, step1623]: loss 0.785721
[epoch8, step1624]: loss 0.439058
[epoch8, step1625]: loss 0.775032
[epoch8, step1626]: loss 0.498496
[epoch8, step1627]: loss 0.638416
[epoch8, step1628]: loss 0.757227
[epoch8, step1629]: loss 0.465654
[epoch8, step1630]: loss 0.490385
[epoch8, step1631]: loss 0.452525
[epoch8, step1632]: loss 0.543459
[epoch8, step1633]: loss 0.562187
[epoch8, step1634]: loss 0.710742
[epoch8, step1635]: loss 0.722505
[epoch8, step1636]: loss 0.477344
[epoch8, step1637]: loss 0.502576
[epoch8, step1638]: loss 0.652483
[epoch8, step1639]: loss 0.646820
[epoch8, step1640]: loss 0.470040
[epoch8, step1641]: loss 0.523722
[epoch8, step1642]: loss 0.407252
[epoch8, step1643]: loss 0.751208
[epoch8, step1644]: loss 0.499244
[epoch8, step1645]: loss 0.696309
[epoch8, step1646]: loss 0.391899
[epoch8, step1647]: loss 0.394891
[epoch8, step1648]: loss 0.850249
[epoch8, step1649]: loss 0.665419
[epoch8, step1650]: loss 0.655260
[epoch8, step1651]: loss 0.382930
[epoch8, step1652]: loss 0.409320
[epoch8, step1653]: loss 0.654378
[epoch8, step1654]: loss 0.604488
[epoch8, step1655]: loss 0.743869
[epoch8, step1656]: loss 0.479072
[epoch8, step1657]: loss 0.737724
[epoch8, step1658]: loss 0.736714
[epoch8, step1659]: loss 0.641888
[epoch8, step1660]: loss 0.783354
[epoch8, step1661]: loss 0.483305
[epoch8, step1662]: loss 0.827923
[epoch8, step1663]: loss 0.625204
[epoch8, step1664]: loss 0.969214
[epoch8, step1665]: loss 0.765540
[epoch8, step1666]: loss 0.641934
[epoch8, step1667]: loss 0.655081
[epoch8, step1668]: loss 0.625030
[epoch8, step1669]: loss 0.553264
[epoch8, step1670]: loss 0.616782
[epoch8, step1671]: loss 0.960180
[epoch8, step1672]: loss 0.528365
[epoch8, step1673]: loss 0.287409
[epoch8, step1674]: loss 0.780916
[epoch8, step1675]: loss 0.557670
[epoch8, step1676]: loss 0.606256
[epoch8, step1677]: loss 0.366673
[epoch8, step1678]: loss 0.583424
[epoch8, step1679]: loss 0.668507
[epoch8, step1680]: loss 0.721028
[epoch8, step1681]: loss 0.549337
[epoch8, step1682]: loss 0.665040
[epoch8, step1683]: loss 0.686740
[epoch8, step1684]: loss 0.676055
[epoch8, step1685]: loss 0.540292
[epoch8, step1686]: loss 0.613552
[epoch8, step1687]: loss 0.615332
[epoch8, step1688]: loss 0.433963
[epoch8, step1689]: loss 0.651780
[epoch8, step1690]: loss 0.385058
[epoch8, step1691]: loss 0.321806
[epoch8, step1692]: loss 0.203906
[epoch8, step1693]: loss 0.603673
[epoch8, step1694]: loss 0.702946
[epoch8, step1695]: loss 0.815504
[epoch8, step1696]: loss 0.451897
[epoch8, step1697]: loss 0.630883
[epoch8, step1698]: loss 0.377488
[epoch8, step1699]: loss 0.576127
[epoch8, step1700]: loss 0.308528
[epoch8, step1701]: loss 0.748366
[epoch8, step1702]: loss 0.449871
[epoch8, step1703]: loss 0.560395
[epoch8, step1704]: loss 0.781076
[epoch8, step1705]: loss 0.654302
[epoch8, step1706]: loss 0.576917
[epoch8, step1707]: loss 0.637124
[epoch8, step1708]: loss 0.717532
[epoch8, step1709]: loss 0.677174
[epoch8, step1710]: loss 0.702389
[epoch8, step1711]: loss 0.699749
[epoch8, step1712]: loss 0.475931
[epoch8, step1713]: loss 0.493746
[epoch8, step1714]: loss 0.663817
[epoch8, step1715]: loss 0.528643
[epoch8, step1716]: loss 0.519438
[epoch8, step1717]: loss 0.623619
[epoch8, step1718]: loss 0.257723
[epoch8, step1719]: loss 0.632330
[epoch8, step1720]: loss 0.649132
[epoch8, step1721]: loss 0.634861
[epoch8, step1722]: loss 0.821159
[epoch8, step1723]: loss 0.846277
[epoch8, step1724]: loss 0.714558
[epoch8, step1725]: loss 0.606908
[epoch8, step1726]: loss 0.691879
[epoch8, step1727]: loss 0.398703
[epoch8, step1728]: loss 0.601426
[epoch8, step1729]: loss 0.774552
[epoch8, step1730]: loss 0.391418
[epoch8, step1731]: loss 0.798647
[epoch8, step1732]: loss 0.529414
[epoch8, step1733]: loss 0.545212
[epoch8, step1734]: loss 0.472535
[epoch8, step1735]: loss 0.545855
[epoch8, step1736]: loss 0.639035
[epoch8, step1737]: loss 0.566944
[epoch8, step1738]: loss 0.829922
[epoch8, step1739]: loss 0.686516
[epoch8, step1740]: loss 0.852815
[epoch8, step1741]: loss 0.405212
[epoch8, step1742]: loss 0.533763
[epoch8, step1743]: loss 0.596956
[epoch8, step1744]: loss 0.686036
[epoch8, step1745]: loss 0.401329
[epoch8, step1746]: loss 0.733707
[epoch8, step1747]: loss 0.742882
[epoch8, step1748]: loss 0.430126
[epoch8, step1749]: loss 0.512093
[epoch8, step1750]: loss 0.571133
[epoch8, step1751]: loss 0.693509
[epoch8, step1752]: loss 0.710181
[epoch8, step1753]: loss 0.581662
[epoch8, step1754]: loss 0.687456
[epoch8, step1755]: loss 0.725478
[epoch8, step1756]: loss 0.635832
[epoch8, step1757]: loss 0.632557
[epoch8, step1758]: loss 0.438674
[epoch8, step1759]: loss 0.461505
[epoch8, step1760]: loss 0.394340
[epoch8, step1761]: loss 0.615162
[epoch8, step1762]: loss 0.345903
[epoch8, step1763]: loss 0.515612
[epoch8, step1764]: loss 0.774141
[epoch8, step1765]: loss 0.577844
[epoch8, step1766]: loss 0.432818
[epoch8, step1767]: loss 0.557503
[epoch8, step1768]: loss 0.489211
[epoch8, step1769]: loss 0.745341
[epoch8, step1770]: loss 0.798539
[epoch8, step1771]: loss 0.637096
[epoch8, step1772]: loss 0.344804
[epoch8, step1773]: loss 0.597629
[epoch8, step1774]: loss 0.411017
[epoch8, step1775]: loss 0.943373
[epoch8, step1776]: loss 0.570780
[epoch8, step1777]: loss 0.630749
[epoch8, step1778]: loss 0.785883
[epoch8, step1779]: loss 0.816390
[epoch8, step1780]: loss 0.553977
[epoch8, step1781]: loss 0.459772
[epoch8, step1782]: loss 0.706943
[epoch8, step1783]: loss 0.682687
[epoch8, step1784]: loss 0.443934
[epoch8, step1785]: loss 0.423233
[epoch8, step1786]: loss 0.621357
[epoch8, step1787]: loss 0.750382
[epoch8, step1788]: loss 0.766394
[epoch8, step1789]: loss 0.714589
[epoch8, step1790]: loss 0.695940
[epoch8, step1791]: loss 0.480486
[epoch8, step1792]: loss 0.231267
[epoch8, step1793]: loss 0.287960
[epoch8, step1794]: loss 0.512822
[epoch8, step1795]: loss 0.431651
[epoch8, step1796]: loss 0.545026
[epoch8, step1797]: loss 0.514420
[epoch8, step1798]: loss 0.738454
[epoch8, step1799]: loss 0.793010
[epoch8, step1800]: loss 0.739363
[epoch8, step1801]: loss 0.874792
[epoch8, step1802]: loss 0.331632
[epoch8, step1803]: loss 0.649538
[epoch8, step1804]: loss 0.543253
[epoch8, step1805]: loss 0.726199
[epoch8, step1806]: loss 0.737334
[epoch8, step1807]: loss 0.432619
[epoch8, step1808]: loss 0.510335
[epoch8, step1809]: loss 0.564025
[epoch8, step1810]: loss 0.375639
[epoch8, step1811]: loss 0.689137
[epoch8, step1812]: loss 0.442738
[epoch8, step1813]: loss 0.492072
[epoch8, step1814]: loss 0.584850
[epoch8, step1815]: loss 0.447336
[epoch8, step1816]: loss 0.633493
[epoch8, step1817]: loss 0.489396
[epoch8, step1818]: loss 0.470878
[epoch8, step1819]: loss 0.774059
[epoch8, step1820]: loss 0.501424
[epoch8, step1821]: loss 0.609274
[epoch8, step1822]: loss 0.496576
[epoch8, step1823]: loss 0.406401
[epoch8, step1824]: loss 0.694331
[epoch8, step1825]: loss 0.578125
[epoch8, step1826]: loss 0.550202
[epoch8, step1827]: loss 0.576361
[epoch8, step1828]: loss 0.620325
[epoch8, step1829]: loss 0.630789
[epoch8, step1830]: loss 0.656006
[epoch8, step1831]: loss 0.465235
[epoch8, step1832]: loss 0.528870
[epoch8, step1833]: loss 0.648658
[epoch8, step1834]: loss 0.674968
[epoch8, step1835]: loss 0.668137
[epoch8, step1836]: loss 0.539225
[epoch8, step1837]: loss 0.502876
[epoch8, step1838]: loss 0.499012
[epoch8, step1839]: loss 0.380664
[epoch8, step1840]: loss 0.841437
[epoch8, step1841]: loss 0.753317
[epoch8, step1842]: loss 0.435342
[epoch8, step1843]: loss 0.666011
[epoch8, step1844]: loss 0.667666
[epoch8, step1845]: loss 0.523186
[epoch8, step1846]: loss 0.593403
[epoch8, step1847]: loss 0.589433
[epoch8, step1848]: loss 0.451126
[epoch8, step1849]: loss 0.684148
[epoch8, step1850]: loss 0.477970
[epoch8, step1851]: loss 0.693644
[epoch8, step1852]: loss 0.751796
[epoch8, step1853]: loss 0.556707
[epoch8, step1854]: loss 0.137408
[epoch8, step1855]: loss 0.759116
[epoch8, step1856]: loss 0.337437
[epoch8, step1857]: loss 1.021837
[epoch8, step1858]: loss 1.036313
[epoch8, step1859]: loss 0.750039
[epoch8, step1860]: loss 0.492208
[epoch8, step1861]: loss 0.892827
[epoch8, step1862]: loss 0.310022
[epoch8, step1863]: loss 0.695248
[epoch8, step1864]: loss 0.797901
[epoch8, step1865]: loss 0.516518
[epoch8, step1866]: loss 0.774268
[epoch8, step1867]: loss 0.642463
[epoch8, step1868]: loss 0.769759
[epoch8, step1869]: loss 0.555168
[epoch8, step1870]: loss 0.590934
[epoch8, step1871]: loss 0.675572
[epoch8, step1872]: loss 0.385846
[epoch8, step1873]: loss 0.532233
[epoch8, step1874]: loss 0.589414
[epoch8, step1875]: loss 0.403865
[epoch8, step1876]: loss 0.518448
[epoch8, step1877]: loss 0.663880
[epoch8, step1878]: loss 0.650144
[epoch8, step1879]: loss 0.591655
[epoch8, step1880]: loss 0.809603
[epoch8, step1881]: loss 0.455660
[epoch8, step1882]: loss 0.534886
[epoch8, step1883]: loss 0.362547
[epoch8, step1884]: loss 0.867395
[epoch8, step1885]: loss 0.763565
[epoch8, step1886]: loss 0.671695
[epoch8, step1887]: loss 0.594229
[epoch8, step1888]: loss 0.364823
[epoch8, step1889]: loss 0.364252
[epoch8, step1890]: loss 0.790491
[epoch8, step1891]: loss 0.593496
[epoch8, step1892]: loss 0.470267
[epoch8, step1893]: loss 0.815615
[epoch8, step1894]: loss 0.696108
[epoch8, step1895]: loss 0.585432
[epoch8, step1896]: loss 0.497972
[epoch8, step1897]: loss 0.759659
[epoch8, step1898]: loss 0.703480
[epoch8, step1899]: loss 0.486288
[epoch8, step1900]: loss 0.551343
[epoch8, step1901]: loss 0.441113
[epoch8, step1902]: loss 0.377082
[epoch8, step1903]: loss 0.426732
[epoch8, step1904]: loss 0.639346
[epoch8, step1905]: loss 0.715284
[epoch8, step1906]: loss 0.717974
[epoch8, step1907]: loss 0.810270
[epoch8, step1908]: loss 0.721197
[epoch8, step1909]: loss 0.572474
[epoch8, step1910]: loss 0.590722
[epoch8, step1911]: loss 0.717676
[epoch8, step1912]: loss 0.607813
[epoch8, step1913]: loss 0.835306
[epoch8, step1914]: loss 0.665737
[epoch8, step1915]: loss 0.731264
[epoch8, step1916]: loss 0.581896
[epoch8, step1917]: loss 0.783834
[epoch8, step1918]: loss 0.544444
[epoch8, step1919]: loss 0.623520
[epoch8, step1920]: loss 0.576766
[epoch8, step1921]: loss 0.633718
[epoch8, step1922]: loss 0.594508
[epoch8, step1923]: loss 0.722188
[epoch8, step1924]: loss 0.538487
[epoch8, step1925]: loss 0.741068
[epoch8, step1926]: loss 0.650777
[epoch8, step1927]: loss 0.763443
[epoch8, step1928]: loss 0.388030
[epoch8, step1929]: loss 0.466693
[epoch8, step1930]: loss 0.674703
[epoch8, step1931]: loss 0.581370
[epoch8, step1932]: loss 0.477582
[epoch8, step1933]: loss 0.923043
[epoch8, step1934]: loss 0.351908
[epoch8, step1935]: loss 0.648642
[epoch8, step1936]: loss 0.598613
[epoch8, step1937]: loss 0.706710
[epoch8, step1938]: loss 0.560934
[epoch8, step1939]: loss 0.443839
[epoch8, step1940]: loss 0.540610
[epoch8, step1941]: loss 0.849321
[epoch8, step1942]: loss 0.786198
[epoch8, step1943]: loss 0.724925
[epoch8, step1944]: loss 0.587476
[epoch8, step1945]: loss 0.565463
[epoch8, step1946]: loss 0.620252
[epoch8, step1947]: loss 0.772228
[epoch8, step1948]: loss 0.843781
[epoch8, step1949]: loss 0.509094
[epoch8, step1950]: loss 0.663800
[epoch8, step1951]: loss 0.539090
[epoch8, step1952]: loss 0.546130
[epoch8, step1953]: loss 0.521540
[epoch8, step1954]: loss 0.633948
[epoch8, step1955]: loss 0.801292
[epoch8, step1956]: loss 0.709248
[epoch8, step1957]: loss 0.525698
[epoch8, step1958]: loss 0.705301
[epoch8, step1959]: loss 0.306053
[epoch8, step1960]: loss 0.892367
[epoch8, step1961]: loss 0.488297
[epoch8, step1962]: loss 0.467680
[epoch8, step1963]: loss 0.550304
[epoch8, step1964]: loss 0.669445
[epoch8, step1965]: loss 0.787396
[epoch8, step1966]: loss 0.481996
[epoch8, step1967]: loss 0.552275
[epoch8, step1968]: loss 0.646044
[epoch8, step1969]: loss 0.455686
[epoch8, step1970]: loss 0.314231
[epoch8, step1971]: loss 0.660649
[epoch8, step1972]: loss 0.959919
[epoch8, step1973]: loss 0.513755
[epoch8, step1974]: loss 0.376860
[epoch8, step1975]: loss 0.505615
[epoch8, step1976]: loss 0.387902
[epoch8, step1977]: loss 0.573885
[epoch8, step1978]: loss 0.795554
[epoch8, step1979]: loss 0.796375
[epoch8, step1980]: loss 0.561046
[epoch8, step1981]: loss 0.430365
[epoch8, step1982]: loss 0.567212
[epoch8, step1983]: loss 0.315941
[epoch8, step1984]: loss 0.551002
[epoch8, step1985]: loss 0.760196
[epoch8, step1986]: loss 0.688431
[epoch8, step1987]: loss 0.650494
[epoch8, step1988]: loss 0.609999
[epoch8, step1989]: loss 0.740634
[epoch8, step1990]: loss 0.573827
[epoch8, step1991]: loss 0.672405
[epoch8, step1992]: loss 0.211955
[epoch8, step1993]: loss 0.521805
[epoch8, step1994]: loss 0.657172
[epoch8, step1995]: loss 0.644800
[epoch8, step1996]: loss 0.478278
[epoch8, step1997]: loss 0.714038
[epoch8, step1998]: loss 0.576770
[epoch8, step1999]: loss 0.875874
[epoch8, step2000]: loss 0.396074
[epoch8, step2001]: loss 0.666523
[epoch8, step2002]: loss 0.694733
[epoch8, step2003]: loss 0.668864
[epoch8, step2004]: loss 0.337302
[epoch8, step2005]: loss 0.482341
[epoch8, step2006]: loss 0.606752
[epoch8, step2007]: loss 0.612886
[epoch8, step2008]: loss 0.345664
[epoch8, step2009]: loss 0.679484
[epoch8, step2010]: loss 0.580342
[epoch8, step2011]: loss 0.735779
[epoch8, step2012]: loss 0.421639
[epoch8, step2013]: loss 0.739491
[epoch8, step2014]: loss 0.646497
[epoch8, step2015]: loss 0.821134
[epoch8, step2016]: loss 0.507278
[epoch8, step2017]: loss 0.481608
[epoch8, step2018]: loss 0.719689
[epoch8, step2019]: loss 0.560783
[epoch8, step2020]: loss 0.723486
[epoch8, step2021]: loss 0.428262
[epoch8, step2022]: loss 0.707607
[epoch8, step2023]: loss 0.637030
[epoch8, step2024]: loss 0.608718
[epoch8, step2025]: loss 0.703685
[epoch8, step2026]: loss 0.842153
[epoch8, step2027]: loss 0.583804
[epoch8, step2028]: loss 0.559442
[epoch8, step2029]: loss 0.735811
[epoch8, step2030]: loss 0.711886
[epoch8, step2031]: loss 0.600045
[epoch8, step2032]: loss 0.562770
[epoch8, step2033]: loss 0.607367
[epoch8, step2034]: loss 0.780725
[epoch8, step2035]: loss 0.396003
[epoch8, step2036]: loss 0.894435
[epoch8, step2037]: loss 0.641784
[epoch8, step2038]: loss 0.541847
[epoch8, step2039]: loss 0.552848
[epoch8, step2040]: loss 0.631336
[epoch8, step2041]: loss 0.547272
[epoch8, step2042]: loss 0.572047
[epoch8, step2043]: loss 0.817221
[epoch8, step2044]: loss 0.731545
[epoch8, step2045]: loss 0.568288
[epoch8, step2046]: loss 0.691649
[epoch8, step2047]: loss 0.491716
[epoch8, step2048]: loss 0.178688
[epoch8, step2049]: loss 0.464870
[epoch8, step2050]: loss 0.633204
[epoch8, step2051]: loss 0.600329
[epoch8, step2052]: loss 0.803572
[epoch8, step2053]: loss 0.472301
[epoch8, step2054]: loss 0.662716
[epoch8, step2055]: loss 0.511437
[epoch8, step2056]: loss 0.828037
[epoch8, step2057]: loss 0.383138
[epoch8, step2058]: loss 0.807563
[epoch8, step2059]: loss 0.726825
[epoch8, step2060]: loss 0.854306
[epoch8, step2061]: loss 0.410765
[epoch8, step2062]: loss 0.769807
[epoch8, step2063]: loss 0.571524
[epoch8, step2064]: loss 0.583929
[epoch8, step2065]: loss 0.514586
[epoch8, step2066]: loss 0.895149
[epoch8, step2067]: loss 0.535733
[epoch8, step2068]: loss 0.694690
[epoch8, step2069]: loss 0.723531
[epoch8, step2070]: loss 0.740910
[epoch8, step2071]: loss 0.728047
[epoch8, step2072]: loss 0.733331
[epoch8, step2073]: loss 0.659664
[epoch8, step2074]: loss 0.738981
[epoch8, step2075]: loss 0.476229
[epoch8, step2076]: loss 0.402760
[epoch8, step2077]: loss 0.708291
[epoch8, step2078]: loss 0.841042
[epoch8, step2079]: loss 0.656169
[epoch8, step2080]: loss 0.350976
[epoch8, step2081]: loss 0.652100
[epoch8, step2082]: loss 0.692111
[epoch8, step2083]: loss 0.664287
[epoch8, step2084]: loss 0.338650
[epoch8, step2085]: loss 0.780668
[epoch8, step2086]: loss 0.532672
[epoch8, step2087]: loss 0.770158
[epoch8, step2088]: loss 0.578024
[epoch8, step2089]: loss 0.589886
[epoch8, step2090]: loss 0.599093
[epoch8, step2091]: loss 0.573889
[epoch8, step2092]: loss 0.560909
[epoch8, step2093]: loss 0.587735
[epoch8, step2094]: loss 0.855990
[epoch8, step2095]: loss 0.580703
[epoch8, step2096]: loss 0.741003
[epoch8, step2097]: loss 0.682442
[epoch8, step2098]: loss 0.637524
[epoch8, step2099]: loss 0.696829
[epoch8, step2100]: loss 0.455904
[epoch8, step2101]: loss 0.541973
[epoch8, step2102]: loss 0.578455
[epoch8, step2103]: loss 0.785714
[epoch8, step2104]: loss 0.381682
[epoch8, step2105]: loss 0.364558
[epoch8, step2106]: loss 0.638887
[epoch8, step2107]: loss 0.504330
[epoch8, step2108]: loss 0.658544
[epoch8, step2109]: loss 0.714925
[epoch8, step2110]: loss 0.657864
[epoch8, step2111]: loss 0.682715
[epoch8, step2112]: loss 0.301634
[epoch8, step2113]: loss 0.749569
[epoch8, step2114]: loss 0.691622
[epoch8, step2115]: loss 0.580666
[epoch8, step2116]: loss 0.362854
[epoch8, step2117]: loss 0.655573
[epoch8, step2118]: loss 0.740094
[epoch8, step2119]: loss 0.487063
[epoch8, step2120]: loss 0.282947
[epoch8, step2121]: loss 0.371531
[epoch8, step2122]: loss 0.717401
[epoch8, step2123]: loss 0.359055
[epoch8, step2124]: loss 0.661905
[epoch8, step2125]: loss 0.645906
[epoch8, step2126]: loss 0.474106
[epoch8, step2127]: loss 0.599032
[epoch8, step2128]: loss 0.585950
[epoch8, step2129]: loss 0.829632
[epoch8, step2130]: loss 0.661569
[epoch8, step2131]: loss 0.833184
[epoch8, step2132]: loss 0.680331
[epoch8, step2133]: loss 0.715676
[epoch8, step2134]: loss 0.715739
[epoch8, step2135]: loss 0.551813
[epoch8, step2136]: loss 0.603610
[epoch8, step2137]: loss 0.405973
[epoch8, step2138]: loss 0.744641
[epoch8, step2139]: loss 0.538135
[epoch8, step2140]: loss 0.879126
[epoch8, step2141]: loss 0.597986
[epoch8, step2142]: loss 0.781248
[epoch8, step2143]: loss 0.726658
[epoch8, step2144]: loss 0.692517
[epoch8, step2145]: loss 0.432317
[epoch8, step2146]: loss 0.787179
[epoch8, step2147]: loss 0.609814
[epoch8, step2148]: loss 0.677141
[epoch8, step2149]: loss 0.754587
[epoch8, step2150]: loss 0.709742
[epoch8, step2151]: loss 0.496341
[epoch8, step2152]: loss 0.773949
[epoch8, step2153]: loss 0.453033
[epoch8, step2154]: loss 0.720589
[epoch8, step2155]: loss 0.878915
[epoch8, step2156]: loss 0.795413
[epoch8, step2157]: loss 0.382225
[epoch8, step2158]: loss 0.582568
[epoch8, step2159]: loss 0.681802
[epoch8, step2160]: loss 0.462473
[epoch8, step2161]: loss 0.348252
[epoch8, step2162]: loss 0.613655
[epoch8, step2163]: loss 0.678157
[epoch8, step2164]: loss 0.527327
[epoch8, step2165]: loss 0.445797
[epoch8, step2166]: loss 0.582034
[epoch8, step2167]: loss 0.476770
[epoch8, step2168]: loss 0.811219
[epoch8, step2169]: loss 0.705906
[epoch8, step2170]: loss 0.502702
[epoch8, step2171]: loss 0.643813
[epoch8, step2172]: loss 0.451248
[epoch8, step2173]: loss 0.425597
[epoch8, step2174]: loss 0.828845
[epoch8, step2175]: loss 0.585384
[epoch8, step2176]: loss 0.588603
[epoch8, step2177]: loss 0.622426
[epoch8, step2178]: loss 0.666544
[epoch8, step2179]: loss 0.335003
[epoch8, step2180]: loss 0.807298
[epoch8, step2181]: loss 0.710821
[epoch8, step2182]: loss 0.540689
[epoch8, step2183]: loss 0.848723
[epoch8, step2184]: loss 0.721264
[epoch8, step2185]: loss 0.667653
[epoch8, step2186]: loss 0.625729
[epoch8, step2187]: loss 0.485654
[epoch8, step2188]: loss 0.647270
[epoch8, step2189]: loss 0.671069
[epoch8, step2190]: loss 0.618338
[epoch8, step2191]: loss 0.459455
[epoch8, step2192]: loss 0.664931
[epoch8, step2193]: loss 0.551834
[epoch8, step2194]: loss 0.526983
[epoch8, step2195]: loss 0.515241
[epoch8, step2196]: loss 0.439782
[epoch8, step2197]: loss 0.497267
[epoch8, step2198]: loss 0.706885
[epoch8, step2199]: loss 0.647156
[epoch8, step2200]: loss 0.492866
[epoch8, step2201]: loss 0.573411
[epoch8, step2202]: loss 0.493978
[epoch8, step2203]: loss 0.480529
[epoch8, step2204]: loss 0.348629
[epoch8, step2205]: loss 0.653739
[epoch8, step2206]: loss 0.487854
[epoch8, step2207]: loss 0.376280
[epoch8, step2208]: loss 0.631184
[epoch8, step2209]: loss 0.466239
[epoch8, step2210]: loss 0.564935
[epoch8, step2211]: loss 0.595908
[epoch8, step2212]: loss 0.602334
[epoch8, step2213]: loss 0.426553
[epoch8, step2214]: loss 0.630787
[epoch8, step2215]: loss 0.639985
[epoch8, step2216]: loss 0.613594
[epoch8, step2217]: loss 0.699378
[epoch8, step2218]: loss 0.796473
[epoch8, step2219]: loss 0.753751
[epoch8, step2220]: loss 0.614314
[epoch8, step2221]: loss 0.565887
[epoch8, step2222]: loss 0.500771
[epoch8, step2223]: loss 0.640610
[epoch8, step2224]: loss 0.645215
[epoch8, step2225]: loss 0.733901
[epoch8, step2226]: loss 0.718841
[epoch8, step2227]: loss 0.543694
[epoch8, step2228]: loss 0.569080
[epoch8, step2229]: loss 0.631561
[epoch8, step2230]: loss 0.671152
[epoch8, step2231]: loss 0.572105
[epoch8, step2232]: loss 0.683553
[epoch8, step2233]: loss 0.516059
[epoch8, step2234]: loss 0.645189
[epoch8, step2235]: loss 0.636319
[epoch8, step2236]: loss 0.565772
[epoch8, step2237]: loss 0.588430
[epoch8, step2238]: loss 0.842287
[epoch8, step2239]: loss 0.597877
[epoch8, step2240]: loss 0.870918
[epoch8, step2241]: loss 0.813263
[epoch8, step2242]: loss 0.601472
[epoch8, step2243]: loss 0.355544
[epoch8, step2244]: loss 0.739121
[epoch8, step2245]: loss 0.692562
[epoch8, step2246]: loss 0.388668
[epoch8, step2247]: loss 0.480052
[epoch8, step2248]: loss 0.629274
[epoch8, step2249]: loss 0.628811
[epoch8, step2250]: loss 0.520520
[epoch8, step2251]: loss 0.828165
[epoch8, step2252]: loss 0.751926
[epoch8, step2253]: loss 0.613376
[epoch8, step2254]: loss 0.544311
[epoch8, step2255]: loss 0.648285
[epoch8, step2256]: loss 0.712353
[epoch8, step2257]: loss 0.639589
[epoch8, step2258]: loss 0.428226
[epoch8, step2259]: loss 0.574573
[epoch8, step2260]: loss 0.686061
[epoch8, step2261]: loss 0.530175
[epoch8, step2262]: loss 0.547995
[epoch8, step2263]: loss 0.817127
[epoch8, step2264]: loss 0.540715
[epoch8, step2265]: loss 0.758292
[epoch8, step2266]: loss 0.503549
[epoch8, step2267]: loss 0.592618
[epoch8, step2268]: loss 0.473878
[epoch8, step2269]: loss 0.675023
[epoch8, step2270]: loss 0.613043
[epoch8, step2271]: loss 0.601672
[epoch8, step2272]: loss 0.623849
[epoch8, step2273]: loss 0.583453
[epoch8, step2274]: loss 0.749079
[epoch8, step2275]: loss 0.582965
[epoch8, step2276]: loss 0.867376
[epoch8, step2277]: loss 0.671419
[epoch8, step2278]: loss 0.783405
[epoch8, step2279]: loss 0.512161
[epoch8, step2280]: loss 0.645318
[epoch8, step2281]: loss 0.432991
[epoch8, step2282]: loss 0.742235
[epoch8, step2283]: loss 0.792971
[epoch8, step2284]: loss 0.717586
[epoch8, step2285]: loss 0.698720
[epoch8, step2286]: loss 0.566106
[epoch8, step2287]: loss 0.838362
[epoch8, step2288]: loss 0.608704
[epoch8, step2289]: loss 0.788913
[epoch8, step2290]: loss 0.678496
[epoch8, step2291]: loss 0.551249
[epoch8, step2292]: loss 0.505232
[epoch8, step2293]: loss 0.742501
[epoch8, step2294]: loss 0.702568
[epoch8, step2295]: loss 0.623265
[epoch8, step2296]: loss 0.579790
[epoch8, step2297]: loss 0.542560
[epoch8, step2298]: loss 0.694315
[epoch8, step2299]: loss 0.617647
[epoch8, step2300]: loss 0.467705
[epoch8, step2301]: loss 0.852015
[epoch8, step2302]: loss 0.485789
[epoch8, step2303]: loss 0.726574
[epoch8, step2304]: loss 0.401452
[epoch8, step2305]: loss 0.418762
[epoch8, step2306]: loss 0.682944
[epoch8, step2307]: loss 0.603681
[epoch8, step2308]: loss 0.841675
[epoch8, step2309]: loss 0.607871
[epoch8, step2310]: loss 0.900313
[epoch8, step2311]: loss 0.484669
[epoch8, step2312]: loss 0.626328
[epoch8, step2313]: loss 0.387725
[epoch8, step2314]: loss 0.456005
[epoch8, step2315]: loss 0.317441
[epoch8, step2316]: loss 0.722683
[epoch8, step2317]: loss 0.186328
[epoch8, step2318]: loss 0.648443
[epoch8, step2319]: loss 0.450580
[epoch8, step2320]: loss 0.727441
[epoch8, step2321]: loss 0.360118
[epoch8, step2322]: loss 0.518483
[epoch8, step2323]: loss 0.720879
[epoch8, step2324]: loss 0.833406
[epoch8, step2325]: loss 0.834131
[epoch8, step2326]: loss 0.762190
[epoch8, step2327]: loss 0.437451
[epoch8, step2328]: loss 0.606455
[epoch8, step2329]: loss 0.317480
[epoch8, step2330]: loss 0.722852
[epoch8, step2331]: loss 0.736706
[epoch8, step2332]: loss 0.660110
[epoch8, step2333]: loss 0.575996
[epoch8, step2334]: loss 0.435025
[epoch8, step2335]: loss 0.771028
[epoch8, step2336]: loss 0.453458
[epoch8, step2337]: loss 0.629825
[epoch8, step2338]: loss 0.603888
[epoch8, step2339]: loss 0.560416
[epoch8, step2340]: loss 0.244057
[epoch8, step2341]: loss 0.665159
[epoch8, step2342]: loss 0.630200
[epoch8, step2343]: loss 0.673645
[epoch8, step2344]: loss 0.549444
[epoch8, step2345]: loss 0.600943
[epoch8, step2346]: loss 0.501840
[epoch8, step2347]: loss 0.478155
[epoch8, step2348]: loss 0.377144
[epoch8, step2349]: loss 0.668818
[epoch8, step2350]: loss 0.591091
[epoch8, step2351]: loss 0.746417
[epoch8, step2352]: loss 0.752227
[epoch8, step2353]: loss 0.765246
[epoch8, step2354]: loss 0.823238
[epoch8, step2355]: loss 0.651960
[epoch8, step2356]: loss 0.582093
[epoch8, step2357]: loss 0.568667
[epoch8, step2358]: loss 0.764036
[epoch8, step2359]: loss 0.564235
[epoch8, step2360]: loss 0.870259
[epoch8, step2361]: loss 0.358376
[epoch8, step2362]: loss 0.528350
[epoch8, step2363]: loss 0.706609
[epoch8, step2364]: loss 0.709039
[epoch8, step2365]: loss 0.649628
[epoch8, step2366]: loss 0.427288
[epoch8, step2367]: loss 0.575722
[epoch8, step2368]: loss 0.317577
[epoch8, step2369]: loss 0.620158
[epoch8, step2370]: loss 0.603959
[epoch8, step2371]: loss 0.563631
[epoch8, step2372]: loss 0.795342
[epoch8, step2373]: loss 0.798294
[epoch8, step2374]: loss 0.762149
[epoch8, step2375]: loss 0.520909
[epoch8, step2376]: loss 0.636546
[epoch8, step2377]: loss 0.905385
[epoch8, step2378]: loss 0.544122
[epoch8, step2379]: loss 0.536522
[epoch8, step2380]: loss 0.481083
[epoch8, step2381]: loss 0.594870
[epoch8, step2382]: loss 0.519579
[epoch8, step2383]: loss 0.597684
[epoch8, step2384]: loss 0.586870
[epoch8, step2385]: loss 0.779377
[epoch8, step2386]: loss 0.616732
[epoch8, step2387]: loss 0.799203
[epoch8, step2388]: loss 0.482447
[epoch8, step2389]: loss 0.566693
[epoch8, step2390]: loss 0.930880
[epoch8, step2391]: loss 0.704839
[epoch8, step2392]: loss 0.513547
[epoch8, step2393]: loss 0.553111
[epoch8, step2394]: loss 0.439665
[epoch8, step2395]: loss 0.528763
[epoch8, step2396]: loss 0.371271
[epoch8, step2397]: loss 0.432323
[epoch8, step2398]: loss 0.630198
[epoch8, step2399]: loss 0.636432
[epoch8, step2400]: loss 0.558340
[epoch8, step2401]: loss 0.423578
[epoch8, step2402]: loss 0.467094
[epoch8, step2403]: loss 0.663931
[epoch8, step2404]: loss 0.806643
[epoch8, step2405]: loss 0.556261
[epoch8, step2406]: loss 0.951939
[epoch8, step2407]: loss 0.405184
[epoch8, step2408]: loss 0.550430
[epoch8, step2409]: loss 0.717128
[epoch8, step2410]: loss 0.334004
[epoch8, step2411]: loss 0.815409
[epoch8, step2412]: loss 0.575768
[epoch8, step2413]: loss 0.436327
[epoch8, step2414]: loss 0.648796
[epoch8, step2415]: loss 0.673197
[epoch8, step2416]: loss 0.672949
[epoch8, step2417]: loss 0.863755
[epoch8, step2418]: loss 0.357353
[epoch8, step2419]: loss 0.651566
[epoch8, step2420]: loss 0.633593
[epoch8, step2421]: loss 0.596007
[epoch8, step2422]: loss 0.691087
[epoch8, step2423]: loss 0.676276
[epoch8, step2424]: loss 0.569741
[epoch8, step2425]: loss 0.612589
[epoch8, step2426]: loss 0.651206
[epoch8, step2427]: loss 0.590821
[epoch8, step2428]: loss 0.634292
[epoch8, step2429]: loss 0.742733
[epoch8, step2430]: loss 0.311263
[epoch8, step2431]: loss 0.690286
[epoch8, step2432]: loss 0.433606
[epoch8, step2433]: loss 0.669641
[epoch8, step2434]: loss 0.680944
[epoch8, step2435]: loss 0.486097
[epoch8, step2436]: loss 0.521022
[epoch8, step2437]: loss 0.688183
[epoch8, step2438]: loss 0.707923
[epoch8, step2439]: loss 0.678802
[epoch8, step2440]: loss 0.632296
[epoch8, step2441]: loss 0.521799
[epoch8, step2442]: loss 0.323205
[epoch8, step2443]: loss 0.774338
[epoch8, step2444]: loss 0.470734
[epoch8, step2445]: loss 0.389805
[epoch8, step2446]: loss 0.496259
[epoch8, step2447]: loss 0.590793
[epoch8, step2448]: loss 0.395228
[epoch8, step2449]: loss 0.534710
[epoch8, step2450]: loss 0.729093
[epoch8, step2451]: loss 0.658217
[epoch8, step2452]: loss 0.705781
[epoch8, step2453]: loss 0.754492
[epoch8, step2454]: loss 0.624905
[epoch8, step2455]: loss 0.605378
[epoch8, step2456]: loss 0.655148
[epoch8, step2457]: loss 0.581874
[epoch8, step2458]: loss 0.468556
[epoch8, step2459]: loss 0.658095
[epoch8, step2460]: loss 0.448938
[epoch8, step2461]: loss 0.387744
[epoch8, step2462]: loss 0.735845
[epoch8, step2463]: loss 0.765875
[epoch8, step2464]: loss 0.714571
[epoch8, step2465]: loss 0.569767
[epoch8, step2466]: loss 0.528019
[epoch8, step2467]: loss 0.436325
[epoch8, step2468]: loss 0.707910
[epoch8, step2469]: loss 0.843926
[epoch8, step2470]: loss 0.705565
[epoch8, step2471]: loss 0.858630
[epoch8, step2472]: loss 0.560252
[epoch8, step2473]: loss 0.714985
[epoch8, step2474]: loss 0.314566
[epoch8, step2475]: loss 0.835960
[epoch8, step2476]: loss 0.514560
[epoch8, step2477]: loss 0.791953
[epoch8, step2478]: loss 0.740622
[epoch8, step2479]: loss 0.328429
[epoch8, step2480]: loss 0.659943
[epoch8, step2481]: loss 0.552726
[epoch8, step2482]: loss 0.454852
[epoch8, step2483]: loss 0.329929
[epoch8, step2484]: loss 0.719746
[epoch8, step2485]: loss 0.518876
[epoch8, step2486]: loss 0.614297
[epoch8, step2487]: loss 0.219102
[epoch8, step2488]: loss 0.891435
[epoch8, step2489]: loss 0.604952
[epoch8, step2490]: loss 0.900583
[epoch8, step2491]: loss 0.483570
[epoch8, step2492]: loss 0.559706
[epoch8, step2493]: loss 0.879960
[epoch8, step2494]: loss 0.783739
[epoch8, step2495]: loss 0.458543
[epoch8, step2496]: loss 0.568168
[epoch8, step2497]: loss 0.531122
[epoch8, step2498]: loss 0.564002
[epoch8, step2499]: loss 0.677800
[epoch8, step2500]: loss 0.473177
[epoch8, step2501]: loss 0.465683
[epoch8, step2502]: loss 0.639903
[epoch8, step2503]: loss 0.501508
[epoch8, step2504]: loss 0.366977
[epoch8, step2505]: loss 0.768450
[epoch8, step2506]: loss 0.705790
[epoch8, step2507]: loss 0.716366
[epoch8, step2508]: loss 0.510489
[epoch8, step2509]: loss 0.528997
[epoch8, step2510]: loss 0.680045
[epoch8, step2511]: loss 0.503767
[epoch8, step2512]: loss 0.644086
[epoch8, step2513]: loss 0.641058
[epoch8, step2514]: loss 0.757482
[epoch8, step2515]: loss 0.528489
[epoch8, step2516]: loss 0.751829
[epoch8, step2517]: loss 0.840016
[epoch8, step2518]: loss 0.494354
[epoch8, step2519]: loss 0.557323
[epoch8, step2520]: loss 0.555330
[epoch8, step2521]: loss 0.470711
[epoch8, step2522]: loss 0.621765
[epoch8, step2523]: loss 0.640484
[epoch8, step2524]: loss 0.611250
[epoch8, step2525]: loss 0.716819
[epoch8, step2526]: loss 0.718990
[epoch8, step2527]: loss 0.738412
[epoch8, step2528]: loss 0.803693
[epoch8, step2529]: loss 0.694659
[epoch8, step2530]: loss 0.549975
[epoch8, step2531]: loss 0.514591
[epoch8, step2532]: loss 0.815162
[epoch8, step2533]: loss 0.642475
[epoch8, step2534]: loss 0.653905
[epoch8, step2535]: loss 0.614213
[epoch8, step2536]: loss 0.203567
[epoch8, step2537]: loss 0.742614
[epoch8, step2538]: loss 0.376410
[epoch8, step2539]: loss 0.429156
[epoch8, step2540]: loss 0.798206
[epoch8, step2541]: loss 0.389173
[epoch8, step2542]: loss 0.561335
[epoch8, step2543]: loss 0.456606
[epoch8, step2544]: loss 0.597403
[epoch8, step2545]: loss 0.501327
[epoch8, step2546]: loss 0.716074
[epoch8, step2547]: loss 0.646663
[epoch8, step2548]: loss 0.782472
[epoch8, step2549]: loss 0.654552
[epoch8, step2550]: loss 0.649350
[epoch8, step2551]: loss 0.496474
[epoch8, step2552]: loss 0.466518
[epoch8, step2553]: loss 0.779111
[epoch8, step2554]: loss 0.144410
[epoch8, step2555]: loss 0.751860
[epoch8, step2556]: loss 0.632604
[epoch8, step2557]: loss 0.672174
[epoch8, step2558]: loss 0.417913
[epoch8, step2559]: loss 0.280967
[epoch8, step2560]: loss 0.857201
[epoch8, step2561]: loss 0.489260
[epoch8, step2562]: loss 0.643009
[epoch8, step2563]: loss 0.626654
[epoch8, step2564]: loss 0.786931
[epoch8, step2565]: loss 0.494000
[epoch8, step2566]: loss 0.688701
[epoch8, step2567]: loss 0.495919
[epoch8, step2568]: loss 0.701760
[epoch8, step2569]: loss 0.638524
[epoch8, step2570]: loss 0.580214
[epoch8, step2571]: loss 0.630037
[epoch8, step2572]: loss 0.658123
[epoch8, step2573]: loss 0.437811
[epoch8, step2574]: loss 0.490522
[epoch8, step2575]: loss 0.431965
[epoch8, step2576]: loss 0.609906
[epoch8, step2577]: loss 0.533656
[epoch8, step2578]: loss 0.378866
[epoch8, step2579]: loss 0.621105
[epoch8, step2580]: loss 0.577794
[epoch8, step2581]: loss 0.772214
[epoch8, step2582]: loss 0.378843
[epoch8, step2583]: loss 0.554105
[epoch8, step2584]: loss 0.346724
[epoch8, step2585]: loss 0.573162
[epoch8, step2586]: loss 0.699998
[epoch8, step2587]: loss 0.612216
[epoch8, step2588]: loss 0.549941
[epoch8, step2589]: loss 0.761219
[epoch8, step2590]: loss 0.477178
[epoch8, step2591]: loss 0.494310
[epoch8, step2592]: loss 0.589445
[epoch8, step2593]: loss 0.513275
[epoch8, step2594]: loss 0.467887
[epoch8, step2595]: loss 0.849285
[epoch8, step2596]: loss 0.518046
[epoch8, step2597]: loss 0.605954
[epoch8, step2598]: loss 0.644841
[epoch8, step2599]: loss 0.735885
[epoch8, step2600]: loss 0.721627
[epoch8, step2601]: loss 0.663587
[epoch8, step2602]: loss 0.665007
[epoch8, step2603]: loss 0.673107
[epoch8, step2604]: loss 0.438596
[epoch8, step2605]: loss 0.736595
[epoch8, step2606]: loss 0.434693
[epoch8, step2607]: loss 0.548486
[epoch8, step2608]: loss 0.757828
[epoch8, step2609]: loss 0.898627
[epoch8, step2610]: loss 0.650370
[epoch8, step2611]: loss 1.033761
[epoch8, step2612]: loss 0.579022
[epoch8, step2613]: loss 0.772159
[epoch8, step2614]: loss 0.773972
[epoch8, step2615]: loss 0.656904
[epoch8, step2616]: loss 0.578143
[epoch8, step2617]: loss 0.587812
[epoch8, step2618]: loss 0.603397
[epoch8, step2619]: loss 0.658827
[epoch8, step2620]: loss 0.509001
[epoch8, step2621]: loss 0.542952
[epoch8, step2622]: loss 0.697439
[epoch8, step2623]: loss 0.432273
[epoch8, step2624]: loss 0.637779
[epoch8, step2625]: loss 0.556751
[epoch8, step2626]: loss 0.565263
[epoch8, step2627]: loss 0.402299
[epoch8, step2628]: loss 0.515272
[epoch8, step2629]: loss 0.421986
[epoch8, step2630]: loss 0.642623
[epoch8, step2631]: loss 0.913055
[epoch8, step2632]: loss 0.352698
[epoch8, step2633]: loss 0.535603
[epoch8, step2634]: loss 0.578641
[epoch8, step2635]: loss 0.652279
[epoch8, step2636]: loss 0.520011
[epoch8, step2637]: loss 0.540566
[epoch8, step2638]: loss 0.684947
[epoch8, step2639]: loss 0.433420
[epoch8, step2640]: loss 0.646393
[epoch8, step2641]: loss 0.701771
[epoch8, step2642]: loss 0.598104
[epoch8, step2643]: loss 0.564938
[epoch8, step2644]: loss 0.514171
[epoch8, step2645]: loss 0.591278
[epoch8, step2646]: loss 0.508239
[epoch8, step2647]: loss 0.385715
[epoch8, step2648]: loss 0.460451
[epoch8, step2649]: loss 0.435452
[epoch8, step2650]: loss 0.661631
[epoch8, step2651]: loss 0.609250
[epoch8, step2652]: loss 0.542858
[epoch8, step2653]: loss 0.783281
[epoch8, step2654]: loss 0.626868
[epoch8, step2655]: loss 0.589763
[epoch8, step2656]: loss 0.804836
[epoch8, step2657]: loss 0.488363
[epoch8, step2658]: loss 0.264255
[epoch8, step2659]: loss 0.731601
[epoch8, step2660]: loss 0.506256
[epoch8, step2661]: loss 0.536286
[epoch8, step2662]: loss 0.565447
[epoch8, step2663]: loss 0.793099
[epoch8, step2664]: loss 0.818792
[epoch8, step2665]: loss 0.684062
[epoch8, step2666]: loss 0.504065
[epoch8, step2667]: loss 0.721822
[epoch8, step2668]: loss 0.728088
[epoch8, step2669]: loss 0.574784
[epoch8, step2670]: loss 0.308692
[epoch8, step2671]: loss 0.447063
[epoch8, step2672]: loss 0.730381
[epoch8, step2673]: loss 0.683414
[epoch8, step2674]: loss 0.830788
[epoch8, step2675]: loss 0.901585
[epoch8, step2676]: loss 0.545142
[epoch8, step2677]: loss 0.329997
[epoch8, step2678]: loss 0.401058
[epoch8, step2679]: loss 0.367154
[epoch8, step2680]: loss 0.989022
[epoch8, step2681]: loss 0.501233
[epoch8, step2682]: loss 0.234541
[epoch8, step2683]: loss 0.624854
[epoch8, step2684]: loss 0.698262
[epoch8, step2685]: loss 0.491757
[epoch8, step2686]: loss 0.570947
[epoch8, step2687]: loss 0.605333
[epoch8, step2688]: loss 0.832173
[epoch8, step2689]: loss 0.759609
[epoch8, step2690]: loss 0.563002
[epoch8, step2691]: loss 0.694848
[epoch8, step2692]: loss 0.700450
[epoch8, step2693]: loss 0.465004
[epoch8, step2694]: loss 0.504259
[epoch8, step2695]: loss 0.328173
[epoch8, step2696]: loss 0.381566
[epoch8, step2697]: loss 0.568841
[epoch8, step2698]: loss 0.731151
[epoch8, step2699]: loss 0.367916
[epoch8, step2700]: loss 0.489968
[epoch8, step2701]: loss 0.784377
[epoch8, step2702]: loss 0.540496
[epoch8, step2703]: loss 0.421873
[epoch8, step2704]: loss 0.601644
[epoch8, step2705]: loss 0.541253
[epoch8, step2706]: loss 0.382212
[epoch8, step2707]: loss 0.591946
[epoch8, step2708]: loss 0.836308
[epoch8, step2709]: loss 0.511549
[epoch8, step2710]: loss 0.552762
[epoch8, step2711]: loss 0.774392
[epoch8, step2712]: loss 0.416649
[epoch8, step2713]: loss 0.945584
[epoch8, step2714]: loss 0.619368
[epoch8, step2715]: loss 0.766629
[epoch8, step2716]: loss 0.468633
[epoch8, step2717]: loss 0.886260
[epoch8, step2718]: loss 0.499127
[epoch8, step2719]: loss 0.676445
[epoch8, step2720]: loss 0.427813
[epoch8, step2721]: loss 0.668467
[epoch8, step2722]: loss 0.601745
[epoch8, step2723]: loss 0.497130
[epoch8, step2724]: loss 0.812888
[epoch8, step2725]: loss 0.395579
[epoch8, step2726]: loss 0.642810
[epoch8, step2727]: loss 0.790473
[epoch8, step2728]: loss 0.762552
[epoch8, step2729]: loss 0.507158
[epoch8, step2730]: loss 0.754521
[epoch8, step2731]: loss 0.596229
[epoch8, step2732]: loss 0.634877
[epoch8, step2733]: loss 0.623029
[epoch8, step2734]: loss 0.891161
[epoch8, step2735]: loss 0.529011
[epoch8, step2736]: loss 0.664079
[epoch8, step2737]: loss 0.784427
[epoch8, step2738]: loss 0.567178
[epoch8, step2739]: loss 0.603880
[epoch8, step2740]: loss 0.684354
[epoch8, step2741]: loss 0.876431
[epoch8, step2742]: loss 0.606886
[epoch8, step2743]: loss 0.835463
[epoch8, step2744]: loss 0.611238
[epoch8, step2745]: loss 0.719041
[epoch8, step2746]: loss 0.620807
[epoch8, step2747]: loss 0.491017
[epoch8, step2748]: loss 0.568596
[epoch8, step2749]: loss 0.808256
[epoch8, step2750]: loss 0.439140
[epoch8, step2751]: loss 0.758968
[epoch8, step2752]: loss 0.591950
[epoch8, step2753]: loss 0.764238
[epoch8, step2754]: loss 0.737890
[epoch8, step2755]: loss 0.688592
[epoch8, step2756]: loss 0.753460
[epoch8, step2757]: loss 0.588419
[epoch8, step2758]: loss 0.544610
[epoch8, step2759]: loss 0.620767
[epoch8, step2760]: loss 0.577918
[epoch8, step2761]: loss 0.765667
[epoch8, step2762]: loss 0.539676
[epoch8, step2763]: loss 0.521067
[epoch8, step2764]: loss 0.477006
[epoch8, step2765]: loss 0.745660
[epoch8, step2766]: loss 0.817842
[epoch8, step2767]: loss 0.601562
[epoch8, step2768]: loss 0.590548
[epoch8, step2769]: loss 0.463675
[epoch8, step2770]: loss 0.659336
[epoch8, step2771]: loss 0.521448
[epoch8, step2772]: loss 0.598382
[epoch8, step2773]: loss 0.521563
[epoch8, step2774]: loss 0.755338
[epoch8, step2775]: loss 0.910261
[epoch8, step2776]: loss 0.722383
[epoch8, step2777]: loss 0.807910
[epoch8, step2778]: loss 0.578733
[epoch8, step2779]: loss 0.581879
[epoch8, step2780]: loss 0.694648
[epoch8, step2781]: loss 0.761064
[epoch8, step2782]: loss 0.195724
[epoch8, step2783]: loss 0.812097
[epoch8, step2784]: loss 0.692400
[epoch8, step2785]: loss 0.496698
[epoch8, step2786]: loss 0.606026
[epoch8, step2787]: loss 0.604708
[epoch8, step2788]: loss 0.614655
[epoch8, step2789]: loss 0.456993
[epoch8, step2790]: loss 0.647118
[epoch8, step2791]: loss 0.725924
[epoch8, step2792]: loss 0.547209
[epoch8, step2793]: loss 0.530261
[epoch8, step2794]: loss 0.337184
[epoch8, step2795]: loss 0.353222
[epoch8, step2796]: loss 0.889145
[epoch8, step2797]: loss 0.542533
[epoch8, step2798]: loss 0.778047
[epoch8, step2799]: loss 0.709966
[epoch8, step2800]: loss 0.506224
[epoch8, step2801]: loss 0.733671
[epoch8, step2802]: loss 0.555075
[epoch8, step2803]: loss 0.580827
[epoch8, step2804]: loss 0.546447
[epoch8, step2805]: loss 0.504316
[epoch8, step2806]: loss 0.643954
[epoch8, step2807]: loss 0.565863
[epoch8, step2808]: loss 0.435146
[epoch8, step2809]: loss 0.407544
[epoch8, step2810]: loss 0.730282
[epoch8, step2811]: loss 0.521489
[epoch8, step2812]: loss 0.535745
[epoch8, step2813]: loss 0.447569
[epoch8, step2814]: loss 0.523192
[epoch8, step2815]: loss 0.685302
[epoch8, step2816]: loss 0.656992
[epoch8, step2817]: loss 0.573272
[epoch8, step2818]: loss 0.477227
[epoch8, step2819]: loss 0.693380
[epoch8, step2820]: loss 0.607180
[epoch8, step2821]: loss 0.560106
[epoch8, step2822]: loss 0.408879
[epoch8, step2823]: loss 0.544274
[epoch8, step2824]: loss 0.588610
[epoch8, step2825]: loss 0.573369
[epoch8, step2826]: loss 0.785730
[epoch8, step2827]: loss 0.708862
[epoch8, step2828]: loss 0.661087
[epoch8, step2829]: loss 0.495628
[epoch8, step2830]: loss 0.573176
[epoch8, step2831]: loss 0.677517
[epoch8, step2832]: loss 0.649599
[epoch8, step2833]: loss 0.717252
[epoch8, step2834]: loss 0.493831
[epoch8, step2835]: loss 0.698586
[epoch8, step2836]: loss 0.732633
[epoch8, step2837]: loss 0.741186
[epoch8, step2838]: loss 0.719087
[epoch8, step2839]: loss 0.475672
[epoch8, step2840]: loss 0.540846
[epoch8, step2841]: loss 0.548998
[epoch8, step2842]: loss 0.456777
[epoch8, step2843]: loss 0.338607
[epoch8, step2844]: loss 0.473670
[epoch8, step2845]: loss 0.863036
[epoch8, step2846]: loss 0.608386
[epoch8, step2847]: loss 0.559265
[epoch8, step2848]: loss 0.770040
[epoch8, step2849]: loss 0.928563
[epoch8, step2850]: loss 0.779704
[epoch8, step2851]: loss 0.660469
[epoch8, step2852]: loss 0.531886
[epoch8, step2853]: loss 0.624981
[epoch8, step2854]: loss 0.594987
[epoch8, step2855]: loss 0.733629
[epoch8, step2856]: loss 0.705440
[epoch8, step2857]: loss 0.602646
[epoch8, step2858]: loss 0.785454
[epoch8, step2859]: loss 0.926055
[epoch8, step2860]: loss 0.657557
[epoch8, step2861]: loss 0.760340
[epoch8, step2862]: loss 0.733333
[epoch8, step2863]: loss 0.713577
[epoch8, step2864]: loss 0.671577
[epoch8, step2865]: loss 0.852184
[epoch8, step2866]: loss 0.618537
[epoch8, step2867]: loss 0.605672
[epoch8, step2868]: loss 0.756851
[epoch8, step2869]: loss 0.592409
[epoch8, step2870]: loss 0.697596
[epoch8, step2871]: loss 0.559553
[epoch8, step2872]: loss 0.577745
[epoch8, step2873]: loss 0.521645
[epoch8, step2874]: loss 0.647608
[epoch8, step2875]: loss 0.601162
[epoch8, step2876]: loss 0.491875
[epoch8, step2877]: loss 0.361734
[epoch8, step2878]: loss 0.631570
[epoch8, step2879]: loss 0.487895
[epoch8, step2880]: loss 0.596115
[epoch8, step2881]: loss 0.530676
[epoch8, step2882]: loss 0.585875
[epoch8, step2883]: loss 0.416900
[epoch8, step2884]: loss 0.845660
[epoch8, step2885]: loss 0.555731
[epoch8, step2886]: loss 0.261899
[epoch8, step2887]: loss 0.589904
[epoch8, step2888]: loss 0.604683
[epoch8, step2889]: loss 0.682313
[epoch8, step2890]: loss 0.348424
[epoch8, step2891]: loss 0.945479
[epoch8, step2892]: loss 0.686120
[epoch8, step2893]: loss 0.385086
[epoch8, step2894]: loss 0.590055
[epoch8, step2895]: loss 0.575770
[epoch8, step2896]: loss 0.623852
[epoch8, step2897]: loss 0.653244
[epoch8, step2898]: loss 0.686185
[epoch8, step2899]: loss 0.621592
[epoch8, step2900]: loss 0.433318
[epoch8, step2901]: loss 0.666704
[epoch8, step2902]: loss 0.580696
[epoch8, step2903]: loss 0.551389
[epoch8, step2904]: loss 0.373781
[epoch8, step2905]: loss 0.616969
[epoch8, step2906]: loss 0.681118
[epoch8, step2907]: loss 0.574762
[epoch8, step2908]: loss 0.650143
[epoch8, step2909]: loss 0.725743
[epoch8, step2910]: loss 0.861832
[epoch8, step2911]: loss 0.479646
[epoch8, step2912]: loss 0.485174
[epoch8, step2913]: loss 0.439001
[epoch8, step2914]: loss 0.545372
[epoch8, step2915]: loss 0.664903
[epoch8, step2916]: loss 0.729788
[epoch8, step2917]: loss 0.717421
[epoch8, step2918]: loss 0.347357
[epoch8, step2919]: loss 0.577398
[epoch8, step2920]: loss 0.806354
[epoch8, step2921]: loss 0.580414
[epoch8, step2922]: loss 0.790957
[epoch8, step2923]: loss 0.840338
[epoch8, step2924]: loss 0.574619
[epoch8, step2925]: loss 0.383654
[epoch8, step2926]: loss 0.569369
[epoch8, step2927]: loss 0.646541
[epoch8, step2928]: loss 0.836104
[epoch8, step2929]: loss 0.664345
[epoch8, step2930]: loss 0.758680
[epoch8, step2931]: loss 0.762410
[epoch8, step2932]: loss 0.439212
[epoch8, step2933]: loss 0.664114
[epoch8, step2934]: loss 0.765310
[epoch8, step2935]: loss 0.811165
[epoch8, step2936]: loss 0.385737
[epoch8, step2937]: loss 0.594219
[epoch8, step2938]: loss 0.503100
[epoch8, step2939]: loss 0.436047
[epoch8, step2940]: loss 0.478156
[epoch8, step2941]: loss 0.476828
[epoch8, step2942]: loss 0.616664
[epoch8, step2943]: loss 0.573601
[epoch8, step2944]: loss 0.705745
[epoch8, step2945]: loss 0.409627
[epoch8, step2946]: loss 0.626573
[epoch8, step2947]: loss 0.679414
[epoch8, step2948]: loss 0.501953
[epoch8, step2949]: loss 0.575814
[epoch8, step2950]: loss 0.655502
[epoch8, step2951]: loss 0.782340
[epoch8, step2952]: loss 0.428873
[epoch8, step2953]: loss 0.598387
[epoch8, step2954]: loss 0.828683
[epoch8, step2955]: loss 0.704114
[epoch8, step2956]: loss 0.441000
[epoch8, step2957]: loss 0.576799
[epoch8, step2958]: loss 0.572802
[epoch8, step2959]: loss 0.681870
[epoch8, step2960]: loss 0.744146
[epoch8, step2961]: loss 0.818856
[epoch8, step2962]: loss 0.727555
[epoch8, step2963]: loss 0.832360
[epoch8, step2964]: loss 0.355982
[epoch8, step2965]: loss 0.327093
[epoch8, step2966]: loss 0.384093
[epoch8, step2967]: loss 0.357142
[epoch8, step2968]: loss 0.412072
[epoch8, step2969]: loss 0.602562
[epoch8, step2970]: loss 0.587070
[epoch8, step2971]: loss 0.576048
[epoch8, step2972]: loss 0.321385
[epoch8, step2973]: loss 0.625259
[epoch8, step2974]: loss 0.473969
[epoch8, step2975]: loss 0.227033
[epoch8, step2976]: loss 0.183076
[epoch8, step2977]: loss 0.646980
[epoch8, step2978]: loss 0.690573
[epoch8, step2979]: loss 0.816741
[epoch8, step2980]: loss 0.663101
[epoch8, step2981]: loss 0.444976
[epoch8, step2982]: loss 0.532575
[epoch8, step2983]: loss 0.667977
[epoch8, step2984]: loss 0.530427
[epoch8, step2985]: loss 0.697289
[epoch8, step2986]: loss 0.695368
[epoch8, step2987]: loss 0.564146
[epoch8, step2988]: loss 0.552159
[epoch8, step2989]: loss 0.726085
[epoch8, step2990]: loss 0.590500
[epoch8, step2991]: loss 0.482980
[epoch8, step2992]: loss 0.687644
[epoch8, step2993]: loss 0.636432
[epoch8, step2994]: loss 0.806135
[epoch8, step2995]: loss 0.719269
[epoch8, step2996]: loss 0.514449
[epoch8, step2997]: loss 0.584162
[epoch8, step2998]: loss 0.514068
[epoch8, step2999]: loss 0.620747
[epoch8, step3000]: loss 0.689506
[epoch8, step3001]: loss 0.441539
[epoch8, step3002]: loss 0.753141
[epoch8, step3003]: loss 0.741816
[epoch8, step3004]: loss 0.472082
[epoch8, step3005]: loss 0.724956
[epoch8, step3006]: loss 0.857532
[epoch8, step3007]: loss 0.300487
[epoch8, step3008]: loss 0.612534
[epoch8, step3009]: loss 0.555596
[epoch8, step3010]: loss 0.458158
[epoch8, step3011]: loss 0.612620
[epoch8, step3012]: loss 0.568583
[epoch8, step3013]: loss 0.781872
[epoch8, step3014]: loss 0.748143
[epoch8, step3015]: loss 0.417130
[epoch8, step3016]: loss 0.511505
[epoch8, step3017]: loss 0.688502
[epoch8, step3018]: loss 0.634159
[epoch8, step3019]: loss 0.510999
[epoch8, step3020]: loss 0.769734
[epoch8, step3021]: loss 0.624256
[epoch8, step3022]: loss 0.600178
[epoch8, step3023]: loss 0.729136
[epoch8, step3024]: loss 0.206331
[epoch8, step3025]: loss 0.799388
[epoch8, step3026]: loss 0.533173
[epoch8, step3027]: loss 0.574223
[epoch8, step3028]: loss 0.476079
[epoch8, step3029]: loss 0.625647
[epoch8, step3030]: loss 0.668357
[epoch8, step3031]: loss 0.747154
[epoch8, step3032]: loss 0.503123
[epoch8, step3033]: loss 0.691535
[epoch8, step3034]: loss 0.518581
[epoch8, step3035]: loss 0.737019
[epoch8, step3036]: loss 0.591080
[epoch8, step3037]: loss 0.496008
[epoch8, step3038]: loss 0.891143
[epoch8, step3039]: loss 0.458969
[epoch8, step3040]: loss 0.564764
[epoch8, step3041]: loss 0.808663
[epoch8, step3042]: loss 0.408978
[epoch8, step3043]: loss 0.582377
[epoch8, step3044]: loss 0.771545
[epoch8, step3045]: loss 0.684990
[epoch8, step3046]: loss 0.579988
[epoch8, step3047]: loss 0.679410
[epoch8, step3048]: loss 0.586499
[epoch8, step3049]: loss 0.777116
[epoch8, step3050]: loss 0.489469
[epoch8, step3051]: loss 0.628318
[epoch8, step3052]: loss 0.637143
[epoch8, step3053]: loss 0.847943
[epoch8, step3054]: loss 0.459425
[epoch8, step3055]: loss 0.700524
[epoch8, step3056]: loss 0.534534
[epoch8, step3057]: loss 0.629171
[epoch8, step3058]: loss 0.819477
[epoch8, step3059]: loss 0.689222
[epoch8, step3060]: loss 0.325009
[epoch8, step3061]: loss 0.672139
[epoch8, step3062]: loss 0.767436
[epoch8, step3063]: loss 0.350183
[epoch8, step3064]: loss 0.747751
[epoch8, step3065]: loss 0.742508
[epoch8, step3066]: loss 0.504018
[epoch8, step3067]: loss 0.494696
[epoch8, step3068]: loss 0.574552
[epoch8, step3069]: loss 0.294740
[epoch8, step3070]: loss 0.244959
[epoch8, step3071]: loss 0.627522
[epoch8, step3072]: loss 0.716031
[epoch8, step3073]: loss 0.564456
[epoch8, step3074]: loss 0.587233
[epoch8, step3075]: loss 0.677352
[epoch8, step3076]: loss 0.680495

[epoch8]: avg loss 0.680495

[epoch9, step1]: loss 0.628902
[epoch9, step2]: loss 0.419818
[epoch9, step3]: loss 0.736948
[epoch9, step4]: loss 0.674476
[epoch9, step5]: loss 0.709775
[epoch9, step6]: loss 0.299854
[epoch9, step7]: loss 0.762154
[epoch9, step8]: loss 0.760170
[epoch9, step9]: loss 0.783724
[epoch9, step10]: loss 0.485156
[epoch9, step11]: loss 0.852267
[epoch9, step12]: loss 0.527327
[epoch9, step13]: loss 0.689430
[epoch9, step14]: loss 0.609827
[epoch9, step15]: loss 0.725498
[epoch9, step16]: loss 0.429620
[epoch9, step17]: loss 0.750144
[epoch9, step18]: loss 0.758372
[epoch9, step19]: loss 0.533795
[epoch9, step20]: loss 0.470163
[epoch9, step21]: loss 0.829208
[epoch9, step22]: loss 0.628262
[epoch9, step23]: loss 0.675941
[epoch9, step24]: loss 0.481789
[epoch9, step25]: loss 0.641530
[epoch9, step26]: loss 0.569286
[epoch9, step27]: loss 0.348873
[epoch9, step28]: loss 0.722891
[epoch9, step29]: loss 0.637239
[epoch9, step30]: loss 0.590389
[epoch9, step31]: loss 0.383622
[epoch9, step32]: loss 0.547546
[epoch9, step33]: loss 0.496663
[epoch9, step34]: loss 0.718607
[epoch9, step35]: loss 0.582216
[epoch9, step36]: loss 0.704148
[epoch9, step37]: loss 0.514060
[epoch9, step38]: loss 0.602226
[epoch9, step39]: loss 0.738391
[epoch9, step40]: loss 0.344903
[epoch9, step41]: loss 0.529012
[epoch9, step42]: loss 0.483356
[epoch9, step43]: loss 0.725461
[epoch9, step44]: loss 0.791930
[epoch9, step45]: loss 0.428499
[epoch9, step46]: loss 0.494346
[epoch9, step47]: loss 0.486396
[epoch9, step48]: loss 0.635435
[epoch9, step49]: loss 0.613568
[epoch9, step50]: loss 0.769181
[epoch9, step51]: loss 0.599046
[epoch9, step52]: loss 0.388415
[epoch9, step53]: loss 0.761936
[epoch9, step54]: loss 0.774195
[epoch9, step55]: loss 0.610440
[epoch9, step56]: loss 0.830620
[epoch9, step57]: loss 0.638976
[epoch9, step58]: loss 0.327751
[epoch9, step59]: loss 0.463031
[epoch9, step60]: loss 0.547501
[epoch9, step61]: loss 0.700626
[epoch9, step62]: loss 0.863757
[epoch9, step63]: loss 0.656382
[epoch9, step64]: loss 0.552014
[epoch9, step65]: loss 0.549724
[epoch9, step66]: loss 0.810062
[epoch9, step67]: loss 0.729369
[epoch9, step68]: loss 0.854116
[epoch9, step69]: loss 0.652190
[epoch9, step70]: loss 0.470213
[epoch9, step71]: loss 0.756323
[epoch9, step72]: loss 0.763877
[epoch9, step73]: loss 0.412346
[epoch9, step74]: loss 0.713317
[epoch9, step75]: loss 0.601804
[epoch9, step76]: loss 0.781825
[epoch9, step77]: loss 0.658197
[epoch9, step78]: loss 0.444081
[epoch9, step79]: loss 0.707575
[epoch9, step80]: loss 0.764957
[epoch9, step81]: loss 0.522079
[epoch9, step82]: loss 0.495292
[epoch9, step83]: loss 0.839634
[epoch9, step84]: loss 0.595385
[epoch9, step85]: loss 0.818383
[epoch9, step86]: loss 0.703043
[epoch9, step87]: loss 0.454871
[epoch9, step88]: loss 0.879707
[epoch9, step89]: loss 0.516100
[epoch9, step90]: loss 0.500392
[epoch9, step91]: loss 0.665961
[epoch9, step92]: loss 0.548328
[epoch9, step93]: loss 0.718704
[epoch9, step94]: loss 0.537133
[epoch9, step95]: loss 0.311143
[epoch9, step96]: loss 0.747686
[epoch9, step97]: loss 0.447417
[epoch9, step98]: loss 0.742903
[epoch9, step99]: loss 0.685139
[epoch9, step100]: loss 0.662161
[epoch9, step101]: loss 0.609087
[epoch9, step102]: loss 0.641193
[epoch9, step103]: loss 0.767664
[epoch9, step104]: loss 0.611284
[epoch9, step105]: loss 0.596848
[epoch9, step106]: loss 0.701903
[epoch9, step107]: loss 0.571886
[epoch9, step108]: loss 0.594450
[epoch9, step109]: loss 0.735634
[epoch9, step110]: loss 0.768801
[epoch9, step111]: loss 0.590641
[epoch9, step112]: loss 0.428802
[epoch9, step113]: loss 0.671991
[epoch9, step114]: loss 0.559554
[epoch9, step115]: loss 0.514315
[epoch9, step116]: loss 0.575171
[epoch9, step117]: loss 0.483121
[epoch9, step118]: loss 0.526684
[epoch9, step119]: loss 0.594879
[epoch9, step120]: loss 0.733763
[epoch9, step121]: loss 0.377076
[epoch9, step122]: loss 0.861006
[epoch9, step123]: loss 0.650592
[epoch9, step124]: loss 0.444013
[epoch9, step125]: loss 0.678470
[epoch9, step126]: loss 0.587816
[epoch9, step127]: loss 0.756928
[epoch9, step128]: loss 0.418869
[epoch9, step129]: loss 0.340297
[epoch9, step130]: loss 0.528527
[epoch9, step131]: loss 0.352193
[epoch9, step132]: loss 0.517173
[epoch9, step133]: loss 0.713690
[epoch9, step134]: loss 0.691993
[epoch9, step135]: loss 0.720644
[epoch9, step136]: loss 0.447638
[epoch9, step137]: loss 0.702291
[epoch9, step138]: loss 0.523327
[epoch9, step139]: loss 0.523057
[epoch9, step140]: loss 0.713903
[epoch9, step141]: loss 0.253145
[epoch9, step142]: loss 0.331496
[epoch9, step143]: loss 0.652491
[epoch9, step144]: loss 0.664981
[epoch9, step145]: loss 0.396294
[epoch9, step146]: loss 0.716669
[epoch9, step147]: loss 0.452621
[epoch9, step148]: loss 0.622717
[epoch9, step149]: loss 0.752632
[epoch9, step150]: loss 0.480106
[epoch9, step151]: loss 0.587251
[epoch9, step152]: loss 1.048566
[epoch9, step153]: loss 0.683084
[epoch9, step154]: loss 0.586161
[epoch9, step155]: loss 0.611078
[epoch9, step156]: loss 0.657978
[epoch9, step157]: loss 0.715659
[epoch9, step158]: loss 0.685469
[epoch9, step159]: loss 0.483789
[epoch9, step160]: loss 0.416913
[epoch9, step161]: loss 0.647346
[epoch9, step162]: loss 0.680326
[epoch9, step163]: loss 0.477516
[epoch9, step164]: loss 0.700866
[epoch9, step165]: loss 0.730187
[epoch9, step166]: loss 0.518394
[epoch9, step167]: loss 0.509883
[epoch9, step168]: loss 0.633396
[epoch9, step169]: loss 0.769299
[epoch9, step170]: loss 0.395718
[epoch9, step171]: loss 0.676526
[epoch9, step172]: loss 0.786776
[epoch9, step173]: loss 0.817524
[epoch9, step174]: loss 0.469289
[epoch9, step175]: loss 0.738493
[epoch9, step176]: loss 0.591641
[epoch9, step177]: loss 0.588291
[epoch9, step178]: loss 0.692674
[epoch9, step179]: loss 0.383045
[epoch9, step180]: loss 0.532628
[epoch9, step181]: loss 0.692043
[epoch9, step182]: loss 0.700975
[epoch9, step183]: loss 0.761804
[epoch9, step184]: loss 0.364970
[epoch9, step185]: loss 0.675233
[epoch9, step186]: loss 0.548334
[epoch9, step187]: loss 0.772800
[epoch9, step188]: loss 0.390616
[epoch9, step189]: loss 0.672024
[epoch9, step190]: loss 0.655938
[epoch9, step191]: loss 0.756842
[epoch9, step192]: loss 0.564765
[epoch9, step193]: loss 0.321320
[epoch9, step194]: loss 0.556338
[epoch9, step195]: loss 0.787529
[epoch9, step196]: loss 0.468636
[epoch9, step197]: loss 0.346159
[epoch9, step198]: loss 0.609610
[epoch9, step199]: loss 0.443985
[epoch9, step200]: loss 0.700730
[epoch9, step201]: loss 0.408723
[epoch9, step202]: loss 0.575913
[epoch9, step203]: loss 0.616514
[epoch9, step204]: loss 0.564628
[epoch9, step205]: loss 0.737565
[epoch9, step206]: loss 0.728231
[epoch9, step207]: loss 0.599712
[epoch9, step208]: loss 0.312529
[epoch9, step209]: loss 0.507187
[epoch9, step210]: loss 0.703329
[epoch9, step211]: loss 0.603383
[epoch9, step212]: loss 0.581452
[epoch9, step213]: loss 0.780274
[epoch9, step214]: loss 0.647110
[epoch9, step215]: loss 0.905893
[epoch9, step216]: loss 0.695823
[epoch9, step217]: loss 0.389326
[epoch9, step218]: loss 0.757976
[epoch9, step219]: loss 0.550275
[epoch9, step220]: loss 0.299510
[epoch9, step221]: loss 0.672804
[epoch9, step222]: loss 0.528762
[epoch9, step223]: loss 0.492662
[epoch9, step224]: loss 0.674567
[epoch9, step225]: loss 0.533240
[epoch9, step226]: loss 0.741509
[epoch9, step227]: loss 0.522136
[epoch9, step228]: loss 0.519536
[epoch9, step229]: loss 0.430440
[epoch9, step230]: loss 0.692445
[epoch9, step231]: loss 0.580721
[epoch9, step232]: loss 0.506839
[epoch9, step233]: loss 0.541049
[epoch9, step234]: loss 0.619980
[epoch9, step235]: loss 0.623881
[epoch9, step236]: loss 0.460164
[epoch9, step237]: loss 0.413743
[epoch9, step238]: loss 0.758094
[epoch9, step239]: loss 0.662149
[epoch9, step240]: loss 0.800158
[epoch9, step241]: loss 0.657959
[epoch9, step242]: loss 0.649262
[epoch9, step243]: loss 0.609737
[epoch9, step244]: loss 0.638301
[epoch9, step245]: loss 0.598617
[epoch9, step246]: loss 0.620994
[epoch9, step247]: loss 0.677447
[epoch9, step248]: loss 0.661137
[epoch9, step249]: loss 0.515702
[epoch9, step250]: loss 0.498320
[epoch9, step251]: loss 0.525916
[epoch9, step252]: loss 0.678794
[epoch9, step253]: loss 0.769376
[epoch9, step254]: loss 0.511384
[epoch9, step255]: loss 0.797570
[epoch9, step256]: loss 0.561515
[epoch9, step257]: loss 0.588811
[epoch9, step258]: loss 0.672361
[epoch9, step259]: loss 0.692852
[epoch9, step260]: loss 0.679032
[epoch9, step261]: loss 0.727966
[epoch9, step262]: loss 0.678109
[epoch9, step263]: loss 0.539918
[epoch9, step264]: loss 0.242256
[epoch9, step265]: loss 0.588417
[epoch9, step266]: loss 0.754982
[epoch9, step267]: loss 0.469255
[epoch9, step268]: loss 0.483162
[epoch9, step269]: loss 0.616360
[epoch9, step270]: loss 0.635147
[epoch9, step271]: loss 0.752643
[epoch9, step272]: loss 0.588673
[epoch9, step273]: loss 0.518584
[epoch9, step274]: loss 0.475912
[epoch9, step275]: loss 0.714333
[epoch9, step276]: loss 0.699786
[epoch9, step277]: loss 0.246676
[epoch9, step278]: loss 0.868699
[epoch9, step279]: loss 0.664978
[epoch9, step280]: loss 0.717382
[epoch9, step281]: loss 0.539039
[epoch9, step282]: loss 0.572449
[epoch9, step283]: loss 0.540667
[epoch9, step284]: loss 0.286809
[epoch9, step285]: loss 0.357259
[epoch9, step286]: loss 0.715522
[epoch9, step287]: loss 0.589419
[epoch9, step288]: loss 0.682584
[epoch9, step289]: loss 0.361664
[epoch9, step290]: loss 0.666435
[epoch9, step291]: loss 0.513226
[epoch9, step292]: loss 0.597550
[epoch9, step293]: loss 0.349284
[epoch9, step294]: loss 0.444681
[epoch9, step295]: loss 0.769897
[epoch9, step296]: loss 0.628591
[epoch9, step297]: loss 0.701561
[epoch9, step298]: loss 0.209009
[epoch9, step299]: loss 0.688731
[epoch9, step300]: loss 0.536479
[epoch9, step301]: loss 0.851922
[epoch9, step302]: loss 0.686935
[epoch9, step303]: loss 0.556666
[epoch9, step304]: loss 0.464539
[epoch9, step305]: loss 0.429103
[epoch9, step306]: loss 0.282296
[epoch9, step307]: loss 0.308500
[epoch9, step308]: loss 0.542040
[epoch9, step309]: loss 0.632968
[epoch9, step310]: loss 0.335395
[epoch9, step311]: loss 0.423684
[epoch9, step312]: loss 0.574059
[epoch9, step313]: loss 0.705487
[epoch9, step314]: loss 0.527826
[epoch9, step315]: loss 0.651593
[epoch9, step316]: loss 0.579370
[epoch9, step317]: loss 0.827872
[epoch9, step318]: loss 0.415480
[epoch9, step319]: loss 0.654900
[epoch9, step320]: loss 0.555040
[epoch9, step321]: loss 0.645271
[epoch9, step322]: loss 0.487309
[epoch9, step323]: loss 0.526288
[epoch9, step324]: loss 0.491493
[epoch9, step325]: loss 0.738338
[epoch9, step326]: loss 0.574661
[epoch9, step327]: loss 0.607636
[epoch9, step328]: loss 0.608796
[epoch9, step329]: loss 0.638069
[epoch9, step330]: loss 0.619077
[epoch9, step331]: loss 0.607300
[epoch9, step332]: loss 0.770275
[epoch9, step333]: loss 0.701273
[epoch9, step334]: loss 0.786599
[epoch9, step335]: loss 0.456346
[epoch9, step336]: loss 0.752015
[epoch9, step337]: loss 0.421266
[epoch9, step338]: loss 0.538545
[epoch9, step339]: loss 0.410136
[epoch9, step340]: loss 0.640442
[epoch9, step341]: loss 0.506564
[epoch9, step342]: loss 0.685688
[epoch9, step343]: loss 0.599083
[epoch9, step344]: loss 0.612948
[epoch9, step345]: loss 0.568994
[epoch9, step346]: loss 0.702969
[epoch9, step347]: loss 0.407829
[epoch9, step348]: loss 0.792503
[epoch9, step349]: loss 0.612869
[epoch9, step350]: loss 0.728460
[epoch9, step351]: loss 0.464790
[epoch9, step352]: loss 0.615967
[epoch9, step353]: loss 0.489893
[epoch9, step354]: loss 0.588098
[epoch9, step355]: loss 0.506646
[epoch9, step356]: loss 0.318433
[epoch9, step357]: loss 0.627780
[epoch9, step358]: loss 0.700672
[epoch9, step359]: loss 0.387037
[epoch9, step360]: loss 0.595981
[epoch9, step361]: loss 0.648596
[epoch9, step362]: loss 0.509124
[epoch9, step363]: loss 0.526654
[epoch9, step364]: loss 0.825560
[epoch9, step365]: loss 0.737491
[epoch9, step366]: loss 0.510674
[epoch9, step367]: loss 0.485063
[epoch9, step368]: loss 0.690828
[epoch9, step369]: loss 0.531912
[epoch9, step370]: loss 0.299999
[epoch9, step371]: loss 0.692968
[epoch9, step372]: loss 0.782898
[epoch9, step373]: loss 0.536293
[epoch9, step374]: loss 0.516194
[epoch9, step375]: loss 0.617071
[epoch9, step376]: loss 0.633179
[epoch9, step377]: loss 0.692379
[epoch9, step378]: loss 0.560108
[epoch9, step379]: loss 0.881136
[epoch9, step380]: loss 0.487812
[epoch9, step381]: loss 0.273360
[epoch9, step382]: loss 0.593572
[epoch9, step383]: loss 0.589234
[epoch9, step384]: loss 0.461798
[epoch9, step385]: loss 0.348188
[epoch9, step386]: loss 0.760352
[epoch9, step387]: loss 0.815642
[epoch9, step388]: loss 0.538967
[epoch9, step389]: loss 0.608845
[epoch9, step390]: loss 0.664804
[epoch9, step391]: loss 0.608255
[epoch9, step392]: loss 0.514111
[epoch9, step393]: loss 0.677106
[epoch9, step394]: loss 0.512420
[epoch9, step395]: loss 0.790207
[epoch9, step396]: loss 0.500452
[epoch9, step397]: loss 0.555141
[epoch9, step398]: loss 0.726428
[epoch9, step399]: loss 0.651378
[epoch9, step400]: loss 0.604078
[epoch9, step401]: loss 0.597914
[epoch9, step402]: loss 0.707097
[epoch9, step403]: loss 0.870019
[epoch9, step404]: loss 0.595853
[epoch9, step405]: loss 0.473148
[epoch9, step406]: loss 0.806643
[epoch9, step407]: loss 0.629994
[epoch9, step408]: loss 0.837730
[epoch9, step409]: loss 0.782197
[epoch9, step410]: loss 0.670486
[epoch9, step411]: loss 0.765317
[epoch9, step412]: loss 0.184650
[epoch9, step413]: loss 0.650704
[epoch9, step414]: loss 0.907139
[epoch9, step415]: loss 0.621623
[epoch9, step416]: loss 0.713831
[epoch9, step417]: loss 0.820443
[epoch9, step418]: loss 0.457409
[epoch9, step419]: loss 0.412455
[epoch9, step420]: loss 0.632710
[epoch9, step421]: loss 0.282033
[epoch9, step422]: loss 0.686081
[epoch9, step423]: loss 0.426443
[epoch9, step424]: loss 0.615519
[epoch9, step425]: loss 0.559305
[epoch9, step426]: loss 0.558508
[epoch9, step427]: loss 0.654936
[epoch9, step428]: loss 0.473041
[epoch9, step429]: loss 0.737047
[epoch9, step430]: loss 0.737467
[epoch9, step431]: loss 0.587107
[epoch9, step432]: loss 0.647029
[epoch9, step433]: loss 0.716044
[epoch9, step434]: loss 0.662877
[epoch9, step435]: loss 0.417314
[epoch9, step436]: loss 0.775336
[epoch9, step437]: loss 0.654511
[epoch9, step438]: loss 0.599340
[epoch9, step439]: loss 0.541460
[epoch9, step440]: loss 0.534612
[epoch9, step441]: loss 0.899147
[epoch9, step442]: loss 0.543828
[epoch9, step443]: loss 0.847005
[epoch9, step444]: loss 0.518058
[epoch9, step445]: loss 0.386678
[epoch9, step446]: loss 0.755685
[epoch9, step447]: loss 0.638915
[epoch9, step448]: loss 0.598488
[epoch9, step449]: loss 0.608411
[epoch9, step450]: loss 0.703621
[epoch9, step451]: loss 0.644754
[epoch9, step452]: loss 0.582759
[epoch9, step453]: loss 0.502774
[epoch9, step454]: loss 0.712744
[epoch9, step455]: loss 0.688460
[epoch9, step456]: loss 0.761968
[epoch9, step457]: loss 0.542577
[epoch9, step458]: loss 0.547311
[epoch9, step459]: loss 0.644904
[epoch9, step460]: loss 0.554902
[epoch9, step461]: loss 0.492808
[epoch9, step462]: loss 0.655946
[epoch9, step463]: loss 0.709605
[epoch9, step464]: loss 0.331012
[epoch9, step465]: loss 0.466437
[epoch9, step466]: loss 0.570838
[epoch9, step467]: loss 0.734080
[epoch9, step468]: loss 0.557727
[epoch9, step469]: loss 0.424588
[epoch9, step470]: loss 0.527811
[epoch9, step471]: loss 0.505181
[epoch9, step472]: loss 0.691082
[epoch9, step473]: loss 0.637606
[epoch9, step474]: loss 0.551189
[epoch9, step475]: loss 0.736004
[epoch9, step476]: loss 0.827088
[epoch9, step477]: loss 0.528009
[epoch9, step478]: loss 0.595764
[epoch9, step479]: loss 0.576024
[epoch9, step480]: loss 0.786476
[epoch9, step481]: loss 0.668461
[epoch9, step482]: loss 0.770782
[epoch9, step483]: loss 0.898115
[epoch9, step484]: loss 0.829378
[epoch9, step485]: loss 0.568237
[epoch9, step486]: loss 0.483492
[epoch9, step487]: loss 0.823324
[epoch9, step488]: loss 0.696223
[epoch9, step489]: loss 0.772181
[epoch9, step490]: loss 0.495389
[epoch9, step491]: loss 0.525590
[epoch9, step492]: loss 0.648098
[epoch9, step493]: loss 0.763981
[epoch9, step494]: loss 0.284334
[epoch9, step495]: loss 0.710080
[epoch9, step496]: loss 0.391813
[epoch9, step497]: loss 0.534927
[epoch9, step498]: loss 0.722684
[epoch9, step499]: loss 0.426535
[epoch9, step500]: loss 0.278350
[epoch9, step501]: loss 0.465632
[epoch9, step502]: loss 0.717625
[epoch9, step503]: loss 0.347278
[epoch9, step504]: loss 0.279564
[epoch9, step505]: loss 0.472020
[epoch9, step506]: loss 0.509581
[epoch9, step507]: loss 0.460043
[epoch9, step508]: loss 0.518534
[epoch9, step509]: loss 0.469505
[epoch9, step510]: loss 0.435969
[epoch9, step511]: loss 0.586084
[epoch9, step512]: loss 0.663348
[epoch9, step513]: loss 0.552150
[epoch9, step514]: loss 0.733426
[epoch9, step515]: loss 0.462780
[epoch9, step516]: loss 0.676347
[epoch9, step517]: loss 0.643581
[epoch9, step518]: loss 0.523851
[epoch9, step519]: loss 0.323488
[epoch9, step520]: loss 0.755955
[epoch9, step521]: loss 0.721642
[epoch9, step522]: loss 0.560397
[epoch9, step523]: loss 0.502618
[epoch9, step524]: loss 0.404973
[epoch9, step525]: loss 0.608882
[epoch9, step526]: loss 0.504774
[epoch9, step527]: loss 0.705011
[epoch9, step528]: loss 0.602102
[epoch9, step529]: loss 0.664116
[epoch9, step530]: loss 0.783641
[epoch9, step531]: loss 0.592699
[epoch9, step532]: loss 0.629175
[epoch9, step533]: loss 0.465386
[epoch9, step534]: loss 0.517961
[epoch9, step535]: loss 0.616321
[epoch9, step536]: loss 0.518724
[epoch9, step537]: loss 0.353774
[epoch9, step538]: loss 0.414041
[epoch9, step539]: loss 0.713922
[epoch9, step540]: loss 0.476500
[epoch9, step541]: loss 0.540842
[epoch9, step542]: loss 0.461972
[epoch9, step543]: loss 0.703998
[epoch9, step544]: loss 0.600782
[epoch9, step545]: loss 0.662566
[epoch9, step546]: loss 0.452121
[epoch9, step547]: loss 0.254482
[epoch9, step548]: loss 0.760097
[epoch9, step549]: loss 0.747632
[epoch9, step550]: loss 0.765524
[epoch9, step551]: loss 0.408954
[epoch9, step552]: loss 0.444970
[epoch9, step553]: loss 0.677496
[epoch9, step554]: loss 0.718476
[epoch9, step555]: loss 0.617261
[epoch9, step556]: loss 0.582087
[epoch9, step557]: loss 0.568493
[epoch9, step558]: loss 0.587258
[epoch9, step559]: loss 0.440890
[epoch9, step560]: loss 0.703688
[epoch9, step561]: loss 0.626832
[epoch9, step562]: loss 0.775555
[epoch9, step563]: loss 0.320590
[epoch9, step564]: loss 0.428251
[epoch9, step565]: loss 0.665420
[epoch9, step566]: loss 0.247344
[epoch9, step567]: loss 0.483438
[epoch9, step568]: loss 0.674918
[epoch9, step569]: loss 0.581178
[epoch9, step570]: loss 0.521802
[epoch9, step571]: loss 0.664592
[epoch9, step572]: loss 0.602881
[epoch9, step573]: loss 0.648157
[epoch9, step574]: loss 0.680759
[epoch9, step575]: loss 0.518510
[epoch9, step576]: loss 0.652695
[epoch9, step577]: loss 0.456867
[epoch9, step578]: loss 0.545498
[epoch9, step579]: loss 0.578828
[epoch9, step580]: loss 0.656394
[epoch9, step581]: loss 0.728109
[epoch9, step582]: loss 0.471671
[epoch9, step583]: loss 0.352371
[epoch9, step584]: loss 0.740326
[epoch9, step585]: loss 0.736925
[epoch9, step586]: loss 0.421316
[epoch9, step587]: loss 0.840651
[epoch9, step588]: loss 0.635172
[epoch9, step589]: loss 0.675005
[epoch9, step590]: loss 0.584107
[epoch9, step591]: loss 0.919925
[epoch9, step592]: loss 0.854973
[epoch9, step593]: loss 0.812967
[epoch9, step594]: loss 0.341217
[epoch9, step595]: loss 0.849525
[epoch9, step596]: loss 0.964197
[epoch9, step597]: loss 0.538230
[epoch9, step598]: loss 0.597896
[epoch9, step599]: loss 0.498355
[epoch9, step600]: loss 0.568817
[epoch9, step601]: loss 0.534885
[epoch9, step602]: loss 0.717496
[epoch9, step603]: loss 0.595552
[epoch9, step604]: loss 0.598459
[epoch9, step605]: loss 0.496337
[epoch9, step606]: loss 0.652275
[epoch9, step607]: loss 0.581523
[epoch9, step608]: loss 0.802291
[epoch9, step609]: loss 0.508914
[epoch9, step610]: loss 0.744839
[epoch9, step611]: loss 0.713975
[epoch9, step612]: loss 0.498421
[epoch9, step613]: loss 0.725757
[epoch9, step614]: loss 0.720231
[epoch9, step615]: loss 0.534327
[epoch9, step616]: loss 0.633338
[epoch9, step617]: loss 0.250118
[epoch9, step618]: loss 0.572615
[epoch9, step619]: loss 0.388578
[epoch9, step620]: loss 0.436292
[epoch9, step621]: loss 0.491548
[epoch9, step622]: loss 0.531528
[epoch9, step623]: loss 0.758443
[epoch9, step624]: loss 0.535000
[epoch9, step625]: loss 0.438274
[epoch9, step626]: loss 0.746136
[epoch9, step627]: loss 0.726758
[epoch9, step628]: loss 0.583912
[epoch9, step629]: loss 0.834355
[epoch9, step630]: loss 0.697113
[epoch9, step631]: loss 0.546805
[epoch9, step632]: loss 0.516351
[epoch9, step633]: loss 0.666596
[epoch9, step634]: loss 0.550180
[epoch9, step635]: loss 0.677168
[epoch9, step636]: loss 0.646822
[epoch9, step637]: loss 0.636491
[epoch9, step638]: loss 0.666012
[epoch9, step639]: loss 0.724478
[epoch9, step640]: loss 0.574233
[epoch9, step641]: loss 0.715477
[epoch9, step642]: loss 0.494948
[epoch9, step643]: loss 0.462901
[epoch9, step644]: loss 0.684999
[epoch9, step645]: loss 0.681855
[epoch9, step646]: loss 0.387371
[epoch9, step647]: loss 0.703677
[epoch9, step648]: loss 0.649174
[epoch9, step649]: loss 0.585665
[epoch9, step650]: loss 0.726313
[epoch9, step651]: loss 0.654056
[epoch9, step652]: loss 0.647533
[epoch9, step653]: loss 0.815021
[epoch9, step654]: loss 0.680273
[epoch9, step655]: loss 0.576644
[epoch9, step656]: loss 0.595215
[epoch9, step657]: loss 0.926633
[epoch9, step658]: loss 0.738680
[epoch9, step659]: loss 0.541877
[epoch9, step660]: loss 0.513797
[epoch9, step661]: loss 0.881821
[epoch9, step662]: loss 0.467574
[epoch9, step663]: loss 0.441579
[epoch9, step664]: loss 0.791732
[epoch9, step665]: loss 0.657606
[epoch9, step666]: loss 0.814779
[epoch9, step667]: loss 0.540064
[epoch9, step668]: loss 0.309907
[epoch9, step669]: loss 0.505804
[epoch9, step670]: loss 0.714663
[epoch9, step671]: loss 0.597385
[epoch9, step672]: loss 0.208610
[epoch9, step673]: loss 0.702618
[epoch9, step674]: loss 0.568262
[epoch9, step675]: loss 0.482328
[epoch9, step676]: loss 0.288845
[epoch9, step677]: loss 0.440728
[epoch9, step678]: loss 0.598240
[epoch9, step679]: loss 0.581630
[epoch9, step680]: loss 0.459822
[epoch9, step681]: loss 0.534730
[epoch9, step682]: loss 0.744172
[epoch9, step683]: loss 0.514617
[epoch9, step684]: loss 0.622114
[epoch9, step685]: loss 0.434076
[epoch9, step686]: loss 0.606392
[epoch9, step687]: loss 0.493845
[epoch9, step688]: loss 0.637468
[epoch9, step689]: loss 0.618330
[epoch9, step690]: loss 0.641557
[epoch9, step691]: loss 0.411626
[epoch9, step692]: loss 0.630476
[epoch9, step693]: loss 0.859145
[epoch9, step694]: loss 0.642831
[epoch9, step695]: loss 0.617552
[epoch9, step696]: loss 0.494613
[epoch9, step697]: loss 0.462745
[epoch9, step698]: loss 0.749553
[epoch9, step699]: loss 0.503819
[epoch9, step700]: loss 0.717795
[epoch9, step701]: loss 0.874084
[epoch9, step702]: loss 0.592186
[epoch9, step703]: loss 0.378913
[epoch9, step704]: loss 0.503133
[epoch9, step705]: loss 0.621751
[epoch9, step706]: loss 0.598453
[epoch9, step707]: loss 0.832092
[epoch9, step708]: loss 0.561663
[epoch9, step709]: loss 0.840527
[epoch9, step710]: loss 0.704308
[epoch9, step711]: loss 0.386311
[epoch9, step712]: loss 0.626186
[epoch9, step713]: loss 0.679867
[epoch9, step714]: loss 0.800997
[epoch9, step715]: loss 0.820246
[epoch9, step716]: loss 0.522665
[epoch9, step717]: loss 0.583979
[epoch9, step718]: loss 0.670969
[epoch9, step719]: loss 0.637108
[epoch9, step720]: loss 0.524862
[epoch9, step721]: loss 0.589015
[epoch9, step722]: loss 0.604627
[epoch9, step723]: loss 0.371361
[epoch9, step724]: loss 0.966075
[epoch9, step725]: loss 0.521650
[epoch9, step726]: loss 0.615509
[epoch9, step727]: loss 0.466041
[epoch9, step728]: loss 0.340646
[epoch9, step729]: loss 0.580021
[epoch9, step730]: loss 0.584432
[epoch9, step731]: loss 0.351903
[epoch9, step732]: loss 0.369321
[epoch9, step733]: loss 0.727166
[epoch9, step734]: loss 0.767324
[epoch9, step735]: loss 0.535770
[epoch9, step736]: loss 0.539720
[epoch9, step737]: loss 0.646088
[epoch9, step738]: loss 0.549909
[epoch9, step739]: loss 0.903260
[epoch9, step740]: loss 0.699113
[epoch9, step741]: loss 0.701544
[epoch9, step742]: loss 0.532294
[epoch9, step743]: loss 0.467246
[epoch9, step744]: loss 0.507005
[epoch9, step745]: loss 0.301184
[epoch9, step746]: loss 0.687511
[epoch9, step747]: loss 0.555018
[epoch9, step748]: loss 0.597977
[epoch9, step749]: loss 0.518890
[epoch9, step750]: loss 0.769864
[epoch9, step751]: loss 0.692095
[epoch9, step752]: loss 0.740212
[epoch9, step753]: loss 0.713848
[epoch9, step754]: loss 0.710538
[epoch9, step755]: loss 0.574749
[epoch9, step756]: loss 0.898580
[epoch9, step757]: loss 0.542926
[epoch9, step758]: loss 0.435018
[epoch9, step759]: loss 0.577204
[epoch9, step760]: loss 0.447903
[epoch9, step761]: loss 0.481075
[epoch9, step762]: loss 0.590911
[epoch9, step763]: loss 0.776107
[epoch9, step764]: loss 0.825434
[epoch9, step765]: loss 0.784294
[epoch9, step766]: loss 0.242542
[epoch9, step767]: loss 0.676929
[epoch9, step768]: loss 0.347081
[epoch9, step769]: loss 0.720476
[epoch9, step770]: loss 0.479207
[epoch9, step771]: loss 0.475181
[epoch9, step772]: loss 0.622997
[epoch9, step773]: loss 0.591387
[epoch9, step774]: loss 0.826867
[epoch9, step775]: loss 0.636770
[epoch9, step776]: loss 0.746649
[epoch9, step777]: loss 0.321874
[epoch9, step778]: loss 0.425189
[epoch9, step779]: loss 0.942309
[epoch9, step780]: loss 0.687170
[epoch9, step781]: loss 0.725600
[epoch9, step782]: loss 0.701508
[epoch9, step783]: loss 0.642805
[epoch9, step784]: loss 0.667331
[epoch9, step785]: loss 0.856216
[epoch9, step786]: loss 0.508942
[epoch9, step787]: loss 0.597282
[epoch9, step788]: loss 0.962130
[epoch9, step789]: loss 0.576722
[epoch9, step790]: loss 0.851407
[epoch9, step791]: loss 0.447251
[epoch9, step792]: loss 0.605620
[epoch9, step793]: loss 0.855716
[epoch9, step794]: loss 0.452267
[epoch9, step795]: loss 0.617217
[epoch9, step796]: loss 0.477861
[epoch9, step797]: loss 0.457617
[epoch9, step798]: loss 0.399435
[epoch9, step799]: loss 0.582248
[epoch9, step800]: loss 0.616756
[epoch9, step801]: loss 0.316318
[epoch9, step802]: loss 0.679304
[epoch9, step803]: loss 0.777829
[epoch9, step804]: loss 0.488025
[epoch9, step805]: loss 0.548839
[epoch9, step806]: loss 0.629065
[epoch9, step807]: loss 0.719034
[epoch9, step808]: loss 0.855123
[epoch9, step809]: loss 0.534261
[epoch9, step810]: loss 0.693123
[epoch9, step811]: loss 0.684396
[epoch9, step812]: loss 0.532779
[epoch9, step813]: loss 0.583458
[epoch9, step814]: loss 0.690863
[epoch9, step815]: loss 0.388353
[epoch9, step816]: loss 0.698076
[epoch9, step817]: loss 0.630254
[epoch9, step818]: loss 0.609895
[epoch9, step819]: loss 0.831935
[epoch9, step820]: loss 0.482156
[epoch9, step821]: loss 0.560176
[epoch9, step822]: loss 0.714284
[epoch9, step823]: loss 0.650618
[epoch9, step824]: loss 0.755591
[epoch9, step825]: loss 0.453334
[epoch9, step826]: loss 0.551084
[epoch9, step827]: loss 0.593432
[epoch9, step828]: loss 0.657102
[epoch9, step829]: loss 0.639252
[epoch9, step830]: loss 0.477276
[epoch9, step831]: loss 0.477093
[epoch9, step832]: loss 0.661381
[epoch9, step833]: loss 0.691876
[epoch9, step834]: loss 0.567238
[epoch9, step835]: loss 0.375361
[epoch9, step836]: loss 0.889417
[epoch9, step837]: loss 0.536703
[epoch9, step838]: loss 0.702282
[epoch9, step839]: loss 0.548013
[epoch9, step840]: loss 0.808818
[epoch9, step841]: loss 0.631388
[epoch9, step842]: loss 0.592658
[epoch9, step843]: loss 0.632661
[epoch9, step844]: loss 0.766732
[epoch9, step845]: loss 0.600077
[epoch9, step846]: loss 0.691699
[epoch9, step847]: loss 0.304166
[epoch9, step848]: loss 0.457536
[epoch9, step849]: loss 0.440336
[epoch9, step850]: loss 0.522995
[epoch9, step851]: loss 0.403854
[epoch9, step852]: loss 0.599611
[epoch9, step853]: loss 0.480506
[epoch9, step854]: loss 0.538685
[epoch9, step855]: loss 0.608885
[epoch9, step856]: loss 0.762771
[epoch9, step857]: loss 0.415744
[epoch9, step858]: loss 0.548353
[epoch9, step859]: loss 0.501750
[epoch9, step860]: loss 0.668814
[epoch9, step861]: loss 0.852033
[epoch9, step862]: loss 0.750301
[epoch9, step863]: loss 0.781500
[epoch9, step864]: loss 0.759463
[epoch9, step865]: loss 0.526119
[epoch9, step866]: loss 0.212825
[epoch9, step867]: loss 0.617708
[epoch9, step868]: loss 0.601851
[epoch9, step869]: loss 0.555068
[epoch9, step870]: loss 0.674241
[epoch9, step871]: loss 0.786928
[epoch9, step872]: loss 0.700675
[epoch9, step873]: loss 0.452283
[epoch9, step874]: loss 0.615029
[epoch9, step875]: loss 0.614004
[epoch9, step876]: loss 0.647457
[epoch9, step877]: loss 0.587541
[epoch9, step878]: loss 0.697961
[epoch9, step879]: loss 0.571935
[epoch9, step880]: loss 0.613506
[epoch9, step881]: loss 0.746131
[epoch9, step882]: loss 0.603492
[epoch9, step883]: loss 0.726775
[epoch9, step884]: loss 0.546938
[epoch9, step885]: loss 0.705060
[epoch9, step886]: loss 0.585721
[epoch9, step887]: loss 0.593941
[epoch9, step888]: loss 0.542665
[epoch9, step889]: loss 0.758135
[epoch9, step890]: loss 0.509607
[epoch9, step891]: loss 0.700448
[epoch9, step892]: loss 0.696739
[epoch9, step893]: loss 0.723411
[epoch9, step894]: loss 1.022833
[epoch9, step895]: loss 0.410234
[epoch9, step896]: loss 0.574116
[epoch9, step897]: loss 0.585452
[epoch9, step898]: loss 0.545106
[epoch9, step899]: loss 0.543297
[epoch9, step900]: loss 0.710156
[epoch9, step901]: loss 0.767998
[epoch9, step902]: loss 0.463182
[epoch9, step903]: loss 0.658175
[epoch9, step904]: loss 0.447850
[epoch9, step905]: loss 0.385636
[epoch9, step906]: loss 0.760278
[epoch9, step907]: loss 0.636542
[epoch9, step908]: loss 0.550986
[epoch9, step909]: loss 0.627693
[epoch9, step910]: loss 0.553154
[epoch9, step911]: loss 0.421237
[epoch9, step912]: loss 0.577497
[epoch9, step913]: loss 0.481058
[epoch9, step914]: loss 0.529940
[epoch9, step915]: loss 0.618395
[epoch9, step916]: loss 0.637495
[epoch9, step917]: loss 0.818746
[epoch9, step918]: loss 0.339939
[epoch9, step919]: loss 0.667509
[epoch9, step920]: loss 0.499563
[epoch9, step921]: loss 0.573364
[epoch9, step922]: loss 0.791865
[epoch9, step923]: loss 0.579505
[epoch9, step924]: loss 0.597039
[epoch9, step925]: loss 0.610631
[epoch9, step926]: loss 0.643883
[epoch9, step927]: loss 0.531009
[epoch9, step928]: loss 0.487516
[epoch9, step929]: loss 0.619815
[epoch9, step930]: loss 0.437507
[epoch9, step931]: loss 0.713995
[epoch9, step932]: loss 0.660093
[epoch9, step933]: loss 0.607087
[epoch9, step934]: loss 0.542626
[epoch9, step935]: loss 0.554353
[epoch9, step936]: loss 0.655092
[epoch9, step937]: loss 0.268592
[epoch9, step938]: loss 0.866812
[epoch9, step939]: loss 0.670620
[epoch9, step940]: loss 0.726921
[epoch9, step941]: loss 0.616888
[epoch9, step942]: loss 0.322460
[epoch9, step943]: loss 0.813997
[epoch9, step944]: loss 0.725974
[epoch9, step945]: loss 0.649722
[epoch9, step946]: loss 0.442147
[epoch9, step947]: loss 0.705046
[epoch9, step948]: loss 0.656270
[epoch9, step949]: loss 0.844473
[epoch9, step950]: loss 0.648357
[epoch9, step951]: loss 0.796600
[epoch9, step952]: loss 0.661279
[epoch9, step953]: loss 0.581264
[epoch9, step954]: loss 0.467551
[epoch9, step955]: loss 0.546244
[epoch9, step956]: loss 0.452431
[epoch9, step957]: loss 0.544835
[epoch9, step958]: loss 0.712881
[epoch9, step959]: loss 0.464616
[epoch9, step960]: loss 0.783988
[epoch9, step961]: loss 0.793411
[epoch9, step962]: loss 0.804680
[epoch9, step963]: loss 0.617509
[epoch9, step964]: loss 0.724506
[epoch9, step965]: loss 0.599279
[epoch9, step966]: loss 0.391687
[epoch9, step967]: loss 0.457708
[epoch9, step968]: loss 0.644566
[epoch9, step969]: loss 0.733138
[epoch9, step970]: loss 0.684937
[epoch9, step971]: loss 0.578995
[epoch9, step972]: loss 0.715920
[epoch9, step973]: loss 0.607175
[epoch9, step974]: loss 0.585387
[epoch9, step975]: loss 0.518619
[epoch9, step976]: loss 0.600559
[epoch9, step977]: loss 0.415070
[epoch9, step978]: loss 0.478192
[epoch9, step979]: loss 0.515536
[epoch9, step980]: loss 0.550214
[epoch9, step981]: loss 0.507717
[epoch9, step982]: loss 0.366212
[epoch9, step983]: loss 0.601163
[epoch9, step984]: loss 0.694653
[epoch9, step985]: loss 0.477856
[epoch9, step986]: loss 0.519118
[epoch9, step987]: loss 0.775019
[epoch9, step988]: loss 0.526705
[epoch9, step989]: loss 0.480263
[epoch9, step990]: loss 0.591912
[epoch9, step991]: loss 0.518532
[epoch9, step992]: loss 0.539902
[epoch9, step993]: loss 0.592878
[epoch9, step994]: loss 0.590909
[epoch9, step995]: loss 0.493879
[epoch9, step996]: loss 0.543005
[epoch9, step997]: loss 0.503783
[epoch9, step998]: loss 0.396149
[epoch9, step999]: loss 0.611195
[epoch9, step1000]: loss 0.391277
[epoch9, step1001]: loss 0.645636
[epoch9, step1002]: loss 0.592807
[epoch9, step1003]: loss 0.628654
[epoch9, step1004]: loss 0.414788
[epoch9, step1005]: loss 0.508127
[epoch9, step1006]: loss 0.522462
[epoch9, step1007]: loss 0.640280
[epoch9, step1008]: loss 0.584339
[epoch9, step1009]: loss 0.117578
[epoch9, step1010]: loss 0.715359
[epoch9, step1011]: loss 0.409747
[epoch9, step1012]: loss 0.633836
[epoch9, step1013]: loss 0.453569
[epoch9, step1014]: loss 0.449055
[epoch9, step1015]: loss 0.373766
[epoch9, step1016]: loss 0.564930
[epoch9, step1017]: loss 0.937203
[epoch9, step1018]: loss 0.777350
[epoch9, step1019]: loss 0.718993
[epoch9, step1020]: loss 0.366222
[epoch9, step1021]: loss 0.620863
[epoch9, step1022]: loss 0.615846
[epoch9, step1023]: loss 0.590557
[epoch9, step1024]: loss 0.524048
[epoch9, step1025]: loss 0.567548
[epoch9, step1026]: loss 0.567481
[epoch9, step1027]: loss 0.650603
[epoch9, step1028]: loss 0.631699
[epoch9, step1029]: loss 0.379601
[epoch9, step1030]: loss 0.468397
[epoch9, step1031]: loss 0.571134
[epoch9, step1032]: loss 0.591337
[epoch9, step1033]: loss 0.385133
[epoch9, step1034]: loss 0.540231
[epoch9, step1035]: loss 0.456328
[epoch9, step1036]: loss 0.427292
[epoch9, step1037]: loss 0.739223
[epoch9, step1038]: loss 0.297931
[epoch9, step1039]: loss 0.500395
[epoch9, step1040]: loss 0.656737
[epoch9, step1041]: loss 0.712856
[epoch9, step1042]: loss 0.415550
[epoch9, step1043]: loss 0.778863
[epoch9, step1044]: loss 0.767066
[epoch9, step1045]: loss 0.804959
[epoch9, step1046]: loss 0.692429
[epoch9, step1047]: loss 0.904979
[epoch9, step1048]: loss 0.839198
[epoch9, step1049]: loss 0.471576
[epoch9, step1050]: loss 0.418699
[epoch9, step1051]: loss 0.649336
[epoch9, step1052]: loss 0.618340
[epoch9, step1053]: loss 0.511661
[epoch9, step1054]: loss 0.645899
[epoch9, step1055]: loss 0.656583
[epoch9, step1056]: loss 0.743130
[epoch9, step1057]: loss 0.712875
[epoch9, step1058]: loss 0.845168
[epoch9, step1059]: loss 0.500186
[epoch9, step1060]: loss 0.542401
[epoch9, step1061]: loss 0.760950
[epoch9, step1062]: loss 0.753031
[epoch9, step1063]: loss 0.628442
[epoch9, step1064]: loss 0.656254
[epoch9, step1065]: loss 0.658454
[epoch9, step1066]: loss 0.655572
[epoch9, step1067]: loss 0.556449
[epoch9, step1068]: loss 0.493627
[epoch9, step1069]: loss 0.640493
[epoch9, step1070]: loss 0.902381
[epoch9, step1071]: loss 0.430845
[epoch9, step1072]: loss 0.654501
[epoch9, step1073]: loss 0.382834
[epoch9, step1074]: loss 0.403073
[epoch9, step1075]: loss 0.914429
[epoch9, step1076]: loss 0.676064
[epoch9, step1077]: loss 0.824719
[epoch9, step1078]: loss 0.674749
[epoch9, step1079]: loss 0.751108
[epoch9, step1080]: loss 0.579587
[epoch9, step1081]: loss 0.610063
[epoch9, step1082]: loss 0.617881
[epoch9, step1083]: loss 0.675164
[epoch9, step1084]: loss 0.586697
[epoch9, step1085]: loss 0.403104
[epoch9, step1086]: loss 0.179092
[epoch9, step1087]: loss 0.600683
[epoch9, step1088]: loss 0.656668
[epoch9, step1089]: loss 0.914002
[epoch9, step1090]: loss 0.822548
[epoch9, step1091]: loss 0.727659
[epoch9, step1092]: loss 0.584014
[epoch9, step1093]: loss 0.306051
[epoch9, step1094]: loss 0.515508
[epoch9, step1095]: loss 0.546601
[epoch9, step1096]: loss 0.652148
[epoch9, step1097]: loss 0.482217
[epoch9, step1098]: loss 0.376679
[epoch9, step1099]: loss 0.580021
[epoch9, step1100]: loss 0.691494
[epoch9, step1101]: loss 0.491154
[epoch9, step1102]: loss 0.538152
[epoch9, step1103]: loss 0.662992
[epoch9, step1104]: loss 0.425478
[epoch9, step1105]: loss 0.783024
[epoch9, step1106]: loss 0.835141
[epoch9, step1107]: loss 0.585052
[epoch9, step1108]: loss 0.695109
[epoch9, step1109]: loss 0.793157
[epoch9, step1110]: loss 0.517877
[epoch9, step1111]: loss 0.678135
[epoch9, step1112]: loss 0.660909
[epoch9, step1113]: loss 0.542868
[epoch9, step1114]: loss 0.387370
[epoch9, step1115]: loss 0.575098
[epoch9, step1116]: loss 0.424469
[epoch9, step1117]: loss 0.764276
[epoch9, step1118]: loss 0.700239
[epoch9, step1119]: loss 0.431989
[epoch9, step1120]: loss 0.455807
[epoch9, step1121]: loss 0.471670
[epoch9, step1122]: loss 0.324644
[epoch9, step1123]: loss 0.804949
[epoch9, step1124]: loss 0.819123
[epoch9, step1125]: loss 0.905898
[epoch9, step1126]: loss 0.506698
[epoch9, step1127]: loss 0.830980
[epoch9, step1128]: loss 0.804593
[epoch9, step1129]: loss 0.714086
[epoch9, step1130]: loss 0.655497
[epoch9, step1131]: loss 0.732135
[epoch9, step1132]: loss 0.441538
[epoch9, step1133]: loss 0.666544
[epoch9, step1134]: loss 0.627661
[epoch9, step1135]: loss 0.682364
[epoch9, step1136]: loss 0.529471
[epoch9, step1137]: loss 0.489650
[epoch9, step1138]: loss 0.595690
[epoch9, step1139]: loss 0.745153
[epoch9, step1140]: loss 0.419919
[epoch9, step1141]: loss 0.443603
[epoch9, step1142]: loss 0.812120
[epoch9, step1143]: loss 0.499265
[epoch9, step1144]: loss 0.440936
[epoch9, step1145]: loss 0.575037
[epoch9, step1146]: loss 0.676326
[epoch9, step1147]: loss 1.155830
[epoch9, step1148]: loss 0.314906
[epoch9, step1149]: loss 0.490825
[epoch9, step1150]: loss 0.624033
[epoch9, step1151]: loss 0.419462
[epoch9, step1152]: loss 0.453655
[epoch9, step1153]: loss 0.967404
[epoch9, step1154]: loss 0.744172
[epoch9, step1155]: loss 0.722839
[epoch9, step1156]: loss 0.671541
[epoch9, step1157]: loss 0.628202
[epoch9, step1158]: loss 0.527515
[epoch9, step1159]: loss 0.566083
[epoch9, step1160]: loss 0.483039
[epoch9, step1161]: loss 0.570341
[epoch9, step1162]: loss 0.450544
[epoch9, step1163]: loss 0.605553
[epoch9, step1164]: loss 0.695062
[epoch9, step1165]: loss 0.663260
[epoch9, step1166]: loss 0.701195
[epoch9, step1167]: loss 0.809177
[epoch9, step1168]: loss 0.762781
[epoch9, step1169]: loss 0.486824
[epoch9, step1170]: loss 0.668814
[epoch9, step1171]: loss 0.468197
[epoch9, step1172]: loss 0.482430
[epoch9, step1173]: loss 0.668259
[epoch9, step1174]: loss 0.358162
[epoch9, step1175]: loss 0.549731
[epoch9, step1176]: loss 0.678463
[epoch9, step1177]: loss 0.604311
[epoch9, step1178]: loss 0.600409
[epoch9, step1179]: loss 0.598500
[epoch9, step1180]: loss 0.569956
[epoch9, step1181]: loss 0.372262
[epoch9, step1182]: loss 0.455969
[epoch9, step1183]: loss 0.265357
[epoch9, step1184]: loss 0.844713
[epoch9, step1185]: loss 0.619127
[epoch9, step1186]: loss 0.543076
[epoch9, step1187]: loss 0.707117
[epoch9, step1188]: loss 0.742890
[epoch9, step1189]: loss 0.600217
[epoch9, step1190]: loss 0.433790
[epoch9, step1191]: loss 0.519432
[epoch9, step1192]: loss 0.689627
[epoch9, step1193]: loss 0.893839
[epoch9, step1194]: loss 0.399773
[epoch9, step1195]: loss 0.528424
[epoch9, step1196]: loss 0.823108
[epoch9, step1197]: loss 0.661833
[epoch9, step1198]: loss 0.709123
[epoch9, step1199]: loss 0.572067
[epoch9, step1200]: loss 0.794168
[epoch9, step1201]: loss 0.399628
[epoch9, step1202]: loss 0.705585
[epoch9, step1203]: loss 0.763228
[epoch9, step1204]: loss 0.677950
[epoch9, step1205]: loss 0.454943
[epoch9, step1206]: loss 0.529445
[epoch9, step1207]: loss 0.471328
[epoch9, step1208]: loss 0.267069
[epoch9, step1209]: loss 0.589599
[epoch9, step1210]: loss 0.570052
[epoch9, step1211]: loss 0.444152
[epoch9, step1212]: loss 0.742169
[epoch9, step1213]: loss 0.551773
[epoch9, step1214]: loss 0.681597
[epoch9, step1215]: loss 0.580005
[epoch9, step1216]: loss 0.693515
[epoch9, step1217]: loss 0.708357
[epoch9, step1218]: loss 0.570687
[epoch9, step1219]: loss 0.360441
[epoch9, step1220]: loss 0.801084
[epoch9, step1221]: loss 0.535754
[epoch9, step1222]: loss 0.303394
[epoch9, step1223]: loss 0.837640
[epoch9, step1224]: loss 0.644288
[epoch9, step1225]: loss 0.709534
[epoch9, step1226]: loss 0.571748
[epoch9, step1227]: loss 0.450418
[epoch9, step1228]: loss 0.661006
[epoch9, step1229]: loss 0.443601
[epoch9, step1230]: loss 0.486318
[epoch9, step1231]: loss 0.773731
[epoch9, step1232]: loss 0.470657
[epoch9, step1233]: loss 0.566271
[epoch9, step1234]: loss 0.765938
[epoch9, step1235]: loss 0.572129
[epoch9, step1236]: loss 0.698066
[epoch9, step1237]: loss 0.630912
[epoch9, step1238]: loss 0.792461
[epoch9, step1239]: loss 0.600422
[epoch9, step1240]: loss 0.636865
[epoch9, step1241]: loss 0.488980
[epoch9, step1242]: loss 0.561071
[epoch9, step1243]: loss 0.687027
[epoch9, step1244]: loss 0.373723
[epoch9, step1245]: loss 0.662755
[epoch9, step1246]: loss 0.848489
[epoch9, step1247]: loss 0.499183
[epoch9, step1248]: loss 0.541490
[epoch9, step1249]: loss 0.639628
[epoch9, step1250]: loss 0.607821
[epoch9, step1251]: loss 0.676202
[epoch9, step1252]: loss 0.568501
[epoch9, step1253]: loss 0.843837
[epoch9, step1254]: loss 0.705453
[epoch9, step1255]: loss 0.570545
[epoch9, step1256]: loss 0.734045
[epoch9, step1257]: loss 0.763351
[epoch9, step1258]: loss 0.835539
[epoch9, step1259]: loss 0.797941
[epoch9, step1260]: loss 0.600134
[epoch9, step1261]: loss 0.690189
[epoch9, step1262]: loss 0.797170
[epoch9, step1263]: loss 0.610251
[epoch9, step1264]: loss 0.827608
[epoch9, step1265]: loss 0.658515
[epoch9, step1266]: loss 0.678469
[epoch9, step1267]: loss 0.170062
[epoch9, step1268]: loss 0.485256
[epoch9, step1269]: loss 0.416982
[epoch9, step1270]: loss 0.660802
[epoch9, step1271]: loss 0.644578
[epoch9, step1272]: loss 0.615104
[epoch9, step1273]: loss 0.673640
[epoch9, step1274]: loss 0.776051
[epoch9, step1275]: loss 0.645126
[epoch9, step1276]: loss 0.748968
[epoch9, step1277]: loss 0.619414
[epoch9, step1278]: loss 0.787966
[epoch9, step1279]: loss 0.426935
[epoch9, step1280]: loss 0.733878
[epoch9, step1281]: loss 0.604116
[epoch9, step1282]: loss 0.615390
[epoch9, step1283]: loss 0.731259
[epoch9, step1284]: loss 0.555119
[epoch9, step1285]: loss 0.815816
[epoch9, step1286]: loss 0.652982
[epoch9, step1287]: loss 0.663757
[epoch9, step1288]: loss 0.678503
[epoch9, step1289]: loss 0.825525
[epoch9, step1290]: loss 0.439387
[epoch9, step1291]: loss 0.682172
[epoch9, step1292]: loss 0.578028
[epoch9, step1293]: loss 0.369626
[epoch9, step1294]: loss 0.721803
[epoch9, step1295]: loss 0.713041
[epoch9, step1296]: loss 0.768265
[epoch9, step1297]: loss 0.448160
[epoch9, step1298]: loss 0.462988
[epoch9, step1299]: loss 0.565959
[epoch9, step1300]: loss 0.686454
[epoch9, step1301]: loss 0.625824
[epoch9, step1302]: loss 0.776046
[epoch9, step1303]: loss 0.537277
[epoch9, step1304]: loss 0.598117
[epoch9, step1305]: loss 0.566837
[epoch9, step1306]: loss 0.577688
[epoch9, step1307]: loss 0.721679
[epoch9, step1308]: loss 0.647427
[epoch9, step1309]: loss 0.369921
[epoch9, step1310]: loss 0.507370
[epoch9, step1311]: loss 0.543566
[epoch9, step1312]: loss 0.644678
[epoch9, step1313]: loss 0.719131
[epoch9, step1314]: loss 0.751290
[epoch9, step1315]: loss 0.502721
[epoch9, step1316]: loss 0.705627
[epoch9, step1317]: loss 0.699346
[epoch9, step1318]: loss 0.691800
[epoch9, step1319]: loss 0.474129
[epoch9, step1320]: loss 0.492215
[epoch9, step1321]: loss 0.489503
[epoch9, step1322]: loss 0.518715
[epoch9, step1323]: loss 0.790809
[epoch9, step1324]: loss 0.711103
[epoch9, step1325]: loss 0.650362
[epoch9, step1326]: loss 0.820127
[epoch9, step1327]: loss 0.568224
[epoch9, step1328]: loss 0.564214
[epoch9, step1329]: loss 0.803649
[epoch9, step1330]: loss 0.703134
[epoch9, step1331]: loss 0.655442
[epoch9, step1332]: loss 0.727422
[epoch9, step1333]: loss 0.418237
[epoch9, step1334]: loss 0.419662
[epoch9, step1335]: loss 0.640518
[epoch9, step1336]: loss 0.322324
[epoch9, step1337]: loss 0.254404
[epoch9, step1338]: loss 0.531184
[epoch9, step1339]: loss 0.473932
[epoch9, step1340]: loss 0.723572
[epoch9, step1341]: loss 0.448549
[epoch9, step1342]: loss 0.903901
[epoch9, step1343]: loss 0.522232
[epoch9, step1344]: loss 0.341870
[epoch9, step1345]: loss 0.631170
[epoch9, step1346]: loss 0.647726
[epoch9, step1347]: loss 0.511322
[epoch9, step1348]: loss 0.723299
[epoch9, step1349]: loss 0.639619
[epoch9, step1350]: loss 0.457456
[epoch9, step1351]: loss 0.453951
[epoch9, step1352]: loss 0.664216
[epoch9, step1353]: loss 0.614963
[epoch9, step1354]: loss 0.612589
[epoch9, step1355]: loss 0.574034
[epoch9, step1356]: loss 0.578292
[epoch9, step1357]: loss 0.511997
[epoch9, step1358]: loss 0.582368
[epoch9, step1359]: loss 0.475143
[epoch9, step1360]: loss 0.587752
[epoch9, step1361]: loss 0.679826
[epoch9, step1362]: loss 0.393789
[epoch9, step1363]: loss 0.517253
[epoch9, step1364]: loss 0.601931
[epoch9, step1365]: loss 0.726587
[epoch9, step1366]: loss 0.362144
[epoch9, step1367]: loss 0.534272
[epoch9, step1368]: loss 0.763330
[epoch9, step1369]: loss 0.693350
[epoch9, step1370]: loss 0.430166
[epoch9, step1371]: loss 0.671550
[epoch9, step1372]: loss 0.350995
[epoch9, step1373]: loss 0.565630
[epoch9, step1374]: loss 0.501908
[epoch9, step1375]: loss 0.828825
[epoch9, step1376]: loss 0.600063
[epoch9, step1377]: loss 0.575921
[epoch9, step1378]: loss 0.617551
[epoch9, step1379]: loss 0.717485
[epoch9, step1380]: loss 0.751372
[epoch9, step1381]: loss 0.633653
[epoch9, step1382]: loss 0.694810
[epoch9, step1383]: loss 0.786294
[epoch9, step1384]: loss 0.585796
[epoch9, step1385]: loss 0.727459
[epoch9, step1386]: loss 0.733886
[epoch9, step1387]: loss 0.721249
[epoch9, step1388]: loss 0.573859
[epoch9, step1389]: loss 0.525964
[epoch9, step1390]: loss 0.523376
[epoch9, step1391]: loss 0.224463
[epoch9, step1392]: loss 0.663544
[epoch9, step1393]: loss 0.876497
[epoch9, step1394]: loss 0.526290
[epoch9, step1395]: loss 0.642268
[epoch9, step1396]: loss 0.624407
[epoch9, step1397]: loss 0.729740
[epoch9, step1398]: loss 0.594994
[epoch9, step1399]: loss 0.646748
[epoch9, step1400]: loss 0.673929
[epoch9, step1401]: loss 0.365650
[epoch9, step1402]: loss 0.577150
[epoch9, step1403]: loss 0.574353
[epoch9, step1404]: loss 0.477939
[epoch9, step1405]: loss 0.738981
[epoch9, step1406]: loss 0.411280
[epoch9, step1407]: loss 0.268333
[epoch9, step1408]: loss 0.622891
[epoch9, step1409]: loss 0.633109
[epoch9, step1410]: loss 0.656764
[epoch9, step1411]: loss 0.432316
[epoch9, step1412]: loss 0.601988
[epoch9, step1413]: loss 0.648756
[epoch9, step1414]: loss 0.444959
[epoch9, step1415]: loss 0.616314
[epoch9, step1416]: loss 0.737683
[epoch9, step1417]: loss 0.315123
[epoch9, step1418]: loss 0.364441
[epoch9, step1419]: loss 0.667942
[epoch9, step1420]: loss 0.728697
[epoch9, step1421]: loss 0.683832
[epoch9, step1422]: loss 0.735708
[epoch9, step1423]: loss 0.485057
[epoch9, step1424]: loss 0.615129
[epoch9, step1425]: loss 0.513905
[epoch9, step1426]: loss 0.533391
[epoch9, step1427]: loss 0.629856
[epoch9, step1428]: loss 0.422969
[epoch9, step1429]: loss 0.703132
[epoch9, step1430]: loss 0.519127
[epoch9, step1431]: loss 0.602766
[epoch9, step1432]: loss 0.664631
[epoch9, step1433]: loss 0.608207
[epoch9, step1434]: loss 0.707938
[epoch9, step1435]: loss 0.653495
[epoch9, step1436]: loss 0.631435
[epoch9, step1437]: loss 0.553613
[epoch9, step1438]: loss 0.719187
[epoch9, step1439]: loss 0.456674
[epoch9, step1440]: loss 0.609313
[epoch9, step1441]: loss 0.519774
[epoch9, step1442]: loss 0.487160
[epoch9, step1443]: loss 0.531286
[epoch9, step1444]: loss 0.658531
[epoch9, step1445]: loss 0.678286
[epoch9, step1446]: loss 0.783468
[epoch9, step1447]: loss 0.385366
[epoch9, step1448]: loss 0.790440
[epoch9, step1449]: loss 0.570466
[epoch9, step1450]: loss 0.680270
[epoch9, step1451]: loss 0.508533
[epoch9, step1452]: loss 0.483202
[epoch9, step1453]: loss 0.681560
[epoch9, step1454]: loss 0.575030
[epoch9, step1455]: loss 0.682469
[epoch9, step1456]: loss 0.457954
[epoch9, step1457]: loss 0.436717
[epoch9, step1458]: loss 0.183298
[epoch9, step1459]: loss 0.824541
[epoch9, step1460]: loss 0.476808
[epoch9, step1461]: loss 0.779332
[epoch9, step1462]: loss 0.272069
[epoch9, step1463]: loss 0.403573
[epoch9, step1464]: loss 0.591970
[epoch9, step1465]: loss 0.657393
[epoch9, step1466]: loss 0.744122
[epoch9, step1467]: loss 0.378354
[epoch9, step1468]: loss 0.795847
[epoch9, step1469]: loss 0.682990
[epoch9, step1470]: loss 0.592078
[epoch9, step1471]: loss 0.682616
[epoch9, step1472]: loss 0.661805
[epoch9, step1473]: loss 0.674524
[epoch9, step1474]: loss 0.601449
[epoch9, step1475]: loss 0.589653
[epoch9, step1476]: loss 0.551194
[epoch9, step1477]: loss 0.618757
[epoch9, step1478]: loss 0.462786
[epoch9, step1479]: loss 0.677155
[epoch9, step1480]: loss 0.518865
[epoch9, step1481]: loss 0.716836
[epoch9, step1482]: loss 0.550158
[epoch9, step1483]: loss 0.874029
[epoch9, step1484]: loss 0.658954
[epoch9, step1485]: loss 0.272828
[epoch9, step1486]: loss 0.509129
[epoch9, step1487]: loss 0.703766
[epoch9, step1488]: loss 0.735084
[epoch9, step1489]: loss 0.641589
[epoch9, step1490]: loss 0.617058
[epoch9, step1491]: loss 0.818668
[epoch9, step1492]: loss 0.807648
[epoch9, step1493]: loss 0.832511
[epoch9, step1494]: loss 0.907322
[epoch9, step1495]: loss 0.640069
[epoch9, step1496]: loss 0.674791
[epoch9, step1497]: loss 0.658107
[epoch9, step1498]: loss 0.739795
[epoch9, step1499]: loss 0.514864
[epoch9, step1500]: loss 0.814883
[epoch9, step1501]: loss 0.560259
[epoch9, step1502]: loss 0.397873
[epoch9, step1503]: loss 0.613858
[epoch9, step1504]: loss 0.647784
[epoch9, step1505]: loss 0.367341
[epoch9, step1506]: loss 0.416771
[epoch9, step1507]: loss 0.530300
[epoch9, step1508]: loss 0.724594
[epoch9, step1509]: loss 0.370893
[epoch9, step1510]: loss 0.530130
[epoch9, step1511]: loss 0.376796
[epoch9, step1512]: loss 0.538493
[epoch9, step1513]: loss 0.487349
[epoch9, step1514]: loss 0.560579
[epoch9, step1515]: loss 0.502332
[epoch9, step1516]: loss 0.665343
[epoch9, step1517]: loss 0.461523
[epoch9, step1518]: loss 0.588179
[epoch9, step1519]: loss 0.890567
[epoch9, step1520]: loss 0.716681
[epoch9, step1521]: loss 0.681544
[epoch9, step1522]: loss 0.559040
[epoch9, step1523]: loss 0.862277
[epoch9, step1524]: loss 0.661565
[epoch9, step1525]: loss 0.413756
[epoch9, step1526]: loss 0.155758
[epoch9, step1527]: loss 0.473696
[epoch9, step1528]: loss 0.673269
[epoch9, step1529]: loss 0.600304
[epoch9, step1530]: loss 0.329222
[epoch9, step1531]: loss 0.339337
[epoch9, step1532]: loss 0.680629
[epoch9, step1533]: loss 0.560995
[epoch9, step1534]: loss 0.539530
[epoch9, step1535]: loss 0.518498
[epoch9, step1536]: loss 0.557402
[epoch9, step1537]: loss 0.725128
[epoch9, step1538]: loss 0.715546
[epoch9, step1539]: loss 0.663559
[epoch9, step1540]: loss 0.603184
[epoch9, step1541]: loss 0.866490
[epoch9, step1542]: loss 0.690014
[epoch9, step1543]: loss 0.441031
[epoch9, step1544]: loss 0.692568
[epoch9, step1545]: loss 0.664436
[epoch9, step1546]: loss 0.654030
[epoch9, step1547]: loss 0.723586
[epoch9, step1548]: loss 0.718958
[epoch9, step1549]: loss 0.603109
[epoch9, step1550]: loss 0.677985
[epoch9, step1551]: loss 0.761782
[epoch9, step1552]: loss 0.463498
[epoch9, step1553]: loss 0.619552
[epoch9, step1554]: loss 0.523302
[epoch9, step1555]: loss 0.360684
[epoch9, step1556]: loss 0.421754
[epoch9, step1557]: loss 0.421692
[epoch9, step1558]: loss 0.747548
[epoch9, step1559]: loss 0.518771
[epoch9, step1560]: loss 0.626187
[epoch9, step1561]: loss 0.164093
[epoch9, step1562]: loss 0.759610
[epoch9, step1563]: loss 0.479736
[epoch9, step1564]: loss 0.732170
[epoch9, step1565]: loss 0.633707
[epoch9, step1566]: loss 0.789888
[epoch9, step1567]: loss 0.830637
[epoch9, step1568]: loss 0.736277
[epoch9, step1569]: loss 0.732487
[epoch9, step1570]: loss 0.614456
[epoch9, step1571]: loss 0.756127
[epoch9, step1572]: loss 0.335928
[epoch9, step1573]: loss 0.815360
[epoch9, step1574]: loss 0.471537
[epoch9, step1575]: loss 0.705367
[epoch9, step1576]: loss 0.642808
[epoch9, step1577]: loss 0.600070
[epoch9, step1578]: loss 0.585814
[epoch9, step1579]: loss 0.580849
[epoch9, step1580]: loss 0.631622
[epoch9, step1581]: loss 0.409829
[epoch9, step1582]: loss 0.714863
[epoch9, step1583]: loss 0.618429
[epoch9, step1584]: loss 0.638333
[epoch9, step1585]: loss 0.212020
[epoch9, step1586]: loss 0.833814
[epoch9, step1587]: loss 0.527979
[epoch9, step1588]: loss 0.713983
[epoch9, step1589]: loss 0.333245
[epoch9, step1590]: loss 0.456893
[epoch9, step1591]: loss 0.641089
[epoch9, step1592]: loss 0.369413
[epoch9, step1593]: loss 0.548510
[epoch9, step1594]: loss 0.644849
[epoch9, step1595]: loss 0.814627
[epoch9, step1596]: loss 0.380436
[epoch9, step1597]: loss 0.677150
[epoch9, step1598]: loss 0.685656
[epoch9, step1599]: loss 0.620183
[epoch9, step1600]: loss 0.604611
[epoch9, step1601]: loss 0.605765
[epoch9, step1602]: loss 0.620525
[epoch9, step1603]: loss 0.618797
[epoch9, step1604]: loss 0.678527
[epoch9, step1605]: loss 0.713437
[epoch9, step1606]: loss 0.525341
[epoch9, step1607]: loss 0.254681
[epoch9, step1608]: loss 0.640444
[epoch9, step1609]: loss 0.641166
[epoch9, step1610]: loss 0.677803
[epoch9, step1611]: loss 0.574508
[epoch9, step1612]: loss 0.799691
[epoch9, step1613]: loss 0.377333
[epoch9, step1614]: loss 0.561117
[epoch9, step1615]: loss 0.730277
[epoch9, step1616]: loss 0.688645
[epoch9, step1617]: loss 0.657512
[epoch9, step1618]: loss 0.847252
[epoch9, step1619]: loss 0.501390
[epoch9, step1620]: loss 0.614256
[epoch9, step1621]: loss 0.779613
[epoch9, step1622]: loss 0.487303
[epoch9, step1623]: loss 0.752274
[epoch9, step1624]: loss 0.391156
[epoch9, step1625]: loss 0.713866
[epoch9, step1626]: loss 0.616324
[epoch9, step1627]: loss 0.581193
[epoch9, step1628]: loss 0.611001
[epoch9, step1629]: loss 0.449694
[epoch9, step1630]: loss 0.861147
[epoch9, step1631]: loss 0.451116
[epoch9, step1632]: loss 0.380199
[epoch9, step1633]: loss 0.579092
[epoch9, step1634]: loss 0.539688
[epoch9, step1635]: loss 0.749958
[epoch9, step1636]: loss 0.468525
[epoch9, step1637]: loss 0.411482
[epoch9, step1638]: loss 0.582372
[epoch9, step1639]: loss 0.381326
[epoch9, step1640]: loss 0.333346
[epoch9, step1641]: loss 0.716178
[epoch9, step1642]: loss 0.755124
[epoch9, step1643]: loss 0.739742
[epoch9, step1644]: loss 0.499292
[epoch9, step1645]: loss 0.420213
[epoch9, step1646]: loss 0.432777
[epoch9, step1647]: loss 0.503458
[epoch9, step1648]: loss 0.472613
[epoch9, step1649]: loss 0.557228
[epoch9, step1650]: loss 0.740369
[epoch9, step1651]: loss 0.607197
[epoch9, step1652]: loss 0.769781
[epoch9, step1653]: loss 0.508433
[epoch9, step1654]: loss 0.585057
[epoch9, step1655]: loss 0.810584
[epoch9, step1656]: loss 0.504547
[epoch9, step1657]: loss 0.735872
[epoch9, step1658]: loss 0.674163
[epoch9, step1659]: loss 0.443097
[epoch9, step1660]: loss 0.896888
[epoch9, step1661]: loss 0.458723
[epoch9, step1662]: loss 0.746942
[epoch9, step1663]: loss 0.637793
[epoch9, step1664]: loss 0.649186
[epoch9, step1665]: loss 0.453136
[epoch9, step1666]: loss 0.629939
[epoch9, step1667]: loss 0.610650
[epoch9, step1668]: loss 0.653099
[epoch9, step1669]: loss 0.567150
[epoch9, step1670]: loss 0.417764
[epoch9, step1671]: loss 0.584602
[epoch9, step1672]: loss 0.605261
[epoch9, step1673]: loss 0.595617
[epoch9, step1674]: loss 0.649899
[epoch9, step1675]: loss 0.695810
[epoch9, step1676]: loss 0.788716
[epoch9, step1677]: loss 0.397850
[epoch9, step1678]: loss 0.572505
[epoch9, step1679]: loss 0.922902
[epoch9, step1680]: loss 0.353895
[epoch9, step1681]: loss 0.430092
[epoch9, step1682]: loss 0.577718
[epoch9, step1683]: loss 0.638619
[epoch9, step1684]: loss 0.199453
[epoch9, step1685]: loss 0.820893
[epoch9, step1686]: loss 0.640946
[epoch9, step1687]: loss 0.587321
[epoch9, step1688]: loss 0.499295
[epoch9, step1689]: loss 0.620443
[epoch9, step1690]: loss 0.471156
[epoch9, step1691]: loss 0.699643
[epoch9, step1692]: loss 0.496266
[epoch9, step1693]: loss 0.601929
[epoch9, step1694]: loss 0.689121
[epoch9, step1695]: loss 0.753312
[epoch9, step1696]: loss 0.762978
[epoch9, step1697]: loss 0.699994
[epoch9, step1698]: loss 0.345912
[epoch9, step1699]: loss 0.683327
[epoch9, step1700]: loss 0.716767
[epoch9, step1701]: loss 0.376505
[epoch9, step1702]: loss 0.674934
[epoch9, step1703]: loss 0.686259
[epoch9, step1704]: loss 0.669592
[epoch9, step1705]: loss 0.560326
[epoch9, step1706]: loss 0.440379
[epoch9, step1707]: loss 0.794179
[epoch9, step1708]: loss 0.637964
[epoch9, step1709]: loss 0.337106
[epoch9, step1710]: loss 0.758749
[epoch9, step1711]: loss 0.656713
[epoch9, step1712]: loss 0.332563
[epoch9, step1713]: loss 0.611301
[epoch9, step1714]: loss 0.553619
[epoch9, step1715]: loss 0.690568
[epoch9, step1716]: loss 0.767335
[epoch9, step1717]: loss 0.367285
[epoch9, step1718]: loss 0.608714
[epoch9, step1719]: loss 0.537193
[epoch9, step1720]: loss 0.451728
[epoch9, step1721]: loss 0.701492
[epoch9, step1722]: loss 0.559826
[epoch9, step1723]: loss 0.548402
[epoch9, step1724]: loss 0.555170
[epoch9, step1725]: loss 0.512922
[epoch9, step1726]: loss 0.624542
[epoch9, step1727]: loss 0.690418
[epoch9, step1728]: loss 0.333349
[epoch9, step1729]: loss 0.518730
[epoch9, step1730]: loss 0.830036
[epoch9, step1731]: loss 0.507512
[epoch9, step1732]: loss 0.635088
[epoch9, step1733]: loss 0.519625
[epoch9, step1734]: loss 0.471106
[epoch9, step1735]: loss 0.692742
[epoch9, step1736]: loss 0.720350
[epoch9, step1737]: loss 0.579174
[epoch9, step1738]: loss 0.697911
[epoch9, step1739]: loss 0.618328
[epoch9, step1740]: loss 0.562726
[epoch9, step1741]: loss 0.623784
[epoch9, step1742]: loss 0.902678
[epoch9, step1743]: loss 0.553690
[epoch9, step1744]: loss 0.640797
[epoch9, step1745]: loss 0.407574
[epoch9, step1746]: loss 0.653716
[epoch9, step1747]: loss 0.799747
[epoch9, step1748]: loss 0.430729
[epoch9, step1749]: loss 0.482761
[epoch9, step1750]: loss 0.680027
[epoch9, step1751]: loss 0.205272
[epoch9, step1752]: loss 0.536022
[epoch9, step1753]: loss 0.534244
[epoch9, step1754]: loss 0.468880
[epoch9, step1755]: loss 0.604629
[epoch9, step1756]: loss 0.805376
[epoch9, step1757]: loss 0.610081
[epoch9, step1758]: loss 0.537536
[epoch9, step1759]: loss 0.490022
[epoch9, step1760]: loss 0.732330
[epoch9, step1761]: loss 0.763157
[epoch9, step1762]: loss 0.788243
[epoch9, step1763]: loss 0.589002
[epoch9, step1764]: loss 0.516252
[epoch9, step1765]: loss 0.477357
[epoch9, step1766]: loss 0.752196
[epoch9, step1767]: loss 0.780980
[epoch9, step1768]: loss 0.473262
[epoch9, step1769]: loss 0.791947
[epoch9, step1770]: loss 0.409300
[epoch9, step1771]: loss 0.543033
[epoch9, step1772]: loss 0.532645
[epoch9, step1773]: loss 0.585466
[epoch9, step1774]: loss 0.387796
[epoch9, step1775]: loss 0.633811
[epoch9, step1776]: loss 0.496113
[epoch9, step1777]: loss 0.684362
[epoch9, step1778]: loss 0.427449
[epoch9, step1779]: loss 0.390537
[epoch9, step1780]: loss 0.578671
[epoch9, step1781]: loss 0.804803
[epoch9, step1782]: loss 0.493033
[epoch9, step1783]: loss 0.725383
[epoch9, step1784]: loss 0.448851
[epoch9, step1785]: loss 0.662742
[epoch9, step1786]: loss 0.549690
[epoch9, step1787]: loss 0.830341
[epoch9, step1788]: loss 0.824957
[epoch9, step1789]: loss 0.572088
[epoch9, step1790]: loss 0.524678
[epoch9, step1791]: loss 0.427417
[epoch9, step1792]: loss 0.812405
[epoch9, step1793]: loss 0.558868
[epoch9, step1794]: loss 0.428341
[epoch9, step1795]: loss 0.750018
[epoch9, step1796]: loss 0.755387
[epoch9, step1797]: loss 0.593989
[epoch9, step1798]: loss 0.185366
[epoch9, step1799]: loss 0.530505
[epoch9, step1800]: loss 0.659523
[epoch9, step1801]: loss 0.667764
[epoch9, step1802]: loss 0.702833
[epoch9, step1803]: loss 0.805676
[epoch9, step1804]: loss 0.836070
[epoch9, step1805]: loss 0.626274
[epoch9, step1806]: loss 0.550246
[epoch9, step1807]: loss 0.567517
[epoch9, step1808]: loss 0.543478
[epoch9, step1809]: loss 0.641420
[epoch9, step1810]: loss 0.585016
[epoch9, step1811]: loss 0.566696
[epoch9, step1812]: loss 0.711820
[epoch9, step1813]: loss 0.541428
[epoch9, step1814]: loss 0.451090
[epoch9, step1815]: loss 0.601737
[epoch9, step1816]: loss 0.678838
[epoch9, step1817]: loss 0.619153
[epoch9, step1818]: loss 0.577218
[epoch9, step1819]: loss 0.391303
[epoch9, step1820]: loss 0.591191
[epoch9, step1821]: loss 0.282641
[epoch9, step1822]: loss 0.601285
[epoch9, step1823]: loss 0.675705
[epoch9, step1824]: loss 0.697402
[epoch9, step1825]: loss 0.578703
[epoch9, step1826]: loss 0.373136
[epoch9, step1827]: loss 0.703879
[epoch9, step1828]: loss 0.702814
[epoch9, step1829]: loss 0.826245
[epoch9, step1830]: loss 0.668833
[epoch9, step1831]: loss 0.458405
[epoch9, step1832]: loss 0.587377
[epoch9, step1833]: loss 0.603864
[epoch9, step1834]: loss 0.718773
[epoch9, step1835]: loss 0.409080
[epoch9, step1836]: loss 0.617646
[epoch9, step1837]: loss 0.491297
[epoch9, step1838]: loss 0.864010
[epoch9, step1839]: loss 0.815483
[epoch9, step1840]: loss 0.569592
[epoch9, step1841]: loss 0.601327
[epoch9, step1842]: loss 0.539717
[epoch9, step1843]: loss 0.516384
[epoch9, step1844]: loss 0.240714
[epoch9, step1845]: loss 0.483498
[epoch9, step1846]: loss 0.590536
[epoch9, step1847]: loss 0.784794
[epoch9, step1848]: loss 0.825532
[epoch9, step1849]: loss 0.733243
[epoch9, step1850]: loss 0.651946
[epoch9, step1851]: loss 0.816941
[epoch9, step1852]: loss 0.782531
[epoch9, step1853]: loss 0.452758
[epoch9, step1854]: loss 0.554261
[epoch9, step1855]: loss 0.449606
[epoch9, step1856]: loss 0.602282
[epoch9, step1857]: loss 0.684312
[epoch9, step1858]: loss 0.519699
[epoch9, step1859]: loss 0.452269
[epoch9, step1860]: loss 0.538020
[epoch9, step1861]: loss 0.527508
[epoch9, step1862]: loss 0.751262
[epoch9, step1863]: loss 0.668990
[epoch9, step1864]: loss 0.703076
[epoch9, step1865]: loss 0.401210
[epoch9, step1866]: loss 0.453271
[epoch9, step1867]: loss 0.690096
[epoch9, step1868]: loss 0.740487
[epoch9, step1869]: loss 0.814619
[epoch9, step1870]: loss 0.524594
[epoch9, step1871]: loss 0.704496
[epoch9, step1872]: loss 0.603797
[epoch9, step1873]: loss 0.530611
[epoch9, step1874]: loss 0.461295
[epoch9, step1875]: loss 0.653794
[epoch9, step1876]: loss 0.277529
[epoch9, step1877]: loss 0.373765
[epoch9, step1878]: loss 0.428112
[epoch9, step1879]: loss 0.677477
[epoch9, step1880]: loss 0.316904
[epoch9, step1881]: loss 0.793664
[epoch9, step1882]: loss 0.702146
[epoch9, step1883]: loss 0.777336
[epoch9, step1884]: loss 0.484675
[epoch9, step1885]: loss 0.797794
[epoch9, step1886]: loss 0.738121
[epoch9, step1887]: loss 0.667539
[epoch9, step1888]: loss 0.687177
[epoch9, step1889]: loss 0.541426
[epoch9, step1890]: loss 0.664451
[epoch9, step1891]: loss 0.865966
[epoch9, step1892]: loss 0.691915
[epoch9, step1893]: loss 0.631919
[epoch9, step1894]: loss 0.303356
[epoch9, step1895]: loss 0.673160
[epoch9, step1896]: loss 0.416568
[epoch9, step1897]: loss 0.459994
[epoch9, step1898]: loss 0.826279
[epoch9, step1899]: loss 0.583729
[epoch9, step1900]: loss 0.618085
[epoch9, step1901]: loss 0.337522
[epoch9, step1902]: loss 0.604397
[epoch9, step1903]: loss 0.740930
[epoch9, step1904]: loss 0.566267
[epoch9, step1905]: loss 0.669136
[epoch9, step1906]: loss 0.639129
[epoch9, step1907]: loss 0.690605
[epoch9, step1908]: loss 0.694130
[epoch9, step1909]: loss 0.518475
[epoch9, step1910]: loss 0.651575
[epoch9, step1911]: loss 0.410000
[epoch9, step1912]: loss 0.614955
[epoch9, step1913]: loss 0.727010
[epoch9, step1914]: loss 0.672804
[epoch9, step1915]: loss 0.595438
[epoch9, step1916]: loss 0.382672
[epoch9, step1917]: loss 0.477490
[epoch9, step1918]: loss 0.742721
[epoch9, step1919]: loss 0.667463
[epoch9, step1920]: loss 0.494224
[epoch9, step1921]: loss 0.569736
[epoch9, step1922]: loss 0.688398
[epoch9, step1923]: loss 0.516297
[epoch9, step1924]: loss 0.437819
[epoch9, step1925]: loss 0.696950
[epoch9, step1926]: loss 0.502488
[epoch9, step1927]: loss 0.827775
[epoch9, step1928]: loss 0.739980
[epoch9, step1929]: loss 0.554256
[epoch9, step1930]: loss 0.678991
[epoch9, step1931]: loss 0.417038
[epoch9, step1932]: loss 0.664007
[epoch9, step1933]: loss 0.725748
[epoch9, step1934]: loss 0.552466
[epoch9, step1935]: loss 0.420543
[epoch9, step1936]: loss 0.595760
[epoch9, step1937]: loss 0.808758
[epoch9, step1938]: loss 0.449773
[epoch9, step1939]: loss 0.663705
[epoch9, step1940]: loss 0.744028
[epoch9, step1941]: loss 0.554987
[epoch9, step1942]: loss 0.395365
[epoch9, step1943]: loss 0.762193
[epoch9, step1944]: loss 0.467489
[epoch9, step1945]: loss 0.428212
[epoch9, step1946]: loss 0.492663
[epoch9, step1947]: loss 0.658832
[epoch9, step1948]: loss 0.600532
[epoch9, step1949]: loss 0.708766
[epoch9, step1950]: loss 0.585288
[epoch9, step1951]: loss 0.454944
[epoch9, step1952]: loss 0.812311
[epoch9, step1953]: loss 0.464710
[epoch9, step1954]: loss 0.728738
[epoch9, step1955]: loss 0.604198
[epoch9, step1956]: loss 0.789579
[epoch9, step1957]: loss 0.716527
[epoch9, step1958]: loss 0.441988
[epoch9, step1959]: loss 0.797873
[epoch9, step1960]: loss 0.433847
[epoch9, step1961]: loss 0.513406
[epoch9, step1962]: loss 0.489519
[epoch9, step1963]: loss 0.858322
[epoch9, step1964]: loss 0.557935
[epoch9, step1965]: loss 0.722390
[epoch9, step1966]: loss 0.460849
[epoch9, step1967]: loss 0.190130
[epoch9, step1968]: loss 0.716988
[epoch9, step1969]: loss 0.584943
[epoch9, step1970]: loss 0.471144
[epoch9, step1971]: loss 0.678260
[epoch9, step1972]: loss 0.729385
[epoch9, step1973]: loss 0.579451
[epoch9, step1974]: loss 0.774238
[epoch9, step1975]: loss 0.705488
[epoch9, step1976]: loss 0.700472
[epoch9, step1977]: loss 0.672641
[epoch9, step1978]: loss 0.764897
[epoch9, step1979]: loss 0.700281
[epoch9, step1980]: loss 0.403385
[epoch9, step1981]: loss 0.692515
[epoch9, step1982]: loss 0.385414
[epoch9, step1983]: loss 0.682421
[epoch9, step1984]: loss 0.565723
[epoch9, step1985]: loss 0.712445
[epoch9, step1986]: loss 0.690424
[epoch9, step1987]: loss 0.476842
[epoch9, step1988]: loss 0.536505
[epoch9, step1989]: loss 0.591597
[epoch9, step1990]: loss 0.610608
[epoch9, step1991]: loss 0.476692
[epoch9, step1992]: loss 0.558381
[epoch9, step1993]: loss 0.546655
[epoch9, step1994]: loss 0.383598
[epoch9, step1995]: loss 0.459479
[epoch9, step1996]: loss 0.636118
[epoch9, step1997]: loss 0.570800
[epoch9, step1998]: loss 0.533372
[epoch9, step1999]: loss 0.582352
[epoch9, step2000]: loss 0.751704
[epoch9, step2001]: loss 0.449276
[epoch9, step2002]: loss 0.785498
[epoch9, step2003]: loss 0.648762
[epoch9, step2004]: loss 0.586853
[epoch9, step2005]: loss 0.304507
[epoch9, step2006]: loss 0.683292
[epoch9, step2007]: loss 0.747427
[epoch9, step2008]: loss 0.804375
[epoch9, step2009]: loss 0.324386
[epoch9, step2010]: loss 0.860305
[epoch9, step2011]: loss 0.657463
[epoch9, step2012]: loss 0.757338
[epoch9, step2013]: loss 0.698332
[epoch9, step2014]: loss 0.659240
[epoch9, step2015]: loss 0.569526
[epoch9, step2016]: loss 0.672651
[epoch9, step2017]: loss 0.388266
[epoch9, step2018]: loss 0.668487
[epoch9, step2019]: loss 0.821094
[epoch9, step2020]: loss 0.837091
[epoch9, step2021]: loss 0.782673
[epoch9, step2022]: loss 0.627855
[epoch9, step2023]: loss 0.476826
[epoch9, step2024]: loss 0.720695
[epoch9, step2025]: loss 0.487231
[epoch9, step2026]: loss 0.720992
[epoch9, step2027]: loss 0.387056
[epoch9, step2028]: loss 0.671021
[epoch9, step2029]: loss 0.673482
[epoch9, step2030]: loss 0.533907
[epoch9, step2031]: loss 0.748105
[epoch9, step2032]: loss 0.511649
[epoch9, step2033]: loss 0.688760
[epoch9, step2034]: loss 0.823477
[epoch9, step2035]: loss 0.477021
[epoch9, step2036]: loss 0.519936
[epoch9, step2037]: loss 0.737068
[epoch9, step2038]: loss 0.733418
[epoch9, step2039]: loss 0.722707
[epoch9, step2040]: loss 0.502226
[epoch9, step2041]: loss 0.762480
[epoch9, step2042]: loss 0.683375
[epoch9, step2043]: loss 0.689408
[epoch9, step2044]: loss 0.690167
[epoch9, step2045]: loss 0.758877
[epoch9, step2046]: loss 0.514398
[epoch9, step2047]: loss 0.811973
[epoch9, step2048]: loss 0.620436
[epoch9, step2049]: loss 0.470184
[epoch9, step2050]: loss 0.656564
[epoch9, step2051]: loss 0.634893
[epoch9, step2052]: loss 0.415441
[epoch9, step2053]: loss 0.824510
[epoch9, step2054]: loss 0.601622
[epoch9, step2055]: loss 0.763099
[epoch9, step2056]: loss 0.582600
[epoch9, step2057]: loss 0.847386
[epoch9, step2058]: loss 0.671791
[epoch9, step2059]: loss 0.629269
[epoch9, step2060]: loss 0.462894
[epoch9, step2061]: loss 0.580559
[epoch9, step2062]: loss 0.417125
[epoch9, step2063]: loss 0.671229
[epoch9, step2064]: loss 0.569111
[epoch9, step2065]: loss 0.392428
[epoch9, step2066]: loss 0.526016
[epoch9, step2067]: loss 0.394566
[epoch9, step2068]: loss 0.387172
[epoch9, step2069]: loss 0.520035
[epoch9, step2070]: loss 0.736270
[epoch9, step2071]: loss 0.659010
[epoch9, step2072]: loss 0.609466
[epoch9, step2073]: loss 0.477068
[epoch9, step2074]: loss 0.314662
[epoch9, step2075]: loss 0.543244
[epoch9, step2076]: loss 0.215563
[epoch9, step2077]: loss 0.675020
[epoch9, step2078]: loss 0.974034
[epoch9, step2079]: loss 0.756435
[epoch9, step2080]: loss 0.675006
[epoch9, step2081]: loss 0.683077
[epoch9, step2082]: loss 0.664921
[epoch9, step2083]: loss 0.531936
[epoch9, step2084]: loss 0.802029
[epoch9, step2085]: loss 0.695995
[epoch9, step2086]: loss 0.789127
[epoch9, step2087]: loss 0.433890
[epoch9, step2088]: loss 0.530804
[epoch9, step2089]: loss 0.635933
[epoch9, step2090]: loss 0.445445
[epoch9, step2091]: loss 0.739610
[epoch9, step2092]: loss 0.793006
[epoch9, step2093]: loss 0.553549
[epoch9, step2094]: loss 0.546495
[epoch9, step2095]: loss 0.763377
[epoch9, step2096]: loss 0.414194
[epoch9, step2097]: loss 0.527517
[epoch9, step2098]: loss 0.615516
[epoch9, step2099]: loss 0.611358
[epoch9, step2100]: loss 0.292740
[epoch9, step2101]: loss 0.662747
[epoch9, step2102]: loss 0.648142
[epoch9, step2103]: loss 0.621476
[epoch9, step2104]: loss 0.750001
[epoch9, step2105]: loss 0.634699
[epoch9, step2106]: loss 0.662105
[epoch9, step2107]: loss 0.799344
[epoch9, step2108]: loss 0.612419
[epoch9, step2109]: loss 0.490542
[epoch9, step2110]: loss 0.397843
[epoch9, step2111]: loss 0.473209
[epoch9, step2112]: loss 0.767759
[epoch9, step2113]: loss 0.669371
[epoch9, step2114]: loss 0.696616
[epoch9, step2115]: loss 0.632041
[epoch9, step2116]: loss 0.356943
[epoch9, step2117]: loss 0.481489
[epoch9, step2118]: loss 0.334498
[epoch9, step2119]: loss 0.604884
[epoch9, step2120]: loss 0.682117
[epoch9, step2121]: loss 0.644658
[epoch9, step2122]: loss 0.623428
[epoch9, step2123]: loss 0.447785
[epoch9, step2124]: loss 0.632541
[epoch9, step2125]: loss 0.674071
[epoch9, step2126]: loss 0.482362
[epoch9, step2127]: loss 0.412996
[epoch9, step2128]: loss 0.600841
[epoch9, step2129]: loss 0.595210
[epoch9, step2130]: loss 0.649518
[epoch9, step2131]: loss 0.428843
[epoch9, step2132]: loss 0.627109
[epoch9, step2133]: loss 0.760561
[epoch9, step2134]: loss 0.443026
[epoch9, step2135]: loss 0.637257
[epoch9, step2136]: loss 0.200986
[epoch9, step2137]: loss 0.528446
[epoch9, step2138]: loss 0.616855
[epoch9, step2139]: loss 0.797820
[epoch9, step2140]: loss 0.423329
[epoch9, step2141]: loss 0.327637
[epoch9, step2142]: loss 0.783273
[epoch9, step2143]: loss 0.305871
[epoch9, step2144]: loss 0.401288
[epoch9, step2145]: loss 0.393242
[epoch9, step2146]: loss 0.629339
[epoch9, step2147]: loss 0.382636
[epoch9, step2148]: loss 0.674172
[epoch9, step2149]: loss 0.580241
[epoch9, step2150]: loss 0.711984
[epoch9, step2151]: loss 0.475316
[epoch9, step2152]: loss 0.678624
[epoch9, step2153]: loss 0.825738
[epoch9, step2154]: loss 0.464933
[epoch9, step2155]: loss 0.670334
[epoch9, step2156]: loss 0.427500
[epoch9, step2157]: loss 0.806992
[epoch9, step2158]: loss 0.688810
[epoch9, step2159]: loss 0.551488
[epoch9, step2160]: loss 0.560433
[epoch9, step2161]: loss 0.610717
[epoch9, step2162]: loss 0.429981
[epoch9, step2163]: loss 0.596362
[epoch9, step2164]: loss 0.598644
[epoch9, step2165]: loss 0.757277
[epoch9, step2166]: loss 0.423759
[epoch9, step2167]: loss 0.720525
[epoch9, step2168]: loss 0.736927
[epoch9, step2169]: loss 0.865830
[epoch9, step2170]: loss 0.584105
[epoch9, step2171]: loss 0.788065
[epoch9, step2172]: loss 0.433079
[epoch9, step2173]: loss 0.739097
[epoch9, step2174]: loss 0.796896
[epoch9, step2175]: loss 0.855650
[epoch9, step2176]: loss 0.387943
[epoch9, step2177]: loss 0.411063
[epoch9, step2178]: loss 0.499315
[epoch9, step2179]: loss 0.716303
[epoch9, step2180]: loss 0.773020
[epoch9, step2181]: loss 0.813192
[epoch9, step2182]: loss 0.490252
[epoch9, step2183]: loss 0.813599
[epoch9, step2184]: loss 0.944986
[epoch9, step2185]: loss 0.482975
[epoch9, step2186]: loss 0.657222
[epoch9, step2187]: loss 0.567351
[epoch9, step2188]: loss 0.603061
[epoch9, step2189]: loss 0.615432
[epoch9, step2190]: loss 0.448426
[epoch9, step2191]: loss 0.732074
[epoch9, step2192]: loss 0.588225
[epoch9, step2193]: loss 0.685376
[epoch9, step2194]: loss 0.526192
[epoch9, step2195]: loss 0.623545
[epoch9, step2196]: loss 0.660391
[epoch9, step2197]: loss 0.559641
[epoch9, step2198]: loss 0.866628
[epoch9, step2199]: loss 0.592076
[epoch9, step2200]: loss 0.575223
[epoch9, step2201]: loss 0.578415
[epoch9, step2202]: loss 0.820820
[epoch9, step2203]: loss 0.660724
[epoch9, step2204]: loss 0.619121
[epoch9, step2205]: loss 0.713044
[epoch9, step2206]: loss 0.121848
[epoch9, step2207]: loss 0.559242
[epoch9, step2208]: loss 0.202114
[epoch9, step2209]: loss 0.735341
[epoch9, step2210]: loss 0.700283
[epoch9, step2211]: loss 0.653295
[epoch9, step2212]: loss 0.653964
[epoch9, step2213]: loss 0.442576
[epoch9, step2214]: loss 0.342242
[epoch9, step2215]: loss 0.743334
[epoch9, step2216]: loss 0.667163
[epoch9, step2217]: loss 0.682681
[epoch9, step2218]: loss 0.546433
[epoch9, step2219]: loss 0.405070
[epoch9, step2220]: loss 0.663707
[epoch9, step2221]: loss 0.787864
[epoch9, step2222]: loss 0.672351
[epoch9, step2223]: loss 0.339553
[epoch9, step2224]: loss 0.641866
[epoch9, step2225]: loss 0.586133
[epoch9, step2226]: loss 0.447525
[epoch9, step2227]: loss 0.757386
[epoch9, step2228]: loss 0.659689
[epoch9, step2229]: loss 0.348132
[epoch9, step2230]: loss 0.537176
[epoch9, step2231]: loss 0.600830
[epoch9, step2232]: loss 0.704896
[epoch9, step2233]: loss 0.632295
[epoch9, step2234]: loss 0.713176
[epoch9, step2235]: loss 0.428812
[epoch9, step2236]: loss 0.583015
[epoch9, step2237]: loss 0.436289
[epoch9, step2238]: loss 0.699970
[epoch9, step2239]: loss 0.802289
[epoch9, step2240]: loss 0.565129
[epoch9, step2241]: loss 0.862842
[epoch9, step2242]: loss 0.760312
[epoch9, step2243]: loss 0.523714
[epoch9, step2244]: loss 0.899342
[epoch9, step2245]: loss 0.297112
[epoch9, step2246]: loss 0.703935
[epoch9, step2247]: loss 0.299228
[epoch9, step2248]: loss 0.585087
[epoch9, step2249]: loss 0.273051
[epoch9, step2250]: loss 0.530657
[epoch9, step2251]: loss 0.519731
[epoch9, step2252]: loss 0.532815
[epoch9, step2253]: loss 0.632761
[epoch9, step2254]: loss 0.346032
[epoch9, step2255]: loss 0.622085
[epoch9, step2256]: loss 0.344737
[epoch9, step2257]: loss 0.594007
[epoch9, step2258]: loss 0.660861
[epoch9, step2259]: loss 0.398073
[epoch9, step2260]: loss 0.763964
[epoch9, step2261]: loss 0.513947
[epoch9, step2262]: loss 0.468805
[epoch9, step2263]: loss 0.585936
[epoch9, step2264]: loss 0.442101
[epoch9, step2265]: loss 0.398945
[epoch9, step2266]: loss 0.380157
[epoch9, step2267]: loss 0.507324
[epoch9, step2268]: loss 0.691477
[epoch9, step2269]: loss 0.341442
[epoch9, step2270]: loss 0.594059
[epoch9, step2271]: loss 0.621907
[epoch9, step2272]: loss 0.688035
[epoch9, step2273]: loss 0.720105
[epoch9, step2274]: loss 0.853680
[epoch9, step2275]: loss 0.625863
[epoch9, step2276]: loss 0.679726
[epoch9, step2277]: loss 0.526300
[epoch9, step2278]: loss 0.811613
[epoch9, step2279]: loss 0.822954
[epoch9, step2280]: loss 0.470730
[epoch9, step2281]: loss 0.460324
[epoch9, step2282]: loss 0.505955
[epoch9, step2283]: loss 0.558990
[epoch9, step2284]: loss 0.441304
[epoch9, step2285]: loss 0.441929
[epoch9, step2286]: loss 0.641013
[epoch9, step2287]: loss 0.443964
[epoch9, step2288]: loss 0.462056
[epoch9, step2289]: loss 0.548376
[epoch9, step2290]: loss 0.794015
[epoch9, step2291]: loss 0.667162
[epoch9, step2292]: loss 0.666238
[epoch9, step2293]: loss 0.634975
[epoch9, step2294]: loss 0.550019
[epoch9, step2295]: loss 0.804734
[epoch9, step2296]: loss 0.685689
[epoch9, step2297]: loss 0.558753
[epoch9, step2298]: loss 0.522436
[epoch9, step2299]: loss 0.685677
[epoch9, step2300]: loss 0.568927
[epoch9, step2301]: loss 0.562030
[epoch9, step2302]: loss 0.654144
[epoch9, step2303]: loss 0.708815
[epoch9, step2304]: loss 0.652804
[epoch9, step2305]: loss 0.662109
[epoch9, step2306]: loss 0.617853
[epoch9, step2307]: loss 0.602221
[epoch9, step2308]: loss 0.735456
[epoch9, step2309]: loss 0.896792
[epoch9, step2310]: loss 0.495749
[epoch9, step2311]: loss 0.738753
[epoch9, step2312]: loss 0.877450
[epoch9, step2313]: loss 0.670692
[epoch9, step2314]: loss 0.821603
[epoch9, step2315]: loss 0.587219
[epoch9, step2316]: loss 0.362754
[epoch9, step2317]: loss 0.733994
[epoch9, step2318]: loss 0.808571
[epoch9, step2319]: loss 0.600218
[epoch9, step2320]: loss 0.486876
[epoch9, step2321]: loss 0.680603
[epoch9, step2322]: loss 0.653367
[epoch9, step2323]: loss 0.805273
[epoch9, step2324]: loss 0.594105
[epoch9, step2325]: loss 0.354135
[epoch9, step2326]: loss 0.645498
[epoch9, step2327]: loss 0.601127
[epoch9, step2328]: loss 0.585503
[epoch9, step2329]: loss 0.741384
[epoch9, step2330]: loss 0.355819
[epoch9, step2331]: loss 0.582521
[epoch9, step2332]: loss 0.411874
[epoch9, step2333]: loss 0.819140
[epoch9, step2334]: loss 0.714643
[epoch9, step2335]: loss 0.809203
[epoch9, step2336]: loss 0.623164
[epoch9, step2337]: loss 0.722877
[epoch9, step2338]: loss 0.764606
[epoch9, step2339]: loss 0.673438
[epoch9, step2340]: loss 0.758879
[epoch9, step2341]: loss 0.575039
[epoch9, step2342]: loss 0.780796
[epoch9, step2343]: loss 0.683466
[epoch9, step2344]: loss 0.650051
[epoch9, step2345]: loss 0.878265
[epoch9, step2346]: loss 0.544818
[epoch9, step2347]: loss 0.350988
[epoch9, step2348]: loss 0.634524
[epoch9, step2349]: loss 0.677416
[epoch9, step2350]: loss 0.584373
[epoch9, step2351]: loss 0.499171
[epoch9, step2352]: loss 0.693019
[epoch9, step2353]: loss 0.292230
[epoch9, step2354]: loss 0.672369
[epoch9, step2355]: loss 0.538246
[epoch9, step2356]: loss 0.713853
[epoch9, step2357]: loss 0.174117
[epoch9, step2358]: loss 0.786326
[epoch9, step2359]: loss 0.592634
[epoch9, step2360]: loss 0.808062
[epoch9, step2361]: loss 0.367164
[epoch9, step2362]: loss 0.549352
[epoch9, step2363]: loss 0.546321
[epoch9, step2364]: loss 0.679491
[epoch9, step2365]: loss 0.408487
[epoch9, step2366]: loss 0.746264
[epoch9, step2367]: loss 0.646137
[epoch9, step2368]: loss 0.810949
[epoch9, step2369]: loss 0.504941
[epoch9, step2370]: loss 0.553492
[epoch9, step2371]: loss 0.295297
[epoch9, step2372]: loss 0.491129
[epoch9, step2373]: loss 0.583797
[epoch9, step2374]: loss 0.613937
[epoch9, step2375]: loss 0.734074
[epoch9, step2376]: loss 0.293918
[epoch9, step2377]: loss 0.671097
[epoch9, step2378]: loss 0.519510
[epoch9, step2379]: loss 0.582105
[epoch9, step2380]: loss 0.602537
[epoch9, step2381]: loss 0.499583
[epoch9, step2382]: loss 0.843160
[epoch9, step2383]: loss 0.638684
[epoch9, step2384]: loss 0.763298
[epoch9, step2385]: loss 0.678920
[epoch9, step2386]: loss 0.430078
[epoch9, step2387]: loss 0.562905
[epoch9, step2388]: loss 0.695307
[epoch9, step2389]: loss 0.518443
[epoch9, step2390]: loss 0.543752
[epoch9, step2391]: loss 0.590407
[epoch9, step2392]: loss 0.470586
[epoch9, step2393]: loss 0.468348
[epoch9, step2394]: loss 0.593273
[epoch9, step2395]: loss 0.689264
[epoch9, step2396]: loss 0.426001
[epoch9, step2397]: loss 0.512032
[epoch9, step2398]: loss 0.764553
[epoch9, step2399]: loss 0.481443
[epoch9, step2400]: loss 0.529700
[epoch9, step2401]: loss 0.705759
[epoch9, step2402]: loss 0.496965
[epoch9, step2403]: loss 0.677046
[epoch9, step2404]: loss 0.646589
[epoch9, step2405]: loss 0.782665
[epoch9, step2406]: loss 0.743623
[epoch9, step2407]: loss 0.601308
[epoch9, step2408]: loss 0.686910
[epoch9, step2409]: loss 0.560438
[epoch9, step2410]: loss 0.632638
[epoch9, step2411]: loss 0.696078
[epoch9, step2412]: loss 0.629659
[epoch9, step2413]: loss 0.205485
[epoch9, step2414]: loss 0.747380
[epoch9, step2415]: loss 0.648100
[epoch9, step2416]: loss 0.722725
[epoch9, step2417]: loss 0.333379
[epoch9, step2418]: loss 0.615712
[epoch9, step2419]: loss 0.565421
[epoch9, step2420]: loss 0.838567
[epoch9, step2421]: loss 0.660120
[epoch9, step2422]: loss 0.577275
[epoch9, step2423]: loss 0.606185
[epoch9, step2424]: loss 0.927770
[epoch9, step2425]: loss 0.256010
[epoch9, step2426]: loss 0.490522
[epoch9, step2427]: loss 0.774289
[epoch9, step2428]: loss 0.589353
[epoch9, step2429]: loss 0.540566
[epoch9, step2430]: loss 0.832947
[epoch9, step2431]: loss 0.235050
[epoch9, step2432]: loss 0.696598
[epoch9, step2433]: loss 0.693585
[epoch9, step2434]: loss 0.396527
[epoch9, step2435]: loss 0.758587
[epoch9, step2436]: loss 0.642832
[epoch9, step2437]: loss 0.350888
[epoch9, step2438]: loss 0.701064
[epoch9, step2439]: loss 0.412765
[epoch9, step2440]: loss 0.505864
[epoch9, step2441]: loss 0.571173
[epoch9, step2442]: loss 0.798974
[epoch9, step2443]: loss 0.487359
[epoch9, step2444]: loss 0.763052
[epoch9, step2445]: loss 0.618560
[epoch9, step2446]: loss 0.570609
[epoch9, step2447]: loss 0.471032
[epoch9, step2448]: loss 0.643652
[epoch9, step2449]: loss 0.381936
[epoch9, step2450]: loss 0.516277
[epoch9, step2451]: loss 0.496926
[epoch9, step2452]: loss 0.549352
[epoch9, step2453]: loss 0.593201
[epoch9, step2454]: loss 0.626664
[epoch9, step2455]: loss 0.318622
[epoch9, step2456]: loss 0.555152
[epoch9, step2457]: loss 0.756524
[epoch9, step2458]: loss 0.738780
[epoch9, step2459]: loss 0.576769
[epoch9, step2460]: loss 0.631660
[epoch9, step2461]: loss 0.813417
[epoch9, step2462]: loss 0.697755
[epoch9, step2463]: loss 0.364830
[epoch9, step2464]: loss 0.471905
[epoch9, step2465]: loss 0.784232
[epoch9, step2466]: loss 0.586844
[epoch9, step2467]: loss 0.578452
[epoch9, step2468]: loss 0.589710
[epoch9, step2469]: loss 0.558192
[epoch9, step2470]: loss 0.541299
[epoch9, step2471]: loss 0.584913
[epoch9, step2472]: loss 0.658261
[epoch9, step2473]: loss 0.711847
[epoch9, step2474]: loss 0.764619
[epoch9, step2475]: loss 0.730803
[epoch9, step2476]: loss 0.568769
[epoch9, step2477]: loss 0.678984
[epoch9, step2478]: loss 0.602692
[epoch9, step2479]: loss 0.478254
[epoch9, step2480]: loss 0.795618
[epoch9, step2481]: loss 0.625562
[epoch9, step2482]: loss 0.574944
[epoch9, step2483]: loss 0.483702
[epoch9, step2484]: loss 0.574954
[epoch9, step2485]: loss 0.518531
[epoch9, step2486]: loss 0.640158
[epoch9, step2487]: loss 0.380199
[epoch9, step2488]: loss 0.483065
[epoch9, step2489]: loss 0.716416
[epoch9, step2490]: loss 0.534089
[epoch9, step2491]: loss 0.461005
[epoch9, step2492]: loss 0.485073
[epoch9, step2493]: loss 0.580568
[epoch9, step2494]: loss 0.657932
[epoch9, step2495]: loss 0.368563
[epoch9, step2496]: loss 0.756594
[epoch9, step2497]: loss 0.642050
[epoch9, step2498]: loss 0.629547
[epoch9, step2499]: loss 0.532106
[epoch9, step2500]: loss 0.380382
[epoch9, step2501]: loss 0.505832
[epoch9, step2502]: loss 0.457014
[epoch9, step2503]: loss 0.823727
[epoch9, step2504]: loss 0.815553
[epoch9, step2505]: loss 0.590234
[epoch9, step2506]: loss 0.447793
[epoch9, step2507]: loss 0.448428
[epoch9, step2508]: loss 0.667425
[epoch9, step2509]: loss 0.481172
[epoch9, step2510]: loss 0.724993
[epoch9, step2511]: loss 0.527310
[epoch9, step2512]: loss 0.643278
[epoch9, step2513]: loss 0.728361
[epoch9, step2514]: loss 0.489552
[epoch9, step2515]: loss 0.573205
[epoch9, step2516]: loss 0.695170
[epoch9, step2517]: loss 0.775825
[epoch9, step2518]: loss 0.638335
[epoch9, step2519]: loss 0.542963
[epoch9, step2520]: loss 0.578758
[epoch9, step2521]: loss 0.540093
[epoch9, step2522]: loss 0.511072
[epoch9, step2523]: loss 0.716299
[epoch9, step2524]: loss 0.600853
[epoch9, step2525]: loss 0.636627
[epoch9, step2526]: loss 0.602563
[epoch9, step2527]: loss 0.709633
[epoch9, step2528]: loss 0.664699
[epoch9, step2529]: loss 0.724158
[epoch9, step2530]: loss 0.581859
[epoch9, step2531]: loss 0.679278
[epoch9, step2532]: loss 0.680508
[epoch9, step2533]: loss 0.644587
[epoch9, step2534]: loss 0.667906
[epoch9, step2535]: loss 0.462710
[epoch9, step2536]: loss 0.683275
[epoch9, step2537]: loss 0.724152
[epoch9, step2538]: loss 0.661824
[epoch9, step2539]: loss 0.630593
[epoch9, step2540]: loss 0.525553
[epoch9, step2541]: loss 0.741332
[epoch9, step2542]: loss 0.543039
[epoch9, step2543]: loss 0.242480
[epoch9, step2544]: loss 0.659504
[epoch9, step2545]: loss 0.630001
[epoch9, step2546]: loss 0.541771
[epoch9, step2547]: loss 0.425996
[epoch9, step2548]: loss 0.618027
[epoch9, step2549]: loss 0.584841
[epoch9, step2550]: loss 0.690970
[epoch9, step2551]: loss 0.517768
[epoch9, step2552]: loss 0.583418
[epoch9, step2553]: loss 0.597109
[epoch9, step2554]: loss 0.818413
[epoch9, step2555]: loss 0.412263
[epoch9, step2556]: loss 0.581150
[epoch9, step2557]: loss 0.610019
[epoch9, step2558]: loss 0.586647
[epoch9, step2559]: loss 0.458946
[epoch9, step2560]: loss 0.417186
[epoch9, step2561]: loss 0.516137
[epoch9, step2562]: loss 0.576943
[epoch9, step2563]: loss 0.713022
[epoch9, step2564]: loss 0.682227
[epoch9, step2565]: loss 0.430236
[epoch9, step2566]: loss 0.371898
[epoch9, step2567]: loss 0.793166
[epoch9, step2568]: loss 0.559276
[epoch9, step2569]: loss 0.671072
[epoch9, step2570]: loss 0.477385
[epoch9, step2571]: loss 0.670839
[epoch9, step2572]: loss 0.640395
[epoch9, step2573]: loss 0.641141
[epoch9, step2574]: loss 0.605597
[epoch9, step2575]: loss 0.610487
[epoch9, step2576]: loss 0.714702
[epoch9, step2577]: loss 0.583481
[epoch9, step2578]: loss 0.782898
[epoch9, step2579]: loss 0.695681
[epoch9, step2580]: loss 0.453191
[epoch9, step2581]: loss 0.357359
[epoch9, step2582]: loss 0.381182
[epoch9, step2583]: loss 0.611422
[epoch9, step2584]: loss 0.457158
[epoch9, step2585]: loss 0.575308
[epoch9, step2586]: loss 0.774573
[epoch9, step2587]: loss 0.144709
[epoch9, step2588]: loss 0.577843
[epoch9, step2589]: loss 0.562439
[epoch9, step2590]: loss 0.645054
[epoch9, step2591]: loss 0.760834
[epoch9, step2592]: loss 0.764289
[epoch9, step2593]: loss 0.607696
[epoch9, step2594]: loss 0.707126
[epoch9, step2595]: loss 0.547886
[epoch9, step2596]: loss 0.697423
[epoch9, step2597]: loss 0.743791
[epoch9, step2598]: loss 0.339384
[epoch9, step2599]: loss 0.834296
[epoch9, step2600]: loss 0.712272
[epoch9, step2601]: loss 0.664733
[epoch9, step2602]: loss 0.382533
[epoch9, step2603]: loss 0.645902
[epoch9, step2604]: loss 0.690655
[epoch9, step2605]: loss 0.578264
[epoch9, step2606]: loss 0.475811
[epoch9, step2607]: loss 0.821815
[epoch9, step2608]: loss 0.599620
[epoch9, step2609]: loss 0.377178
[epoch9, step2610]: loss 0.486426
[epoch9, step2611]: loss 0.561171
[epoch9, step2612]: loss 0.612419
[epoch9, step2613]: loss 0.587067
[epoch9, step2614]: loss 0.611824
[epoch9, step2615]: loss 0.592225
[epoch9, step2616]: loss 0.802322
[epoch9, step2617]: loss 0.758121
[epoch9, step2618]: loss 0.391092
[epoch9, step2619]: loss 0.253128
[epoch9, step2620]: loss 0.367049
[epoch9, step2621]: loss 0.665117
[epoch9, step2622]: loss 0.767720
[epoch9, step2623]: loss 0.475455
[epoch9, step2624]: loss 0.613769
[epoch9, step2625]: loss 0.830063
[epoch9, step2626]: loss 0.702502
[epoch9, step2627]: loss 0.616374
[epoch9, step2628]: loss 0.523881
[epoch9, step2629]: loss 0.664093
[epoch9, step2630]: loss 0.540628
[epoch9, step2631]: loss 0.726327
[epoch9, step2632]: loss 0.567541
[epoch9, step2633]: loss 0.467296
[epoch9, step2634]: loss 0.522941
[epoch9, step2635]: loss 0.689735
[epoch9, step2636]: loss 0.676483
[epoch9, step2637]: loss 0.798922
[epoch9, step2638]: loss 0.586576
[epoch9, step2639]: loss 0.527644
[epoch9, step2640]: loss 0.653610
[epoch9, step2641]: loss 0.568098
[epoch9, step2642]: loss 0.629157
[epoch9, step2643]: loss 0.743402
[epoch9, step2644]: loss 0.576236
[epoch9, step2645]: loss 0.678345
[epoch9, step2646]: loss 0.637411
[epoch9, step2647]: loss 0.646929
[epoch9, step2648]: loss 0.402818
[epoch9, step2649]: loss 0.539089
[epoch9, step2650]: loss 0.394622
[epoch9, step2651]: loss 0.421029
[epoch9, step2652]: loss 0.461383
[epoch9, step2653]: loss 0.686378
[epoch9, step2654]: loss 0.725578
[epoch9, step2655]: loss 0.535415
[epoch9, step2656]: loss 0.620354
[epoch9, step2657]: loss 0.617704
[epoch9, step2658]: loss 0.775649
[epoch9, step2659]: loss 0.495785
[epoch9, step2660]: loss 0.636743
[epoch9, step2661]: loss 0.559857
[epoch9, step2662]: loss 0.691737
[epoch9, step2663]: loss 0.757227
[epoch9, step2664]: loss 0.297110
[epoch9, step2665]: loss 0.825766
[epoch9, step2666]: loss 0.514953
[epoch9, step2667]: loss 0.615294
[epoch9, step2668]: loss 0.630929
[epoch9, step2669]: loss 0.531827
[epoch9, step2670]: loss 0.475361
[epoch9, step2671]: loss 0.524338
[epoch9, step2672]: loss 0.587878
[epoch9, step2673]: loss 0.582369
[epoch9, step2674]: loss 0.726144
[epoch9, step2675]: loss 0.801687
[epoch9, step2676]: loss 0.620527
[epoch9, step2677]: loss 0.658017
[epoch9, step2678]: loss 0.611644
[epoch9, step2679]: loss 0.635436
[epoch9, step2680]: loss 0.403415
[epoch9, step2681]: loss 0.834223
[epoch9, step2682]: loss 0.577132
[epoch9, step2683]: loss 0.582452
[epoch9, step2684]: loss 0.307994
[epoch9, step2685]: loss 0.770912
[epoch9, step2686]: loss 0.448039
[epoch9, step2687]: loss 0.623118
[epoch9, step2688]: loss 0.510029
[epoch9, step2689]: loss 0.383907
[epoch9, step2690]: loss 0.579762
[epoch9, step2691]: loss 0.510930
[epoch9, step2692]: loss 0.731932
[epoch9, step2693]: loss 0.520253
[epoch9, step2694]: loss 0.644737
[epoch9, step2695]: loss 0.579988
[epoch9, step2696]: loss 0.445663
[epoch9, step2697]: loss 0.714406
[epoch9, step2698]: loss 0.685553
[epoch9, step2699]: loss 0.635230
[epoch9, step2700]: loss 0.364040
[epoch9, step2701]: loss 0.359564
[epoch9, step2702]: loss 0.491609
[epoch9, step2703]: loss 0.530551
[epoch9, step2704]: loss 0.678000
[epoch9, step2705]: loss 0.511270
[epoch9, step2706]: loss 0.664658
[epoch9, step2707]: loss 0.521338
[epoch9, step2708]: loss 0.678169
[epoch9, step2709]: loss 0.371562
[epoch9, step2710]: loss 0.428828
[epoch9, step2711]: loss 0.344043
[epoch9, step2712]: loss 0.629288
[epoch9, step2713]: loss 0.560355
[epoch9, step2714]: loss 0.557745
[epoch9, step2715]: loss 0.742601
[epoch9, step2716]: loss 0.574187
[epoch9, step2717]: loss 0.576035
[epoch9, step2718]: loss 0.654392
[epoch9, step2719]: loss 0.717453
[epoch9, step2720]: loss 0.653835
[epoch9, step2721]: loss 0.723363
[epoch9, step2722]: loss 0.681760
[epoch9, step2723]: loss 0.758838
[epoch9, step2724]: loss 0.625078
[epoch9, step2725]: loss 0.514795
[epoch9, step2726]: loss 0.700509
[epoch9, step2727]: loss 0.685369
[epoch9, step2728]: loss 0.408597
[epoch9, step2729]: loss 0.770032
[epoch9, step2730]: loss 0.522029
[epoch9, step2731]: loss 0.590580
[epoch9, step2732]: loss 0.827096
[epoch9, step2733]: loss 0.484761
[epoch9, step2734]: loss 0.564327
[epoch9, step2735]: loss 0.786440
[epoch9, step2736]: loss 0.442793
[epoch9, step2737]: loss 0.561046
[epoch9, step2738]: loss 0.652631
[epoch9, step2739]: loss 0.204527
[epoch9, step2740]: loss 0.583310
[epoch9, step2741]: loss 0.694156
[epoch9, step2742]: loss 0.636901
[epoch9, step2743]: loss 0.704299
[epoch9, step2744]: loss 0.746733
[epoch9, step2745]: loss 0.728734
[epoch9, step2746]: loss 0.646029
[epoch9, step2747]: loss 0.530338
[epoch9, step2748]: loss 0.384542
[epoch9, step2749]: loss 0.829501
[epoch9, step2750]: loss 0.508899
[epoch9, step2751]: loss 0.565630
[epoch9, step2752]: loss 0.553241
[epoch9, step2753]: loss 0.465239
[epoch9, step2754]: loss 0.585855
[epoch9, step2755]: loss 0.675002
[epoch9, step2756]: loss 0.587575
[epoch9, step2757]: loss 0.364896
[epoch9, step2758]: loss 0.397877
[epoch9, step2759]: loss 0.663358
[epoch9, step2760]: loss 0.548642
[epoch9, step2761]: loss 0.731061
[epoch9, step2762]: loss 0.559171
[epoch9, step2763]: loss 0.439055
[epoch9, step2764]: loss 0.370750
[epoch9, step2765]: loss 0.427869
[epoch9, step2766]: loss 0.463901
[epoch9, step2767]: loss 0.492634
[epoch9, step2768]: loss 0.667585
[epoch9, step2769]: loss 0.491755
[epoch9, step2770]: loss 0.447108
[epoch9, step2771]: loss 0.588737
[epoch9, step2772]: loss 0.790870
[epoch9, step2773]: loss 0.729384
[epoch9, step2774]: loss 0.355218
[epoch9, step2775]: loss 0.801171
[epoch9, step2776]: loss 0.499738
[epoch9, step2777]: loss 0.669869
[epoch9, step2778]: loss 0.698929
[epoch9, step2779]: loss 0.477319
[epoch9, step2780]: loss 0.267410
[epoch9, step2781]: loss 0.557565
[epoch9, step2782]: loss 0.848153
[epoch9, step2783]: loss 0.489449
[epoch9, step2784]: loss 0.641571
[epoch9, step2785]: loss 0.709710
[epoch9, step2786]: loss 0.641741
[epoch9, step2787]: loss 0.580949
[epoch9, step2788]: loss 0.331253
[epoch9, step2789]: loss 0.642843
[epoch9, step2790]: loss 0.378736
[epoch9, step2791]: loss 0.508854
[epoch9, step2792]: loss 0.381313
[epoch9, step2793]: loss 0.310103
[epoch9, step2794]: loss 0.805494
[epoch9, step2795]: loss 0.508312
[epoch9, step2796]: loss 0.734231
[epoch9, step2797]: loss 0.575085
[epoch9, step2798]: loss 0.475992
[epoch9, step2799]: loss 0.382757
[epoch9, step2800]: loss 0.611268
[epoch9, step2801]: loss 0.733395
[epoch9, step2802]: loss 0.641609
[epoch9, step2803]: loss 0.354298
[epoch9, step2804]: loss 0.418596
[epoch9, step2805]: loss 0.530600
[epoch9, step2806]: loss 0.541143
[epoch9, step2807]: loss 0.598678
[epoch9, step2808]: loss 0.670014
[epoch9, step2809]: loss 0.615456
[epoch9, step2810]: loss 0.685382
[epoch9, step2811]: loss 0.782593
[epoch9, step2812]: loss 0.764648
[epoch9, step2813]: loss 0.474646
[epoch9, step2814]: loss 0.654839
[epoch9, step2815]: loss 0.566291
[epoch9, step2816]: loss 0.821952
[epoch9, step2817]: loss 0.347046
[epoch9, step2818]: loss 0.702064
[epoch9, step2819]: loss 0.566989
[epoch9, step2820]: loss 0.657604
[epoch9, step2821]: loss 0.761373
[epoch9, step2822]: loss 0.431791
[epoch9, step2823]: loss 0.451445
[epoch9, step2824]: loss 0.540777
[epoch9, step2825]: loss 0.520509
[epoch9, step2826]: loss 0.520797
[epoch9, step2827]: loss 0.591185
[epoch9, step2828]: loss 0.838651
[epoch9, step2829]: loss 0.494818
[epoch9, step2830]: loss 0.469152
[epoch9, step2831]: loss 0.463387
[epoch9, step2832]: loss 0.329449
[epoch9, step2833]: loss 0.594371
[epoch9, step2834]: loss 0.477482
[epoch9, step2835]: loss 0.382843
[epoch9, step2836]: loss 0.743933
[epoch9, step2837]: loss 0.714792
[epoch9, step2838]: loss 0.576105
[epoch9, step2839]: loss 0.429723
[epoch9, step2840]: loss 0.588738
[epoch9, step2841]: loss 0.238268
[epoch9, step2842]: loss 0.725882
[epoch9, step2843]: loss 0.612246
[epoch9, step2844]: loss 0.668478
[epoch9, step2845]: loss 0.496012
[epoch9, step2846]: loss 0.686177
[epoch9, step2847]: loss 0.512521
[epoch9, step2848]: loss 0.748330
[epoch9, step2849]: loss 0.549255
[epoch9, step2850]: loss 0.704540
[epoch9, step2851]: loss 0.631355
[epoch9, step2852]: loss 0.792403
[epoch9, step2853]: loss 0.717703
[epoch9, step2854]: loss 0.722671
[epoch9, step2855]: loss 0.711902
[epoch9, step2856]: loss 0.619566
[epoch9, step2857]: loss 0.342436
[epoch9, step2858]: loss 0.413546
[epoch9, step2859]: loss 0.601866
[epoch9, step2860]: loss 0.558774
[epoch9, step2861]: loss 0.834767
[epoch9, step2862]: loss 0.667373
[epoch9, step2863]: loss 0.500374
[epoch9, step2864]: loss 0.601504
[epoch9, step2865]: loss 0.520753
[epoch9, step2866]: loss 0.305478
[epoch9, step2867]: loss 0.517157
[epoch9, step2868]: loss 0.312500
[epoch9, step2869]: loss 0.597144
[epoch9, step2870]: loss 0.698055
[epoch9, step2871]: loss 0.592839
[epoch9, step2872]: loss 0.352122
[epoch9, step2873]: loss 0.644355
[epoch9, step2874]: loss 0.604880
[epoch9, step2875]: loss 0.465660
[epoch9, step2876]: loss 0.177868
[epoch9, step2877]: loss 0.540144
[epoch9, step2878]: loss 0.863727
[epoch9, step2879]: loss 0.371933
[epoch9, step2880]: loss 0.779452
[epoch9, step2881]: loss 0.718517
[epoch9, step2882]: loss 0.407642
[epoch9, step2883]: loss 0.637471
[epoch9, step2884]: loss 0.367831
[epoch9, step2885]: loss 0.578565
[epoch9, step2886]: loss 0.430025
[epoch9, step2887]: loss 0.574349
[epoch9, step2888]: loss 0.488396
[epoch9, step2889]: loss 0.619478
[epoch9, step2890]: loss 0.617732
[epoch9, step2891]: loss 0.652084
[epoch9, step2892]: loss 0.456386
[epoch9, step2893]: loss 0.547509
[epoch9, step2894]: loss 0.594179
[epoch9, step2895]: loss 0.548564
[epoch9, step2896]: loss 0.266837
[epoch9, step2897]: loss 0.554860
[epoch9, step2898]: loss 0.538350
[epoch9, step2899]: loss 0.413763
[epoch9, step2900]: loss 0.726706
[epoch9, step2901]: loss 0.623001
[epoch9, step2902]: loss 0.652147
[epoch9, step2903]: loss 0.691470
[epoch9, step2904]: loss 0.689615
[epoch9, step2905]: loss 0.667331
[epoch9, step2906]: loss 0.598857
[epoch9, step2907]: loss 0.821916
[epoch9, step2908]: loss 0.437429
[epoch9, step2909]: loss 0.408252
[epoch9, step2910]: loss 0.983972
[epoch9, step2911]: loss 0.620323
[epoch9, step2912]: loss 0.767319
[epoch9, step2913]: loss 0.250150
[epoch9, step2914]: loss 0.654999
[epoch9, step2915]: loss 0.576589
[epoch9, step2916]: loss 0.690331
[epoch9, step2917]: loss 0.883962
[epoch9, step2918]: loss 0.664310
[epoch9, step2919]: loss 0.637790
[epoch9, step2920]: loss 0.647095
[epoch9, step2921]: loss 0.817467
[epoch9, step2922]: loss 0.406353
[epoch9, step2923]: loss 0.661411
[epoch9, step2924]: loss 0.341404
[epoch9, step2925]: loss 0.603783
[epoch9, step2926]: loss 0.510174
[epoch9, step2927]: loss 0.502683
[epoch9, step2928]: loss 0.556572
[epoch9, step2929]: loss 0.544494
[epoch9, step2930]: loss 0.556741
[epoch9, step2931]: loss 0.489041
[epoch9, step2932]: loss 0.675707
[epoch9, step2933]: loss 0.382582
[epoch9, step2934]: loss 0.542492
[epoch9, step2935]: loss 0.782334
[epoch9, step2936]: loss 0.742674
[epoch9, step2937]: loss 0.520789
[epoch9, step2938]: loss 0.569113
[epoch9, step2939]: loss 0.323464
[epoch9, step2940]: loss 0.374686
[epoch9, step2941]: loss 0.889700
[epoch9, step2942]: loss 0.480304
[epoch9, step2943]: loss 0.668992
[epoch9, step2944]: loss 0.365599
[epoch9, step2945]: loss 0.551274
[epoch9, step2946]: loss 0.385623
[epoch9, step2947]: loss 0.704441
[epoch9, step2948]: loss 0.484070
[epoch9, step2949]: loss 0.621006
[epoch9, step2950]: loss 0.563639
[epoch9, step2951]: loss 0.710640
[epoch9, step2952]: loss 0.536332
[epoch9, step2953]: loss 0.523758
[epoch9, step2954]: loss 0.496680
[epoch9, step2955]: loss 0.370203
[epoch9, step2956]: loss 0.637669
[epoch9, step2957]: loss 0.330316
[epoch9, step2958]: loss 0.459497
[epoch9, step2959]: loss 0.371744
[epoch9, step2960]: loss 0.687384
[epoch9, step2961]: loss 0.499020
[epoch9, step2962]: loss 0.528794
[epoch9, step2963]: loss 0.662236
[epoch9, step2964]: loss 0.463030
[epoch9, step2965]: loss 0.706613
[epoch9, step2966]: loss 0.557220
[epoch9, step2967]: loss 0.646696
[epoch9, step2968]: loss 0.595755
[epoch9, step2969]: loss 0.519779
[epoch9, step2970]: loss 0.816892
[epoch9, step2971]: loss 0.694332
[epoch9, step2972]: loss 0.662557
[epoch9, step2973]: loss 0.401237
[epoch9, step2974]: loss 0.754466
[epoch9, step2975]: loss 0.480907
[epoch9, step2976]: loss 0.474765
[epoch9, step2977]: loss 0.780728
[epoch9, step2978]: loss 0.403168
[epoch9, step2979]: loss 0.501633
[epoch9, step2980]: loss 0.566718
[epoch9, step2981]: loss 0.616466
[epoch9, step2982]: loss 0.718667
[epoch9, step2983]: loss 0.349999
[epoch9, step2984]: loss 0.468420
[epoch9, step2985]: loss 0.606083
[epoch9, step2986]: loss 0.507797
[epoch9, step2987]: loss 0.827618
[epoch9, step2988]: loss 0.422438
[epoch9, step2989]: loss 0.294635
[epoch9, step2990]: loss 0.533249
[epoch9, step2991]: loss 0.726423
[epoch9, step2992]: loss 0.720146
[epoch9, step2993]: loss 0.716471
[epoch9, step2994]: loss 0.581059
[epoch9, step2995]: loss 0.325529
[epoch9, step2996]: loss 0.552664
[epoch9, step2997]: loss 0.500786
[epoch9, step2998]: loss 0.802559
[epoch9, step2999]: loss 0.405532
[epoch9, step3000]: loss 0.827809
[epoch9, step3001]: loss 0.604782
[epoch9, step3002]: loss 0.531378
[epoch9, step3003]: loss 0.575589
[epoch9, step3004]: loss 0.678489
[epoch9, step3005]: loss 0.631932
[epoch9, step3006]: loss 0.879925
[epoch9, step3007]: loss 0.843881
[epoch9, step3008]: loss 0.575047
[epoch9, step3009]: loss 0.607810
[epoch9, step3010]: loss 0.590886
[epoch9, step3011]: loss 0.654626
[epoch9, step3012]: loss 0.534778
[epoch9, step3013]: loss 0.668513
[epoch9, step3014]: loss 0.580099
[epoch9, step3015]: loss 0.541751
[epoch9, step3016]: loss 0.602782
[epoch9, step3017]: loss 0.626895
[epoch9, step3018]: loss 0.675196
[epoch9, step3019]: loss 0.671814
[epoch9, step3020]: loss 0.713406
[epoch9, step3021]: loss 0.628658
[epoch9, step3022]: loss 0.530010
[epoch9, step3023]: loss 0.812472
[epoch9, step3024]: loss 0.236909
[epoch9, step3025]: loss 0.454020
[epoch9, step3026]: loss 0.780180
[epoch9, step3027]: loss 0.257885
[epoch9, step3028]: loss 0.373569
[epoch9, step3029]: loss 0.609598
[epoch9, step3030]: loss 0.613759
[epoch9, step3031]: loss 0.282751
[epoch9, step3032]: loss 0.509136
[epoch9, step3033]: loss 0.335782
[epoch9, step3034]: loss 0.722594
[epoch9, step3035]: loss 0.372377
[epoch9, step3036]: loss 0.690298
[epoch9, step3037]: loss 0.808727
[epoch9, step3038]: loss 0.673694
[epoch9, step3039]: loss 0.431752
[epoch9, step3040]: loss 0.700199
[epoch9, step3041]: loss 0.864921
[epoch9, step3042]: loss 0.505297
[epoch9, step3043]: loss 0.547073
[epoch9, step3044]: loss 0.738979
[epoch9, step3045]: loss 0.655154
[epoch9, step3046]: loss 0.678269
[epoch9, step3047]: loss 0.440561
[epoch9, step3048]: loss 0.497202
[epoch9, step3049]: loss 0.694407
[epoch9, step3050]: loss 0.882464
[epoch9, step3051]: loss 0.237559
[epoch9, step3052]: loss 0.532180
[epoch9, step3053]: loss 0.708798
[epoch9, step3054]: loss 0.565244
[epoch9, step3055]: loss 0.858972
[epoch9, step3056]: loss 0.654226
[epoch9, step3057]: loss 0.798818
[epoch9, step3058]: loss 0.469894
[epoch9, step3059]: loss 0.760528
[epoch9, step3060]: loss 0.410011
[epoch9, step3061]: loss 0.667342
[epoch9, step3062]: loss 0.313805
[epoch9, step3063]: loss 0.352256
[epoch9, step3064]: loss 0.942761
[epoch9, step3065]: loss 0.714560
[epoch9, step3066]: loss 0.529418
[epoch9, step3067]: loss 0.644612
[epoch9, step3068]: loss 0.499278
[epoch9, step3069]: loss 0.587790
[epoch9, step3070]: loss 0.360645
[epoch9, step3071]: loss 0.406796
[epoch9, step3072]: loss 0.726239
[epoch9, step3073]: loss 0.729241
[epoch9, step3074]: loss 0.775290
[epoch9, step3075]: loss 0.537294
[epoch9, step3076]: loss 0.736902

[epoch9]: avg loss 0.736902

[epoch10, step1]: loss 0.489767
[epoch10, step2]: loss 0.565382
[epoch10, step3]: loss 0.595062
[epoch10, step4]: loss 0.812641
[epoch10, step5]: loss 0.732637
[epoch10, step6]: loss 0.494040
[epoch10, step7]: loss 0.549702
[epoch10, step8]: loss 0.697054
[epoch10, step9]: loss 0.663800
[epoch10, step10]: loss 0.605215
[epoch10, step11]: loss 0.521265
[epoch10, step12]: loss 0.740413
[epoch10, step13]: loss 0.456413
[epoch10, step14]: loss 0.641078
[epoch10, step15]: loss 0.716349
[epoch10, step16]: loss 0.699045
[epoch10, step17]: loss 0.380664
[epoch10, step18]: loss 0.480128
[epoch10, step19]: loss 1.002603
[epoch10, step20]: loss 0.658063
[epoch10, step21]: loss 0.744341
[epoch10, step22]: loss 0.640680
[epoch10, step23]: loss 0.438601
[epoch10, step24]: loss 0.693734
[epoch10, step25]: loss 0.688482
[epoch10, step26]: loss 0.649538
[epoch10, step27]: loss 0.746267
[epoch10, step28]: loss 0.422420
[epoch10, step29]: loss 0.546727
[epoch10, step30]: loss 0.478159
[epoch10, step31]: loss 0.384466
[epoch10, step32]: loss 0.442619
[epoch10, step33]: loss 0.633262
[epoch10, step34]: loss 0.713896
[epoch10, step35]: loss 0.374121
[epoch10, step36]: loss 0.766828
[epoch10, step37]: loss 0.603020
[epoch10, step38]: loss 0.406334
[epoch10, step39]: loss 0.560108
[epoch10, step40]: loss 0.423991
[epoch10, step41]: loss 0.721335
[epoch10, step42]: loss 0.541107
[epoch10, step43]: loss 0.588839
[epoch10, step44]: loss 0.558328
[epoch10, step45]: loss 0.346899
[epoch10, step46]: loss 0.456678
[epoch10, step47]: loss 0.579717
[epoch10, step48]: loss 0.522707
[epoch10, step49]: loss 0.494281
[epoch10, step50]: loss 0.531974
[epoch10, step51]: loss 0.462212
[epoch10, step52]: loss 0.583389
[epoch10, step53]: loss 0.504748
[epoch10, step54]: loss 0.681227
[epoch10, step55]: loss 0.644606
[epoch10, step56]: loss 0.406918
[epoch10, step57]: loss 0.714399
[epoch10, step58]: loss 0.563362
[epoch10, step59]: loss 0.721776
[epoch10, step60]: loss 0.548475
[epoch10, step61]: loss 0.687804
[epoch10, step62]: loss 0.514979
[epoch10, step63]: loss 0.854576
[epoch10, step64]: loss 0.504861
[epoch10, step65]: loss 0.663754
[epoch10, step66]: loss 0.485742
[epoch10, step67]: loss 0.543552
[epoch10, step68]: loss 0.337281
[epoch10, step69]: loss 0.357923
[epoch10, step70]: loss 0.540848
[epoch10, step71]: loss 0.792165
[epoch10, step72]: loss 0.681407
[epoch10, step73]: loss 0.657588
[epoch10, step74]: loss 0.417387
[epoch10, step75]: loss 0.427302
[epoch10, step76]: loss 0.472162
[epoch10, step77]: loss 0.757150
[epoch10, step78]: loss 0.381389
[epoch10, step79]: loss 0.831677
[epoch10, step80]: loss 0.668739
[epoch10, step81]: loss 0.592149
[epoch10, step82]: loss 0.213974
[epoch10, step83]: loss 0.708186
[epoch10, step84]: loss 0.381644
[epoch10, step85]: loss 0.784139
[epoch10, step86]: loss 0.672439
[epoch10, step87]: loss 0.637073
[epoch10, step88]: loss 0.520963
[epoch10, step89]: loss 0.522901
[epoch10, step90]: loss 0.726640
[epoch10, step91]: loss 0.602616
[epoch10, step92]: loss 0.863735
[epoch10, step93]: loss 0.746094
[epoch10, step94]: loss 0.662935
[epoch10, step95]: loss 0.779488
[epoch10, step96]: loss 0.692197
[epoch10, step97]: loss 0.639322
[epoch10, step98]: loss 0.538747
[epoch10, step99]: loss 0.516876
[epoch10, step100]: loss 0.528245
[epoch10, step101]: loss 0.560274
[epoch10, step102]: loss 0.725364
[epoch10, step103]: loss 0.483024
[epoch10, step104]: loss 0.804614
[epoch10, step105]: loss 0.852457
[epoch10, step106]: loss 0.219897
[epoch10, step107]: loss 0.551817
[epoch10, step108]: loss 0.799949
[epoch10, step109]: loss 0.523864
[epoch10, step110]: loss 0.727675
[epoch10, step111]: loss 0.634877
[epoch10, step112]: loss 0.614038
[epoch10, step113]: loss 0.594071
[epoch10, step114]: loss 0.855529
[epoch10, step115]: loss 0.487089
[epoch10, step116]: loss 0.432608
[epoch10, step117]: loss 0.366438
[epoch10, step118]: loss 0.556240
[epoch10, step119]: loss 0.529139
[epoch10, step120]: loss 0.657048
[epoch10, step121]: loss 0.468562
[epoch10, step122]: loss 0.591301
[epoch10, step123]: loss 0.578744
[epoch10, step124]: loss 0.470643
[epoch10, step125]: loss 0.557588
[epoch10, step126]: loss 0.449800
[epoch10, step127]: loss 0.493333
[epoch10, step128]: loss 0.736596
[epoch10, step129]: loss 0.690712
[epoch10, step130]: loss 0.830977
[epoch10, step131]: loss 0.381394
[epoch10, step132]: loss 0.778685
[epoch10, step133]: loss 0.300921
[epoch10, step134]: loss 0.399522
[epoch10, step135]: loss 0.776056
[epoch10, step136]: loss 0.521887
[epoch10, step137]: loss 0.538827
[epoch10, step138]: loss 0.470134
[epoch10, step139]: loss 0.583651
[epoch10, step140]: loss 0.596724
[epoch10, step141]: loss 0.685959
[epoch10, step142]: loss 0.777659
[epoch10, step143]: loss 0.428804
[epoch10, step144]: loss 0.689612
[epoch10, step145]: loss 0.697365
[epoch10, step146]: loss 0.503147
[epoch10, step147]: loss 0.480079
[epoch10, step148]: loss 0.782249
[epoch10, step149]: loss 0.433452
[epoch10, step150]: loss 0.682882
[epoch10, step151]: loss 0.581660
[epoch10, step152]: loss 0.733178
[epoch10, step153]: loss 0.673507
[epoch10, step154]: loss 0.628538
[epoch10, step155]: loss 0.582516
[epoch10, step156]: loss 0.223896
[epoch10, step157]: loss 0.662197
[epoch10, step158]: loss 0.473567
[epoch10, step159]: loss 0.953683
[epoch10, step160]: loss 0.439746
[epoch10, step161]: loss 0.472376
[epoch10, step162]: loss 0.594993
[epoch10, step163]: loss 0.440705
[epoch10, step164]: loss 0.832986
[epoch10, step165]: loss 0.491992
[epoch10, step166]: loss 0.377830
[epoch10, step167]: loss 0.762510
[epoch10, step168]: loss 0.623562
[epoch10, step169]: loss 0.700008
[epoch10, step170]: loss 0.649952
[epoch10, step171]: loss 0.589714
[epoch10, step172]: loss 0.482940
[epoch10, step173]: loss 0.676981
[epoch10, step174]: loss 0.547951
[epoch10, step175]: loss 0.481607
[epoch10, step176]: loss 0.771304
[epoch10, step177]: loss 0.595629
[epoch10, step178]: loss 0.427793
[epoch10, step179]: loss 0.692880
[epoch10, step180]: loss 0.507668
[epoch10, step181]: loss 0.159159
[epoch10, step182]: loss 0.303839
[epoch10, step183]: loss 0.484457
[epoch10, step184]: loss 0.726433
[epoch10, step185]: loss 0.757458
[epoch10, step186]: loss 0.562074
[epoch10, step187]: loss 0.263772
[epoch10, step188]: loss 0.732428
[epoch10, step189]: loss 0.541564
[epoch10, step190]: loss 0.532310
[epoch10, step191]: loss 0.575145
[epoch10, step192]: loss 0.874687
[epoch10, step193]: loss 0.679891
[epoch10, step194]: loss 0.546234
[epoch10, step195]: loss 0.687861
[epoch10, step196]: loss 0.499599
[epoch10, step197]: loss 0.525015
[epoch10, step198]: loss 0.360574
[epoch10, step199]: loss 0.853780
[epoch10, step200]: loss 0.426443
[epoch10, step201]: loss 0.471545
[epoch10, step202]: loss 0.831383
[epoch10, step203]: loss 0.455828
[epoch10, step204]: loss 0.373932
[epoch10, step205]: loss 0.581340
[epoch10, step206]: loss 0.813666
[epoch10, step207]: loss 0.811599
[epoch10, step208]: loss 0.745542
[epoch10, step209]: loss 0.712903
[epoch10, step210]: loss 0.780455
[epoch10, step211]: loss 0.581926
[epoch10, step212]: loss 0.353322
[epoch10, step213]: loss 0.547380
[epoch10, step214]: loss 0.559065
[epoch10, step215]: loss 0.601596
[epoch10, step216]: loss 0.313614
[epoch10, step217]: loss 0.511183
[epoch10, step218]: loss 0.351767
[epoch10, step219]: loss 0.319414
[epoch10, step220]: loss 0.615021
[epoch10, step221]: loss 0.720847
[epoch10, step222]: loss 0.554742
[epoch10, step223]: loss 0.673040
[epoch10, step224]: loss 0.498150
[epoch10, step225]: loss 0.648974
[epoch10, step226]: loss 0.625075
[epoch10, step227]: loss 0.729562
[epoch10, step228]: loss 0.321916
[epoch10, step229]: loss 0.314117
[epoch10, step230]: loss 0.364324
[epoch10, step231]: loss 0.425720
[epoch10, step232]: loss 0.458527
[epoch10, step233]: loss 0.457465
[epoch10, step234]: loss 0.623229
[epoch10, step235]: loss 0.790712
[epoch10, step236]: loss 0.259990
[epoch10, step237]: loss 0.796036
[epoch10, step238]: loss 0.560434
[epoch10, step239]: loss 0.651231
[epoch10, step240]: loss 0.796576
[epoch10, step241]: loss 0.498071
[epoch10, step242]: loss 0.664728
[epoch10, step243]: loss 0.652303
[epoch10, step244]: loss 0.545947
[epoch10, step245]: loss 0.627753
[epoch10, step246]: loss 0.751576
[epoch10, step247]: loss 0.385385
[epoch10, step248]: loss 0.638406
[epoch10, step249]: loss 0.538191
[epoch10, step250]: loss 0.659587
[epoch10, step251]: loss 0.534264
[epoch10, step252]: loss 0.557644
[epoch10, step253]: loss 0.693127
[epoch10, step254]: loss 0.824378
[epoch10, step255]: loss 0.760677
[epoch10, step256]: loss 0.510316
[epoch10, step257]: loss 0.426967
[epoch10, step258]: loss 0.637428
[epoch10, step259]: loss 0.294347
[epoch10, step260]: loss 0.470014
[epoch10, step261]: loss 0.393615
[epoch10, step262]: loss 0.755931
[epoch10, step263]: loss 0.741020
[epoch10, step264]: loss 0.464939
[epoch10, step265]: loss 0.666737
[epoch10, step266]: loss 0.543954
[epoch10, step267]: loss 0.502850
[epoch10, step268]: loss 0.738163
[epoch10, step269]: loss 0.369888
[epoch10, step270]: loss 0.500484
[epoch10, step271]: loss 0.387661
[epoch10, step272]: loss 0.583692
[epoch10, step273]: loss 0.428624
[epoch10, step274]: loss 0.416409
[epoch10, step275]: loss 0.468841
[epoch10, step276]: loss 0.590107
[epoch10, step277]: loss 0.501678
[epoch10, step278]: loss 0.465149
[epoch10, step279]: loss 0.787991
[epoch10, step280]: loss 0.615516
[epoch10, step281]: loss 0.283727
[epoch10, step282]: loss 0.369644
[epoch10, step283]: loss 0.495182
[epoch10, step284]: loss 0.334648
[epoch10, step285]: loss 0.638020
[epoch10, step286]: loss 0.614443
[epoch10, step287]: loss 0.753948
[epoch10, step288]: loss 0.583340
[epoch10, step289]: loss 0.483437
[epoch10, step290]: loss 0.682232
[epoch10, step291]: loss 0.618111
[epoch10, step292]: loss 0.459933
[epoch10, step293]: loss 0.520518
[epoch10, step294]: loss 0.820390
[epoch10, step295]: loss 0.609419
[epoch10, step296]: loss 0.286541
[epoch10, step297]: loss 0.640400
[epoch10, step298]: loss 0.541589
[epoch10, step299]: loss 0.280604
[epoch10, step300]: loss 0.629860
[epoch10, step301]: loss 0.581404
[epoch10, step302]: loss 0.403241
[epoch10, step303]: loss 0.690346
[epoch10, step304]: loss 0.477790
[epoch10, step305]: loss 0.444645
[epoch10, step306]: loss 0.468022
[epoch10, step307]: loss 0.571401
[epoch10, step308]: loss 0.614331
[epoch10, step309]: loss 0.688569
[epoch10, step310]: loss 0.411855
[epoch10, step311]: loss 0.482546
[epoch10, step312]: loss 0.804105
[epoch10, step313]: loss 0.539296
[epoch10, step314]: loss 0.734262
[epoch10, step315]: loss 0.795927
[epoch10, step316]: loss 0.588237
[epoch10, step317]: loss 0.486985
[epoch10, step318]: loss 0.521168
[epoch10, step319]: loss 0.497826
[epoch10, step320]: loss 0.454095
[epoch10, step321]: loss 0.503902
[epoch10, step322]: loss 0.741293
[epoch10, step323]: loss 0.706056
[epoch10, step324]: loss 0.385201
[epoch10, step325]: loss 0.252977
[epoch10, step326]: loss 0.444136
[epoch10, step327]: loss 0.674137
[epoch10, step328]: loss 0.630561
[epoch10, step329]: loss 0.609000
[epoch10, step330]: loss 0.479167
[epoch10, step331]: loss 0.645028
[epoch10, step332]: loss 0.403388
[epoch10, step333]: loss 0.714812
[epoch10, step334]: loss 0.465721
[epoch10, step335]: loss 0.441028
[epoch10, step336]: loss 0.579623
[epoch10, step337]: loss 0.582706
[epoch10, step338]: loss 0.301277
[epoch10, step339]: loss 0.769690
[epoch10, step340]: loss 0.624171
[epoch10, step341]: loss 0.839107
[epoch10, step342]: loss 0.567604
[epoch10, step343]: loss 0.569553
[epoch10, step344]: loss 0.597387
[epoch10, step345]: loss 0.562434
[epoch10, step346]: loss 0.560250
[epoch10, step347]: loss 0.632014
[epoch10, step348]: loss 0.727008
[epoch10, step349]: loss 0.704665
[epoch10, step350]: loss 0.758038
[epoch10, step351]: loss 0.551750
[epoch10, step352]: loss 0.641315
[epoch10, step353]: loss 0.470688
[epoch10, step354]: loss 0.289450
[epoch10, step355]: loss 0.429187
[epoch10, step356]: loss 0.542306
[epoch10, step357]: loss 0.686631
[epoch10, step358]: loss 0.729901
[epoch10, step359]: loss 0.454594
[epoch10, step360]: loss 0.508312
[epoch10, step361]: loss 0.692515
[epoch10, step362]: loss 0.660690
[epoch10, step363]: loss 0.608557
[epoch10, step364]: loss 0.767370
[epoch10, step365]: loss 0.719160
[epoch10, step366]: loss 0.785129
[epoch10, step367]: loss 0.676919
[epoch10, step368]: loss 0.758991
[epoch10, step369]: loss 0.996501
[epoch10, step370]: loss 0.481791
[epoch10, step371]: loss 0.660456
[epoch10, step372]: loss 0.629539
[epoch10, step373]: loss 0.401056
[epoch10, step374]: loss 0.725557
[epoch10, step375]: loss 0.544805
[epoch10, step376]: loss 0.673142
[epoch10, step377]: loss 0.539559
[epoch10, step378]: loss 0.507975
[epoch10, step379]: loss 0.745194
[epoch10, step380]: loss 0.745835
[epoch10, step381]: loss 0.797617
[epoch10, step382]: loss 0.684297
[epoch10, step383]: loss 0.722089
[epoch10, step384]: loss 0.631548
[epoch10, step385]: loss 0.744306
[epoch10, step386]: loss 0.617062
[epoch10, step387]: loss 0.571988
[epoch10, step388]: loss 0.494652
[epoch10, step389]: loss 0.606536
[epoch10, step390]: loss 0.466814
[epoch10, step391]: loss 0.667331
[epoch10, step392]: loss 0.530168
[epoch10, step393]: loss 0.702172
[epoch10, step394]: loss 0.476909
[epoch10, step395]: loss 0.628744
[epoch10, step396]: loss 0.583266
[epoch10, step397]: loss 0.584055
[epoch10, step398]: loss 0.507598
[epoch10, step399]: loss 0.608033
[epoch10, step400]: loss 0.294931
[epoch10, step401]: loss 0.765314
[epoch10, step402]: loss 0.817374
[epoch10, step403]: loss 0.570629
[epoch10, step404]: loss 0.467325
[epoch10, step405]: loss 0.373164
[epoch10, step406]: loss 0.787306
[epoch10, step407]: loss 0.476758
[epoch10, step408]: loss 0.628981
[epoch10, step409]: loss 0.735568
[epoch10, step410]: loss 0.585711
[epoch10, step411]: loss 0.806940
[epoch10, step412]: loss 0.662130
[epoch10, step413]: loss 0.499734
[epoch10, step414]: loss 0.430270
[epoch10, step415]: loss 0.725100
[epoch10, step416]: loss 0.444619
[epoch10, step417]: loss 0.721778
[epoch10, step418]: loss 0.411845
[epoch10, step419]: loss 0.594033
[epoch10, step420]: loss 0.700746
[epoch10, step421]: loss 0.604640
[epoch10, step422]: loss 0.604728
[epoch10, step423]: loss 0.415032
[epoch10, step424]: loss 0.582086
[epoch10, step425]: loss 0.540800
[epoch10, step426]: loss 0.465595
[epoch10, step427]: loss 0.399085
[epoch10, step428]: loss 0.471060
[epoch10, step429]: loss 0.466707
[epoch10, step430]: loss 0.631031
[epoch10, step431]: loss 0.630375
[epoch10, step432]: loss 0.617436
[epoch10, step433]: loss 0.608200
[epoch10, step434]: loss 0.667283
[epoch10, step435]: loss 0.690663
[epoch10, step436]: loss 0.693437
[epoch10, step437]: loss 0.495174
[epoch10, step438]: loss 0.343214
[epoch10, step439]: loss 0.630956
[epoch10, step440]: loss 0.649904
[epoch10, step441]: loss 0.728085
[epoch10, step442]: loss 0.672604
[epoch10, step443]: loss 0.315678
[epoch10, step444]: loss 0.517391
[epoch10, step445]: loss 0.597659
[epoch10, step446]: loss 0.844457
[epoch10, step447]: loss 0.670845
[epoch10, step448]: loss 0.539963
[epoch10, step449]: loss 0.662117
[epoch10, step450]: loss 0.709696
[epoch10, step451]: loss 0.590069
[epoch10, step452]: loss 0.656189
[epoch10, step453]: loss 0.483470
[epoch10, step454]: loss 0.565961
[epoch10, step455]: loss 0.162201
[epoch10, step456]: loss 0.726621
[epoch10, step457]: loss 0.865575
[epoch10, step458]: loss 0.360781
[epoch10, step459]: loss 0.646024
[epoch10, step460]: loss 0.891618
[epoch10, step461]: loss 0.615141
[epoch10, step462]: loss 0.761833
[epoch10, step463]: loss 0.703938
[epoch10, step464]: loss 0.756728
[epoch10, step465]: loss 0.454079
[epoch10, step466]: loss 0.400949
[epoch10, step467]: loss 0.422844
[epoch10, step468]: loss 0.646312
[epoch10, step469]: loss 0.800599
[epoch10, step470]: loss 0.665730
[epoch10, step471]: loss 0.653221
[epoch10, step472]: loss 0.457893
[epoch10, step473]: loss 0.460811
[epoch10, step474]: loss 0.494640
[epoch10, step475]: loss 0.544734
[epoch10, step476]: loss 0.677083
[epoch10, step477]: loss 0.734452
[epoch10, step478]: loss 0.395814
[epoch10, step479]: loss 0.735014
[epoch10, step480]: loss 0.519854
[epoch10, step481]: loss 0.763865
[epoch10, step482]: loss 0.458674
[epoch10, step483]: loss 0.533747
[epoch10, step484]: loss 0.789281
[epoch10, step485]: loss 0.571243
[epoch10, step486]: loss 0.560252
[epoch10, step487]: loss 0.696678
[epoch10, step488]: loss 0.602653
[epoch10, step489]: loss 0.305293
[epoch10, step490]: loss 0.697895
[epoch10, step491]: loss 0.707792
[epoch10, step492]: loss 0.619303
[epoch10, step493]: loss 0.825058
[epoch10, step494]: loss 0.456591
[epoch10, step495]: loss 0.693160
[epoch10, step496]: loss 0.601142
[epoch10, step497]: loss 0.395007
[epoch10, step498]: loss 0.659884
[epoch10, step499]: loss 0.711794
[epoch10, step500]: loss 0.479868
[epoch10, step501]: loss 0.333066
[epoch10, step502]: loss 0.588617
[epoch10, step503]: loss 0.462135
[epoch10, step504]: loss 0.452282
[epoch10, step505]: loss 0.600418
[epoch10, step506]: loss 0.623582
[epoch10, step507]: loss 0.660499
[epoch10, step508]: loss 0.440099
[epoch10, step509]: loss 0.519841
[epoch10, step510]: loss 0.718144
[epoch10, step511]: loss 0.653099
[epoch10, step512]: loss 0.707678
[epoch10, step513]: loss 0.603969
[epoch10, step514]: loss 0.416274
[epoch10, step515]: loss 0.639363
[epoch10, step516]: loss 0.402985
[epoch10, step517]: loss 0.606066
[epoch10, step518]: loss 0.546596
[epoch10, step519]: loss 0.808659
[epoch10, step520]: loss 0.640160
[epoch10, step521]: loss 0.564792
[epoch10, step522]: loss 0.658196
[epoch10, step523]: loss 0.614375
[epoch10, step524]: loss 0.462814
[epoch10, step525]: loss 0.582617
[epoch10, step526]: loss 0.475785
[epoch10, step527]: loss 0.352880
[epoch10, step528]: loss 0.409025
[epoch10, step529]: loss 0.674891
[epoch10, step530]: loss 0.670373
[epoch10, step531]: loss 0.492542
[epoch10, step532]: loss 0.539465
[epoch10, step533]: loss 0.716160
[epoch10, step534]: loss 0.645562
[epoch10, step535]: loss 0.633299
[epoch10, step536]: loss 0.598827
[epoch10, step537]: loss 0.415336
[epoch10, step538]: loss 0.261026
[epoch10, step539]: loss 0.640335
[epoch10, step540]: loss 0.533713
[epoch10, step541]: loss 0.676835
[epoch10, step542]: loss 0.608998
[epoch10, step543]: loss 0.705100
[epoch10, step544]: loss 0.388202
[epoch10, step545]: loss 0.602868
[epoch10, step546]: loss 0.689441
[epoch10, step547]: loss 0.513209
[epoch10, step548]: loss 0.630579
[epoch10, step549]: loss 0.577589
[epoch10, step550]: loss 0.770430
[epoch10, step551]: loss 0.709884
[epoch10, step552]: loss 0.409596
[epoch10, step553]: loss 0.317457
[epoch10, step554]: loss 0.685559
[epoch10, step555]: loss 0.724438
[epoch10, step556]: loss 0.703189
[epoch10, step557]: loss 0.600598
[epoch10, step558]: loss 0.637035
[epoch10, step559]: loss 0.437409
[epoch10, step560]: loss 0.627770
[epoch10, step561]: loss 0.650376
[epoch10, step562]: loss 0.549070
[epoch10, step563]: loss 0.823651
[epoch10, step564]: loss 0.676917
[epoch10, step565]: loss 0.574045
[epoch10, step566]: loss 0.721252
[epoch10, step567]: loss 0.396138
[epoch10, step568]: loss 0.562683
[epoch10, step569]: loss 0.590090
[epoch10, step570]: loss 0.647357
[epoch10, step571]: loss 0.517475
[epoch10, step572]: loss 0.729100
[epoch10, step573]: loss 0.653036
[epoch10, step574]: loss 0.727812
[epoch10, step575]: loss 0.283291
[epoch10, step576]: loss 0.656395
[epoch10, step577]: loss 0.605216
[epoch10, step578]: loss 0.801953
[epoch10, step579]: loss 0.558095
[epoch10, step580]: loss 0.615560
[epoch10, step581]: loss 0.696370
[epoch10, step582]: loss 0.609504
[epoch10, step583]: loss 0.405484
[epoch10, step584]: loss 0.562620
[epoch10, step585]: loss 0.393412
[epoch10, step586]: loss 0.523912
[epoch10, step587]: loss 0.614046
[epoch10, step588]: loss 0.668457
[epoch10, step589]: loss 0.549510
[epoch10, step590]: loss 0.738298
[epoch10, step591]: loss 0.635180
[epoch10, step592]: loss 0.626416
[epoch10, step593]: loss 0.705885
[epoch10, step594]: loss 0.351068
[epoch10, step595]: loss 0.542701
[epoch10, step596]: loss 0.600311
[epoch10, step597]: loss 0.668004
[epoch10, step598]: loss 0.652271
[epoch10, step599]: loss 0.652122
[epoch10, step600]: loss 0.346888
[epoch10, step601]: loss 0.403744
[epoch10, step602]: loss 0.432811
[epoch10, step603]: loss 0.741905
[epoch10, step604]: loss 0.828783
[epoch10, step605]: loss 0.481934
[epoch10, step606]: loss 0.695570
[epoch10, step607]: loss 0.563414
[epoch10, step608]: loss 0.559496
[epoch10, step609]: loss 0.439646
[epoch10, step610]: loss 0.901154
[epoch10, step611]: loss 0.611797
[epoch10, step612]: loss 0.721711
[epoch10, step613]: loss 0.592459
[epoch10, step614]: loss 0.564533
[epoch10, step615]: loss 0.768264
[epoch10, step616]: loss 0.774187
[epoch10, step617]: loss 0.591954
[epoch10, step618]: loss 0.481282
[epoch10, step619]: loss 0.476978
[epoch10, step620]: loss 0.763559
[epoch10, step621]: loss 0.737663
[epoch10, step622]: loss 0.426599
[epoch10, step623]: loss 0.709187
[epoch10, step624]: loss 0.821569
[epoch10, step625]: loss 0.477696
[epoch10, step626]: loss 0.683596
[epoch10, step627]: loss 0.528632
[epoch10, step628]: loss 0.631421
[epoch10, step629]: loss 0.471028
[epoch10, step630]: loss 0.616749
[epoch10, step631]: loss 0.699810
[epoch10, step632]: loss 0.705801
[epoch10, step633]: loss 0.908543
[epoch10, step634]: loss 0.605434
[epoch10, step635]: loss 0.543451
[epoch10, step636]: loss 0.849352
[epoch10, step637]: loss 0.560792
[epoch10, step638]: loss 0.637008
[epoch10, step639]: loss 0.435169
[epoch10, step640]: loss 0.580796
[epoch10, step641]: loss 0.610385
[epoch10, step642]: loss 0.714304
[epoch10, step643]: loss 0.424858
[epoch10, step644]: loss 0.703126
[epoch10, step645]: loss 0.585426
[epoch10, step646]: loss 0.588260
[epoch10, step647]: loss 0.603241
[epoch10, step648]: loss 0.526534
[epoch10, step649]: loss 0.505668
[epoch10, step650]: loss 0.609643
[epoch10, step651]: loss 0.730752
[epoch10, step652]: loss 0.563086
[epoch10, step653]: loss 0.603094
[epoch10, step654]: loss 0.460294
[epoch10, step655]: loss 0.587079
[epoch10, step656]: loss 0.598741
[epoch10, step657]: loss 0.379507
[epoch10, step658]: loss 0.705372
[epoch10, step659]: loss 0.812775
[epoch10, step660]: loss 0.539024
[epoch10, step661]: loss 0.611633
[epoch10, step662]: loss 0.743973
[epoch10, step663]: loss 0.403737
[epoch10, step664]: loss 0.635240
[epoch10, step665]: loss 0.589032
[epoch10, step666]: loss 0.352718
[epoch10, step667]: loss 0.440739
[epoch10, step668]: loss 0.475720
[epoch10, step669]: loss 0.574571
[epoch10, step670]: loss 0.125567
[epoch10, step671]: loss 0.755888
[epoch10, step672]: loss 0.882664
[epoch10, step673]: loss 0.636299
[epoch10, step674]: loss 0.618226
[epoch10, step675]: loss 0.459997
[epoch10, step676]: loss 0.607833
[epoch10, step677]: loss 0.537896
[epoch10, step678]: loss 0.515958
[epoch10, step679]: loss 0.606268
[epoch10, step680]: loss 0.649969
[epoch10, step681]: loss 0.613729
[epoch10, step682]: loss 0.540511
[epoch10, step683]: loss 0.805466
[epoch10, step684]: loss 0.437490
[epoch10, step685]: loss 0.744202
[epoch10, step686]: loss 0.544522
[epoch10, step687]: loss 0.456679
[epoch10, step688]: loss 0.335384
[epoch10, step689]: loss 0.553500
[epoch10, step690]: loss 0.634097
[epoch10, step691]: loss 0.710633
[epoch10, step692]: loss 0.477324
[epoch10, step693]: loss 0.430064
[epoch10, step694]: loss 0.825160
[epoch10, step695]: loss 0.801611
[epoch10, step696]: loss 0.805621
[epoch10, step697]: loss 0.607957
[epoch10, step698]: loss 0.576745
[epoch10, step699]: loss 0.804369
[epoch10, step700]: loss 0.561374
[epoch10, step701]: loss 0.844986
[epoch10, step702]: loss 0.638264
[epoch10, step703]: loss 0.604239
[epoch10, step704]: loss 0.705613
[epoch10, step705]: loss 0.533100
[epoch10, step706]: loss 0.771571
[epoch10, step707]: loss 0.461876
[epoch10, step708]: loss 0.623428
[epoch10, step709]: loss 0.586393
[epoch10, step710]: loss 0.452625
[epoch10, step711]: loss 0.507406
[epoch10, step712]: loss 0.709639
[epoch10, step713]: loss 0.353233
[epoch10, step714]: loss 0.872066
[epoch10, step715]: loss 0.416105
[epoch10, step716]: loss 0.471074
[epoch10, step717]: loss 0.374565
[epoch10, step718]: loss 0.617079
[epoch10, step719]: loss 0.492254
[epoch10, step720]: loss 0.793992
[epoch10, step721]: loss 0.543154
[epoch10, step722]: loss 0.651862
[epoch10, step723]: loss 0.397974
[epoch10, step724]: loss 0.803488
[epoch10, step725]: loss 0.644034
[epoch10, step726]: loss 0.436960
[epoch10, step727]: loss 0.687720
[epoch10, step728]: loss 0.739565
[epoch10, step729]: loss 0.573924
[epoch10, step730]: loss 0.286317
[epoch10, step731]: loss 0.545434
[epoch10, step732]: loss 0.696054
[epoch10, step733]: loss 0.461891
[epoch10, step734]: loss 0.656429
[epoch10, step735]: loss 0.723231
[epoch10, step736]: loss 0.810777
[epoch10, step737]: loss 0.734496
[epoch10, step738]: loss 0.517551
[epoch10, step739]: loss 0.756262
[epoch10, step740]: loss 0.395801
[epoch10, step741]: loss 0.681754
[epoch10, step742]: loss 0.505475
[epoch10, step743]: loss 0.615791
[epoch10, step744]: loss 0.358741
[epoch10, step745]: loss 0.438810
[epoch10, step746]: loss 0.402434
[epoch10, step747]: loss 0.721442
[epoch10, step748]: loss 0.588020
[epoch10, step749]: loss 0.439093
[epoch10, step750]: loss 0.850685
[epoch10, step751]: loss 0.387335
[epoch10, step752]: loss 0.603384
[epoch10, step753]: loss 0.580917
[epoch10, step754]: loss 0.786324
[epoch10, step755]: loss 0.616667
[epoch10, step756]: loss 0.655618
[epoch10, step757]: loss 0.578235
[epoch10, step758]: loss 0.294819
[epoch10, step759]: loss 0.807484
[epoch10, step760]: loss 0.612422
[epoch10, step761]: loss 0.550523
[epoch10, step762]: loss 0.732517
[epoch10, step763]: loss 0.596649
[epoch10, step764]: loss 0.373967
[epoch10, step765]: loss 0.379169
[epoch10, step766]: loss 0.826639
[epoch10, step767]: loss 0.574513
[epoch10, step768]: loss 0.537824
[epoch10, step769]: loss 0.719971
[epoch10, step770]: loss 0.778043
[epoch10, step771]: loss 0.648601
[epoch10, step772]: loss 0.554434
[epoch10, step773]: loss 0.349109
[epoch10, step774]: loss 0.585684
[epoch10, step775]: loss 0.665694
[epoch10, step776]: loss 0.731665
[epoch10, step777]: loss 0.654830
[epoch10, step778]: loss 0.569512
[epoch10, step779]: loss 0.694812
[epoch10, step780]: loss 0.694201
[epoch10, step781]: loss 0.726905
[epoch10, step782]: loss 0.783217
[epoch10, step783]: loss 0.728423
[epoch10, step784]: loss 0.582232
[epoch10, step785]: loss 0.675402
[epoch10, step786]: loss 0.440763
[epoch10, step787]: loss 0.526988
[epoch10, step788]: loss 0.720487
[epoch10, step789]: loss 0.679041
[epoch10, step790]: loss 0.719215
[epoch10, step791]: loss 0.545003
[epoch10, step792]: loss 0.753349
[epoch10, step793]: loss 0.507038
[epoch10, step794]: loss 0.548678
[epoch10, step795]: loss 0.682503
[epoch10, step796]: loss 0.509629
[epoch10, step797]: loss 0.596356
[epoch10, step798]: loss 0.754471
[epoch10, step799]: loss 0.688179
[epoch10, step800]: loss 0.764825
[epoch10, step801]: loss 0.618292
[epoch10, step802]: loss 0.550561
[epoch10, step803]: loss 0.547081
[epoch10, step804]: loss 0.484555
[epoch10, step805]: loss 0.456591
[epoch10, step806]: loss 0.556702
[epoch10, step807]: loss 0.519022
[epoch10, step808]: loss 0.311774
[epoch10, step809]: loss 0.711994
[epoch10, step810]: loss 0.613127
[epoch10, step811]: loss 0.751414
[epoch10, step812]: loss 0.526696
[epoch10, step813]: loss 0.677507
[epoch10, step814]: loss 0.677148
[epoch10, step815]: loss 0.707033
[epoch10, step816]: loss 0.699527
[epoch10, step817]: loss 0.651549
[epoch10, step818]: loss 0.462742
[epoch10, step819]: loss 0.301311
[epoch10, step820]: loss 0.667395
[epoch10, step821]: loss 0.374747
[epoch10, step822]: loss 0.439818
[epoch10, step823]: loss 0.544434
[epoch10, step824]: loss 0.565400
[epoch10, step825]: loss 0.624552
[epoch10, step826]: loss 0.659376
[epoch10, step827]: loss 0.530400
[epoch10, step828]: loss 0.541167
[epoch10, step829]: loss 0.424458
[epoch10, step830]: loss 0.408892
[epoch10, step831]: loss 0.434539
[epoch10, step832]: loss 0.465227
[epoch10, step833]: loss 0.826859
[epoch10, step834]: loss 0.744039
[epoch10, step835]: loss 0.754994
[epoch10, step836]: loss 0.595530
[epoch10, step837]: loss 0.760285
[epoch10, step838]: loss 0.475714
[epoch10, step839]: loss 0.573976
[epoch10, step840]: loss 0.674354
[epoch10, step841]: loss 0.660001
[epoch10, step842]: loss 0.520045
[epoch10, step843]: loss 0.833768
[epoch10, step844]: loss 0.720045
[epoch10, step845]: loss 0.583686
[epoch10, step846]: loss 0.302263
[epoch10, step847]: loss 0.407653
[epoch10, step848]: loss 0.304090
[epoch10, step849]: loss 0.364471
[epoch10, step850]: loss 0.897343
[epoch10, step851]: loss 0.589055
[epoch10, step852]: loss 0.770398
[epoch10, step853]: loss 0.423986
[epoch10, step854]: loss 0.654329
[epoch10, step855]: loss 0.747346
[epoch10, step856]: loss 0.492810
[epoch10, step857]: loss 0.600778
[epoch10, step858]: loss 0.561836
[epoch10, step859]: loss 0.505432
[epoch10, step860]: loss 0.625105
[epoch10, step861]: loss 0.375220
[epoch10, step862]: loss 0.173036
[epoch10, step863]: loss 0.612434
[epoch10, step864]: loss 0.574163
[epoch10, step865]: loss 0.416929
[epoch10, step866]: loss 0.523899
[epoch10, step867]: loss 0.659090
[epoch10, step868]: loss 0.524709
[epoch10, step869]: loss 0.361968
[epoch10, step870]: loss 0.592494
[epoch10, step871]: loss 0.774890
[epoch10, step872]: loss 0.488958
[epoch10, step873]: loss 0.717852
[epoch10, step874]: loss 0.455237
[epoch10, step875]: loss 0.686470
[epoch10, step876]: loss 0.845168
[epoch10, step877]: loss 0.581804
[epoch10, step878]: loss 0.743652
[epoch10, step879]: loss 0.441495
[epoch10, step880]: loss 0.418983
[epoch10, step881]: loss 0.586541
[epoch10, step882]: loss 0.559608
[epoch10, step883]: loss 0.551705
[epoch10, step884]: loss 0.659219
[epoch10, step885]: loss 0.508494
[epoch10, step886]: loss 0.700321
[epoch10, step887]: loss 0.632500
[epoch10, step888]: loss 0.561842
[epoch10, step889]: loss 0.641333
[epoch10, step890]: loss 0.462120
[epoch10, step891]: loss 0.431350
[epoch10, step892]: loss 0.688510
[epoch10, step893]: loss 0.617765
[epoch10, step894]: loss 0.745476
[epoch10, step895]: loss 0.464970
[epoch10, step896]: loss 0.538648
[epoch10, step897]: loss 0.709690
[epoch10, step898]: loss 0.447453
[epoch10, step899]: loss 0.499299
[epoch10, step900]: loss 0.506212
[epoch10, step901]: loss 0.416173
[epoch10, step902]: loss 0.400105
[epoch10, step903]: loss 0.676160
[epoch10, step904]: loss 0.841385
[epoch10, step905]: loss 0.402991
[epoch10, step906]: loss 0.616898
[epoch10, step907]: loss 0.609536
[epoch10, step908]: loss 0.655621
[epoch10, step909]: loss 0.634021
[epoch10, step910]: loss 0.681182
[epoch10, step911]: loss 0.173818
[epoch10, step912]: loss 0.515704
[epoch10, step913]: loss 0.792325
[epoch10, step914]: loss 0.644400
[epoch10, step915]: loss 0.651431
[epoch10, step916]: loss 0.589733
[epoch10, step917]: loss 0.558408
[epoch10, step918]: loss 0.606375
[epoch10, step919]: loss 0.665910
[epoch10, step920]: loss 0.351219
[epoch10, step921]: loss 0.699391
[epoch10, step922]: loss 0.579208
[epoch10, step923]: loss 0.694501
[epoch10, step924]: loss 0.531337
[epoch10, step925]: loss 0.592377
[epoch10, step926]: loss 0.796889
[epoch10, step927]: loss 0.572728
[epoch10, step928]: loss 0.776191
[epoch10, step929]: loss 0.495542
[epoch10, step930]: loss 0.631451
[epoch10, step931]: loss 0.467247
[epoch10, step932]: loss 0.605199
[epoch10, step933]: loss 0.722272
[epoch10, step934]: loss 0.610282
[epoch10, step935]: loss 0.518684
[epoch10, step936]: loss 0.745847
[epoch10, step937]: loss 0.458788
[epoch10, step938]: loss 0.870023
[epoch10, step939]: loss 0.620282
[epoch10, step940]: loss 0.460600
[epoch10, step941]: loss 0.629636
[epoch10, step942]: loss 0.531440
[epoch10, step943]: loss 0.260503
[epoch10, step944]: loss 0.641099
[epoch10, step945]: loss 0.597648
[epoch10, step946]: loss 0.595056
[epoch10, step947]: loss 0.529214
[epoch10, step948]: loss 0.500336
[epoch10, step949]: loss 0.891018
[epoch10, step950]: loss 0.554473
[epoch10, step951]: loss 0.800511
[epoch10, step952]: loss 0.493362
[epoch10, step953]: loss 0.760193
[epoch10, step954]: loss 0.555727
[epoch10, step955]: loss 0.513468
[epoch10, step956]: loss 0.535504
[epoch10, step957]: loss 0.527824
[epoch10, step958]: loss 0.675947
[epoch10, step959]: loss 0.670439
[epoch10, step960]: loss 0.723156
[epoch10, step961]: loss 0.666015
[epoch10, step962]: loss 0.464679
[epoch10, step963]: loss 0.678790
[epoch10, step964]: loss 0.649481
[epoch10, step965]: loss 0.695067
[epoch10, step966]: loss 0.633568
[epoch10, step967]: loss 0.393083
[epoch10, step968]: loss 0.528887
[epoch10, step969]: loss 0.683422
[epoch10, step970]: loss 0.465972
[epoch10, step971]: loss 0.645539
[epoch10, step972]: loss 0.530716
[epoch10, step973]: loss 0.282776
[epoch10, step974]: loss 0.568455
[epoch10, step975]: loss 0.486873
[epoch10, step976]: loss 0.365956
[epoch10, step977]: loss 0.691435
[epoch10, step978]: loss 0.606177
[epoch10, step979]: loss 0.709148
[epoch10, step980]: loss 0.543743
[epoch10, step981]: loss 0.665151
[epoch10, step982]: loss 0.645466
[epoch10, step983]: loss 0.459511
[epoch10, step984]: loss 0.606887
[epoch10, step985]: loss 0.753432
[epoch10, step986]: loss 0.616958
[epoch10, step987]: loss 0.608052
[epoch10, step988]: loss 0.765163
[epoch10, step989]: loss 0.457945
[epoch10, step990]: loss 0.604333
[epoch10, step991]: loss 0.698660
[epoch10, step992]: loss 0.573169
[epoch10, step993]: loss 0.789793
[epoch10, step994]: loss 0.593320
[epoch10, step995]: loss 0.369522
[epoch10, step996]: loss 0.713571
[epoch10, step997]: loss 0.689856
[epoch10, step998]: loss 0.430778
[epoch10, step999]: loss 0.519934
[epoch10, step1000]: loss 0.543397
[epoch10, step1001]: loss 0.618319
[epoch10, step1002]: loss 0.408310
[epoch10, step1003]: loss 0.385187
[epoch10, step1004]: loss 0.620677
[epoch10, step1005]: loss 0.696047
[epoch10, step1006]: loss 0.318897
[epoch10, step1007]: loss 0.677209
[epoch10, step1008]: loss 0.692694
[epoch10, step1009]: loss 0.665149
[epoch10, step1010]: loss 0.526246
[epoch10, step1011]: loss 0.440106
[epoch10, step1012]: loss 0.524885
[epoch10, step1013]: loss 0.392671
[epoch10, step1014]: loss 0.730170
[epoch10, step1015]: loss 0.318047
[epoch10, step1016]: loss 0.783106
[epoch10, step1017]: loss 0.644509
[epoch10, step1018]: loss 0.539740
[epoch10, step1019]: loss 0.459016
[epoch10, step1020]: loss 0.759674
[epoch10, step1021]: loss 0.648061
[epoch10, step1022]: loss 0.557802
[epoch10, step1023]: loss 0.619606
[epoch10, step1024]: loss 0.512867
[epoch10, step1025]: loss 0.737789
[epoch10, step1026]: loss 0.600130
[epoch10, step1027]: loss 0.569258
[epoch10, step1028]: loss 0.436844
[epoch10, step1029]: loss 0.758551
[epoch10, step1030]: loss 0.594402
[epoch10, step1031]: loss 0.741380
[epoch10, step1032]: loss 0.511139
[epoch10, step1033]: loss 0.570351
[epoch10, step1034]: loss 0.669953
[epoch10, step1035]: loss 0.528330
[epoch10, step1036]: loss 0.462636
[epoch10, step1037]: loss 0.495130
[epoch10, step1038]: loss 0.537215
[epoch10, step1039]: loss 0.536020
[epoch10, step1040]: loss 0.438426
[epoch10, step1041]: loss 0.901746
[epoch10, step1042]: loss 0.784993
[epoch10, step1043]: loss 0.592535
[epoch10, step1044]: loss 0.413768
[epoch10, step1045]: loss 0.637181
[epoch10, step1046]: loss 0.788694
[epoch10, step1047]: loss 0.514022
[epoch10, step1048]: loss 0.483143
[epoch10, step1049]: loss 0.496528
[epoch10, step1050]: loss 0.736190
[epoch10, step1051]: loss 0.655699
[epoch10, step1052]: loss 0.519369
[epoch10, step1053]: loss 0.768193
[epoch10, step1054]: loss 0.894418
[epoch10, step1055]: loss 0.353929
[epoch10, step1056]: loss 0.604225
[epoch10, step1057]: loss 0.736595
[epoch10, step1058]: loss 0.673780
[epoch10, step1059]: loss 0.799006
[epoch10, step1060]: loss 0.445424
[epoch10, step1061]: loss 0.717794
[epoch10, step1062]: loss 0.503102
[epoch10, step1063]: loss 0.395380
[epoch10, step1064]: loss 0.828760
[epoch10, step1065]: loss 0.453063
[epoch10, step1066]: loss 0.653227
[epoch10, step1067]: loss 0.471226
[epoch10, step1068]: loss 0.596528
[epoch10, step1069]: loss 0.137598
[epoch10, step1070]: loss 0.146184
[epoch10, step1071]: loss 0.696896
[epoch10, step1072]: loss 0.355274
[epoch10, step1073]: loss 0.606259
[epoch10, step1074]: loss 0.662387
[epoch10, step1075]: loss 0.430761
[epoch10, step1076]: loss 0.759498
[epoch10, step1077]: loss 0.337816
[epoch10, step1078]: loss 0.401323
[epoch10, step1079]: loss 0.669669
[epoch10, step1080]: loss 0.559222
[epoch10, step1081]: loss 0.703744
[epoch10, step1082]: loss 0.608907
[epoch10, step1083]: loss 0.753115
[epoch10, step1084]: loss 0.336196
[epoch10, step1085]: loss 0.689605
[epoch10, step1086]: loss 0.705960
[epoch10, step1087]: loss 0.581776
[epoch10, step1088]: loss 0.165271
[epoch10, step1089]: loss 0.704908
[epoch10, step1090]: loss 0.508969
[epoch10, step1091]: loss 0.617949
[epoch10, step1092]: loss 0.486444
[epoch10, step1093]: loss 0.316528
[epoch10, step1094]: loss 0.183378
[epoch10, step1095]: loss 0.522898
[epoch10, step1096]: loss 0.449793
[epoch10, step1097]: loss 0.705788
[epoch10, step1098]: loss 0.799151
[epoch10, step1099]: loss 0.886616
[epoch10, step1100]: loss 0.580643
[epoch10, step1101]: loss 0.719434
[epoch10, step1102]: loss 0.512132
[epoch10, step1103]: loss 0.502117
[epoch10, step1104]: loss 0.548776
[epoch10, step1105]: loss 0.815508
[epoch10, step1106]: loss 0.625842
[epoch10, step1107]: loss 0.807393
[epoch10, step1108]: loss 0.671617
[epoch10, step1109]: loss 0.695756
[epoch10, step1110]: loss 0.749089
[epoch10, step1111]: loss 0.564836
[epoch10, step1112]: loss 0.541461
[epoch10, step1113]: loss 0.585120
[epoch10, step1114]: loss 0.574216
[epoch10, step1115]: loss 0.585831
[epoch10, step1116]: loss 0.628556
[epoch10, step1117]: loss 0.641994
[epoch10, step1118]: loss 0.600378
[epoch10, step1119]: loss 0.836452
[epoch10, step1120]: loss 0.234296
[epoch10, step1121]: loss 0.489609
[epoch10, step1122]: loss 0.350086
[epoch10, step1123]: loss 0.673395
[epoch10, step1124]: loss 0.274762
[epoch10, step1125]: loss 0.591129
[epoch10, step1126]: loss 0.347523
[epoch10, step1127]: loss 0.465562
[epoch10, step1128]: loss 0.721900
[epoch10, step1129]: loss 0.781490
[epoch10, step1130]: loss 0.602049
[epoch10, step1131]: loss 0.688283
[epoch10, step1132]: loss 0.516640
[epoch10, step1133]: loss 0.625995
[epoch10, step1134]: loss 0.479242
[epoch10, step1135]: loss 0.576803
[epoch10, step1136]: loss 0.728758
[epoch10, step1137]: loss 0.466809
[epoch10, step1138]: loss 0.679074
[epoch10, step1139]: loss 0.641715
[epoch10, step1140]: loss 0.378386
[epoch10, step1141]: loss 0.700806
[epoch10, step1142]: loss 0.601259
[epoch10, step1143]: loss 0.671080
[epoch10, step1144]: loss 0.545042
[epoch10, step1145]: loss 0.585995
[epoch10, step1146]: loss 0.758135
[epoch10, step1147]: loss 0.673379
[epoch10, step1148]: loss 0.853828
[epoch10, step1149]: loss 0.594947
[epoch10, step1150]: loss 0.269663
[epoch10, step1151]: loss 0.490338
[epoch10, step1152]: loss 0.537256
[epoch10, step1153]: loss 0.619114
[epoch10, step1154]: loss 0.497524
[epoch10, step1155]: loss 0.490708
[epoch10, step1156]: loss 0.884126
[epoch10, step1157]: loss 0.567427
[epoch10, step1158]: loss 0.730610
[epoch10, step1159]: loss 0.741490
[epoch10, step1160]: loss 0.793839
[epoch10, step1161]: loss 0.605143
[epoch10, step1162]: loss 0.625777
[epoch10, step1163]: loss 0.443389
[epoch10, step1164]: loss 0.399541
[epoch10, step1165]: loss 0.544123
[epoch10, step1166]: loss 0.607609
[epoch10, step1167]: loss 0.607355
[epoch10, step1168]: loss 0.454162
[epoch10, step1169]: loss 0.699807
[epoch10, step1170]: loss 0.841108
[epoch10, step1171]: loss 0.362483
[epoch10, step1172]: loss 0.650207
[epoch10, step1173]: loss 0.578177
[epoch10, step1174]: loss 0.520230
[epoch10, step1175]: loss 0.443292
[epoch10, step1176]: loss 0.363169
[epoch10, step1177]: loss 0.739910
[epoch10, step1178]: loss 0.800898
[epoch10, step1179]: loss 0.479620
[epoch10, step1180]: loss 0.755298
[epoch10, step1181]: loss 0.518648
[epoch10, step1182]: loss 0.699601
[epoch10, step1183]: loss 0.607768
[epoch10, step1184]: loss 0.737101
[epoch10, step1185]: loss 0.620862
[epoch10, step1186]: loss 0.645794
[epoch10, step1187]: loss 0.434409
[epoch10, step1188]: loss 0.578617
[epoch10, step1189]: loss 0.187711
[epoch10, step1190]: loss 0.478178
[epoch10, step1191]: loss 0.686985
[epoch10, step1192]: loss 0.525912
[epoch10, step1193]: loss 0.569144
[epoch10, step1194]: loss 0.459528
[epoch10, step1195]: loss 0.665527
[epoch10, step1196]: loss 0.637493
[epoch10, step1197]: loss 0.638283
[epoch10, step1198]: loss 0.407718
[epoch10, step1199]: loss 0.580955
[epoch10, step1200]: loss 0.628414
[epoch10, step1201]: loss 0.422198
[epoch10, step1202]: loss 0.891297
[epoch10, step1203]: loss 0.146419
[epoch10, step1204]: loss 0.560567
[epoch10, step1205]: loss 0.841724
[epoch10, step1206]: loss 0.617134
[epoch10, step1207]: loss 0.832891
[epoch10, step1208]: loss 0.742973
[epoch10, step1209]: loss 0.706156
[epoch10, step1210]: loss 0.604410
[epoch10, step1211]: loss 0.378180
[epoch10, step1212]: loss 0.744247
[epoch10, step1213]: loss 0.650425
[epoch10, step1214]: loss 0.680754
[epoch10, step1215]: loss 0.720884
[epoch10, step1216]: loss 0.164175
[epoch10, step1217]: loss 0.316841
[epoch10, step1218]: loss 0.628137
[epoch10, step1219]: loss 0.740995
[epoch10, step1220]: loss 0.467488
[epoch10, step1221]: loss 0.591058
[epoch10, step1222]: loss 0.685679
[epoch10, step1223]: loss 0.620506
[epoch10, step1224]: loss 0.782377
[epoch10, step1225]: loss 0.628610
[epoch10, step1226]: loss 0.429093
[epoch10, step1227]: loss 0.479839
[epoch10, step1228]: loss 0.527639
[epoch10, step1229]: loss 0.655883
[epoch10, step1230]: loss 0.680905
[epoch10, step1231]: loss 0.398528
[epoch10, step1232]: loss 0.621589
[epoch10, step1233]: loss 0.459237
[epoch10, step1234]: loss 0.544578
[epoch10, step1235]: loss 0.781817
[epoch10, step1236]: loss 0.766619
[epoch10, step1237]: loss 0.706882
[epoch10, step1238]: loss 0.540702
[epoch10, step1239]: loss 0.504851
[epoch10, step1240]: loss 0.799181
[epoch10, step1241]: loss 0.379570
[epoch10, step1242]: loss 0.682848
[epoch10, step1243]: loss 0.481554
[epoch10, step1244]: loss 0.346835
[epoch10, step1245]: loss 0.306461
[epoch10, step1246]: loss 0.443012
[epoch10, step1247]: loss 0.837864
[epoch10, step1248]: loss 0.511871
[epoch10, step1249]: loss 0.277895
[epoch10, step1250]: loss 0.293455
[epoch10, step1251]: loss 0.424199
[epoch10, step1252]: loss 0.482811
[epoch10, step1253]: loss 0.500129
[epoch10, step1254]: loss 0.345900
[epoch10, step1255]: loss 0.287892
[epoch10, step1256]: loss 0.599204
[epoch10, step1257]: loss 0.415564
[epoch10, step1258]: loss 0.508840
[epoch10, step1259]: loss 0.769937
[epoch10, step1260]: loss 0.668898
[epoch10, step1261]: loss 0.601740
[epoch10, step1262]: loss 0.418275
[epoch10, step1263]: loss 0.765939
[epoch10, step1264]: loss 0.611937
[epoch10, step1265]: loss 0.639422
[epoch10, step1266]: loss 0.618288
[epoch10, step1267]: loss 0.733278
[epoch10, step1268]: loss 0.652574
[epoch10, step1269]: loss 0.710226
[epoch10, step1270]: loss 0.542844
[epoch10, step1271]: loss 0.412884
[epoch10, step1272]: loss 0.520598
[epoch10, step1273]: loss 0.219130
[epoch10, step1274]: loss 0.449476
[epoch10, step1275]: loss 0.461698
[epoch10, step1276]: loss 0.573375
[epoch10, step1277]: loss 0.543358
[epoch10, step1278]: loss 0.522884
[epoch10, step1279]: loss 0.734711
[epoch10, step1280]: loss 0.663469
[epoch10, step1281]: loss 0.646483
[epoch10, step1282]: loss 0.612537
[epoch10, step1283]: loss 0.546713
[epoch10, step1284]: loss 0.411636
[epoch10, step1285]: loss 0.766193
[epoch10, step1286]: loss 0.592316
[epoch10, step1287]: loss 0.694865
[epoch10, step1288]: loss 0.366528
[epoch10, step1289]: loss 0.412675
[epoch10, step1290]: loss 0.694287
[epoch10, step1291]: loss 0.763844
[epoch10, step1292]: loss 0.516135
[epoch10, step1293]: loss 0.642544
[epoch10, step1294]: loss 0.682023
[epoch10, step1295]: loss 0.752373
[epoch10, step1296]: loss 0.576964
[epoch10, step1297]: loss 0.607302
[epoch10, step1298]: loss 0.547851
[epoch10, step1299]: loss 0.407582
[epoch10, step1300]: loss 0.425157
[epoch10, step1301]: loss 0.552831
[epoch10, step1302]: loss 0.708376
[epoch10, step1303]: loss 0.770056
[epoch10, step1304]: loss 0.551494
[epoch10, step1305]: loss 0.616025
[epoch10, step1306]: loss 0.476286
[epoch10, step1307]: loss 0.673081
[epoch10, step1308]: loss 0.662113
[epoch10, step1309]: loss 0.469947
[epoch10, step1310]: loss 0.410376
[epoch10, step1311]: loss 0.532675
[epoch10, step1312]: loss 0.884459
[epoch10, step1313]: loss 0.672927
[epoch10, step1314]: loss 0.516604
[epoch10, step1315]: loss 0.363061
[epoch10, step1316]: loss 0.681190
[epoch10, step1317]: loss 0.630986
[epoch10, step1318]: loss 0.499833
[epoch10, step1319]: loss 0.570370
[epoch10, step1320]: loss 0.733507
[epoch10, step1321]: loss 0.666538
[epoch10, step1322]: loss 0.696348
[epoch10, step1323]: loss 0.679850
[epoch10, step1324]: loss 0.555602
[epoch10, step1325]: loss 0.731014
[epoch10, step1326]: loss 0.549330
[epoch10, step1327]: loss 0.514495
[epoch10, step1328]: loss 0.655652
[epoch10, step1329]: loss 0.649136
[epoch10, step1330]: loss 0.319414
[epoch10, step1331]: loss 0.715795
[epoch10, step1332]: loss 0.653887
[epoch10, step1333]: loss 0.898269
[epoch10, step1334]: loss 0.450407
[epoch10, step1335]: loss 0.749504
[epoch10, step1336]: loss 0.352630
[epoch10, step1337]: loss 0.465202
[epoch10, step1338]: loss 0.696892
[epoch10, step1339]: loss 0.447906
[epoch10, step1340]: loss 0.702589
[epoch10, step1341]: loss 0.708691
[epoch10, step1342]: loss 0.352570
[epoch10, step1343]: loss 0.903822
[epoch10, step1344]: loss 0.556126
[epoch10, step1345]: loss 0.679967
[epoch10, step1346]: loss 0.411328
[epoch10, step1347]: loss 0.521743
[epoch10, step1348]: loss 0.572993
[epoch10, step1349]: loss 0.295538
[epoch10, step1350]: loss 0.547363
[epoch10, step1351]: loss 0.209141
[epoch10, step1352]: loss 0.331811
[epoch10, step1353]: loss 0.626985
[epoch10, step1354]: loss 0.464375
[epoch10, step1355]: loss 0.429525
[epoch10, step1356]: loss 0.451612
[epoch10, step1357]: loss 0.819752
[epoch10, step1358]: loss 0.654502
[epoch10, step1359]: loss 0.675901
[epoch10, step1360]: loss 0.412966
[epoch10, step1361]: loss 0.562585
[epoch10, step1362]: loss 0.572935
[epoch10, step1363]: loss 0.711814
[epoch10, step1364]: loss 0.841823
[epoch10, step1365]: loss 0.479229
[epoch10, step1366]: loss 0.674567
[epoch10, step1367]: loss 0.572713
[epoch10, step1368]: loss 0.637506
[epoch10, step1369]: loss 0.537475
[epoch10, step1370]: loss 0.486922
[epoch10, step1371]: loss 0.660873
[epoch10, step1372]: loss 0.718555
[epoch10, step1373]: loss 0.828971
[epoch10, step1374]: loss 0.523423
[epoch10, step1375]: loss 0.307695
[epoch10, step1376]: loss 0.642619
[epoch10, step1377]: loss 0.756384
[epoch10, step1378]: loss 0.456129
[epoch10, step1379]: loss 0.579445
[epoch10, step1380]: loss 0.312034
[epoch10, step1381]: loss 0.612280
[epoch10, step1382]: loss 0.627974
[epoch10, step1383]: loss 0.397038
[epoch10, step1384]: loss 0.584264
[epoch10, step1385]: loss 0.646192
[epoch10, step1386]: loss 0.773528
[epoch10, step1387]: loss 0.661553
[epoch10, step1388]: loss 0.453632
[epoch10, step1389]: loss 0.532521
[epoch10, step1390]: loss 0.589843
[epoch10, step1391]: loss 0.754767
[epoch10, step1392]: loss 0.693865
[epoch10, step1393]: loss 0.306147
[epoch10, step1394]: loss 0.411131
[epoch10, step1395]: loss 0.568470
[epoch10, step1396]: loss 0.494442
[epoch10, step1397]: loss 0.356545
[epoch10, step1398]: loss 0.631943
[epoch10, step1399]: loss 0.413841
[epoch10, step1400]: loss 0.625499
[epoch10, step1401]: loss 0.563876
[epoch10, step1402]: loss 0.558090
[epoch10, step1403]: loss 0.334786
[epoch10, step1404]: loss 0.818036
[epoch10, step1405]: loss 0.554897
[epoch10, step1406]: loss 0.684375
[epoch10, step1407]: loss 0.368527
[epoch10, step1408]: loss 0.525846
[epoch10, step1409]: loss 0.568042
[epoch10, step1410]: loss 0.683092
[epoch10, step1411]: loss 0.602989
[epoch10, step1412]: loss 0.416966
[epoch10, step1413]: loss 0.813044
[epoch10, step1414]: loss 0.683841
[epoch10, step1415]: loss 0.556302
[epoch10, step1416]: loss 0.439337
[epoch10, step1417]: loss 0.630868
[epoch10, step1418]: loss 0.782569
[epoch10, step1419]: loss 0.477252
[epoch10, step1420]: loss 0.412831
[epoch10, step1421]: loss 0.821980
[epoch10, step1422]: loss 0.463577
[epoch10, step1423]: loss 0.532073
[epoch10, step1424]: loss 0.924189
[epoch10, step1425]: loss 0.575640
[epoch10, step1426]: loss 0.529270
[epoch10, step1427]: loss 0.504591
[epoch10, step1428]: loss 0.565563
[epoch10, step1429]: loss 0.777433
[epoch10, step1430]: loss 0.706804
[epoch10, step1431]: loss 0.681927
[epoch10, step1432]: loss 0.634024
[epoch10, step1433]: loss 0.683734
[epoch10, step1434]: loss 0.632418
[epoch10, step1435]: loss 0.536005
[epoch10, step1436]: loss 0.522841
[epoch10, step1437]: loss 0.541527
[epoch10, step1438]: loss 0.682362
[epoch10, step1439]: loss 0.736405
[epoch10, step1440]: loss 0.534342
[epoch10, step1441]: loss 0.534809
[epoch10, step1442]: loss 0.663199
[epoch10, step1443]: loss 0.192368
[epoch10, step1444]: loss 0.404191
[epoch10, step1445]: loss 0.524458
[epoch10, step1446]: loss 0.396817
[epoch10, step1447]: loss 0.626795
[epoch10, step1448]: loss 0.643640
[epoch10, step1449]: loss 0.723901
[epoch10, step1450]: loss 0.683864
[epoch10, step1451]: loss 0.737726
[epoch10, step1452]: loss 0.573304
[epoch10, step1453]: loss 0.459188
[epoch10, step1454]: loss 0.806423
[epoch10, step1455]: loss 0.568979
[epoch10, step1456]: loss 0.316615
[epoch10, step1457]: loss 0.703307
[epoch10, step1458]: loss 0.654176
[epoch10, step1459]: loss 0.520876
[epoch10, step1460]: loss 0.669436
[epoch10, step1461]: loss 0.586555
[epoch10, step1462]: loss 0.677743
[epoch10, step1463]: loss 0.648774
[epoch10, step1464]: loss 0.717833
[epoch10, step1465]: loss 0.710000
[epoch10, step1466]: loss 0.563707
[epoch10, step1467]: loss 0.689553
[epoch10, step1468]: loss 0.319474
[epoch10, step1469]: loss 0.675576
[epoch10, step1470]: loss 0.620763
[epoch10, step1471]: loss 0.629749
[epoch10, step1472]: loss 0.700061
[epoch10, step1473]: loss 0.711926
[epoch10, step1474]: loss 0.750549
[epoch10, step1475]: loss 0.759825
[epoch10, step1476]: loss 0.620387
[epoch10, step1477]: loss 0.446648
[epoch10, step1478]: loss 0.583992
[epoch10, step1479]: loss 0.480583
[epoch10, step1480]: loss 0.364724
[epoch10, step1481]: loss 0.464044
[epoch10, step1482]: loss 0.746990
[epoch10, step1483]: loss 0.607565
[epoch10, step1484]: loss 0.507017
[epoch10, step1485]: loss 0.544686
[epoch10, step1486]: loss 0.402867
[epoch10, step1487]: loss 0.672747
[epoch10, step1488]: loss 0.372088
[epoch10, step1489]: loss 0.758613
[epoch10, step1490]: loss 0.134852
[epoch10, step1491]: loss 0.434767
[epoch10, step1492]: loss 0.503138
[epoch10, step1493]: loss 0.500082
[epoch10, step1494]: loss 0.568326
[epoch10, step1495]: loss 0.607567
[epoch10, step1496]: loss 0.497387
[epoch10, step1497]: loss 0.781079
[epoch10, step1498]: loss 0.527330
[epoch10, step1499]: loss 0.667091
[epoch10, step1500]: loss 0.425283
[epoch10, step1501]: loss 0.320884
[epoch10, step1502]: loss 0.541755
[epoch10, step1503]: loss 0.576676
[epoch10, step1504]: loss 0.344565
[epoch10, step1505]: loss 0.627669
[epoch10, step1506]: loss 0.584718
[epoch10, step1507]: loss 0.778163
[epoch10, step1508]: loss 0.717469
[epoch10, step1509]: loss 0.569122
[epoch10, step1510]: loss 0.408112
[epoch10, step1511]: loss 0.599716
[epoch10, step1512]: loss 0.564062
[epoch10, step1513]: loss 0.485128
[epoch10, step1514]: loss 0.553130
[epoch10, step1515]: loss 0.666411
[epoch10, step1516]: loss 0.714488
[epoch10, step1517]: loss 0.771231
[epoch10, step1518]: loss 0.418990
[epoch10, step1519]: loss 0.485947
[epoch10, step1520]: loss 0.668619
[epoch10, step1521]: loss 0.759932
[epoch10, step1522]: loss 0.722051
[epoch10, step1523]: loss 0.581704
[epoch10, step1524]: loss 0.613509
[epoch10, step1525]: loss 0.556732
[epoch10, step1526]: loss 0.605717
[epoch10, step1527]: loss 0.790091
[epoch10, step1528]: loss 0.591346
[epoch10, step1529]: loss 0.362268
[epoch10, step1530]: loss 0.589485
[epoch10, step1531]: loss 0.697106
[epoch10, step1532]: loss 0.653691
[epoch10, step1533]: loss 0.615923
[epoch10, step1534]: loss 0.280709
[epoch10, step1535]: loss 0.467176
[epoch10, step1536]: loss 0.612898
[epoch10, step1537]: loss 0.580811
[epoch10, step1538]: loss 0.735098
[epoch10, step1539]: loss 0.766095
[epoch10, step1540]: loss 0.660578
[epoch10, step1541]: loss 0.468871
[epoch10, step1542]: loss 0.563809
[epoch10, step1543]: loss 0.716105
[epoch10, step1544]: loss 0.658471
[epoch10, step1545]: loss 0.152617
[epoch10, step1546]: loss 0.616101
[epoch10, step1547]: loss 0.606682
[epoch10, step1548]: loss 0.656365
[epoch10, step1549]: loss 0.832209
[epoch10, step1550]: loss 0.491296
[epoch10, step1551]: loss 0.520021
[epoch10, step1552]: loss 0.458902
[epoch10, step1553]: loss 0.368557
[epoch10, step1554]: loss 0.583624
[epoch10, step1555]: loss 0.458902
[epoch10, step1556]: loss 0.405952
[epoch10, step1557]: loss 0.461513
[epoch10, step1558]: loss 0.490080
[epoch10, step1559]: loss 0.739979
[epoch10, step1560]: loss 0.624574
[epoch10, step1561]: loss 0.403968
[epoch10, step1562]: loss 0.545452
[epoch10, step1563]: loss 0.499646
[epoch10, step1564]: loss 0.667542
[epoch10, step1565]: loss 0.528519
[epoch10, step1566]: loss 0.748586
[epoch10, step1567]: loss 0.335792
[epoch10, step1568]: loss 0.250704
[epoch10, step1569]: loss 0.449965
[epoch10, step1570]: loss 0.736991
[epoch10, step1571]: loss 0.173384
[epoch10, step1572]: loss 0.703197
[epoch10, step1573]: loss 0.690084
[epoch10, step1574]: loss 0.499505
[epoch10, step1575]: loss 0.579971
[epoch10, step1576]: loss 0.721871
[epoch10, step1577]: loss 0.515134
[epoch10, step1578]: loss 0.536458
[epoch10, step1579]: loss 0.735076
[epoch10, step1580]: loss 0.666909
[epoch10, step1581]: loss 0.578851
[epoch10, step1582]: loss 0.649080
[epoch10, step1583]: loss 0.426763
[epoch10, step1584]: loss 0.683792
[epoch10, step1585]: loss 0.581183
[epoch10, step1586]: loss 0.764538
[epoch10, step1587]: loss 0.729496
[epoch10, step1588]: loss 0.687069
[epoch10, step1589]: loss 0.342225
[epoch10, step1590]: loss 0.436605
[epoch10, step1591]: loss 0.564391
[epoch10, step1592]: loss 0.835068
[epoch10, step1593]: loss 0.566240
[epoch10, step1594]: loss 0.403526
[epoch10, step1595]: loss 0.641438
[epoch10, step1596]: loss 0.526025
[epoch10, step1597]: loss 0.650074
[epoch10, step1598]: loss 0.349713
[epoch10, step1599]: loss 0.648699
[epoch10, step1600]: loss 0.481595
[epoch10, step1601]: loss 0.606682
[epoch10, step1602]: loss 0.806056
[epoch10, step1603]: loss 0.530807
[epoch10, step1604]: loss 0.675369
[epoch10, step1605]: loss 0.597513
[epoch10, step1606]: loss 0.477514
[epoch10, step1607]: loss 0.568811
[epoch10, step1608]: loss 0.596938
[epoch10, step1609]: loss 0.463248
[epoch10, step1610]: loss 0.886843
[epoch10, step1611]: loss 0.629073
[epoch10, step1612]: loss 0.754322
[epoch10, step1613]: loss 0.553552
[epoch10, step1614]: loss 0.618592
[epoch10, step1615]: loss 0.553504
[epoch10, step1616]: loss 0.313494
[epoch10, step1617]: loss 0.620692
[epoch10, step1618]: loss 0.463724
[epoch10, step1619]: loss 0.734190
[epoch10, step1620]: loss 0.362986
[epoch10, step1621]: loss 0.484874
[epoch10, step1622]: loss 0.621814
[epoch10, step1623]: loss 0.740319
[epoch10, step1624]: loss 0.378935
[epoch10, step1625]: loss 0.784442
[epoch10, step1626]: loss 0.394649
[epoch10, step1627]: loss 0.528782
[epoch10, step1628]: loss 0.345625
[epoch10, step1629]: loss 0.421343
[epoch10, step1630]: loss 0.764926
[epoch10, step1631]: loss 0.427143
[epoch10, step1632]: loss 0.314483
[epoch10, step1633]: loss 0.406420
[epoch10, step1634]: loss 0.476912
[epoch10, step1635]: loss 0.407167
[epoch10, step1636]: loss 0.482899
[epoch10, step1637]: loss 0.602383
[epoch10, step1638]: loss 0.537498
[epoch10, step1639]: loss 0.430831
[epoch10, step1640]: loss 0.579278
[epoch10, step1641]: loss 0.260198
[epoch10, step1642]: loss 0.622633
[epoch10, step1643]: loss 0.470391
[epoch10, step1644]: loss 0.687168
[epoch10, step1645]: loss 0.400035
[epoch10, step1646]: loss 0.477854
[epoch10, step1647]: loss 0.691019
[epoch10, step1648]: loss 0.602596
[epoch10, step1649]: loss 0.703626
[epoch10, step1650]: loss 0.767195
[epoch10, step1651]: loss 0.648400
[epoch10, step1652]: loss 0.600989
[epoch10, step1653]: loss 0.557570
[epoch10, step1654]: loss 0.508975
[epoch10, step1655]: loss 0.581402
[epoch10, step1656]: loss 0.552826
[epoch10, step1657]: loss 0.417413
[epoch10, step1658]: loss 0.741479
[epoch10, step1659]: loss 0.563300
[epoch10, step1660]: loss 0.550495
[epoch10, step1661]: loss 0.696255
[epoch10, step1662]: loss 0.669445
[epoch10, step1663]: loss 0.551430
[epoch10, step1664]: loss 0.347745
[epoch10, step1665]: loss 0.539005
[epoch10, step1666]: loss 0.675189
[epoch10, step1667]: loss 0.791879
[epoch10, step1668]: loss 0.572580
[epoch10, step1669]: loss 0.540061
[epoch10, step1670]: loss 0.771948
[epoch10, step1671]: loss 0.468062
[epoch10, step1672]: loss 0.634957
[epoch10, step1673]: loss 0.803151
[epoch10, step1674]: loss 0.516893
[epoch10, step1675]: loss 0.673880
[epoch10, step1676]: loss 0.565298
[epoch10, step1677]: loss 0.745885
[epoch10, step1678]: loss 0.617377
[epoch10, step1679]: loss 0.714490
[epoch10, step1680]: loss 0.530064
[epoch10, step1681]: loss 0.687738
[epoch10, step1682]: loss 0.324797
[epoch10, step1683]: loss 0.542726
[epoch10, step1684]: loss 0.254829
[epoch10, step1685]: loss 0.510123
[epoch10, step1686]: loss 0.671862
[epoch10, step1687]: loss 0.574042
[epoch10, step1688]: loss 0.661924
[epoch10, step1689]: loss 0.681905
[epoch10, step1690]: loss 0.716979
[epoch10, step1691]: loss 0.521946
[epoch10, step1692]: loss 0.775024
[epoch10, step1693]: loss 0.790058
[epoch10, step1694]: loss 0.785299
[epoch10, step1695]: loss 0.530619
[epoch10, step1696]: loss 0.510976
[epoch10, step1697]: loss 0.413645
[epoch10, step1698]: loss 0.429671
[epoch10, step1699]: loss 0.787702
[epoch10, step1700]: loss 0.368679
[epoch10, step1701]: loss 0.726258
[epoch10, step1702]: loss 0.834847
[epoch10, step1703]: loss 0.680608
[epoch10, step1704]: loss 0.622939
[epoch10, step1705]: loss 0.360207
[epoch10, step1706]: loss 0.644311
[epoch10, step1707]: loss 0.567913
[epoch10, step1708]: loss 0.539725
[epoch10, step1709]: loss 0.385782
[epoch10, step1710]: loss 0.650013
[epoch10, step1711]: loss 0.490606
[epoch10, step1712]: loss 0.642093
[epoch10, step1713]: loss 0.574146
[epoch10, step1714]: loss 0.493515
[epoch10, step1715]: loss 0.700359
[epoch10, step1716]: loss 0.479792
[epoch10, step1717]: loss 0.578665
[epoch10, step1718]: loss 0.419199
[epoch10, step1719]: loss 0.798692
[epoch10, step1720]: loss 0.546510
[epoch10, step1721]: loss 0.706916
[epoch10, step1722]: loss 0.487574
[epoch10, step1723]: loss 0.218055
[epoch10, step1724]: loss 0.702209
[epoch10, step1725]: loss 0.645803
[epoch10, step1726]: loss 0.691239
[epoch10, step1727]: loss 0.627349
[epoch10, step1728]: loss 0.434811
[epoch10, step1729]: loss 0.851556
[epoch10, step1730]: loss 0.459966
[epoch10, step1731]: loss 0.227176
[epoch10, step1732]: loss 0.424872
[epoch10, step1733]: loss 0.894050
[epoch10, step1734]: loss 0.664636
[epoch10, step1735]: loss 0.460129
[epoch10, step1736]: loss 0.692416
[epoch10, step1737]: loss 0.908860
[epoch10, step1738]: loss 0.673186
[epoch10, step1739]: loss 0.710255
[epoch10, step1740]: loss 0.512437
[epoch10, step1741]: loss 0.575990
[epoch10, step1742]: loss 0.426603
[epoch10, step1743]: loss 0.455861
[epoch10, step1744]: loss 0.509384
[epoch10, step1745]: loss 0.232741
[epoch10, step1746]: loss 0.589045
[epoch10, step1747]: loss 0.683472
[epoch10, step1748]: loss 0.704947
[epoch10, step1749]: loss 0.381652
[epoch10, step1750]: loss 0.663889
[epoch10, step1751]: loss 0.623270
[epoch10, step1752]: loss 0.588791
[epoch10, step1753]: loss 0.511185
[epoch10, step1754]: loss 0.741796
[epoch10, step1755]: loss 0.654626
[epoch10, step1756]: loss 0.324166
[epoch10, step1757]: loss 0.337717
[epoch10, step1758]: loss 0.453124
[epoch10, step1759]: loss 0.621208
[epoch10, step1760]: loss 0.563267
[epoch10, step1761]: loss 0.719394
[epoch10, step1762]: loss 0.392979
[epoch10, step1763]: loss 0.566267
[epoch10, step1764]: loss 0.470823
[epoch10, step1765]: loss 0.686164
[epoch10, step1766]: loss 0.452221
[epoch10, step1767]: loss 0.434656
[epoch10, step1768]: loss 0.594431
[epoch10, step1769]: loss 0.818935
[epoch10, step1770]: loss 0.477593
[epoch10, step1771]: loss 0.721859
[epoch10, step1772]: loss 0.493698
[epoch10, step1773]: loss 0.812687
[epoch10, step1774]: loss 0.812975
[epoch10, step1775]: loss 0.345798
[epoch10, step1776]: loss 0.751044
[epoch10, step1777]: loss 0.218128
[epoch10, step1778]: loss 0.467841
[epoch10, step1779]: loss 0.436250
[epoch10, step1780]: loss 0.552555
[epoch10, step1781]: loss 0.499270
[epoch10, step1782]: loss 0.448056
[epoch10, step1783]: loss 0.686706
[epoch10, step1784]: loss 0.517941
[epoch10, step1785]: loss 0.603511
[epoch10, step1786]: loss 0.399822
[epoch10, step1787]: loss 0.713853
[epoch10, step1788]: loss 0.650581
[epoch10, step1789]: loss 0.440918
[epoch10, step1790]: loss 0.628268
[epoch10, step1791]: loss 0.561586
[epoch10, step1792]: loss 0.315132
[epoch10, step1793]: loss 0.680343
[epoch10, step1794]: loss 0.521173
[epoch10, step1795]: loss 0.684111
[epoch10, step1796]: loss 0.320731
[epoch10, step1797]: loss 0.862080
[epoch10, step1798]: loss 0.349646
[epoch10, step1799]: loss 0.344568
[epoch10, step1800]: loss 0.688679
[epoch10, step1801]: loss 0.638639
[epoch10, step1802]: loss 0.584123
[epoch10, step1803]: loss 0.701857
[epoch10, step1804]: loss 0.345774
[epoch10, step1805]: loss 0.593232
[epoch10, step1806]: loss 0.803858
[epoch10, step1807]: loss 0.599375
[epoch10, step1808]: loss 0.687728
[epoch10, step1809]: loss 0.452914
[epoch10, step1810]: loss 0.495045
[epoch10, step1811]: loss 0.483065
[epoch10, step1812]: loss 0.545331
[epoch10, step1813]: loss 0.517670
[epoch10, step1814]: loss 0.584467
[epoch10, step1815]: loss 0.585798
[epoch10, step1816]: loss 0.567830
[epoch10, step1817]: loss 0.839286
[epoch10, step1818]: loss 0.613883
[epoch10, step1819]: loss 0.610251
[epoch10, step1820]: loss 0.608805
[epoch10, step1821]: loss 0.658075
[epoch10, step1822]: loss 0.433713
[epoch10, step1823]: loss 0.730654
[epoch10, step1824]: loss 0.679651
[epoch10, step1825]: loss 0.698686
[epoch10, step1826]: loss 0.502389
[epoch10, step1827]: loss 0.604851
[epoch10, step1828]: loss 0.521480
[epoch10, step1829]: loss 0.703556
[epoch10, step1830]: loss 0.830533
[epoch10, step1831]: loss 0.443916
[epoch10, step1832]: loss 0.628500
[epoch10, step1833]: loss 0.753125
[epoch10, step1834]: loss 0.621937
[epoch10, step1835]: loss 0.449100
[epoch10, step1836]: loss 0.467883
[epoch10, step1837]: loss 0.651898
[epoch10, step1838]: loss 0.415770
[epoch10, step1839]: loss 0.406973
[epoch10, step1840]: loss 0.251995
[epoch10, step1841]: loss 0.723955
[epoch10, step1842]: loss 0.535579
[epoch10, step1843]: loss 0.665849
[epoch10, step1844]: loss 0.547099
[epoch10, step1845]: loss 0.446037
[epoch10, step1846]: loss 0.697367
[epoch10, step1847]: loss 0.591314
[epoch10, step1848]: loss 0.365706
[epoch10, step1849]: loss 0.627419
[epoch10, step1850]: loss 0.426219
[epoch10, step1851]: loss 0.478787
[epoch10, step1852]: loss 0.598337
[epoch10, step1853]: loss 0.556178
[epoch10, step1854]: loss 0.626180
[epoch10, step1855]: loss 0.297884
[epoch10, step1856]: loss 0.514683
[epoch10, step1857]: loss 0.492550
[epoch10, step1858]: loss 0.512076
[epoch10, step1859]: loss 0.664447
[epoch10, step1860]: loss 0.694851
[epoch10, step1861]: loss 0.888752
[epoch10, step1862]: loss 0.814855
[epoch10, step1863]: loss 0.338105
[epoch10, step1864]: loss 0.830262
[epoch10, step1865]: loss 0.528118
[epoch10, step1866]: loss 0.760766
[epoch10, step1867]: loss 0.525646
[epoch10, step1868]: loss 0.505331
[epoch10, step1869]: loss 0.639807
[epoch10, step1870]: loss 0.379184
[epoch10, step1871]: loss 0.871525
[epoch10, step1872]: loss 0.424164
[epoch10, step1873]: loss 0.453804
[epoch10, step1874]: loss 0.617814
[epoch10, step1875]: loss 0.487581
[epoch10, step1876]: loss 0.288788
[epoch10, step1877]: loss 0.606565
[epoch10, step1878]: loss 0.525276
[epoch10, step1879]: loss 0.624466
[epoch10, step1880]: loss 0.335434
[epoch10, step1881]: loss 0.439012
[epoch10, step1882]: loss 0.658370
[epoch10, step1883]: loss 0.552468
[epoch10, step1884]: loss 0.660201
[epoch10, step1885]: loss 0.481570
[epoch10, step1886]: loss 0.734050
[epoch10, step1887]: loss 0.719210
[epoch10, step1888]: loss 0.730593
[epoch10, step1889]: loss 0.471093
[epoch10, step1890]: loss 0.598605
[epoch10, step1891]: loss 0.667551
[epoch10, step1892]: loss 0.510938
[epoch10, step1893]: loss 0.746091
[epoch10, step1894]: loss 0.520478
[epoch10, step1895]: loss 0.500126
[epoch10, step1896]: loss 0.607903
[epoch10, step1897]: loss 0.592631
[epoch10, step1898]: loss 0.512551
[epoch10, step1899]: loss 0.477276
[epoch10, step1900]: loss 0.680407
[epoch10, step1901]: loss 0.393447
[epoch10, step1902]: loss 0.492999
[epoch10, step1903]: loss 0.696065
[epoch10, step1904]: loss 0.733436
[epoch10, step1905]: loss 0.645070
[epoch10, step1906]: loss 0.590686
[epoch10, step1907]: loss 0.757131
[epoch10, step1908]: loss 0.527453
[epoch10, step1909]: loss 0.734839
[epoch10, step1910]: loss 0.787090
[epoch10, step1911]: loss 0.675471
[epoch10, step1912]: loss 0.669511
[epoch10, step1913]: loss 0.702842
[epoch10, step1914]: loss 0.794485
[epoch10, step1915]: loss 0.509480
[epoch10, step1916]: loss 0.524387
[epoch10, step1917]: loss 0.482875
[epoch10, step1918]: loss 0.502063
[epoch10, step1919]: loss 0.574394
[epoch10, step1920]: loss 0.712888
[epoch10, step1921]: loss 0.504887
[epoch10, step1922]: loss 0.534589
[epoch10, step1923]: loss 0.759296
[epoch10, step1924]: loss 0.873623
[epoch10, step1925]: loss 0.263641
[epoch10, step1926]: loss 0.762072
[epoch10, step1927]: loss 0.564783
[epoch10, step1928]: loss 0.576232
[epoch10, step1929]: loss 0.400988
[epoch10, step1930]: loss 0.504187
[epoch10, step1931]: loss 0.402229
[epoch10, step1932]: loss 0.689973
[epoch10, step1933]: loss 0.430416
[epoch10, step1934]: loss 0.722352
[epoch10, step1935]: loss 0.212533
[epoch10, step1936]: loss 0.612192
[epoch10, step1937]: loss 0.710922
[epoch10, step1938]: loss 0.615335
[epoch10, step1939]: loss 0.445352
[epoch10, step1940]: loss 0.481983
[epoch10, step1941]: loss 0.490777
[epoch10, step1942]: loss 0.654377
[epoch10, step1943]: loss 0.749442
[epoch10, step1944]: loss 0.477096
[epoch10, step1945]: loss 0.553641
[epoch10, step1946]: loss 0.440838
[epoch10, step1947]: loss 0.631176
[epoch10, step1948]: loss 0.664557
[epoch10, step1949]: loss 0.640173
[epoch10, step1950]: loss 0.664762
[epoch10, step1951]: loss 0.444135
[epoch10, step1952]: loss 0.761750
[epoch10, step1953]: loss 0.391858
[epoch10, step1954]: loss 0.513557
[epoch10, step1955]: loss 0.586779
[epoch10, step1956]: loss 0.764113
[epoch10, step1957]: loss 0.723932
[epoch10, step1958]: loss 0.680189
[epoch10, step1959]: loss 0.534812
[epoch10, step1960]: loss 0.561657
[epoch10, step1961]: loss 0.519447
[epoch10, step1962]: loss 0.525143
[epoch10, step1963]: loss 0.478352
[epoch10, step1964]: loss 0.489520
[epoch10, step1965]: loss 0.692030
[epoch10, step1966]: loss 0.544888
[epoch10, step1967]: loss 0.720666
[epoch10, step1968]: loss 0.583481
[epoch10, step1969]: loss 0.686671
[epoch10, step1970]: loss 0.853923
[epoch10, step1971]: loss 0.553491
[epoch10, step1972]: loss 0.740724
[epoch10, step1973]: loss 0.783071
[epoch10, step1974]: loss 0.477103
[epoch10, step1975]: loss 0.277887
[epoch10, step1976]: loss 0.873388
[epoch10, step1977]: loss 0.483685
[epoch10, step1978]: loss 0.455234
[epoch10, step1979]: loss 0.737235
[epoch10, step1980]: loss 0.752305
[epoch10, step1981]: loss 0.589330
[epoch10, step1982]: loss 0.770842
[epoch10, step1983]: loss 0.447377
[epoch10, step1984]: loss 0.629585
[epoch10, step1985]: loss 0.741029
[epoch10, step1986]: loss 0.667129
[epoch10, step1987]: loss 0.420134
[epoch10, step1988]: loss 0.749833
[epoch10, step1989]: loss 0.681365
[epoch10, step1990]: loss 0.595411
[epoch10, step1991]: loss 0.384624
[epoch10, step1992]: loss 0.795040
[epoch10, step1993]: loss 0.579968
[epoch10, step1994]: loss 0.549871
[epoch10, step1995]: loss 0.524782
[epoch10, step1996]: loss 0.597170
[epoch10, step1997]: loss 0.384580
[epoch10, step1998]: loss 0.730793
[epoch10, step1999]: loss 0.769694
[epoch10, step2000]: loss 0.301926
[epoch10, step2001]: loss 0.560506
[epoch10, step2002]: loss 0.702546
[epoch10, step2003]: loss 0.400833
[epoch10, step2004]: loss 0.507401
[epoch10, step2005]: loss 0.415870
[epoch10, step2006]: loss 0.197304
[epoch10, step2007]: loss 0.546148
[epoch10, step2008]: loss 0.486050
[epoch10, step2009]: loss 0.790721
[epoch10, step2010]: loss 0.987535
[epoch10, step2011]: loss 0.658081
[epoch10, step2012]: loss 0.476676
[epoch10, step2013]: loss 0.309311
[epoch10, step2014]: loss 0.345656
[epoch10, step2015]: loss 0.301578
[epoch10, step2016]: loss 0.515658
[epoch10, step2017]: loss 0.588704
[epoch10, step2018]: loss 0.698093
[epoch10, step2019]: loss 0.583679
[epoch10, step2020]: loss 0.383630
[epoch10, step2021]: loss 0.549212
[epoch10, step2022]: loss 0.506540
[epoch10, step2023]: loss 0.510931
[epoch10, step2024]: loss 0.695007
[epoch10, step2025]: loss 0.559144
[epoch10, step2026]: loss 0.704134
[epoch10, step2027]: loss 0.535199
[epoch10, step2028]: loss 0.508554
[epoch10, step2029]: loss 0.684198
[epoch10, step2030]: loss 0.666993
[epoch10, step2031]: loss 0.456673
[epoch10, step2032]: loss 0.827939
[epoch10, step2033]: loss 0.366773
[epoch10, step2034]: loss 0.677243
[epoch10, step2035]: loss 0.675691
[epoch10, step2036]: loss 0.695730
[epoch10, step2037]: loss 0.455856
[epoch10, step2038]: loss 0.666892
[epoch10, step2039]: loss 0.687252
[epoch10, step2040]: loss 0.536513
[epoch10, step2041]: loss 0.670386
[epoch10, step2042]: loss 0.638879
[epoch10, step2043]: loss 0.608107
[epoch10, step2044]: loss 0.682612
[epoch10, step2045]: loss 0.196708
[epoch10, step2046]: loss 0.559905
[epoch10, step2047]: loss 0.687182
[epoch10, step2048]: loss 0.535002
[epoch10, step2049]: loss 0.669428
[epoch10, step2050]: loss 0.476215
[epoch10, step2051]: loss 0.744007
[epoch10, step2052]: loss 0.643085
[epoch10, step2053]: loss 0.650944
[epoch10, step2054]: loss 0.489750
[epoch10, step2055]: loss 0.566659
[epoch10, step2056]: loss 0.764274
[epoch10, step2057]: loss 0.595871
[epoch10, step2058]: loss 0.712928
[epoch10, step2059]: loss 0.725347
[epoch10, step2060]: loss 0.609789
[epoch10, step2061]: loss 0.464148
[epoch10, step2062]: loss 0.460476
[epoch10, step2063]: loss 0.557244
[epoch10, step2064]: loss 0.615755
[epoch10, step2065]: loss 0.410984
[epoch10, step2066]: loss 0.539084
[epoch10, step2067]: loss 0.353553
[epoch10, step2068]: loss 0.628226
[epoch10, step2069]: loss 0.675262
[epoch10, step2070]: loss 0.592071
[epoch10, step2071]: loss 0.531170
[epoch10, step2072]: loss 0.663861
[epoch10, step2073]: loss 0.645531
[epoch10, step2074]: loss 0.576101
[epoch10, step2075]: loss 0.707252
[epoch10, step2076]: loss 0.315923
[epoch10, step2077]: loss 0.733126
[epoch10, step2078]: loss 0.700041
[epoch10, step2079]: loss 0.836379
[epoch10, step2080]: loss 0.697513
[epoch10, step2081]: loss 0.515481
[epoch10, step2082]: loss 0.821385
[epoch10, step2083]: loss 0.553785
[epoch10, step2084]: loss 0.460736
[epoch10, step2085]: loss 0.569840
[epoch10, step2086]: loss 0.508470
[epoch10, step2087]: loss 0.400076
[epoch10, step2088]: loss 0.600763
[epoch10, step2089]: loss 0.484821
[epoch10, step2090]: loss 0.491591
[epoch10, step2091]: loss 0.645057
[epoch10, step2092]: loss 0.504799
[epoch10, step2093]: loss 0.672101
[epoch10, step2094]: loss 0.424991
[epoch10, step2095]: loss 0.557979
[epoch10, step2096]: loss 0.497851
[epoch10, step2097]: loss 0.395708
[epoch10, step2098]: loss 0.712415
[epoch10, step2099]: loss 0.593289
[epoch10, step2100]: loss 0.588187
[epoch10, step2101]: loss 0.376142
[epoch10, step2102]: loss 0.790731
[epoch10, step2103]: loss 0.486262
[epoch10, step2104]: loss 0.485463
[epoch10, step2105]: loss 0.567705
[epoch10, step2106]: loss 0.440911
[epoch10, step2107]: loss 0.710196
[epoch10, step2108]: loss 0.734439
[epoch10, step2109]: loss 0.662812
[epoch10, step2110]: loss 0.443893
[epoch10, step2111]: loss 0.651215
[epoch10, step2112]: loss 0.729173
[epoch10, step2113]: loss 0.643267
[epoch10, step2114]: loss 0.704113
[epoch10, step2115]: loss 0.641229
[epoch10, step2116]: loss 0.492541
[epoch10, step2117]: loss 0.739563
[epoch10, step2118]: loss 0.712370
[epoch10, step2119]: loss 0.675460
[epoch10, step2120]: loss 0.663574
[epoch10, step2121]: loss 0.492270
[epoch10, step2122]: loss 0.611758
[epoch10, step2123]: loss 0.696383
[epoch10, step2124]: loss 0.629272
[epoch10, step2125]: loss 0.702932
[epoch10, step2126]: loss 0.346619
[epoch10, step2127]: loss 0.593397
[epoch10, step2128]: loss 0.417250
[epoch10, step2129]: loss 0.514691
[epoch10, step2130]: loss 0.119504
[epoch10, step2131]: loss 0.644373
[epoch10, step2132]: loss 0.639814
[epoch10, step2133]: loss 0.619760
[epoch10, step2134]: loss 0.505233
[epoch10, step2135]: loss 0.582070
[epoch10, step2136]: loss 0.708645
[epoch10, step2137]: loss 0.599264
[epoch10, step2138]: loss 0.583161
[epoch10, step2139]: loss 0.457998
[epoch10, step2140]: loss 0.685799
[epoch10, step2141]: loss 0.614505
[epoch10, step2142]: loss 0.563062
[epoch10, step2143]: loss 0.345979
[epoch10, step2144]: loss 0.857381
[epoch10, step2145]: loss 0.269266
[epoch10, step2146]: loss 0.348263
[epoch10, step2147]: loss 0.506896
[epoch10, step2148]: loss 0.705525
[epoch10, step2149]: loss 0.622178
[epoch10, step2150]: loss 0.418001
[epoch10, step2151]: loss 0.493469
[epoch10, step2152]: loss 0.664598
[epoch10, step2153]: loss 0.951389
[epoch10, step2154]: loss 0.760117
[epoch10, step2155]: loss 0.565523
[epoch10, step2156]: loss 0.737543
[epoch10, step2157]: loss 0.591046
[epoch10, step2158]: loss 0.612843
[epoch10, step2159]: loss 0.339009
[epoch10, step2160]: loss 0.382325
[epoch10, step2161]: loss 0.689317
[epoch10, step2162]: loss 0.675855
[epoch10, step2163]: loss 0.608977
[epoch10, step2164]: loss 0.660206
[epoch10, step2165]: loss 0.683346
[epoch10, step2166]: loss 0.708167
[epoch10, step2167]: loss 0.609110
[epoch10, step2168]: loss 0.506105
[epoch10, step2169]: loss 0.481669
[epoch10, step2170]: loss 0.336609
[epoch10, step2171]: loss 0.805999
[epoch10, step2172]: loss 0.718138
[epoch10, step2173]: loss 0.606855
[epoch10, step2174]: loss 0.523030
[epoch10, step2175]: loss 0.708233
[epoch10, step2176]: loss 0.503627
[epoch10, step2177]: loss 0.608661
[epoch10, step2178]: loss 0.553820
[epoch10, step2179]: loss 0.726656
[epoch10, step2180]: loss 0.505386
[epoch10, step2181]: loss 0.728025
[epoch10, step2182]: loss 0.309585
[epoch10, step2183]: loss 0.589416
[epoch10, step2184]: loss 0.639158
[epoch10, step2185]: loss 0.548623
[epoch10, step2186]: loss 0.590083
[epoch10, step2187]: loss 0.498870
[epoch10, step2188]: loss 0.455672
[epoch10, step2189]: loss 0.583171
[epoch10, step2190]: loss 0.453535
[epoch10, step2191]: loss 0.701392
[epoch10, step2192]: loss 0.487850
[epoch10, step2193]: loss 0.514880
[epoch10, step2194]: loss 0.810349
[epoch10, step2195]: loss 0.597835
[epoch10, step2196]: loss 0.525180
[epoch10, step2197]: loss 0.521693
[epoch10, step2198]: loss 0.612522
[epoch10, step2199]: loss 0.597885
[epoch10, step2200]: loss 0.448621
[epoch10, step2201]: loss 0.461998
[epoch10, step2202]: loss 0.842652
[epoch10, step2203]: loss 0.759614
[epoch10, step2204]: loss 0.192980
[epoch10, step2205]: loss 0.395335
[epoch10, step2206]: loss 0.904962
[epoch10, step2207]: loss 0.511343
[epoch10, step2208]: loss 0.652176
[epoch10, step2209]: loss 0.330809
[epoch10, step2210]: loss 0.617881
[epoch10, step2211]: loss 0.592480
[epoch10, step2212]: loss 0.577870
[epoch10, step2213]: loss 0.753926
[epoch10, step2214]: loss 0.721902
[epoch10, step2215]: loss 0.583210
[epoch10, step2216]: loss 0.367845
[epoch10, step2217]: loss 0.560345
[epoch10, step2218]: loss 0.546421
[epoch10, step2219]: loss 0.417854
[epoch10, step2220]: loss 0.624043
[epoch10, step2221]: loss 0.396189
[epoch10, step2222]: loss 0.648852
[epoch10, step2223]: loss 0.193873
[epoch10, step2224]: loss 0.764598
[epoch10, step2225]: loss 0.181463
[epoch10, step2226]: loss 0.628131
[epoch10, step2227]: loss 0.459271
[epoch10, step2228]: loss 0.610606
[epoch10, step2229]: loss 0.812631
[epoch10, step2230]: loss 0.879739
[epoch10, step2231]: loss 0.633385
[epoch10, step2232]: loss 0.636673
[epoch10, step2233]: loss 0.196728
[epoch10, step2234]: loss 0.413708
[epoch10, step2235]: loss 0.867800
[epoch10, step2236]: loss 0.515659
[epoch10, step2237]: loss 0.429955
[epoch10, step2238]: loss 0.549920
[epoch10, step2239]: loss 0.597669
[epoch10, step2240]: loss 0.519409
[epoch10, step2241]: loss 0.717828
[epoch10, step2242]: loss 0.715667
[epoch10, step2243]: loss 0.409929
[epoch10, step2244]: loss 0.349264
[epoch10, step2245]: loss 0.316972
[epoch10, step2246]: loss 0.741923
[epoch10, step2247]: loss 0.551541
[epoch10, step2248]: loss 0.755536
[epoch10, step2249]: loss 0.515598
[epoch10, step2250]: loss 0.910360
[epoch10, step2251]: loss 0.753526
[epoch10, step2252]: loss 0.229921
[epoch10, step2253]: loss 0.377916
[epoch10, step2254]: loss 0.629794
[epoch10, step2255]: loss 0.667287
[epoch10, step2256]: loss 0.566729
[epoch10, step2257]: loss 0.398890
[epoch10, step2258]: loss 0.774323
[epoch10, step2259]: loss 0.626490
[epoch10, step2260]: loss 0.691473
[epoch10, step2261]: loss 0.485730
[epoch10, step2262]: loss 0.522949
[epoch10, step2263]: loss 0.714168
[epoch10, step2264]: loss 0.544771
[epoch10, step2265]: loss 0.661901
[epoch10, step2266]: loss 0.769658
[epoch10, step2267]: loss 0.544277
[epoch10, step2268]: loss 0.549164
[epoch10, step2269]: loss 0.574341
[epoch10, step2270]: loss 0.596515
[epoch10, step2271]: loss 0.613993
[epoch10, step2272]: loss 0.604445
[epoch10, step2273]: loss 0.553428
[epoch10, step2274]: loss 0.802272
[epoch10, step2275]: loss 0.641028
[epoch10, step2276]: loss 0.553697
[epoch10, step2277]: loss 0.619615
[epoch10, step2278]: loss 0.258306
[epoch10, step2279]: loss 0.706943
[epoch10, step2280]: loss 0.480837
[epoch10, step2281]: loss 0.519423
[epoch10, step2282]: loss 0.572205
[epoch10, step2283]: loss 0.456725
[epoch10, step2284]: loss 0.567456
[epoch10, step2285]: loss 0.479604
[epoch10, step2286]: loss 0.450436
[epoch10, step2287]: loss 0.445450
[epoch10, step2288]: loss 0.609592
[epoch10, step2289]: loss 0.888182
[epoch10, step2290]: loss 0.690498
[epoch10, step2291]: loss 0.638099
[epoch10, step2292]: loss 0.836868
[epoch10, step2293]: loss 0.625718
[epoch10, step2294]: loss 0.705939
[epoch10, step2295]: loss 0.431714
[epoch10, step2296]: loss 0.180404
[epoch10, step2297]: loss 0.589346
[epoch10, step2298]: loss 0.667434
[epoch10, step2299]: loss 0.909616
[epoch10, step2300]: loss 0.573165
[epoch10, step2301]: loss 0.568506
[epoch10, step2302]: loss 0.409858
[epoch10, step2303]: loss 0.463597
[epoch10, step2304]: loss 0.437299
[epoch10, step2305]: loss 0.541492
[epoch10, step2306]: loss 0.673864
[epoch10, step2307]: loss 0.592053
[epoch10, step2308]: loss 0.419149
[epoch10, step2309]: loss 0.275648
[epoch10, step2310]: loss 0.521624
[epoch10, step2311]: loss 0.690550
[epoch10, step2312]: loss 0.541355
[epoch10, step2313]: loss 0.480596
[epoch10, step2314]: loss 0.654276
[epoch10, step2315]: loss 0.639928
[epoch10, step2316]: loss 0.472198
[epoch10, step2317]: loss 0.587109
[epoch10, step2318]: loss 0.265501
[epoch10, step2319]: loss 0.551479
[epoch10, step2320]: loss 0.427739
[epoch10, step2321]: loss 0.643147
[epoch10, step2322]: loss 0.584314
[epoch10, step2323]: loss 0.842346
[epoch10, step2324]: loss 0.623537
[epoch10, step2325]: loss 0.858074
[epoch10, step2326]: loss 0.686770
[epoch10, step2327]: loss 0.718448
[epoch10, step2328]: loss 0.687168
[epoch10, step2329]: loss 0.669521
[epoch10, step2330]: loss 0.569448
[epoch10, step2331]: loss 0.501789
[epoch10, step2332]: loss 0.610249
[epoch10, step2333]: loss 0.448654
[epoch10, step2334]: loss 0.469199
[epoch10, step2335]: loss 0.530648
[epoch10, step2336]: loss 0.831567
[epoch10, step2337]: loss 0.304013
[epoch10, step2338]: loss 0.524205
[epoch10, step2339]: loss 0.677286
[epoch10, step2340]: loss 0.569309
[epoch10, step2341]: loss 0.514299
[epoch10, step2342]: loss 0.564183
[epoch10, step2343]: loss 0.427321
[epoch10, step2344]: loss 0.729927
[epoch10, step2345]: loss 0.704543
[epoch10, step2346]: loss 0.591785
[epoch10, step2347]: loss 0.478899
[epoch10, step2348]: loss 0.563691
[epoch10, step2349]: loss 0.821302
[epoch10, step2350]: loss 0.583258
[epoch10, step2351]: loss 0.870580
[epoch10, step2352]: loss 0.611554
[epoch10, step2353]: loss 0.516286
[epoch10, step2354]: loss 0.613162
[epoch10, step2355]: loss 0.638645
[epoch10, step2356]: loss 0.698797
[epoch10, step2357]: loss 0.148895
[epoch10, step2358]: loss 0.439601
[epoch10, step2359]: loss 0.737575
[epoch10, step2360]: loss 0.649864
[epoch10, step2361]: loss 0.731882
[epoch10, step2362]: loss 0.508793
[epoch10, step2363]: loss 0.593224
[epoch10, step2364]: loss 0.589900
[epoch10, step2365]: loss 0.643474
[epoch10, step2366]: loss 0.296563
[epoch10, step2367]: loss 0.652086
[epoch10, step2368]: loss 0.596298
[epoch10, step2369]: loss 0.527922
[epoch10, step2370]: loss 0.504792
[epoch10, step2371]: loss 0.740235
[epoch10, step2372]: loss 0.705700
[epoch10, step2373]: loss 0.768336
[epoch10, step2374]: loss 0.635522
[epoch10, step2375]: loss 0.494051
[epoch10, step2376]: loss 0.720482
[epoch10, step2377]: loss 0.809902
[epoch10, step2378]: loss 0.881663
[epoch10, step2379]: loss 0.361366
[epoch10, step2380]: loss 0.692093
[epoch10, step2381]: loss 0.671097
[epoch10, step2382]: loss 0.444471
[epoch10, step2383]: loss 0.684833
[epoch10, step2384]: loss 0.572451
[epoch10, step2385]: loss 0.523877
[epoch10, step2386]: loss 0.483906
[epoch10, step2387]: loss 0.861892
[epoch10, step2388]: loss 0.723870
[epoch10, step2389]: loss 0.831137
[epoch10, step2390]: loss 0.480381
[epoch10, step2391]: loss 0.642113
[epoch10, step2392]: loss 0.277433
[epoch10, step2393]: loss 0.638900
[epoch10, step2394]: loss 0.725625
[epoch10, step2395]: loss 0.208347
[epoch10, step2396]: loss 0.730025
[epoch10, step2397]: loss 0.513990
[epoch10, step2398]: loss 0.683769
[epoch10, step2399]: loss 0.426782
[epoch10, step2400]: loss 0.550012
[epoch10, step2401]: loss 0.440354
[epoch10, step2402]: loss 0.750900
[epoch10, step2403]: loss 0.789527
[epoch10, step2404]: loss 0.890713
[epoch10, step2405]: loss 0.710549
[epoch10, step2406]: loss 0.604584
[epoch10, step2407]: loss 0.490216
[epoch10, step2408]: loss 0.691979
[epoch10, step2409]: loss 0.737541
[epoch10, step2410]: loss 0.806971
[epoch10, step2411]: loss 0.723044
[epoch10, step2412]: loss 0.296864
[epoch10, step2413]: loss 0.870388
[epoch10, step2414]: loss 0.527331
[epoch10, step2415]: loss 0.863262
[epoch10, step2416]: loss 0.606069
[epoch10, step2417]: loss 0.339725
[epoch10, step2418]: loss 0.538359
[epoch10, step2419]: loss 0.615144
[epoch10, step2420]: loss 0.779609
[epoch10, step2421]: loss 0.258167
[epoch10, step2422]: loss 0.751949
[epoch10, step2423]: loss 0.699975
[epoch10, step2424]: loss 0.574355
[epoch10, step2425]: loss 0.280501
[epoch10, step2426]: loss 0.640014
[epoch10, step2427]: loss 0.369795
[epoch10, step2428]: loss 0.680367
[epoch10, step2429]: loss 0.163650
[epoch10, step2430]: loss 0.534005
[epoch10, step2431]: loss 0.264243
[epoch10, step2432]: loss 0.619279
[epoch10, step2433]: loss 0.708620
[epoch10, step2434]: loss 0.679268
[epoch10, step2435]: loss 0.520195
[epoch10, step2436]: loss 0.688498
[epoch10, step2437]: loss 0.612973
[epoch10, step2438]: loss 0.717408
[epoch10, step2439]: loss 0.588200
[epoch10, step2440]: loss 0.718863
[epoch10, step2441]: loss 0.324385
[epoch10, step2442]: loss 0.577516
[epoch10, step2443]: loss 0.500238
[epoch10, step2444]: loss 0.707231
[epoch10, step2445]: loss 0.707413
[epoch10, step2446]: loss 0.534293
[epoch10, step2447]: loss 0.491878
[epoch10, step2448]: loss 0.436757
[epoch10, step2449]: loss 0.399163
[epoch10, step2450]: loss 0.695860
[epoch10, step2451]: loss 0.884183
[epoch10, step2452]: loss 0.704295
[epoch10, step2453]: loss 0.859992
[epoch10, step2454]: loss 0.544969
[epoch10, step2455]: loss 0.487939
[epoch10, step2456]: loss 0.661299
[epoch10, step2457]: loss 0.428382
[epoch10, step2458]: loss 0.748227
[epoch10, step2459]: loss 0.725986
[epoch10, step2460]: loss 0.582065
[epoch10, step2461]: loss 0.494613
[epoch10, step2462]: loss 0.771276
[epoch10, step2463]: loss 0.732700
[epoch10, step2464]: loss 0.949032
[epoch10, step2465]: loss 0.499089
[epoch10, step2466]: loss 0.200236
[epoch10, step2467]: loss 0.552511
[epoch10, step2468]: loss 0.785857
[epoch10, step2469]: loss 0.511079
[epoch10, step2470]: loss 0.677360
[epoch10, step2471]: loss 0.566683
[epoch10, step2472]: loss 0.658952
[epoch10, step2473]: loss 0.423595
[epoch10, step2474]: loss 0.691221
[epoch10, step2475]: loss 0.369610
[epoch10, step2476]: loss 0.596671
[epoch10, step2477]: loss 0.604170
[epoch10, step2478]: loss 0.556651
[epoch10, step2479]: loss 0.293991
[epoch10, step2480]: loss 0.481387
[epoch10, step2481]: loss 0.776941
[epoch10, step2482]: loss 0.640498
[epoch10, step2483]: loss 0.454713
[epoch10, step2484]: loss 0.657655
[epoch10, step2485]: loss 0.596838
[epoch10, step2486]: loss 0.456919
[epoch10, step2487]: loss 0.676272
[epoch10, step2488]: loss 0.716298
[epoch10, step2489]: loss 0.583749
[epoch10, step2490]: loss 0.727443
[epoch10, step2491]: loss 0.570284
[epoch10, step2492]: loss 0.430607
[epoch10, step2493]: loss 0.406225
[epoch10, step2494]: loss 0.586884
[epoch10, step2495]: loss 0.606331
[epoch10, step2496]: loss 0.711740
[epoch10, step2497]: loss 0.871026
[epoch10, step2498]: loss 0.674178
[epoch10, step2499]: loss 0.718529
[epoch10, step2500]: loss 0.565471
[epoch10, step2501]: loss 0.429513
[epoch10, step2502]: loss 0.731804
[epoch10, step2503]: loss 0.163024
[epoch10, step2504]: loss 0.686699
[epoch10, step2505]: loss 0.533358
[epoch10, step2506]: loss 0.593892
[epoch10, step2507]: loss 0.761202
[epoch10, step2508]: loss 0.698249
[epoch10, step2509]: loss 0.642567
[epoch10, step2510]: loss 0.718770
[epoch10, step2511]: loss 0.776963
[epoch10, step2512]: loss 0.651324
[epoch10, step2513]: loss 0.710458
[epoch10, step2514]: loss 0.658684
[epoch10, step2515]: loss 0.665861
[epoch10, step2516]: loss 0.697619
[epoch10, step2517]: loss 0.654428
[epoch10, step2518]: loss 0.541102
[epoch10, step2519]: loss 0.808346
[epoch10, step2520]: loss 0.279509
[epoch10, step2521]: loss 0.412586
[epoch10, step2522]: loss 0.668766
[epoch10, step2523]: loss 0.633078
[epoch10, step2524]: loss 0.249200
[epoch10, step2525]: loss 0.711446
[epoch10, step2526]: loss 0.675687
[epoch10, step2527]: loss 0.611613
[epoch10, step2528]: loss 0.506390
[epoch10, step2529]: loss 0.512587
[epoch10, step2530]: loss 0.446584
[epoch10, step2531]: loss 0.561224
[epoch10, step2532]: loss 0.691368
[epoch10, step2533]: loss 0.565898
[epoch10, step2534]: loss 0.563059
[epoch10, step2535]: loss 0.660008
[epoch10, step2536]: loss 0.579365
[epoch10, step2537]: loss 0.504536
[epoch10, step2538]: loss 0.785083
[epoch10, step2539]: loss 0.682676
[epoch10, step2540]: loss 0.674964
[epoch10, step2541]: loss 0.451410
[epoch10, step2542]: loss 0.418950
[epoch10, step2543]: loss 0.763340
[epoch10, step2544]: loss 0.614151
[epoch10, step2545]: loss 0.523384
[epoch10, step2546]: loss 0.765382
[epoch10, step2547]: loss 0.685018
[epoch10, step2548]: loss 0.821395
[epoch10, step2549]: loss 0.886373
[epoch10, step2550]: loss 0.680164
[epoch10, step2551]: loss 0.413002
[epoch10, step2552]: loss 0.499447
[epoch10, step2553]: loss 0.325863
[epoch10, step2554]: loss 0.698989
[epoch10, step2555]: loss 0.602531
[epoch10, step2556]: loss 0.582983
[epoch10, step2557]: loss 0.660749
[epoch10, step2558]: loss 0.737202
[epoch10, step2559]: loss 0.805298
[epoch10, step2560]: loss 0.781522
[epoch10, step2561]: loss 0.499446
[epoch10, step2562]: loss 0.874753
[epoch10, step2563]: loss 0.422050
[epoch10, step2564]: loss 0.777219
[epoch10, step2565]: loss 0.485681
[epoch10, step2566]: loss 0.721679
[epoch10, step2567]: loss 0.716403
[epoch10, step2568]: loss 0.445683
[epoch10, step2569]: loss 0.469154
[epoch10, step2570]: loss 0.427840
[epoch10, step2571]: loss 0.627475
[epoch10, step2572]: loss 0.459548
[epoch10, step2573]: loss 0.772132
[epoch10, step2574]: loss 0.755145
[epoch10, step2575]: loss 0.520652
[epoch10, step2576]: loss 0.576991
[epoch10, step2577]: loss 0.578249
[epoch10, step2578]: loss 0.493717
[epoch10, step2579]: loss 0.420636
[epoch10, step2580]: loss 0.704507
[epoch10, step2581]: loss 0.605394
[epoch10, step2582]: loss 0.242810
[epoch10, step2583]: loss 0.728072
[epoch10, step2584]: loss 0.581780
[epoch10, step2585]: loss 0.674620
[epoch10, step2586]: loss 0.664570
[epoch10, step2587]: loss 0.698577
[epoch10, step2588]: loss 0.588247
[epoch10, step2589]: loss 0.403631
[epoch10, step2590]: loss 0.460072
[epoch10, step2591]: loss 0.665897
[epoch10, step2592]: loss 0.858187
[epoch10, step2593]: loss 0.679789
[epoch10, step2594]: loss 0.345243
[epoch10, step2595]: loss 0.504372
[epoch10, step2596]: loss 0.798037
[epoch10, step2597]: loss 0.374234
[epoch10, step2598]: loss 0.675410
[epoch10, step2599]: loss 0.626839
[epoch10, step2600]: loss 0.434960
[epoch10, step2601]: loss 0.443057
[epoch10, step2602]: loss 0.323172
[epoch10, step2603]: loss 0.417495
[epoch10, step2604]: loss 0.206018
[epoch10, step2605]: loss 0.579662
[epoch10, step2606]: loss 0.729855
[epoch10, step2607]: loss 0.664189
[epoch10, step2608]: loss 0.698767
[epoch10, step2609]: loss 0.344379
[epoch10, step2610]: loss 0.393733
[epoch10, step2611]: loss 0.701254
[epoch10, step2612]: loss 0.369007
[epoch10, step2613]: loss 0.574011
[epoch10, step2614]: loss 0.687995
[epoch10, step2615]: loss 0.737355
[epoch10, step2616]: loss 0.742282
[epoch10, step2617]: loss 0.639933
[epoch10, step2618]: loss 0.633471
[epoch10, step2619]: loss 0.462907
[epoch10, step2620]: loss 0.558456
[epoch10, step2621]: loss 0.629887
[epoch10, step2622]: loss 0.587279
[epoch10, step2623]: loss 0.500286
[epoch10, step2624]: loss 0.610281
[epoch10, step2625]: loss 0.329791
[epoch10, step2626]: loss 0.686765
[epoch10, step2627]: loss 0.498147
[epoch10, step2628]: loss 0.422590
[epoch10, step2629]: loss 0.276854
[epoch10, step2630]: loss 0.551576
[epoch10, step2631]: loss 0.427336
[epoch10, step2632]: loss 0.666758
[epoch10, step2633]: loss 0.524062
[epoch10, step2634]: loss 0.634993
[epoch10, step2635]: loss 0.550301
[epoch10, step2636]: loss 0.535524
[epoch10, step2637]: loss 0.606392
[epoch10, step2638]: loss 0.837790
[epoch10, step2639]: loss 0.671871
[epoch10, step2640]: loss 0.630764
[epoch10, step2641]: loss 0.517666
[epoch10, step2642]: loss 0.539874
[epoch10, step2643]: loss 0.644692
[epoch10, step2644]: loss 0.604057
[epoch10, step2645]: loss 0.619644
[epoch10, step2646]: loss 0.701554
[epoch10, step2647]: loss 0.561096
[epoch10, step2648]: loss 0.718256
[epoch10, step2649]: loss 0.568229
[epoch10, step2650]: loss 0.705354
[epoch10, step2651]: loss 0.408009
[epoch10, step2652]: loss 0.528121
[epoch10, step2653]: loss 0.459457
[epoch10, step2654]: loss 0.653377
[epoch10, step2655]: loss 0.348178
[epoch10, step2656]: loss 0.458930
[epoch10, step2657]: loss 0.579504
[epoch10, step2658]: loss 0.322631
[epoch10, step2659]: loss 0.848437
[epoch10, step2660]: loss 0.719477
[epoch10, step2661]: loss 0.711207
[epoch10, step2662]: loss 0.637833
[epoch10, step2663]: loss 0.314769
[epoch10, step2664]: loss 0.546487
[epoch10, step2665]: loss 0.520445
[epoch10, step2666]: loss 0.725653
[epoch10, step2667]: loss 0.587340
[epoch10, step2668]: loss 0.455854
[epoch10, step2669]: loss 0.581935
[epoch10, step2670]: loss 0.452621
[epoch10, step2671]: loss 0.515044
[epoch10, step2672]: loss 0.520505
[epoch10, step2673]: loss 0.553430
[epoch10, step2674]: loss 0.696445
[epoch10, step2675]: loss 0.599675
[epoch10, step2676]: loss 0.256164
[epoch10, step2677]: loss 0.685038
[epoch10, step2678]: loss 0.512405
[epoch10, step2679]: loss 0.626915
[epoch10, step2680]: loss 0.393771
[epoch10, step2681]: loss 0.472023
[epoch10, step2682]: loss 0.648088
[epoch10, step2683]: loss 0.749213
[epoch10, step2684]: loss 0.623441
[epoch10, step2685]: loss 0.523540
[epoch10, step2686]: loss 0.397033
[epoch10, step2687]: loss 0.592652
[epoch10, step2688]: loss 0.602984
[epoch10, step2689]: loss 0.347787
[epoch10, step2690]: loss 0.696318
[epoch10, step2691]: loss 0.338327
[epoch10, step2692]: loss 0.385310
[epoch10, step2693]: loss 0.607541
[epoch10, step2694]: loss 0.592478
[epoch10, step2695]: loss 0.717600
[epoch10, step2696]: loss 0.635809
[epoch10, step2697]: loss 0.645079
[epoch10, step2698]: loss 0.601960
[epoch10, step2699]: loss 0.645034
[epoch10, step2700]: loss 0.496369
[epoch10, step2701]: loss 0.656490
[epoch10, step2702]: loss 0.427842
[epoch10, step2703]: loss 0.554663
[epoch10, step2704]: loss 0.455732
[epoch10, step2705]: loss 0.865374
[epoch10, step2706]: loss 0.713474
[epoch10, step2707]: loss 0.722498
[epoch10, step2708]: loss 0.714797
[epoch10, step2709]: loss 0.527384
[epoch10, step2710]: loss 0.650159
[epoch10, step2711]: loss 0.648914
[epoch10, step2712]: loss 0.537135
[epoch10, step2713]: loss 0.348139
[epoch10, step2714]: loss 0.338212
[epoch10, step2715]: loss 0.760392
[epoch10, step2716]: loss 0.518903
[epoch10, step2717]: loss 0.576371
[epoch10, step2718]: loss 0.337622
[epoch10, step2719]: loss 0.497069
[epoch10, step2720]: loss 0.457656
[epoch10, step2721]: loss 0.744527
[epoch10, step2722]: loss 0.548596
[epoch10, step2723]: loss 0.593068
[epoch10, step2724]: loss 0.751136
[epoch10, step2725]: loss 0.477841
[epoch10, step2726]: loss 0.415637
[epoch10, step2727]: loss 0.347103
[epoch10, step2728]: loss 0.747469
[epoch10, step2729]: loss 0.846184
[epoch10, step2730]: loss 0.633343
[epoch10, step2731]: loss 0.441317
[epoch10, step2732]: loss 0.454922
[epoch10, step2733]: loss 0.606100
[epoch10, step2734]: loss 0.771659
[epoch10, step2735]: loss 0.536867
[epoch10, step2736]: loss 0.694827
[epoch10, step2737]: loss 0.691229
[epoch10, step2738]: loss 0.479454
[epoch10, step2739]: loss 0.642076
[epoch10, step2740]: loss 0.666510
[epoch10, step2741]: loss 0.665214
[epoch10, step2742]: loss 0.773434
[epoch10, step2743]: loss 0.684415
[epoch10, step2744]: loss 0.758379
[epoch10, step2745]: loss 0.559980
[epoch10, step2746]: loss 0.503427
[epoch10, step2747]: loss 0.554400
[epoch10, step2748]: loss 0.610276
[epoch10, step2749]: loss 0.629115
[epoch10, step2750]: loss 0.591980
[epoch10, step2751]: loss 0.490984
[epoch10, step2752]: loss 0.477833
[epoch10, step2753]: loss 0.674758
[epoch10, step2754]: loss 0.557478
[epoch10, step2755]: loss 0.596949
[epoch10, step2756]: loss 0.622380
[epoch10, step2757]: loss 0.724988
[epoch10, step2758]: loss 0.792976
[epoch10, step2759]: loss 0.682303
[epoch10, step2760]: loss 0.426682
[epoch10, step2761]: loss 0.569849
[epoch10, step2762]: loss 0.812817
[epoch10, step2763]: loss 0.540354
[epoch10, step2764]: loss 0.614301
[epoch10, step2765]: loss 0.663926
[epoch10, step2766]: loss 0.545460
[epoch10, step2767]: loss 0.388381
[epoch10, step2768]: loss 0.437524
[epoch10, step2769]: loss 0.307531
[epoch10, step2770]: loss 0.580898
[epoch10, step2771]: loss 0.655073
[epoch10, step2772]: loss 0.491802
[epoch10, step2773]: loss 0.621200
[epoch10, step2774]: loss 0.402185
[epoch10, step2775]: loss 0.455756
[epoch10, step2776]: loss 0.457878
[epoch10, step2777]: loss 0.850462
[epoch10, step2778]: loss 0.500225
[epoch10, step2779]: loss 0.562401
[epoch10, step2780]: loss 0.702767
[epoch10, step2781]: loss 0.447103
[epoch10, step2782]: loss 0.809966
[epoch10, step2783]: loss 0.491018
[epoch10, step2784]: loss 0.748953
[epoch10, step2785]: loss 0.433657
[epoch10, step2786]: loss 0.535549
[epoch10, step2787]: loss 0.476069
[epoch10, step2788]: loss 0.457171
[epoch10, step2789]: loss 0.392852
[epoch10, step2790]: loss 0.664440
[epoch10, step2791]: loss 0.686913
[epoch10, step2792]: loss 0.657373
[epoch10, step2793]: loss 0.465243
[epoch10, step2794]: loss 0.625411
[epoch10, step2795]: loss 0.497369
[epoch10, step2796]: loss 0.730741
[epoch10, step2797]: loss 0.619747
[epoch10, step2798]: loss 0.724197
[epoch10, step2799]: loss 0.765257
[epoch10, step2800]: loss 0.810574
[epoch10, step2801]: loss 0.690365
[epoch10, step2802]: loss 0.758344
[epoch10, step2803]: loss 0.718901
[epoch10, step2804]: loss 0.548335
[epoch10, step2805]: loss 0.716088
[epoch10, step2806]: loss 0.832173
[epoch10, step2807]: loss 0.761910
[epoch10, step2808]: loss 0.322808
[epoch10, step2809]: loss 0.806340
[epoch10, step2810]: loss 0.459296
[epoch10, step2811]: loss 0.583657
[epoch10, step2812]: loss 0.520371
[epoch10, step2813]: loss 0.611758
[epoch10, step2814]: loss 0.343206
[epoch10, step2815]: loss 0.632151
[epoch10, step2816]: loss 0.765653
[epoch10, step2817]: loss 0.759409
[epoch10, step2818]: loss 0.591269
[epoch10, step2819]: loss 0.619587
[epoch10, step2820]: loss 0.605623
[epoch10, step2821]: loss 0.554530
[epoch10, step2822]: loss 0.743114
[epoch10, step2823]: loss 0.445939
[epoch10, step2824]: loss 0.691111
[epoch10, step2825]: loss 0.365782
[epoch10, step2826]: loss 0.817054
[epoch10, step2827]: loss 0.855062
[epoch10, step2828]: loss 0.655299
[epoch10, step2829]: loss 0.606728
[epoch10, step2830]: loss 0.653690
[epoch10, step2831]: loss 0.672100
[epoch10, step2832]: loss 0.698699
[epoch10, step2833]: loss 0.543592
[epoch10, step2834]: loss 0.487528
[epoch10, step2835]: loss 0.598359
[epoch10, step2836]: loss 0.386232
[epoch10, step2837]: loss 0.701687
[epoch10, step2838]: loss 0.533398
[epoch10, step2839]: loss 0.574740
[epoch10, step2840]: loss 0.642005
[epoch10, step2841]: loss 0.527556
[epoch10, step2842]: loss 0.446208
[epoch10, step2843]: loss 0.661466
[epoch10, step2844]: loss 0.257540
[epoch10, step2845]: loss 0.757320
[epoch10, step2846]: loss 0.795754
[epoch10, step2847]: loss 0.755635
[epoch10, step2848]: loss 0.412505
[epoch10, step2849]: loss 0.670151
[epoch10, step2850]: loss 0.773018
[epoch10, step2851]: loss 0.727962
[epoch10, step2852]: loss 0.430387
[epoch10, step2853]: loss 0.664254
[epoch10, step2854]: loss 0.698802
[epoch10, step2855]: loss 0.544591
[epoch10, step2856]: loss 0.408869
[epoch10, step2857]: loss 0.482022
[epoch10, step2858]: loss 0.842071
[epoch10, step2859]: loss 0.902320
[epoch10, step2860]: loss 0.799686
[epoch10, step2861]: loss 0.416394
[epoch10, step2862]: loss 0.733610
[epoch10, step2863]: loss 0.658209
[epoch10, step2864]: loss 0.180533
[epoch10, step2865]: loss 0.693406
[epoch10, step2866]: loss 0.561317
[epoch10, step2867]: loss 0.383310
[epoch10, step2868]: loss 0.760147
[epoch10, step2869]: loss 0.536004
[epoch10, step2870]: loss 0.554732
[epoch10, step2871]: loss 0.502949
[epoch10, step2872]: loss 0.656135
[epoch10, step2873]: loss 0.675050
[epoch10, step2874]: loss 0.601873
[epoch10, step2875]: loss 0.293534
[epoch10, step2876]: loss 0.417838
[epoch10, step2877]: loss 0.435074
[epoch10, step2878]: loss 0.736811
[epoch10, step2879]: loss 0.275376
[epoch10, step2880]: loss 0.636076
[epoch10, step2881]: loss 0.544240
[epoch10, step2882]: loss 0.837100
[epoch10, step2883]: loss 0.720878
[epoch10, step2884]: loss 0.558354
[epoch10, step2885]: loss 0.735126
[epoch10, step2886]: loss 0.556796
[epoch10, step2887]: loss 0.623178
[epoch10, step2888]: loss 0.737871
[epoch10, step2889]: loss 0.580281
[epoch10, step2890]: loss 0.421154
[epoch10, step2891]: loss 0.848566
[epoch10, step2892]: loss 0.820347
[epoch10, step2893]: loss 0.623569
[epoch10, step2894]: loss 0.689432
[epoch10, step2895]: loss 0.740014
[epoch10, step2896]: loss 0.422310
[epoch10, step2897]: loss 0.428756
[epoch10, step2898]: loss 0.462701
[epoch10, step2899]: loss 0.442000
[epoch10, step2900]: loss 0.666864
[epoch10, step2901]: loss 0.568589
[epoch10, step2902]: loss 0.747119
[epoch10, step2903]: loss 0.707704
[epoch10, step2904]: loss 0.714516
[epoch10, step2905]: loss 0.334044
[epoch10, step2906]: loss 0.517413
[epoch10, step2907]: loss 0.669902
[epoch10, step2908]: loss 0.446346
[epoch10, step2909]: loss 0.559028
[epoch10, step2910]: loss 0.646172
[epoch10, step2911]: loss 0.736108
[epoch10, step2912]: loss 0.760269
[epoch10, step2913]: loss 0.641684
[epoch10, step2914]: loss 0.763020
[epoch10, step2915]: loss 0.696213
[epoch10, step2916]: loss 0.644065
[epoch10, step2917]: loss 0.862856
[epoch10, step2918]: loss 0.138524
[epoch10, step2919]: loss 0.335929
[epoch10, step2920]: loss 0.604948
[epoch10, step2921]: loss 0.446897
[epoch10, step2922]: loss 0.643523
[epoch10, step2923]: loss 0.568573
[epoch10, step2924]: loss 0.682151
[epoch10, step2925]: loss 0.326424
[epoch10, step2926]: loss 0.444621
[epoch10, step2927]: loss 0.464096
[epoch10, step2928]: loss 0.830755
[epoch10, step2929]: loss 0.358770
[epoch10, step2930]: loss 0.717703
[epoch10, step2931]: loss 0.332073
[epoch10, step2932]: loss 0.458315
[epoch10, step2933]: loss 0.523448
[epoch10, step2934]: loss 0.463463
[epoch10, step2935]: loss 0.582893
[epoch10, step2936]: loss 0.724464
[epoch10, step2937]: loss 0.753382
[epoch10, step2938]: loss 0.353424
[epoch10, step2939]: loss 0.601187
[epoch10, step2940]: loss 0.598659
[epoch10, step2941]: loss 0.589582
[epoch10, step2942]: loss 0.740539
[epoch10, step2943]: loss 0.430193
[epoch10, step2944]: loss 0.750499
[epoch10, step2945]: loss 0.609644
[epoch10, step2946]: loss 0.522373
[epoch10, step2947]: loss 0.463335
[epoch10, step2948]: loss 0.634899
[epoch10, step2949]: loss 0.491005
[epoch10, step2950]: loss 0.607451
[epoch10, step2951]: loss 0.646062
[epoch10, step2952]: loss 0.600579
[epoch10, step2953]: loss 0.348688
[epoch10, step2954]: loss 0.336744
[epoch10, step2955]: loss 0.669832
[epoch10, step2956]: loss 0.573936
[epoch10, step2957]: loss 0.263146
[epoch10, step2958]: loss 0.634054
[epoch10, step2959]: loss 0.629907
[epoch10, step2960]: loss 0.650143
[epoch10, step2961]: loss 0.539393
[epoch10, step2962]: loss 0.481491
[epoch10, step2963]: loss 0.468037
[epoch10, step2964]: loss 0.606658
[epoch10, step2965]: loss 0.750838
[epoch10, step2966]: loss 0.684101
[epoch10, step2967]: loss 0.469100
[epoch10, step2968]: loss 0.931562
[epoch10, step2969]: loss 0.459892
[epoch10, step2970]: loss 0.232402
[epoch10, step2971]: loss 0.670956
[epoch10, step2972]: loss 0.702616
[epoch10, step2973]: loss 0.666436
[epoch10, step2974]: loss 0.687473
[epoch10, step2975]: loss 0.452057
[epoch10, step2976]: loss 0.484182
[epoch10, step2977]: loss 0.540394
[epoch10, step2978]: loss 0.507517
[epoch10, step2979]: loss 0.529951
[epoch10, step2980]: loss 0.546925
[epoch10, step2981]: loss 0.425313
[epoch10, step2982]: loss 0.534020
[epoch10, step2983]: loss 0.619959
[epoch10, step2984]: loss 0.720580
[epoch10, step2985]: loss 0.775375
[epoch10, step2986]: loss 0.616616
[epoch10, step2987]: loss 0.575755
[epoch10, step2988]: loss 0.544637
[epoch10, step2989]: loss 0.324113
[epoch10, step2990]: loss 0.614708
[epoch10, step2991]: loss 0.746229
[epoch10, step2992]: loss 0.390016
[epoch10, step2993]: loss 0.390725
[epoch10, step2994]: loss 0.831381
[epoch10, step2995]: loss 0.641923
[epoch10, step2996]: loss 0.574461
[epoch10, step2997]: loss 0.442952
[epoch10, step2998]: loss 0.525966
[epoch10, step2999]: loss 0.651691
[epoch10, step3000]: loss 0.513090
[epoch10, step3001]: loss 0.500236
[epoch10, step3002]: loss 0.428260
[epoch10, step3003]: loss 0.608194
[epoch10, step3004]: loss 0.247845
[epoch10, step3005]: loss 0.682776
[epoch10, step3006]: loss 0.755606
[epoch10, step3007]: loss 0.632127
[epoch10, step3008]: loss 0.565365
[epoch10, step3009]: loss 0.544321
[epoch10, step3010]: loss 0.488057
[epoch10, step3011]: loss 0.732807
[epoch10, step3012]: loss 0.311807
[epoch10, step3013]: loss 0.790013
[epoch10, step3014]: loss 0.540267
[epoch10, step3015]: loss 0.485044
[epoch10, step3016]: loss 0.802067
[epoch10, step3017]: loss 0.574631
[epoch10, step3018]: loss 0.826548
[epoch10, step3019]: loss 0.323599
[epoch10, step3020]: loss 0.627785
[epoch10, step3021]: loss 0.643708
[epoch10, step3022]: loss 0.476239
[epoch10, step3023]: loss 0.684443
[epoch10, step3024]: loss 0.430434
[epoch10, step3025]: loss 0.664867
[epoch10, step3026]: loss 0.543591
[epoch10, step3027]: loss 0.646159
[epoch10, step3028]: loss 0.359689
[epoch10, step3029]: loss 0.587871
[epoch10, step3030]: loss 0.700094
[epoch10, step3031]: loss 0.822284
[epoch10, step3032]: loss 0.922679
[epoch10, step3033]: loss 0.637597
[epoch10, step3034]: loss 0.764508
[epoch10, step3035]: loss 0.718685
[epoch10, step3036]: loss 0.658369
[epoch10, step3037]: loss 0.598567
[epoch10, step3038]: loss 0.546633
[epoch10, step3039]: loss 0.749706
[epoch10, step3040]: loss 0.409786
[epoch10, step3041]: loss 0.579710
[epoch10, step3042]: loss 0.492677
[epoch10, step3043]: loss 0.440352
[epoch10, step3044]: loss 0.331850
[epoch10, step3045]: loss 0.530468
[epoch10, step3046]: loss 0.836878
[epoch10, step3047]: loss 0.947171
[epoch10, step3048]: loss 0.440742
[epoch10, step3049]: loss 0.606721
[epoch10, step3050]: loss 0.667024
[epoch10, step3051]: loss 0.743258
[epoch10, step3052]: loss 0.604560
[epoch10, step3053]: loss 0.780257
[epoch10, step3054]: loss 0.582794
[epoch10, step3055]: loss 0.583912
[epoch10, step3056]: loss 0.564339
[epoch10, step3057]: loss 0.509265
[epoch10, step3058]: loss 0.694998
[epoch10, step3059]: loss 0.882442
[epoch10, step3060]: loss 0.668991
[epoch10, step3061]: loss 0.662350
[epoch10, step3062]: loss 0.328621
[epoch10, step3063]: loss 0.746513
[epoch10, step3064]: loss 0.567138
[epoch10, step3065]: loss 0.687357
[epoch10, step3066]: loss 0.568702
[epoch10, step3067]: loss 0.531511
[epoch10, step3068]: loss 0.646193
[epoch10, step3069]: loss 0.480731
[epoch10, step3070]: loss 0.525221
[epoch10, step3071]: loss 0.510362
[epoch10, step3072]: loss 0.662107
[epoch10, step3073]: loss 0.514585
[epoch10, step3074]: loss 0.800332
[epoch10, step3075]: loss 0.672340
[epoch10, step3076]: loss 0.486976

[epoch10]: avg loss 0.486976

[epoch11, step1]: loss 0.689669
[epoch11, step2]: loss 0.791462
[epoch11, step3]: loss 0.765577
[epoch11, step4]: loss 0.513000
[epoch11, step5]: loss 0.484586
[epoch11, step6]: loss 0.530516
[epoch11, step7]: loss 0.328076
[epoch11, step8]: loss 0.776082
[epoch11, step9]: loss 0.610935
[epoch11, step10]: loss 0.551800
[epoch11, step11]: loss 0.265372
[epoch11, step12]: loss 0.625528
[epoch11, step13]: loss 0.550430
[epoch11, step14]: loss 0.445719
[epoch11, step15]: loss 0.332642
[epoch11, step16]: loss 0.661022
[epoch11, step17]: loss 0.500123
[epoch11, step18]: loss 0.597722
[epoch11, step19]: loss 0.553912
[epoch11, step20]: loss 0.442598
[epoch11, step21]: loss 0.483691
[epoch11, step22]: loss 0.331616
[epoch11, step23]: loss 0.854686
[epoch11, step24]: loss 0.626161
[epoch11, step25]: loss 0.683668
[epoch11, step26]: loss 0.383865
[epoch11, step27]: loss 0.701858
[epoch11, step28]: loss 0.476816
[epoch11, step29]: loss 0.834414
[epoch11, step30]: loss 0.671777
[epoch11, step31]: loss 0.528457
[epoch11, step32]: loss 0.552398
[epoch11, step33]: loss 0.470917
[epoch11, step34]: loss 0.786466
[epoch11, step35]: loss 0.641607
[epoch11, step36]: loss 0.404388
[epoch11, step37]: loss 0.533640
[epoch11, step38]: loss 0.819539
[epoch11, step39]: loss 0.587090
[epoch11, step40]: loss 0.627830
[epoch11, step41]: loss 0.453812
[epoch11, step42]: loss 0.525065
[epoch11, step43]: loss 0.771545
[epoch11, step44]: loss 0.545203
[epoch11, step45]: loss 0.401384
[epoch11, step46]: loss 0.416746
[epoch11, step47]: loss 0.480450
[epoch11, step48]: loss 0.685478
[epoch11, step49]: loss 0.572920
[epoch11, step50]: loss 0.144334
[epoch11, step51]: loss 0.650364
[epoch11, step52]: loss 0.448895
[epoch11, step53]: loss 0.641639
[epoch11, step54]: loss 0.487131
[epoch11, step55]: loss 0.357980
[epoch11, step56]: loss 0.721174
[epoch11, step57]: loss 0.643108
[epoch11, step58]: loss 0.395949
[epoch11, step59]: loss 0.559957
[epoch11, step60]: loss 0.555015
[epoch11, step61]: loss 0.534193
[epoch11, step62]: loss 0.623265
[epoch11, step63]: loss 0.602076
[epoch11, step64]: loss 0.413397
[epoch11, step65]: loss 0.301760
[epoch11, step66]: loss 0.555039
[epoch11, step67]: loss 0.735971
[epoch11, step68]: loss 0.605863
[epoch11, step69]: loss 0.667139
[epoch11, step70]: loss 0.455897
[epoch11, step71]: loss 0.528200
[epoch11, step72]: loss 0.636499
[epoch11, step73]: loss 0.334610
[epoch11, step74]: loss 0.828735
[epoch11, step75]: loss 0.558008
[epoch11, step76]: loss 0.870585
[epoch11, step77]: loss 0.767309
[epoch11, step78]: loss 0.602426
[epoch11, step79]: loss 0.497553
[epoch11, step80]: loss 0.652055
[epoch11, step81]: loss 0.474325
[epoch11, step82]: loss 0.788410
[epoch11, step83]: loss 0.668035
[epoch11, step84]: loss 0.599780
[epoch11, step85]: loss 0.323089
[epoch11, step86]: loss 0.849386
[epoch11, step87]: loss 0.618596
[epoch11, step88]: loss 0.511415
[epoch11, step89]: loss 0.514730
[epoch11, step90]: loss 0.461336
[epoch11, step91]: loss 0.756388
[epoch11, step92]: loss 0.755914
[epoch11, step93]: loss 0.341448
[epoch11, step94]: loss 0.736674
[epoch11, step95]: loss 0.615793
[epoch11, step96]: loss 0.570013
[epoch11, step97]: loss 0.586325
[epoch11, step98]: loss 0.566530
[epoch11, step99]: loss 0.517200
[epoch11, step100]: loss 0.641984
[epoch11, step101]: loss 0.396249
[epoch11, step102]: loss 0.430518
[epoch11, step103]: loss 0.602570
[epoch11, step104]: loss 0.465377
[epoch11, step105]: loss 0.664786
[epoch11, step106]: loss 0.475192
[epoch11, step107]: loss 0.399834
[epoch11, step108]: loss 0.735081
[epoch11, step109]: loss 0.750486
[epoch11, step110]: loss 0.559243
[epoch11, step111]: loss 0.854486
[epoch11, step112]: loss 0.654277
[epoch11, step113]: loss 0.379048
[epoch11, step114]: loss 0.375725
[epoch11, step115]: loss 0.527575
[epoch11, step116]: loss 0.563482
[epoch11, step117]: loss 0.345385
[epoch11, step118]: loss 0.609891
[epoch11, step119]: loss 0.679266
[epoch11, step120]: loss 0.483543
[epoch11, step121]: loss 0.698554
[epoch11, step122]: loss 0.633655
[epoch11, step123]: loss 0.627057
[epoch11, step124]: loss 0.694123
[epoch11, step125]: loss 0.640333
[epoch11, step126]: loss 0.481717
[epoch11, step127]: loss 0.452971
[epoch11, step128]: loss 0.451093
[epoch11, step129]: loss 0.610659
[epoch11, step130]: loss 0.521079
[epoch11, step131]: loss 0.541302
[epoch11, step132]: loss 0.418183
[epoch11, step133]: loss 0.820157
[epoch11, step134]: loss 0.578138
[epoch11, step135]: loss 0.657230
[epoch11, step136]: loss 0.434001
[epoch11, step137]: loss 0.754599
[epoch11, step138]: loss 0.818888
[epoch11, step139]: loss 0.441746
[epoch11, step140]: loss 0.525999
[epoch11, step141]: loss 0.545507
[epoch11, step142]: loss 0.808798
[epoch11, step143]: loss 0.625652
[epoch11, step144]: loss 0.500071
[epoch11, step145]: loss 0.668322
[epoch11, step146]: loss 0.720486
[epoch11, step147]: loss 0.743357
[epoch11, step148]: loss 0.626014
[epoch11, step149]: loss 0.548174
[epoch11, step150]: loss 0.550686
[epoch11, step151]: loss 0.561220
[epoch11, step152]: loss 0.610864
[epoch11, step153]: loss 0.415471
[epoch11, step154]: loss 0.735292
[epoch11, step155]: loss 0.746541
[epoch11, step156]: loss 0.693716
[epoch11, step157]: loss 0.889555
[epoch11, step158]: loss 0.660064
[epoch11, step159]: loss 0.675833
[epoch11, step160]: loss 0.716451
[epoch11, step161]: loss 0.444510
[epoch11, step162]: loss 0.625479
[epoch11, step163]: loss 0.655911
[epoch11, step164]: loss 0.803718
[epoch11, step165]: loss 0.647594
[epoch11, step166]: loss 0.580181
[epoch11, step167]: loss 0.582565
[epoch11, step168]: loss 0.408993
[epoch11, step169]: loss 0.627397
[epoch11, step170]: loss 0.700167
[epoch11, step171]: loss 0.675096
[epoch11, step172]: loss 0.825058
[epoch11, step173]: loss 0.555825
[epoch11, step174]: loss 0.697643
[epoch11, step175]: loss 0.756922
[epoch11, step176]: loss 0.605980
[epoch11, step177]: loss 0.638090
[epoch11, step178]: loss 0.610491
[epoch11, step179]: loss 0.603139
[epoch11, step180]: loss 0.393255
[epoch11, step181]: loss 0.505142
[epoch11, step182]: loss 0.677972
[epoch11, step183]: loss 0.460629
[epoch11, step184]: loss 0.678498
[epoch11, step185]: loss 0.179989
[epoch11, step186]: loss 0.780862
[epoch11, step187]: loss 0.778899
[epoch11, step188]: loss 0.187082
[epoch11, step189]: loss 0.755320
[epoch11, step190]: loss 0.458039
[epoch11, step191]: loss 0.528662
[epoch11, step192]: loss 0.746358
[epoch11, step193]: loss 0.767326
[epoch11, step194]: loss 0.550274
[epoch11, step195]: loss 0.667695
[epoch11, step196]: loss 0.694145
[epoch11, step197]: loss 0.550879
[epoch11, step198]: loss 0.398106
[epoch11, step199]: loss 0.789913
[epoch11, step200]: loss 0.681829
[epoch11, step201]: loss 0.592196
[epoch11, step202]: loss 0.483989
[epoch11, step203]: loss 0.438069
[epoch11, step204]: loss 0.663048
[epoch11, step205]: loss 0.720490
[epoch11, step206]: loss 0.528717
[epoch11, step207]: loss 0.470235
[epoch11, step208]: loss 0.487734
[epoch11, step209]: loss 0.709281
[epoch11, step210]: loss 0.755371
[epoch11, step211]: loss 0.321110
[epoch11, step212]: loss 0.667148
[epoch11, step213]: loss 0.588093
[epoch11, step214]: loss 0.720432
[epoch11, step215]: loss 0.781983
[epoch11, step216]: loss 0.619507
[epoch11, step217]: loss 0.349016
[epoch11, step218]: loss 0.520917
[epoch11, step219]: loss 0.581877
[epoch11, step220]: loss 0.481921
[epoch11, step221]: loss 0.657473
[epoch11, step222]: loss 0.541800
[epoch11, step223]: loss 0.556282
[epoch11, step224]: loss 0.618833
[epoch11, step225]: loss 0.677652
[epoch11, step226]: loss 0.508803
[epoch11, step227]: loss 0.366084
[epoch11, step228]: loss 0.659807
[epoch11, step229]: loss 0.679493
[epoch11, step230]: loss 0.550069
[epoch11, step231]: loss 0.661769
[epoch11, step232]: loss 0.624251
[epoch11, step233]: loss 0.638624
[epoch11, step234]: loss 0.661647
[epoch11, step235]: loss 0.702576
[epoch11, step236]: loss 0.472324
[epoch11, step237]: loss 0.611197
[epoch11, step238]: loss 0.523943
[epoch11, step239]: loss 0.466579
[epoch11, step240]: loss 0.762418
[epoch11, step241]: loss 0.680155
[epoch11, step242]: loss 0.651587
[epoch11, step243]: loss 0.569410
[epoch11, step244]: loss 0.712805
[epoch11, step245]: loss 0.728299
[epoch11, step246]: loss 0.494733
[epoch11, step247]: loss 0.660922
[epoch11, step248]: loss 0.450210
[epoch11, step249]: loss 0.638778
[epoch11, step250]: loss 0.485636
[epoch11, step251]: loss 0.630365
[epoch11, step252]: loss 0.675637
[epoch11, step253]: loss 0.500847
[epoch11, step254]: loss 0.492966
[epoch11, step255]: loss 0.523945
[epoch11, step256]: loss 0.726190
[epoch11, step257]: loss 0.423492
[epoch11, step258]: loss 0.698031
[epoch11, step259]: loss 0.644287
[epoch11, step260]: loss 0.412287
[epoch11, step261]: loss 0.550409
[epoch11, step262]: loss 0.643148
[epoch11, step263]: loss 0.584527
[epoch11, step264]: loss 0.673529
[epoch11, step265]: loss 0.514919
[epoch11, step266]: loss 0.545030
[epoch11, step267]: loss 0.437125
[epoch11, step268]: loss 0.388577
[epoch11, step269]: loss 0.650801
[epoch11, step270]: loss 0.631965
[epoch11, step271]: loss 0.478257
[epoch11, step272]: loss 0.531172
[epoch11, step273]: loss 0.380934
[epoch11, step274]: loss 0.554360
[epoch11, step275]: loss 0.649437
[epoch11, step276]: loss 0.568169
[epoch11, step277]: loss 0.632590
[epoch11, step278]: loss 0.319740
[epoch11, step279]: loss 0.702795
[epoch11, step280]: loss 0.295400
[epoch11, step281]: loss 0.513525
[epoch11, step282]: loss 0.432456
[epoch11, step283]: loss 0.496482
[epoch11, step284]: loss 0.715399
[epoch11, step285]: loss 0.490775
[epoch11, step286]: loss 0.680488
[epoch11, step287]: loss 0.621875
[epoch11, step288]: loss 0.419251
[epoch11, step289]: loss 0.764951
[epoch11, step290]: loss 0.606151
[epoch11, step291]: loss 0.480766
[epoch11, step292]: loss 0.501165
[epoch11, step293]: loss 0.314066
[epoch11, step294]: loss 0.693447
[epoch11, step295]: loss 0.556237
[epoch11, step296]: loss 0.632188
[epoch11, step297]: loss 0.334722
[epoch11, step298]: loss 0.737126
[epoch11, step299]: loss 0.871999
[epoch11, step300]: loss 0.704696
[epoch11, step301]: loss 0.562731
[epoch11, step302]: loss 0.689780
[epoch11, step303]: loss 0.598798
[epoch11, step304]: loss 0.711307
[epoch11, step305]: loss 0.349876
[epoch11, step306]: loss 0.370883
[epoch11, step307]: loss 0.675227
[epoch11, step308]: loss 0.677813
[epoch11, step309]: loss 0.741965
[epoch11, step310]: loss 0.561426
[epoch11, step311]: loss 0.649066
[epoch11, step312]: loss 0.629098
[epoch11, step313]: loss 0.693372
[epoch11, step314]: loss 0.612587
[epoch11, step315]: loss 0.545787
[epoch11, step316]: loss 0.793920
[epoch11, step317]: loss 0.224179
[epoch11, step318]: loss 0.437028
[epoch11, step319]: loss 0.494401
[epoch11, step320]: loss 0.350792
[epoch11, step321]: loss 0.785977
[epoch11, step322]: loss 0.341369
[epoch11, step323]: loss 0.420396
[epoch11, step324]: loss 0.439001
[epoch11, step325]: loss 0.621159
[epoch11, step326]: loss 0.588589
[epoch11, step327]: loss 0.742281
[epoch11, step328]: loss 0.735449
[epoch11, step329]: loss 0.622887
[epoch11, step330]: loss 0.481722
[epoch11, step331]: loss 0.546410
[epoch11, step332]: loss 0.684177
[epoch11, step333]: loss 0.522126
[epoch11, step334]: loss 0.738961
[epoch11, step335]: loss 0.449482
[epoch11, step336]: loss 0.349883
[epoch11, step337]: loss 0.416959
[epoch11, step338]: loss 0.702618
[epoch11, step339]: loss 0.206248
[epoch11, step340]: loss 0.547651
[epoch11, step341]: loss 0.514377
[epoch11, step342]: loss 0.592372
[epoch11, step343]: loss 0.594231
[epoch11, step344]: loss 0.525497
[epoch11, step345]: loss 0.679462
[epoch11, step346]: loss 0.614123
[epoch11, step347]: loss 0.683507
[epoch11, step348]: loss 0.343597
[epoch11, step349]: loss 0.671604
[epoch11, step350]: loss 0.681594
[epoch11, step351]: loss 0.426252
[epoch11, step352]: loss 0.573289
[epoch11, step353]: loss 0.843497
[epoch11, step354]: loss 0.760647
[epoch11, step355]: loss 0.598563
[epoch11, step356]: loss 0.317623
[epoch11, step357]: loss 0.511735
[epoch11, step358]: loss 0.407730
[epoch11, step359]: loss 0.527585
[epoch11, step360]: loss 0.267134
[epoch11, step361]: loss 0.910727
[epoch11, step362]: loss 0.377672
[epoch11, step363]: loss 0.618577
[epoch11, step364]: loss 0.334912
[epoch11, step365]: loss 0.521277
[epoch11, step366]: loss 0.510747
[epoch11, step367]: loss 0.717200
[epoch11, step368]: loss 0.494866
[epoch11, step369]: loss 0.544622
[epoch11, step370]: loss 0.470410
[epoch11, step371]: loss 0.382552
[epoch11, step372]: loss 0.546023
[epoch11, step373]: loss 0.675058
[epoch11, step374]: loss 0.676527
[epoch11, step375]: loss 0.427762
[epoch11, step376]: loss 0.636929
[epoch11, step377]: loss 0.810881
[epoch11, step378]: loss 0.344832
[epoch11, step379]: loss 0.685626
[epoch11, step380]: loss 0.776108
[epoch11, step381]: loss 0.573683
[epoch11, step382]: loss 0.550356
[epoch11, step383]: loss 0.613429
[epoch11, step384]: loss 0.557996
[epoch11, step385]: loss 0.686749
[epoch11, step386]: loss 0.540838
[epoch11, step387]: loss 0.504658
[epoch11, step388]: loss 0.679384
[epoch11, step389]: loss 0.783492
[epoch11, step390]: loss 0.715569
[epoch11, step391]: loss 0.398317
[epoch11, step392]: loss 0.527056
[epoch11, step393]: loss 0.421574
[epoch11, step394]: loss 0.361383
[epoch11, step395]: loss 0.638170
[epoch11, step396]: loss 0.642661
[epoch11, step397]: loss 0.763922
[epoch11, step398]: loss 0.743900
[epoch11, step399]: loss 0.396539
[epoch11, step400]: loss 0.626188
[epoch11, step401]: loss 0.528284
[epoch11, step402]: loss 0.182575
[epoch11, step403]: loss 0.632710
[epoch11, step404]: loss 0.718393
[epoch11, step405]: loss 0.569084
[epoch11, step406]: loss 0.429755
[epoch11, step407]: loss 0.650670
[epoch11, step408]: loss 0.183788
[epoch11, step409]: loss 0.446577
[epoch11, step410]: loss 0.701463
[epoch11, step411]: loss 0.523841
[epoch11, step412]: loss 0.487974
[epoch11, step413]: loss 0.338010
[epoch11, step414]: loss 0.641086
[epoch11, step415]: loss 0.534410
[epoch11, step416]: loss 0.423478
[epoch11, step417]: loss 0.773490
[epoch11, step418]: loss 0.633989
[epoch11, step419]: loss 0.402694
[epoch11, step420]: loss 0.766330
[epoch11, step421]: loss 0.281978
[epoch11, step422]: loss 0.758995
[epoch11, step423]: loss 0.508537
[epoch11, step424]: loss 0.636348
[epoch11, step425]: loss 0.656788
[epoch11, step426]: loss 0.580535
[epoch11, step427]: loss 0.413541
[epoch11, step428]: loss 0.564908
[epoch11, step429]: loss 0.387425
[epoch11, step430]: loss 0.758966
[epoch11, step431]: loss 0.714256
[epoch11, step432]: loss 0.537267
[epoch11, step433]: loss 0.620302
[epoch11, step434]: loss 0.607482
[epoch11, step435]: loss 0.699483
[epoch11, step436]: loss 0.409522
[epoch11, step437]: loss 0.697727
[epoch11, step438]: loss 0.678358
[epoch11, step439]: loss 0.719088
[epoch11, step440]: loss 0.514477
[epoch11, step441]: loss 0.592893
[epoch11, step442]: loss 0.535328
[epoch11, step443]: loss 0.551772
[epoch11, step444]: loss 0.657456
[epoch11, step445]: loss 0.567373
[epoch11, step446]: loss 0.569494
[epoch11, step447]: loss 0.529095
[epoch11, step448]: loss 0.462860
[epoch11, step449]: loss 0.609184
[epoch11, step450]: loss 0.622190
[epoch11, step451]: loss 0.769793
[epoch11, step452]: loss 0.411123
[epoch11, step453]: loss 0.435184
[epoch11, step454]: loss 0.211504
[epoch11, step455]: loss 0.742886
[epoch11, step456]: loss 0.631938
[epoch11, step457]: loss 0.494187
[epoch11, step458]: loss 0.634075
[epoch11, step459]: loss 0.447374
[epoch11, step460]: loss 0.584731
[epoch11, step461]: loss 0.454511
[epoch11, step462]: loss 0.767943
[epoch11, step463]: loss 0.282533
[epoch11, step464]: loss 0.693837
[epoch11, step465]: loss 0.491447
[epoch11, step466]: loss 0.652543
[epoch11, step467]: loss 0.623013
[epoch11, step468]: loss 0.793617
[epoch11, step469]: loss 0.968750
[epoch11, step470]: loss 0.685078
[epoch11, step471]: loss 0.759830
[epoch11, step472]: loss 0.724238
[epoch11, step473]: loss 0.667912
[epoch11, step474]: loss 0.673122
[epoch11, step475]: loss 0.690832
[epoch11, step476]: loss 0.553119
[epoch11, step477]: loss 0.510782
[epoch11, step478]: loss 0.740163
[epoch11, step479]: loss 0.495063
[epoch11, step480]: loss 0.437151
[epoch11, step481]: loss 0.745382
[epoch11, step482]: loss 0.327694
[epoch11, step483]: loss 0.578286
[epoch11, step484]: loss 0.782980
[epoch11, step485]: loss 0.564095
[epoch11, step486]: loss 0.838094
[epoch11, step487]: loss 0.421739
[epoch11, step488]: loss 0.426833
[epoch11, step489]: loss 0.456478
[epoch11, step490]: loss 0.583715
[epoch11, step491]: loss 0.866149
[epoch11, step492]: loss 0.510263
[epoch11, step493]: loss 0.475803
[epoch11, step494]: loss 0.700426
[epoch11, step495]: loss 0.588174
[epoch11, step496]: loss 0.655482
[epoch11, step497]: loss 0.748725
[epoch11, step498]: loss 0.362734
[epoch11, step499]: loss 0.413716
[epoch11, step500]: loss 0.383736
[epoch11, step501]: loss 0.530798
[epoch11, step502]: loss 0.479013
[epoch11, step503]: loss 0.403467
[epoch11, step504]: loss 0.580616
[epoch11, step505]: loss 0.407449
[epoch11, step506]: loss 0.648211
[epoch11, step507]: loss 0.785276
[epoch11, step508]: loss 0.527055
[epoch11, step509]: loss 0.730520
[epoch11, step510]: loss 0.778451
[epoch11, step511]: loss 0.722668
[epoch11, step512]: loss 0.548124
[epoch11, step513]: loss 0.586950
[epoch11, step514]: loss 0.535970
[epoch11, step515]: loss 0.419367
[epoch11, step516]: loss 0.781690
[epoch11, step517]: loss 0.215052
[epoch11, step518]: loss 0.657513
[epoch11, step519]: loss 0.548842
[epoch11, step520]: loss 0.773768
[epoch11, step521]: loss 0.612799
[epoch11, step522]: loss 0.586241
[epoch11, step523]: loss 0.627830
[epoch11, step524]: loss 0.505131
[epoch11, step525]: loss 0.700357
[epoch11, step526]: loss 0.475976
[epoch11, step527]: loss 0.463622
[epoch11, step528]: loss 0.440708
[epoch11, step529]: loss 0.680618
[epoch11, step530]: loss 0.443158
[epoch11, step531]: loss 0.707520
[epoch11, step532]: loss 0.444781
[epoch11, step533]: loss 0.754553
[epoch11, step534]: loss 0.690293
[epoch11, step535]: loss 0.725740
[epoch11, step536]: loss 0.675092
[epoch11, step537]: loss 0.481045
[epoch11, step538]: loss 0.657950
[epoch11, step539]: loss 0.563033
[epoch11, step540]: loss 0.732704
[epoch11, step541]: loss 0.597603
[epoch11, step542]: loss 0.411121
[epoch11, step543]: loss 0.209392
[epoch11, step544]: loss 0.571481
[epoch11, step545]: loss 0.605780
[epoch11, step546]: loss 0.461657
[epoch11, step547]: loss 0.658038
[epoch11, step548]: loss 0.724085
[epoch11, step549]: loss 0.742569
[epoch11, step550]: loss 0.664719
[epoch11, step551]: loss 0.418162
[epoch11, step552]: loss 0.487012
[epoch11, step553]: loss 0.315879
[epoch11, step554]: loss 0.573513
[epoch11, step555]: loss 0.614815
[epoch11, step556]: loss 0.426611
[epoch11, step557]: loss 0.442294
[epoch11, step558]: loss 0.690576
[epoch11, step559]: loss 0.229350
[epoch11, step560]: loss 0.559629
[epoch11, step561]: loss 0.640491
[epoch11, step562]: loss 0.591597
[epoch11, step563]: loss 0.655832
[epoch11, step564]: loss 0.360565
[epoch11, step565]: loss 0.623861
[epoch11, step566]: loss 0.591315
[epoch11, step567]: loss 0.161579
[epoch11, step568]: loss 0.509379
[epoch11, step569]: loss 0.527244
[epoch11, step570]: loss 0.567989
[epoch11, step571]: loss 0.566049
[epoch11, step572]: loss 0.689802
[epoch11, step573]: loss 0.597058
[epoch11, step574]: loss 0.440092
[epoch11, step575]: loss 0.729972
[epoch11, step576]: loss 0.601071
[epoch11, step577]: loss 0.509985
[epoch11, step578]: loss 0.623996
[epoch11, step579]: loss 0.587313
[epoch11, step580]: loss 0.571438
[epoch11, step581]: loss 0.603112
[epoch11, step582]: loss 0.597761
[epoch11, step583]: loss 0.618010
[epoch11, step584]: loss 0.351829
[epoch11, step585]: loss 0.704879
[epoch11, step586]: loss 0.468439
[epoch11, step587]: loss 0.697152
[epoch11, step588]: loss 0.412801
[epoch11, step589]: loss 0.599213
[epoch11, step590]: loss 0.743291
[epoch11, step591]: loss 0.725244
[epoch11, step592]: loss 0.351485
[epoch11, step593]: loss 0.733156
[epoch11, step594]: loss 0.494438
[epoch11, step595]: loss 0.275506
[epoch11, step596]: loss 0.643964
[epoch11, step597]: loss 0.305061
[epoch11, step598]: loss 0.401507
[epoch11, step599]: loss 0.575390
[epoch11, step600]: loss 0.447015
[epoch11, step601]: loss 0.782764
[epoch11, step602]: loss 0.758946
[epoch11, step603]: loss 0.704647
[epoch11, step604]: loss 0.379731
[epoch11, step605]: loss 0.572995
[epoch11, step606]: loss 0.499750
[epoch11, step607]: loss 0.605105
[epoch11, step608]: loss 0.582967
[epoch11, step609]: loss 0.893312
[epoch11, step610]: loss 0.633700
[epoch11, step611]: loss 0.572854
[epoch11, step612]: loss 0.619183
[epoch11, step613]: loss 0.545957
[epoch11, step614]: loss 0.283162
[epoch11, step615]: loss 0.749801
[epoch11, step616]: loss 0.547262
[epoch11, step617]: loss 0.584497
[epoch11, step618]: loss 0.374182
[epoch11, step619]: loss 0.637393
[epoch11, step620]: loss 0.719526
[epoch11, step621]: loss 0.603911
[epoch11, step622]: loss 0.391396
[epoch11, step623]: loss 0.584698
[epoch11, step624]: loss 0.252685
[epoch11, step625]: loss 0.442347
[epoch11, step626]: loss 0.750949
[epoch11, step627]: loss 0.620100
[epoch11, step628]: loss 0.514062
[epoch11, step629]: loss 0.496986
[epoch11, step630]: loss 0.636777
[epoch11, step631]: loss 0.591581
[epoch11, step632]: loss 0.870435
[epoch11, step633]: loss 0.697970
[epoch11, step634]: loss 0.599675
[epoch11, step635]: loss 0.335569
[epoch11, step636]: loss 0.673351
[epoch11, step637]: loss 0.493577
[epoch11, step638]: loss 0.704132
[epoch11, step639]: loss 0.479186
[epoch11, step640]: loss 0.670480
[epoch11, step641]: loss 0.343418
[epoch11, step642]: loss 0.645820
[epoch11, step643]: loss 0.894916
[epoch11, step644]: loss 0.323667
[epoch11, step645]: loss 0.762665
[epoch11, step646]: loss 0.585997
[epoch11, step647]: loss 0.523420
[epoch11, step648]: loss 0.637993
[epoch11, step649]: loss 0.617986
[epoch11, step650]: loss 0.555498
[epoch11, step651]: loss 0.550848
[epoch11, step652]: loss 0.880368
[epoch11, step653]: loss 0.303661
[epoch11, step654]: loss 0.632277
[epoch11, step655]: loss 0.213209
[epoch11, step656]: loss 0.682298
[epoch11, step657]: loss 0.358670
[epoch11, step658]: loss 0.852217
[epoch11, step659]: loss 0.298318
[epoch11, step660]: loss 0.346627
[epoch11, step661]: loss 0.577583
[epoch11, step662]: loss 0.838816
[epoch11, step663]: loss 0.625665
[epoch11, step664]: loss 0.757687
[epoch11, step665]: loss 0.686845
[epoch11, step666]: loss 0.596386
[epoch11, step667]: loss 0.348414
[epoch11, step668]: loss 0.679330
[epoch11, step669]: loss 0.773948
[epoch11, step670]: loss 0.528133
[epoch11, step671]: loss 0.406785
[epoch11, step672]: loss 0.709530
[epoch11, step673]: loss 0.729787
[epoch11, step674]: loss 0.538724
[epoch11, step675]: loss 0.802028
[epoch11, step676]: loss 0.723669
[epoch11, step677]: loss 0.424502
[epoch11, step678]: loss 0.590169
[epoch11, step679]: loss 0.719882
[epoch11, step680]: loss 0.576639
[epoch11, step681]: loss 0.450605
[epoch11, step682]: loss 0.678704
[epoch11, step683]: loss 0.474250
[epoch11, step684]: loss 0.504417
[epoch11, step685]: loss 0.803928
[epoch11, step686]: loss 0.529312
[epoch11, step687]: loss 0.443297
[epoch11, step688]: loss 0.458315
[epoch11, step689]: loss 0.745143
[epoch11, step690]: loss 0.646396
[epoch11, step691]: loss 0.709690
[epoch11, step692]: loss 0.606290
[epoch11, step693]: loss 0.440636
[epoch11, step694]: loss 0.609763
[epoch11, step695]: loss 0.723979
[epoch11, step696]: loss 0.334950
[epoch11, step697]: loss 0.698589
[epoch11, step698]: loss 0.678462
[epoch11, step699]: loss 0.544199
[epoch11, step700]: loss 0.927545
[epoch11, step701]: loss 0.724740
[epoch11, step702]: loss 0.505985
[epoch11, step703]: loss 0.424353
[epoch11, step704]: loss 0.495315
[epoch11, step705]: loss 0.710398
[epoch11, step706]: loss 0.446746
[epoch11, step707]: loss 0.513815
[epoch11, step708]: loss 0.445386
[epoch11, step709]: loss 0.712209
[epoch11, step710]: loss 0.690725
[epoch11, step711]: loss 0.448702
[epoch11, step712]: loss 0.404009
[epoch11, step713]: loss 0.696472
[epoch11, step714]: loss 0.589969
[epoch11, step715]: loss 0.578760
[epoch11, step716]: loss 0.585605
[epoch11, step717]: loss 0.606028
[epoch11, step718]: loss 0.462060
[epoch11, step719]: loss 0.584363
[epoch11, step720]: loss 0.459444
[epoch11, step721]: loss 0.548161
[epoch11, step722]: loss 0.550564
[epoch11, step723]: loss 0.665813
[epoch11, step724]: loss 0.430478
[epoch11, step725]: loss 0.646089
[epoch11, step726]: loss 0.601368
[epoch11, step727]: loss 0.606622
[epoch11, step728]: loss 0.560057
[epoch11, step729]: loss 0.544276
[epoch11, step730]: loss 0.367919
[epoch11, step731]: loss 0.602741
[epoch11, step732]: loss 0.637697
[epoch11, step733]: loss 0.787481
[epoch11, step734]: loss 0.405623
[epoch11, step735]: loss 0.465351
[epoch11, step736]: loss 0.771146
[epoch11, step737]: loss 0.562705
[epoch11, step738]: loss 0.581782
[epoch11, step739]: loss 0.405044
[epoch11, step740]: loss 0.505966
[epoch11, step741]: loss 0.687348
[epoch11, step742]: loss 0.414736
[epoch11, step743]: loss 0.586529
[epoch11, step744]: loss 0.793355
[epoch11, step745]: loss 0.279146
[epoch11, step746]: loss 0.259348
[epoch11, step747]: loss 0.647554
[epoch11, step748]: loss 0.514805
[epoch11, step749]: loss 0.741154
[epoch11, step750]: loss 0.435015
[epoch11, step751]: loss 0.797550
[epoch11, step752]: loss 0.363396
[epoch11, step753]: loss 0.717049
[epoch11, step754]: loss 0.401840
[epoch11, step755]: loss 0.390495
[epoch11, step756]: loss 0.798387
[epoch11, step757]: loss 0.870856
[epoch11, step758]: loss 0.690688
[epoch11, step759]: loss 0.493692
[epoch11, step760]: loss 0.529101
[epoch11, step761]: loss 0.416793
[epoch11, step762]: loss 0.388190
[epoch11, step763]: loss 0.631571
[epoch11, step764]: loss 0.651727
[epoch11, step765]: loss 0.757520
[epoch11, step766]: loss 0.410778
[epoch11, step767]: loss 0.441821
[epoch11, step768]: loss 0.653457
[epoch11, step769]: loss 0.127614
[epoch11, step770]: loss 0.473015
[epoch11, step771]: loss 0.697393
[epoch11, step772]: loss 0.537932
[epoch11, step773]: loss 0.503728
[epoch11, step774]: loss 0.516606
[epoch11, step775]: loss 0.190742
[epoch11, step776]: loss 0.537818
[epoch11, step777]: loss 0.538134
[epoch11, step778]: loss 0.475090
[epoch11, step779]: loss 0.609726
[epoch11, step780]: loss 0.589570
[epoch11, step781]: loss 0.461394
[epoch11, step782]: loss 0.960799
[epoch11, step783]: loss 0.609588
[epoch11, step784]: loss 0.772104
[epoch11, step785]: loss 0.744963
[epoch11, step786]: loss 0.496708
[epoch11, step787]: loss 0.577974
[epoch11, step788]: loss 0.272956
[epoch11, step789]: loss 0.547744
[epoch11, step790]: loss 0.515944
[epoch11, step791]: loss 0.376486
[epoch11, step792]: loss 0.611180
[epoch11, step793]: loss 0.497115
[epoch11, step794]: loss 0.599624
[epoch11, step795]: loss 0.594247
[epoch11, step796]: loss 0.488309
[epoch11, step797]: loss 0.665751
[epoch11, step798]: loss 0.577578
[epoch11, step799]: loss 0.575806
[epoch11, step800]: loss 0.637043
[epoch11, step801]: loss 0.665925
[epoch11, step802]: loss 0.376011
[epoch11, step803]: loss 0.730117
[epoch11, step804]: loss 0.751789
[epoch11, step805]: loss 0.583916
[epoch11, step806]: loss 0.668249
[epoch11, step807]: loss 0.424442
[epoch11, step808]: loss 0.409869
[epoch11, step809]: loss 0.628010
[epoch11, step810]: loss 0.550965
[epoch11, step811]: loss 0.776773
[epoch11, step812]: loss 0.617078
[epoch11, step813]: loss 0.339868
[epoch11, step814]: loss 0.774213
[epoch11, step815]: loss 0.633583
[epoch11, step816]: loss 0.820293
[epoch11, step817]: loss 0.393793
[epoch11, step818]: loss 0.511992
[epoch11, step819]: loss 0.518227
[epoch11, step820]: loss 0.660513
[epoch11, step821]: loss 0.614380
[epoch11, step822]: loss 0.695213
[epoch11, step823]: loss 0.434939
[epoch11, step824]: loss 0.454224
[epoch11, step825]: loss 0.512942
[epoch11, step826]: loss 0.716879
[epoch11, step827]: loss 0.758217
[epoch11, step828]: loss 0.755979
[epoch11, step829]: loss 0.637644
[epoch11, step830]: loss 0.724455
[epoch11, step831]: loss 0.376428
[epoch11, step832]: loss 0.738250
[epoch11, step833]: loss 0.633458
[epoch11, step834]: loss 0.474045
[epoch11, step835]: loss 0.473738
[epoch11, step836]: loss 0.529607
[epoch11, step837]: loss 0.413200
[epoch11, step838]: loss 0.557122
[epoch11, step839]: loss 0.669372
[epoch11, step840]: loss 0.494287
[epoch11, step841]: loss 0.763871
[epoch11, step842]: loss 0.477321
[epoch11, step843]: loss 0.530071
[epoch11, step844]: loss 0.627410
[epoch11, step845]: loss 0.792848
[epoch11, step846]: loss 0.589218
[epoch11, step847]: loss 0.556753
[epoch11, step848]: loss 0.485757
[epoch11, step849]: loss 0.497737
[epoch11, step850]: loss 0.445239
[epoch11, step851]: loss 0.436442
[epoch11, step852]: loss 0.520933
[epoch11, step853]: loss 0.731157
[epoch11, step854]: loss 0.629205
[epoch11, step855]: loss 0.722423
[epoch11, step856]: loss 0.558462
[epoch11, step857]: loss 0.541428
[epoch11, step858]: loss 0.559829
[epoch11, step859]: loss 0.524353
[epoch11, step860]: loss 0.537261
[epoch11, step861]: loss 0.504007
[epoch11, step862]: loss 0.921839
[epoch11, step863]: loss 0.647908
[epoch11, step864]: loss 0.464090
[epoch11, step865]: loss 0.534216
[epoch11, step866]: loss 0.296399
[epoch11, step867]: loss 0.407249
[epoch11, step868]: loss 0.525536
[epoch11, step869]: loss 0.408390
[epoch11, step870]: loss 0.720758
[epoch11, step871]: loss 0.400973
[epoch11, step872]: loss 0.739783
[epoch11, step873]: loss 0.632761
[epoch11, step874]: loss 0.684442
[epoch11, step875]: loss 0.567656
[epoch11, step876]: loss 0.410186
[epoch11, step877]: loss 0.357003
[epoch11, step878]: loss 0.700453
[epoch11, step879]: loss 0.634280
[epoch11, step880]: loss 0.593446
[epoch11, step881]: loss 0.483905
[epoch11, step882]: loss 0.427580
[epoch11, step883]: loss 0.680254
[epoch11, step884]: loss 0.692060
[epoch11, step885]: loss 0.610798
[epoch11, step886]: loss 0.884426
[epoch11, step887]: loss 0.801092
[epoch11, step888]: loss 0.334026
[epoch11, step889]: loss 0.794731
[epoch11, step890]: loss 0.798519
[epoch11, step891]: loss 0.568065
[epoch11, step892]: loss 0.365296
[epoch11, step893]: loss 0.704488
[epoch11, step894]: loss 0.627457
[epoch11, step895]: loss 0.584668
[epoch11, step896]: loss 0.403459
[epoch11, step897]: loss 0.280447
[epoch11, step898]: loss 0.718488
[epoch11, step899]: loss 0.805451
[epoch11, step900]: loss 0.557483
[epoch11, step901]: loss 0.505593
[epoch11, step902]: loss 0.623923
[epoch11, step903]: loss 0.526851
[epoch11, step904]: loss 0.865677
[epoch11, step905]: loss 0.539387
[epoch11, step906]: loss 0.517268
[epoch11, step907]: loss 0.640577
[epoch11, step908]: loss 0.505121
[epoch11, step909]: loss 0.662436
[epoch11, step910]: loss 0.504201
[epoch11, step911]: loss 0.581992
[epoch11, step912]: loss 0.581521
[epoch11, step913]: loss 0.542980
[epoch11, step914]: loss 0.511062
[epoch11, step915]: loss 0.428574
[epoch11, step916]: loss 0.535215
[epoch11, step917]: loss 0.604449
[epoch11, step918]: loss 0.754809
[epoch11, step919]: loss 0.560903
[epoch11, step920]: loss 0.278936
[epoch11, step921]: loss 0.565115
[epoch11, step922]: loss 0.538232
[epoch11, step923]: loss 0.610280
[epoch11, step924]: loss 0.499945
[epoch11, step925]: loss 0.383143
[epoch11, step926]: loss 0.735252
[epoch11, step927]: loss 0.426725
[epoch11, step928]: loss 0.670314
[epoch11, step929]: loss 0.675044
[epoch11, step930]: loss 0.591482
[epoch11, step931]: loss 0.828125
[epoch11, step932]: loss 0.460289
[epoch11, step933]: loss 0.470374
[epoch11, step934]: loss 0.556001
[epoch11, step935]: loss 0.503291
[epoch11, step936]: loss 0.378376
[epoch11, step937]: loss 0.816124
[epoch11, step938]: loss 0.633827
[epoch11, step939]: loss 0.426712
[epoch11, step940]: loss 0.525764
[epoch11, step941]: loss 0.740431
[epoch11, step942]: loss 0.372598
[epoch11, step943]: loss 0.551440
[epoch11, step944]: loss 0.202779
[epoch11, step945]: loss 0.699185
[epoch11, step946]: loss 0.342053
[epoch11, step947]: loss 0.500271
[epoch11, step948]: loss 0.666311
[epoch11, step949]: loss 0.442765
[epoch11, step950]: loss 0.614292
[epoch11, step951]: loss 0.662543
[epoch11, step952]: loss 0.490715
[epoch11, step953]: loss 0.765700
[epoch11, step954]: loss 0.601769
[epoch11, step955]: loss 0.605926
[epoch11, step956]: loss 0.257056
[epoch11, step957]: loss 0.674032
[epoch11, step958]: loss 0.615048
[epoch11, step959]: loss 0.546917
[epoch11, step960]: loss 0.711500
[epoch11, step961]: loss 0.557544
[epoch11, step962]: loss 0.508357
[epoch11, step963]: loss 0.503119
[epoch11, step964]: loss 0.802293
[epoch11, step965]: loss 0.532975
[epoch11, step966]: loss 0.488815
[epoch11, step967]: loss 0.481422
[epoch11, step968]: loss 0.477372
[epoch11, step969]: loss 0.456772
[epoch11, step970]: loss 0.690842
[epoch11, step971]: loss 0.483020
[epoch11, step972]: loss 0.705653
[epoch11, step973]: loss 0.427852
[epoch11, step974]: loss 0.615696
[epoch11, step975]: loss 0.636492
[epoch11, step976]: loss 0.736320
[epoch11, step977]: loss 0.630103
[epoch11, step978]: loss 0.615006
[epoch11, step979]: loss 0.641991
[epoch11, step980]: loss 0.652258
[epoch11, step981]: loss 0.581181
[epoch11, step982]: loss 0.712619
[epoch11, step983]: loss 0.844811
[epoch11, step984]: loss 0.342132
[epoch11, step985]: loss 0.777532
[epoch11, step986]: loss 0.661641
[epoch11, step987]: loss 0.537352
[epoch11, step988]: loss 0.588065
[epoch11, step989]: loss 0.514237
[epoch11, step990]: loss 0.810483
[epoch11, step991]: loss 0.718664
[epoch11, step992]: loss 0.561790
[epoch11, step993]: loss 0.792151
[epoch11, step994]: loss 0.380071
[epoch11, step995]: loss 0.392742
[epoch11, step996]: loss 0.497398
[epoch11, step997]: loss 0.441162
[epoch11, step998]: loss 0.528697
[epoch11, step999]: loss 0.495659
[epoch11, step1000]: loss 0.665516
[epoch11, step1001]: loss 0.459338
[epoch11, step1002]: loss 0.740378
[epoch11, step1003]: loss 0.349492
[epoch11, step1004]: loss 0.709905
[epoch11, step1005]: loss 0.456370
[epoch11, step1006]: loss 0.471296
[epoch11, step1007]: loss 0.725425
[epoch11, step1008]: loss 0.477867
[epoch11, step1009]: loss 0.630292
[epoch11, step1010]: loss 0.415650
[epoch11, step1011]: loss 0.349150
[epoch11, step1012]: loss 0.594679
[epoch11, step1013]: loss 0.521641
[epoch11, step1014]: loss 0.801988
[epoch11, step1015]: loss 0.513861
[epoch11, step1016]: loss 0.571526
[epoch11, step1017]: loss 0.633735
[epoch11, step1018]: loss 0.384012
[epoch11, step1019]: loss 0.385463
[epoch11, step1020]: loss 0.435014
[epoch11, step1021]: loss 0.491279
[epoch11, step1022]: loss 0.319671
[epoch11, step1023]: loss 0.566955
[epoch11, step1024]: loss 0.463149
[epoch11, step1025]: loss 0.728426
[epoch11, step1026]: loss 0.269238
[epoch11, step1027]: loss 0.500933
[epoch11, step1028]: loss 0.679318
[epoch11, step1029]: loss 0.519656
[epoch11, step1030]: loss 0.468452
[epoch11, step1031]: loss 0.414218
[epoch11, step1032]: loss 0.445817
[epoch11, step1033]: loss 0.448615
[epoch11, step1034]: loss 0.466759
[epoch11, step1035]: loss 0.726685
[epoch11, step1036]: loss 0.636359
[epoch11, step1037]: loss 0.491051
[epoch11, step1038]: loss 0.656912
[epoch11, step1039]: loss 0.400411
[epoch11, step1040]: loss 0.819013
[epoch11, step1041]: loss 0.697601
[epoch11, step1042]: loss 0.490533
[epoch11, step1043]: loss 0.495571
[epoch11, step1044]: loss 0.713707
[epoch11, step1045]: loss 0.355670
[epoch11, step1046]: loss 0.669750
[epoch11, step1047]: loss 0.441065
[epoch11, step1048]: loss 0.610187
[epoch11, step1049]: loss 0.449598
[epoch11, step1050]: loss 0.474625
[epoch11, step1051]: loss 0.773266
[epoch11, step1052]: loss 0.447587
[epoch11, step1053]: loss 0.645995
[epoch11, step1054]: loss 0.378174
[epoch11, step1055]: loss 0.784144
[epoch11, step1056]: loss 0.390875
[epoch11, step1057]: loss 0.576962
[epoch11, step1058]: loss 0.299119
[epoch11, step1059]: loss 0.688845
[epoch11, step1060]: loss 0.685164
[epoch11, step1061]: loss 0.322922
[epoch11, step1062]: loss 0.567661
[epoch11, step1063]: loss 0.473938
[epoch11, step1064]: loss 0.561514
[epoch11, step1065]: loss 0.731747
[epoch11, step1066]: loss 0.745729
[epoch11, step1067]: loss 0.475188
[epoch11, step1068]: loss 0.631956
[epoch11, step1069]: loss 0.600093
[epoch11, step1070]: loss 0.366232
[epoch11, step1071]: loss 0.352081
[epoch11, step1072]: loss 0.727463
[epoch11, step1073]: loss 0.545383
[epoch11, step1074]: loss 0.353648
[epoch11, step1075]: loss 0.593175
[epoch11, step1076]: loss 0.491022
[epoch11, step1077]: loss 0.608545
[epoch11, step1078]: loss 0.776245
[epoch11, step1079]: loss 0.764915
[epoch11, step1080]: loss 0.553034
[epoch11, step1081]: loss 0.425426
[epoch11, step1082]: loss 0.433035
[epoch11, step1083]: loss 0.498924
[epoch11, step1084]: loss 0.901738
[epoch11, step1085]: loss 0.454418
[epoch11, step1086]: loss 0.517637
[epoch11, step1087]: loss 0.551685
[epoch11, step1088]: loss 0.386427
[epoch11, step1089]: loss 0.453504
[epoch11, step1090]: loss 0.789818
[epoch11, step1091]: loss 0.703050
[epoch11, step1092]: loss 0.501696
[epoch11, step1093]: loss 0.603339
[epoch11, step1094]: loss 0.453109
[epoch11, step1095]: loss 0.758981
[epoch11, step1096]: loss 0.662472
[epoch11, step1097]: loss 0.743213
[epoch11, step1098]: loss 0.569491
[epoch11, step1099]: loss 0.437821
[epoch11, step1100]: loss 0.625255
[epoch11, step1101]: loss 0.390642
[epoch11, step1102]: loss 0.413444
[epoch11, step1103]: loss 0.626082
[epoch11, step1104]: loss 0.634123
[epoch11, step1105]: loss 0.758000
[epoch11, step1106]: loss 0.421506
[epoch11, step1107]: loss 0.431223
[epoch11, step1108]: loss 0.285744
[epoch11, step1109]: loss 0.833890
[epoch11, step1110]: loss 0.603684
[epoch11, step1111]: loss 0.580072
[epoch11, step1112]: loss 0.499524
[epoch11, step1113]: loss 0.297317
[epoch11, step1114]: loss 0.438135
[epoch11, step1115]: loss 0.434203
[epoch11, step1116]: loss 0.529460
[epoch11, step1117]: loss 0.647695
[epoch11, step1118]: loss 0.411986
[epoch11, step1119]: loss 0.388152
[epoch11, step1120]: loss 0.378948
[epoch11, step1121]: loss 0.266052
[epoch11, step1122]: loss 0.470994
[epoch11, step1123]: loss 0.852587
[epoch11, step1124]: loss 0.603797
[epoch11, step1125]: loss 0.493706
[epoch11, step1126]: loss 0.632473
[epoch11, step1127]: loss 0.601943
[epoch11, step1128]: loss 0.400929
[epoch11, step1129]: loss 0.540976
[epoch11, step1130]: loss 0.610867
[epoch11, step1131]: loss 0.706415
[epoch11, step1132]: loss 0.688276
[epoch11, step1133]: loss 0.485316
[epoch11, step1134]: loss 0.416605
[epoch11, step1135]: loss 0.587600
[epoch11, step1136]: loss 0.705035
[epoch11, step1137]: loss 0.688505
[epoch11, step1138]: loss 0.635664
[epoch11, step1139]: loss 0.425283
[epoch11, step1140]: loss 0.842279
[epoch11, step1141]: loss 0.631004
[epoch11, step1142]: loss 0.586183
[epoch11, step1143]: loss 0.816842
[epoch11, step1144]: loss 0.805727
[epoch11, step1145]: loss 0.864660
[epoch11, step1146]: loss 0.529578
[epoch11, step1147]: loss 0.722175
[epoch11, step1148]: loss 0.646149
[epoch11, step1149]: loss 0.487366
[epoch11, step1150]: loss 0.550539
[epoch11, step1151]: loss 0.401929
[epoch11, step1152]: loss 0.673513
[epoch11, step1153]: loss 0.374392
[epoch11, step1154]: loss 0.512305
[epoch11, step1155]: loss 0.801379
[epoch11, step1156]: loss 0.567382
[epoch11, step1157]: loss 0.477725
[epoch11, step1158]: loss 0.717211
[epoch11, step1159]: loss 0.445431
[epoch11, step1160]: loss 0.782304
[epoch11, step1161]: loss 0.685875
[epoch11, step1162]: loss 0.659212
[epoch11, step1163]: loss 0.573835
[epoch11, step1164]: loss 0.640206
[epoch11, step1165]: loss 0.662124
[epoch11, step1166]: loss 0.667126
[epoch11, step1167]: loss 0.498814
[epoch11, step1168]: loss 0.465036
[epoch11, step1169]: loss 0.733918
[epoch11, step1170]: loss 0.562451
[epoch11, step1171]: loss 0.618366
[epoch11, step1172]: loss 0.451718
[epoch11, step1173]: loss 0.619268
[epoch11, step1174]: loss 0.576548
[epoch11, step1175]: loss 0.433422
[epoch11, step1176]: loss 0.482792
[epoch11, step1177]: loss 0.507816
[epoch11, step1178]: loss 0.618945
[epoch11, step1179]: loss 0.657664
[epoch11, step1180]: loss 0.558425
[epoch11, step1181]: loss 0.592456
[epoch11, step1182]: loss 0.563955
[epoch11, step1183]: loss 0.693093
[epoch11, step1184]: loss 0.699412
[epoch11, step1185]: loss 0.713002
[epoch11, step1186]: loss 0.452582
[epoch11, step1187]: loss 0.638491
[epoch11, step1188]: loss 0.703943
[epoch11, step1189]: loss 0.550782
[epoch11, step1190]: loss 0.613790
[epoch11, step1191]: loss 0.370832
[epoch11, step1192]: loss 0.634370
[epoch11, step1193]: loss 0.500831
[epoch11, step1194]: loss 0.802988
[epoch11, step1195]: loss 0.519805
[epoch11, step1196]: loss 0.436174
[epoch11, step1197]: loss 0.529714
[epoch11, step1198]: loss 0.521601
[epoch11, step1199]: loss 0.834111
[epoch11, step1200]: loss 0.734240
[epoch11, step1201]: loss 0.333173
[epoch11, step1202]: loss 0.635212
[epoch11, step1203]: loss 0.639611
[epoch11, step1204]: loss 0.548265
[epoch11, step1205]: loss 0.747025
[epoch11, step1206]: loss 0.740409
[epoch11, step1207]: loss 0.674152
[epoch11, step1208]: loss 0.831732
[epoch11, step1209]: loss 0.698971
[epoch11, step1210]: loss 0.709560
[epoch11, step1211]: loss 0.646978
[epoch11, step1212]: loss 0.688573
[epoch11, step1213]: loss 0.203191
[epoch11, step1214]: loss 0.608380
[epoch11, step1215]: loss 0.773429
[epoch11, step1216]: loss 0.639152
[epoch11, step1217]: loss 0.765784
[epoch11, step1218]: loss 0.538747
[epoch11, step1219]: loss 0.705554
[epoch11, step1220]: loss 0.771410
[epoch11, step1221]: loss 0.848556
[epoch11, step1222]: loss 0.776455
[epoch11, step1223]: loss 0.376340
[epoch11, step1224]: loss 0.747027
[epoch11, step1225]: loss 0.700162
[epoch11, step1226]: loss 0.580797
[epoch11, step1227]: loss 0.656541
[epoch11, step1228]: loss 0.612367
[epoch11, step1229]: loss 0.473958
[epoch11, step1230]: loss 0.442051
[epoch11, step1231]: loss 0.712974
[epoch11, step1232]: loss 0.658698
[epoch11, step1233]: loss 0.728300
[epoch11, step1234]: loss 0.519687
[epoch11, step1235]: loss 0.630869
[epoch11, step1236]: loss 0.325168
[epoch11, step1237]: loss 0.479790
[epoch11, step1238]: loss 0.785619
[epoch11, step1239]: loss 0.406390
[epoch11, step1240]: loss 0.163462
[epoch11, step1241]: loss 0.738238
[epoch11, step1242]: loss 0.686234
[epoch11, step1243]: loss 0.585404
[epoch11, step1244]: loss 0.591271
[epoch11, step1245]: loss 0.777133
[epoch11, step1246]: loss 0.563632
[epoch11, step1247]: loss 0.573251
[epoch11, step1248]: loss 0.318377
[epoch11, step1249]: loss 0.682291
[epoch11, step1250]: loss 0.561988
[epoch11, step1251]: loss 0.656443
[epoch11, step1252]: loss 0.336753
[epoch11, step1253]: loss 0.780439
[epoch11, step1254]: loss 0.792682
[epoch11, step1255]: loss 0.297367
[epoch11, step1256]: loss 0.582000
[epoch11, step1257]: loss 0.574678
[epoch11, step1258]: loss 0.723087
[epoch11, step1259]: loss 0.557685
[epoch11, step1260]: loss 0.793676
[epoch11, step1261]: loss 0.445328
[epoch11, step1262]: loss 0.729790
[epoch11, step1263]: loss 0.668987
[epoch11, step1264]: loss 0.731786
[epoch11, step1265]: loss 0.553196
[epoch11, step1266]: loss 0.723975
[epoch11, step1267]: loss 0.502145
[epoch11, step1268]: loss 0.685544
[epoch11, step1269]: loss 0.557510
[epoch11, step1270]: loss 0.361474
[epoch11, step1271]: loss 0.603861
[epoch11, step1272]: loss 0.809588
[epoch11, step1273]: loss 0.501982
[epoch11, step1274]: loss 0.521363
[epoch11, step1275]: loss 0.485523
[epoch11, step1276]: loss 0.617932
[epoch11, step1277]: loss 0.582385
[epoch11, step1278]: loss 0.714356
[epoch11, step1279]: loss 0.668615
[epoch11, step1280]: loss 0.905406
[epoch11, step1281]: loss 0.670096
[epoch11, step1282]: loss 0.642627
[epoch11, step1283]: loss 0.604813
[epoch11, step1284]: loss 0.607518
[epoch11, step1285]: loss 0.597499
[epoch11, step1286]: loss 0.682416
[epoch11, step1287]: loss 0.401748
[epoch11, step1288]: loss 0.671303
[epoch11, step1289]: loss 0.538716
[epoch11, step1290]: loss 0.653441
[epoch11, step1291]: loss 0.444608
[epoch11, step1292]: loss 0.462851
[epoch11, step1293]: loss 0.695376
[epoch11, step1294]: loss 0.449894
[epoch11, step1295]: loss 0.740137
[epoch11, step1296]: loss 0.713834
[epoch11, step1297]: loss 0.478125
[epoch11, step1298]: loss 0.673504
[epoch11, step1299]: loss 0.619626
[epoch11, step1300]: loss 0.321855
[epoch11, step1301]: loss 0.614569
[epoch11, step1302]: loss 0.438171
[epoch11, step1303]: loss 0.514926
[epoch11, step1304]: loss 0.498064
[epoch11, step1305]: loss 0.542124
[epoch11, step1306]: loss 0.585400
[epoch11, step1307]: loss 0.169363
[epoch11, step1308]: loss 0.572487
[epoch11, step1309]: loss 0.411494
[epoch11, step1310]: loss 0.414288
[epoch11, step1311]: loss 0.518129
[epoch11, step1312]: loss 0.632467
[epoch11, step1313]: loss 0.572157
[epoch11, step1314]: loss 0.593554
[epoch11, step1315]: loss 0.525980
[epoch11, step1316]: loss 0.484240
[epoch11, step1317]: loss 0.684241
[epoch11, step1318]: loss 0.410257
[epoch11, step1319]: loss 0.431020
[epoch11, step1320]: loss 0.661586
[epoch11, step1321]: loss 0.665850
[epoch11, step1322]: loss 0.553947
[epoch11, step1323]: loss 0.819216
[epoch11, step1324]: loss 0.705704
[epoch11, step1325]: loss 0.494552
[epoch11, step1326]: loss 0.660847
[epoch11, step1327]: loss 0.690386
[epoch11, step1328]: loss 0.548894
[epoch11, step1329]: loss 0.786220
[epoch11, step1330]: loss 0.739102
[epoch11, step1331]: loss 0.410617
[epoch11, step1332]: loss 0.549967
[epoch11, step1333]: loss 0.605043
[epoch11, step1334]: loss 0.585320
[epoch11, step1335]: loss 0.771290
[epoch11, step1336]: loss 0.260229
[epoch11, step1337]: loss 0.629602
[epoch11, step1338]: loss 0.757083
[epoch11, step1339]: loss 0.635561
[epoch11, step1340]: loss 0.653259
[epoch11, step1341]: loss 0.747322
[epoch11, step1342]: loss 0.593145
[epoch11, step1343]: loss 0.550072
[epoch11, step1344]: loss 0.629027
[epoch11, step1345]: loss 0.539331
[epoch11, step1346]: loss 0.745632
[epoch11, step1347]: loss 0.455230
[epoch11, step1348]: loss 0.667293
[epoch11, step1349]: loss 0.389207
[epoch11, step1350]: loss 0.636793
[epoch11, step1351]: loss 0.642273
[epoch11, step1352]: loss 0.671038
[epoch11, step1353]: loss 0.640830
[epoch11, step1354]: loss 0.436472
[epoch11, step1355]: loss 0.638378
[epoch11, step1356]: loss 0.295690
[epoch11, step1357]: loss 0.427390
[epoch11, step1358]: loss 0.603224
[epoch11, step1359]: loss 0.530864
[epoch11, step1360]: loss 0.491810
[epoch11, step1361]: loss 0.530602
[epoch11, step1362]: loss 0.530766
[epoch11, step1363]: loss 0.796590
[epoch11, step1364]: loss 0.672897
[epoch11, step1365]: loss 0.487165
[epoch11, step1366]: loss 0.519192
[epoch11, step1367]: loss 0.727687
[epoch11, step1368]: loss 0.704819
[epoch11, step1369]: loss 0.735890
[epoch11, step1370]: loss 0.536024
[epoch11, step1371]: loss 0.939720
[epoch11, step1372]: loss 0.533321
[epoch11, step1373]: loss 0.543242
[epoch11, step1374]: loss 0.661876
[epoch11, step1375]: loss 0.545618
[epoch11, step1376]: loss 0.607013
[epoch11, step1377]: loss 0.632278
[epoch11, step1378]: loss 0.406430
[epoch11, step1379]: loss 0.363556
[epoch11, step1380]: loss 0.631289
[epoch11, step1381]: loss 0.573490
[epoch11, step1382]: loss 0.580002
[epoch11, step1383]: loss 0.632334
[epoch11, step1384]: loss 0.565161
[epoch11, step1385]: loss 0.562721
[epoch11, step1386]: loss 0.522290
[epoch11, step1387]: loss 0.601140
[epoch11, step1388]: loss 0.329076
[epoch11, step1389]: loss 0.626886
[epoch11, step1390]: loss 0.494827
[epoch11, step1391]: loss 0.813747
[epoch11, step1392]: loss 0.382660
[epoch11, step1393]: loss 0.689193
[epoch11, step1394]: loss 0.489485
[epoch11, step1395]: loss 0.611937
[epoch11, step1396]: loss 0.408995
[epoch11, step1397]: loss 0.677091
[epoch11, step1398]: loss 0.574376
[epoch11, step1399]: loss 0.454447
[epoch11, step1400]: loss 0.720971
[epoch11, step1401]: loss 0.798939
[epoch11, step1402]: loss 0.365419
[epoch11, step1403]: loss 0.666406
[epoch11, step1404]: loss 0.628659
[epoch11, step1405]: loss 0.758449
[epoch11, step1406]: loss 0.529735
[epoch11, step1407]: loss 0.687238
[epoch11, step1408]: loss 0.559657
[epoch11, step1409]: loss 0.454898
[epoch11, step1410]: loss 0.530882
[epoch11, step1411]: loss 0.647181
[epoch11, step1412]: loss 0.700482
[epoch11, step1413]: loss 0.659453
[epoch11, step1414]: loss 0.707025
[epoch11, step1415]: loss 0.583467
[epoch11, step1416]: loss 0.536529
[epoch11, step1417]: loss 0.670092
[epoch11, step1418]: loss 0.536480
[epoch11, step1419]: loss 0.360862
[epoch11, step1420]: loss 0.675459
[epoch11, step1421]: loss 0.445576
[epoch11, step1422]: loss 0.741734
[epoch11, step1423]: loss 0.462223
[epoch11, step1424]: loss 0.403353
[epoch11, step1425]: loss 0.408361
[epoch11, step1426]: loss 0.713597
[epoch11, step1427]: loss 0.519557
[epoch11, step1428]: loss 0.499022
[epoch11, step1429]: loss 0.530703
[epoch11, step1430]: loss 0.711567
[epoch11, step1431]: loss 0.615736
[epoch11, step1432]: loss 0.478853
[epoch11, step1433]: loss 0.608984
[epoch11, step1434]: loss 0.535231
[epoch11, step1435]: loss 0.619650
[epoch11, step1436]: loss 0.523331
[epoch11, step1437]: loss 0.441587
[epoch11, step1438]: loss 0.296156
[epoch11, step1439]: loss 0.558028
[epoch11, step1440]: loss 0.392417
[epoch11, step1441]: loss 0.687925
[epoch11, step1442]: loss 0.520917
[epoch11, step1443]: loss 0.817288
[epoch11, step1444]: loss 0.315216
[epoch11, step1445]: loss 0.766530
[epoch11, step1446]: loss 0.481595
[epoch11, step1447]: loss 0.654627
[epoch11, step1448]: loss 0.668810
[epoch11, step1449]: loss 0.720918
[epoch11, step1450]: loss 0.349721
[epoch11, step1451]: loss 0.270663
[epoch11, step1452]: loss 0.345639
[epoch11, step1453]: loss 0.581808
[epoch11, step1454]: loss 0.796929
[epoch11, step1455]: loss 0.534624
[epoch11, step1456]: loss 0.520160
[epoch11, step1457]: loss 0.723913
[epoch11, step1458]: loss 0.686367
[epoch11, step1459]: loss 0.654441
[epoch11, step1460]: loss 0.476874
[epoch11, step1461]: loss 0.409196
[epoch11, step1462]: loss 0.682724
[epoch11, step1463]: loss 0.626545
[epoch11, step1464]: loss 0.532425
[epoch11, step1465]: loss 0.669708
[epoch11, step1466]: loss 0.634029
[epoch11, step1467]: loss 0.529962
[epoch11, step1468]: loss 0.661017
[epoch11, step1469]: loss 0.617480
[epoch11, step1470]: loss 0.677160
[epoch11, step1471]: loss 0.604272
[epoch11, step1472]: loss 0.338664
[epoch11, step1473]: loss 0.377428
[epoch11, step1474]: loss 0.480548
[epoch11, step1475]: loss 0.530568
[epoch11, step1476]: loss 0.832448
[epoch11, step1477]: loss 0.236910
[epoch11, step1478]: loss 0.570569
[epoch11, step1479]: loss 0.629559
[epoch11, step1480]: loss 0.567651
[epoch11, step1481]: loss 0.614749
[epoch11, step1482]: loss 0.318582
[epoch11, step1483]: loss 0.355245
[epoch11, step1484]: loss 0.383069
[epoch11, step1485]: loss 0.462000
[epoch11, step1486]: loss 0.768556
[epoch11, step1487]: loss 0.557077
[epoch11, step1488]: loss 0.743907
[epoch11, step1489]: loss 0.602457
[epoch11, step1490]: loss 0.393175
[epoch11, step1491]: loss 0.538491
[epoch11, step1492]: loss 0.533536
[epoch11, step1493]: loss 0.426943
[epoch11, step1494]: loss 0.399913
[epoch11, step1495]: loss 0.454018
[epoch11, step1496]: loss 0.516914
[epoch11, step1497]: loss 0.628695
[epoch11, step1498]: loss 0.503658
[epoch11, step1499]: loss 0.463519
[epoch11, step1500]: loss 0.328311
[epoch11, step1501]: loss 0.708058
[epoch11, step1502]: loss 0.412030
[epoch11, step1503]: loss 0.459393
[epoch11, step1504]: loss 0.690099
[epoch11, step1505]: loss 0.783725
[epoch11, step1506]: loss 0.617661
[epoch11, step1507]: loss 0.496913
[epoch11, step1508]: loss 0.661811
[epoch11, step1509]: loss 0.516808
[epoch11, step1510]: loss 0.651304
[epoch11, step1511]: loss 0.504459
[epoch11, step1512]: loss 0.456759
[epoch11, step1513]: loss 0.596374
[epoch11, step1514]: loss 0.255654
[epoch11, step1515]: loss 0.527337
[epoch11, step1516]: loss 0.697154
[epoch11, step1517]: loss 0.355904
[epoch11, step1518]: loss 0.328233
[epoch11, step1519]: loss 0.420520
[epoch11, step1520]: loss 0.414672
[epoch11, step1521]: loss 0.717439
[epoch11, step1522]: loss 0.860714
[epoch11, step1523]: loss 0.550823
[epoch11, step1524]: loss 0.805296
[epoch11, step1525]: loss 0.574452
[epoch11, step1526]: loss 0.690059
[epoch11, step1527]: loss 0.793117
[epoch11, step1528]: loss 0.618668
[epoch11, step1529]: loss 0.697809
[epoch11, step1530]: loss 0.404635
[epoch11, step1531]: loss 0.526304
[epoch11, step1532]: loss 0.774418
[epoch11, step1533]: loss 0.674674
[epoch11, step1534]: loss 0.471264
[epoch11, step1535]: loss 0.486713
[epoch11, step1536]: loss 0.827318
[epoch11, step1537]: loss 0.453466
[epoch11, step1538]: loss 0.673849
[epoch11, step1539]: loss 0.634690
[epoch11, step1540]: loss 0.603932
[epoch11, step1541]: loss 0.686832
[epoch11, step1542]: loss 0.649261
[epoch11, step1543]: loss 0.518255
[epoch11, step1544]: loss 0.899263
[epoch11, step1545]: loss 0.561008
[epoch11, step1546]: loss 0.356796
[epoch11, step1547]: loss 0.622391
[epoch11, step1548]: loss 0.688264
[epoch11, step1549]: loss 0.470740
[epoch11, step1550]: loss 0.543918
[epoch11, step1551]: loss 0.483887
[epoch11, step1552]: loss 0.438952
[epoch11, step1553]: loss 0.673276
[epoch11, step1554]: loss 0.508642
[epoch11, step1555]: loss 0.743354
[epoch11, step1556]: loss 0.495429
[epoch11, step1557]: loss 0.441537
[epoch11, step1558]: loss 0.867682
[epoch11, step1559]: loss 0.321703
[epoch11, step1560]: loss 0.414565
[epoch11, step1561]: loss 0.565435
[epoch11, step1562]: loss 0.477231
[epoch11, step1563]: loss 0.625351
[epoch11, step1564]: loss 0.636150
[epoch11, step1565]: loss 0.734262
[epoch11, step1566]: loss 0.349666
[epoch11, step1567]: loss 0.618212
[epoch11, step1568]: loss 0.668894
[epoch11, step1569]: loss 0.601244
[epoch11, step1570]: loss 0.643053
[epoch11, step1571]: loss 0.405462
[epoch11, step1572]: loss 0.785394
[epoch11, step1573]: loss 0.798104
[epoch11, step1574]: loss 0.363204
[epoch11, step1575]: loss 0.590979
[epoch11, step1576]: loss 0.325578
[epoch11, step1577]: loss 0.623242
[epoch11, step1578]: loss 0.513323
[epoch11, step1579]: loss 0.452054
[epoch11, step1580]: loss 0.810466
[epoch11, step1581]: loss 0.546921
[epoch11, step1582]: loss 0.544710
[epoch11, step1583]: loss 0.550434
[epoch11, step1584]: loss 0.512093
[epoch11, step1585]: loss 0.632171
[epoch11, step1586]: loss 0.606191
[epoch11, step1587]: loss 0.634100
[epoch11, step1588]: loss 0.828061
[epoch11, step1589]: loss 0.469919
[epoch11, step1590]: loss 0.603653
[epoch11, step1591]: loss 0.494537
[epoch11, step1592]: loss 0.382866
[epoch11, step1593]: loss 0.488945
[epoch11, step1594]: loss 0.485616
[epoch11, step1595]: loss 0.604022
[epoch11, step1596]: loss 0.331393
[epoch11, step1597]: loss 0.452575
[epoch11, step1598]: loss 0.752425
[epoch11, step1599]: loss 0.616033
[epoch11, step1600]: loss 0.644067
[epoch11, step1601]: loss 0.643844
[epoch11, step1602]: loss 0.489886
[epoch11, step1603]: loss 0.658910
[epoch11, step1604]: loss 0.502800
[epoch11, step1605]: loss 0.698035
[epoch11, step1606]: loss 0.586366
[epoch11, step1607]: loss 0.461482
[epoch11, step1608]: loss 0.644681
[epoch11, step1609]: loss 0.222255
[epoch11, step1610]: loss 0.635727
[epoch11, step1611]: loss 0.706324
[epoch11, step1612]: loss 0.694580
[epoch11, step1613]: loss 0.515879
[epoch11, step1614]: loss 0.552264
[epoch11, step1615]: loss 0.736748
[epoch11, step1616]: loss 0.519147
[epoch11, step1617]: loss 0.622951
[epoch11, step1618]: loss 0.764112
[epoch11, step1619]: loss 0.441277
[epoch11, step1620]: loss 0.378083
[epoch11, step1621]: loss 0.391341
[epoch11, step1622]: loss 0.467450
[epoch11, step1623]: loss 0.805046
[epoch11, step1624]: loss 0.778706
[epoch11, step1625]: loss 0.806512
[epoch11, step1626]: loss 0.809794
[epoch11, step1627]: loss 0.382695
[epoch11, step1628]: loss 0.318332
[epoch11, step1629]: loss 0.660771
[epoch11, step1630]: loss 0.792074
[epoch11, step1631]: loss 0.685972
[epoch11, step1632]: loss 0.827071
[epoch11, step1633]: loss 0.505409
[epoch11, step1634]: loss 0.573701
[epoch11, step1635]: loss 0.583811
[epoch11, step1636]: loss 0.411673
[epoch11, step1637]: loss 0.733178
[epoch11, step1638]: loss 0.530557
[epoch11, step1639]: loss 0.572882
[epoch11, step1640]: loss 0.709245
[epoch11, step1641]: loss 0.625962
[epoch11, step1642]: loss 0.352024
[epoch11, step1643]: loss 0.405411
[epoch11, step1644]: loss 0.646617
[epoch11, step1645]: loss 0.586820
[epoch11, step1646]: loss 0.606102
[epoch11, step1647]: loss 0.588735
[epoch11, step1648]: loss 0.464924
[epoch11, step1649]: loss 0.426410
[epoch11, step1650]: loss 0.702351
[epoch11, step1651]: loss 0.675521
[epoch11, step1652]: loss 0.741879
[epoch11, step1653]: loss 0.516357
[epoch11, step1654]: loss 0.644703
[epoch11, step1655]: loss 0.604621
[epoch11, step1656]: loss 0.471142
[epoch11, step1657]: loss 0.340662
[epoch11, step1658]: loss 0.603536
[epoch11, step1659]: loss 0.639524
[epoch11, step1660]: loss 0.629291
[epoch11, step1661]: loss 0.800050
[epoch11, step1662]: loss 0.573423
[epoch11, step1663]: loss 0.786598
[epoch11, step1664]: loss 0.358665
[epoch11, step1665]: loss 0.719085
[epoch11, step1666]: loss 0.586111
[epoch11, step1667]: loss 0.677502
[epoch11, step1668]: loss 0.771812
[epoch11, step1669]: loss 0.561338
[epoch11, step1670]: loss 0.682891
[epoch11, step1671]: loss 0.815010
[epoch11, step1672]: loss 0.723356
[epoch11, step1673]: loss 0.802063
[epoch11, step1674]: loss 0.448267
[epoch11, step1675]: loss 0.520019
[epoch11, step1676]: loss 0.272396
[epoch11, step1677]: loss 0.750403
[epoch11, step1678]: loss 0.538295
[epoch11, step1679]: loss 0.558843
[epoch11, step1680]: loss 0.790885
[epoch11, step1681]: loss 0.680671
[epoch11, step1682]: loss 0.641723
[epoch11, step1683]: loss 0.365124
[epoch11, step1684]: loss 0.596714
[epoch11, step1685]: loss 0.482306
[epoch11, step1686]: loss 0.477421
[epoch11, step1687]: loss 0.471511
[epoch11, step1688]: loss 0.593732
[epoch11, step1689]: loss 0.510792
[epoch11, step1690]: loss 0.536833
[epoch11, step1691]: loss 0.410071
[epoch11, step1692]: loss 0.430077
[epoch11, step1693]: loss 0.399821
[epoch11, step1694]: loss 0.668350
[epoch11, step1695]: loss 0.812325
[epoch11, step1696]: loss 0.580021
[epoch11, step1697]: loss 0.461146
[epoch11, step1698]: loss 0.686163
[epoch11, step1699]: loss 0.771137
[epoch11, step1700]: loss 0.544244
[epoch11, step1701]: loss 0.364916
[epoch11, step1702]: loss 0.789063
[epoch11, step1703]: loss 0.561425
[epoch11, step1704]: loss 0.469156
[epoch11, step1705]: loss 0.734156
[epoch11, step1706]: loss 0.683062
[epoch11, step1707]: loss 0.643682
[epoch11, step1708]: loss 0.496347
[epoch11, step1709]: loss 0.301898
[epoch11, step1710]: loss 0.583801
[epoch11, step1711]: loss 0.705393
[epoch11, step1712]: loss 0.449314
[epoch11, step1713]: loss 0.389590
[epoch11, step1714]: loss 0.505964
[epoch11, step1715]: loss 0.822733
[epoch11, step1716]: loss 0.512047
[epoch11, step1717]: loss 0.377311
[epoch11, step1718]: loss 0.520999
[epoch11, step1719]: loss 0.547078
[epoch11, step1720]: loss 0.675217
[epoch11, step1721]: loss 0.622690
[epoch11, step1722]: loss 0.865223
[epoch11, step1723]: loss 0.479396
[epoch11, step1724]: loss 0.502009
[epoch11, step1725]: loss 0.567336
[epoch11, step1726]: loss 0.669536
[epoch11, step1727]: loss 0.266719
[epoch11, step1728]: loss 0.359013
[epoch11, step1729]: loss 0.521789
[epoch11, step1730]: loss 0.705057
[epoch11, step1731]: loss 0.527776
[epoch11, step1732]: loss 0.631914
[epoch11, step1733]: loss 0.856233
[epoch11, step1734]: loss 0.210582
[epoch11, step1735]: loss 0.669125
[epoch11, step1736]: loss 0.497407
[epoch11, step1737]: loss 0.550285
[epoch11, step1738]: loss 0.738385
[epoch11, step1739]: loss 0.608507
[epoch11, step1740]: loss 0.525727
[epoch11, step1741]: loss 0.588387
[epoch11, step1742]: loss 0.700452
[epoch11, step1743]: loss 0.438466
[epoch11, step1744]: loss 0.585853
[epoch11, step1745]: loss 0.612618
[epoch11, step1746]: loss 0.340867
[epoch11, step1747]: loss 0.391928
[epoch11, step1748]: loss 0.369107
[epoch11, step1749]: loss 0.688040
[epoch11, step1750]: loss 0.633850
[epoch11, step1751]: loss 0.660792
[epoch11, step1752]: loss 0.655514
[epoch11, step1753]: loss 0.556548
[epoch11, step1754]: loss 0.593244
[epoch11, step1755]: loss 0.445030
[epoch11, step1756]: loss 0.812070
[epoch11, step1757]: loss 0.524288
[epoch11, step1758]: loss 0.739732
[epoch11, step1759]: loss 0.619959
[epoch11, step1760]: loss 0.384180
[epoch11, step1761]: loss 0.431732
[epoch11, step1762]: loss 0.442443
[epoch11, step1763]: loss 0.532497
[epoch11, step1764]: loss 0.587301
[epoch11, step1765]: loss 0.715191
[epoch11, step1766]: loss 0.361705
[epoch11, step1767]: loss 0.628549
[epoch11, step1768]: loss 0.528696
[epoch11, step1769]: loss 0.500951
[epoch11, step1770]: loss 0.588948
[epoch11, step1771]: loss 0.453395
[epoch11, step1772]: loss 0.411635
[epoch11, step1773]: loss 0.280053
[epoch11, step1774]: loss 0.400202
[epoch11, step1775]: loss 0.691026
[epoch11, step1776]: loss 0.741821
[epoch11, step1777]: loss 0.470840
[epoch11, step1778]: loss 0.603468
[epoch11, step1779]: loss 0.589525
[epoch11, step1780]: loss 0.506034
[epoch11, step1781]: loss 0.675142
[epoch11, step1782]: loss 0.527517
[epoch11, step1783]: loss 0.634024
[epoch11, step1784]: loss 0.835242
[epoch11, step1785]: loss 0.420297
[epoch11, step1786]: loss 0.567413
[epoch11, step1787]: loss 0.593383
[epoch11, step1788]: loss 0.597370
[epoch11, step1789]: loss 0.711986
[epoch11, step1790]: loss 0.372362
[epoch11, step1791]: loss 0.393687
[epoch11, step1792]: loss 0.554106
[epoch11, step1793]: loss 0.649025
[epoch11, step1794]: loss 0.546185
[epoch11, step1795]: loss 0.707463
[epoch11, step1796]: loss 0.475983
[epoch11, step1797]: loss 0.662901
[epoch11, step1798]: loss 0.699193
[epoch11, step1799]: loss 0.597770
[epoch11, step1800]: loss 0.529831
[epoch11, step1801]: loss 0.826628
[epoch11, step1802]: loss 0.483500
[epoch11, step1803]: loss 0.196751
[epoch11, step1804]: loss 0.504080
[epoch11, step1805]: loss 0.774436
[epoch11, step1806]: loss 0.601339
[epoch11, step1807]: loss 0.651849
[epoch11, step1808]: loss 0.696085
[epoch11, step1809]: loss 0.501289
[epoch11, step1810]: loss 0.598675
[epoch11, step1811]: loss 0.597856
[epoch11, step1812]: loss 0.480486
[epoch11, step1813]: loss 0.381286
[epoch11, step1814]: loss 0.521586
[epoch11, step1815]: loss 0.452436
[epoch11, step1816]: loss 0.510052
[epoch11, step1817]: loss 0.584939
[epoch11, step1818]: loss 0.634059
[epoch11, step1819]: loss 0.262666
[epoch11, step1820]: loss 0.594607
[epoch11, step1821]: loss 0.512738
[epoch11, step1822]: loss 0.464105
[epoch11, step1823]: loss 0.758551
[epoch11, step1824]: loss 0.475645
[epoch11, step1825]: loss 0.639799
[epoch11, step1826]: loss 0.595882
[epoch11, step1827]: loss 0.751826
[epoch11, step1828]: loss 0.469812
[epoch11, step1829]: loss 0.779843
[epoch11, step1830]: loss 0.588978
[epoch11, step1831]: loss 0.408654
[epoch11, step1832]: loss 0.730115
[epoch11, step1833]: loss 0.612946
[epoch11, step1834]: loss 0.535249
[epoch11, step1835]: loss 0.658292
[epoch11, step1836]: loss 0.531965
[epoch11, step1837]: loss 0.770525
[epoch11, step1838]: loss 0.488887
[epoch11, step1839]: loss 0.755493
[epoch11, step1840]: loss 0.636271
[epoch11, step1841]: loss 0.677054
[epoch11, step1842]: loss 0.636772
[epoch11, step1843]: loss 0.700067
[epoch11, step1844]: loss 0.751094
[epoch11, step1845]: loss 0.694727
[epoch11, step1846]: loss 0.414124
[epoch11, step1847]: loss 0.500024
[epoch11, step1848]: loss 0.655845
[epoch11, step1849]: loss 0.506570
[epoch11, step1850]: loss 0.481319
[epoch11, step1851]: loss 0.686708
[epoch11, step1852]: loss 0.562012
[epoch11, step1853]: loss 0.590581
[epoch11, step1854]: loss 0.473330
[epoch11, step1855]: loss 0.802140
[epoch11, step1856]: loss 0.393400
[epoch11, step1857]: loss 0.447493
[epoch11, step1858]: loss 0.681916
[epoch11, step1859]: loss 0.559237
[epoch11, step1860]: loss 0.636914
[epoch11, step1861]: loss 0.660565
[epoch11, step1862]: loss 0.686216
[epoch11, step1863]: loss 0.589514
[epoch11, step1864]: loss 0.691915
[epoch11, step1865]: loss 0.686220
[epoch11, step1866]: loss 0.532695
[epoch11, step1867]: loss 0.504769
[epoch11, step1868]: loss 0.647347
[epoch11, step1869]: loss 0.525811
[epoch11, step1870]: loss 0.492138
[epoch11, step1871]: loss 0.670969
[epoch11, step1872]: loss 0.635046
[epoch11, step1873]: loss 0.458003
[epoch11, step1874]: loss 0.763953
[epoch11, step1875]: loss 0.546810
[epoch11, step1876]: loss 0.601164
[epoch11, step1877]: loss 0.409978
[epoch11, step1878]: loss 0.410435
[epoch11, step1879]: loss 0.518657
[epoch11, step1880]: loss 0.652460
[epoch11, step1881]: loss 0.772717
[epoch11, step1882]: loss 0.572533
[epoch11, step1883]: loss 0.530834
[epoch11, step1884]: loss 0.510758
[epoch11, step1885]: loss 0.398996
[epoch11, step1886]: loss 0.669490
[epoch11, step1887]: loss 0.470169
[epoch11, step1888]: loss 0.569779
[epoch11, step1889]: loss 0.657759
[epoch11, step1890]: loss 0.499017
[epoch11, step1891]: loss 0.325278
[epoch11, step1892]: loss 0.708691
[epoch11, step1893]: loss 0.483978
[epoch11, step1894]: loss 0.528805
[epoch11, step1895]: loss 0.494384
[epoch11, step1896]: loss 0.482449
[epoch11, step1897]: loss 0.518387
[epoch11, step1898]: loss 0.545559
[epoch11, step1899]: loss 0.592632
[epoch11, step1900]: loss 0.704069
[epoch11, step1901]: loss 0.682429
[epoch11, step1902]: loss 0.519237
[epoch11, step1903]: loss 0.620241
[epoch11, step1904]: loss 0.449245
[epoch11, step1905]: loss 0.316715
[epoch11, step1906]: loss 0.784608
[epoch11, step1907]: loss 0.846606
[epoch11, step1908]: loss 0.679135
[epoch11, step1909]: loss 0.740608
[epoch11, step1910]: loss 0.687199
[epoch11, step1911]: loss 0.474539
[epoch11, step1912]: loss 0.771011
[epoch11, step1913]: loss 0.566342
[epoch11, step1914]: loss 0.801582
[epoch11, step1915]: loss 0.482968
[epoch11, step1916]: loss 0.698221
[epoch11, step1917]: loss 0.769335
[epoch11, step1918]: loss 0.459804
[epoch11, step1919]: loss 0.618790
[epoch11, step1920]: loss 0.682030
[epoch11, step1921]: loss 0.702529
[epoch11, step1922]: loss 0.664867
[epoch11, step1923]: loss 0.506170
[epoch11, step1924]: loss 0.765197
[epoch11, step1925]: loss 0.420555
[epoch11, step1926]: loss 0.712699
[epoch11, step1927]: loss 0.726638
[epoch11, step1928]: loss 0.683644
[epoch11, step1929]: loss 0.342782
[epoch11, step1930]: loss 0.773911
[epoch11, step1931]: loss 0.709580
[epoch11, step1932]: loss 0.913180
[epoch11, step1933]: loss 0.514380
[epoch11, step1934]: loss 0.646617
[epoch11, step1935]: loss 0.548726
[epoch11, step1936]: loss 0.675321
[epoch11, step1937]: loss 0.604959
[epoch11, step1938]: loss 0.593839
[epoch11, step1939]: loss 0.693647
[epoch11, step1940]: loss 0.741106
[epoch11, step1941]: loss 0.490997
[epoch11, step1942]: loss 0.531473
[epoch11, step1943]: loss 0.725239
[epoch11, step1944]: loss 0.660036
[epoch11, step1945]: loss 0.597206
[epoch11, step1946]: loss 0.489032
[epoch11, step1947]: loss 0.704011
[epoch11, step1948]: loss 0.369772
[epoch11, step1949]: loss 0.688340
[epoch11, step1950]: loss 0.679697
[epoch11, step1951]: loss 0.664379
[epoch11, step1952]: loss 0.541223
[epoch11, step1953]: loss 0.486170
[epoch11, step1954]: loss 0.516107
[epoch11, step1955]: loss 0.553975
[epoch11, step1956]: loss 0.666726
[epoch11, step1957]: loss 0.382406
[epoch11, step1958]: loss 0.519552
[epoch11, step1959]: loss 0.777402
[epoch11, step1960]: loss 0.759679
[epoch11, step1961]: loss 0.558103
[epoch11, step1962]: loss 0.427659
[epoch11, step1963]: loss 0.762716
[epoch11, step1964]: loss 0.543723
[epoch11, step1965]: loss 0.567760
[epoch11, step1966]: loss 0.575193
[epoch11, step1967]: loss 0.503731
[epoch11, step1968]: loss 0.703339
[epoch11, step1969]: loss 0.695274
[epoch11, step1970]: loss 0.730117
[epoch11, step1971]: loss 0.530926
[epoch11, step1972]: loss 0.574944
[epoch11, step1973]: loss 0.576642
[epoch11, step1974]: loss 0.544472
[epoch11, step1975]: loss 0.594114
[epoch11, step1976]: loss 0.804012
[epoch11, step1977]: loss 0.521318
[epoch11, step1978]: loss 0.574417
[epoch11, step1979]: loss 0.290306
[epoch11, step1980]: loss 0.513098
[epoch11, step1981]: loss 0.258956
[epoch11, step1982]: loss 0.667461
[epoch11, step1983]: loss 0.341272
[epoch11, step1984]: loss 0.611912
[epoch11, step1985]: loss 0.428423
[epoch11, step1986]: loss 0.728572
[epoch11, step1987]: loss 0.774361
[epoch11, step1988]: loss 0.570658
[epoch11, step1989]: loss 0.255798
[epoch11, step1990]: loss 0.380251
[epoch11, step1991]: loss 0.651247
[epoch11, step1992]: loss 0.334621
[epoch11, step1993]: loss 0.450866
[epoch11, step1994]: loss 0.624288
[epoch11, step1995]: loss 0.245964
[epoch11, step1996]: loss 0.422611
[epoch11, step1997]: loss 0.595501
[epoch11, step1998]: loss 0.620182
[epoch11, step1999]: loss 0.794080
[epoch11, step2000]: loss 0.603411
[epoch11, step2001]: loss 0.670796
[epoch11, step2002]: loss 0.614512
[epoch11, step2003]: loss 0.468192
[epoch11, step2004]: loss 0.409602
[epoch11, step2005]: loss 0.676740
[epoch11, step2006]: loss 0.501615
[epoch11, step2007]: loss 0.365054
[epoch11, step2008]: loss 0.506658
[epoch11, step2009]: loss 0.663388
[epoch11, step2010]: loss 0.533790
[epoch11, step2011]: loss 0.346933
[epoch11, step2012]: loss 0.386971
[epoch11, step2013]: loss 0.679942
[epoch11, step2014]: loss 0.371294
[epoch11, step2015]: loss 0.774827
[epoch11, step2016]: loss 0.698677
[epoch11, step2017]: loss 0.641303
[epoch11, step2018]: loss 0.550879
[epoch11, step2019]: loss 0.594725
[epoch11, step2020]: loss 0.537147
[epoch11, step2021]: loss 0.285525
[epoch11, step2022]: loss 0.695591
[epoch11, step2023]: loss 0.506589
[epoch11, step2024]: loss 0.736049
[epoch11, step2025]: loss 0.258404
[epoch11, step2026]: loss 0.187528
[epoch11, step2027]: loss 0.504825
[epoch11, step2028]: loss 0.772140
[epoch11, step2029]: loss 0.776177
[epoch11, step2030]: loss 0.572300
[epoch11, step2031]: loss 0.271613
[epoch11, step2032]: loss 0.823168
[epoch11, step2033]: loss 0.545565
[epoch11, step2034]: loss 0.809476
[epoch11, step2035]: loss 0.443287
[epoch11, step2036]: loss 0.516595
[epoch11, step2037]: loss 0.721661
[epoch11, step2038]: loss 0.576052
[epoch11, step2039]: loss 0.557845
[epoch11, step2040]: loss 0.459860
[epoch11, step2041]: loss 0.533034
[epoch11, step2042]: loss 0.460537
[epoch11, step2043]: loss 0.378965
[epoch11, step2044]: loss 0.635877
[epoch11, step2045]: loss 0.625190
[epoch11, step2046]: loss 0.428326
[epoch11, step2047]: loss 0.527701
[epoch11, step2048]: loss 0.747246
[epoch11, step2049]: loss 0.583392
[epoch11, step2050]: loss 0.679388
[epoch11, step2051]: loss 0.585799
[epoch11, step2052]: loss 0.746610
[epoch11, step2053]: loss 0.812316
[epoch11, step2054]: loss 0.722851
[epoch11, step2055]: loss 0.700177
[epoch11, step2056]: loss 0.695018
[epoch11, step2057]: loss 0.562485
[epoch11, step2058]: loss 0.862276
[epoch11, step2059]: loss 0.591564
[epoch11, step2060]: loss 0.311927
[epoch11, step2061]: loss 0.594430
[epoch11, step2062]: loss 0.919211
[epoch11, step2063]: loss 0.458876
[epoch11, step2064]: loss 0.597418
[epoch11, step2065]: loss 0.659082
[epoch11, step2066]: loss 0.820536
[epoch11, step2067]: loss 0.620384
[epoch11, step2068]: loss 0.751274
[epoch11, step2069]: loss 0.714220
[epoch11, step2070]: loss 0.702846
[epoch11, step2071]: loss 0.614869
[epoch11, step2072]: loss 0.535958
[epoch11, step2073]: loss 0.585370
[epoch11, step2074]: loss 0.829287
[epoch11, step2075]: loss 0.491231
[epoch11, step2076]: loss 0.409163
[epoch11, step2077]: loss 0.504950
[epoch11, step2078]: loss 0.786807
[epoch11, step2079]: loss 0.569775
[epoch11, step2080]: loss 0.597581
[epoch11, step2081]: loss 0.660907
[epoch11, step2082]: loss 0.586196
[epoch11, step2083]: loss 0.468042
[epoch11, step2084]: loss 0.648876
[epoch11, step2085]: loss 0.567649
[epoch11, step2086]: loss 0.589638
[epoch11, step2087]: loss 0.629043
[epoch11, step2088]: loss 0.593118
[epoch11, step2089]: loss 0.574211
[epoch11, step2090]: loss 0.485758
[epoch11, step2091]: loss 0.645063
[epoch11, step2092]: loss 0.743021
[epoch11, step2093]: loss 0.681669
[epoch11, step2094]: loss 0.398059
[epoch11, step2095]: loss 0.541502
[epoch11, step2096]: loss 0.590495
[epoch11, step2097]: loss 0.428236
[epoch11, step2098]: loss 0.586759
[epoch11, step2099]: loss 0.679613
[epoch11, step2100]: loss 0.429869
[epoch11, step2101]: loss 0.685517
[epoch11, step2102]: loss 0.732599
[epoch11, step2103]: loss 0.675990
[epoch11, step2104]: loss 0.332350
[epoch11, step2105]: loss 0.622407
[epoch11, step2106]: loss 0.478302
[epoch11, step2107]: loss 0.526387
[epoch11, step2108]: loss 0.550951
[epoch11, step2109]: loss 0.590437
[epoch11, step2110]: loss 0.451693
[epoch11, step2111]: loss 0.664587
[epoch11, step2112]: loss 0.317297
[epoch11, step2113]: loss 0.564637
[epoch11, step2114]: loss 0.565590
[epoch11, step2115]: loss 0.402196
[epoch11, step2116]: loss 0.738562
[epoch11, step2117]: loss 0.529572
[epoch11, step2118]: loss 0.604202
[epoch11, step2119]: loss 0.802161
[epoch11, step2120]: loss 0.494419
[epoch11, step2121]: loss 0.394914
[epoch11, step2122]: loss 0.394455
[epoch11, step2123]: loss 0.580420
[epoch11, step2124]: loss 0.510654
[epoch11, step2125]: loss 0.638875
[epoch11, step2126]: loss 0.643471
[epoch11, step2127]: loss 0.372573
[epoch11, step2128]: loss 0.690013
[epoch11, step2129]: loss 0.702332
[epoch11, step2130]: loss 0.583416
[epoch11, step2131]: loss 0.731438
[epoch11, step2132]: loss 0.462898
[epoch11, step2133]: loss 0.592889
[epoch11, step2134]: loss 0.111977
[epoch11, step2135]: loss 0.501106
[epoch11, step2136]: loss 0.287053
[epoch11, step2137]: loss 0.479067
[epoch11, step2138]: loss 0.388394
[epoch11, step2139]: loss 0.516328
[epoch11, step2140]: loss 0.733059
[epoch11, step2141]: loss 0.482241
[epoch11, step2142]: loss 0.698163
[epoch11, step2143]: loss 0.727227
[epoch11, step2144]: loss 0.611729
[epoch11, step2145]: loss 0.778917
[epoch11, step2146]: loss 0.771277
[epoch11, step2147]: loss 0.597458
[epoch11, step2148]: loss 0.517971
[epoch11, step2149]: loss 0.581854
[epoch11, step2150]: loss 0.683248
[epoch11, step2151]: loss 0.736339
[epoch11, step2152]: loss 0.455096
[epoch11, step2153]: loss 0.635606
[epoch11, step2154]: loss 0.666070
[epoch11, step2155]: loss 0.674169
[epoch11, step2156]: loss 0.572524
[epoch11, step2157]: loss 0.508790
[epoch11, step2158]: loss 0.604133
[epoch11, step2159]: loss 0.303837
[epoch11, step2160]: loss 0.597021
[epoch11, step2161]: loss 0.623171
[epoch11, step2162]: loss 0.608478
[epoch11, step2163]: loss 0.731874
[epoch11, step2164]: loss 0.663356
[epoch11, step2165]: loss 0.676136
[epoch11, step2166]: loss 0.759935
[epoch11, step2167]: loss 0.784192
[epoch11, step2168]: loss 0.444024
[epoch11, step2169]: loss 0.169737
[epoch11, step2170]: loss 0.763525
[epoch11, step2171]: loss 0.581364
[epoch11, step2172]: loss 0.484729
[epoch11, step2173]: loss 0.602436
[epoch11, step2174]: loss 0.782421
[epoch11, step2175]: loss 0.540904
[epoch11, step2176]: loss 0.604131
[epoch11, step2177]: loss 0.778559
[epoch11, step2178]: loss 0.693638
[epoch11, step2179]: loss 0.581930
[epoch11, step2180]: loss 0.656952
[epoch11, step2181]: loss 0.882217
[epoch11, step2182]: loss 0.675720
[epoch11, step2183]: loss 0.628613
[epoch11, step2184]: loss 0.658751
[epoch11, step2185]: loss 0.558097
[epoch11, step2186]: loss 0.600637
[epoch11, step2187]: loss 0.682080
[epoch11, step2188]: loss 0.657167
[epoch11, step2189]: loss 0.627851
[epoch11, step2190]: loss 0.376348
[epoch11, step2191]: loss 0.421314
[epoch11, step2192]: loss 0.909846
[epoch11, step2193]: loss 0.569049
[epoch11, step2194]: loss 0.637591
[epoch11, step2195]: loss 0.488219
[epoch11, step2196]: loss 0.462992
[epoch11, step2197]: loss 0.503996
[epoch11, step2198]: loss 0.653085
[epoch11, step2199]: loss 0.625249
[epoch11, step2200]: loss 0.755970
[epoch11, step2201]: loss 0.790412
[epoch11, step2202]: loss 0.592664
[epoch11, step2203]: loss 0.610624
[epoch11, step2204]: loss 0.746736
[epoch11, step2205]: loss 0.821302
[epoch11, step2206]: loss 0.612689
[epoch11, step2207]: loss 0.569603
[epoch11, step2208]: loss 0.473674
[epoch11, step2209]: loss 0.145008
[epoch11, step2210]: loss 0.470612
[epoch11, step2211]: loss 0.559141
[epoch11, step2212]: loss 0.487045
[epoch11, step2213]: loss 0.195583
[epoch11, step2214]: loss 0.797314
[epoch11, step2215]: loss 0.460508
[epoch11, step2216]: loss 0.553642
[epoch11, step2217]: loss 0.518836
[epoch11, step2218]: loss 0.610417
[epoch11, step2219]: loss 0.449735
[epoch11, step2220]: loss 0.416679
[epoch11, step2221]: loss 0.641893
[epoch11, step2222]: loss 0.438799
[epoch11, step2223]: loss 0.637931
[epoch11, step2224]: loss 0.376388
[epoch11, step2225]: loss 0.535886
[epoch11, step2226]: loss 0.577165
[epoch11, step2227]: loss 0.576264
[epoch11, step2228]: loss 0.707578
[epoch11, step2229]: loss 0.707817
[epoch11, step2230]: loss 0.696256
[epoch11, step2231]: loss 0.641391
[epoch11, step2232]: loss 0.537457
[epoch11, step2233]: loss 0.764891
[epoch11, step2234]: loss 0.625923
[epoch11, step2235]: loss 0.457721
[epoch11, step2236]: loss 0.423203
[epoch11, step2237]: loss 0.579924
[epoch11, step2238]: loss 0.664220
[epoch11, step2239]: loss 0.775963
[epoch11, step2240]: loss 0.313124
[epoch11, step2241]: loss 0.350262
[epoch11, step2242]: loss 0.402600
[epoch11, step2243]: loss 0.701822
[epoch11, step2244]: loss 0.469512
[epoch11, step2245]: loss 0.466600
[epoch11, step2246]: loss 0.459173
[epoch11, step2247]: loss 0.715094
[epoch11, step2248]: loss 0.597735
[epoch11, step2249]: loss 0.654679
[epoch11, step2250]: loss 0.398708
[epoch11, step2251]: loss 0.304483
[epoch11, step2252]: loss 0.462939
[epoch11, step2253]: loss 0.597488
[epoch11, step2254]: loss 0.198603
[epoch11, step2255]: loss 0.498347
[epoch11, step2256]: loss 0.443249
[epoch11, step2257]: loss 0.558549
[epoch11, step2258]: loss 0.682539
[epoch11, step2259]: loss 0.467204
[epoch11, step2260]: loss 0.594505
[epoch11, step2261]: loss 0.877832
[epoch11, step2262]: loss 0.641743
[epoch11, step2263]: loss 0.505860
[epoch11, step2264]: loss 0.471414
[epoch11, step2265]: loss 0.845050
[epoch11, step2266]: loss 0.638104
[epoch11, step2267]: loss 0.630652
[epoch11, step2268]: loss 0.342334
[epoch11, step2269]: loss 0.423725
[epoch11, step2270]: loss 0.664123
[epoch11, step2271]: loss 0.339026
[epoch11, step2272]: loss 0.515713
[epoch11, step2273]: loss 0.494714
[epoch11, step2274]: loss 0.221138
[epoch11, step2275]: loss 0.754477
[epoch11, step2276]: loss 0.503841
[epoch11, step2277]: loss 0.789451
[epoch11, step2278]: loss 0.705101
[epoch11, step2279]: loss 0.835534
[epoch11, step2280]: loss 0.413612
[epoch11, step2281]: loss 0.642845
[epoch11, step2282]: loss 0.731411
[epoch11, step2283]: loss 0.755122
[epoch11, step2284]: loss 0.430227
[epoch11, step2285]: loss 0.610952
[epoch11, step2286]: loss 0.372792
[epoch11, step2287]: loss 0.304137
[epoch11, step2288]: loss 0.588811
[epoch11, step2289]: loss 0.455438
[epoch11, step2290]: loss 0.734326
[epoch11, step2291]: loss 0.585029
[epoch11, step2292]: loss 0.542594
[epoch11, step2293]: loss 0.740322
[epoch11, step2294]: loss 0.699394
[epoch11, step2295]: loss 0.551488
[epoch11, step2296]: loss 0.541479
[epoch11, step2297]: loss 0.581743
[epoch11, step2298]: loss 0.587767
[epoch11, step2299]: loss 0.457168
[epoch11, step2300]: loss 0.447674
[epoch11, step2301]: loss 0.403694
[epoch11, step2302]: loss 0.486974
[epoch11, step2303]: loss 0.524865
[epoch11, step2304]: loss 0.371279
[epoch11, step2305]: loss 0.692676
[epoch11, step2306]: loss 0.714607
[epoch11, step2307]: loss 0.421627
[epoch11, step2308]: loss 0.737414
[epoch11, step2309]: loss 0.816099
[epoch11, step2310]: loss 0.269801
[epoch11, step2311]: loss 0.512146
[epoch11, step2312]: loss 0.680218
[epoch11, step2313]: loss 0.428354
[epoch11, step2314]: loss 0.846848
[epoch11, step2315]: loss 0.421517
[epoch11, step2316]: loss 0.715176
[epoch11, step2317]: loss 0.582969
[epoch11, step2318]: loss 0.726976
[epoch11, step2319]: loss 0.510514
[epoch11, step2320]: loss 0.489186
[epoch11, step2321]: loss 0.548768
[epoch11, step2322]: loss 0.531328
[epoch11, step2323]: loss 0.610913
[epoch11, step2324]: loss 0.555410
[epoch11, step2325]: loss 0.350922
[epoch11, step2326]: loss 0.370188
[epoch11, step2327]: loss 0.109695
[epoch11, step2328]: loss 0.451436
[epoch11, step2329]: loss 0.465630
[epoch11, step2330]: loss 0.285676
[epoch11, step2331]: loss 0.354851
[epoch11, step2332]: loss 0.719516
[epoch11, step2333]: loss 0.673764
[epoch11, step2334]: loss 0.679081
[epoch11, step2335]: loss 0.554527
[epoch11, step2336]: loss 0.552327
[epoch11, step2337]: loss 0.507796
[epoch11, step2338]: loss 0.665926
[epoch11, step2339]: loss 0.508352
[epoch11, step2340]: loss 0.600946
[epoch11, step2341]: loss 0.722257
[epoch11, step2342]: loss 0.695013
[epoch11, step2343]: loss 0.662892
[epoch11, step2344]: loss 0.829263
[epoch11, step2345]: loss 0.744419
[epoch11, step2346]: loss 0.454847
[epoch11, step2347]: loss 0.234583
[epoch11, step2348]: loss 0.319416
[epoch11, step2349]: loss 0.364559
[epoch11, step2350]: loss 0.610720
[epoch11, step2351]: loss 0.513987
[epoch11, step2352]: loss 0.685391
[epoch11, step2353]: loss 0.470821
[epoch11, step2354]: loss 0.502865
[epoch11, step2355]: loss 0.655956
[epoch11, step2356]: loss 0.400229
[epoch11, step2357]: loss 0.606067
[epoch11, step2358]: loss 0.722706
[epoch11, step2359]: loss 0.406744
[epoch11, step2360]: loss 0.735150
[epoch11, step2361]: loss 0.615853
[epoch11, step2362]: loss 0.463866
[epoch11, step2363]: loss 0.402481
[epoch11, step2364]: loss 0.547841
[epoch11, step2365]: loss 0.699680
[epoch11, step2366]: loss 0.425788
[epoch11, step2367]: loss 0.514054
[epoch11, step2368]: loss 0.517824
[epoch11, step2369]: loss 0.620269
[epoch11, step2370]: loss 0.889932
[epoch11, step2371]: loss 0.723321
[epoch11, step2372]: loss 0.338258
[epoch11, step2373]: loss 0.678788
[epoch11, step2374]: loss 0.417059
[epoch11, step2375]: loss 0.647363
[epoch11, step2376]: loss 0.596577
[epoch11, step2377]: loss 0.681347
[epoch11, step2378]: loss 0.682333
[epoch11, step2379]: loss 0.534907
[epoch11, step2380]: loss 0.669312
[epoch11, step2381]: loss 0.479756
[epoch11, step2382]: loss 0.820523
[epoch11, step2383]: loss 0.536016
[epoch11, step2384]: loss 0.573223
[epoch11, step2385]: loss 0.521778
[epoch11, step2386]: loss 0.562803
[epoch11, step2387]: loss 0.472282
[epoch11, step2388]: loss 0.835481
[epoch11, step2389]: loss 0.321987
[epoch11, step2390]: loss 0.605337
[epoch11, step2391]: loss 0.377428
[epoch11, step2392]: loss 0.283544
[epoch11, step2393]: loss 0.549830
[epoch11, step2394]: loss 0.636197
[epoch11, step2395]: loss 0.614138
[epoch11, step2396]: loss 0.560488
[epoch11, step2397]: loss 0.513312
[epoch11, step2398]: loss 0.745071
[epoch11, step2399]: loss 0.699831
[epoch11, step2400]: loss 0.324308
[epoch11, step2401]: loss 0.647611
[epoch11, step2402]: loss 0.521352
[epoch11, step2403]: loss 0.547015
[epoch11, step2404]: loss 0.584743
[epoch11, step2405]: loss 0.661933
[epoch11, step2406]: loss 0.632463
[epoch11, step2407]: loss 0.485831
[epoch11, step2408]: loss 0.568353
[epoch11, step2409]: loss 0.665936
[epoch11, step2410]: loss 0.513274
[epoch11, step2411]: loss 0.770369
[epoch11, step2412]: loss 0.614843
[epoch11, step2413]: loss 0.550757
[epoch11, step2414]: loss 0.591224
[epoch11, step2415]: loss 0.645688
[epoch11, step2416]: loss 0.442392
[epoch11, step2417]: loss 0.723502
[epoch11, step2418]: loss 0.476418
[epoch11, step2419]: loss 0.572352
[epoch11, step2420]: loss 0.629968
[epoch11, step2421]: loss 0.472132
[epoch11, step2422]: loss 0.575023
[epoch11, step2423]: loss 0.667568
[epoch11, step2424]: loss 0.366258
[epoch11, step2425]: loss 0.637205
[epoch11, step2426]: loss 0.389352
[epoch11, step2427]: loss 0.495783
[epoch11, step2428]: loss 0.581857
[epoch11, step2429]: loss 0.674106
[epoch11, step2430]: loss 0.537972
[epoch11, step2431]: loss 0.411359
[epoch11, step2432]: loss 0.482164
[epoch11, step2433]: loss 0.557640
[epoch11, step2434]: loss 0.352731
[epoch11, step2435]: loss 0.679645
[epoch11, step2436]: loss 0.641425
[epoch11, step2437]: loss 0.723910
[epoch11, step2438]: loss 0.449537
[epoch11, step2439]: loss 0.314463
[epoch11, step2440]: loss 0.573018
[epoch11, step2441]: loss 0.599330
[epoch11, step2442]: loss 0.586102
[epoch11, step2443]: loss 0.661170
[epoch11, step2444]: loss 0.634966
[epoch11, step2445]: loss 0.523037
[epoch11, step2446]: loss 0.719411
[epoch11, step2447]: loss 0.723675
[epoch11, step2448]: loss 0.666550
[epoch11, step2449]: loss 0.671262
[epoch11, step2450]: loss 0.862674
[epoch11, step2451]: loss 0.489101
[epoch11, step2452]: loss 0.402175
[epoch11, step2453]: loss 0.336512
[epoch11, step2454]: loss 0.558276
[epoch11, step2455]: loss 0.336723
[epoch11, step2456]: loss 0.565754
[epoch11, step2457]: loss 0.484769
[epoch11, step2458]: loss 0.552772
[epoch11, step2459]: loss 0.673293
[epoch11, step2460]: loss 0.625180
[epoch11, step2461]: loss 0.590992
[epoch11, step2462]: loss 0.590586
[epoch11, step2463]: loss 0.559037
[epoch11, step2464]: loss 0.352276
[epoch11, step2465]: loss 0.486707
[epoch11, step2466]: loss 0.434139
[epoch11, step2467]: loss 0.546156
[epoch11, step2468]: loss 0.426646
[epoch11, step2469]: loss 0.562923
[epoch11, step2470]: loss 0.895814
[epoch11, step2471]: loss 0.526900
[epoch11, step2472]: loss 0.505568
[epoch11, step2473]: loss 0.683158
[epoch11, step2474]: loss 0.633541
[epoch11, step2475]: loss 0.465914
[epoch11, step2476]: loss 0.717246
[epoch11, step2477]: loss 0.485283
[epoch11, step2478]: loss 0.775420
[epoch11, step2479]: loss 0.679711
[epoch11, step2480]: loss 0.671795
[epoch11, step2481]: loss 0.712709
[epoch11, step2482]: loss 0.673764
[epoch11, step2483]: loss 0.644962
[epoch11, step2484]: loss 0.358135
[epoch11, step2485]: loss 0.850211
[epoch11, step2486]: loss 0.737190
[epoch11, step2487]: loss 0.647342
[epoch11, step2488]: loss 0.650824
[epoch11, step2489]: loss 0.606986
[epoch11, step2490]: loss 0.543781
[epoch11, step2491]: loss 0.660443
[epoch11, step2492]: loss 0.363735
[epoch11, step2493]: loss 0.486934
[epoch11, step2494]: loss 0.685764
[epoch11, step2495]: loss 0.791187
[epoch11, step2496]: loss 0.315132
[epoch11, step2497]: loss 0.463701
[epoch11, step2498]: loss 0.788113
[epoch11, step2499]: loss 0.463862
[epoch11, step2500]: loss 0.683758
[epoch11, step2501]: loss 0.603042
[epoch11, step2502]: loss 0.581366
[epoch11, step2503]: loss 0.780212
[epoch11, step2504]: loss 0.704772
[epoch11, step2505]: loss 0.645572
[epoch11, step2506]: loss 0.704618
[epoch11, step2507]: loss 0.728025
[epoch11, step2508]: loss 0.586950
[epoch11, step2509]: loss 0.378893
[epoch11, step2510]: loss 0.486698
[epoch11, step2511]: loss 0.642362
[epoch11, step2512]: loss 0.570917
[epoch11, step2513]: loss 0.296751
[epoch11, step2514]: loss 0.574882
[epoch11, step2515]: loss 0.400044
[epoch11, step2516]: loss 0.726353
[epoch11, step2517]: loss 0.268916
[epoch11, step2518]: loss 0.487493
[epoch11, step2519]: loss 0.801432
[epoch11, step2520]: loss 0.510972
[epoch11, step2521]: loss 0.394836
[epoch11, step2522]: loss 0.670641
[epoch11, step2523]: loss 0.726891
[epoch11, step2524]: loss 0.788577
[epoch11, step2525]: loss 0.363986
[epoch11, step2526]: loss 0.662323
[epoch11, step2527]: loss 0.703849
[epoch11, step2528]: loss 0.636467
[epoch11, step2529]: loss 0.635791
[epoch11, step2530]: loss 0.195078
[epoch11, step2531]: loss 0.731654
[epoch11, step2532]: loss 0.521588
[epoch11, step2533]: loss 0.547116
[epoch11, step2534]: loss 0.565884
[epoch11, step2535]: loss 0.713457
[epoch11, step2536]: loss 0.382262
[epoch11, step2537]: loss 0.534132
[epoch11, step2538]: loss 0.583506
[epoch11, step2539]: loss 0.385031
[epoch11, step2540]: loss 0.484276
[epoch11, step2541]: loss 0.277907
[epoch11, step2542]: loss 0.455581
[epoch11, step2543]: loss 0.530781
[epoch11, step2544]: loss 0.192266
[epoch11, step2545]: loss 0.620979
[epoch11, step2546]: loss 0.752686
[epoch11, step2547]: loss 0.444266
[epoch11, step2548]: loss 0.695396
[epoch11, step2549]: loss 0.367146
[epoch11, step2550]: loss 0.763849
[epoch11, step2551]: loss 0.542099
[epoch11, step2552]: loss 0.897823
[epoch11, step2553]: loss 0.332686
[epoch11, step2554]: loss 0.524052
[epoch11, step2555]: loss 0.451479
[epoch11, step2556]: loss 0.714566
[epoch11, step2557]: loss 0.618445
[epoch11, step2558]: loss 0.628982
[epoch11, step2559]: loss 0.772592
[epoch11, step2560]: loss 0.602046
[epoch11, step2561]: loss 0.760128
[epoch11, step2562]: loss 0.320424
[epoch11, step2563]: loss 0.652054
[epoch11, step2564]: loss 0.533986
[epoch11, step2565]: loss 0.374887
[epoch11, step2566]: loss 0.414713
[epoch11, step2567]: loss 0.798276
[epoch11, step2568]: loss 0.463771
[epoch11, step2569]: loss 0.598431
[epoch11, step2570]: loss 0.449486
[epoch11, step2571]: loss 0.851076
[epoch11, step2572]: loss 0.587959
[epoch11, step2573]: loss 0.536573
[epoch11, step2574]: loss 0.685213
[epoch11, step2575]: loss 0.700262
[epoch11, step2576]: loss 0.613914
[epoch11, step2577]: loss 0.644865
[epoch11, step2578]: loss 0.419153
[epoch11, step2579]: loss 0.519498
[epoch11, step2580]: loss 0.583260
[epoch11, step2581]: loss 0.618547
[epoch11, step2582]: loss 0.596331
[epoch11, step2583]: loss 0.580795
[epoch11, step2584]: loss 0.499970
[epoch11, step2585]: loss 0.754730
[epoch11, step2586]: loss 0.491990
[epoch11, step2587]: loss 0.616711
[epoch11, step2588]: loss 0.457349
[epoch11, step2589]: loss 0.335719
[epoch11, step2590]: loss 0.516471
[epoch11, step2591]: loss 0.652443
[epoch11, step2592]: loss 0.482650
[epoch11, step2593]: loss 0.625604
[epoch11, step2594]: loss 0.755514
[epoch11, step2595]: loss 0.422105
[epoch11, step2596]: loss 0.579415
[epoch11, step2597]: loss 0.418527
[epoch11, step2598]: loss 0.547239
[epoch11, step2599]: loss 0.546565
[epoch11, step2600]: loss 0.712730
[epoch11, step2601]: loss 0.417532
[epoch11, step2602]: loss 0.611134
[epoch11, step2603]: loss 0.541068
[epoch11, step2604]: loss 0.485818
[epoch11, step2605]: loss 0.685813
[epoch11, step2606]: loss 0.445679
[epoch11, step2607]: loss 0.305752
[epoch11, step2608]: loss 0.319131
[epoch11, step2609]: loss 0.764606
[epoch11, step2610]: loss 0.732082
[epoch11, step2611]: loss 0.847987
[epoch11, step2612]: loss 0.708346
[epoch11, step2613]: loss 0.541416
[epoch11, step2614]: loss 0.401422
[epoch11, step2615]: loss 0.671713
[epoch11, step2616]: loss 0.496796
[epoch11, step2617]: loss 0.411084
[epoch11, step2618]: loss 0.478865
[epoch11, step2619]: loss 0.231159
[epoch11, step2620]: loss 0.243324
[epoch11, step2621]: loss 0.629768
[epoch11, step2622]: loss 0.706989
[epoch11, step2623]: loss 0.790811
[epoch11, step2624]: loss 0.714887
[epoch11, step2625]: loss 0.185462
[epoch11, step2626]: loss 0.592103
[epoch11, step2627]: loss 0.812677
[epoch11, step2628]: loss 0.680663
[epoch11, step2629]: loss 0.525133
[epoch11, step2630]: loss 0.495194
[epoch11, step2631]: loss 0.531179
[epoch11, step2632]: loss 0.458474
[epoch11, step2633]: loss 0.705603
[epoch11, step2634]: loss 0.471655
[epoch11, step2635]: loss 0.676041
[epoch11, step2636]: loss 0.654645
[epoch11, step2637]: loss 0.623640
[epoch11, step2638]: loss 0.524037
[epoch11, step2639]: loss 0.489597
[epoch11, step2640]: loss 0.520952
[epoch11, step2641]: loss 0.501118
[epoch11, step2642]: loss 0.412384
[epoch11, step2643]: loss 0.559954
[epoch11, step2644]: loss 0.585672
[epoch11, step2645]: loss 0.620750
[epoch11, step2646]: loss 0.666223
[epoch11, step2647]: loss 0.351892
[epoch11, step2648]: loss 0.612627
[epoch11, step2649]: loss 0.329920
[epoch11, step2650]: loss 0.592766
[epoch11, step2651]: loss 0.402896
[epoch11, step2652]: loss 0.578817
[epoch11, step2653]: loss 0.655735
[epoch11, step2654]: loss 0.508069
[epoch11, step2655]: loss 0.369643
[epoch11, step2656]: loss 0.757886
[epoch11, step2657]: loss 0.443317
[epoch11, step2658]: loss 0.691044
[epoch11, step2659]: loss 0.407767
[epoch11, step2660]: loss 0.572861
[epoch11, step2661]: loss 0.762489
[epoch11, step2662]: loss 0.645972
[epoch11, step2663]: loss 0.684014
[epoch11, step2664]: loss 0.714668
[epoch11, step2665]: loss 0.761001
[epoch11, step2666]: loss 0.520020
[epoch11, step2667]: loss 0.574206
[epoch11, step2668]: loss 0.405638
[epoch11, step2669]: loss 0.317244
[epoch11, step2670]: loss 0.703397
[epoch11, step2671]: loss 0.613751
[epoch11, step2672]: loss 0.422431
[epoch11, step2673]: loss 0.402586
[epoch11, step2674]: loss 0.587796
[epoch11, step2675]: loss 0.382811
[epoch11, step2676]: loss 0.669498
[epoch11, step2677]: loss 0.407832
[epoch11, step2678]: loss 0.648299
[epoch11, step2679]: loss 0.626704
[epoch11, step2680]: loss 0.655680
[epoch11, step2681]: loss 0.612138
[epoch11, step2682]: loss 0.686298
[epoch11, step2683]: loss 0.733688
[epoch11, step2684]: loss 0.439927
[epoch11, step2685]: loss 0.611670
[epoch11, step2686]: loss 0.474178
[epoch11, step2687]: loss 0.736427
[epoch11, step2688]: loss 0.578462
[epoch11, step2689]: loss 0.619693
[epoch11, step2690]: loss 0.716458
[epoch11, step2691]: loss 0.503846
[epoch11, step2692]: loss 0.618858
[epoch11, step2693]: loss 0.647036
[epoch11, step2694]: loss 0.620186
[epoch11, step2695]: loss 0.715493
[epoch11, step2696]: loss 0.698988
[epoch11, step2697]: loss 0.621234
[epoch11, step2698]: loss 0.829361
[epoch11, step2699]: loss 0.843194
[epoch11, step2700]: loss 0.545842
[epoch11, step2701]: loss 0.602161
[epoch11, step2702]: loss 0.400205
[epoch11, step2703]: loss 0.674451
[epoch11, step2704]: loss 0.419375
[epoch11, step2705]: loss 0.415149
[epoch11, step2706]: loss 0.688540
[epoch11, step2707]: loss 0.388235
[epoch11, step2708]: loss 0.745537
[epoch11, step2709]: loss 0.556743
[epoch11, step2710]: loss 0.383096
[epoch11, step2711]: loss 0.955664
[epoch11, step2712]: loss 0.549861
[epoch11, step2713]: loss 0.671559
[epoch11, step2714]: loss 0.552724
[epoch11, step2715]: loss 0.449591
[epoch11, step2716]: loss 0.546753
[epoch11, step2717]: loss 0.579629
[epoch11, step2718]: loss 0.418547
[epoch11, step2719]: loss 0.713256
[epoch11, step2720]: loss 0.500949
[epoch11, step2721]: loss 0.468596
[epoch11, step2722]: loss 0.502765
[epoch11, step2723]: loss 0.618533
[epoch11, step2724]: loss 0.626907
[epoch11, step2725]: loss 0.683273
[epoch11, step2726]: loss 0.538226
[epoch11, step2727]: loss 0.458827
[epoch11, step2728]: loss 0.542146
[epoch11, step2729]: loss 0.530174
[epoch11, step2730]: loss 0.364138
[epoch11, step2731]: loss 0.434230
[epoch11, step2732]: loss 0.330942
[epoch11, step2733]: loss 0.361659
[epoch11, step2734]: loss 0.401120
[epoch11, step2735]: loss 0.483166
[epoch11, step2736]: loss 0.668929
[epoch11, step2737]: loss 0.705301
[epoch11, step2738]: loss 0.569789
[epoch11, step2739]: loss 0.476292
[epoch11, step2740]: loss 0.744626
[epoch11, step2741]: loss 0.543736
[epoch11, step2742]: loss 0.821606
[epoch11, step2743]: loss 0.610359
[epoch11, step2744]: loss 0.598304
[epoch11, step2745]: loss 0.754956
[epoch11, step2746]: loss 0.319724
[epoch11, step2747]: loss 0.546929
[epoch11, step2748]: loss 0.597154
[epoch11, step2749]: loss 0.658528
[epoch11, step2750]: loss 0.524058
[epoch11, step2751]: loss 0.519162
[epoch11, step2752]: loss 0.329136
[epoch11, step2753]: loss 0.684940
[epoch11, step2754]: loss 0.679021
[epoch11, step2755]: loss 0.536874
[epoch11, step2756]: loss 0.504065
[epoch11, step2757]: loss 0.235267
[epoch11, step2758]: loss 0.565345
[epoch11, step2759]: loss 0.544591
[epoch11, step2760]: loss 0.466993
[epoch11, step2761]: loss 0.374147
[epoch11, step2762]: loss 0.489954
[epoch11, step2763]: loss 0.655831
[epoch11, step2764]: loss 0.681589
[epoch11, step2765]: loss 0.399004
[epoch11, step2766]: loss 0.481263
[epoch11, step2767]: loss 0.519028
[epoch11, step2768]: loss 0.680820
[epoch11, step2769]: loss 0.461883
[epoch11, step2770]: loss 0.712781
[epoch11, step2771]: loss 0.609277
[epoch11, step2772]: loss 0.824963
[epoch11, step2773]: loss 0.615251
[epoch11, step2774]: loss 0.387945
[epoch11, step2775]: loss 0.898719
[epoch11, step2776]: loss 0.621473
[epoch11, step2777]: loss 0.317128
[epoch11, step2778]: loss 0.636897
[epoch11, step2779]: loss 0.772056
[epoch11, step2780]: loss 0.522609
[epoch11, step2781]: loss 0.736105
[epoch11, step2782]: loss 0.584482
[epoch11, step2783]: loss 0.426017
[epoch11, step2784]: loss 0.845221
[epoch11, step2785]: loss 0.841574
[epoch11, step2786]: loss 0.427335
[epoch11, step2787]: loss 0.505662
[epoch11, step2788]: loss 0.544717
[epoch11, step2789]: loss 0.264310
[epoch11, step2790]: loss 0.412834
[epoch11, step2791]: loss 0.737222
[epoch11, step2792]: loss 0.446246
[epoch11, step2793]: loss 0.550450
[epoch11, step2794]: loss 0.197891
[epoch11, step2795]: loss 0.675595
[epoch11, step2796]: loss 0.372247
[epoch11, step2797]: loss 0.555614
[epoch11, step2798]: loss 0.759665
[epoch11, step2799]: loss 0.582263
[epoch11, step2800]: loss 0.598181
[epoch11, step2801]: loss 0.460676
[epoch11, step2802]: loss 0.683860
[epoch11, step2803]: loss 0.687346
[epoch11, step2804]: loss 0.705879
[epoch11, step2805]: loss 0.569157
[epoch11, step2806]: loss 0.545224
[epoch11, step2807]: loss 0.661170
[epoch11, step2808]: loss 0.348512
[epoch11, step2809]: loss 0.563526
[epoch11, step2810]: loss 0.558304
[epoch11, step2811]: loss 0.557116
[epoch11, step2812]: loss 0.732566
[epoch11, step2813]: loss 0.640041
[epoch11, step2814]: loss 0.352692
[epoch11, step2815]: loss 0.617745
[epoch11, step2816]: loss 0.716421
[epoch11, step2817]: loss 0.549351
[epoch11, step2818]: loss 0.614730
[epoch11, step2819]: loss 0.564013
[epoch11, step2820]: loss 0.770469
[epoch11, step2821]: loss 0.402686
[epoch11, step2822]: loss 0.478302
[epoch11, step2823]: loss 0.584290
[epoch11, step2824]: loss 0.738226
[epoch11, step2825]: loss 0.303491
[epoch11, step2826]: loss 0.363160
[epoch11, step2827]: loss 0.591656
[epoch11, step2828]: loss 0.554764
[epoch11, step2829]: loss 0.742734
[epoch11, step2830]: loss 0.621713
[epoch11, step2831]: loss 0.510945
[epoch11, step2832]: loss 0.488508
[epoch11, step2833]: loss 0.720086
[epoch11, step2834]: loss 0.693351
[epoch11, step2835]: loss 0.521314
[epoch11, step2836]: loss 0.731298
[epoch11, step2837]: loss 0.690347
[epoch11, step2838]: loss 0.664457
[epoch11, step2839]: loss 0.204012
[epoch11, step2840]: loss 0.503538
[epoch11, step2841]: loss 0.574593
[epoch11, step2842]: loss 0.560467
[epoch11, step2843]: loss 0.673512
[epoch11, step2844]: loss 0.521018
[epoch11, step2845]: loss 0.397920
[epoch11, step2846]: loss 0.375210
[epoch11, step2847]: loss 0.389791
[epoch11, step2848]: loss 0.647860
[epoch11, step2849]: loss 0.735515
[epoch11, step2850]: loss 0.796075
[epoch11, step2851]: loss 0.445459
[epoch11, step2852]: loss 0.525200
[epoch11, step2853]: loss 0.580340
[epoch11, step2854]: loss 0.614823
[epoch11, step2855]: loss 0.508754
[epoch11, step2856]: loss 0.607498
[epoch11, step2857]: loss 0.533182
[epoch11, step2858]: loss 0.541031
[epoch11, step2859]: loss 0.862137
[epoch11, step2860]: loss 0.738645
[epoch11, step2861]: loss 0.273012
[epoch11, step2862]: loss 0.135752
[epoch11, step2863]: loss 0.665801
[epoch11, step2864]: loss 0.572700
[epoch11, step2865]: loss 0.440712
[epoch11, step2866]: loss 0.586997
[epoch11, step2867]: loss 0.850580
[epoch11, step2868]: loss 0.422098
[epoch11, step2869]: loss 0.358402
[epoch11, step2870]: loss 0.249848
[epoch11, step2871]: loss 0.608030
[epoch11, step2872]: loss 0.694655
[epoch11, step2873]: loss 0.752772
[epoch11, step2874]: loss 0.484466
[epoch11, step2875]: loss 0.573359
[epoch11, step2876]: loss 0.718361
[epoch11, step2877]: loss 0.754270
[epoch11, step2878]: loss 0.526091
[epoch11, step2879]: loss 0.615188
[epoch11, step2880]: loss 0.631257
[epoch11, step2881]: loss 0.791701
[epoch11, step2882]: loss 0.694865
[epoch11, step2883]: loss 0.804377
[epoch11, step2884]: loss 0.570514
[epoch11, step2885]: loss 0.624769
[epoch11, step2886]: loss 0.611171
[epoch11, step2887]: loss 0.409238
[epoch11, step2888]: loss 0.650086
[epoch11, step2889]: loss 0.662377
[epoch11, step2890]: loss 0.614054
[epoch11, step2891]: loss 0.696564
[epoch11, step2892]: loss 0.466953
[epoch11, step2893]: loss 0.600037
[epoch11, step2894]: loss 0.572204
[epoch11, step2895]: loss 0.509180
[epoch11, step2896]: loss 0.236029
[epoch11, step2897]: loss 0.687951
[epoch11, step2898]: loss 0.492959
[epoch11, step2899]: loss 0.635309
[epoch11, step2900]: loss 0.640323
[epoch11, step2901]: loss 0.544382
[epoch11, step2902]: loss 0.608936
[epoch11, step2903]: loss 0.662977
[epoch11, step2904]: loss 0.672411
[epoch11, step2905]: loss 0.421404
[epoch11, step2906]: loss 0.352236
[epoch11, step2907]: loss 0.501689
[epoch11, step2908]: loss 0.691596
[epoch11, step2909]: loss 0.711862
[epoch11, step2910]: loss 0.631806
[epoch11, step2911]: loss 0.492992
[epoch11, step2912]: loss 0.337062
[epoch11, step2913]: loss 0.438933
[epoch11, step2914]: loss 0.313788
[epoch11, step2915]: loss 0.285002
[epoch11, step2916]: loss 0.588867
[epoch11, step2917]: loss 0.741513
[epoch11, step2918]: loss 0.489865
[epoch11, step2919]: loss 0.615033
[epoch11, step2920]: loss 0.467873
[epoch11, step2921]: loss 0.588383
[epoch11, step2922]: loss 0.770276
[epoch11, step2923]: loss 0.503741
[epoch11, step2924]: loss 0.858206
[epoch11, step2925]: loss 0.543244
[epoch11, step2926]: loss 0.676265
[epoch11, step2927]: loss 0.590234
[epoch11, step2928]: loss 0.531384
[epoch11, step2929]: loss 0.706610
[epoch11, step2930]: loss 0.585913
[epoch11, step2931]: loss 0.698734
[epoch11, step2932]: loss 0.665109
[epoch11, step2933]: loss 0.576592
[epoch11, step2934]: loss 0.797419
[epoch11, step2935]: loss 0.740859
[epoch11, step2936]: loss 0.778134
[epoch11, step2937]: loss 0.640348
[epoch11, step2938]: loss 0.386105
[epoch11, step2939]: loss 0.760768
[epoch11, step2940]: loss 0.656223
[epoch11, step2941]: loss 0.430827
[epoch11, step2942]: loss 0.624305
[epoch11, step2943]: loss 0.314454
[epoch11, step2944]: loss 0.680738
[epoch11, step2945]: loss 0.735192
[epoch11, step2946]: loss 0.614427
[epoch11, step2947]: loss 0.581495
[epoch11, step2948]: loss 0.442134
[epoch11, step2949]: loss 0.364176
[epoch11, step2950]: loss 0.519599
[epoch11, step2951]: loss 0.843047
[epoch11, step2952]: loss 0.524516
[epoch11, step2953]: loss 0.775771
[epoch11, step2954]: loss 0.391132
[epoch11, step2955]: loss 0.499091
[epoch11, step2956]: loss 0.739098
[epoch11, step2957]: loss 0.661309
[epoch11, step2958]: loss 0.790629
[epoch11, step2959]: loss 0.629048
[epoch11, step2960]: loss 0.610495
[epoch11, step2961]: loss 0.146535
[epoch11, step2962]: loss 0.253983
[epoch11, step2963]: loss 0.609374
[epoch11, step2964]: loss 0.642598
[epoch11, step2965]: loss 0.714083
[epoch11, step2966]: loss 0.356135
[epoch11, step2967]: loss 0.583763
[epoch11, step2968]: loss 0.374659
[epoch11, step2969]: loss 0.544723
[epoch11, step2970]: loss 0.280452
[epoch11, step2971]: loss 0.481456
[epoch11, step2972]: loss 0.537206
[epoch11, step2973]: loss 0.405419
[epoch11, step2974]: loss 0.726604
[epoch11, step2975]: loss 0.678137
[epoch11, step2976]: loss 0.351583
[epoch11, step2977]: loss 0.725751
[epoch11, step2978]: loss 0.672772
[epoch11, step2979]: loss 0.555361
[epoch11, step2980]: loss 0.800144
[epoch11, step2981]: loss 0.594510
[epoch11, step2982]: loss 0.426202
[epoch11, step2983]: loss 0.758524
[epoch11, step2984]: loss 0.792941
[epoch11, step2985]: loss 0.484957
[epoch11, step2986]: loss 0.692396
[epoch11, step2987]: loss 0.662336
[epoch11, step2988]: loss 0.624906
[epoch11, step2989]: loss 0.704703
[epoch11, step2990]: loss 0.640200
[epoch11, step2991]: loss 0.424488
[epoch11, step2992]: loss 0.675258
[epoch11, step2993]: loss 0.426852
[epoch11, step2994]: loss 0.488109
[epoch11, step2995]: loss 0.588944
[epoch11, step2996]: loss 0.105667
[epoch11, step2997]: loss 0.614017
[epoch11, step2998]: loss 0.630683
[epoch11, step2999]: loss 0.579566
[epoch11, step3000]: loss 0.489785
[epoch11, step3001]: loss 0.530005
[epoch11, step3002]: loss 0.444005
[epoch11, step3003]: loss 0.774120
[epoch11, step3004]: loss 0.681345
[epoch11, step3005]: loss 0.431984
[epoch11, step3006]: loss 0.388491
[epoch11, step3007]: loss 0.505713
[epoch11, step3008]: loss 0.651576
[epoch11, step3009]: loss 0.539850
[epoch11, step3010]: loss 0.520601
[epoch11, step3011]: loss 0.426593
[epoch11, step3012]: loss 0.605110
[epoch11, step3013]: loss 0.600827
[epoch11, step3014]: loss 0.483770
[epoch11, step3015]: loss 0.598287
[epoch11, step3016]: loss 0.470816
[epoch11, step3017]: loss 0.540878
[epoch11, step3018]: loss 0.523996
[epoch11, step3019]: loss 0.423961
[epoch11, step3020]: loss 0.687499
[epoch11, step3021]: loss 0.641132
[epoch11, step3022]: loss 0.561072
[epoch11, step3023]: loss 0.644560
[epoch11, step3024]: loss 0.678121
[epoch11, step3025]: loss 0.489761
[epoch11, step3026]: loss 0.562666
[epoch11, step3027]: loss 0.484537
[epoch11, step3028]: loss 0.401646
[epoch11, step3029]: loss 0.498006
[epoch11, step3030]: loss 0.771029
[epoch11, step3031]: loss 0.578218
[epoch11, step3032]: loss 0.415646
[epoch11, step3033]: loss 0.486638
[epoch11, step3034]: loss 0.879807
[epoch11, step3035]: loss 0.815335
[epoch11, step3036]: loss 0.740084
[epoch11, step3037]: loss 0.730602
[epoch11, step3038]: loss 0.626282
[epoch11, step3039]: loss 0.373446
[epoch11, step3040]: loss 0.573915
[epoch11, step3041]: loss 0.463105
[epoch11, step3042]: loss 0.552116
[epoch11, step3043]: loss 0.734673
[epoch11, step3044]: loss 0.658697
[epoch11, step3045]: loss 0.500974
[epoch11, step3046]: loss 0.478780
[epoch11, step3047]: loss 0.703726
[epoch11, step3048]: loss 0.549177
[epoch11, step3049]: loss 0.511906
[epoch11, step3050]: loss 0.463107
[epoch11, step3051]: loss 0.818394
[epoch11, step3052]: loss 0.515187
[epoch11, step3053]: loss 0.432322
[epoch11, step3054]: loss 0.376045
[epoch11, step3055]: loss 0.260204
[epoch11, step3056]: loss 0.669956
[epoch11, step3057]: loss 0.652494
[epoch11, step3058]: loss 0.713403
[epoch11, step3059]: loss 0.513883
[epoch11, step3060]: loss 0.365296
[epoch11, step3061]: loss 0.472541
[epoch11, step3062]: loss 0.404593
[epoch11, step3063]: loss 0.680212
[epoch11, step3064]: loss 0.597804
[epoch11, step3065]: loss 0.739745
[epoch11, step3066]: loss 0.409895
[epoch11, step3067]: loss 0.582014
[epoch11, step3068]: loss 0.515516
[epoch11, step3069]: loss 0.447042
[epoch11, step3070]: loss 0.719931
[epoch11, step3071]: loss 0.614157
[epoch11, step3072]: loss 0.681276
[epoch11, step3073]: loss 0.460189
[epoch11, step3074]: loss 0.603507
[epoch11, step3075]: loss 0.487123
[epoch11, step3076]: loss 0.255041

[epoch11]: avg loss 0.255041

[epoch12, step1]: loss 0.489112
[epoch12, step2]: loss 0.444772
[epoch12, step3]: loss 0.587993
[epoch12, step4]: loss 0.435874
[epoch12, step5]: loss 0.583693
[epoch12, step6]: loss 0.420375
[epoch12, step7]: loss 0.395078
[epoch12, step8]: loss 0.830867
[epoch12, step9]: loss 0.628988
[epoch12, step10]: loss 0.654363
[epoch12, step11]: loss 0.564407
[epoch12, step12]: loss 0.552030
[epoch12, step13]: loss 0.752295
[epoch12, step14]: loss 0.552755
[epoch12, step15]: loss 0.679422
[epoch12, step16]: loss 0.372079
[epoch12, step17]: loss 0.544620
[epoch12, step18]: loss 0.312594
[epoch12, step19]: loss 0.648369
[epoch12, step20]: loss 0.442197
[epoch12, step21]: loss 0.149102
[epoch12, step22]: loss 0.462356
[epoch12, step23]: loss 0.685321
[epoch12, step24]: loss 0.493502
[epoch12, step25]: loss 0.579573
[epoch12, step26]: loss 0.746662
[epoch12, step27]: loss 0.685789
[epoch12, step28]: loss 0.566933
[epoch12, step29]: loss 0.617810
[epoch12, step30]: loss 0.551299
[epoch12, step31]: loss 0.644830
[epoch12, step32]: loss 0.522873
[epoch12, step33]: loss 0.763855
[epoch12, step34]: loss 0.690183
[epoch12, step35]: loss 0.601101
[epoch12, step36]: loss 0.577509
[epoch12, step37]: loss 0.563329
[epoch12, step38]: loss 0.753164
[epoch12, step39]: loss 0.513101
[epoch12, step40]: loss 0.778758
[epoch12, step41]: loss 0.419905
[epoch12, step42]: loss 0.592130
[epoch12, step43]: loss 0.777892
[epoch12, step44]: loss 0.450145
[epoch12, step45]: loss 0.625632
[epoch12, step46]: loss 0.686077
[epoch12, step47]: loss 0.656835
[epoch12, step48]: loss 0.452300
[epoch12, step49]: loss 0.598683
[epoch12, step50]: loss 0.475062
[epoch12, step51]: loss 0.596389
[epoch12, step52]: loss 0.687779
[epoch12, step53]: loss 0.741768
[epoch12, step54]: loss 0.294790
[epoch12, step55]: loss 0.572985
[epoch12, step56]: loss 0.704789
[epoch12, step57]: loss 0.699024
[epoch12, step58]: loss 0.469274
[epoch12, step59]: loss 0.307857
[epoch12, step60]: loss 0.418298
[epoch12, step61]: loss 0.660113
[epoch12, step62]: loss 0.807366
[epoch12, step63]: loss 0.620904
[epoch12, step64]: loss 0.652947
[epoch12, step65]: loss 0.575727
[epoch12, step66]: loss 0.794418
[epoch12, step67]: loss 0.370868
[epoch12, step68]: loss 0.730118
[epoch12, step69]: loss 0.641866
[epoch12, step70]: loss 0.604442
[epoch12, step71]: loss 0.730976
[epoch12, step72]: loss 0.711143
[epoch12, step73]: loss 0.721845
[epoch12, step74]: loss 0.480326
[epoch12, step75]: loss 0.652557
[epoch12, step76]: loss 0.509705
[epoch12, step77]: loss 0.745664
[epoch12, step78]: loss 0.522847
[epoch12, step79]: loss 0.409876
[epoch12, step80]: loss 0.574233
[epoch12, step81]: loss 0.686661
[epoch12, step82]: loss 0.603670
[epoch12, step83]: loss 0.370409
[epoch12, step84]: loss 0.796223
[epoch12, step85]: loss 0.538332
[epoch12, step86]: loss 0.861072
[epoch12, step87]: loss 0.720252
[epoch12, step88]: loss 0.670486
[epoch12, step89]: loss 0.674066
[epoch12, step90]: loss 0.559185
[epoch12, step91]: loss 0.695104
[epoch12, step92]: loss 0.744773
[epoch12, step93]: loss 0.425610
[epoch12, step94]: loss 0.583091
[epoch12, step95]: loss 0.589877
[epoch12, step96]: loss 0.391035
[epoch12, step97]: loss 0.525693
[epoch12, step98]: loss 0.736206
[epoch12, step99]: loss 0.405166
[epoch12, step100]: loss 0.449476
[epoch12, step101]: loss 0.696708
[epoch12, step102]: loss 0.423921
[epoch12, step103]: loss 0.556056
[epoch12, step104]: loss 0.472546
[epoch12, step105]: loss 0.487187
[epoch12, step106]: loss 0.674029
[epoch12, step107]: loss 0.714926
[epoch12, step108]: loss 0.812009
[epoch12, step109]: loss 0.665759
[epoch12, step110]: loss 0.529726
[epoch12, step111]: loss 0.468279
[epoch12, step112]: loss 0.580074
[epoch12, step113]: loss 0.669831
[epoch12, step114]: loss 0.745969
[epoch12, step115]: loss 0.559555
[epoch12, step116]: loss 0.346490
[epoch12, step117]: loss 0.610709
[epoch12, step118]: loss 0.200741
[epoch12, step119]: loss 0.616289
[epoch12, step120]: loss 0.704808
[epoch12, step121]: loss 0.711334
[epoch12, step122]: loss 0.619084
[epoch12, step123]: loss 0.592444
[epoch12, step124]: loss 0.692112
[epoch12, step125]: loss 0.583586
[epoch12, step126]: loss 0.618842
[epoch12, step127]: loss 0.465394
[epoch12, step128]: loss 0.250502
[epoch12, step129]: loss 0.619341
[epoch12, step130]: loss 0.312155
[epoch12, step131]: loss 0.593553
[epoch12, step132]: loss 0.542351
[epoch12, step133]: loss 0.461859
[epoch12, step134]: loss 0.464105
[epoch12, step135]: loss 0.657899
[epoch12, step136]: loss 0.366919
[epoch12, step137]: loss 0.607965
[epoch12, step138]: loss 0.733727
[epoch12, step139]: loss 0.616777
[epoch12, step140]: loss 0.373385
[epoch12, step141]: loss 0.527615
[epoch12, step142]: loss 0.717409
[epoch12, step143]: loss 0.629667
[epoch12, step144]: loss 0.470524
[epoch12, step145]: loss 0.488583
[epoch12, step146]: loss 0.444040
[epoch12, step147]: loss 0.637640
[epoch12, step148]: loss 0.510321
[epoch12, step149]: loss 0.760828
[epoch12, step150]: loss 0.591947
[epoch12, step151]: loss 0.618429
[epoch12, step152]: loss 0.514866
[epoch12, step153]: loss 0.631658
[epoch12, step154]: loss 0.664339
[epoch12, step155]: loss 0.685643
[epoch12, step156]: loss 0.538103
[epoch12, step157]: loss 0.546643
[epoch12, step158]: loss 0.592440
[epoch12, step159]: loss 0.613710
[epoch12, step160]: loss 0.319334
[epoch12, step161]: loss 0.458616
[epoch12, step162]: loss 0.497069
[epoch12, step163]: loss 0.666763
[epoch12, step164]: loss 0.472609
[epoch12, step165]: loss 0.543298
[epoch12, step166]: loss 0.633361
[epoch12, step167]: loss 0.592922
[epoch12, step168]: loss 0.584022
[epoch12, step169]: loss 0.593727
[epoch12, step170]: loss 0.586956
[epoch12, step171]: loss 0.610307
[epoch12, step172]: loss 0.633680
[epoch12, step173]: loss 0.333330
[epoch12, step174]: loss 0.414017
[epoch12, step175]: loss 0.518765
[epoch12, step176]: loss 0.498255
[epoch12, step177]: loss 0.569977
[epoch12, step178]: loss 0.526816
[epoch12, step179]: loss 0.562873
[epoch12, step180]: loss 0.419793
[epoch12, step181]: loss 0.445420
[epoch12, step182]: loss 0.764646
[epoch12, step183]: loss 0.708704
[epoch12, step184]: loss 0.702922
[epoch12, step185]: loss 0.747863
[epoch12, step186]: loss 0.472501
[epoch12, step187]: loss 0.496639
[epoch12, step188]: loss 0.592964
[epoch12, step189]: loss 0.646345
[epoch12, step190]: loss 0.501588
[epoch12, step191]: loss 0.582189
[epoch12, step192]: loss 0.701182
[epoch12, step193]: loss 0.450623
[epoch12, step194]: loss 0.530889
[epoch12, step195]: loss 0.324136
[epoch12, step196]: loss 0.511866
[epoch12, step197]: loss 0.825090
[epoch12, step198]: loss 0.637420
[epoch12, step199]: loss 0.532854
[epoch12, step200]: loss 0.713700
[epoch12, step201]: loss 0.676810
[epoch12, step202]: loss 0.347221
[epoch12, step203]: loss 0.350931
[epoch12, step204]: loss 0.466279
[epoch12, step205]: loss 0.702697
[epoch12, step206]: loss 0.511962
[epoch12, step207]: loss 0.503742
[epoch12, step208]: loss 0.496561
[epoch12, step209]: loss 0.248845
[epoch12, step210]: loss 0.765976
[epoch12, step211]: loss 0.528529
[epoch12, step212]: loss 0.596435
[epoch12, step213]: loss 0.579131
[epoch12, step214]: loss 0.755187
[epoch12, step215]: loss 0.516864
[epoch12, step216]: loss 0.815996
[epoch12, step217]: loss 0.780917
[epoch12, step218]: loss 0.590055
[epoch12, step219]: loss 0.639130
[epoch12, step220]: loss 0.391111
[epoch12, step221]: loss 0.196247
[epoch12, step222]: loss 0.647657
[epoch12, step223]: loss 0.389764
[epoch12, step224]: loss 0.640461
[epoch12, step225]: loss 0.632978
[epoch12, step226]: loss 0.891520
[epoch12, step227]: loss 0.646837
[epoch12, step228]: loss 0.684960
[epoch12, step229]: loss 0.817095
[epoch12, step230]: loss 0.421941
[epoch12, step231]: loss 0.463636
[epoch12, step232]: loss 0.498941
[epoch12, step233]: loss 0.556045
[epoch12, step234]: loss 0.634583
[epoch12, step235]: loss 0.188635
[epoch12, step236]: loss 0.517015
[epoch12, step237]: loss 0.751172
[epoch12, step238]: loss 0.563421
[epoch12, step239]: loss 0.575797
[epoch12, step240]: loss 0.629344
[epoch12, step241]: loss 0.549618
[epoch12, step242]: loss 0.907862
[epoch12, step243]: loss 0.661921
[epoch12, step244]: loss 0.556398
[epoch12, step245]: loss 0.583497
[epoch12, step246]: loss 0.726278
[epoch12, step247]: loss 0.628837
[epoch12, step248]: loss 0.637531
[epoch12, step249]: loss 0.560897
[epoch12, step250]: loss 0.696163
[epoch12, step251]: loss 0.603731
[epoch12, step252]: loss 0.170307
[epoch12, step253]: loss 0.116046
[epoch12, step254]: loss 0.569462
[epoch12, step255]: loss 0.419947
[epoch12, step256]: loss 0.435453
[epoch12, step257]: loss 0.323362
[epoch12, step258]: loss 0.480745
[epoch12, step259]: loss 0.613360
[epoch12, step260]: loss 0.517284
[epoch12, step261]: loss 0.490910
[epoch12, step262]: loss 0.440211
[epoch12, step263]: loss 0.675456
[epoch12, step264]: loss 0.467327
[epoch12, step265]: loss 0.731967
[epoch12, step266]: loss 0.597172
[epoch12, step267]: loss 0.591430
[epoch12, step268]: loss 0.543955
[epoch12, step269]: loss 0.708973
[epoch12, step270]: loss 0.663518
[epoch12, step271]: loss 0.360687
[epoch12, step272]: loss 0.560498
[epoch12, step273]: loss 0.596374
[epoch12, step274]: loss 0.678145
[epoch12, step275]: loss 0.648675
[epoch12, step276]: loss 0.568884
[epoch12, step277]: loss 0.377676
[epoch12, step278]: loss 0.463227
[epoch12, step279]: loss 0.635786
[epoch12, step280]: loss 0.567564
[epoch12, step281]: loss 0.364589
[epoch12, step282]: loss 0.630094
[epoch12, step283]: loss 0.584358
[epoch12, step284]: loss 0.237858
[epoch12, step285]: loss 0.448552
[epoch12, step286]: loss 0.666380
[epoch12, step287]: loss 0.566137
[epoch12, step288]: loss 0.352409
[epoch12, step289]: loss 0.734959
[epoch12, step290]: loss 0.664985
[epoch12, step291]: loss 0.314136
[epoch12, step292]: loss 0.723307
[epoch12, step293]: loss 0.629363
[epoch12, step294]: loss 0.810120
[epoch12, step295]: loss 0.251978
[epoch12, step296]: loss 0.468512
[epoch12, step297]: loss 0.638023
[epoch12, step298]: loss 0.679562
[epoch12, step299]: loss 0.584501
[epoch12, step300]: loss 0.574798
[epoch12, step301]: loss 0.742298
[epoch12, step302]: loss 0.591568
[epoch12, step303]: loss 0.626508
[epoch12, step304]: loss 0.445332
[epoch12, step305]: loss 0.594896
[epoch12, step306]: loss 0.693600
[epoch12, step307]: loss 0.217850
[epoch12, step308]: loss 0.477527
[epoch12, step309]: loss 0.310575
[epoch12, step310]: loss 0.632884
[epoch12, step311]: loss 0.435911
[epoch12, step312]: loss 0.604006
[epoch12, step313]: loss 0.740661
[epoch12, step314]: loss 0.558870
[epoch12, step315]: loss 0.533842
[epoch12, step316]: loss 0.577981
[epoch12, step317]: loss 0.701794
[epoch12, step318]: loss 0.716087
[epoch12, step319]: loss 0.520857
[epoch12, step320]: loss 0.539216
[epoch12, step321]: loss 0.749956
[epoch12, step322]: loss 0.474602
[epoch12, step323]: loss 0.617484
[epoch12, step324]: loss 0.450286
[epoch12, step325]: loss 0.609430
[epoch12, step326]: loss 0.675176
[epoch12, step327]: loss 0.517522
[epoch12, step328]: loss 0.518332
[epoch12, step329]: loss 0.324507
[epoch12, step330]: loss 0.476626
[epoch12, step331]: loss 0.477181
[epoch12, step332]: loss 0.672087
[epoch12, step333]: loss 0.403960
[epoch12, step334]: loss 0.385387
[epoch12, step335]: loss 0.546723
[epoch12, step336]: loss 0.698219
[epoch12, step337]: loss 0.419577
[epoch12, step338]: loss 0.666297
[epoch12, step339]: loss 0.773646
[epoch12, step340]: loss 0.457917
[epoch12, step341]: loss 0.632302
[epoch12, step342]: loss 0.620547
[epoch12, step343]: loss 0.567406
[epoch12, step344]: loss 0.315520
[epoch12, step345]: loss 0.504848
[epoch12, step346]: loss 0.380360
[epoch12, step347]: loss 0.592812
[epoch12, step348]: loss 0.543572
[epoch12, step349]: loss 0.690666
[epoch12, step350]: loss 0.739357
[epoch12, step351]: loss 0.373075
[epoch12, step352]: loss 0.554373
[epoch12, step353]: loss 0.520903
[epoch12, step354]: loss 0.272051
[epoch12, step355]: loss 0.598299
[epoch12, step356]: loss 0.471312
[epoch12, step357]: loss 0.499409
[epoch12, step358]: loss 0.523208
[epoch12, step359]: loss 0.359509
[epoch12, step360]: loss 0.411856
[epoch12, step361]: loss 0.648664
[epoch12, step362]: loss 0.554775
[epoch12, step363]: loss 0.487188
[epoch12, step364]: loss 0.822030
[epoch12, step365]: loss 0.499420
[epoch12, step366]: loss 0.584502
[epoch12, step367]: loss 0.575731
[epoch12, step368]: loss 0.490561
[epoch12, step369]: loss 0.651586
[epoch12, step370]: loss 0.520786
[epoch12, step371]: loss 0.705058
[epoch12, step372]: loss 0.772857
[epoch12, step373]: loss 0.570051
[epoch12, step374]: loss 0.604908
[epoch12, step375]: loss 0.611282
[epoch12, step376]: loss 0.586345
[epoch12, step377]: loss 0.399690
[epoch12, step378]: loss 0.193638
[epoch12, step379]: loss 0.514579
[epoch12, step380]: loss 0.466167
[epoch12, step381]: loss 0.504759
[epoch12, step382]: loss 0.756674
[epoch12, step383]: loss 0.347244
[epoch12, step384]: loss 0.743562
[epoch12, step385]: loss 0.561088
[epoch12, step386]: loss 0.473775
[epoch12, step387]: loss 0.724671
[epoch12, step388]: loss 0.774110
[epoch12, step389]: loss 0.705905
[epoch12, step390]: loss 0.429576
[epoch12, step391]: loss 0.446513
[epoch12, step392]: loss 0.755292
[epoch12, step393]: loss 0.625772
[epoch12, step394]: loss 0.306046
[epoch12, step395]: loss 0.651043
[epoch12, step396]: loss 0.722330
[epoch12, step397]: loss 0.338077
[epoch12, step398]: loss 0.635883
[epoch12, step399]: loss 0.630121
[epoch12, step400]: loss 0.595885
[epoch12, step401]: loss 0.637364
[epoch12, step402]: loss 0.755555
[epoch12, step403]: loss 0.617324
[epoch12, step404]: loss 0.449059
[epoch12, step405]: loss 0.748613
[epoch12, step406]: loss 1.023916
[epoch12, step407]: loss 0.936823
[epoch12, step408]: loss 0.297519
[epoch12, step409]: loss 0.624119
[epoch12, step410]: loss 0.698967
[epoch12, step411]: loss 0.648812
[epoch12, step412]: loss 0.604690
[epoch12, step413]: loss 0.652147
[epoch12, step414]: loss 0.756171
[epoch12, step415]: loss 0.752841
[epoch12, step416]: loss 0.538293
[epoch12, step417]: loss 0.321473
[epoch12, step418]: loss 0.120556
[epoch12, step419]: loss 0.554862
[epoch12, step420]: loss 0.550626
[epoch12, step421]: loss 0.599130
[epoch12, step422]: loss 0.559956
[epoch12, step423]: loss 0.581851
[epoch12, step424]: loss 0.520608
[epoch12, step425]: loss 0.585706
[epoch12, step426]: loss 0.370429
[epoch12, step427]: loss 0.588612
[epoch12, step428]: loss 0.377161
[epoch12, step429]: loss 0.225952
[epoch12, step430]: loss 0.807985
[epoch12, step431]: loss 0.682022
[epoch12, step432]: loss 0.441169
[epoch12, step433]: loss 0.245901
[epoch12, step434]: loss 0.430685
[epoch12, step435]: loss 0.404022
[epoch12, step436]: loss 0.634734
[epoch12, step437]: loss 0.283865
[epoch12, step438]: loss 0.777370
[epoch12, step439]: loss 0.478812
[epoch12, step440]: loss 0.531806
[epoch12, step441]: loss 0.579263
[epoch12, step442]: loss 0.573471
[epoch12, step443]: loss 0.288295
[epoch12, step444]: loss 0.656134
[epoch12, step445]: loss 0.458184
[epoch12, step446]: loss 0.404297
[epoch12, step447]: loss 0.347114
[epoch12, step448]: loss 0.278144
[epoch12, step449]: loss 0.520266
[epoch12, step450]: loss 0.687548
[epoch12, step451]: loss 0.582359
[epoch12, step452]: loss 0.420615
[epoch12, step453]: loss 0.847124
[epoch12, step454]: loss 0.573063
[epoch12, step455]: loss 0.528077
[epoch12, step456]: loss 0.547428
[epoch12, step457]: loss 0.427692
[epoch12, step458]: loss 0.602440
[epoch12, step459]: loss 0.397390
[epoch12, step460]: loss 0.346322
[epoch12, step461]: loss 0.699639
[epoch12, step462]: loss 0.558505
[epoch12, step463]: loss 0.693781
[epoch12, step464]: loss 0.675084
[epoch12, step465]: loss 0.503973
[epoch12, step466]: loss 0.684957
[epoch12, step467]: loss 0.721747
[epoch12, step468]: loss 0.591123
[epoch12, step469]: loss 0.690875
[epoch12, step470]: loss 0.460669
[epoch12, step471]: loss 0.194203
[epoch12, step472]: loss 0.581294
[epoch12, step473]: loss 0.483384
[epoch12, step474]: loss 0.487964
[epoch12, step475]: loss 0.738831
[epoch12, step476]: loss 0.632007
[epoch12, step477]: loss 0.423197
[epoch12, step478]: loss 0.614227
[epoch12, step479]: loss 0.627298
[epoch12, step480]: loss 0.667657
[epoch12, step481]: loss 0.571258
[epoch12, step482]: loss 0.712297
[epoch12, step483]: loss 0.527690
[epoch12, step484]: loss 0.664591
[epoch12, step485]: loss 0.835174
[epoch12, step486]: loss 0.605292
[epoch12, step487]: loss 0.693929
[epoch12, step488]: loss 0.636658
[epoch12, step489]: loss 0.705472
[epoch12, step490]: loss 0.730128
[epoch12, step491]: loss 0.760653
[epoch12, step492]: loss 0.528920
[epoch12, step493]: loss 0.681662
[epoch12, step494]: loss 0.535131
[epoch12, step495]: loss 0.399439
[epoch12, step496]: loss 0.615589
[epoch12, step497]: loss 0.402137
[epoch12, step498]: loss 0.692298
[epoch12, step499]: loss 0.595393
[epoch12, step500]: loss 0.290411
[epoch12, step501]: loss 0.553738
[epoch12, step502]: loss 0.627800
[epoch12, step503]: loss 0.700645
[epoch12, step504]: loss 0.595832
[epoch12, step505]: loss 0.427545
[epoch12, step506]: loss 0.680843
[epoch12, step507]: loss 0.499367
[epoch12, step508]: loss 0.565944
[epoch12, step509]: loss 0.309721
[epoch12, step510]: loss 0.516586
[epoch12, step511]: loss 0.716670
[epoch12, step512]: loss 0.747526
[epoch12, step513]: loss 0.145422
[epoch12, step514]: loss 0.744700
[epoch12, step515]: loss 0.500731
[epoch12, step516]: loss 0.433009
[epoch12, step517]: loss 0.719868
[epoch12, step518]: loss 0.375379
[epoch12, step519]: loss 0.701957
[epoch12, step520]: loss 0.598010
[epoch12, step521]: loss 0.496617
[epoch12, step522]: loss 0.541070
[epoch12, step523]: loss 0.590330
[epoch12, step524]: loss 0.441104
[epoch12, step525]: loss 0.605646
[epoch12, step526]: loss 0.832178
[epoch12, step527]: loss 0.348930
[epoch12, step528]: loss 0.745767
[epoch12, step529]: loss 0.590987
[epoch12, step530]: loss 0.373337
[epoch12, step531]: loss 0.643423
[epoch12, step532]: loss 0.792345
[epoch12, step533]: loss 0.350043
[epoch12, step534]: loss 0.470888
[epoch12, step535]: loss 0.747437
[epoch12, step536]: loss 0.653268
[epoch12, step537]: loss 0.757664
[epoch12, step538]: loss 0.868737
[epoch12, step539]: loss 0.309675
[epoch12, step540]: loss 0.630444
[epoch12, step541]: loss 0.784015
[epoch12, step542]: loss 0.588460
[epoch12, step543]: loss 0.576002
[epoch12, step544]: loss 0.861934
[epoch12, step545]: loss 0.442066
[epoch12, step546]: loss 0.565761
[epoch12, step547]: loss 0.847603
[epoch12, step548]: loss 0.433221
[epoch12, step549]: loss 0.565691
[epoch12, step550]: loss 0.695994
[epoch12, step551]: loss 0.722185
[epoch12, step552]: loss 0.556125
[epoch12, step553]: loss 0.669318
[epoch12, step554]: loss 0.442047
[epoch12, step555]: loss 0.604620
[epoch12, step556]: loss 0.763893
[epoch12, step557]: loss 0.544048
[epoch12, step558]: loss 0.349982
[epoch12, step559]: loss 0.503966
[epoch12, step560]: loss 0.604839
[epoch12, step561]: loss 0.593713
[epoch12, step562]: loss 0.671126
[epoch12, step563]: loss 0.615190
[epoch12, step564]: loss 0.218561
[epoch12, step565]: loss 0.536393
[epoch12, step566]: loss 0.777778
[epoch12, step567]: loss 0.569318
[epoch12, step568]: loss 0.662401
[epoch12, step569]: loss 0.688516
[epoch12, step570]: loss 0.489745
[epoch12, step571]: loss 0.469023
[epoch12, step572]: loss 0.692000
[epoch12, step573]: loss 0.722305
[epoch12, step574]: loss 0.378016
[epoch12, step575]: loss 0.126968
[epoch12, step576]: loss 0.717007
[epoch12, step577]: loss 0.341417
[epoch12, step578]: loss 0.453494
[epoch12, step579]: loss 0.429245
[epoch12, step580]: loss 0.666521
[epoch12, step581]: loss 0.830165
[epoch12, step582]: loss 0.722951
[epoch12, step583]: loss 0.465180
[epoch12, step584]: loss 0.678128
[epoch12, step585]: loss 0.576711
[epoch12, step586]: loss 0.582652
[epoch12, step587]: loss 0.750540
[epoch12, step588]: loss 0.794020
[epoch12, step589]: loss 0.428438
[epoch12, step590]: loss 0.502287
[epoch12, step591]: loss 0.764417
[epoch12, step592]: loss 0.641915
[epoch12, step593]: loss 0.668743
[epoch12, step594]: loss 0.561179
[epoch12, step595]: loss 0.611906
[epoch12, step596]: loss 0.525914
[epoch12, step597]: loss 0.665601
[epoch12, step598]: loss 0.630104
[epoch12, step599]: loss 0.385040
[epoch12, step600]: loss 0.420226
[epoch12, step601]: loss 0.371082
[epoch12, step602]: loss 0.551161
[epoch12, step603]: loss 0.412571
[epoch12, step604]: loss 0.655800
[epoch12, step605]: loss 0.709372
[epoch12, step606]: loss 0.441869
[epoch12, step607]: loss 0.599822
[epoch12, step608]: loss 0.372330
[epoch12, step609]: loss 0.594198
[epoch12, step610]: loss 0.618858
[epoch12, step611]: loss 0.622300
[epoch12, step612]: loss 0.756815
[epoch12, step613]: loss 0.538190
[epoch12, step614]: loss 0.664950
[epoch12, step615]: loss 0.508208
[epoch12, step616]: loss 0.610327
[epoch12, step617]: loss 0.370952
[epoch12, step618]: loss 0.443978
[epoch12, step619]: loss 0.392694
[epoch12, step620]: loss 0.546018
[epoch12, step621]: loss 0.669110
[epoch12, step622]: loss 0.503375
[epoch12, step623]: loss 0.719656
[epoch12, step624]: loss 0.343271
[epoch12, step625]: loss 0.494604
[epoch12, step626]: loss 0.691638
[epoch12, step627]: loss 0.571432
[epoch12, step628]: loss 0.791307
[epoch12, step629]: loss 0.403506
[epoch12, step630]: loss 0.723178
[epoch12, step631]: loss 0.841525
[epoch12, step632]: loss 0.343043
[epoch12, step633]: loss 0.591604
[epoch12, step634]: loss 0.820473
[epoch12, step635]: loss 0.472357
[epoch12, step636]: loss 0.416388
[epoch12, step637]: loss 0.253665
[epoch12, step638]: loss 0.490039
[epoch12, step639]: loss 0.566612
[epoch12, step640]: loss 0.659016
[epoch12, step641]: loss 0.609099
[epoch12, step642]: loss 0.455765
[epoch12, step643]: loss 0.633785
[epoch12, step644]: loss 0.881195
[epoch12, step645]: loss 0.771984
[epoch12, step646]: loss 0.647111
[epoch12, step647]: loss 0.665517
[epoch12, step648]: loss 0.230644
[epoch12, step649]: loss 0.253448
[epoch12, step650]: loss 0.534661
[epoch12, step651]: loss 0.719712
[epoch12, step652]: loss 0.514492
[epoch12, step653]: loss 0.567076
[epoch12, step654]: loss 0.430698
[epoch12, step655]: loss 0.459997
[epoch12, step656]: loss 0.303088
[epoch12, step657]: loss 0.629727
[epoch12, step658]: loss 0.612523
[epoch12, step659]: loss 0.628843
[epoch12, step660]: loss 0.688440
[epoch12, step661]: loss 0.544648
[epoch12, step662]: loss 0.563293
[epoch12, step663]: loss 0.411894
[epoch12, step664]: loss 0.482413
[epoch12, step665]: loss 0.603634
[epoch12, step666]: loss 0.565693
[epoch12, step667]: loss 0.436277
[epoch12, step668]: loss 0.516713
[epoch12, step669]: loss 0.498423
[epoch12, step670]: loss 0.655505
[epoch12, step671]: loss 0.636699
[epoch12, step672]: loss 0.383197
[epoch12, step673]: loss 0.645602
[epoch12, step674]: loss 0.596790
[epoch12, step675]: loss 0.663316
[epoch12, step676]: loss 0.912620
[epoch12, step677]: loss 0.563036
[epoch12, step678]: loss 0.582976
[epoch12, step679]: loss 0.586646
[epoch12, step680]: loss 0.647507
[epoch12, step681]: loss 0.359935
[epoch12, step682]: loss 0.415188
[epoch12, step683]: loss 0.727645
[epoch12, step684]: loss 0.603651
[epoch12, step685]: loss 0.606463
[epoch12, step686]: loss 0.741826
[epoch12, step687]: loss 0.375618
[epoch12, step688]: loss 0.416097
[epoch12, step689]: loss 0.524691
[epoch12, step690]: loss 0.488898
[epoch12, step691]: loss 0.717364
[epoch12, step692]: loss 0.672983
[epoch12, step693]: loss 0.507639
[epoch12, step694]: loss 0.904950
[epoch12, step695]: loss 0.555896
[epoch12, step696]: loss 0.531364
[epoch12, step697]: loss 0.810230
[epoch12, step698]: loss 0.406203
[epoch12, step699]: loss 0.424755
[epoch12, step700]: loss 0.753772
[epoch12, step701]: loss 0.484130
[epoch12, step702]: loss 0.624753
[epoch12, step703]: loss 0.567360
[epoch12, step704]: loss 0.625657
[epoch12, step705]: loss 0.518787
[epoch12, step706]: loss 0.607599
[epoch12, step707]: loss 0.502910
[epoch12, step708]: loss 0.705691
[epoch12, step709]: loss 0.414955
[epoch12, step710]: loss 0.522934
[epoch12, step711]: loss 0.369015
[epoch12, step712]: loss 0.554104
[epoch12, step713]: loss 0.433763
[epoch12, step714]: loss 0.380559
[epoch12, step715]: loss 0.527854
[epoch12, step716]: loss 0.792384
[epoch12, step717]: loss 0.370964
[epoch12, step718]: loss 0.473241
[epoch12, step719]: loss 0.684254
[epoch12, step720]: loss 0.555160
[epoch12, step721]: loss 0.697376
[epoch12, step722]: loss 0.345709
[epoch12, step723]: loss 0.105513
[epoch12, step724]: loss 0.580739
[epoch12, step725]: loss 0.594791
[epoch12, step726]: loss 0.557232
[epoch12, step727]: loss 0.572176
[epoch12, step728]: loss 0.460754
[epoch12, step729]: loss 0.313964
[epoch12, step730]: loss 0.573751
[epoch12, step731]: loss 0.331491
[epoch12, step732]: loss 0.617235
[epoch12, step733]: loss 0.666190
[epoch12, step734]: loss 0.656967
[epoch12, step735]: loss 0.586534
[epoch12, step736]: loss 0.473740
[epoch12, step737]: loss 0.504316
[epoch12, step738]: loss 0.610612
[epoch12, step739]: loss 0.263511
[epoch12, step740]: loss 0.681489
[epoch12, step741]: loss 0.661269
[epoch12, step742]: loss 0.602222
[epoch12, step743]: loss 0.721058
[epoch12, step744]: loss 0.270820
[epoch12, step745]: loss 0.386142
[epoch12, step746]: loss 0.768828
[epoch12, step747]: loss 0.388652
[epoch12, step748]: loss 0.496718
[epoch12, step749]: loss 0.562488
[epoch12, step750]: loss 0.642855
[epoch12, step751]: loss 0.623326
[epoch12, step752]: loss 0.672469
[epoch12, step753]: loss 0.519592
[epoch12, step754]: loss 0.709009
[epoch12, step755]: loss 0.642197
[epoch12, step756]: loss 0.431567
[epoch12, step757]: loss 0.742046
[epoch12, step758]: loss 0.749382
[epoch12, step759]: loss 0.494198
[epoch12, step760]: loss 0.631666
[epoch12, step761]: loss 0.592050
[epoch12, step762]: loss 0.458004
[epoch12, step763]: loss 0.458652
[epoch12, step764]: loss 0.591618
[epoch12, step765]: loss 0.654424
[epoch12, step766]: loss 0.630397
[epoch12, step767]: loss 0.359314
[epoch12, step768]: loss 0.638990
[epoch12, step769]: loss 0.458783
[epoch12, step770]: loss 0.583398
[epoch12, step771]: loss 0.319503
[epoch12, step772]: loss 0.465876
[epoch12, step773]: loss 0.797859
[epoch12, step774]: loss 0.469430
[epoch12, step775]: loss 0.291622
[epoch12, step776]: loss 0.582084
[epoch12, step777]: loss 0.533525
[epoch12, step778]: loss 0.627376
[epoch12, step779]: loss 0.401861
[epoch12, step780]: loss 0.786656
[epoch12, step781]: loss 0.487640
[epoch12, step782]: loss 0.689603
[epoch12, step783]: loss 0.700282
[epoch12, step784]: loss 0.589621
[epoch12, step785]: loss 0.567282
[epoch12, step786]: loss 0.464775
[epoch12, step787]: loss 0.558206
[epoch12, step788]: loss 0.578002
[epoch12, step789]: loss 0.633225
[epoch12, step790]: loss 0.527020
[epoch12, step791]: loss 0.648454
[epoch12, step792]: loss 0.674285
[epoch12, step793]: loss 0.457592
[epoch12, step794]: loss 0.375916
[epoch12, step795]: loss 0.750573
[epoch12, step796]: loss 0.698346
[epoch12, step797]: loss 0.420867
[epoch12, step798]: loss 0.583426
[epoch12, step799]: loss 0.367476
[epoch12, step800]: loss 0.380758
[epoch12, step801]: loss 0.138599
[epoch12, step802]: loss 0.637169
[epoch12, step803]: loss 0.500818
[epoch12, step804]: loss 0.607453
[epoch12, step805]: loss 0.775945
[epoch12, step806]: loss 0.441459
[epoch12, step807]: loss 0.802049
[epoch12, step808]: loss 0.632245
[epoch12, step809]: loss 0.584393
[epoch12, step810]: loss 0.422435
[epoch12, step811]: loss 0.648654
[epoch12, step812]: loss 0.496032
[epoch12, step813]: loss 0.552031
[epoch12, step814]: loss 0.704616
[epoch12, step815]: loss 0.364346
[epoch12, step816]: loss 0.262229
[epoch12, step817]: loss 0.525864
[epoch12, step818]: loss 0.691293
[epoch12, step819]: loss 0.536029
[epoch12, step820]: loss 0.361932
[epoch12, step821]: loss 0.725414
[epoch12, step822]: loss 0.479435
[epoch12, step823]: loss 0.847149
[epoch12, step824]: loss 0.816073
[epoch12, step825]: loss 0.584724
[epoch12, step826]: loss 0.623901
[epoch12, step827]: loss 0.549502
[epoch12, step828]: loss 0.591109
[epoch12, step829]: loss 0.503186
[epoch12, step830]: loss 0.529147
[epoch12, step831]: loss 0.530491
[epoch12, step832]: loss 0.607087
[epoch12, step833]: loss 0.479824
[epoch12, step834]: loss 0.642576
[epoch12, step835]: loss 0.345448
[epoch12, step836]: loss 0.379279
[epoch12, step837]: loss 0.540512
[epoch12, step838]: loss 0.555746
[epoch12, step839]: loss 0.664336
[epoch12, step840]: loss 0.565231
[epoch12, step841]: loss 0.548029
[epoch12, step842]: loss 0.611430
[epoch12, step843]: loss 0.688093
[epoch12, step844]: loss 0.640158
[epoch12, step845]: loss 0.413606
[epoch12, step846]: loss 0.463338
[epoch12, step847]: loss 0.659400
[epoch12, step848]: loss 0.615351
[epoch12, step849]: loss 0.737777
[epoch12, step850]: loss 0.534539
[epoch12, step851]: loss 0.900352
[epoch12, step852]: loss 0.539111
[epoch12, step853]: loss 0.295753
[epoch12, step854]: loss 0.424089
[epoch12, step855]: loss 0.622897
[epoch12, step856]: loss 0.759743
[epoch12, step857]: loss 0.506313
[epoch12, step858]: loss 0.663023
[epoch12, step859]: loss 0.506908
[epoch12, step860]: loss 0.676606
[epoch12, step861]: loss 0.465150
[epoch12, step862]: loss 0.502409
[epoch12, step863]: loss 0.707257
[epoch12, step864]: loss 0.763026
[epoch12, step865]: loss 0.650212
[epoch12, step866]: loss 0.725625
[epoch12, step867]: loss 0.461069
[epoch12, step868]: loss 0.564415
[epoch12, step869]: loss 0.667316
[epoch12, step870]: loss 0.344142
[epoch12, step871]: loss 0.318107
[epoch12, step872]: loss 0.659574
[epoch12, step873]: loss 0.557952
[epoch12, step874]: loss 0.420704
[epoch12, step875]: loss 0.377722
[epoch12, step876]: loss 0.768328
[epoch12, step877]: loss 0.508410
[epoch12, step878]: loss 0.435989
[epoch12, step879]: loss 0.582153
[epoch12, step880]: loss 0.550431
[epoch12, step881]: loss 0.389273
[epoch12, step882]: loss 0.647437
[epoch12, step883]: loss 0.482550
[epoch12, step884]: loss 0.552625
[epoch12, step885]: loss 0.516623
[epoch12, step886]: loss 0.553220
[epoch12, step887]: loss 0.772445
[epoch12, step888]: loss 0.563608
[epoch12, step889]: loss 0.458021
[epoch12, step890]: loss 0.373854
[epoch12, step891]: loss 0.658926
[epoch12, step892]: loss 0.679600
[epoch12, step893]: loss 0.443120
[epoch12, step894]: loss 0.732141
[epoch12, step895]: loss 0.532742
[epoch12, step896]: loss 0.508411
[epoch12, step897]: loss 0.412636
[epoch12, step898]: loss 0.382876
[epoch12, step899]: loss 0.369823
[epoch12, step900]: loss 0.509760
[epoch12, step901]: loss 0.659376
[epoch12, step902]: loss 0.553432
[epoch12, step903]: loss 0.540860
[epoch12, step904]: loss 0.674006
[epoch12, step905]: loss 0.608631
[epoch12, step906]: loss 0.514266
[epoch12, step907]: loss 0.741287
[epoch12, step908]: loss 0.651022
[epoch12, step909]: loss 0.497957
[epoch12, step910]: loss 0.554268
[epoch12, step911]: loss 0.401740
[epoch12, step912]: loss 0.455341
[epoch12, step913]: loss 0.694163
[epoch12, step914]: loss 0.839366
[epoch12, step915]: loss 0.454633
[epoch12, step916]: loss 0.423959
[epoch12, step917]: loss 0.699293
[epoch12, step918]: loss 0.585930
[epoch12, step919]: loss 0.591548
[epoch12, step920]: loss 0.591730
[epoch12, step921]: loss 0.486233
[epoch12, step922]: loss 0.612096
[epoch12, step923]: loss 0.521872
[epoch12, step924]: loss 0.684430
[epoch12, step925]: loss 0.532372
[epoch12, step926]: loss 0.683863
[epoch12, step927]: loss 0.362022
[epoch12, step928]: loss 0.378044
[epoch12, step929]: loss 0.708401
[epoch12, step930]: loss 0.794529
[epoch12, step931]: loss 0.674397
[epoch12, step932]: loss 0.423990
[epoch12, step933]: loss 0.482418
[epoch12, step934]: loss 0.464181
[epoch12, step935]: loss 0.749959
[epoch12, step936]: loss 0.614586
[epoch12, step937]: loss 0.494894
[epoch12, step938]: loss 0.670088
[epoch12, step939]: loss 0.513452
[epoch12, step940]: loss 0.397389
[epoch12, step941]: loss 0.889786
[epoch12, step942]: loss 0.350077
[epoch12, step943]: loss 0.716449
[epoch12, step944]: loss 0.552469
[epoch12, step945]: loss 0.403177
[epoch12, step946]: loss 0.540526
[epoch12, step947]: loss 0.647920
[epoch12, step948]: loss 0.815180
[epoch12, step949]: loss 0.523903
[epoch12, step950]: loss 0.533885
[epoch12, step951]: loss 0.662268
[epoch12, step952]: loss 0.618868
[epoch12, step953]: loss 0.671859
[epoch12, step954]: loss 0.788232
[epoch12, step955]: loss 0.280313
[epoch12, step956]: loss 0.552171
[epoch12, step957]: loss 0.515473
[epoch12, step958]: loss 0.460312
[epoch12, step959]: loss 0.529718
[epoch12, step960]: loss 0.655952
[epoch12, step961]: loss 0.691313
[epoch12, step962]: loss 0.480233
[epoch12, step963]: loss 0.422575
[epoch12, step964]: loss 0.633404
[epoch12, step965]: loss 0.683967
[epoch12, step966]: loss 0.630283
[epoch12, step967]: loss 0.444847
[epoch12, step968]: loss 0.692616
[epoch12, step969]: loss 0.455050
[epoch12, step970]: loss 0.586706
[epoch12, step971]: loss 0.592191
[epoch12, step972]: loss 0.664998
[epoch12, step973]: loss 0.792648
[epoch12, step974]: loss 0.806689
[epoch12, step975]: loss 0.541502
[epoch12, step976]: loss 0.322715
[epoch12, step977]: loss 0.513243
[epoch12, step978]: loss 0.751020
[epoch12, step979]: loss 0.565015
[epoch12, step980]: loss 0.637201
[epoch12, step981]: loss 0.360069
[epoch12, step982]: loss 0.658099
[epoch12, step983]: loss 0.665017
[epoch12, step984]: loss 0.606566
[epoch12, step985]: loss 0.434972
[epoch12, step986]: loss 0.545936
[epoch12, step987]: loss 0.657866
[epoch12, step988]: loss 0.345550
[epoch12, step989]: loss 0.710448
[epoch12, step990]: loss 0.585194
[epoch12, step991]: loss 0.789624
[epoch12, step992]: loss 0.623183
[epoch12, step993]: loss 0.479706
[epoch12, step994]: loss 0.389257
[epoch12, step995]: loss 0.420301
[epoch12, step996]: loss 0.656732
[epoch12, step997]: loss 0.544879
[epoch12, step998]: loss 0.544764
[epoch12, step999]: loss 0.576444
[epoch12, step1000]: loss 0.626252
[epoch12, step1001]: loss 0.686545
[epoch12, step1002]: loss 0.518222
[epoch12, step1003]: loss 0.570927
[epoch12, step1004]: loss 0.863396
[epoch12, step1005]: loss 0.534591
[epoch12, step1006]: loss 0.858934
[epoch12, step1007]: loss 0.573999
[epoch12, step1008]: loss 0.621050
[epoch12, step1009]: loss 0.539500
[epoch12, step1010]: loss 0.635659
[epoch12, step1011]: loss 0.375726
[epoch12, step1012]: loss 0.366857
[epoch12, step1013]: loss 0.616955
[epoch12, step1014]: loss 0.209931
[epoch12, step1015]: loss 0.631601
[epoch12, step1016]: loss 0.620592
[epoch12, step1017]: loss 0.721604
[epoch12, step1018]: loss 0.542271
[epoch12, step1019]: loss 0.784432
[epoch12, step1020]: loss 0.667278
[epoch12, step1021]: loss 0.546792
[epoch12, step1022]: loss 0.707299
[epoch12, step1023]: loss 0.572635
[epoch12, step1024]: loss 0.701900
[epoch12, step1025]: loss 0.717302
[epoch12, step1026]: loss 0.684503
[epoch12, step1027]: loss 0.435259
[epoch12, step1028]: loss 0.623262
[epoch12, step1029]: loss 0.393880
[epoch12, step1030]: loss 0.562884
[epoch12, step1031]: loss 0.707036
[epoch12, step1032]: loss 0.670649
[epoch12, step1033]: loss 0.489671
[epoch12, step1034]: loss 0.492332
[epoch12, step1035]: loss 0.684506
[epoch12, step1036]: loss 0.445008
[epoch12, step1037]: loss 0.379799
[epoch12, step1038]: loss 0.620648
[epoch12, step1039]: loss 0.823660
[epoch12, step1040]: loss 0.574354
[epoch12, step1041]: loss 0.290844
[epoch12, step1042]: loss 0.318487
[epoch12, step1043]: loss 0.894844
[epoch12, step1044]: loss 0.763057
[epoch12, step1045]: loss 0.633508
[epoch12, step1046]: loss 0.713122
[epoch12, step1047]: loss 0.702592
[epoch12, step1048]: loss 0.707607
[epoch12, step1049]: loss 0.501874
[epoch12, step1050]: loss 0.757167
[epoch12, step1051]: loss 0.497424
[epoch12, step1052]: loss 0.572587
[epoch12, step1053]: loss 0.558918
[epoch12, step1054]: loss 0.466266
[epoch12, step1055]: loss 0.429311
[epoch12, step1056]: loss 0.582016
[epoch12, step1057]: loss 0.675557
[epoch12, step1058]: loss 0.520926
[epoch12, step1059]: loss 0.651565
[epoch12, step1060]: loss 0.549978
[epoch12, step1061]: loss 0.719352
[epoch12, step1062]: loss 0.707886
[epoch12, step1063]: loss 0.458492
[epoch12, step1064]: loss 0.721238
[epoch12, step1065]: loss 0.396880
[epoch12, step1066]: loss 0.482526
[epoch12, step1067]: loss 0.498385
[epoch12, step1068]: loss 0.523078
[epoch12, step1069]: loss 0.414423
[epoch12, step1070]: loss 0.438724
[epoch12, step1071]: loss 0.657666
[epoch12, step1072]: loss 0.586157
[epoch12, step1073]: loss 0.731883
[epoch12, step1074]: loss 0.531766
[epoch12, step1075]: loss 0.647532
[epoch12, step1076]: loss 0.632277
[epoch12, step1077]: loss 0.792345
[epoch12, step1078]: loss 0.568787
[epoch12, step1079]: loss 0.634275
[epoch12, step1080]: loss 0.902919
[epoch12, step1081]: loss 0.611690
[epoch12, step1082]: loss 0.728227
[epoch12, step1083]: loss 0.628605
[epoch12, step1084]: loss 0.231375
[epoch12, step1085]: loss 0.705046
[epoch12, step1086]: loss 0.616888
[epoch12, step1087]: loss 0.581682
[epoch12, step1088]: loss 0.557083
[epoch12, step1089]: loss 0.398617
[epoch12, step1090]: loss 0.799752
[epoch12, step1091]: loss 0.638037
[epoch12, step1092]: loss 0.338973
[epoch12, step1093]: loss 0.624866
[epoch12, step1094]: loss 0.528623
[epoch12, step1095]: loss 0.395858
[epoch12, step1096]: loss 0.681689
[epoch12, step1097]: loss 0.507621
[epoch12, step1098]: loss 0.417387
[epoch12, step1099]: loss 0.477731
[epoch12, step1100]: loss 0.468985
[epoch12, step1101]: loss 0.382859
[epoch12, step1102]: loss 0.619770
[epoch12, step1103]: loss 0.561787
[epoch12, step1104]: loss 0.555356
[epoch12, step1105]: loss 0.725854
[epoch12, step1106]: loss 0.648156
[epoch12, step1107]: loss 0.681970
[epoch12, step1108]: loss 0.657326
[epoch12, step1109]: loss 0.423860
[epoch12, step1110]: loss 0.669222
[epoch12, step1111]: loss 0.535443
[epoch12, step1112]: loss 0.593804
[epoch12, step1113]: loss 0.545915
[epoch12, step1114]: loss 0.650546
[epoch12, step1115]: loss 0.688906
[epoch12, step1116]: loss 0.575629
[epoch12, step1117]: loss 0.472677
[epoch12, step1118]: loss 0.586605
[epoch12, step1119]: loss 0.748029
[epoch12, step1120]: loss 0.627652
[epoch12, step1121]: loss 0.664137
[epoch12, step1122]: loss 0.598191
[epoch12, step1123]: loss 0.640563
[epoch12, step1124]: loss 0.472442
[epoch12, step1125]: loss 0.458050
[epoch12, step1126]: loss 0.613260
[epoch12, step1127]: loss 0.869048
[epoch12, step1128]: loss 0.393681
[epoch12, step1129]: loss 0.672095
[epoch12, step1130]: loss 0.623478
[epoch12, step1131]: loss 0.646788
[epoch12, step1132]: loss 0.763884
[epoch12, step1133]: loss 0.550547
[epoch12, step1134]: loss 0.735997
[epoch12, step1135]: loss 0.767501
[epoch12, step1136]: loss 0.293742
[epoch12, step1137]: loss 0.378734
[epoch12, step1138]: loss 0.349312
[epoch12, step1139]: loss 0.450320
[epoch12, step1140]: loss 0.531764
[epoch12, step1141]: loss 0.775304
[epoch12, step1142]: loss 0.762862
[epoch12, step1143]: loss 0.525434
[epoch12, step1144]: loss 0.514408
[epoch12, step1145]: loss 0.721283
[epoch12, step1146]: loss 0.659872
[epoch12, step1147]: loss 0.622671
[epoch12, step1148]: loss 0.589792
[epoch12, step1149]: loss 0.654814
[epoch12, step1150]: loss 0.583208
[epoch12, step1151]: loss 0.433244
[epoch12, step1152]: loss 0.533523
[epoch12, step1153]: loss 0.657857
[epoch12, step1154]: loss 0.597471
[epoch12, step1155]: loss 0.763249
[epoch12, step1156]: loss 0.599469
[epoch12, step1157]: loss 0.264111
[epoch12, step1158]: loss 0.671434
[epoch12, step1159]: loss 0.615993
[epoch12, step1160]: loss 0.742521
[epoch12, step1161]: loss 0.479166
[epoch12, step1162]: loss 0.478389
[epoch12, step1163]: loss 0.811469
[epoch12, step1164]: loss 0.669865
[epoch12, step1165]: loss 0.539443
[epoch12, step1166]: loss 0.689749
[epoch12, step1167]: loss 0.510249
[epoch12, step1168]: loss 0.500884
[epoch12, step1169]: loss 0.579197
[epoch12, step1170]: loss 0.790110
[epoch12, step1171]: loss 0.392667
[epoch12, step1172]: loss 0.790272
[epoch12, step1173]: loss 0.396558
[epoch12, step1174]: loss 0.494505
[epoch12, step1175]: loss 0.567119
[epoch12, step1176]: loss 0.708563
[epoch12, step1177]: loss 0.582131
[epoch12, step1178]: loss 0.570319
[epoch12, step1179]: loss 0.596143
[epoch12, step1180]: loss 0.317296
[epoch12, step1181]: loss 0.628049
[epoch12, step1182]: loss 0.510793
[epoch12, step1183]: loss 0.807862
[epoch12, step1184]: loss 0.601085
[epoch12, step1185]: loss 0.475640
[epoch12, step1186]: loss 0.653700
[epoch12, step1187]: loss 0.330785
[epoch12, step1188]: loss 0.628999
[epoch12, step1189]: loss 0.710262
[epoch12, step1190]: loss 0.568862
[epoch12, step1191]: loss 0.408146
[epoch12, step1192]: loss 0.546582
[epoch12, step1193]: loss 0.435137
[epoch12, step1194]: loss 0.396286
[epoch12, step1195]: loss 0.577941
[epoch12, step1196]: loss 0.620617
[epoch12, step1197]: loss 0.639956
[epoch12, step1198]: loss 0.648996
[epoch12, step1199]: loss 0.753981
[epoch12, step1200]: loss 0.769610
[epoch12, step1201]: loss 0.797922
[epoch12, step1202]: loss 0.646315
[epoch12, step1203]: loss 0.234260
[epoch12, step1204]: loss 0.680464
[epoch12, step1205]: loss 0.612793
[epoch12, step1206]: loss 0.657666
[epoch12, step1207]: loss 0.533413
[epoch12, step1208]: loss 0.435125
[epoch12, step1209]: loss 0.487278
[epoch12, step1210]: loss 0.749544
[epoch12, step1211]: loss 0.686632
[epoch12, step1212]: loss 0.452272
[epoch12, step1213]: loss 0.426279
[epoch12, step1214]: loss 0.698968
[epoch12, step1215]: loss 0.561184
[epoch12, step1216]: loss 0.385611
[epoch12, step1217]: loss 0.566811
[epoch12, step1218]: loss 0.590173
[epoch12, step1219]: loss 0.563668
[epoch12, step1220]: loss 0.693457
[epoch12, step1221]: loss 0.790926
[epoch12, step1222]: loss 0.858352
[epoch12, step1223]: loss 0.623211
[epoch12, step1224]: loss 0.580646
[epoch12, step1225]: loss 0.293151
[epoch12, step1226]: loss 0.427198
[epoch12, step1227]: loss 0.826435
[epoch12, step1228]: loss 0.464413
[epoch12, step1229]: loss 0.263364
[epoch12, step1230]: loss 0.662418
[epoch12, step1231]: loss 0.668344
[epoch12, step1232]: loss 0.509724
[epoch12, step1233]: loss 0.997447
[epoch12, step1234]: loss 0.335618
[epoch12, step1235]: loss 0.484267
[epoch12, step1236]: loss 0.623063
[epoch12, step1237]: loss 0.417819
[epoch12, step1238]: loss 0.841869
[epoch12, step1239]: loss 0.295608
[epoch12, step1240]: loss 0.467835
[epoch12, step1241]: loss 0.468759
[epoch12, step1242]: loss 0.808192
[epoch12, step1243]: loss 0.446582
[epoch12, step1244]: loss 0.780629
[epoch12, step1245]: loss 0.716630
[epoch12, step1246]: loss 0.499138
[epoch12, step1247]: loss 0.741998
[epoch12, step1248]: loss 0.394723
[epoch12, step1249]: loss 0.682091
[epoch12, step1250]: loss 0.499471
[epoch12, step1251]: loss 0.361483
[epoch12, step1252]: loss 0.697492
[epoch12, step1253]: loss 0.814370
[epoch12, step1254]: loss 0.441602
[epoch12, step1255]: loss 0.288422
[epoch12, step1256]: loss 0.479950
[epoch12, step1257]: loss 0.607034
[epoch12, step1258]: loss 0.685357
[epoch12, step1259]: loss 0.747189
[epoch12, step1260]: loss 0.587825
[epoch12, step1261]: loss 0.720331
[epoch12, step1262]: loss 0.759102
[epoch12, step1263]: loss 0.401693
[epoch12, step1264]: loss 0.752221
[epoch12, step1265]: loss 0.590313
[epoch12, step1266]: loss 0.696562
[epoch12, step1267]: loss 0.695988
[epoch12, step1268]: loss 0.536247
[epoch12, step1269]: loss 0.254538
[epoch12, step1270]: loss 0.759820
[epoch12, step1271]: loss 0.532007
[epoch12, step1272]: loss 0.541042
[epoch12, step1273]: loss 0.499612
[epoch12, step1274]: loss 0.343823
[epoch12, step1275]: loss 0.474514
[epoch12, step1276]: loss 0.733219
[epoch12, step1277]: loss 0.453748
[epoch12, step1278]: loss 0.347344
[epoch12, step1279]: loss 0.564813
[epoch12, step1280]: loss 0.520711
[epoch12, step1281]: loss 0.810418
[epoch12, step1282]: loss 0.603352
[epoch12, step1283]: loss 0.604917
[epoch12, step1284]: loss 0.617593
[epoch12, step1285]: loss 0.572921
[epoch12, step1286]: loss 0.377232
[epoch12, step1287]: loss 0.530922
[epoch12, step1288]: loss 0.396557
[epoch12, step1289]: loss 0.585579
[epoch12, step1290]: loss 0.601321
[epoch12, step1291]: loss 0.515520
[epoch12, step1292]: loss 0.552962
[epoch12, step1293]: loss 0.381159
[epoch12, step1294]: loss 0.397015
[epoch12, step1295]: loss 0.595842
[epoch12, step1296]: loss 0.571118
[epoch12, step1297]: loss 0.566156
[epoch12, step1298]: loss 0.307182
[epoch12, step1299]: loss 0.837914
[epoch12, step1300]: loss 0.465743
[epoch12, step1301]: loss 0.585496
[epoch12, step1302]: loss 0.493310
[epoch12, step1303]: loss 0.375559
[epoch12, step1304]: loss 0.639387
[epoch12, step1305]: loss 0.359213
[epoch12, step1306]: loss 0.740742
[epoch12, step1307]: loss 0.540556
[epoch12, step1308]: loss 0.661504
[epoch12, step1309]: loss 0.528080
[epoch12, step1310]: loss 0.841154
[epoch12, step1311]: loss 0.624135
[epoch12, step1312]: loss 0.654396
[epoch12, step1313]: loss 0.662473
[epoch12, step1314]: loss 0.748300
[epoch12, step1315]: loss 0.893746
[epoch12, step1316]: loss 0.618404
[epoch12, step1317]: loss 0.646359
[epoch12, step1318]: loss 0.233369
[epoch12, step1319]: loss 0.732103
[epoch12, step1320]: loss 0.407354
[epoch12, step1321]: loss 0.716140
[epoch12, step1322]: loss 0.562258
[epoch12, step1323]: loss 0.599303
[epoch12, step1324]: loss 0.401997
[epoch12, step1325]: loss 0.401714
[epoch12, step1326]: loss 0.401294
[epoch12, step1327]: loss 0.384350
[epoch12, step1328]: loss 0.672344
[epoch12, step1329]: loss 0.604662
[epoch12, step1330]: loss 0.349963
[epoch12, step1331]: loss 0.448497
[epoch12, step1332]: loss 0.595191
[epoch12, step1333]: loss 0.597469
[epoch12, step1334]: loss 0.505682
[epoch12, step1335]: loss 0.701148
[epoch12, step1336]: loss 0.435925
[epoch12, step1337]: loss 0.216542
[epoch12, step1338]: loss 0.264676
[epoch12, step1339]: loss 0.406673
[epoch12, step1340]: loss 0.322894
[epoch12, step1341]: loss 0.259536
[epoch12, step1342]: loss 0.478657
[epoch12, step1343]: loss 0.629095
[epoch12, step1344]: loss 0.484840
[epoch12, step1345]: loss 0.701765
[epoch12, step1346]: loss 0.345732
[epoch12, step1347]: loss 0.650096
[epoch12, step1348]: loss 0.570107
[epoch12, step1349]: loss 0.639346
[epoch12, step1350]: loss 0.526910
[epoch12, step1351]: loss 0.508775
[epoch12, step1352]: loss 0.710429
[epoch12, step1353]: loss 0.517310
[epoch12, step1354]: loss 0.571612
[epoch12, step1355]: loss 0.637063
[epoch12, step1356]: loss 0.635596
[epoch12, step1357]: loss 0.631872
[epoch12, step1358]: loss 0.419321
[epoch12, step1359]: loss 0.464627
[epoch12, step1360]: loss 0.506506
[epoch12, step1361]: loss 0.568115
[epoch12, step1362]: loss 0.669449
[epoch12, step1363]: loss 0.426523
[epoch12, step1364]: loss 0.465797
[epoch12, step1365]: loss 0.704448
[epoch12, step1366]: loss 0.832563
[epoch12, step1367]: loss 0.635763
[epoch12, step1368]: loss 0.508775
[epoch12, step1369]: loss 0.763927
[epoch12, step1370]: loss 0.514977
[epoch12, step1371]: loss 0.559250
[epoch12, step1372]: loss 0.510951
[epoch12, step1373]: loss 0.632352
[epoch12, step1374]: loss 0.457013
[epoch12, step1375]: loss 0.613835
[epoch12, step1376]: loss 0.634699
[epoch12, step1377]: loss 0.516091
[epoch12, step1378]: loss 0.705565
[epoch12, step1379]: loss 0.427590
[epoch12, step1380]: loss 0.656780
[epoch12, step1381]: loss 0.527128
[epoch12, step1382]: loss 0.610215
[epoch12, step1383]: loss 0.615879
[epoch12, step1384]: loss 0.303234
[epoch12, step1385]: loss 0.400211
[epoch12, step1386]: loss 0.764956
[epoch12, step1387]: loss 0.521881
[epoch12, step1388]: loss 0.655436
[epoch12, step1389]: loss 0.510908
[epoch12, step1390]: loss 0.465423
[epoch12, step1391]: loss 0.578111
[epoch12, step1392]: loss 0.426687
[epoch12, step1393]: loss 0.912734
[epoch12, step1394]: loss 0.409076
[epoch12, step1395]: loss 0.335253
[epoch12, step1396]: loss 0.404678
[epoch12, step1397]: loss 0.535994
[epoch12, step1398]: loss 0.522088
[epoch12, step1399]: loss 0.352168
[epoch12, step1400]: loss 0.315528
[epoch12, step1401]: loss 0.652206
[epoch12, step1402]: loss 0.614701
[epoch12, step1403]: loss 0.550345
[epoch12, step1404]: loss 0.641286
[epoch12, step1405]: loss 0.614930
[epoch12, step1406]: loss 0.648986
[epoch12, step1407]: loss 0.546114
[epoch12, step1408]: loss 0.623389
[epoch12, step1409]: loss 0.376094
[epoch12, step1410]: loss 0.699636
[epoch12, step1411]: loss 0.412605
[epoch12, step1412]: loss 0.537199
[epoch12, step1413]: loss 0.611312
[epoch12, step1414]: loss 0.654254
[epoch12, step1415]: loss 0.483055
[epoch12, step1416]: loss 0.483993
[epoch12, step1417]: loss 0.620940
[epoch12, step1418]: loss 0.282999
[epoch12, step1419]: loss 0.246893
[epoch12, step1420]: loss 0.469256
[epoch12, step1421]: loss 0.611750
[epoch12, step1422]: loss 0.681287
[epoch12, step1423]: loss 0.446057
[epoch12, step1424]: loss 0.528503
[epoch12, step1425]: loss 0.517329
[epoch12, step1426]: loss 0.822270
[epoch12, step1427]: loss 0.465405
[epoch12, step1428]: loss 0.628388
[epoch12, step1429]: loss 0.494127
[epoch12, step1430]: loss 0.575857
[epoch12, step1431]: loss 0.625052
[epoch12, step1432]: loss 0.667052
[epoch12, step1433]: loss 0.698690
[epoch12, step1434]: loss 0.591989
[epoch12, step1435]: loss 0.681117
[epoch12, step1436]: loss 0.337028
[epoch12, step1437]: loss 0.551952
[epoch12, step1438]: loss 0.601564
[epoch12, step1439]: loss 0.561258
[epoch12, step1440]: loss 0.232480
[epoch12, step1441]: loss 0.517576
[epoch12, step1442]: loss 0.575238
[epoch12, step1443]: loss 0.641456
[epoch12, step1444]: loss 0.652485
[epoch12, step1445]: loss 0.589067
[epoch12, step1446]: loss 0.607322
[epoch12, step1447]: loss 0.575201
[epoch12, step1448]: loss 0.782201
[epoch12, step1449]: loss 0.414741
[epoch12, step1450]: loss 0.831583
[epoch12, step1451]: loss 0.671948
[epoch12, step1452]: loss 0.614968
[epoch12, step1453]: loss 0.591259
[epoch12, step1454]: loss 0.658922
[epoch12, step1455]: loss 0.754271
[epoch12, step1456]: loss 0.509871
[epoch12, step1457]: loss 0.640777
[epoch12, step1458]: loss 0.421931
[epoch12, step1459]: loss 0.606488
[epoch12, step1460]: loss 0.787359
[epoch12, step1461]: loss 0.426647
[epoch12, step1462]: loss 0.551929
[epoch12, step1463]: loss 0.783618
[epoch12, step1464]: loss 0.780124
[epoch12, step1465]: loss 0.633871
[epoch12, step1466]: loss 0.617250
[epoch12, step1467]: loss 0.765100
[epoch12, step1468]: loss 0.658615
[epoch12, step1469]: loss 0.525701
[epoch12, step1470]: loss 0.521419
[epoch12, step1471]: loss 0.542876
[epoch12, step1472]: loss 0.575740
[epoch12, step1473]: loss 0.711087
[epoch12, step1474]: loss 0.737181
[epoch12, step1475]: loss 0.635077
[epoch12, step1476]: loss 0.691600
[epoch12, step1477]: loss 0.492736
[epoch12, step1478]: loss 0.318999
[epoch12, step1479]: loss 0.670852
[epoch12, step1480]: loss 0.498145
[epoch12, step1481]: loss 0.553532
[epoch12, step1482]: loss 0.519180
[epoch12, step1483]: loss 0.422357
[epoch12, step1484]: loss 0.366535
[epoch12, step1485]: loss 0.643815
[epoch12, step1486]: loss 0.459738
[epoch12, step1487]: loss 0.397151
[epoch12, step1488]: loss 0.720863
[epoch12, step1489]: loss 0.352836
[epoch12, step1490]: loss 0.427600
[epoch12, step1491]: loss 0.542685
[epoch12, step1492]: loss 0.287228
[epoch12, step1493]: loss 0.464414
[epoch12, step1494]: loss 0.598061
[epoch12, step1495]: loss 0.610438
[epoch12, step1496]: loss 0.667497
[epoch12, step1497]: loss 0.490208
[epoch12, step1498]: loss 0.690966
[epoch12, step1499]: loss 0.668747
[epoch12, step1500]: loss 0.706043
[epoch12, step1501]: loss 0.436902
[epoch12, step1502]: loss 0.581937
[epoch12, step1503]: loss 0.457004
[epoch12, step1504]: loss 0.407722
[epoch12, step1505]: loss 0.664174
[epoch12, step1506]: loss 0.610482
[epoch12, step1507]: loss 0.537331
[epoch12, step1508]: loss 0.641398
[epoch12, step1509]: loss 0.742447
[epoch12, step1510]: loss 0.315876
[epoch12, step1511]: loss 0.621445
[epoch12, step1512]: loss 0.714392
[epoch12, step1513]: loss 0.543936
[epoch12, step1514]: loss 0.268473
[epoch12, step1515]: loss 0.458337
[epoch12, step1516]: loss 0.491530
[epoch12, step1517]: loss 0.640109
[epoch12, step1518]: loss 0.478334
[epoch12, step1519]: loss 0.560302
[epoch12, step1520]: loss 0.607547
[epoch12, step1521]: loss 0.825220
[epoch12, step1522]: loss 0.435760
[epoch12, step1523]: loss 0.487252
[epoch12, step1524]: loss 0.792180
[epoch12, step1525]: loss 0.670862
[epoch12, step1526]: loss 0.568784
[epoch12, step1527]: loss 0.810282
[epoch12, step1528]: loss 0.599504
[epoch12, step1529]: loss 0.497790
[epoch12, step1530]: loss 0.551782
[epoch12, step1531]: loss 0.464864
[epoch12, step1532]: loss 0.623164
[epoch12, step1533]: loss 0.666773
[epoch12, step1534]: loss 0.495081
[epoch12, step1535]: loss 0.460395
[epoch12, step1536]: loss 0.395248
[epoch12, step1537]: loss 0.562404
[epoch12, step1538]: loss 0.415645
[epoch12, step1539]: loss 0.426847
[epoch12, step1540]: loss 0.431383
[epoch12, step1541]: loss 0.762084
[epoch12, step1542]: loss 0.495818
[epoch12, step1543]: loss 0.628243
[epoch12, step1544]: loss 0.312340
[epoch12, step1545]: loss 0.461866
[epoch12, step1546]: loss 0.660942
[epoch12, step1547]: loss 0.481029
[epoch12, step1548]: loss 0.530862
[epoch12, step1549]: loss 0.439973
[epoch12, step1550]: loss 0.713403
[epoch12, step1551]: loss 0.685153
[epoch12, step1552]: loss 0.383965
[epoch12, step1553]: loss 0.409089
[epoch12, step1554]: loss 0.501314
[epoch12, step1555]: loss 0.460978
[epoch12, step1556]: loss 0.580333
[epoch12, step1557]: loss 0.733692
[epoch12, step1558]: loss 0.314548
[epoch12, step1559]: loss 0.503108
[epoch12, step1560]: loss 0.452295
[epoch12, step1561]: loss 0.188069
[epoch12, step1562]: loss 0.534219
[epoch12, step1563]: loss 0.751458
[epoch12, step1564]: loss 0.565284
[epoch12, step1565]: loss 0.601567
[epoch12, step1566]: loss 0.608112
[epoch12, step1567]: loss 0.816374
[epoch12, step1568]: loss 0.245372
[epoch12, step1569]: loss 0.661444
[epoch12, step1570]: loss 0.353593
[epoch12, step1571]: loss 0.798154
[epoch12, step1572]: loss 0.463102
[epoch12, step1573]: loss 0.614393
[epoch12, step1574]: loss 0.609960
[epoch12, step1575]: loss 0.610642
[epoch12, step1576]: loss 0.757401
[epoch12, step1577]: loss 0.546995
[epoch12, step1578]: loss 0.375736
[epoch12, step1579]: loss 0.634036
[epoch12, step1580]: loss 0.639590
[epoch12, step1581]: loss 0.711642
[epoch12, step1582]: loss 0.421482
[epoch12, step1583]: loss 0.195979
[epoch12, step1584]: loss 0.706887
[epoch12, step1585]: loss 0.589352
[epoch12, step1586]: loss 0.494090
[epoch12, step1587]: loss 0.442697
[epoch12, step1588]: loss 0.474753
[epoch12, step1589]: loss 0.445535
[epoch12, step1590]: loss 0.638283
[epoch12, step1591]: loss 0.563916
[epoch12, step1592]: loss 0.707081
[epoch12, step1593]: loss 0.698821
[epoch12, step1594]: loss 0.676204
[epoch12, step1595]: loss 0.402674
[epoch12, step1596]: loss 0.804273
[epoch12, step1597]: loss 0.529237
[epoch12, step1598]: loss 0.548129
[epoch12, step1599]: loss 0.712744
[epoch12, step1600]: loss 0.449313
[epoch12, step1601]: loss 0.808048
[epoch12, step1602]: loss 0.680901
[epoch12, step1603]: loss 0.291953
[epoch12, step1604]: loss 0.416204
[epoch12, step1605]: loss 0.646525
[epoch12, step1606]: loss 0.497219
[epoch12, step1607]: loss 0.357930
[epoch12, step1608]: loss 0.512269
[epoch12, step1609]: loss 0.371473
[epoch12, step1610]: loss 0.681881
[epoch12, step1611]: loss 0.493378
[epoch12, step1612]: loss 0.692464
[epoch12, step1613]: loss 0.666754
[epoch12, step1614]: loss 0.466312
[epoch12, step1615]: loss 0.576342
[epoch12, step1616]: loss 0.683327
[epoch12, step1617]: loss 0.671547
[epoch12, step1618]: loss 0.566013
[epoch12, step1619]: loss 0.671289
[epoch12, step1620]: loss 0.490448
[epoch12, step1621]: loss 0.817854
[epoch12, step1622]: loss 0.318088
[epoch12, step1623]: loss 0.513536
[epoch12, step1624]: loss 0.424312
[epoch12, step1625]: loss 0.499785
[epoch12, step1626]: loss 0.508680
[epoch12, step1627]: loss 0.623692
[epoch12, step1628]: loss 0.609824
[epoch12, step1629]: loss 0.559866
[epoch12, step1630]: loss 0.750567
[epoch12, step1631]: loss 0.527354
[epoch12, step1632]: loss 0.821005
[epoch12, step1633]: loss 0.638118
[epoch12, step1634]: loss 0.574614
[epoch12, step1635]: loss 0.736680
[epoch12, step1636]: loss 0.533579
[epoch12, step1637]: loss 0.678805
[epoch12, step1638]: loss 0.569893
[epoch12, step1639]: loss 0.571674
[epoch12, step1640]: loss 0.420242
[epoch12, step1641]: loss 0.783519
[epoch12, step1642]: loss 0.753425
[epoch12, step1643]: loss 0.600742
[epoch12, step1644]: loss 0.358306
[epoch12, step1645]: loss 0.478324
[epoch12, step1646]: loss 0.615196
[epoch12, step1647]: loss 0.774241
[epoch12, step1648]: loss 0.639990
[epoch12, step1649]: loss 0.322351
[epoch12, step1650]: loss 0.570085
[epoch12, step1651]: loss 0.354618
[epoch12, step1652]: loss 0.804938
[epoch12, step1653]: loss 0.556307
[epoch12, step1654]: loss 0.553733
[epoch12, step1655]: loss 0.449196
[epoch12, step1656]: loss 0.273267
[epoch12, step1657]: loss 0.525629
[epoch12, step1658]: loss 0.515112
[epoch12, step1659]: loss 0.704961
[epoch12, step1660]: loss 0.554974
[epoch12, step1661]: loss 0.365599
[epoch12, step1662]: loss 0.598707
[epoch12, step1663]: loss 0.749337
[epoch12, step1664]: loss 0.627931
[epoch12, step1665]: loss 0.588644
[epoch12, step1666]: loss 0.499572
[epoch12, step1667]: loss 0.640428
[epoch12, step1668]: loss 0.354833
[epoch12, step1669]: loss 0.843007
[epoch12, step1670]: loss 0.619772
[epoch12, step1671]: loss 0.757446
[epoch12, step1672]: loss 0.279768
[epoch12, step1673]: loss 0.563525
[epoch12, step1674]: loss 0.581846
[epoch12, step1675]: loss 0.639715
[epoch12, step1676]: loss 0.471618
[epoch12, step1677]: loss 0.589455
[epoch12, step1678]: loss 0.421187
[epoch12, step1679]: loss 0.553406
[epoch12, step1680]: loss 0.627887
[epoch12, step1681]: loss 0.358751
[epoch12, step1682]: loss 0.711706
[epoch12, step1683]: loss 0.646208
[epoch12, step1684]: loss 0.619342
[epoch12, step1685]: loss 0.677738
[epoch12, step1686]: loss 0.843299
[epoch12, step1687]: loss 0.524427
[epoch12, step1688]: loss 0.897076
[epoch12, step1689]: loss 0.582236
[epoch12, step1690]: loss 0.727707
[epoch12, step1691]: loss 0.350644
[epoch12, step1692]: loss 0.592749
[epoch12, step1693]: loss 0.539500
[epoch12, step1694]: loss 0.801448
[epoch12, step1695]: loss 0.476693
[epoch12, step1696]: loss 0.759924
[epoch12, step1697]: loss 0.483089
[epoch12, step1698]: loss 0.537692
[epoch12, step1699]: loss 0.581278
[epoch12, step1700]: loss 0.709597
[epoch12, step1701]: loss 0.460708
[epoch12, step1702]: loss 0.684963
[epoch12, step1703]: loss 0.524632
[epoch12, step1704]: loss 0.613072
[epoch12, step1705]: loss 0.641633
[epoch12, step1706]: loss 0.571634
[epoch12, step1707]: loss 0.443169
[epoch12, step1708]: loss 0.489305
[epoch12, step1709]: loss 0.847591
[epoch12, step1710]: loss 0.470968
[epoch12, step1711]: loss 0.566502
[epoch12, step1712]: loss 0.644632
[epoch12, step1713]: loss 0.453381
[epoch12, step1714]: loss 0.373860
[epoch12, step1715]: loss 0.428445
[epoch12, step1716]: loss 0.497536
[epoch12, step1717]: loss 0.714165
[epoch12, step1718]: loss 0.573820
[epoch12, step1719]: loss 0.463901
[epoch12, step1720]: loss 0.600451
[epoch12, step1721]: loss 0.383516
[epoch12, step1722]: loss 0.353208
[epoch12, step1723]: loss 0.533236
[epoch12, step1724]: loss 0.440618
[epoch12, step1725]: loss 0.447839
[epoch12, step1726]: loss 0.137579
[epoch12, step1727]: loss 0.426872
[epoch12, step1728]: loss 0.583828
[epoch12, step1729]: loss 0.595304
[epoch12, step1730]: loss 0.578594
[epoch12, step1731]: loss 0.441633
[epoch12, step1732]: loss 0.743113
[epoch12, step1733]: loss 0.620264
[epoch12, step1734]: loss 0.819323
[epoch12, step1735]: loss 0.519836
[epoch12, step1736]: loss 0.226249
[epoch12, step1737]: loss 0.557692
[epoch12, step1738]: loss 0.629344
[epoch12, step1739]: loss 0.463176
[epoch12, step1740]: loss 0.818765
[epoch12, step1741]: loss 0.467667
[epoch12, step1742]: loss 0.470799
[epoch12, step1743]: loss 0.663083
[epoch12, step1744]: loss 0.570231
[epoch12, step1745]: loss 0.646000
[epoch12, step1746]: loss 0.676590
[epoch12, step1747]: loss 0.617862
[epoch12, step1748]: loss 0.496807
[epoch12, step1749]: loss 0.745121
[epoch12, step1750]: loss 0.736707
[epoch12, step1751]: loss 0.540617
[epoch12, step1752]: loss 0.492746
[epoch12, step1753]: loss 0.632013
[epoch12, step1754]: loss 0.667142
[epoch12, step1755]: loss 0.888171
[epoch12, step1756]: loss 0.521450
[epoch12, step1757]: loss 0.614289
[epoch12, step1758]: loss 0.545299
[epoch12, step1759]: loss 0.579969
[epoch12, step1760]: loss 0.383544
[epoch12, step1761]: loss 0.154733
[epoch12, step1762]: loss 0.366023
[epoch12, step1763]: loss 0.623547
[epoch12, step1764]: loss 0.532849
[epoch12, step1765]: loss 0.391061
[epoch12, step1766]: loss 0.565811
[epoch12, step1767]: loss 0.586495
[epoch12, step1768]: loss 0.822906
[epoch12, step1769]: loss 0.449199
[epoch12, step1770]: loss 0.509869
[epoch12, step1771]: loss 0.798140
[epoch12, step1772]: loss 0.429331
[epoch12, step1773]: loss 0.613496
[epoch12, step1774]: loss 0.499526
[epoch12, step1775]: loss 0.668118
[epoch12, step1776]: loss 0.341870
[epoch12, step1777]: loss 0.586268
[epoch12, step1778]: loss 0.477692
[epoch12, step1779]: loss 0.674775
[epoch12, step1780]: loss 0.640666
[epoch12, step1781]: loss 0.581664
[epoch12, step1782]: loss 0.715362
[epoch12, step1783]: loss 0.344885
[epoch12, step1784]: loss 0.483246
[epoch12, step1785]: loss 0.527361
[epoch12, step1786]: loss 0.448500
[epoch12, step1787]: loss 0.676927
[epoch12, step1788]: loss 0.473761
[epoch12, step1789]: loss 0.572938
[epoch12, step1790]: loss 0.766757
[epoch12, step1791]: loss 0.469713
[epoch12, step1792]: loss 0.620210
[epoch12, step1793]: loss 0.477132
[epoch12, step1794]: loss 0.472636
[epoch12, step1795]: loss 0.661191
[epoch12, step1796]: loss 0.415695
[epoch12, step1797]: loss 0.731082
[epoch12, step1798]: loss 0.683065
[epoch12, step1799]: loss 0.392632
[epoch12, step1800]: loss 0.597963
[epoch12, step1801]: loss 0.569041
[epoch12, step1802]: loss 0.528067
[epoch12, step1803]: loss 0.646931
[epoch12, step1804]: loss 0.487583
[epoch12, step1805]: loss 0.694568
[epoch12, step1806]: loss 0.576583
[epoch12, step1807]: loss 0.400931
[epoch12, step1808]: loss 0.679101
[epoch12, step1809]: loss 0.385052
[epoch12, step1810]: loss 0.560358
[epoch12, step1811]: loss 0.718869
[epoch12, step1812]: loss 0.581084
[epoch12, step1813]: loss 0.723459
[epoch12, step1814]: loss 0.530116
[epoch12, step1815]: loss 0.535436
[epoch12, step1816]: loss 0.480475
[epoch12, step1817]: loss 0.353017
[epoch12, step1818]: loss 0.330117
[epoch12, step1819]: loss 0.509002
[epoch12, step1820]: loss 0.390102
[epoch12, step1821]: loss 0.508811
[epoch12, step1822]: loss 0.622836
[epoch12, step1823]: loss 0.302505
[epoch12, step1824]: loss 0.646005
[epoch12, step1825]: loss 0.787190
[epoch12, step1826]: loss 0.432246
[epoch12, step1827]: loss 0.473064
[epoch12, step1828]: loss 0.531682
[epoch12, step1829]: loss 0.622337
[epoch12, step1830]: loss 0.467287
[epoch12, step1831]: loss 0.366848
[epoch12, step1832]: loss 0.459728
[epoch12, step1833]: loss 0.744277
[epoch12, step1834]: loss 0.552960
[epoch12, step1835]: loss 0.822821
[epoch12, step1836]: loss 0.484154
[epoch12, step1837]: loss 0.347334
[epoch12, step1838]: loss 0.686563
[epoch12, step1839]: loss 0.786431
[epoch12, step1840]: loss 0.564766
[epoch12, step1841]: loss 0.630496
[epoch12, step1842]: loss 0.839238
[epoch12, step1843]: loss 0.646438
[epoch12, step1844]: loss 0.319728
[epoch12, step1845]: loss 0.560026
[epoch12, step1846]: loss 0.606614
[epoch12, step1847]: loss 0.565115
[epoch12, step1848]: loss 0.478925
[epoch12, step1849]: loss 0.292637
[epoch12, step1850]: loss 0.811107
[epoch12, step1851]: loss 0.916929
[epoch12, step1852]: loss 0.453365
[epoch12, step1853]: loss 0.645410
[epoch12, step1854]: loss 0.518826
[epoch12, step1855]: loss 0.543221
[epoch12, step1856]: loss 0.537201
[epoch12, step1857]: loss 0.648202
[epoch12, step1858]: loss 0.371292
[epoch12, step1859]: loss 0.617757
[epoch12, step1860]: loss 0.187032
[epoch12, step1861]: loss 0.413047
[epoch12, step1862]: loss 0.665693
[epoch12, step1863]: loss 0.446016
[epoch12, step1864]: loss 0.621587
[epoch12, step1865]: loss 0.639914
[epoch12, step1866]: loss 0.717545
[epoch12, step1867]: loss 0.503435
[epoch12, step1868]: loss 0.757364
[epoch12, step1869]: loss 0.447686
[epoch12, step1870]: loss 0.469533
[epoch12, step1871]: loss 0.741604
[epoch12, step1872]: loss 0.731623
[epoch12, step1873]: loss 0.392413
[epoch12, step1874]: loss 0.610252
[epoch12, step1875]: loss 0.394350
[epoch12, step1876]: loss 0.556026
[epoch12, step1877]: loss 0.779034
[epoch12, step1878]: loss 0.539233
[epoch12, step1879]: loss 0.528515
[epoch12, step1880]: loss 0.764223
[epoch12, step1881]: loss 0.445819
[epoch12, step1882]: loss 0.670785
[epoch12, step1883]: loss 0.399020
[epoch12, step1884]: loss 0.625966
[epoch12, step1885]: loss 0.549400
[epoch12, step1886]: loss 0.492667
[epoch12, step1887]: loss 0.429730
[epoch12, step1888]: loss 0.525328
[epoch12, step1889]: loss 0.468635
[epoch12, step1890]: loss 0.625464
[epoch12, step1891]: loss 0.335949
[epoch12, step1892]: loss 0.472147
[epoch12, step1893]: loss 0.513913
[epoch12, step1894]: loss 0.331035
[epoch12, step1895]: loss 0.447075
[epoch12, step1896]: loss 0.564507
[epoch12, step1897]: loss 0.640066
[epoch12, step1898]: loss 0.628140
[epoch12, step1899]: loss 0.466744
[epoch12, step1900]: loss 0.597908
[epoch12, step1901]: loss 0.570345
[epoch12, step1902]: loss 0.499226
[epoch12, step1903]: loss 0.566346
[epoch12, step1904]: loss 0.813742
[epoch12, step1905]: loss 0.634229
[epoch12, step1906]: loss 0.536396
[epoch12, step1907]: loss 0.489515
[epoch12, step1908]: loss 0.701497
[epoch12, step1909]: loss 0.605818
[epoch12, step1910]: loss 0.290857
[epoch12, step1911]: loss 0.617915
[epoch12, step1912]: loss 0.570235
[epoch12, step1913]: loss 0.591056
[epoch12, step1914]: loss 0.771454
[epoch12, step1915]: loss 0.792192
[epoch12, step1916]: loss 0.683837
[epoch12, step1917]: loss 0.466142
[epoch12, step1918]: loss 0.425555
[epoch12, step1919]: loss 0.529523
[epoch12, step1920]: loss 0.576813
[epoch12, step1921]: loss 0.620829
[epoch12, step1922]: loss 0.497719
[epoch12, step1923]: loss 0.643014
[epoch12, step1924]: loss 0.565622
[epoch12, step1925]: loss 0.806246
[epoch12, step1926]: loss 0.792798
[epoch12, step1927]: loss 0.634400
[epoch12, step1928]: loss 0.439225
[epoch12, step1929]: loss 0.186208
[epoch12, step1930]: loss 0.487277
[epoch12, step1931]: loss 0.652390
[epoch12, step1932]: loss 0.522852
[epoch12, step1933]: loss 0.484761
[epoch12, step1934]: loss 0.641401
[epoch12, step1935]: loss 0.276890
[epoch12, step1936]: loss 0.597287
[epoch12, step1937]: loss 0.541376
[epoch12, step1938]: loss 0.636198
[epoch12, step1939]: loss 0.493909
[epoch12, step1940]: loss 0.741257
[epoch12, step1941]: loss 0.593679
[epoch12, step1942]: loss 0.866131
[epoch12, step1943]: loss 0.768851
[epoch12, step1944]: loss 0.664809
[epoch12, step1945]: loss 0.444501
[epoch12, step1946]: loss 0.597393
[epoch12, step1947]: loss 0.535584
[epoch12, step1948]: loss 0.646593
[epoch12, step1949]: loss 0.605481
[epoch12, step1950]: loss 0.741501
[epoch12, step1951]: loss 0.642334
[epoch12, step1952]: loss 0.751103
[epoch12, step1953]: loss 0.610723
[epoch12, step1954]: loss 0.553057
[epoch12, step1955]: loss 0.452340
[epoch12, step1956]: loss 0.636630
[epoch12, step1957]: loss 0.616513
[epoch12, step1958]: loss 0.506828
[epoch12, step1959]: loss 0.705474
[epoch12, step1960]: loss 0.473258
[epoch12, step1961]: loss 0.536106
[epoch12, step1962]: loss 0.278771
[epoch12, step1963]: loss 0.413336
[epoch12, step1964]: loss 0.569378
[epoch12, step1965]: loss 0.353048
[epoch12, step1966]: loss 0.801130
[epoch12, step1967]: loss 0.494513
[epoch12, step1968]: loss 0.803537
[epoch12, step1969]: loss 0.484219
[epoch12, step1970]: loss 0.552404
[epoch12, step1971]: loss 0.763522
[epoch12, step1972]: loss 0.531617
[epoch12, step1973]: loss 0.452940
[epoch12, step1974]: loss 0.460292
[epoch12, step1975]: loss 0.577936
[epoch12, step1976]: loss 0.451721
[epoch12, step1977]: loss 0.322374
[epoch12, step1978]: loss 0.495687
[epoch12, step1979]: loss 0.630389
[epoch12, step1980]: loss 0.295553
[epoch12, step1981]: loss 0.655270
[epoch12, step1982]: loss 0.492205
[epoch12, step1983]: loss 0.510767
[epoch12, step1984]: loss 0.509767
[epoch12, step1985]: loss 0.680502
[epoch12, step1986]: loss 0.703319
[epoch12, step1987]: loss 0.385196
[epoch12, step1988]: loss 0.735161
[epoch12, step1989]: loss 0.737496
[epoch12, step1990]: loss 0.767802
[epoch12, step1991]: loss 0.469279
[epoch12, step1992]: loss 0.745190
[epoch12, step1993]: loss 0.862870
[epoch12, step1994]: loss 0.745863
[epoch12, step1995]: loss 0.366433
[epoch12, step1996]: loss 0.588636
[epoch12, step1997]: loss 0.544873
[epoch12, step1998]: loss 0.617013
[epoch12, step1999]: loss 0.501261
[epoch12, step2000]: loss 0.569883
[epoch12, step2001]: loss 0.293246
[epoch12, step2002]: loss 0.626557
[epoch12, step2003]: loss 0.569706
[epoch12, step2004]: loss 0.576266
[epoch12, step2005]: loss 0.707720
[epoch12, step2006]: loss 0.650550
[epoch12, step2007]: loss 0.505386
[epoch12, step2008]: loss 0.543380
[epoch12, step2009]: loss 0.478025
[epoch12, step2010]: loss 0.292752
[epoch12, step2011]: loss 0.814546
[epoch12, step2012]: loss 0.300671
[epoch12, step2013]: loss 0.622676
[epoch12, step2014]: loss 0.446192
[epoch12, step2015]: loss 0.578224
[epoch12, step2016]: loss 0.697754
[epoch12, step2017]: loss 0.728756
[epoch12, step2018]: loss 0.618837
[epoch12, step2019]: loss 0.526750
[epoch12, step2020]: loss 0.508879
[epoch12, step2021]: loss 0.871002
[epoch12, step2022]: loss 0.325928
[epoch12, step2023]: loss 0.452255
[epoch12, step2024]: loss 0.522837
[epoch12, step2025]: loss 0.524037
[epoch12, step2026]: loss 0.483681
[epoch12, step2027]: loss 0.621069
[epoch12, step2028]: loss 0.705327
[epoch12, step2029]: loss 0.675592
[epoch12, step2030]: loss 0.548668
[epoch12, step2031]: loss 0.569221
[epoch12, step2032]: loss 0.595143
[epoch12, step2033]: loss 0.412288
[epoch12, step2034]: loss 0.381921
[epoch12, step2035]: loss 0.631137
[epoch12, step2036]: loss 0.616034
[epoch12, step2037]: loss 0.353453
[epoch12, step2038]: loss 0.746539
[epoch12, step2039]: loss 0.372015
[epoch12, step2040]: loss 0.530822
[epoch12, step2041]: loss 0.186177
[epoch12, step2042]: loss 0.536156
[epoch12, step2043]: loss 0.645071
[epoch12, step2044]: loss 0.654047
[epoch12, step2045]: loss 0.571174
[epoch12, step2046]: loss 0.557295
[epoch12, step2047]: loss 0.559937
[epoch12, step2048]: loss 0.505711
[epoch12, step2049]: loss 0.764016
[epoch12, step2050]: loss 0.503957
[epoch12, step2051]: loss 0.600756
[epoch12, step2052]: loss 0.578525
[epoch12, step2053]: loss 0.419283
[epoch12, step2054]: loss 0.695371
[epoch12, step2055]: loss 0.433816
[epoch12, step2056]: loss 0.603743
[epoch12, step2057]: loss 0.730891
[epoch12, step2058]: loss 0.582712
[epoch12, step2059]: loss 0.458662
[epoch12, step2060]: loss 0.259747
[epoch12, step2061]: loss 0.624387
[epoch12, step2062]: loss 0.572757
[epoch12, step2063]: loss 0.584961
[epoch12, step2064]: loss 0.604431
[epoch12, step2065]: loss 0.444125
[epoch12, step2066]: loss 0.374676
[epoch12, step2067]: loss 0.571122
[epoch12, step2068]: loss 0.340954
[epoch12, step2069]: loss 0.519732
[epoch12, step2070]: loss 0.371190
[epoch12, step2071]: loss 0.573511
[epoch12, step2072]: loss 0.521660
[epoch12, step2073]: loss 0.410174
[epoch12, step2074]: loss 0.375986
[epoch12, step2075]: loss 0.665940
[epoch12, step2076]: loss 0.690539
[epoch12, step2077]: loss 0.561401
[epoch12, step2078]: loss 0.175650
[epoch12, step2079]: loss 0.768089
[epoch12, step2080]: loss 0.577856
[epoch12, step2081]: loss 0.446529
[epoch12, step2082]: loss 0.366754
[epoch12, step2083]: loss 0.674589
[epoch12, step2084]: loss 0.701096
[epoch12, step2085]: loss 0.512984
[epoch12, step2086]: loss 0.673033
[epoch12, step2087]: loss 0.511915
[epoch12, step2088]: loss 0.701540
[epoch12, step2089]: loss 0.585119
[epoch12, step2090]: loss 0.409401
[epoch12, step2091]: loss 0.521906
[epoch12, step2092]: loss 0.474998
[epoch12, step2093]: loss 0.348699
[epoch12, step2094]: loss 0.760672
[epoch12, step2095]: loss 0.385788
[epoch12, step2096]: loss 0.565408
[epoch12, step2097]: loss 0.368420
[epoch12, step2098]: loss 0.635766
[epoch12, step2099]: loss 0.403197
[epoch12, step2100]: loss 0.573523
[epoch12, step2101]: loss 0.734754
[epoch12, step2102]: loss 0.481514
[epoch12, step2103]: loss 0.705675
[epoch12, step2104]: loss 0.461346
[epoch12, step2105]: loss 0.403382
[epoch12, step2106]: loss 0.761544
[epoch12, step2107]: loss 0.634134
[epoch12, step2108]: loss 0.520149
[epoch12, step2109]: loss 0.530665
[epoch12, step2110]: loss 0.623537
[epoch12, step2111]: loss 0.469379
[epoch12, step2112]: loss 0.776471
[epoch12, step2113]: loss 0.462366
[epoch12, step2114]: loss 0.579821
[epoch12, step2115]: loss 0.543872
[epoch12, step2116]: loss 0.464244
[epoch12, step2117]: loss 0.305732
[epoch12, step2118]: loss 0.577420
[epoch12, step2119]: loss 0.597693
[epoch12, step2120]: loss 0.882645
[epoch12, step2121]: loss 0.459686
[epoch12, step2122]: loss 0.680509
[epoch12, step2123]: loss 0.516399
[epoch12, step2124]: loss 0.680930
[epoch12, step2125]: loss 0.703533
[epoch12, step2126]: loss 0.599106
[epoch12, step2127]: loss 0.580697
[epoch12, step2128]: loss 0.493007
[epoch12, step2129]: loss 0.602018
[epoch12, step2130]: loss 0.484633
[epoch12, step2131]: loss 0.678464
[epoch12, step2132]: loss 0.665731
[epoch12, step2133]: loss 0.521793
[epoch12, step2134]: loss 0.796047
[epoch12, step2135]: loss 0.594791
[epoch12, step2136]: loss 0.445773
[epoch12, step2137]: loss 0.414538
[epoch12, step2138]: loss 0.463139
[epoch12, step2139]: loss 0.462925
[epoch12, step2140]: loss 0.359174
[epoch12, step2141]: loss 0.560391
[epoch12, step2142]: loss 0.591840
[epoch12, step2143]: loss 0.443672
[epoch12, step2144]: loss 0.541001
[epoch12, step2145]: loss 0.509682
[epoch12, step2146]: loss 0.629316
[epoch12, step2147]: loss 0.452730
[epoch12, step2148]: loss 0.614834
[epoch12, step2149]: loss 0.356212
[epoch12, step2150]: loss 0.550060
[epoch12, step2151]: loss 0.553276
[epoch12, step2152]: loss 0.485545
[epoch12, step2153]: loss 0.577732
[epoch12, step2154]: loss 0.262289
[epoch12, step2155]: loss 0.327847
[epoch12, step2156]: loss 0.576343
[epoch12, step2157]: loss 0.253960
[epoch12, step2158]: loss 0.771256
[epoch12, step2159]: loss 0.599095
[epoch12, step2160]: loss 0.505377
[epoch12, step2161]: loss 0.452291
[epoch12, step2162]: loss 0.639407
[epoch12, step2163]: loss 0.506457
[epoch12, step2164]: loss 0.285076
[epoch12, step2165]: loss 0.621503
[epoch12, step2166]: loss 0.531654
[epoch12, step2167]: loss 0.695565
[epoch12, step2168]: loss 0.304574
[epoch12, step2169]: loss 0.661341
[epoch12, step2170]: loss 0.726837
[epoch12, step2171]: loss 0.708744
[epoch12, step2172]: loss 0.591911
[epoch12, step2173]: loss 0.612747
[epoch12, step2174]: loss 0.600334
[epoch12, step2175]: loss 0.544590
[epoch12, step2176]: loss 0.546813
[epoch12, step2177]: loss 0.638427
[epoch12, step2178]: loss 0.539341
[epoch12, step2179]: loss 0.788616
[epoch12, step2180]: loss 0.515809
[epoch12, step2181]: loss 0.604242
[epoch12, step2182]: loss 0.402972
[epoch12, step2183]: loss 0.463846
[epoch12, step2184]: loss 0.618706
[epoch12, step2185]: loss 0.440830
[epoch12, step2186]: loss 0.500252
[epoch12, step2187]: loss 0.611410
[epoch12, step2188]: loss 0.645606
[epoch12, step2189]: loss 0.536013
[epoch12, step2190]: loss 0.505801
[epoch12, step2191]: loss 0.444640
[epoch12, step2192]: loss 0.736964
[epoch12, step2193]: loss 0.802645
[epoch12, step2194]: loss 0.662279
[epoch12, step2195]: loss 0.700627
[epoch12, step2196]: loss 0.527628
[epoch12, step2197]: loss 0.531177
[epoch12, step2198]: loss 0.431865
[epoch12, step2199]: loss 0.598343
[epoch12, step2200]: loss 0.565649
[epoch12, step2201]: loss 0.828832
[epoch12, step2202]: loss 0.512615
[epoch12, step2203]: loss 0.408057
[epoch12, step2204]: loss 0.493779
[epoch12, step2205]: loss 0.436373
[epoch12, step2206]: loss 0.606072
[epoch12, step2207]: loss 0.460480
[epoch12, step2208]: loss 0.610240
[epoch12, step2209]: loss 0.695692
[epoch12, step2210]: loss 0.719404
[epoch12, step2211]: loss 0.472085
[epoch12, step2212]: loss 0.795521
[epoch12, step2213]: loss 0.663862
[epoch12, step2214]: loss 0.583462
[epoch12, step2215]: loss 0.699387
[epoch12, step2216]: loss 0.554068
[epoch12, step2217]: loss 0.675545
[epoch12, step2218]: loss 0.695127
[epoch12, step2219]: loss 0.743427
[epoch12, step2220]: loss 0.673199
[epoch12, step2221]: loss 0.571453
[epoch12, step2222]: loss 0.542825
[epoch12, step2223]: loss 0.493614
[epoch12, step2224]: loss 0.569609
[epoch12, step2225]: loss 0.670432
[epoch12, step2226]: loss 0.582297
[epoch12, step2227]: loss 0.594633
[epoch12, step2228]: loss 0.605642
[epoch12, step2229]: loss 0.771999
[epoch12, step2230]: loss 0.663203
[epoch12, step2231]: loss 0.514938
[epoch12, step2232]: loss 0.627974
[epoch12, step2233]: loss 0.481088
[epoch12, step2234]: loss 0.415883
[epoch12, step2235]: loss 0.574792
[epoch12, step2236]: loss 0.452481
[epoch12, step2237]: loss 0.621396
[epoch12, step2238]: loss 0.725177
[epoch12, step2239]: loss 0.612281
[epoch12, step2240]: loss 0.735079
[epoch12, step2241]: loss 0.722183
[epoch12, step2242]: loss 0.448229
[epoch12, step2243]: loss 0.708061
[epoch12, step2244]: loss 0.580959
[epoch12, step2245]: loss 0.681464
[epoch12, step2246]: loss 0.328354
[epoch12, step2247]: loss 0.516253
[epoch12, step2248]: loss 0.563772
[epoch12, step2249]: loss 0.339594
[epoch12, step2250]: loss 0.648868
[epoch12, step2251]: loss 0.281924
[epoch12, step2252]: loss 0.405199
[epoch12, step2253]: loss 0.522283
[epoch12, step2254]: loss 0.530629
[epoch12, step2255]: loss 0.433059
[epoch12, step2256]: loss 0.451323
[epoch12, step2257]: loss 0.476549
[epoch12, step2258]: loss 0.383328
[epoch12, step2259]: loss 0.512119
[epoch12, step2260]: loss 0.540839
[epoch12, step2261]: loss 0.585664
[epoch12, step2262]: loss 0.632653
[epoch12, step2263]: loss 0.645545
[epoch12, step2264]: loss 0.404200
[epoch12, step2265]: loss 0.606507
[epoch12, step2266]: loss 0.556904
[epoch12, step2267]: loss 0.561634
[epoch12, step2268]: loss 0.722463
[epoch12, step2269]: loss 0.536119
[epoch12, step2270]: loss 0.475826
[epoch12, step2271]: loss 0.449804
[epoch12, step2272]: loss 0.541746
[epoch12, step2273]: loss 0.624802
[epoch12, step2274]: loss 0.623439
[epoch12, step2275]: loss 0.542205
[epoch12, step2276]: loss 0.799479
[epoch12, step2277]: loss 0.409986
[epoch12, step2278]: loss 0.742160
[epoch12, step2279]: loss 0.501764
[epoch12, step2280]: loss 0.562491
[epoch12, step2281]: loss 0.548453
[epoch12, step2282]: loss 0.458319
[epoch12, step2283]: loss 0.651648
[epoch12, step2284]: loss 0.340094
[epoch12, step2285]: loss 0.765143
[epoch12, step2286]: loss 0.801629
[epoch12, step2287]: loss 0.479709
[epoch12, step2288]: loss 0.633184
[epoch12, step2289]: loss 0.508951
[epoch12, step2290]: loss 0.578222
[epoch12, step2291]: loss 0.605200
[epoch12, step2292]: loss 0.651596
[epoch12, step2293]: loss 0.545839
[epoch12, step2294]: loss 0.599397
[epoch12, step2295]: loss 0.722305
[epoch12, step2296]: loss 0.550567
[epoch12, step2297]: loss 0.810724
[epoch12, step2298]: loss 0.407371
[epoch12, step2299]: loss 0.646072
[epoch12, step2300]: loss 0.518021
[epoch12, step2301]: loss 0.524364
[epoch12, step2302]: loss 0.835050
[epoch12, step2303]: loss 0.733805
[epoch12, step2304]: loss 0.551936
[epoch12, step2305]: loss 0.681121
[epoch12, step2306]: loss 0.482188
[epoch12, step2307]: loss 0.817324
[epoch12, step2308]: loss 0.587631
[epoch12, step2309]: loss 0.530923
[epoch12, step2310]: loss 0.539900
[epoch12, step2311]: loss 0.600563
[epoch12, step2312]: loss 0.560971
[epoch12, step2313]: loss 0.833452
[epoch12, step2314]: loss 0.547098
[epoch12, step2315]: loss 0.574193
[epoch12, step2316]: loss 0.729794
[epoch12, step2317]: loss 0.495228
[epoch12, step2318]: loss 0.494247
[epoch12, step2319]: loss 0.284311
[epoch12, step2320]: loss 0.526570
[epoch12, step2321]: loss 0.303203
[epoch12, step2322]: loss 0.305609
[epoch12, step2323]: loss 0.697915
[epoch12, step2324]: loss 0.771159
[epoch12, step2325]: loss 0.604575
[epoch12, step2326]: loss 0.548078
[epoch12, step2327]: loss 0.421796
[epoch12, step2328]: loss 0.696573
[epoch12, step2329]: loss 0.524306
[epoch12, step2330]: loss 0.609041
[epoch12, step2331]: loss 0.667805
[epoch12, step2332]: loss 0.698555
[epoch12, step2333]: loss 0.205158
[epoch12, step2334]: loss 0.699481
[epoch12, step2335]: loss 0.348001
[epoch12, step2336]: loss 0.322013
[epoch12, step2337]: loss 0.646460
[epoch12, step2338]: loss 0.611067
[epoch12, step2339]: loss 0.619843
[epoch12, step2340]: loss 0.652673
[epoch12, step2341]: loss 0.633763
[epoch12, step2342]: loss 0.680070
[epoch12, step2343]: loss 0.416674
[epoch12, step2344]: loss 0.796098
[epoch12, step2345]: loss 0.601419
[epoch12, step2346]: loss 0.530642
[epoch12, step2347]: loss 0.465469
[epoch12, step2348]: loss 0.568816
[epoch12, step2349]: loss 0.766763
[epoch12, step2350]: loss 0.596559
[epoch12, step2351]: loss 0.494582
[epoch12, step2352]: loss 0.687380
[epoch12, step2353]: loss 0.462813
[epoch12, step2354]: loss 0.612960
[epoch12, step2355]: loss 0.533120
[epoch12, step2356]: loss 0.548057
[epoch12, step2357]: loss 0.492184
[epoch12, step2358]: loss 0.719392
[epoch12, step2359]: loss 0.549665
[epoch12, step2360]: loss 0.735075
[epoch12, step2361]: loss 0.654384
[epoch12, step2362]: loss 0.511135
[epoch12, step2363]: loss 0.646275
[epoch12, step2364]: loss 0.636972
[epoch12, step2365]: loss 0.498917
[epoch12, step2366]: loss 0.779352
[epoch12, step2367]: loss 0.478118
[epoch12, step2368]: loss 0.511764
[epoch12, step2369]: loss 0.702675
[epoch12, step2370]: loss 0.456917
[epoch12, step2371]: loss 0.625196
[epoch12, step2372]: loss 0.391615
[epoch12, step2373]: loss 0.669042
[epoch12, step2374]: loss 0.668453
[epoch12, step2375]: loss 0.535433
[epoch12, step2376]: loss 0.566961
[epoch12, step2377]: loss 0.459786
[epoch12, step2378]: loss 0.503051
[epoch12, step2379]: loss 0.763850
[epoch12, step2380]: loss 0.664217
[epoch12, step2381]: loss 0.516001
[epoch12, step2382]: loss 0.696957
[epoch12, step2383]: loss 0.717047
[epoch12, step2384]: loss 0.745387
[epoch12, step2385]: loss 0.815614
[epoch12, step2386]: loss 0.770053
[epoch12, step2387]: loss 0.601981
[epoch12, step2388]: loss 0.477080
[epoch12, step2389]: loss 0.579634
[epoch12, step2390]: loss 0.619976
[epoch12, step2391]: loss 0.425442
[epoch12, step2392]: loss 0.622216
[epoch12, step2393]: loss 0.685129
[epoch12, step2394]: loss 0.458924
[epoch12, step2395]: loss 0.564989
[epoch12, step2396]: loss 0.375072
[epoch12, step2397]: loss 0.318652
[epoch12, step2398]: loss 0.527114
[epoch12, step2399]: loss 0.485176
[epoch12, step2400]: loss 0.712712
[epoch12, step2401]: loss 0.570059
[epoch12, step2402]: loss 0.618661
[epoch12, step2403]: loss 0.796444
[epoch12, step2404]: loss 0.711859
[epoch12, step2405]: loss 0.564867
[epoch12, step2406]: loss 0.632740
[epoch12, step2407]: loss 0.431734
[epoch12, step2408]: loss 0.689783
[epoch12, step2409]: loss 0.459897
[epoch12, step2410]: loss 0.624022
[epoch12, step2411]: loss 0.722876
[epoch12, step2412]: loss 0.707658
[epoch12, step2413]: loss 0.395195
[epoch12, step2414]: loss 0.615166
[epoch12, step2415]: loss 0.470595
[epoch12, step2416]: loss 0.434353
[epoch12, step2417]: loss 0.801582
[epoch12, step2418]: loss 0.167315
[epoch12, step2419]: loss 0.663054
[epoch12, step2420]: loss 0.654566
[epoch12, step2421]: loss 0.247755
[epoch12, step2422]: loss 0.777030
[epoch12, step2423]: loss 0.727123
[epoch12, step2424]: loss 0.496140
[epoch12, step2425]: loss 0.457332
[epoch12, step2426]: loss 0.671824
[epoch12, step2427]: loss 0.552785
[epoch12, step2428]: loss 0.471245
[epoch12, step2429]: loss 0.482046
[epoch12, step2430]: loss 0.558249
[epoch12, step2431]: loss 0.539936
[epoch12, step2432]: loss 0.711139
[epoch12, step2433]: loss 0.514460
[epoch12, step2434]: loss 0.645672
[epoch12, step2435]: loss 0.566280
[epoch12, step2436]: loss 0.445270
[epoch12, step2437]: loss 0.588371
[epoch12, step2438]: loss 0.574445
[epoch12, step2439]: loss 0.449595
[epoch12, step2440]: loss 0.618282
[epoch12, step2441]: loss 0.543231
[epoch12, step2442]: loss 0.712940
[epoch12, step2443]: loss 0.527386
[epoch12, step2444]: loss 0.538920
[epoch12, step2445]: loss 0.470368
[epoch12, step2446]: loss 0.527585
[epoch12, step2447]: loss 0.674608
[epoch12, step2448]: loss 0.593688
[epoch12, step2449]: loss 0.781494
[epoch12, step2450]: loss 0.570260
[epoch12, step2451]: loss 0.580865
[epoch12, step2452]: loss 0.610181
[epoch12, step2453]: loss 0.821008
[epoch12, step2454]: loss 0.547429
[epoch12, step2455]: loss 0.535831
[epoch12, step2456]: loss 0.456452
[epoch12, step2457]: loss 0.576487
[epoch12, step2458]: loss 0.717132
[epoch12, step2459]: loss 0.328266
[epoch12, step2460]: loss 0.703202
[epoch12, step2461]: loss 0.593300
[epoch12, step2462]: loss 0.630793
[epoch12, step2463]: loss 0.549183
[epoch12, step2464]: loss 0.473239
[epoch12, step2465]: loss 0.262147
[epoch12, step2466]: loss 0.567247
[epoch12, step2467]: loss 0.543073
[epoch12, step2468]: loss 0.557547
[epoch12, step2469]: loss 0.508612
[epoch12, step2470]: loss 0.613441
[epoch12, step2471]: loss 0.369233
[epoch12, step2472]: loss 0.525412
[epoch12, step2473]: loss 0.401441
[epoch12, step2474]: loss 0.467437
[epoch12, step2475]: loss 0.572000
[epoch12, step2476]: loss 0.674568
[epoch12, step2477]: loss 0.600109
[epoch12, step2478]: loss 0.584442
[epoch12, step2479]: loss 0.573607
[epoch12, step2480]: loss 0.662647
[epoch12, step2481]: loss 0.698849
[epoch12, step2482]: loss 0.477958
[epoch12, step2483]: loss 0.665734
[epoch12, step2484]: loss 0.115959
[epoch12, step2485]: loss 0.453484
[epoch12, step2486]: loss 0.737385
[epoch12, step2487]: loss 0.565455
[epoch12, step2488]: loss 0.363304
[epoch12, step2489]: loss 0.568472
[epoch12, step2490]: loss 0.475077
[epoch12, step2491]: loss 0.595150
[epoch12, step2492]: loss 0.801718
[epoch12, step2493]: loss 0.613965
[epoch12, step2494]: loss 0.645681
[epoch12, step2495]: loss 0.489997
[epoch12, step2496]: loss 0.681100
[epoch12, step2497]: loss 0.631124
[epoch12, step2498]: loss 0.525575
[epoch12, step2499]: loss 0.486521
[epoch12, step2500]: loss 0.553770
[epoch12, step2501]: loss 0.774947
[epoch12, step2502]: loss 0.569754
[epoch12, step2503]: loss 0.703427
[epoch12, step2504]: loss 0.579107
[epoch12, step2505]: loss 0.736866
[epoch12, step2506]: loss 0.803336
[epoch12, step2507]: loss 0.547406
[epoch12, step2508]: loss 0.596768
[epoch12, step2509]: loss 0.709722
[epoch12, step2510]: loss 0.406812
[epoch12, step2511]: loss 0.533363
[epoch12, step2512]: loss 0.489694
[epoch12, step2513]: loss 0.516103
[epoch12, step2514]: loss 0.557502
[epoch12, step2515]: loss 0.522675
[epoch12, step2516]: loss 0.552829
[epoch12, step2517]: loss 0.495737
[epoch12, step2518]: loss 0.530354
[epoch12, step2519]: loss 0.632113
[epoch12, step2520]: loss 0.712221
[epoch12, step2521]: loss 0.550171
[epoch12, step2522]: loss 0.444908
[epoch12, step2523]: loss 0.339817
[epoch12, step2524]: loss 0.588142
[epoch12, step2525]: loss 0.618586
[epoch12, step2526]: loss 0.632353
[epoch12, step2527]: loss 0.189445
[epoch12, step2528]: loss 0.744179
[epoch12, step2529]: loss 0.833612
[epoch12, step2530]: loss 0.540942
[epoch12, step2531]: loss 0.592729
[epoch12, step2532]: loss 0.484965
[epoch12, step2533]: loss 0.739890
[epoch12, step2534]: loss 0.663562
[epoch12, step2535]: loss 0.616478
[epoch12, step2536]: loss 0.731695
[epoch12, step2537]: loss 0.527512
[epoch12, step2538]: loss 0.356328
[epoch12, step2539]: loss 0.518851
[epoch12, step2540]: loss 0.647622
[epoch12, step2541]: loss 0.810195
[epoch12, step2542]: loss 0.806867
[epoch12, step2543]: loss 0.385229
[epoch12, step2544]: loss 0.482663
[epoch12, step2545]: loss 0.698476
[epoch12, step2546]: loss 0.376079
[epoch12, step2547]: loss 0.562589
[epoch12, step2548]: loss 0.693080
[epoch12, step2549]: loss 0.856338
[epoch12, step2550]: loss 0.653640
[epoch12, step2551]: loss 0.836377
[epoch12, step2552]: loss 0.616241
[epoch12, step2553]: loss 0.541966
[epoch12, step2554]: loss 0.404334
[epoch12, step2555]: loss 0.555387
[epoch12, step2556]: loss 0.385347
[epoch12, step2557]: loss 0.745772
[epoch12, step2558]: loss 0.791084
[epoch12, step2559]: loss 0.624896
[epoch12, step2560]: loss 0.519910
[epoch12, step2561]: loss 0.597644
[epoch12, step2562]: loss 0.575762
[epoch12, step2563]: loss 0.546849
[epoch12, step2564]: loss 0.367874
[epoch12, step2565]: loss 0.580337
[epoch12, step2566]: loss 0.506097
[epoch12, step2567]: loss 0.690721
[epoch12, step2568]: loss 0.764130
[epoch12, step2569]: loss 0.286596
[epoch12, step2570]: loss 0.635184
[epoch12, step2571]: loss 0.431699
[epoch12, step2572]: loss 0.704981
[epoch12, step2573]: loss 0.588718
[epoch12, step2574]: loss 0.318539
[epoch12, step2575]: loss 0.572889
[epoch12, step2576]: loss 0.524769
[epoch12, step2577]: loss 0.571507
[epoch12, step2578]: loss 0.461692
[epoch12, step2579]: loss 0.663827
[epoch12, step2580]: loss 0.749812
[epoch12, step2581]: loss 0.644380
[epoch12, step2582]: loss 0.625443
[epoch12, step2583]: loss 0.709826
[epoch12, step2584]: loss 0.685031
[epoch12, step2585]: loss 0.300139
[epoch12, step2586]: loss 0.695221
[epoch12, step2587]: loss 0.627803
[epoch12, step2588]: loss 0.782910
[epoch12, step2589]: loss 0.791951
[epoch12, step2590]: loss 0.597192
[epoch12, step2591]: loss 0.290564
[epoch12, step2592]: loss 0.512790
[epoch12, step2593]: loss 0.413188
[epoch12, step2594]: loss 0.499724
[epoch12, step2595]: loss 0.729596
[epoch12, step2596]: loss 0.716801
[epoch12, step2597]: loss 0.642081
[epoch12, step2598]: loss 0.672639
[epoch12, step2599]: loss 0.519533
[epoch12, step2600]: loss 0.365961
[epoch12, step2601]: loss 0.401797
[epoch12, step2602]: loss 0.601602
[epoch12, step2603]: loss 0.711595
[epoch12, step2604]: loss 0.444332
[epoch12, step2605]: loss 0.574987
[epoch12, step2606]: loss 0.586695
[epoch12, step2607]: loss 0.545138
[epoch12, step2608]: loss 0.546477
[epoch12, step2609]: loss 0.639408
[epoch12, step2610]: loss 0.639315
[epoch12, step2611]: loss 0.677320
[epoch12, step2612]: loss 0.585205
[epoch12, step2613]: loss 0.619616
[epoch12, step2614]: loss 0.609934
[epoch12, step2615]: loss 0.623921
[epoch12, step2616]: loss 0.606098
[epoch12, step2617]: loss 0.604270
[epoch12, step2618]: loss 0.655021
[epoch12, step2619]: loss 0.769009
[epoch12, step2620]: loss 0.278853
[epoch12, step2621]: loss 0.479537
[epoch12, step2622]: loss 0.581392
[epoch12, step2623]: loss 0.397890
[epoch12, step2624]: loss 0.565141
[epoch12, step2625]: loss 0.571441
[epoch12, step2626]: loss 0.429996
[epoch12, step2627]: loss 0.624854
[epoch12, step2628]: loss 0.832446
[epoch12, step2629]: loss 0.358981
[epoch12, step2630]: loss 0.554052
[epoch12, step2631]: loss 0.363428
[epoch12, step2632]: loss 0.599889
[epoch12, step2633]: loss 0.692452
[epoch12, step2634]: loss 0.744444
[epoch12, step2635]: loss 0.547936
[epoch12, step2636]: loss 0.587295
[epoch12, step2637]: loss 0.694183
[epoch12, step2638]: loss 0.603757
[epoch12, step2639]: loss 0.312182
[epoch12, step2640]: loss 0.792168
[epoch12, step2641]: loss 0.630635
[epoch12, step2642]: loss 0.685235
[epoch12, step2643]: loss 0.687775
[epoch12, step2644]: loss 0.828752
[epoch12, step2645]: loss 0.354321
[epoch12, step2646]: loss 0.777357
[epoch12, step2647]: loss 0.378282
[epoch12, step2648]: loss 0.579983
[epoch12, step2649]: loss 0.692940
[epoch12, step2650]: loss 0.397445
[epoch12, step2651]: loss 0.543348
[epoch12, step2652]: loss 0.665439
[epoch12, step2653]: loss 0.857369
[epoch12, step2654]: loss 0.783959
[epoch12, step2655]: loss 0.502317
[epoch12, step2656]: loss 0.646949
[epoch12, step2657]: loss 0.432064
[epoch12, step2658]: loss 0.419992
[epoch12, step2659]: loss 0.317039
[epoch12, step2660]: loss 0.665049
[epoch12, step2661]: loss 0.522779
[epoch12, step2662]: loss 0.404035
[epoch12, step2663]: loss 0.513106
[epoch12, step2664]: loss 0.500126
[epoch12, step2665]: loss 0.484718
[epoch12, step2666]: loss 0.382148
[epoch12, step2667]: loss 0.481991
[epoch12, step2668]: loss 0.632483
[epoch12, step2669]: loss 0.681709
[epoch12, step2670]: loss 0.569730
[epoch12, step2671]: loss 0.550524
[epoch12, step2672]: loss 0.544907
[epoch12, step2673]: loss 0.404608
[epoch12, step2674]: loss 0.495046
[epoch12, step2675]: loss 0.507077
[epoch12, step2676]: loss 0.780446
[epoch12, step2677]: loss 0.577571
[epoch12, step2678]: loss 0.456212
[epoch12, step2679]: loss 0.319013
[epoch12, step2680]: loss 0.560654
[epoch12, step2681]: loss 0.492426
[epoch12, step2682]: loss 0.659741
[epoch12, step2683]: loss 0.681412
[epoch12, step2684]: loss 0.600656
[epoch12, step2685]: loss 0.611625
[epoch12, step2686]: loss 0.415047
[epoch12, step2687]: loss 0.600428
[epoch12, step2688]: loss 0.449785
[epoch12, step2689]: loss 0.604265
[epoch12, step2690]: loss 0.636194
[epoch12, step2691]: loss 0.872504
[epoch12, step2692]: loss 0.512579
[epoch12, step2693]: loss 0.661796
[epoch12, step2694]: loss 0.467934
[epoch12, step2695]: loss 0.604075
[epoch12, step2696]: loss 0.417293
[epoch12, step2697]: loss 0.626904
[epoch12, step2698]: loss 0.403799
[epoch12, step2699]: loss 0.430599
[epoch12, step2700]: loss 0.630775
[epoch12, step2701]: loss 0.622359
[epoch12, step2702]: loss 0.520619
[epoch12, step2703]: loss 0.727546
[epoch12, step2704]: loss 0.801180
[epoch12, step2705]: loss 0.395817
[epoch12, step2706]: loss 0.655742
[epoch12, step2707]: loss 0.487793
[epoch12, step2708]: loss 0.543407
[epoch12, step2709]: loss 0.607514
[epoch12, step2710]: loss 0.379520
[epoch12, step2711]: loss 0.575174
[epoch12, step2712]: loss 0.331080
[epoch12, step2713]: loss 0.613852
[epoch12, step2714]: loss 0.655599
[epoch12, step2715]: loss 0.499562
[epoch12, step2716]: loss 0.396774
[epoch12, step2717]: loss 0.670957
[epoch12, step2718]: loss 0.471098
[epoch12, step2719]: loss 0.528309
[epoch12, step2720]: loss 0.603676
[epoch12, step2721]: loss 0.431921
[epoch12, step2722]: loss 0.525531
[epoch12, step2723]: loss 0.405523
[epoch12, step2724]: loss 0.691336
[epoch12, step2725]: loss 0.486711
[epoch12, step2726]: loss 0.341559
[epoch12, step2727]: loss 0.765123
[epoch12, step2728]: loss 0.592894
[epoch12, step2729]: loss 0.508411
[epoch12, step2730]: loss 0.792860
[epoch12, step2731]: loss 0.573978
[epoch12, step2732]: loss 0.590373
[epoch12, step2733]: loss 0.636083
[epoch12, step2734]: loss 0.365526
[epoch12, step2735]: loss 0.496737
[epoch12, step2736]: loss 0.581003
[epoch12, step2737]: loss 0.600031
[epoch12, step2738]: loss 0.679559
[epoch12, step2739]: loss 0.847366
[epoch12, step2740]: loss 0.241163
[epoch12, step2741]: loss 0.616192
[epoch12, step2742]: loss 0.570782
[epoch12, step2743]: loss 0.592513
[epoch12, step2744]: loss 0.613977
[epoch12, step2745]: loss 0.552423
[epoch12, step2746]: loss 0.540933
[epoch12, step2747]: loss 0.438746
[epoch12, step2748]: loss 0.558569
[epoch12, step2749]: loss 0.585573
[epoch12, step2750]: loss 0.576326
[epoch12, step2751]: loss 0.407421
[epoch12, step2752]: loss 0.532700
[epoch12, step2753]: loss 0.439539
[epoch12, step2754]: loss 0.578963
[epoch12, step2755]: loss 0.678033
[epoch12, step2756]: loss 0.300187
[epoch12, step2757]: loss 0.599965
[epoch12, step2758]: loss 0.561208
[epoch12, step2759]: loss 0.696163
[epoch12, step2760]: loss 0.620886
[epoch12, step2761]: loss 0.799445
[epoch12, step2762]: loss 0.647746
[epoch12, step2763]: loss 0.259532
[epoch12, step2764]: loss 0.570616
[epoch12, step2765]: loss 0.333332
[epoch12, step2766]: loss 0.733450
[epoch12, step2767]: loss 0.568727
[epoch12, step2768]: loss 0.619837
[epoch12, step2769]: loss 0.444988
[epoch12, step2770]: loss 0.582715
[epoch12, step2771]: loss 0.417402
[epoch12, step2772]: loss 0.381213
[epoch12, step2773]: loss 0.662807
[epoch12, step2774]: loss 0.536245
[epoch12, step2775]: loss 0.382135
[epoch12, step2776]: loss 0.540025
[epoch12, step2777]: loss 0.544120
[epoch12, step2778]: loss 0.514236
[epoch12, step2779]: loss 0.325769
[epoch12, step2780]: loss 0.571397
[epoch12, step2781]: loss 0.511146
[epoch12, step2782]: loss 0.413364
[epoch12, step2783]: loss 0.536655
[epoch12, step2784]: loss 0.650151
[epoch12, step2785]: loss 0.801754
[epoch12, step2786]: loss 0.600687
[epoch12, step2787]: loss 0.694802
[epoch12, step2788]: loss 0.563088
[epoch12, step2789]: loss 0.445256
[epoch12, step2790]: loss 0.689697
[epoch12, step2791]: loss 0.671371
[epoch12, step2792]: loss 0.638882
[epoch12, step2793]: loss 0.682581
[epoch12, step2794]: loss 0.722494
[epoch12, step2795]: loss 0.732886
[epoch12, step2796]: loss 0.767652
[epoch12, step2797]: loss 0.774667
[epoch12, step2798]: loss 0.313970
[epoch12, step2799]: loss 0.661050
[epoch12, step2800]: loss 0.501318
[epoch12, step2801]: loss 0.549931
[epoch12, step2802]: loss 0.524961
[epoch12, step2803]: loss 0.518294
[epoch12, step2804]: loss 0.341265
[epoch12, step2805]: loss 0.537567
[epoch12, step2806]: loss 0.531911
[epoch12, step2807]: loss 0.423844
[epoch12, step2808]: loss 0.232255
[epoch12, step2809]: loss 0.596343
[epoch12, step2810]: loss 0.672378
[epoch12, step2811]: loss 0.375127
[epoch12, step2812]: loss 0.414200
[epoch12, step2813]: loss 0.777142
[epoch12, step2814]: loss 0.430088
[epoch12, step2815]: loss 0.666973
[epoch12, step2816]: loss 0.772772
[epoch12, step2817]: loss 0.378474
[epoch12, step2818]: loss 0.661447
[epoch12, step2819]: loss 0.560343
[epoch12, step2820]: loss 0.516894
[epoch12, step2821]: loss 0.585610
[epoch12, step2822]: loss 0.486707
[epoch12, step2823]: loss 0.418477
[epoch12, step2824]: loss 0.775793
[epoch12, step2825]: loss 0.551777
[epoch12, step2826]: loss 0.591494
[epoch12, step2827]: loss 0.246608
[epoch12, step2828]: loss 0.575243
[epoch12, step2829]: loss 0.638050
[epoch12, step2830]: loss 0.573285
[epoch12, step2831]: loss 0.376955
[epoch12, step2832]: loss 0.659661
[epoch12, step2833]: loss 0.508474
[epoch12, step2834]: loss 0.550013
[epoch12, step2835]: loss 0.752050
[epoch12, step2836]: loss 0.443006
[epoch12, step2837]: loss 0.576114
[epoch12, step2838]: loss 0.287910
[epoch12, step2839]: loss 0.350231
[epoch12, step2840]: loss 0.501823
[epoch12, step2841]: loss 0.446310
[epoch12, step2842]: loss 0.495719
[epoch12, step2843]: loss 0.564997
[epoch12, step2844]: loss 0.503621
[epoch12, step2845]: loss 0.650069
[epoch12, step2846]: loss 0.520342
[epoch12, step2847]: loss 0.464714
[epoch12, step2848]: loss 0.671941
[epoch12, step2849]: loss 0.658653
[epoch12, step2850]: loss 0.556235
[epoch12, step2851]: loss 0.666213
[epoch12, step2852]: loss 0.356069
[epoch12, step2853]: loss 0.796312
[epoch12, step2854]: loss 0.593359
[epoch12, step2855]: loss 0.815818
[epoch12, step2856]: loss 0.571616
[epoch12, step2857]: loss 0.556941
[epoch12, step2858]: loss 0.364691
[epoch12, step2859]: loss 0.725684
[epoch12, step2860]: loss 0.496903
[epoch12, step2861]: loss 0.489933
[epoch12, step2862]: loss 0.720802
[epoch12, step2863]: loss 0.339355
[epoch12, step2864]: loss 0.831395
[epoch12, step2865]: loss 0.642529
[epoch12, step2866]: loss 0.727984
[epoch12, step2867]: loss 0.294661
[epoch12, step2868]: loss 0.682956
[epoch12, step2869]: loss 0.143853
[epoch12, step2870]: loss 0.631845
[epoch12, step2871]: loss 0.380569
[epoch12, step2872]: loss 0.577933
[epoch12, step2873]: loss 0.533029
[epoch12, step2874]: loss 0.444456
[epoch12, step2875]: loss 0.424428
[epoch12, step2876]: loss 0.435776
[epoch12, step2877]: loss 0.454516
[epoch12, step2878]: loss 0.678576
[epoch12, step2879]: loss 0.504718
[epoch12, step2880]: loss 0.386557
[epoch12, step2881]: loss 0.815457
[epoch12, step2882]: loss 0.517786
[epoch12, step2883]: loss 0.364225
[epoch12, step2884]: loss 0.494096
[epoch12, step2885]: loss 0.587147
[epoch12, step2886]: loss 0.575053
[epoch12, step2887]: loss 0.581673
[epoch12, step2888]: loss 0.446019
[epoch12, step2889]: loss 0.433086
[epoch12, step2890]: loss 0.603726
[epoch12, step2891]: loss 0.665452
[epoch12, step2892]: loss 0.735142
[epoch12, step2893]: loss 0.368704
[epoch12, step2894]: loss 0.520628
[epoch12, step2895]: loss 0.687750
[epoch12, step2896]: loss 0.577969
[epoch12, step2897]: loss 0.441690
[epoch12, step2898]: loss 0.600428
[epoch12, step2899]: loss 0.595211
[epoch12, step2900]: loss 0.502650
[epoch12, step2901]: loss 0.654192
[epoch12, step2902]: loss 0.562005
[epoch12, step2903]: loss 0.645632
[epoch12, step2904]: loss 0.539394
[epoch12, step2905]: loss 0.758131
[epoch12, step2906]: loss 0.498871
[epoch12, step2907]: loss 0.533054
[epoch12, step2908]: loss 0.493736
[epoch12, step2909]: loss 0.727250
[epoch12, step2910]: loss 0.830173
[epoch12, step2911]: loss 0.567416
[epoch12, step2912]: loss 0.538868
[epoch12, step2913]: loss 0.505118
[epoch12, step2914]: loss 0.652877
[epoch12, step2915]: loss 0.715776
[epoch12, step2916]: loss 0.445594
[epoch12, step2917]: loss 0.476989
[epoch12, step2918]: loss 0.600284
[epoch12, step2919]: loss 0.378626
[epoch12, step2920]: loss 0.471090
[epoch12, step2921]: loss 0.467741
[epoch12, step2922]: loss 0.522013
[epoch12, step2923]: loss 0.473128
[epoch12, step2924]: loss 0.393151
[epoch12, step2925]: loss 0.495263
[epoch12, step2926]: loss 0.552250
[epoch12, step2927]: loss 0.540042
[epoch12, step2928]: loss 0.459414
[epoch12, step2929]: loss 0.360804
[epoch12, step2930]: loss 0.833552
[epoch12, step2931]: loss 0.646617
[epoch12, step2932]: loss 0.630215
[epoch12, step2933]: loss 0.724319
[epoch12, step2934]: loss 0.724282
[epoch12, step2935]: loss 0.595409
[epoch12, step2936]: loss 0.542351
[epoch12, step2937]: loss 0.323582
[epoch12, step2938]: loss 0.661172
[epoch12, step2939]: loss 0.587707
[epoch12, step2940]: loss 0.704179
[epoch12, step2941]: loss 0.574783
[epoch12, step2942]: loss 0.724027
[epoch12, step2943]: loss 0.594467
[epoch12, step2944]: loss 0.618045
[epoch12, step2945]: loss 0.546881
[epoch12, step2946]: loss 0.524463
[epoch12, step2947]: loss 0.436394
[epoch12, step2948]: loss 0.606535
[epoch12, step2949]: loss 0.614087
[epoch12, step2950]: loss 0.704393
[epoch12, step2951]: loss 0.626375
[epoch12, step2952]: loss 0.470184
[epoch12, step2953]: loss 0.369967
[epoch12, step2954]: loss 0.551976
[epoch12, step2955]: loss 0.813429
[epoch12, step2956]: loss 0.245479
[epoch12, step2957]: loss 0.739931
[epoch12, step2958]: loss 0.572379
[epoch12, step2959]: loss 0.738253
[epoch12, step2960]: loss 0.584026
[epoch12, step2961]: loss 0.786354
[epoch12, step2962]: loss 0.408548
[epoch12, step2963]: loss 0.479856
[epoch12, step2964]: loss 0.492449
[epoch12, step2965]: loss 0.560144
[epoch12, step2966]: loss 0.763506
[epoch12, step2967]: loss 0.462098
[epoch12, step2968]: loss 0.547096
[epoch12, step2969]: loss 0.632238
[epoch12, step2970]: loss 0.553574
[epoch12, step2971]: loss 0.540577
[epoch12, step2972]: loss 0.404619
[epoch12, step2973]: loss 0.657619
[epoch12, step2974]: loss 0.340087
[epoch12, step2975]: loss 0.415119
[epoch12, step2976]: loss 0.622196
[epoch12, step2977]: loss 0.477442
[epoch12, step2978]: loss 0.671033
[epoch12, step2979]: loss 0.609687
[epoch12, step2980]: loss 0.566856
[epoch12, step2981]: loss 0.436284
[epoch12, step2982]: loss 0.369212
[epoch12, step2983]: loss 0.562511
[epoch12, step2984]: loss 0.371371
[epoch12, step2985]: loss 0.684008
[epoch12, step2986]: loss 0.632687
[epoch12, step2987]: loss 0.539222
[epoch12, step2988]: loss 0.705959
[epoch12, step2989]: loss 0.740966
[epoch12, step2990]: loss 0.452378
[epoch12, step2991]: loss 0.745038
[epoch12, step2992]: loss 0.615466
[epoch12, step2993]: loss 0.577329
[epoch12, step2994]: loss 0.572532
[epoch12, step2995]: loss 0.709420
[epoch12, step2996]: loss 0.653145
[epoch12, step2997]: loss 0.615693
[epoch12, step2998]: loss 0.445911
[epoch12, step2999]: loss 0.764546
[epoch12, step3000]: loss 0.538598
[epoch12, step3001]: loss 0.446607
[epoch12, step3002]: loss 0.439855
[epoch12, step3003]: loss 0.638255
[epoch12, step3004]: loss 0.282535
[epoch12, step3005]: loss 0.446252
[epoch12, step3006]: loss 0.312886
[epoch12, step3007]: loss 0.572195
[epoch12, step3008]: loss 0.666051
[epoch12, step3009]: loss 0.485994
[epoch12, step3010]: loss 0.485349
[epoch12, step3011]: loss 0.666412
[epoch12, step3012]: loss 0.479250
[epoch12, step3013]: loss 0.627918
[epoch12, step3014]: loss 0.419199
[epoch12, step3015]: loss 0.395283
[epoch12, step3016]: loss 0.437706
[epoch12, step3017]: loss 0.748917
[epoch12, step3018]: loss 0.548092
[epoch12, step3019]: loss 0.533903
[epoch12, step3020]: loss 0.364482
[epoch12, step3021]: loss 0.441537
[epoch12, step3022]: loss 0.535212
[epoch12, step3023]: loss 0.586612
[epoch12, step3024]: loss 0.559488
[epoch12, step3025]: loss 0.508509
[epoch12, step3026]: loss 0.669529
[epoch12, step3027]: loss 0.681782
[epoch12, step3028]: loss 0.433890
[epoch12, step3029]: loss 0.682089
[epoch12, step3030]: loss 0.516539
[epoch12, step3031]: loss 0.206884
[epoch12, step3032]: loss 0.517745
[epoch12, step3033]: loss 0.934448
[epoch12, step3034]: loss 0.447205
[epoch12, step3035]: loss 0.433745
[epoch12, step3036]: loss 0.728566
[epoch12, step3037]: loss 0.579954
[epoch12, step3038]: loss 0.819059
[epoch12, step3039]: loss 0.631183
[epoch12, step3040]: loss 0.810434
[epoch12, step3041]: loss 0.527729
[epoch12, step3042]: loss 0.325289
[epoch12, step3043]: loss 0.524104
[epoch12, step3044]: loss 0.573012
[epoch12, step3045]: loss 0.450086
[epoch12, step3046]: loss 0.457191
[epoch12, step3047]: loss 0.239982
[epoch12, step3048]: loss 0.631003
[epoch12, step3049]: loss 0.686339
[epoch12, step3050]: loss 0.680084
[epoch12, step3051]: loss 0.512169
[epoch12, step3052]: loss 0.703397
[epoch12, step3053]: loss 0.436264
[epoch12, step3054]: loss 0.645633
[epoch12, step3055]: loss 0.418710
[epoch12, step3056]: loss 0.737106
[epoch12, step3057]: loss 0.612062
[epoch12, step3058]: loss 0.501187
[epoch12, step3059]: loss 0.272600
[epoch12, step3060]: loss 0.453322
[epoch12, step3061]: loss 0.661594
[epoch12, step3062]: loss 0.630500
[epoch12, step3063]: loss 0.549712
[epoch12, step3064]: loss 0.517513
[epoch12, step3065]: loss 0.439279
[epoch12, step3066]: loss 0.517573
[epoch12, step3067]: loss 0.436663
[epoch12, step3068]: loss 0.286323
[epoch12, step3069]: loss 0.569607
[epoch12, step3070]: loss 0.544359
[epoch12, step3071]: loss 0.435242
[epoch12, step3072]: loss 0.366796
[epoch12, step3073]: loss 0.675965
[epoch12, step3074]: loss 0.486749
[epoch12, step3075]: loss 0.694656
[epoch12, step3076]: loss 0.731682

[epoch12]: avg loss 0.731682

[epoch13, step1]: loss 0.559591
[epoch13, step2]: loss 0.506523
[epoch13, step3]: loss 0.539205
[epoch13, step4]: loss 0.588143
[epoch13, step5]: loss 0.639277
[epoch13, step6]: loss 0.473460
[epoch13, step7]: loss 0.716703
[epoch13, step8]: loss 0.480049
[epoch13, step9]: loss 0.706249
[epoch13, step10]: loss 0.714475
[epoch13, step11]: loss 0.580411
[epoch13, step12]: loss 0.697289
[epoch13, step13]: loss 0.552672
[epoch13, step14]: loss 0.569967
[epoch13, step15]: loss 0.714870
[epoch13, step16]: loss 0.539771
[epoch13, step17]: loss 0.519546
[epoch13, step18]: loss 0.491784
[epoch13, step19]: loss 0.536170
[epoch13, step20]: loss 0.471223
[epoch13, step21]: loss 0.627048
[epoch13, step22]: loss 0.457791
[epoch13, step23]: loss 0.822709
[epoch13, step24]: loss 0.428501
[epoch13, step25]: loss 0.499894
[epoch13, step26]: loss 0.433173
[epoch13, step27]: loss 0.499974
[epoch13, step28]: loss 0.709654
[epoch13, step29]: loss 0.583671
[epoch13, step30]: loss 0.664910
[epoch13, step31]: loss 0.394183
[epoch13, step32]: loss 0.151202
[epoch13, step33]: loss 0.707748
[epoch13, step34]: loss 0.549035
[epoch13, step35]: loss 0.472140
[epoch13, step36]: loss 0.245431
[epoch13, step37]: loss 0.549536
[epoch13, step38]: loss 0.442172
[epoch13, step39]: loss 0.664421
[epoch13, step40]: loss 0.775197
[epoch13, step41]: loss 0.340286
[epoch13, step42]: loss 0.462566
[epoch13, step43]: loss 0.828217
[epoch13, step44]: loss 0.787486
[epoch13, step45]: loss 0.637127
[epoch13, step46]: loss 0.622777
[epoch13, step47]: loss 0.323631
[epoch13, step48]: loss 0.648332
[epoch13, step49]: loss 0.518859
[epoch13, step50]: loss 0.459363
[epoch13, step51]: loss 0.583606
[epoch13, step52]: loss 0.405153
[epoch13, step53]: loss 0.547729
[epoch13, step54]: loss 0.467829
[epoch13, step55]: loss 0.803754
[epoch13, step56]: loss 0.696907
[epoch13, step57]: loss 0.585653
[epoch13, step58]: loss 0.661651
[epoch13, step59]: loss 0.428174
[epoch13, step60]: loss 0.433581
[epoch13, step61]: loss 0.701131
[epoch13, step62]: loss 0.427858
[epoch13, step63]: loss 0.632544
[epoch13, step64]: loss 0.560649
[epoch13, step65]: loss 0.316548
[epoch13, step66]: loss 0.586988
[epoch13, step67]: loss 0.665291
[epoch13, step68]: loss 0.558839
[epoch13, step69]: loss 0.719232
[epoch13, step70]: loss 0.830231
[epoch13, step71]: loss 0.324338
[epoch13, step72]: loss 0.520812
[epoch13, step73]: loss 0.694597
[epoch13, step74]: loss 0.599442
[epoch13, step75]: loss 0.284212
[epoch13, step76]: loss 0.286955
[epoch13, step77]: loss 0.483129
[epoch13, step78]: loss 0.538953
[epoch13, step79]: loss 0.829910
[epoch13, step80]: loss 0.667271
[epoch13, step81]: loss 0.564601
[epoch13, step82]: loss 0.739905
[epoch13, step83]: loss 0.514857
[epoch13, step84]: loss 0.451902
[epoch13, step85]: loss 0.179662
[epoch13, step86]: loss 0.553471
[epoch13, step87]: loss 0.407929
[epoch13, step88]: loss 0.404876
[epoch13, step89]: loss 0.647193
[epoch13, step90]: loss 0.579108
[epoch13, step91]: loss 0.394580
[epoch13, step92]: loss 0.648660
[epoch13, step93]: loss 0.691681
[epoch13, step94]: loss 0.561551
[epoch13, step95]: loss 0.777849
[epoch13, step96]: loss 0.713391
[epoch13, step97]: loss 0.589717
[epoch13, step98]: loss 0.667655
[epoch13, step99]: loss 0.555927
[epoch13, step100]: loss 0.446105
[epoch13, step101]: loss 0.448514
[epoch13, step102]: loss 0.409295
[epoch13, step103]: loss 0.655244
[epoch13, step104]: loss 0.374782
[epoch13, step105]: loss 0.676099
[epoch13, step106]: loss 0.679438
[epoch13, step107]: loss 0.401872
[epoch13, step108]: loss 0.579099
[epoch13, step109]: loss 0.195269
[epoch13, step110]: loss 0.617301
[epoch13, step111]: loss 0.568916
[epoch13, step112]: loss 0.456403
[epoch13, step113]: loss 0.648227
[epoch13, step114]: loss 0.721288
[epoch13, step115]: loss 0.510742
[epoch13, step116]: loss 0.627878
[epoch13, step117]: loss 0.587576
[epoch13, step118]: loss 0.471707
[epoch13, step119]: loss 0.637399
[epoch13, step120]: loss 0.407310
[epoch13, step121]: loss 0.716347
[epoch13, step122]: loss 0.660348
[epoch13, step123]: loss 0.796378
[epoch13, step124]: loss 0.675749
[epoch13, step125]: loss 0.722368
[epoch13, step126]: loss 0.522350
[epoch13, step127]: loss 0.579064
[epoch13, step128]: loss 0.480773
[epoch13, step129]: loss 0.445688
[epoch13, step130]: loss 0.600337
[epoch13, step131]: loss 0.602103
[epoch13, step132]: loss 0.479876
[epoch13, step133]: loss 0.611129
[epoch13, step134]: loss 0.738633
[epoch13, step135]: loss 0.378050
[epoch13, step136]: loss 0.863576
[epoch13, step137]: loss 0.538961
[epoch13, step138]: loss 0.326160
[epoch13, step139]: loss 0.668770
[epoch13, step140]: loss 0.420262
[epoch13, step141]: loss 0.654554
[epoch13, step142]: loss 0.763742
[epoch13, step143]: loss 0.755628
[epoch13, step144]: loss 0.589436
[epoch13, step145]: loss 0.671621
[epoch13, step146]: loss 0.633055
[epoch13, step147]: loss 0.607565
[epoch13, step148]: loss 0.456040
[epoch13, step149]: loss 0.652384
[epoch13, step150]: loss 0.243715
[epoch13, step151]: loss 0.314732
[epoch13, step152]: loss 0.681659
[epoch13, step153]: loss 0.599298
[epoch13, step154]: loss 0.664366
[epoch13, step155]: loss 0.630368
[epoch13, step156]: loss 0.693004
[epoch13, step157]: loss 0.336780
[epoch13, step158]: loss 0.410297
[epoch13, step159]: loss 0.365491
[epoch13, step160]: loss 0.938952
[epoch13, step161]: loss 0.515885
[epoch13, step162]: loss 0.476548
[epoch13, step163]: loss 0.388939
[epoch13, step164]: loss 0.365431
[epoch13, step165]: loss 0.327876
[epoch13, step166]: loss 0.434694
[epoch13, step167]: loss 0.734937
[epoch13, step168]: loss 0.502042
[epoch13, step169]: loss 0.665452
[epoch13, step170]: loss 0.485808
[epoch13, step171]: loss 0.508838
[epoch13, step172]: loss 0.543374
[epoch13, step173]: loss 0.436718
[epoch13, step174]: loss 0.567806
[epoch13, step175]: loss 0.537780
[epoch13, step176]: loss 0.330302
[epoch13, step177]: loss 0.695795
[epoch13, step178]: loss 0.563617
[epoch13, step179]: loss 0.681220
[epoch13, step180]: loss 0.654757
[epoch13, step181]: loss 0.492641
[epoch13, step182]: loss 0.356604
[epoch13, step183]: loss 0.517922
[epoch13, step184]: loss 0.311276
[epoch13, step185]: loss 0.339406
[epoch13, step186]: loss 0.293831
[epoch13, step187]: loss 0.712885
[epoch13, step188]: loss 0.346568
[epoch13, step189]: loss 0.899701
[epoch13, step190]: loss 0.776963
[epoch13, step191]: loss 0.672076
[epoch13, step192]: loss 0.374968
[epoch13, step193]: loss 0.701124
[epoch13, step194]: loss 0.527734
[epoch13, step195]: loss 0.693613
[epoch13, step196]: loss 0.519397
[epoch13, step197]: loss 0.425182
[epoch13, step198]: loss 0.393487
[epoch13, step199]: loss 0.512426
[epoch13, step200]: loss 0.712387
[epoch13, step201]: loss 0.505636
[epoch13, step202]: loss 0.872763
[epoch13, step203]: loss 0.487803
[epoch13, step204]: loss 0.381247
[epoch13, step205]: loss 0.436722
[epoch13, step206]: loss 0.624180
[epoch13, step207]: loss 0.704213
[epoch13, step208]: loss 0.142736
[epoch13, step209]: loss 0.470885
[epoch13, step210]: loss 0.518543
[epoch13, step211]: loss 0.648875
[epoch13, step212]: loss 0.650957
[epoch13, step213]: loss 0.598963
[epoch13, step214]: loss 0.543244
[epoch13, step215]: loss 0.415423
[epoch13, step216]: loss 0.593353
[epoch13, step217]: loss 0.449839
[epoch13, step218]: loss 0.469095
[epoch13, step219]: loss 0.543933
[epoch13, step220]: loss 0.618384
[epoch13, step221]: loss 0.671693
[epoch13, step222]: loss 0.701498
[epoch13, step223]: loss 0.323972
[epoch13, step224]: loss 0.687177
[epoch13, step225]: loss 0.534473
[epoch13, step226]: loss 0.292629
[epoch13, step227]: loss 0.709172
[epoch13, step228]: loss 0.413185
[epoch13, step229]: loss 0.695984
[epoch13, step230]: loss 0.237821
[epoch13, step231]: loss 0.602070
[epoch13, step232]: loss 0.641887
[epoch13, step233]: loss 0.358804
[epoch13, step234]: loss 0.391859
[epoch13, step235]: loss 0.640138
[epoch13, step236]: loss 0.524196
[epoch13, step237]: loss 0.596330
[epoch13, step238]: loss 0.405889
[epoch13, step239]: loss 0.681631
[epoch13, step240]: loss 0.463470
[epoch13, step241]: loss 0.429684
[epoch13, step242]: loss 0.713857
[epoch13, step243]: loss 0.477526
[epoch13, step244]: loss 0.433082
[epoch13, step245]: loss 0.543647
[epoch13, step246]: loss 0.386443
[epoch13, step247]: loss 0.649782
[epoch13, step248]: loss 0.765781
[epoch13, step249]: loss 0.741043
[epoch13, step250]: loss 0.624798
[epoch13, step251]: loss 0.474667
[epoch13, step252]: loss 0.530616
[epoch13, step253]: loss 0.531434
[epoch13, step254]: loss 0.644812
[epoch13, step255]: loss 0.417771
[epoch13, step256]: loss 0.656381
[epoch13, step257]: loss 0.671243
[epoch13, step258]: loss 0.534934
[epoch13, step259]: loss 0.503920
[epoch13, step260]: loss 0.659843
[epoch13, step261]: loss 0.508621
[epoch13, step262]: loss 0.419963
[epoch13, step263]: loss 0.448072
[epoch13, step264]: loss 0.560583
[epoch13, step265]: loss 0.710270
[epoch13, step266]: loss 0.595920
[epoch13, step267]: loss 0.491419
[epoch13, step268]: loss 0.409313
[epoch13, step269]: loss 0.583523
[epoch13, step270]: loss 0.705232
[epoch13, step271]: loss 0.621467
[epoch13, step272]: loss 0.611158
[epoch13, step273]: loss 0.839324
[epoch13, step274]: loss 0.351997
[epoch13, step275]: loss 0.558858
[epoch13, step276]: loss 0.739008
[epoch13, step277]: loss 0.463669
[epoch13, step278]: loss 0.399347
[epoch13, step279]: loss 0.385954
[epoch13, step280]: loss 0.736988
[epoch13, step281]: loss 0.603867
[epoch13, step282]: loss 0.772226
[epoch13, step283]: loss 0.496331
[epoch13, step284]: loss 0.826409
[epoch13, step285]: loss 0.408041
[epoch13, step286]: loss 0.644024
[epoch13, step287]: loss 0.593250
[epoch13, step288]: loss 0.432560
[epoch13, step289]: loss 0.604773
[epoch13, step290]: loss 0.662987
[epoch13, step291]: loss 0.576924
[epoch13, step292]: loss 0.634026
[epoch13, step293]: loss 0.379922
[epoch13, step294]: loss 0.645089
[epoch13, step295]: loss 0.429565
[epoch13, step296]: loss 0.559570
[epoch13, step297]: loss 0.592331
[epoch13, step298]: loss 0.444047
[epoch13, step299]: loss 0.572269
[epoch13, step300]: loss 0.624957
[epoch13, step301]: loss 0.445859
[epoch13, step302]: loss 0.563105
[epoch13, step303]: loss 0.645481
[epoch13, step304]: loss 0.788591
[epoch13, step305]: loss 0.517543
[epoch13, step306]: loss 0.343494
[epoch13, step307]: loss 0.521672
[epoch13, step308]: loss 0.743705
[epoch13, step309]: loss 0.724301
[epoch13, step310]: loss 0.604454
[epoch13, step311]: loss 0.634544
[epoch13, step312]: loss 0.757231
[epoch13, step313]: loss 0.338767
[epoch13, step314]: loss 0.643092
[epoch13, step315]: loss 0.477978
[epoch13, step316]: loss 0.507506
[epoch13, step317]: loss 0.527953
[epoch13, step318]: loss 0.870654
[epoch13, step319]: loss 0.495706
[epoch13, step320]: loss 0.757333
[epoch13, step321]: loss 0.674915
[epoch13, step322]: loss 0.389351
[epoch13, step323]: loss 0.509519
[epoch13, step324]: loss 0.388290
[epoch13, step325]: loss 0.547505
[epoch13, step326]: loss 0.671833
[epoch13, step327]: loss 0.440496
[epoch13, step328]: loss 0.638603
[epoch13, step329]: loss 0.547140
[epoch13, step330]: loss 0.586014
[epoch13, step331]: loss 0.481766
[epoch13, step332]: loss 0.387345
[epoch13, step333]: loss 0.546121
[epoch13, step334]: loss 0.324617
[epoch13, step335]: loss 0.729524
[epoch13, step336]: loss 0.434217
[epoch13, step337]: loss 0.484438
[epoch13, step338]: loss 0.216121
[epoch13, step339]: loss 0.558198
[epoch13, step340]: loss 0.847342
[epoch13, step341]: loss 0.575594
[epoch13, step342]: loss 0.430178
[epoch13, step343]: loss 0.503274
[epoch13, step344]: loss 0.361615
[epoch13, step345]: loss 0.549982
[epoch13, step346]: loss 0.455333
[epoch13, step347]: loss 0.710551
[epoch13, step348]: loss 0.731747
[epoch13, step349]: loss 0.595232
[epoch13, step350]: loss 0.524112
[epoch13, step351]: loss 0.563955
[epoch13, step352]: loss 0.545989
[epoch13, step353]: loss 0.316301
[epoch13, step354]: loss 0.613519
[epoch13, step355]: loss 0.514336
[epoch13, step356]: loss 0.355692
[epoch13, step357]: loss 0.263085
[epoch13, step358]: loss 0.511900
[epoch13, step359]: loss 0.701265
[epoch13, step360]: loss 0.717958
[epoch13, step361]: loss 0.551703
[epoch13, step362]: loss 0.588151
[epoch13, step363]: loss 0.717549
[epoch13, step364]: loss 0.584987
[epoch13, step365]: loss 0.562833
[epoch13, step366]: loss 0.755604
[epoch13, step367]: loss 0.730062
[epoch13, step368]: loss 0.679855
[epoch13, step369]: loss 0.505352
[epoch13, step370]: loss 0.351401
[epoch13, step371]: loss 0.751200
[epoch13, step372]: loss 0.507006
[epoch13, step373]: loss 0.802780
[epoch13, step374]: loss 0.610317
[epoch13, step375]: loss 0.523837
[epoch13, step376]: loss 0.889942
[epoch13, step377]: loss 0.341534
[epoch13, step378]: loss 0.675425
[epoch13, step379]: loss 0.762262
[epoch13, step380]: loss 0.367893
[epoch13, step381]: loss 0.612283
[epoch13, step382]: loss 0.759333
[epoch13, step383]: loss 0.742362
[epoch13, step384]: loss 0.582310
[epoch13, step385]: loss 0.758849
[epoch13, step386]: loss 0.255173
[epoch13, step387]: loss 0.302724
[epoch13, step388]: loss 0.174711
[epoch13, step389]: loss 0.681170
[epoch13, step390]: loss 0.808399
[epoch13, step391]: loss 0.515450
[epoch13, step392]: loss 0.402581
[epoch13, step393]: loss 0.514803
[epoch13, step394]: loss 0.375205
[epoch13, step395]: loss 0.467088
[epoch13, step396]: loss 0.441525
[epoch13, step397]: loss 0.388989
[epoch13, step398]: loss 0.671168
[epoch13, step399]: loss 0.443464
[epoch13, step400]: loss 0.486404
[epoch13, step401]: loss 0.580439
[epoch13, step402]: loss 0.788599
[epoch13, step403]: loss 0.494380
[epoch13, step404]: loss 0.789711
[epoch13, step405]: loss 0.404642
[epoch13, step406]: loss 0.584621
[epoch13, step407]: loss 0.697432
[epoch13, step408]: loss 0.617566
[epoch13, step409]: loss 0.421205
[epoch13, step410]: loss 0.804287
[epoch13, step411]: loss 0.491858
[epoch13, step412]: loss 0.613142
[epoch13, step413]: loss 0.690264
[epoch13, step414]: loss 0.470597
[epoch13, step415]: loss 0.419152
[epoch13, step416]: loss 0.711792
[epoch13, step417]: loss 0.748082
[epoch13, step418]: loss 0.379735
[epoch13, step419]: loss 0.558085
[epoch13, step420]: loss 0.565188
[epoch13, step421]: loss 0.383412
[epoch13, step422]: loss 0.718748
[epoch13, step423]: loss 0.390613
[epoch13, step424]: loss 0.469492
[epoch13, step425]: loss 0.581094
[epoch13, step426]: loss 0.563970
[epoch13, step427]: loss 0.692716
[epoch13, step428]: loss 0.440988
[epoch13, step429]: loss 0.710387
[epoch13, step430]: loss 0.741746
[epoch13, step431]: loss 0.237159
[epoch13, step432]: loss 0.478065
[epoch13, step433]: loss 0.739939
[epoch13, step434]: loss 0.672156
[epoch13, step435]: loss 0.812831
[epoch13, step436]: loss 0.299477
[epoch13, step437]: loss 0.579607
[epoch13, step438]: loss 0.635523
[epoch13, step439]: loss 0.362298
[epoch13, step440]: loss 0.301023
[epoch13, step441]: loss 0.328700
[epoch13, step442]: loss 0.644713
[epoch13, step443]: loss 0.548529
[epoch13, step444]: loss 0.725716
[epoch13, step445]: loss 0.391006
[epoch13, step446]: loss 0.613255
[epoch13, step447]: loss 0.741008
[epoch13, step448]: loss 0.535624
[epoch13, step449]: loss 0.676031
[epoch13, step450]: loss 0.225337
[epoch13, step451]: loss 0.679628
[epoch13, step452]: loss 0.602843
[epoch13, step453]: loss 0.715980
[epoch13, step454]: loss 0.431015
[epoch13, step455]: loss 0.577291
[epoch13, step456]: loss 0.565116
[epoch13, step457]: loss 0.711449
[epoch13, step458]: loss 0.507177
[epoch13, step459]: loss 0.631813
[epoch13, step460]: loss 0.599154
[epoch13, step461]: loss 0.650121
[epoch13, step462]: loss 0.729751
[epoch13, step463]: loss 0.531037
[epoch13, step464]: loss 0.704073
[epoch13, step465]: loss 0.363174
[epoch13, step466]: loss 0.692672
[epoch13, step467]: loss 0.789127
[epoch13, step468]: loss 0.696573
[epoch13, step469]: loss 0.472056
[epoch13, step470]: loss 0.744608
[epoch13, step471]: loss 0.317166
[epoch13, step472]: loss 0.358415
[epoch13, step473]: loss 0.484109
[epoch13, step474]: loss 0.630937
[epoch13, step475]: loss 0.828328
[epoch13, step476]: loss 0.640726
[epoch13, step477]: loss 0.792743
[epoch13, step478]: loss 0.252825
[epoch13, step479]: loss 0.674250
[epoch13, step480]: loss 0.524582
[epoch13, step481]: loss 0.527953
[epoch13, step482]: loss 0.688853
[epoch13, step483]: loss 0.501524
[epoch13, step484]: loss 0.643979
[epoch13, step485]: loss 0.531885
[epoch13, step486]: loss 0.825363
[epoch13, step487]: loss 0.654254
[epoch13, step488]: loss 0.796448
[epoch13, step489]: loss 0.612785
[epoch13, step490]: loss 0.639439
[epoch13, step491]: loss 0.430583
[epoch13, step492]: loss 0.549055
[epoch13, step493]: loss 0.447388
[epoch13, step494]: loss 0.545085
[epoch13, step495]: loss 0.479297
[epoch13, step496]: loss 0.439430
[epoch13, step497]: loss 0.303476
[epoch13, step498]: loss 0.566053
[epoch13, step499]: loss 0.489104
[epoch13, step500]: loss 0.569362
[epoch13, step501]: loss 0.840110
[epoch13, step502]: loss 0.419394
[epoch13, step503]: loss 0.575821
[epoch13, step504]: loss 0.445136
[epoch13, step505]: loss 0.470975
[epoch13, step506]: loss 0.579463
[epoch13, step507]: loss 0.752175
[epoch13, step508]: loss 0.562125
[epoch13, step509]: loss 0.538566
[epoch13, step510]: loss 0.612343
[epoch13, step511]: loss 0.397059
[epoch13, step512]: loss 0.522959
[epoch13, step513]: loss 0.469743
[epoch13, step514]: loss 0.541412
[epoch13, step515]: loss 0.650374
[epoch13, step516]: loss 0.805406
[epoch13, step517]: loss 0.713867
[epoch13, step518]: loss 0.638815
[epoch13, step519]: loss 0.691506
[epoch13, step520]: loss 0.691868
[epoch13, step521]: loss 0.417646
[epoch13, step522]: loss 0.548794
[epoch13, step523]: loss 0.803992
[epoch13, step524]: loss 0.537394
[epoch13, step525]: loss 0.505951
[epoch13, step526]: loss 0.519160
[epoch13, step527]: loss 0.579258
[epoch13, step528]: loss 0.353090
[epoch13, step529]: loss 0.724545
[epoch13, step530]: loss 0.404961
[epoch13, step531]: loss 0.341334
[epoch13, step532]: loss 0.438251
[epoch13, step533]: loss 0.640618
[epoch13, step534]: loss 0.836037
[epoch13, step535]: loss 0.800113
[epoch13, step536]: loss 0.661151
[epoch13, step537]: loss 0.332916
[epoch13, step538]: loss 0.587579
[epoch13, step539]: loss 0.581078
[epoch13, step540]: loss 0.577172
[epoch13, step541]: loss 0.375785
[epoch13, step542]: loss 0.522101
[epoch13, step543]: loss 0.395583
[epoch13, step544]: loss 0.744979
[epoch13, step545]: loss 0.505259
[epoch13, step546]: loss 0.626957
[epoch13, step547]: loss 0.586962
[epoch13, step548]: loss 0.543938
[epoch13, step549]: loss 0.558200
[epoch13, step550]: loss 0.384280
[epoch13, step551]: loss 0.911274
[epoch13, step552]: loss 0.508776
[epoch13, step553]: loss 0.468422
[epoch13, step554]: loss 0.284146
[epoch13, step555]: loss 0.639261
[epoch13, step556]: loss 0.339908
[epoch13, step557]: loss 0.676979
[epoch13, step558]: loss 0.467977
[epoch13, step559]: loss 0.408990
[epoch13, step560]: loss 0.841092
[epoch13, step561]: loss 0.555034
[epoch13, step562]: loss 0.572884
[epoch13, step563]: loss 0.443164
[epoch13, step564]: loss 0.670649
[epoch13, step565]: loss 0.540370
[epoch13, step566]: loss 0.338292
[epoch13, step567]: loss 0.369633
[epoch13, step568]: loss 0.553209
[epoch13, step569]: loss 0.445091
[epoch13, step570]: loss 0.686143
[epoch13, step571]: loss 0.732288
[epoch13, step572]: loss 0.453819
[epoch13, step573]: loss 0.548753
[epoch13, step574]: loss 0.676782
[epoch13, step575]: loss 0.542938
[epoch13, step576]: loss 0.579842
[epoch13, step577]: loss 0.660170
[epoch13, step578]: loss 0.338572
[epoch13, step579]: loss 0.226343
[epoch13, step580]: loss 0.599305
[epoch13, step581]: loss 0.583484
[epoch13, step582]: loss 0.329061
[epoch13, step583]: loss 0.583273
[epoch13, step584]: loss 0.389237
[epoch13, step585]: loss 0.309325
[epoch13, step586]: loss 0.567795
[epoch13, step587]: loss 0.462595
[epoch13, step588]: loss 0.707771
[epoch13, step589]: loss 0.556520
[epoch13, step590]: loss 0.804109
[epoch13, step591]: loss 0.716377
[epoch13, step592]: loss 0.631531
[epoch13, step593]: loss 0.678722
[epoch13, step594]: loss 0.378694
[epoch13, step595]: loss 0.441811
[epoch13, step596]: loss 0.542436
[epoch13, step597]: loss 0.573325
[epoch13, step598]: loss 0.581024
[epoch13, step599]: loss 0.397825
[epoch13, step600]: loss 0.582392
[epoch13, step601]: loss 0.720370
[epoch13, step602]: loss 0.844323
[epoch13, step603]: loss 0.604306
[epoch13, step604]: loss 0.439611
[epoch13, step605]: loss 0.687630
[epoch13, step606]: loss 0.540676
[epoch13, step607]: loss 0.657057
[epoch13, step608]: loss 0.575553
[epoch13, step609]: loss 0.749600
[epoch13, step610]: loss 0.421354
[epoch13, step611]: loss 0.752564
[epoch13, step612]: loss 0.609822
[epoch13, step613]: loss 0.414763
[epoch13, step614]: loss 0.406053
[epoch13, step615]: loss 0.640331
[epoch13, step616]: loss 0.746605
[epoch13, step617]: loss 0.510560
[epoch13, step618]: loss 0.595935
[epoch13, step619]: loss 0.458052
[epoch13, step620]: loss 0.561675
[epoch13, step621]: loss 0.636049
[epoch13, step622]: loss 0.411326
[epoch13, step623]: loss 0.722866
[epoch13, step624]: loss 0.441516
[epoch13, step625]: loss 0.680808
[epoch13, step626]: loss 0.592847
[epoch13, step627]: loss 0.482935
[epoch13, step628]: loss 0.824335
[epoch13, step629]: loss 0.566426
[epoch13, step630]: loss 0.483419
[epoch13, step631]: loss 0.479732
[epoch13, step632]: loss 0.418567
[epoch13, step633]: loss 0.525219
[epoch13, step634]: loss 0.264433
[epoch13, step635]: loss 0.427062
[epoch13, step636]: loss 0.592221
[epoch13, step637]: loss 0.372004
[epoch13, step638]: loss 0.503759
[epoch13, step639]: loss 0.595402
[epoch13, step640]: loss 0.707756
[epoch13, step641]: loss 0.675062
[epoch13, step642]: loss 0.575896
[epoch13, step643]: loss 0.282578
[epoch13, step644]: loss 0.405708
[epoch13, step645]: loss 0.718004
[epoch13, step646]: loss 0.777026
[epoch13, step647]: loss 0.483174
[epoch13, step648]: loss 0.713998
[epoch13, step649]: loss 0.424091
[epoch13, step650]: loss 0.496778
[epoch13, step651]: loss 0.572283
[epoch13, step652]: loss 0.398053
[epoch13, step653]: loss 0.688098
[epoch13, step654]: loss 0.592052
[epoch13, step655]: loss 0.631705
[epoch13, step656]: loss 0.461728
[epoch13, step657]: loss 0.639099
[epoch13, step658]: loss 0.848249
[epoch13, step659]: loss 0.649156
[epoch13, step660]: loss 0.234823
[epoch13, step661]: loss 0.504843
[epoch13, step662]: loss 0.528289
[epoch13, step663]: loss 0.380400
[epoch13, step664]: loss 0.790798
[epoch13, step665]: loss 0.640599
[epoch13, step666]: loss 0.428901
[epoch13, step667]: loss 0.285521
[epoch13, step668]: loss 0.592085
[epoch13, step669]: loss 0.603672
[epoch13, step670]: loss 0.338140
[epoch13, step671]: loss 0.591985
[epoch13, step672]: loss 0.534695
[epoch13, step673]: loss 0.505792
[epoch13, step674]: loss 0.570456
[epoch13, step675]: loss 0.631434
[epoch13, step676]: loss 0.750768
[epoch13, step677]: loss 0.418894
[epoch13, step678]: loss 0.479831
[epoch13, step679]: loss 0.628280
[epoch13, step680]: loss 0.374100
[epoch13, step681]: loss 0.659241
[epoch13, step682]: loss 0.661508
[epoch13, step683]: loss 0.498100
[epoch13, step684]: loss 0.377831
[epoch13, step685]: loss 0.560359
[epoch13, step686]: loss 0.388046
[epoch13, step687]: loss 0.586730
[epoch13, step688]: loss 0.578580
[epoch13, step689]: loss 0.593582
[epoch13, step690]: loss 0.792184
[epoch13, step691]: loss 0.726253
[epoch13, step692]: loss 0.527714
[epoch13, step693]: loss 0.494287
[epoch13, step694]: loss 0.743532
[epoch13, step695]: loss 0.604342
[epoch13, step696]: loss 0.554175
[epoch13, step697]: loss 0.678333
[epoch13, step698]: loss 0.762828
[epoch13, step699]: loss 0.602126
[epoch13, step700]: loss 0.464525
[epoch13, step701]: loss 0.470047
[epoch13, step702]: loss 0.562238
[epoch13, step703]: loss 0.505837
[epoch13, step704]: loss 0.629859
[epoch13, step705]: loss 0.612822
[epoch13, step706]: loss 0.403344
[epoch13, step707]: loss 0.689239
[epoch13, step708]: loss 0.626218
[epoch13, step709]: loss 0.665696
[epoch13, step710]: loss 0.636209
[epoch13, step711]: loss 0.455152
[epoch13, step712]: loss 0.570928
[epoch13, step713]: loss 0.600638
[epoch13, step714]: loss 0.493129
[epoch13, step715]: loss 0.492899
[epoch13, step716]: loss 0.455128
[epoch13, step717]: loss 0.395671
[epoch13, step718]: loss 0.769383
[epoch13, step719]: loss 0.683503
[epoch13, step720]: loss 0.506890
[epoch13, step721]: loss 0.450838
[epoch13, step722]: loss 0.477992
[epoch13, step723]: loss 0.666212
[epoch13, step724]: loss 0.860505
[epoch13, step725]: loss 0.451613
[epoch13, step726]: loss 0.445605
[epoch13, step727]: loss 0.269843
[epoch13, step728]: loss 0.337289
[epoch13, step729]: loss 0.516529
[epoch13, step730]: loss 0.577129
[epoch13, step731]: loss 0.469296
[epoch13, step732]: loss 0.725814
[epoch13, step733]: loss 0.666875
[epoch13, step734]: loss 0.522253
[epoch13, step735]: loss 0.529470
[epoch13, step736]: loss 0.490433
[epoch13, step737]: loss 0.516918
[epoch13, step738]: loss 0.590117
[epoch13, step739]: loss 0.566145
[epoch13, step740]: loss 0.559296
[epoch13, step741]: loss 0.434453
[epoch13, step742]: loss 0.743521
[epoch13, step743]: loss 0.706751
[epoch13, step744]: loss 0.437128
[epoch13, step745]: loss 0.419069
[epoch13, step746]: loss 0.603138
[epoch13, step747]: loss 0.656892
[epoch13, step748]: loss 0.361347
[epoch13, step749]: loss 0.723435
[epoch13, step750]: loss 0.612571
[epoch13, step751]: loss 0.290783
[epoch13, step752]: loss 0.598817
[epoch13, step753]: loss 0.546241
[epoch13, step754]: loss 0.374646
[epoch13, step755]: loss 0.398509
[epoch13, step756]: loss 0.698844
[epoch13, step757]: loss 0.586996
[epoch13, step758]: loss 0.690224
[epoch13, step759]: loss 0.281707
[epoch13, step760]: loss 0.618502
[epoch13, step761]: loss 0.465939
[epoch13, step762]: loss 0.251395
[epoch13, step763]: loss 0.375184
[epoch13, step764]: loss 0.826908
[epoch13, step765]: loss 0.562839
[epoch13, step766]: loss 0.673625
[epoch13, step767]: loss 0.532027
[epoch13, step768]: loss 0.557136
[epoch13, step769]: loss 0.577417
[epoch13, step770]: loss 0.434010
[epoch13, step771]: loss 0.736421
[epoch13, step772]: loss 0.470115
[epoch13, step773]: loss 0.412928
[epoch13, step774]: loss 0.456272
[epoch13, step775]: loss 0.584993
[epoch13, step776]: loss 0.690150
[epoch13, step777]: loss 0.474732
[epoch13, step778]: loss 0.296763
[epoch13, step779]: loss 0.636736
[epoch13, step780]: loss 0.516626
[epoch13, step781]: loss 0.831451
[epoch13, step782]: loss 0.456104
[epoch13, step783]: loss 0.539438
[epoch13, step784]: loss 0.676663
[epoch13, step785]: loss 0.796366
[epoch13, step786]: loss 0.343671
[epoch13, step787]: loss 0.455392
[epoch13, step788]: loss 0.378470
[epoch13, step789]: loss 0.504343
[epoch13, step790]: loss 0.483264
[epoch13, step791]: loss 0.444802
[epoch13, step792]: loss 0.692779
[epoch13, step793]: loss 0.477142
[epoch13, step794]: loss 0.532365
[epoch13, step795]: loss 0.520560
[epoch13, step796]: loss 0.385190
[epoch13, step797]: loss 0.549932
[epoch13, step798]: loss 0.443757
[epoch13, step799]: loss 0.747912
[epoch13, step800]: loss 0.655896
[epoch13, step801]: loss 0.289182
[epoch13, step802]: loss 0.796817
[epoch13, step803]: loss 0.676761
[epoch13, step804]: loss 0.727903
[epoch13, step805]: loss 0.498362
[epoch13, step806]: loss 0.432959
[epoch13, step807]: loss 0.663375
[epoch13, step808]: loss 0.595238
[epoch13, step809]: loss 0.309724
[epoch13, step810]: loss 0.519419
[epoch13, step811]: loss 0.732740
[epoch13, step812]: loss 0.569464
[epoch13, step813]: loss 0.636990
[epoch13, step814]: loss 0.279895
[epoch13, step815]: loss 0.735705
[epoch13, step816]: loss 0.691323
[epoch13, step817]: loss 0.443356
[epoch13, step818]: loss 0.410563
[epoch13, step819]: loss 0.529024
[epoch13, step820]: loss 0.708162
[epoch13, step821]: loss 0.491668
[epoch13, step822]: loss 0.691065
[epoch13, step823]: loss 0.712453
[epoch13, step824]: loss 0.413184
[epoch13, step825]: loss 0.528589
[epoch13, step826]: loss 0.381580
[epoch13, step827]: loss 0.764497
[epoch13, step828]: loss 0.496916
[epoch13, step829]: loss 0.511343
[epoch13, step830]: loss 0.430484
[epoch13, step831]: loss 0.388221
[epoch13, step832]: loss 0.464824
[epoch13, step833]: loss 0.815931
[epoch13, step834]: loss 0.657683
[epoch13, step835]: loss 0.453513
[epoch13, step836]: loss 0.339144
[epoch13, step837]: loss 0.447140
[epoch13, step838]: loss 0.527163
[epoch13, step839]: loss 0.226425
[epoch13, step840]: loss 0.605241
[epoch13, step841]: loss 0.395635
[epoch13, step842]: loss 0.554716
[epoch13, step843]: loss 0.613153
[epoch13, step844]: loss 0.753572
[epoch13, step845]: loss 0.616751
[epoch13, step846]: loss 0.521299
[epoch13, step847]: loss 0.710648
[epoch13, step848]: loss 0.718401
[epoch13, step849]: loss 0.793230
[epoch13, step850]: loss 0.461818
[epoch13, step851]: loss 0.342357
[epoch13, step852]: loss 0.474160
[epoch13, step853]: loss 0.685366
[epoch13, step854]: loss 0.446025
[epoch13, step855]: loss 0.491206
[epoch13, step856]: loss 0.698777
[epoch13, step857]: loss 0.657241
[epoch13, step858]: loss 0.657260
[epoch13, step859]: loss 0.594638
[epoch13, step860]: loss 0.385530
[epoch13, step861]: loss 0.473008
[epoch13, step862]: loss 0.612141
[epoch13, step863]: loss 0.539439
[epoch13, step864]: loss 0.376160
[epoch13, step865]: loss 0.565971
[epoch13, step866]: loss 0.590947
[epoch13, step867]: loss 0.634903
[epoch13, step868]: loss 0.371667
[epoch13, step869]: loss 0.702198
[epoch13, step870]: loss 0.711435
[epoch13, step871]: loss 0.445835
[epoch13, step872]: loss 0.534888
[epoch13, step873]: loss 0.595032
[epoch13, step874]: loss 0.577016
[epoch13, step875]: loss 0.472938
[epoch13, step876]: loss 0.449594
[epoch13, step877]: loss 0.673369
[epoch13, step878]: loss 0.569064
[epoch13, step879]: loss 0.350855
[epoch13, step880]: loss 0.465817
[epoch13, step881]: loss 0.702661
[epoch13, step882]: loss 0.793102
[epoch13, step883]: loss 0.446214
[epoch13, step884]: loss 0.504215
[epoch13, step885]: loss 0.532011
[epoch13, step886]: loss 0.418713
[epoch13, step887]: loss 0.606282
[epoch13, step888]: loss 0.701220
[epoch13, step889]: loss 0.601542
[epoch13, step890]: loss 0.460762
[epoch13, step891]: loss 0.681000
[epoch13, step892]: loss 0.649787
[epoch13, step893]: loss 0.513288
[epoch13, step894]: loss 0.614040
[epoch13, step895]: loss 0.643397
[epoch13, step896]: loss 0.632149
[epoch13, step897]: loss 0.574690
[epoch13, step898]: loss 0.649603
[epoch13, step899]: loss 0.539433
[epoch13, step900]: loss 0.723441
[epoch13, step901]: loss 0.558171
[epoch13, step902]: loss 0.700803
[epoch13, step903]: loss 0.342444
[epoch13, step904]: loss 0.691980
[epoch13, step905]: loss 0.409749
[epoch13, step906]: loss 0.591196
[epoch13, step907]: loss 0.299267
[epoch13, step908]: loss 0.548010
[epoch13, step909]: loss 0.408398
[epoch13, step910]: loss 0.321451
[epoch13, step911]: loss 0.682994
[epoch13, step912]: loss 0.424883
[epoch13, step913]: loss 0.353840
[epoch13, step914]: loss 0.626192
[epoch13, step915]: loss 0.480040
[epoch13, step916]: loss 0.390138
[epoch13, step917]: loss 0.721914
[epoch13, step918]: loss 0.626835
[epoch13, step919]: loss 0.711344
[epoch13, step920]: loss 0.764121
[epoch13, step921]: loss 0.538562
[epoch13, step922]: loss 0.738630
[epoch13, step923]: loss 0.812036
[epoch13, step924]: loss 0.687045
[epoch13, step925]: loss 0.520779
[epoch13, step926]: loss 0.582625
[epoch13, step927]: loss 0.588643
[epoch13, step928]: loss 0.761173
[epoch13, step929]: loss 0.713529
[epoch13, step930]: loss 0.639986
[epoch13, step931]: loss 0.564402
[epoch13, step932]: loss 0.739315
[epoch13, step933]: loss 0.454339
[epoch13, step934]: loss 0.446658
[epoch13, step935]: loss 0.663043
[epoch13, step936]: loss 0.580431
[epoch13, step937]: loss 0.495607
[epoch13, step938]: loss 0.335328
[epoch13, step939]: loss 0.559431
[epoch13, step940]: loss 0.488842
[epoch13, step941]: loss 0.740465
[epoch13, step942]: loss 0.410190
[epoch13, step943]: loss 0.487557
[epoch13, step944]: loss 0.824270
[epoch13, step945]: loss 0.424655
[epoch13, step946]: loss 0.700433
[epoch13, step947]: loss 0.480286
[epoch13, step948]: loss 0.320492
[epoch13, step949]: loss 0.547843
[epoch13, step950]: loss 0.350608
[epoch13, step951]: loss 0.536642
[epoch13, step952]: loss 0.431590
[epoch13, step953]: loss 0.486142
[epoch13, step954]: loss 0.672862
[epoch13, step955]: loss 0.335139
[epoch13, step956]: loss 0.442198
[epoch13, step957]: loss 0.648313
[epoch13, step958]: loss 0.666173
[epoch13, step959]: loss 0.815625
[epoch13, step960]: loss 0.638182
[epoch13, step961]: loss 0.652456
[epoch13, step962]: loss 0.561047
[epoch13, step963]: loss 0.668995
[epoch13, step964]: loss 0.758017
[epoch13, step965]: loss 0.495998
[epoch13, step966]: loss 0.323620
[epoch13, step967]: loss 0.515812
[epoch13, step968]: loss 0.640710
[epoch13, step969]: loss 0.602184
[epoch13, step970]: loss 0.410307
[epoch13, step971]: loss 0.630692
[epoch13, step972]: loss 0.529937
[epoch13, step973]: loss 0.393781
[epoch13, step974]: loss 0.603093
[epoch13, step975]: loss 0.085937
[epoch13, step976]: loss 0.587736
[epoch13, step977]: loss 0.276634
[epoch13, step978]: loss 0.378435
[epoch13, step979]: loss 0.494541
[epoch13, step980]: loss 0.518077
[epoch13, step981]: loss 0.665140
[epoch13, step982]: loss 0.691469
[epoch13, step983]: loss 0.615628
[epoch13, step984]: loss 0.158639
[epoch13, step985]: loss 0.609940
[epoch13, step986]: loss 0.636516
[epoch13, step987]: loss 0.554739
[epoch13, step988]: loss 0.524999
[epoch13, step989]: loss 0.685703
[epoch13, step990]: loss 0.722927
[epoch13, step991]: loss 0.463992
[epoch13, step992]: loss 0.420321
[epoch13, step993]: loss 0.420606
[epoch13, step994]: loss 0.600915
[epoch13, step995]: loss 0.646436
[epoch13, step996]: loss 0.582064
[epoch13, step997]: loss 0.685152
[epoch13, step998]: loss 0.804220
[epoch13, step999]: loss 0.463453
[epoch13, step1000]: loss 0.605110
[epoch13, step1001]: loss 0.547710
[epoch13, step1002]: loss 0.697286
[epoch13, step1003]: loss 0.635841
[epoch13, step1004]: loss 0.559696
[epoch13, step1005]: loss 0.620273
[epoch13, step1006]: loss 0.350587
[epoch13, step1007]: loss 0.511871
[epoch13, step1008]: loss 0.836585
[epoch13, step1009]: loss 0.531122
[epoch13, step1010]: loss 0.812550
[epoch13, step1011]: loss 0.486155
[epoch13, step1012]: loss 0.598223
[epoch13, step1013]: loss 0.486929
[epoch13, step1014]: loss 0.481046
[epoch13, step1015]: loss 0.519376
[epoch13, step1016]: loss 0.488280
[epoch13, step1017]: loss 0.347099
[epoch13, step1018]: loss 0.732739
[epoch13, step1019]: loss 0.668796
[epoch13, step1020]: loss 0.597391
[epoch13, step1021]: loss 0.651768
[epoch13, step1022]: loss 0.745517
[epoch13, step1023]: loss 0.361489
[epoch13, step1024]: loss 0.333565
[epoch13, step1025]: loss 0.744450
[epoch13, step1026]: loss 0.597441
[epoch13, step1027]: loss 0.627695
[epoch13, step1028]: loss 0.445077
[epoch13, step1029]: loss 0.547444
[epoch13, step1030]: loss 0.859555
[epoch13, step1031]: loss 0.807066
[epoch13, step1032]: loss 0.718525
[epoch13, step1033]: loss 0.400753
[epoch13, step1034]: loss 0.307731
[epoch13, step1035]: loss 0.590257
[epoch13, step1036]: loss 0.652009
[epoch13, step1037]: loss 0.511473
[epoch13, step1038]: loss 0.543956
[epoch13, step1039]: loss 0.667357
[epoch13, step1040]: loss 0.283410
[epoch13, step1041]: loss 0.598351
[epoch13, step1042]: loss 0.216578
[epoch13, step1043]: loss 0.508721
[epoch13, step1044]: loss 0.676814
[epoch13, step1045]: loss 0.419901
[epoch13, step1046]: loss 0.577859
[epoch13, step1047]: loss 0.517649
[epoch13, step1048]: loss 0.703219
[epoch13, step1049]: loss 0.656256
[epoch13, step1050]: loss 0.554107
[epoch13, step1051]: loss 0.541120
[epoch13, step1052]: loss 0.217393
[epoch13, step1053]: loss 0.362997
[epoch13, step1054]: loss 0.543052
[epoch13, step1055]: loss 0.766899
[epoch13, step1056]: loss 0.559460
[epoch13, step1057]: loss 0.821000
[epoch13, step1058]: loss 0.497276
[epoch13, step1059]: loss 0.592351
[epoch13, step1060]: loss 0.709569
[epoch13, step1061]: loss 0.495546
[epoch13, step1062]: loss 0.428933
[epoch13, step1063]: loss 0.595185
[epoch13, step1064]: loss 0.545758
[epoch13, step1065]: loss 0.537103
[epoch13, step1066]: loss 0.624735
[epoch13, step1067]: loss 0.384580
[epoch13, step1068]: loss 0.636768
[epoch13, step1069]: loss 0.680399
[epoch13, step1070]: loss 0.570722
[epoch13, step1071]: loss 0.446208
[epoch13, step1072]: loss 0.614626
[epoch13, step1073]: loss 0.389299
[epoch13, step1074]: loss 0.678062
[epoch13, step1075]: loss 0.657686
[epoch13, step1076]: loss 0.762342
[epoch13, step1077]: loss 0.669473
[epoch13, step1078]: loss 0.835867
[epoch13, step1079]: loss 0.478733
[epoch13, step1080]: loss 0.256701
[epoch13, step1081]: loss 0.679500
[epoch13, step1082]: loss 0.459354
[epoch13, step1083]: loss 0.454056
[epoch13, step1084]: loss 0.495185
[epoch13, step1085]: loss 0.555770
[epoch13, step1086]: loss 0.234715
[epoch13, step1087]: loss 0.507374
[epoch13, step1088]: loss 0.631596
[epoch13, step1089]: loss 0.443238
[epoch13, step1090]: loss 0.414586
[epoch13, step1091]: loss 0.661760
[epoch13, step1092]: loss 0.364931
[epoch13, step1093]: loss 0.476306
[epoch13, step1094]: loss 0.623723
[epoch13, step1095]: loss 0.470209
[epoch13, step1096]: loss 0.432745
[epoch13, step1097]: loss 0.287417
[epoch13, step1098]: loss 0.460748
[epoch13, step1099]: loss 0.759631
[epoch13, step1100]: loss 0.698229
[epoch13, step1101]: loss 0.813958
[epoch13, step1102]: loss 0.411609
[epoch13, step1103]: loss 0.554312
[epoch13, step1104]: loss 0.606665
[epoch13, step1105]: loss 0.586893
[epoch13, step1106]: loss 0.593969
[epoch13, step1107]: loss 0.402901
[epoch13, step1108]: loss 0.606622
[epoch13, step1109]: loss 0.714598
[epoch13, step1110]: loss 0.627457
[epoch13, step1111]: loss 0.503221
[epoch13, step1112]: loss 0.556524
[epoch13, step1113]: loss 0.420788
[epoch13, step1114]: loss 0.630736
[epoch13, step1115]: loss 0.614360
[epoch13, step1116]: loss 0.588953
[epoch13, step1117]: loss 0.477079
[epoch13, step1118]: loss 0.571422
[epoch13, step1119]: loss 0.528011
[epoch13, step1120]: loss 0.831280
[epoch13, step1121]: loss 0.451094
[epoch13, step1122]: loss 0.823611
[epoch13, step1123]: loss 0.467499
[epoch13, step1124]: loss 0.712895
[epoch13, step1125]: loss 0.521921
[epoch13, step1126]: loss 0.476522
[epoch13, step1127]: loss 0.717732
[epoch13, step1128]: loss 0.563712
[epoch13, step1129]: loss 0.433881
[epoch13, step1130]: loss 0.499784
[epoch13, step1131]: loss 0.776190
[epoch13, step1132]: loss 0.148083
[epoch13, step1133]: loss 0.303060
[epoch13, step1134]: loss 0.503117
[epoch13, step1135]: loss 0.667711
[epoch13, step1136]: loss 0.353471
[epoch13, step1137]: loss 0.552727
[epoch13, step1138]: loss 0.591374
[epoch13, step1139]: loss 0.737686
[epoch13, step1140]: loss 0.551354
[epoch13, step1141]: loss 0.681813
[epoch13, step1142]: loss 0.723780
[epoch13, step1143]: loss 0.536166
[epoch13, step1144]: loss 0.822558
[epoch13, step1145]: loss 0.683607
[epoch13, step1146]: loss 0.698352
[epoch13, step1147]: loss 0.617344
[epoch13, step1148]: loss 0.496676
[epoch13, step1149]: loss 0.820403
[epoch13, step1150]: loss 0.564718
[epoch13, step1151]: loss 0.301519
[epoch13, step1152]: loss 0.371665
[epoch13, step1153]: loss 0.549014
[epoch13, step1154]: loss 0.654131
[epoch13, step1155]: loss 0.689719
[epoch13, step1156]: loss 0.504363
[epoch13, step1157]: loss 0.669526
[epoch13, step1158]: loss 0.712321
[epoch13, step1159]: loss 0.590082
[epoch13, step1160]: loss 0.436913
[epoch13, step1161]: loss 0.521403
[epoch13, step1162]: loss 0.404568
[epoch13, step1163]: loss 0.246888
[epoch13, step1164]: loss 0.457347
[epoch13, step1165]: loss 0.370099
[epoch13, step1166]: loss 0.522443
[epoch13, step1167]: loss 0.698723
[epoch13, step1168]: loss 0.628944
[epoch13, step1169]: loss 0.667194
[epoch13, step1170]: loss 0.792372
[epoch13, step1171]: loss 0.756758
[epoch13, step1172]: loss 0.558833
[epoch13, step1173]: loss 0.445703
[epoch13, step1174]: loss 0.755857
[epoch13, step1175]: loss 0.588765
[epoch13, step1176]: loss 0.752900
[epoch13, step1177]: loss 0.562812
[epoch13, step1178]: loss 0.566671
[epoch13, step1179]: loss 0.467296
[epoch13, step1180]: loss 0.305616
[epoch13, step1181]: loss 0.417584
[epoch13, step1182]: loss 0.760617
[epoch13, step1183]: loss 0.373284
[epoch13, step1184]: loss 0.612636
[epoch13, step1185]: loss 0.616835
[epoch13, step1186]: loss 0.465298
[epoch13, step1187]: loss 0.547047
[epoch13, step1188]: loss 0.572717
[epoch13, step1189]: loss 0.413433
[epoch13, step1190]: loss 0.672619
[epoch13, step1191]: loss 0.751925
[epoch13, step1192]: loss 0.303646
[epoch13, step1193]: loss 0.721214
[epoch13, step1194]: loss 0.462829
[epoch13, step1195]: loss 0.525903
[epoch13, step1196]: loss 0.301754
[epoch13, step1197]: loss 0.370057
[epoch13, step1198]: loss 0.647575
[epoch13, step1199]: loss 0.546958
[epoch13, step1200]: loss 0.700533
[epoch13, step1201]: loss 0.642899
[epoch13, step1202]: loss 0.556781
[epoch13, step1203]: loss 0.751876
[epoch13, step1204]: loss 0.539492
[epoch13, step1205]: loss 0.341464
[epoch13, step1206]: loss 0.492386
[epoch13, step1207]: loss 0.812130
[epoch13, step1208]: loss 0.567500
[epoch13, step1209]: loss 0.382887
[epoch13, step1210]: loss 0.637524
[epoch13, step1211]: loss 0.531049
[epoch13, step1212]: loss 0.393642
[epoch13, step1213]: loss 0.834511
[epoch13, step1214]: loss 0.793751
[epoch13, step1215]: loss 0.455898
[epoch13, step1216]: loss 0.590197
[epoch13, step1217]: loss 0.550853
[epoch13, step1218]: loss 0.129020
[epoch13, step1219]: loss 0.681194
[epoch13, step1220]: loss 0.536154
[epoch13, step1221]: loss 0.538071
[epoch13, step1222]: loss 0.588035
[epoch13, step1223]: loss 0.426299
[epoch13, step1224]: loss 0.612532
[epoch13, step1225]: loss 0.318130
[epoch13, step1226]: loss 0.618083
[epoch13, step1227]: loss 0.525340
[epoch13, step1228]: loss 0.663247
[epoch13, step1229]: loss 0.703445
[epoch13, step1230]: loss 0.580931
[epoch13, step1231]: loss 0.689734
[epoch13, step1232]: loss 0.480963
[epoch13, step1233]: loss 0.404098
[epoch13, step1234]: loss 0.611547
[epoch13, step1235]: loss 0.276801
[epoch13, step1236]: loss 0.665313
[epoch13, step1237]: loss 0.489432
[epoch13, step1238]: loss 0.483790
[epoch13, step1239]: loss 0.436738
[epoch13, step1240]: loss 0.573536
[epoch13, step1241]: loss 0.580831
[epoch13, step1242]: loss 0.550819
[epoch13, step1243]: loss 0.560614
[epoch13, step1244]: loss 0.489617
[epoch13, step1245]: loss 0.487244
[epoch13, step1246]: loss 0.424095
[epoch13, step1247]: loss 0.450190
[epoch13, step1248]: loss 0.695824
[epoch13, step1249]: loss 0.558301
[epoch13, step1250]: loss 0.565606
[epoch13, step1251]: loss 0.532666
[epoch13, step1252]: loss 0.713453
[epoch13, step1253]: loss 0.376935
[epoch13, step1254]: loss 0.299389
[epoch13, step1255]: loss 0.801004
[epoch13, step1256]: loss 0.647609
[epoch13, step1257]: loss 0.786730
[epoch13, step1258]: loss 0.737161
[epoch13, step1259]: loss 0.415700
[epoch13, step1260]: loss 0.573723
[epoch13, step1261]: loss 0.555025
[epoch13, step1262]: loss 0.557642
[epoch13, step1263]: loss 0.604773
[epoch13, step1264]: loss 0.481571
[epoch13, step1265]: loss 0.748923
[epoch13, step1266]: loss 0.590834
[epoch13, step1267]: loss 0.697871
[epoch13, step1268]: loss 0.480136
[epoch13, step1269]: loss 0.553002
[epoch13, step1270]: loss 0.696013
[epoch13, step1271]: loss 0.673688
[epoch13, step1272]: loss 0.136145
[epoch13, step1273]: loss 0.617145
[epoch13, step1274]: loss 0.644595
[epoch13, step1275]: loss 0.260238
[epoch13, step1276]: loss 0.462740
[epoch13, step1277]: loss 0.620123
[epoch13, step1278]: loss 0.503943
[epoch13, step1279]: loss 0.412416
[epoch13, step1280]: loss 0.381700
[epoch13, step1281]: loss 0.714840
[epoch13, step1282]: loss 0.427631
[epoch13, step1283]: loss 0.453528
[epoch13, step1284]: loss 0.564914
[epoch13, step1285]: loss 0.485049
[epoch13, step1286]: loss 0.484962
[epoch13, step1287]: loss 0.719539
[epoch13, step1288]: loss 0.489945
[epoch13, step1289]: loss 0.548275
[epoch13, step1290]: loss 0.821705
[epoch13, step1291]: loss 0.644897
[epoch13, step1292]: loss 0.658447
[epoch13, step1293]: loss 0.173932
[epoch13, step1294]: loss 0.501611
[epoch13, step1295]: loss 0.815012
[epoch13, step1296]: loss 0.705100
[epoch13, step1297]: loss 0.318028
[epoch13, step1298]: loss 0.332170
[epoch13, step1299]: loss 0.716775
[epoch13, step1300]: loss 0.696563
[epoch13, step1301]: loss 0.818940
[epoch13, step1302]: loss 0.633698
[epoch13, step1303]: loss 0.555931
[epoch13, step1304]: loss 0.590968
[epoch13, step1305]: loss 0.856469
[epoch13, step1306]: loss 0.602454
[epoch13, step1307]: loss 0.373938
[epoch13, step1308]: loss 0.542591
[epoch13, step1309]: loss 0.646015
[epoch13, step1310]: loss 0.232397
[epoch13, step1311]: loss 0.554503
[epoch13, step1312]: loss 0.526487
[epoch13, step1313]: loss 0.411435
[epoch13, step1314]: loss 0.715359
[epoch13, step1315]: loss 0.413316
[epoch13, step1316]: loss 0.642615
[epoch13, step1317]: loss 0.274781
[epoch13, step1318]: loss 0.388426
[epoch13, step1319]: loss 0.478929
[epoch13, step1320]: loss 0.692302
[epoch13, step1321]: loss 0.692488
[epoch13, step1322]: loss 0.471488
[epoch13, step1323]: loss 0.725574
[epoch13, step1324]: loss 0.599006
[epoch13, step1325]: loss 0.588620
[epoch13, step1326]: loss 0.620225
[epoch13, step1327]: loss 0.720877
[epoch13, step1328]: loss 0.749136
[epoch13, step1329]: loss 0.724609
[epoch13, step1330]: loss 0.466484
[epoch13, step1331]: loss 0.564623
[epoch13, step1332]: loss 0.941255
[epoch13, step1333]: loss 0.638016
[epoch13, step1334]: loss 0.681225
[epoch13, step1335]: loss 0.628815
[epoch13, step1336]: loss 0.637271
[epoch13, step1337]: loss 0.584362
[epoch13, step1338]: loss 0.570896
[epoch13, step1339]: loss 0.656835
[epoch13, step1340]: loss 0.549766
[epoch13, step1341]: loss 0.695557
[epoch13, step1342]: loss 0.466833
[epoch13, step1343]: loss 0.580851
[epoch13, step1344]: loss 0.797483
[epoch13, step1345]: loss 0.484880
[epoch13, step1346]: loss 0.486530
[epoch13, step1347]: loss 0.603326
[epoch13, step1348]: loss 0.726865
[epoch13, step1349]: loss 0.421995
[epoch13, step1350]: loss 0.599137
[epoch13, step1351]: loss 0.705020
[epoch13, step1352]: loss 0.718800
[epoch13, step1353]: loss 0.323159
[epoch13, step1354]: loss 0.564311
[epoch13, step1355]: loss 0.629715
[epoch13, step1356]: loss 0.424997
[epoch13, step1357]: loss 0.718208
[epoch13, step1358]: loss 0.464776
[epoch13, step1359]: loss 0.420896
[epoch13, step1360]: loss 0.673014
[epoch13, step1361]: loss 0.602799
[epoch13, step1362]: loss 0.596770
[epoch13, step1363]: loss 0.624875
[epoch13, step1364]: loss 0.436443
[epoch13, step1365]: loss 0.588895
[epoch13, step1366]: loss 0.454265
[epoch13, step1367]: loss 0.667022
[epoch13, step1368]: loss 0.500938
[epoch13, step1369]: loss 0.463478
[epoch13, step1370]: loss 0.784979
[epoch13, step1371]: loss 0.628916
[epoch13, step1372]: loss 0.728761
[epoch13, step1373]: loss 0.467746
[epoch13, step1374]: loss 0.559272
[epoch13, step1375]: loss 0.580067
[epoch13, step1376]: loss 0.661905
[epoch13, step1377]: loss 0.605769
[epoch13, step1378]: loss 0.464781
[epoch13, step1379]: loss 0.655306
[epoch13, step1380]: loss 0.165883
[epoch13, step1381]: loss 0.450619
[epoch13, step1382]: loss 0.595629
[epoch13, step1383]: loss 0.573877
[epoch13, step1384]: loss 0.433589
[epoch13, step1385]: loss 0.530275
[epoch13, step1386]: loss 0.666673
[epoch13, step1387]: loss 0.540067
[epoch13, step1388]: loss 0.642828
[epoch13, step1389]: loss 0.422162
[epoch13, step1390]: loss 0.788018
[epoch13, step1391]: loss 0.689713
[epoch13, step1392]: loss 0.404360
[epoch13, step1393]: loss 0.522536
[epoch13, step1394]: loss 0.659168
[epoch13, step1395]: loss 0.495243
[epoch13, step1396]: loss 0.687863
[epoch13, step1397]: loss 0.469765
[epoch13, step1398]: loss 0.836523
[epoch13, step1399]: loss 0.664687
[epoch13, step1400]: loss 0.754604
[epoch13, step1401]: loss 0.527145
[epoch13, step1402]: loss 0.683957
[epoch13, step1403]: loss 0.431547
[epoch13, step1404]: loss 0.662185
[epoch13, step1405]: loss 0.377112
[epoch13, step1406]: loss 0.708671
[epoch13, step1407]: loss 0.546470
[epoch13, step1408]: loss 0.377207
[epoch13, step1409]: loss 0.664810
[epoch13, step1410]: loss 0.553429
[epoch13, step1411]: loss 0.591619
[epoch13, step1412]: loss 0.688633
[epoch13, step1413]: loss 0.728194
[epoch13, step1414]: loss 0.642038
[epoch13, step1415]: loss 0.670111
[epoch13, step1416]: loss 0.479792
[epoch13, step1417]: loss 0.466574
[epoch13, step1418]: loss 0.610033
[epoch13, step1419]: loss 0.783798
[epoch13, step1420]: loss 0.516779
[epoch13, step1421]: loss 0.391982
[epoch13, step1422]: loss 0.468757
[epoch13, step1423]: loss 0.536981
[epoch13, step1424]: loss 0.500970
[epoch13, step1425]: loss 0.723941
[epoch13, step1426]: loss 0.500768
[epoch13, step1427]: loss 0.532394
[epoch13, step1428]: loss 0.554393
[epoch13, step1429]: loss 0.698384
[epoch13, step1430]: loss 0.457878
[epoch13, step1431]: loss 0.594525
[epoch13, step1432]: loss 0.609977
[epoch13, step1433]: loss 0.425101
[epoch13, step1434]: loss 0.620002
[epoch13, step1435]: loss 0.558489
[epoch13, step1436]: loss 0.546783
[epoch13, step1437]: loss 0.672692
[epoch13, step1438]: loss 0.435301
[epoch13, step1439]: loss 0.566196
[epoch13, step1440]: loss 0.439916
[epoch13, step1441]: loss 0.704828
[epoch13, step1442]: loss 0.563108
[epoch13, step1443]: loss 0.377837
[epoch13, step1444]: loss 0.679929
[epoch13, step1445]: loss 0.427050
[epoch13, step1446]: loss 0.699037
[epoch13, step1447]: loss 0.560115
[epoch13, step1448]: loss 0.471667
[epoch13, step1449]: loss 0.511072
[epoch13, step1450]: loss 0.444058
[epoch13, step1451]: loss 0.443410
[epoch13, step1452]: loss 0.399932
[epoch13, step1453]: loss 0.368302
[epoch13, step1454]: loss 0.610377
[epoch13, step1455]: loss 0.291717
[epoch13, step1456]: loss 0.685922
[epoch13, step1457]: loss 0.406780
[epoch13, step1458]: loss 0.669761
[epoch13, step1459]: loss 0.626874
[epoch13, step1460]: loss 0.390704
[epoch13, step1461]: loss 0.406100
[epoch13, step1462]: loss 0.174454
[epoch13, step1463]: loss 0.747081
[epoch13, step1464]: loss 0.619513
[epoch13, step1465]: loss 0.584935
[epoch13, step1466]: loss 0.652463
[epoch13, step1467]: loss 0.626921
[epoch13, step1468]: loss 0.743734
[epoch13, step1469]: loss 0.876699
[epoch13, step1470]: loss 0.440368
[epoch13, step1471]: loss 0.650371
[epoch13, step1472]: loss 0.696096
[epoch13, step1473]: loss 0.535588
[epoch13, step1474]: loss 0.566270
[epoch13, step1475]: loss 0.622005
[epoch13, step1476]: loss 0.660579
[epoch13, step1477]: loss 0.727307
[epoch13, step1478]: loss 0.621937
[epoch13, step1479]: loss 0.535941
[epoch13, step1480]: loss 0.554323
[epoch13, step1481]: loss 0.538632
[epoch13, step1482]: loss 0.397102
[epoch13, step1483]: loss 0.662195
[epoch13, step1484]: loss 0.270745
[epoch13, step1485]: loss 0.368415
[epoch13, step1486]: loss 0.599706
[epoch13, step1487]: loss 0.656332
[epoch13, step1488]: loss 0.596840
[epoch13, step1489]: loss 0.702478
[epoch13, step1490]: loss 0.775416
[epoch13, step1491]: loss 0.636346
[epoch13, step1492]: loss 0.778683
[epoch13, step1493]: loss 0.614417
[epoch13, step1494]: loss 0.333908
[epoch13, step1495]: loss 0.646444
[epoch13, step1496]: loss 0.654146
[epoch13, step1497]: loss 0.720834
[epoch13, step1498]: loss 0.360461
[epoch13, step1499]: loss 0.299780
[epoch13, step1500]: loss 0.630026
[epoch13, step1501]: loss 0.650527
[epoch13, step1502]: loss 0.683779
[epoch13, step1503]: loss 0.698630
[epoch13, step1504]: loss 0.503684
[epoch13, step1505]: loss 0.740012
[epoch13, step1506]: loss 0.516798
[epoch13, step1507]: loss 0.510429
[epoch13, step1508]: loss 0.634247
[epoch13, step1509]: loss 0.779900
[epoch13, step1510]: loss 0.575885
[epoch13, step1511]: loss 0.653744
[epoch13, step1512]: loss 0.672940
[epoch13, step1513]: loss 0.540732
[epoch13, step1514]: loss 0.574639
[epoch13, step1515]: loss 0.479374
[epoch13, step1516]: loss 0.464147
[epoch13, step1517]: loss 0.455214
[epoch13, step1518]: loss 0.578916
[epoch13, step1519]: loss 0.518777
[epoch13, step1520]: loss 0.562714
[epoch13, step1521]: loss 0.695959
[epoch13, step1522]: loss 0.555382
[epoch13, step1523]: loss 0.598284
[epoch13, step1524]: loss 0.114474
[epoch13, step1525]: loss 0.866787
[epoch13, step1526]: loss 0.783355
[epoch13, step1527]: loss 0.634188
[epoch13, step1528]: loss 0.429026
[epoch13, step1529]: loss 0.445735
[epoch13, step1530]: loss 0.701125
[epoch13, step1531]: loss 0.450153
[epoch13, step1532]: loss 0.622640
[epoch13, step1533]: loss 0.409861
[epoch13, step1534]: loss 0.367251
[epoch13, step1535]: loss 0.563240
[epoch13, step1536]: loss 0.532688
[epoch13, step1537]: loss 0.623340
[epoch13, step1538]: loss 0.496486
[epoch13, step1539]: loss 0.426699
[epoch13, step1540]: loss 0.653326
[epoch13, step1541]: loss 0.442284
[epoch13, step1542]: loss 0.383319
[epoch13, step1543]: loss 0.363084
[epoch13, step1544]: loss 0.489305
[epoch13, step1545]: loss 0.579794
[epoch13, step1546]: loss 0.764674
[epoch13, step1547]: loss 0.525793
[epoch13, step1548]: loss 0.594415
[epoch13, step1549]: loss 0.264897
[epoch13, step1550]: loss 0.572791
[epoch13, step1551]: loss 0.586324
[epoch13, step1552]: loss 0.782842
[epoch13, step1553]: loss 0.529240
[epoch13, step1554]: loss 0.592293
[epoch13, step1555]: loss 0.580096
[epoch13, step1556]: loss 0.711258
[epoch13, step1557]: loss 0.467775
[epoch13, step1558]: loss 0.664764
[epoch13, step1559]: loss 0.495636
[epoch13, step1560]: loss 0.778807
[epoch13, step1561]: loss 0.419500
[epoch13, step1562]: loss 0.540932
[epoch13, step1563]: loss 0.698927
[epoch13, step1564]: loss 0.476009
[epoch13, step1565]: loss 0.646492
[epoch13, step1566]: loss 0.661258
[epoch13, step1567]: loss 0.418167
[epoch13, step1568]: loss 0.624036
[epoch13, step1569]: loss 0.752492
[epoch13, step1570]: loss 0.370702
[epoch13, step1571]: loss 0.678224
[epoch13, step1572]: loss 0.567445
[epoch13, step1573]: loss 0.522859
[epoch13, step1574]: loss 0.599697
[epoch13, step1575]: loss 0.564248
[epoch13, step1576]: loss 0.424430
[epoch13, step1577]: loss 0.675369
[epoch13, step1578]: loss 0.387156
[epoch13, step1579]: loss 0.551767
[epoch13, step1580]: loss 0.577166
[epoch13, step1581]: loss 0.614903
[epoch13, step1582]: loss 0.296810
[epoch13, step1583]: loss 0.502594
[epoch13, step1584]: loss 0.786473
[epoch13, step1585]: loss 0.751507
[epoch13, step1586]: loss 0.747740
[epoch13, step1587]: loss 0.576706
[epoch13, step1588]: loss 0.389320
[epoch13, step1589]: loss 0.486997
[epoch13, step1590]: loss 0.446885
[epoch13, step1591]: loss 0.413217
[epoch13, step1592]: loss 0.550412
[epoch13, step1593]: loss 0.650451
[epoch13, step1594]: loss 0.700412
[epoch13, step1595]: loss 0.651105
[epoch13, step1596]: loss 0.623245
[epoch13, step1597]: loss 0.695017
[epoch13, step1598]: loss 0.492395
[epoch13, step1599]: loss 0.663729
[epoch13, step1600]: loss 0.522858
[epoch13, step1601]: loss 0.377394
[epoch13, step1602]: loss 0.509114
[epoch13, step1603]: loss 0.728808
[epoch13, step1604]: loss 0.530951
[epoch13, step1605]: loss 0.561421
[epoch13, step1606]: loss 0.455866
[epoch13, step1607]: loss 0.642239
[epoch13, step1608]: loss 0.532220
[epoch13, step1609]: loss 0.614712
[epoch13, step1610]: loss 0.778908
[epoch13, step1611]: loss 0.594793
[epoch13, step1612]: loss 0.594373
[epoch13, step1613]: loss 0.720880
[epoch13, step1614]: loss 0.749782
[epoch13, step1615]: loss 0.821085
[epoch13, step1616]: loss 0.508396
[epoch13, step1617]: loss 0.740371
[epoch13, step1618]: loss 0.706310
[epoch13, step1619]: loss 0.462520
[epoch13, step1620]: loss 0.557455
[epoch13, step1621]: loss 0.667552
[epoch13, step1622]: loss 0.698746
[epoch13, step1623]: loss 0.639551
[epoch13, step1624]: loss 0.514592
[epoch13, step1625]: loss 0.538997
[epoch13, step1626]: loss 0.387887
[epoch13, step1627]: loss 0.486095
[epoch13, step1628]: loss 0.507635
[epoch13, step1629]: loss 0.716924
[epoch13, step1630]: loss 0.742325
[epoch13, step1631]: loss 0.831668
[epoch13, step1632]: loss 0.492983
[epoch13, step1633]: loss 0.561450
[epoch13, step1634]: loss 0.670258
[epoch13, step1635]: loss 0.596234
[epoch13, step1636]: loss 0.680161
[epoch13, step1637]: loss 0.414119
[epoch13, step1638]: loss 0.583864
[epoch13, step1639]: loss 0.355502
[epoch13, step1640]: loss 0.200339
[epoch13, step1641]: loss 0.553958
[epoch13, step1642]: loss 0.623942
[epoch13, step1643]: loss 0.471895
[epoch13, step1644]: loss 0.394033
[epoch13, step1645]: loss 0.291012
[epoch13, step1646]: loss 0.632784
[epoch13, step1647]: loss 0.295127
[epoch13, step1648]: loss 0.590632
[epoch13, step1649]: loss 0.494057
[epoch13, step1650]: loss 0.491714
[epoch13, step1651]: loss 0.776611
[epoch13, step1652]: loss 0.535382
[epoch13, step1653]: loss 0.569752
[epoch13, step1654]: loss 0.479974
[epoch13, step1655]: loss 0.475956
[epoch13, step1656]: loss 0.600706
[epoch13, step1657]: loss 0.524382
[epoch13, step1658]: loss 0.499213
[epoch13, step1659]: loss 0.614475
[epoch13, step1660]: loss 0.524924
[epoch13, step1661]: loss 0.433794
[epoch13, step1662]: loss 0.490705
[epoch13, step1663]: loss 0.601833
[epoch13, step1664]: loss 0.658053
[epoch13, step1665]: loss 0.595799
[epoch13, step1666]: loss 0.636152
[epoch13, step1667]: loss 0.568028
[epoch13, step1668]: loss 0.447943
[epoch13, step1669]: loss 0.665213
[epoch13, step1670]: loss 0.761057
[epoch13, step1671]: loss 0.355360
[epoch13, step1672]: loss 0.588126
[epoch13, step1673]: loss 0.543978
[epoch13, step1674]: loss 0.636246
[epoch13, step1675]: loss 0.787820
[epoch13, step1676]: loss 0.669891
[epoch13, step1677]: loss 0.714111
[epoch13, step1678]: loss 0.638650
[epoch13, step1679]: loss 0.324056
[epoch13, step1680]: loss 0.599727
[epoch13, step1681]: loss 0.337480
[epoch13, step1682]: loss 0.334916
[epoch13, step1683]: loss 0.497170
[epoch13, step1684]: loss 0.204797
[epoch13, step1685]: loss 0.657211
[epoch13, step1686]: loss 0.309353
[epoch13, step1687]: loss 0.501440
[epoch13, step1688]: loss 0.210256
[epoch13, step1689]: loss 0.638896
[epoch13, step1690]: loss 0.751653
[epoch13, step1691]: loss 0.554681
[epoch13, step1692]: loss 0.372320
[epoch13, step1693]: loss 0.480380
[epoch13, step1694]: loss 0.420831
[epoch13, step1695]: loss 0.567667
[epoch13, step1696]: loss 0.643264
[epoch13, step1697]: loss 0.565355
[epoch13, step1698]: loss 0.359106
[epoch13, step1699]: loss 0.444433
[epoch13, step1700]: loss 0.520004
[epoch13, step1701]: loss 0.705719
[epoch13, step1702]: loss 0.457013
[epoch13, step1703]: loss 0.674635
[epoch13, step1704]: loss 0.335116
[epoch13, step1705]: loss 0.428126
[epoch13, step1706]: loss 0.595710
[epoch13, step1707]: loss 0.812118
[epoch13, step1708]: loss 0.753086
[epoch13, step1709]: loss 0.367127
[epoch13, step1710]: loss 0.390260
[epoch13, step1711]: loss 0.622470
[epoch13, step1712]: loss 0.402719
[epoch13, step1713]: loss 0.528691
[epoch13, step1714]: loss 0.425288
[epoch13, step1715]: loss 0.323793
[epoch13, step1716]: loss 0.454093
[epoch13, step1717]: loss 0.569400
[epoch13, step1718]: loss 0.188309
[epoch13, step1719]: loss 0.586711
[epoch13, step1720]: loss 0.692351
[epoch13, step1721]: loss 0.533879
[epoch13, step1722]: loss 0.288410
[epoch13, step1723]: loss 0.754417
[epoch13, step1724]: loss 0.694597
[epoch13, step1725]: loss 0.644500
[epoch13, step1726]: loss 0.647730
[epoch13, step1727]: loss 0.552429
[epoch13, step1728]: loss 0.354247
[epoch13, step1729]: loss 0.783889
[epoch13, step1730]: loss 0.654646
[epoch13, step1731]: loss 0.502968
[epoch13, step1732]: loss 0.315253
[epoch13, step1733]: loss 0.547196
[epoch13, step1734]: loss 0.402569
[epoch13, step1735]: loss 0.830098
[epoch13, step1736]: loss 0.255463
[epoch13, step1737]: loss 0.712719
[epoch13, step1738]: loss 0.467773
[epoch13, step1739]: loss 0.676155
[epoch13, step1740]: loss 0.667447
[epoch13, step1741]: loss 0.516355
[epoch13, step1742]: loss 0.540844
[epoch13, step1743]: loss 0.571015
[epoch13, step1744]: loss 0.780533
[epoch13, step1745]: loss 0.375335
[epoch13, step1746]: loss 0.173812
[epoch13, step1747]: loss 0.690241
[epoch13, step1748]: loss 0.494759
[epoch13, step1749]: loss 0.713803
[epoch13, step1750]: loss 0.473801
[epoch13, step1751]: loss 0.705432
[epoch13, step1752]: loss 0.812151
[epoch13, step1753]: loss 0.458495
[epoch13, step1754]: loss 0.411026
[epoch13, step1755]: loss 0.748940
[epoch13, step1756]: loss 0.538882
[epoch13, step1757]: loss 0.555838
[epoch13, step1758]: loss 0.445894
[epoch13, step1759]: loss 0.642386
[epoch13, step1760]: loss 0.186109
[epoch13, step1761]: loss 0.543425
[epoch13, step1762]: loss 0.436220
[epoch13, step1763]: loss 0.581107
[epoch13, step1764]: loss 0.627046
[epoch13, step1765]: loss 0.583038
[epoch13, step1766]: loss 0.386721
[epoch13, step1767]: loss 0.419445
[epoch13, step1768]: loss 0.623682
[epoch13, step1769]: loss 0.706688
[epoch13, step1770]: loss 0.429040
[epoch13, step1771]: loss 0.379998
[epoch13, step1772]: loss 0.577679
[epoch13, step1773]: loss 0.468396
[epoch13, step1774]: loss 0.408987
[epoch13, step1775]: loss 0.616840
[epoch13, step1776]: loss 0.397909
[epoch13, step1777]: loss 0.590961
[epoch13, step1778]: loss 0.488067
[epoch13, step1779]: loss 0.617654
[epoch13, step1780]: loss 0.713698
[epoch13, step1781]: loss 0.558667
[epoch13, step1782]: loss 0.661733
[epoch13, step1783]: loss 0.440263
[epoch13, step1784]: loss 0.633009
[epoch13, step1785]: loss 0.404120
[epoch13, step1786]: loss 0.534553
[epoch13, step1787]: loss 0.745354
[epoch13, step1788]: loss 0.527716
[epoch13, step1789]: loss 0.453539
[epoch13, step1790]: loss 0.679543
[epoch13, step1791]: loss 0.619605
[epoch13, step1792]: loss 0.595197
[epoch13, step1793]: loss 0.831302
[epoch13, step1794]: loss 0.297907
[epoch13, step1795]: loss 0.640379
[epoch13, step1796]: loss 0.544750
[epoch13, step1797]: loss 0.584801
[epoch13, step1798]: loss 0.402292
[epoch13, step1799]: loss 0.513435
[epoch13, step1800]: loss 0.187961
[epoch13, step1801]: loss 0.724590
[epoch13, step1802]: loss 0.866920
[epoch13, step1803]: loss 0.599338
[epoch13, step1804]: loss 0.375232
[epoch13, step1805]: loss 0.368228
[epoch13, step1806]: loss 0.561400
[epoch13, step1807]: loss 0.695180
[epoch13, step1808]: loss 0.579756
[epoch13, step1809]: loss 0.742494
[epoch13, step1810]: loss 0.628613
[epoch13, step1811]: loss 0.597771
[epoch13, step1812]: loss 0.521624
[epoch13, step1813]: loss 0.280486
[epoch13, step1814]: loss 0.573287
[epoch13, step1815]: loss 0.303038
[epoch13, step1816]: loss 0.770785
[epoch13, step1817]: loss 0.650969
[epoch13, step1818]: loss 0.485839
[epoch13, step1819]: loss 0.543111
[epoch13, step1820]: loss 0.533878
[epoch13, step1821]: loss 0.634574
[epoch13, step1822]: loss 0.566432
[epoch13, step1823]: loss 0.644105
[epoch13, step1824]: loss 0.664267
[epoch13, step1825]: loss 0.708891
[epoch13, step1826]: loss 0.524959
[epoch13, step1827]: loss 0.709001
[epoch13, step1828]: loss 0.576266
[epoch13, step1829]: loss 0.462490
[epoch13, step1830]: loss 0.721938
[epoch13, step1831]: loss 0.495125
[epoch13, step1832]: loss 0.648657
[epoch13, step1833]: loss 0.570418
[epoch13, step1834]: loss 0.589610
[epoch13, step1835]: loss 0.444597
[epoch13, step1836]: loss 0.461553
[epoch13, step1837]: loss 0.646106
[epoch13, step1838]: loss 0.850415
[epoch13, step1839]: loss 0.421168
[epoch13, step1840]: loss 0.427184
[epoch13, step1841]: loss 0.498728
[epoch13, step1842]: loss 0.560204
[epoch13, step1843]: loss 0.499573
[epoch13, step1844]: loss 0.409437
[epoch13, step1845]: loss 0.468675
[epoch13, step1846]: loss 0.585319
[epoch13, step1847]: loss 0.632886
[epoch13, step1848]: loss 0.709586
[epoch13, step1849]: loss 0.819760
[epoch13, step1850]: loss 0.680659
[epoch13, step1851]: loss 0.322233
[epoch13, step1852]: loss 0.614141
[epoch13, step1853]: loss 0.581761
[epoch13, step1854]: loss 0.630331
[epoch13, step1855]: loss 0.592008
[epoch13, step1856]: loss 0.624213
[epoch13, step1857]: loss 0.675984
[epoch13, step1858]: loss 0.459791
[epoch13, step1859]: loss 0.430667
[epoch13, step1860]: loss 0.385445
[epoch13, step1861]: loss 0.616070
[epoch13, step1862]: loss 0.780545
[epoch13, step1863]: loss 0.344933
[epoch13, step1864]: loss 0.551279
[epoch13, step1865]: loss 0.521927
[epoch13, step1866]: loss 0.534064
[epoch13, step1867]: loss 0.194055
[epoch13, step1868]: loss 0.447218
[epoch13, step1869]: loss 0.118570
[epoch13, step1870]: loss 0.214336
[epoch13, step1871]: loss 0.556093
[epoch13, step1872]: loss 0.607662
[epoch13, step1873]: loss 0.593985
[epoch13, step1874]: loss 0.533778
[epoch13, step1875]: loss 0.832421
[epoch13, step1876]: loss 0.733796
[epoch13, step1877]: loss 0.567005
[epoch13, step1878]: loss 0.318463
[epoch13, step1879]: loss 0.531584
[epoch13, step1880]: loss 0.743603
[epoch13, step1881]: loss 0.461577
[epoch13, step1882]: loss 0.672208
[epoch13, step1883]: loss 0.626760
[epoch13, step1884]: loss 0.663036
[epoch13, step1885]: loss 0.308748
[epoch13, step1886]: loss 0.571227
[epoch13, step1887]: loss 0.366032
[epoch13, step1888]: loss 0.501707
[epoch13, step1889]: loss 0.571956
[epoch13, step1890]: loss 0.600180
[epoch13, step1891]: loss 0.839021
[epoch13, step1892]: loss 0.273479
[epoch13, step1893]: loss 0.511639
[epoch13, step1894]: loss 0.399798
[epoch13, step1895]: loss 0.583692
[epoch13, step1896]: loss 0.703697
[epoch13, step1897]: loss 0.384942
[epoch13, step1898]: loss 0.775956
[epoch13, step1899]: loss 0.633684
[epoch13, step1900]: loss 0.416283
[epoch13, step1901]: loss 0.623221
[epoch13, step1902]: loss 0.610417
[epoch13, step1903]: loss 0.706737
[epoch13, step1904]: loss 0.579280
[epoch13, step1905]: loss 0.645353
[epoch13, step1906]: loss 0.561822
[epoch13, step1907]: loss 0.709860
[epoch13, step1908]: loss 0.755336
[epoch13, step1909]: loss 0.578135
[epoch13, step1910]: loss 0.268046
[epoch13, step1911]: loss 0.614017
[epoch13, step1912]: loss 0.636867
[epoch13, step1913]: loss 0.620313
[epoch13, step1914]: loss 0.480108
[epoch13, step1915]: loss 0.401205
[epoch13, step1916]: loss 0.309027
[epoch13, step1917]: loss 0.471826
[epoch13, step1918]: loss 0.379017
[epoch13, step1919]: loss 0.600109
[epoch13, step1920]: loss 0.491639
[epoch13, step1921]: loss 0.745004
[epoch13, step1922]: loss 0.603652
[epoch13, step1923]: loss 0.735894
[epoch13, step1924]: loss 0.243492
[epoch13, step1925]: loss 0.703042
[epoch13, step1926]: loss 0.572840
[epoch13, step1927]: loss 0.362508
[epoch13, step1928]: loss 0.554372
[epoch13, step1929]: loss 0.443898
[epoch13, step1930]: loss 0.602720
[epoch13, step1931]: loss 0.717568
[epoch13, step1932]: loss 0.812553
[epoch13, step1933]: loss 0.505035
[epoch13, step1934]: loss 0.491930
[epoch13, step1935]: loss 0.551932
[epoch13, step1936]: loss 0.291576
[epoch13, step1937]: loss 0.632828
[epoch13, step1938]: loss 0.561949
[epoch13, step1939]: loss 0.540502
[epoch13, step1940]: loss 0.480431
[epoch13, step1941]: loss 0.591635
[epoch13, step1942]: loss 0.610605
[epoch13, step1943]: loss 0.726683
[epoch13, step1944]: loss 0.575817
[epoch13, step1945]: loss 0.584270
[epoch13, step1946]: loss 0.708871
[epoch13, step1947]: loss 0.547821
[epoch13, step1948]: loss 0.359900
[epoch13, step1949]: loss 0.622226
[epoch13, step1950]: loss 0.454423
[epoch13, step1951]: loss 0.569784
[epoch13, step1952]: loss 0.394969
[epoch13, step1953]: loss 0.800625
[epoch13, step1954]: loss 0.801935
[epoch13, step1955]: loss 0.547688
[epoch13, step1956]: loss 0.223967
[epoch13, step1957]: loss 0.458655
[epoch13, step1958]: loss 0.448436
[epoch13, step1959]: loss 0.432977
[epoch13, step1960]: loss 0.490171
[epoch13, step1961]: loss 0.183857
[epoch13, step1962]: loss 0.624216
[epoch13, step1963]: loss 0.546374
[epoch13, step1964]: loss 0.624122
[epoch13, step1965]: loss 0.632665
[epoch13, step1966]: loss 0.692970
[epoch13, step1967]: loss 0.466253
[epoch13, step1968]: loss 0.406744
[epoch13, step1969]: loss 0.529819
[epoch13, step1970]: loss 0.518025
[epoch13, step1971]: loss 0.712522
[epoch13, step1972]: loss 0.480999
[epoch13, step1973]: loss 0.443745
[epoch13, step1974]: loss 0.610720
[epoch13, step1975]: loss 0.684408
[epoch13, step1976]: loss 0.590115
[epoch13, step1977]: loss 0.475419
[epoch13, step1978]: loss 0.325558
[epoch13, step1979]: loss 0.786572
[epoch13, step1980]: loss 0.780856
[epoch13, step1981]: loss 0.435706
[epoch13, step1982]: loss 0.608872
[epoch13, step1983]: loss 0.513094
[epoch13, step1984]: loss 0.577320
[epoch13, step1985]: loss 0.577880
[epoch13, step1986]: loss 0.481411
[epoch13, step1987]: loss 0.603227
[epoch13, step1988]: loss 0.599605
[epoch13, step1989]: loss 0.336112
[epoch13, step1990]: loss 0.617445
[epoch13, step1991]: loss 0.590142
[epoch13, step1992]: loss 0.485589
[epoch13, step1993]: loss 0.659136
[epoch13, step1994]: loss 0.365811
[epoch13, step1995]: loss 0.684502
[epoch13, step1996]: loss 0.594859
[epoch13, step1997]: loss 0.590946
[epoch13, step1998]: loss 0.703509
[epoch13, step1999]: loss 0.800466
[epoch13, step2000]: loss 0.285441
[epoch13, step2001]: loss 0.499136
[epoch13, step2002]: loss 0.622341
[epoch13, step2003]: loss 0.583579
[epoch13, step2004]: loss 0.558394
[epoch13, step2005]: loss 0.453202
[epoch13, step2006]: loss 0.761604
[epoch13, step2007]: loss 0.733050
[epoch13, step2008]: loss 0.677494
[epoch13, step2009]: loss 0.634129
[epoch13, step2010]: loss 0.631846
[epoch13, step2011]: loss 0.642910
[epoch13, step2012]: loss 0.729660
[epoch13, step2013]: loss 0.750574
[epoch13, step2014]: loss 0.537068
[epoch13, step2015]: loss 0.445754
[epoch13, step2016]: loss 0.673209
[epoch13, step2017]: loss 0.355994
[epoch13, step2018]: loss 0.555554
[epoch13, step2019]: loss 0.490973
[epoch13, step2020]: loss 0.592503
[epoch13, step2021]: loss 0.393965
[epoch13, step2022]: loss 0.636608
[epoch13, step2023]: loss 0.518251
[epoch13, step2024]: loss 0.609703
[epoch13, step2025]: loss 0.541304
[epoch13, step2026]: loss 0.806284
[epoch13, step2027]: loss 0.761562
[epoch13, step2028]: loss 0.724966
[epoch13, step2029]: loss 0.642466
[epoch13, step2030]: loss 0.460591
[epoch13, step2031]: loss 0.511843
[epoch13, step2032]: loss 0.579177
[epoch13, step2033]: loss 0.311542
[epoch13, step2034]: loss 0.756072
[epoch13, step2035]: loss 0.658606
[epoch13, step2036]: loss 0.550143
[epoch13, step2037]: loss 0.469060
[epoch13, step2038]: loss 0.703164
[epoch13, step2039]: loss 0.663299
[epoch13, step2040]: loss 0.669344
[epoch13, step2041]: loss 0.825480
[epoch13, step2042]: loss 0.395251
[epoch13, step2043]: loss 0.617233
[epoch13, step2044]: loss 0.589626
[epoch13, step2045]: loss 0.632569
[epoch13, step2046]: loss 0.575494
[epoch13, step2047]: loss 0.443880
[epoch13, step2048]: loss 0.652104
[epoch13, step2049]: loss 0.521952
[epoch13, step2050]: loss 0.534563
[epoch13, step2051]: loss 0.552931
[epoch13, step2052]: loss 0.571700
[epoch13, step2053]: loss 0.541098
[epoch13, step2054]: loss 0.586963
[epoch13, step2055]: loss 0.661451
[epoch13, step2056]: loss 0.624136
[epoch13, step2057]: loss 0.619187
[epoch13, step2058]: loss 0.546610
[epoch13, step2059]: loss 0.358094
[epoch13, step2060]: loss 0.503067
[epoch13, step2061]: loss 0.596603
[epoch13, step2062]: loss 0.537335
[epoch13, step2063]: loss 0.344812
[epoch13, step2064]: loss 0.583551
[epoch13, step2065]: loss 0.582742
[epoch13, step2066]: loss 0.602119
[epoch13, step2067]: loss 0.773021
[epoch13, step2068]: loss 0.555441
[epoch13, step2069]: loss 0.576700
[epoch13, step2070]: loss 0.719512
[epoch13, step2071]: loss 0.430288
[epoch13, step2072]: loss 0.647530
[epoch13, step2073]: loss 0.585872
[epoch13, step2074]: loss 0.474852
[epoch13, step2075]: loss 0.579957
[epoch13, step2076]: loss 0.430676
[epoch13, step2077]: loss 0.695637
[epoch13, step2078]: loss 0.582784
[epoch13, step2079]: loss 0.680103
[epoch13, step2080]: loss 0.638789
[epoch13, step2081]: loss 0.587538
[epoch13, step2082]: loss 0.672790
[epoch13, step2083]: loss 0.407576
[epoch13, step2084]: loss 0.751396
[epoch13, step2085]: loss 0.546075
[epoch13, step2086]: loss 0.658474
[epoch13, step2087]: loss 0.630029
[epoch13, step2088]: loss 0.593696
[epoch13, step2089]: loss 0.637726
[epoch13, step2090]: loss 0.522686
[epoch13, step2091]: loss 0.605804
[epoch13, step2092]: loss 0.747901
[epoch13, step2093]: loss 0.494638
[epoch13, step2094]: loss 0.654613
[epoch13, step2095]: loss 0.356984
[epoch13, step2096]: loss 0.338412
[epoch13, step2097]: loss 0.575224
[epoch13, step2098]: loss 0.460067
[epoch13, step2099]: loss 0.714425
[epoch13, step2100]: loss 0.515381
[epoch13, step2101]: loss 0.726260
[epoch13, step2102]: loss 0.527007
[epoch13, step2103]: loss 0.461002
[epoch13, step2104]: loss 0.557682
[epoch13, step2105]: loss 0.879217
[epoch13, step2106]: loss 0.720835
[epoch13, step2107]: loss 0.795865
[epoch13, step2108]: loss 0.461601
[epoch13, step2109]: loss 0.293971
[epoch13, step2110]: loss 0.627634
[epoch13, step2111]: loss 0.442326
[epoch13, step2112]: loss 0.523876
[epoch13, step2113]: loss 0.722875
[epoch13, step2114]: loss 0.502143
[epoch13, step2115]: loss 0.369163
[epoch13, step2116]: loss 0.538097
[epoch13, step2117]: loss 0.363920
[epoch13, step2118]: loss 0.561629
[epoch13, step2119]: loss 0.644631
[epoch13, step2120]: loss 0.649492
[epoch13, step2121]: loss 0.465228
[epoch13, step2122]: loss 0.547853
[epoch13, step2123]: loss 0.673445
[epoch13, step2124]: loss 0.295849
[epoch13, step2125]: loss 0.650183
[epoch13, step2126]: loss 0.704260
[epoch13, step2127]: loss 0.642112
[epoch13, step2128]: loss 0.274276
[epoch13, step2129]: loss 0.592871
[epoch13, step2130]: loss 0.404991
[epoch13, step2131]: loss 0.456588
[epoch13, step2132]: loss 0.586794
[epoch13, step2133]: loss 0.257478
[epoch13, step2134]: loss 0.437675
[epoch13, step2135]: loss 0.532213
[epoch13, step2136]: loss 0.586808
[epoch13, step2137]: loss 0.335040
[epoch13, step2138]: loss 0.472397
[epoch13, step2139]: loss 0.433407
[epoch13, step2140]: loss 0.624728
[epoch13, step2141]: loss 0.733172
[epoch13, step2142]: loss 0.616833
[epoch13, step2143]: loss 0.369878
[epoch13, step2144]: loss 0.569173
[epoch13, step2145]: loss 0.507842
[epoch13, step2146]: loss 0.534725
[epoch13, step2147]: loss 0.681537
[epoch13, step2148]: loss 0.715935
[epoch13, step2149]: loss 0.671749
[epoch13, step2150]: loss 0.247244
[epoch13, step2151]: loss 0.622045
[epoch13, step2152]: loss 0.559379
[epoch13, step2153]: loss 0.512826
[epoch13, step2154]: loss 0.635721
[epoch13, step2155]: loss 0.654710
[epoch13, step2156]: loss 0.669657
[epoch13, step2157]: loss 0.606383
[epoch13, step2158]: loss 0.494823
[epoch13, step2159]: loss 0.583583
[epoch13, step2160]: loss 0.537958
[epoch13, step2161]: loss 0.762046
[epoch13, step2162]: loss 0.497809
[epoch13, step2163]: loss 0.516817
[epoch13, step2164]: loss 0.408968
[epoch13, step2165]: loss 0.473499
[epoch13, step2166]: loss 0.657933
[epoch13, step2167]: loss 0.845213
[epoch13, step2168]: loss 0.706794
[epoch13, step2169]: loss 0.479944
[epoch13, step2170]: loss 0.222660
[epoch13, step2171]: loss 0.413481
[epoch13, step2172]: loss 0.495894
[epoch13, step2173]: loss 0.597417
[epoch13, step2174]: loss 0.360686
[epoch13, step2175]: loss 0.524542
[epoch13, step2176]: loss 0.452284
[epoch13, step2177]: loss 0.294365
[epoch13, step2178]: loss 0.299436
[epoch13, step2179]: loss 0.639837
[epoch13, step2180]: loss 0.439131
[epoch13, step2181]: loss 0.354295
[epoch13, step2182]: loss 0.342942
[epoch13, step2183]: loss 0.517168
[epoch13, step2184]: loss 0.626909
[epoch13, step2185]: loss 0.255232
[epoch13, step2186]: loss 0.678798
[epoch13, step2187]: loss 0.695200
[epoch13, step2188]: loss 0.487630
[epoch13, step2189]: loss 0.590421
[epoch13, step2190]: loss 0.615279
[epoch13, step2191]: loss 0.485410
[epoch13, step2192]: loss 0.469758
[epoch13, step2193]: loss 0.560961
[epoch13, step2194]: loss 0.121323
[epoch13, step2195]: loss 0.633542
[epoch13, step2196]: loss 0.610345
[epoch13, step2197]: loss 0.555793
[epoch13, step2198]: loss 0.667976
[epoch13, step2199]: loss 0.705694
[epoch13, step2200]: loss 0.437756
[epoch13, step2201]: loss 0.599868
[epoch13, step2202]: loss 0.480858
[epoch13, step2203]: loss 0.569422
[epoch13, step2204]: loss 0.425426
[epoch13, step2205]: loss 0.307333
[epoch13, step2206]: loss 0.491800
[epoch13, step2207]: loss 0.343646
[epoch13, step2208]: loss 0.487827
[epoch13, step2209]: loss 0.535874
[epoch13, step2210]: loss 0.636592
[epoch13, step2211]: loss 0.614361
[epoch13, step2212]: loss 0.607003
[epoch13, step2213]: loss 0.736555
[epoch13, step2214]: loss 0.738708
[epoch13, step2215]: loss 0.432780
[epoch13, step2216]: loss 0.454792
[epoch13, step2217]: loss 0.620535
[epoch13, step2218]: loss 0.691903
[epoch13, step2219]: loss 0.372087
[epoch13, step2220]: loss 0.461048
[epoch13, step2221]: loss 0.846401
[epoch13, step2222]: loss 0.402775
[epoch13, step2223]: loss 0.543832
[epoch13, step2224]: loss 0.436092
[epoch13, step2225]: loss 0.584372
[epoch13, step2226]: loss 0.363473
[epoch13, step2227]: loss 0.443166
[epoch13, step2228]: loss 0.505814
[epoch13, step2229]: loss 0.406636
[epoch13, step2230]: loss 0.653281
[epoch13, step2231]: loss 0.680225
[epoch13, step2232]: loss 0.466840
[epoch13, step2233]: loss 0.712575
[epoch13, step2234]: loss 0.312590
[epoch13, step2235]: loss 0.423794
[epoch13, step2236]: loss 0.653137
[epoch13, step2237]: loss 0.646693
[epoch13, step2238]: loss 0.511245
[epoch13, step2239]: loss 0.703439
[epoch13, step2240]: loss 0.481013
[epoch13, step2241]: loss 0.444545
[epoch13, step2242]: loss 0.487122
[epoch13, step2243]: loss 0.580260
[epoch13, step2244]: loss 0.581395
[epoch13, step2245]: loss 0.402271
[epoch13, step2246]: loss 0.693025
[epoch13, step2247]: loss 0.321239
[epoch13, step2248]: loss 0.536143
[epoch13, step2249]: loss 0.253901
[epoch13, step2250]: loss 0.554394
[epoch13, step2251]: loss 0.603949
[epoch13, step2252]: loss 0.659464
[epoch13, step2253]: loss 0.528426
[epoch13, step2254]: loss 0.499050
[epoch13, step2255]: loss 0.608940
[epoch13, step2256]: loss 0.625272
[epoch13, step2257]: loss 0.762062
[epoch13, step2258]: loss 0.620045
[epoch13, step2259]: loss 0.751779
[epoch13, step2260]: loss 0.530456
[epoch13, step2261]: loss 0.686580
[epoch13, step2262]: loss 0.617289
[epoch13, step2263]: loss 0.581833
[epoch13, step2264]: loss 0.614460
[epoch13, step2265]: loss 0.625367
[epoch13, step2266]: loss 0.353158
[epoch13, step2267]: loss 0.480486
[epoch13, step2268]: loss 0.509855
[epoch13, step2269]: loss 0.456354
[epoch13, step2270]: loss 0.578238
[epoch13, step2271]: loss 0.702298
[epoch13, step2272]: loss 0.642666
[epoch13, step2273]: loss 0.723036
[epoch13, step2274]: loss 0.550817
[epoch13, step2275]: loss 0.745484
[epoch13, step2276]: loss 0.446241
[epoch13, step2277]: loss 0.754547
[epoch13, step2278]: loss 0.767854
[epoch13, step2279]: loss 0.531363
[epoch13, step2280]: loss 0.569964
[epoch13, step2281]: loss 0.305389
[epoch13, step2282]: loss 0.599034
[epoch13, step2283]: loss 0.875152
[epoch13, step2284]: loss 0.552896
[epoch13, step2285]: loss 0.502884
[epoch13, step2286]: loss 0.419041
[epoch13, step2287]: loss 0.538653
[epoch13, step2288]: loss 0.503308
[epoch13, step2289]: loss 0.418193
[epoch13, step2290]: loss 0.683517
[epoch13, step2291]: loss 0.718216
[epoch13, step2292]: loss 0.652384
[epoch13, step2293]: loss 0.491613
[epoch13, step2294]: loss 0.571260
[epoch13, step2295]: loss 0.678171
[epoch13, step2296]: loss 0.613597
[epoch13, step2297]: loss 0.787008
[epoch13, step2298]: loss 0.405917
[epoch13, step2299]: loss 0.452739
[epoch13, step2300]: loss 0.648554
[epoch13, step2301]: loss 0.644069
[epoch13, step2302]: loss 0.500097
[epoch13, step2303]: loss 0.369754
[epoch13, step2304]: loss 0.604766
[epoch13, step2305]: loss 0.602897
[epoch13, step2306]: loss 0.698532
[epoch13, step2307]: loss 0.738928
[epoch13, step2308]: loss 0.314726
[epoch13, step2309]: loss 0.681374
[epoch13, step2310]: loss 0.560822
[epoch13, step2311]: loss 0.767791
[epoch13, step2312]: loss 0.503123
[epoch13, step2313]: loss 0.401226
[epoch13, step2314]: loss 0.478804
[epoch13, step2315]: loss 0.688446
[epoch13, step2316]: loss 0.385657
[epoch13, step2317]: loss 0.792163
[epoch13, step2318]: loss 0.381548
[epoch13, step2319]: loss 0.618621
[epoch13, step2320]: loss 0.507156
[epoch13, step2321]: loss 0.688198
[epoch13, step2322]: loss 0.511269
[epoch13, step2323]: loss 0.563345
[epoch13, step2324]: loss 0.768173
[epoch13, step2325]: loss 0.763307
[epoch13, step2326]: loss 0.232341
[epoch13, step2327]: loss 0.716116
[epoch13, step2328]: loss 0.843764
[epoch13, step2329]: loss 0.601011
[epoch13, step2330]: loss 0.574941
[epoch13, step2331]: loss 0.330365
[epoch13, step2332]: loss 0.586687
[epoch13, step2333]: loss 0.690402
[epoch13, step2334]: loss 0.524410
[epoch13, step2335]: loss 0.594118
[epoch13, step2336]: loss 0.620261
[epoch13, step2337]: loss 0.594982
[epoch13, step2338]: loss 0.601135
[epoch13, step2339]: loss 0.469843
[epoch13, step2340]: loss 0.472572
[epoch13, step2341]: loss 0.748350
[epoch13, step2342]: loss 0.549701
[epoch13, step2343]: loss 0.502494
[epoch13, step2344]: loss 0.635885
[epoch13, step2345]: loss 0.692791
[epoch13, step2346]: loss 0.534304
[epoch13, step2347]: loss 0.519194
[epoch13, step2348]: loss 0.661098
[epoch13, step2349]: loss 0.633214
[epoch13, step2350]: loss 0.594531
[epoch13, step2351]: loss 0.819113
[epoch13, step2352]: loss 0.482089
[epoch13, step2353]: loss 0.555231
[epoch13, step2354]: loss 0.518404
[epoch13, step2355]: loss 0.573526
[epoch13, step2356]: loss 0.650191
[epoch13, step2357]: loss 0.494280
[epoch13, step2358]: loss 0.639692
[epoch13, step2359]: loss 0.584329
[epoch13, step2360]: loss 0.344298
[epoch13, step2361]: loss 0.665737
[epoch13, step2362]: loss 0.497427
[epoch13, step2363]: loss 0.745993
[epoch13, step2364]: loss 0.537335
[epoch13, step2365]: loss 0.511942
[epoch13, step2366]: loss 0.629009
[epoch13, step2367]: loss 0.477328
[epoch13, step2368]: loss 0.343132
[epoch13, step2369]: loss 0.497727
[epoch13, step2370]: loss 0.344474
[epoch13, step2371]: loss 0.665809
[epoch13, step2372]: loss 0.731088
[epoch13, step2373]: loss 0.515554
[epoch13, step2374]: loss 0.380608
[epoch13, step2375]: loss 0.684214
[epoch13, step2376]: loss 0.610745
[epoch13, step2377]: loss 0.725235
[epoch13, step2378]: loss 0.319542
[epoch13, step2379]: loss 0.666759
[epoch13, step2380]: loss 0.703066
[epoch13, step2381]: loss 0.600591
[epoch13, step2382]: loss 0.643797
[epoch13, step2383]: loss 0.639053
[epoch13, step2384]: loss 0.396881
[epoch13, step2385]: loss 0.494365
[epoch13, step2386]: loss 0.444390
[epoch13, step2387]: loss 0.539790
[epoch13, step2388]: loss 0.671625
[epoch13, step2389]: loss 0.122751
[epoch13, step2390]: loss 0.428189
[epoch13, step2391]: loss 0.618845
[epoch13, step2392]: loss 0.454777
[epoch13, step2393]: loss 0.440187
[epoch13, step2394]: loss 0.661918
[epoch13, step2395]: loss 0.619922
[epoch13, step2396]: loss 0.553298
[epoch13, step2397]: loss 0.779131
[epoch13, step2398]: loss 0.637551
[epoch13, step2399]: loss 0.550226
[epoch13, step2400]: loss 0.589882
[epoch13, step2401]: loss 0.712790
[epoch13, step2402]: loss 0.717163
[epoch13, step2403]: loss 0.569222
[epoch13, step2404]: loss 0.579073
[epoch13, step2405]: loss 0.581545
[epoch13, step2406]: loss 0.451351
[epoch13, step2407]: loss 0.601145
[epoch13, step2408]: loss 0.588095
[epoch13, step2409]: loss 0.608718
[epoch13, step2410]: loss 0.349748
[epoch13, step2411]: loss 0.522325
[epoch13, step2412]: loss 0.657862
[epoch13, step2413]: loss 0.472762
[epoch13, step2414]: loss 0.352624
[epoch13, step2415]: loss 0.420743
[epoch13, step2416]: loss 0.552834
[epoch13, step2417]: loss 0.521337
[epoch13, step2418]: loss 0.737207
[epoch13, step2419]: loss 0.454731
[epoch13, step2420]: loss 0.700610
[epoch13, step2421]: loss 0.808249
[epoch13, step2422]: loss 0.356180
[epoch13, step2423]: loss 0.514988
[epoch13, step2424]: loss 0.661175
[epoch13, step2425]: loss 0.542428
[epoch13, step2426]: loss 0.748934
[epoch13, step2427]: loss 0.536231
[epoch13, step2428]: loss 0.572136
[epoch13, step2429]: loss 0.519424
[epoch13, step2430]: loss 0.550325
[epoch13, step2431]: loss 0.356653
[epoch13, step2432]: loss 0.441918
[epoch13, step2433]: loss 0.839370
[epoch13, step2434]: loss 0.299675
[epoch13, step2435]: loss 0.507544
[epoch13, step2436]: loss 0.351603
[epoch13, step2437]: loss 0.663770
[epoch13, step2438]: loss 0.602561
[epoch13, step2439]: loss 0.736322
[epoch13, step2440]: loss 0.554883
[epoch13, step2441]: loss 0.683533
[epoch13, step2442]: loss 0.425680
[epoch13, step2443]: loss 0.416178
[epoch13, step2444]: loss 0.494577
[epoch13, step2445]: loss 0.638682
[epoch13, step2446]: loss 0.499505
[epoch13, step2447]: loss 0.591316
[epoch13, step2448]: loss 0.449966
[epoch13, step2449]: loss 0.681537
[epoch13, step2450]: loss 0.486968
[epoch13, step2451]: loss 0.748518
[epoch13, step2452]: loss 0.731678
[epoch13, step2453]: loss 0.725788
[epoch13, step2454]: loss 0.505779
[epoch13, step2455]: loss 0.578904
[epoch13, step2456]: loss 0.607524
[epoch13, step2457]: loss 0.562209
[epoch13, step2458]: loss 0.499516
[epoch13, step2459]: loss 0.477548
[epoch13, step2460]: loss 0.529327
[epoch13, step2461]: loss 0.378983
[epoch13, step2462]: loss 0.639764
[epoch13, step2463]: loss 0.767223
[epoch13, step2464]: loss 0.562733
[epoch13, step2465]: loss 0.580603
[epoch13, step2466]: loss 0.520740
[epoch13, step2467]: loss 0.687015
[epoch13, step2468]: loss 0.120924
[epoch13, step2469]: loss 0.691456
[epoch13, step2470]: loss 0.341489
[epoch13, step2471]: loss 0.441571
[epoch13, step2472]: loss 0.580916
[epoch13, step2473]: loss 0.636918
[epoch13, step2474]: loss 0.484951
[epoch13, step2475]: loss 0.491846
[epoch13, step2476]: loss 0.630382
[epoch13, step2477]: loss 0.612508
[epoch13, step2478]: loss 0.360726
[epoch13, step2479]: loss 0.477417
[epoch13, step2480]: loss 0.722594
[epoch13, step2481]: loss 0.501466
[epoch13, step2482]: loss 0.598109
[epoch13, step2483]: loss 0.726267
[epoch13, step2484]: loss 0.390734
[epoch13, step2485]: loss 0.448863
[epoch13, step2486]: loss 0.650541
[epoch13, step2487]: loss 0.506193
[epoch13, step2488]: loss 0.694892
[epoch13, step2489]: loss 0.401837
[epoch13, step2490]: loss 0.573624
[epoch13, step2491]: loss 0.352434
[epoch13, step2492]: loss 0.449927
[epoch13, step2493]: loss 0.508560
[epoch13, step2494]: loss 0.493629
[epoch13, step2495]: loss 0.701208
[epoch13, step2496]: loss 0.446463
[epoch13, step2497]: loss 0.640807
[epoch13, step2498]: loss 0.580480
[epoch13, step2499]: loss 0.718462
[epoch13, step2500]: loss 0.465909
[epoch13, step2501]: loss 0.386208
[epoch13, step2502]: loss 0.341955
[epoch13, step2503]: loss 0.274724
[epoch13, step2504]: loss 0.443622
[epoch13, step2505]: loss 0.854152
[epoch13, step2506]: loss 0.782197
[epoch13, step2507]: loss 0.672121
[epoch13, step2508]: loss 0.506493
[epoch13, step2509]: loss 0.502049
[epoch13, step2510]: loss 0.502697
[epoch13, step2511]: loss 0.265749
[epoch13, step2512]: loss 0.664024
[epoch13, step2513]: loss 0.625013
[epoch13, step2514]: loss 0.661506
[epoch13, step2515]: loss 0.619469
[epoch13, step2516]: loss 0.565955
[epoch13, step2517]: loss 0.744052
[epoch13, step2518]: loss 0.449279
[epoch13, step2519]: loss 0.430657
[epoch13, step2520]: loss 0.514131
[epoch13, step2521]: loss 0.428228
[epoch13, step2522]: loss 0.653280
[epoch13, step2523]: loss 0.454167
[epoch13, step2524]: loss 0.516018
[epoch13, step2525]: loss 0.539090
[epoch13, step2526]: loss 0.725576
[epoch13, step2527]: loss 0.649805
[epoch13, step2528]: loss 0.615926
[epoch13, step2529]: loss 0.604923
[epoch13, step2530]: loss 0.396677
[epoch13, step2531]: loss 0.481581
[epoch13, step2532]: loss 0.623302
[epoch13, step2533]: loss 0.590036
[epoch13, step2534]: loss 0.374343
[epoch13, step2535]: loss 0.596824
[epoch13, step2536]: loss 0.568737
[epoch13, step2537]: loss 0.687285
[epoch13, step2538]: loss 0.758811
[epoch13, step2539]: loss 0.551757
[epoch13, step2540]: loss 0.611518
[epoch13, step2541]: loss 0.435436
[epoch13, step2542]: loss 0.456991
[epoch13, step2543]: loss 0.472232
[epoch13, step2544]: loss 0.609357
[epoch13, step2545]: loss 0.408910
[epoch13, step2546]: loss 0.678847
[epoch13, step2547]: loss 0.641416
[epoch13, step2548]: loss 0.731859
[epoch13, step2549]: loss 0.736972
[epoch13, step2550]: loss 0.671030
[epoch13, step2551]: loss 0.671146
[epoch13, step2552]: loss 0.487517
[epoch13, step2553]: loss 0.823835
[epoch13, step2554]: loss 0.582276
[epoch13, step2555]: loss 0.695655
[epoch13, step2556]: loss 0.527424
[epoch13, step2557]: loss 0.722874
[epoch13, step2558]: loss 0.498385
[epoch13, step2559]: loss 0.604124
[epoch13, step2560]: loss 0.413009
[epoch13, step2561]: loss 0.382295
[epoch13, step2562]: loss 0.634821
[epoch13, step2563]: loss 0.408076
[epoch13, step2564]: loss 0.662574
[epoch13, step2565]: loss 0.699521
[epoch13, step2566]: loss 0.687171
[epoch13, step2567]: loss 0.602315
[epoch13, step2568]: loss 0.474316
[epoch13, step2569]: loss 0.586939
[epoch13, step2570]: loss 0.384261
[epoch13, step2571]: loss 0.790998
[epoch13, step2572]: loss 0.469661
[epoch13, step2573]: loss 0.504456
[epoch13, step2574]: loss 0.598005
[epoch13, step2575]: loss 0.866196
[epoch13, step2576]: loss 0.175353
[epoch13, step2577]: loss 0.655302
[epoch13, step2578]: loss 0.663113
[epoch13, step2579]: loss 0.670647
[epoch13, step2580]: loss 0.614513
[epoch13, step2581]: loss 0.586111
[epoch13, step2582]: loss 0.599712
[epoch13, step2583]: loss 0.788474
[epoch13, step2584]: loss 0.390191
[epoch13, step2585]: loss 0.637161
[epoch13, step2586]: loss 0.728461
[epoch13, step2587]: loss 0.479848
[epoch13, step2588]: loss 0.710454
[epoch13, step2589]: loss 0.590029
[epoch13, step2590]: loss 0.453713
[epoch13, step2591]: loss 0.544693
[epoch13, step2592]: loss 0.432119
[epoch13, step2593]: loss 0.609307
[epoch13, step2594]: loss 0.604544
[epoch13, step2595]: loss 0.427913
[epoch13, step2596]: loss 0.670873
[epoch13, step2597]: loss 0.597443
[epoch13, step2598]: loss 0.524973
[epoch13, step2599]: loss 0.585676
[epoch13, step2600]: loss 0.577231
[epoch13, step2601]: loss 0.479513
[epoch13, step2602]: loss 0.495675
[epoch13, step2603]: loss 0.486888
[epoch13, step2604]: loss 0.592667
[epoch13, step2605]: loss 0.640826
[epoch13, step2606]: loss 0.486098
[epoch13, step2607]: loss 0.738118
[epoch13, step2608]: loss 0.380657
[epoch13, step2609]: loss 0.643374
[epoch13, step2610]: loss 0.558675
[epoch13, step2611]: loss 0.609177
[epoch13, step2612]: loss 0.520207
[epoch13, step2613]: loss 0.411283
[epoch13, step2614]: loss 0.598518
[epoch13, step2615]: loss 0.553564
[epoch13, step2616]: loss 0.590818
[epoch13, step2617]: loss 0.627068
[epoch13, step2618]: loss 0.501841
[epoch13, step2619]: loss 0.583483
[epoch13, step2620]: loss 0.119759
[epoch13, step2621]: loss 0.679234
[epoch13, step2622]: loss 0.589138
[epoch13, step2623]: loss 0.811470
[epoch13, step2624]: loss 0.583062
[epoch13, step2625]: loss 0.306727
[epoch13, step2626]: loss 0.423885
[epoch13, step2627]: loss 0.428726
[epoch13, step2628]: loss 0.576443
[epoch13, step2629]: loss 0.559626
[epoch13, step2630]: loss 0.675830
[epoch13, step2631]: loss 0.626765
[epoch13, step2632]: loss 0.559955
[epoch13, step2633]: loss 0.536410
[epoch13, step2634]: loss 0.388857
[epoch13, step2635]: loss 0.453598
[epoch13, step2636]: loss 0.419349
[epoch13, step2637]: loss 0.476780
[epoch13, step2638]: loss 0.590853
[epoch13, step2639]: loss 0.711012
[epoch13, step2640]: loss 0.618051
[epoch13, step2641]: loss 0.335052
[epoch13, step2642]: loss 0.413960
[epoch13, step2643]: loss 0.322147
[epoch13, step2644]: loss 0.542316
[epoch13, step2645]: loss 0.286708
[epoch13, step2646]: loss 0.454314
[epoch13, step2647]: loss 0.290534
[epoch13, step2648]: loss 0.329996
[epoch13, step2649]: loss 0.433045
[epoch13, step2650]: loss 0.614376
[epoch13, step2651]: loss 0.749870
[epoch13, step2652]: loss 0.762906
[epoch13, step2653]: loss 0.507106
[epoch13, step2654]: loss 0.660509
[epoch13, step2655]: loss 0.670344
[epoch13, step2656]: loss 0.417133
[epoch13, step2657]: loss 0.637380
[epoch13, step2658]: loss 0.278535
[epoch13, step2659]: loss 0.570921
[epoch13, step2660]: loss 0.414784
[epoch13, step2661]: loss 0.611232
[epoch13, step2662]: loss 0.592472
[epoch13, step2663]: loss 0.709258
[epoch13, step2664]: loss 0.532989
[epoch13, step2665]: loss 0.801236
[epoch13, step2666]: loss 0.635608
[epoch13, step2667]: loss 0.191920
[epoch13, step2668]: loss 0.254893
[epoch13, step2669]: loss 0.573055
[epoch13, step2670]: loss 0.738026
[epoch13, step2671]: loss 0.437623
[epoch13, step2672]: loss 0.846757
[epoch13, step2673]: loss 0.714225
[epoch13, step2674]: loss 0.465292
[epoch13, step2675]: loss 0.646993
[epoch13, step2676]: loss 0.745527
[epoch13, step2677]: loss 0.548947
[epoch13, step2678]: loss 0.528743
[epoch13, step2679]: loss 0.668786
[epoch13, step2680]: loss 0.345664
[epoch13, step2681]: loss 0.585063
[epoch13, step2682]: loss 0.563634
[epoch13, step2683]: loss 0.716284
[epoch13, step2684]: loss 0.559772
[epoch13, step2685]: loss 0.674608
[epoch13, step2686]: loss 0.242932
[epoch13, step2687]: loss 0.708221
[epoch13, step2688]: loss 0.620724
[epoch13, step2689]: loss 0.639570
[epoch13, step2690]: loss 0.565487
[epoch13, step2691]: loss 0.557047
[epoch13, step2692]: loss 0.445812
[epoch13, step2693]: loss 0.441317
[epoch13, step2694]: loss 0.497053
[epoch13, step2695]: loss 0.698030
[epoch13, step2696]: loss 0.486453
[epoch13, step2697]: loss 0.662378
[epoch13, step2698]: loss 0.414192
[epoch13, step2699]: loss 0.637317
[epoch13, step2700]: loss 0.364012
[epoch13, step2701]: loss 0.304576
[epoch13, step2702]: loss 0.744500
[epoch13, step2703]: loss 0.393579
[epoch13, step2704]: loss 0.604241
[epoch13, step2705]: loss 0.734429
[epoch13, step2706]: loss 0.692936
[epoch13, step2707]: loss 0.761104
[epoch13, step2708]: loss 0.292104
[epoch13, step2709]: loss 0.276228
[epoch13, step2710]: loss 0.660524
[epoch13, step2711]: loss 0.520486
[epoch13, step2712]: loss 0.604992
[epoch13, step2713]: loss 0.732572
[epoch13, step2714]: loss 0.548831
[epoch13, step2715]: loss 0.380450
[epoch13, step2716]: loss 0.597804
[epoch13, step2717]: loss 0.477596
[epoch13, step2718]: loss 0.361133
[epoch13, step2719]: loss 0.653377
[epoch13, step2720]: loss 0.366835
[epoch13, step2721]: loss 0.555144
[epoch13, step2722]: loss 0.631441
[epoch13, step2723]: loss 0.686963
[epoch13, step2724]: loss 0.396314
[epoch13, step2725]: loss 0.519678
[epoch13, step2726]: loss 0.489741
[epoch13, step2727]: loss 0.682473
[epoch13, step2728]: loss 0.560339
[epoch13, step2729]: loss 0.687712
[epoch13, step2730]: loss 0.336719
[epoch13, step2731]: loss 0.778404
[epoch13, step2732]: loss 0.564002
[epoch13, step2733]: loss 0.537315
[epoch13, step2734]: loss 0.641483
[epoch13, step2735]: loss 0.619961
[epoch13, step2736]: loss 0.552675
[epoch13, step2737]: loss 0.587315
[epoch13, step2738]: loss 0.427223
[epoch13, step2739]: loss 0.797594
[epoch13, step2740]: loss 0.434135
[epoch13, step2741]: loss 0.712117
[epoch13, step2742]: loss 0.709529
[epoch13, step2743]: loss 0.336236
[epoch13, step2744]: loss 0.611544
[epoch13, step2745]: loss 0.421354
[epoch13, step2746]: loss 0.833890
[epoch13, step2747]: loss 0.705717
[epoch13, step2748]: loss 0.626308
[epoch13, step2749]: loss 0.621051
[epoch13, step2750]: loss 0.663389
[epoch13, step2751]: loss 0.478655
[epoch13, step2752]: loss 0.432356
[epoch13, step2753]: loss 0.439868
[epoch13, step2754]: loss 0.423716
[epoch13, step2755]: loss 0.522956
[epoch13, step2756]: loss 0.431465
[epoch13, step2757]: loss 0.286798
[epoch13, step2758]: loss 0.711369
[epoch13, step2759]: loss 0.595104
[epoch13, step2760]: loss 0.514063
[epoch13, step2761]: loss 0.701044
[epoch13, step2762]: loss 0.282084
[epoch13, step2763]: loss 0.749120
[epoch13, step2764]: loss 0.730983
[epoch13, step2765]: loss 0.692884
[epoch13, step2766]: loss 0.314203
[epoch13, step2767]: loss 0.496401
[epoch13, step2768]: loss 0.365552
[epoch13, step2769]: loss 0.620371
[epoch13, step2770]: loss 0.560018
[epoch13, step2771]: loss 0.611164
[epoch13, step2772]: loss 0.645490
[epoch13, step2773]: loss 0.471351
[epoch13, step2774]: loss 0.764514
[epoch13, step2775]: loss 0.441135
[epoch13, step2776]: loss 0.644095
[epoch13, step2777]: loss 0.295293
[epoch13, step2778]: loss 0.531335
[epoch13, step2779]: loss 0.457516
[epoch13, step2780]: loss 0.637178
[epoch13, step2781]: loss 0.505514
[epoch13, step2782]: loss 0.656185
[epoch13, step2783]: loss 0.571413
[epoch13, step2784]: loss 0.630912
[epoch13, step2785]: loss 0.774966
[epoch13, step2786]: loss 0.588584
[epoch13, step2787]: loss 0.365628
[epoch13, step2788]: loss 0.554776
[epoch13, step2789]: loss 0.641165
[epoch13, step2790]: loss 0.473106
[epoch13, step2791]: loss 0.643264
[epoch13, step2792]: loss 0.487866
[epoch13, step2793]: loss 0.761555
[epoch13, step2794]: loss 0.720494
[epoch13, step2795]: loss 0.495237
[epoch13, step2796]: loss 0.555060
[epoch13, step2797]: loss 0.409039
[epoch13, step2798]: loss 0.138144
[epoch13, step2799]: loss 0.548024
[epoch13, step2800]: loss 0.478503
[epoch13, step2801]: loss 0.574909
[epoch13, step2802]: loss 0.455043
[epoch13, step2803]: loss 0.412235
[epoch13, step2804]: loss 0.502331
[epoch13, step2805]: loss 0.608854
[epoch13, step2806]: loss 0.669026
[epoch13, step2807]: loss 0.574481
[epoch13, step2808]: loss 0.345728
[epoch13, step2809]: loss 0.781971
[epoch13, step2810]: loss 0.645544
[epoch13, step2811]: loss 0.413937
[epoch13, step2812]: loss 0.639333
[epoch13, step2813]: loss 0.509142
[epoch13, step2814]: loss 0.535659
[epoch13, step2815]: loss 0.596166
[epoch13, step2816]: loss 0.445929
[epoch13, step2817]: loss 0.394940
[epoch13, step2818]: loss 0.566512
[epoch13, step2819]: loss 0.787436
[epoch13, step2820]: loss 0.630525
[epoch13, step2821]: loss 0.574980
[epoch13, step2822]: loss 0.672335
[epoch13, step2823]: loss 0.597880
[epoch13, step2824]: loss 0.513945
[epoch13, step2825]: loss 0.579859
[epoch13, step2826]: loss 0.481092
[epoch13, step2827]: loss 0.560772
[epoch13, step2828]: loss 0.462303
[epoch13, step2829]: loss 0.300038
[epoch13, step2830]: loss 0.542817
[epoch13, step2831]: loss 0.469398
[epoch13, step2832]: loss 0.453119
[epoch13, step2833]: loss 0.457112
[epoch13, step2834]: loss 0.567500
[epoch13, step2835]: loss 0.664150
[epoch13, step2836]: loss 0.536679
[epoch13, step2837]: loss 0.354754
[epoch13, step2838]: loss 0.470415
[epoch13, step2839]: loss 0.511572
[epoch13, step2840]: loss 0.611241
[epoch13, step2841]: loss 0.360826
[epoch13, step2842]: loss 0.346753
[epoch13, step2843]: loss 0.707564
[epoch13, step2844]: loss 0.632372
[epoch13, step2845]: loss 0.462827
[epoch13, step2846]: loss 0.585800
[epoch13, step2847]: loss 0.332970
[epoch13, step2848]: loss 0.511563
[epoch13, step2849]: loss 0.469999
[epoch13, step2850]: loss 0.621768
[epoch13, step2851]: loss 0.502336
[epoch13, step2852]: loss 0.497821
[epoch13, step2853]: loss 0.521167
[epoch13, step2854]: loss 0.602827
[epoch13, step2855]: loss 0.521036
[epoch13, step2856]: loss 0.495881
[epoch13, step2857]: loss 0.516271
[epoch13, step2858]: loss 0.845553
[epoch13, step2859]: loss 0.652405
[epoch13, step2860]: loss 0.618732
[epoch13, step2861]: loss 0.613603
[epoch13, step2862]: loss 0.424230
[epoch13, step2863]: loss 0.563854
[epoch13, step2864]: loss 0.565400
[epoch13, step2865]: loss 0.543225
[epoch13, step2866]: loss 0.727472
[epoch13, step2867]: loss 0.699202
[epoch13, step2868]: loss 0.534070
[epoch13, step2869]: loss 0.287520
[epoch13, step2870]: loss 0.391303
[epoch13, step2871]: loss 0.512941
[epoch13, step2872]: loss 0.625161
[epoch13, step2873]: loss 0.569670
[epoch13, step2874]: loss 0.822603
[epoch13, step2875]: loss 0.418505
[epoch13, step2876]: loss 0.507328
[epoch13, step2877]: loss 0.410851
[epoch13, step2878]: loss 0.554723
[epoch13, step2879]: loss 0.486501
[epoch13, step2880]: loss 0.625674
[epoch13, step2881]: loss 0.563332
[epoch13, step2882]: loss 0.783148
[epoch13, step2883]: loss 0.524989
[epoch13, step2884]: loss 0.463652
[epoch13, step2885]: loss 0.345669
[epoch13, step2886]: loss 0.518911
[epoch13, step2887]: loss 0.738190
[epoch13, step2888]: loss 0.201093
[epoch13, step2889]: loss 0.503417
[epoch13, step2890]: loss 0.449964
[epoch13, step2891]: loss 0.364561
[epoch13, step2892]: loss 0.760821
[epoch13, step2893]: loss 0.377503
[epoch13, step2894]: loss 0.747346
[epoch13, step2895]: loss 0.426634
[epoch13, step2896]: loss 0.652207
[epoch13, step2897]: loss 0.789515
[epoch13, step2898]: loss 0.516118
[epoch13, step2899]: loss 0.839344
[epoch13, step2900]: loss 0.387392
[epoch13, step2901]: loss 0.626078
[epoch13, step2902]: loss 0.387827
[epoch13, step2903]: loss 0.598113
[epoch13, step2904]: loss 0.618191
[epoch13, step2905]: loss 0.454267
[epoch13, step2906]: loss 0.625054
[epoch13, step2907]: loss 0.609041
[epoch13, step2908]: loss 0.485513
[epoch13, step2909]: loss 0.259491
[epoch13, step2910]: loss 0.901866
[epoch13, step2911]: loss 0.597137
[epoch13, step2912]: loss 0.176564
[epoch13, step2913]: loss 0.244803
[epoch13, step2914]: loss 0.901623
[epoch13, step2915]: loss 0.779573
[epoch13, step2916]: loss 0.631124
[epoch13, step2917]: loss 0.502721
[epoch13, step2918]: loss 0.297949
[epoch13, step2919]: loss 0.310320
[epoch13, step2920]: loss 0.765588
[epoch13, step2921]: loss 0.410957
[epoch13, step2922]: loss 0.649769
[epoch13, step2923]: loss 0.593858
[epoch13, step2924]: loss 0.763932
[epoch13, step2925]: loss 0.727625
[epoch13, step2926]: loss 0.622223
[epoch13, step2927]: loss 0.538163
[epoch13, step2928]: loss 0.525710
[epoch13, step2929]: loss 0.613995
[epoch13, step2930]: loss 0.666769
[epoch13, step2931]: loss 0.610913
[epoch13, step2932]: loss 0.475637
[epoch13, step2933]: loss 0.682843
[epoch13, step2934]: loss 0.557433
[epoch13, step2935]: loss 0.679559
[epoch13, step2936]: loss 0.357719
[epoch13, step2937]: loss 0.258691
[epoch13, step2938]: loss 0.489676
[epoch13, step2939]: loss 0.510477
[epoch13, step2940]: loss 0.465855
[epoch13, step2941]: loss 0.649588
[epoch13, step2942]: loss 0.089375
[epoch13, step2943]: loss 0.751094
[epoch13, step2944]: loss 0.499150
[epoch13, step2945]: loss 0.246749
[epoch13, step2946]: loss 0.585753
[epoch13, step2947]: loss 0.680909
[epoch13, step2948]: loss 0.831482
[epoch13, step2949]: loss 0.685172
[epoch13, step2950]: loss 0.398915
[epoch13, step2951]: loss 0.610771
[epoch13, step2952]: loss 0.730083
[epoch13, step2953]: loss 0.418112
[epoch13, step2954]: loss 0.467460
[epoch13, step2955]: loss 0.603751
[epoch13, step2956]: loss 0.480049
[epoch13, step2957]: loss 0.476505
[epoch13, step2958]: loss 0.388474
[epoch13, step2959]: loss 0.409827
[epoch13, step2960]: loss 0.276225
[epoch13, step2961]: loss 0.572846
[epoch13, step2962]: loss 0.527007
[epoch13, step2963]: loss 0.484034
[epoch13, step2964]: loss 0.573893
[epoch13, step2965]: loss 0.452394
[epoch13, step2966]: loss 0.839588
[epoch13, step2967]: loss 0.500034
[epoch13, step2968]: loss 0.460905
[epoch13, step2969]: loss 0.617673
[epoch13, step2970]: loss 0.585476
[epoch13, step2971]: loss 0.716228
[epoch13, step2972]: loss 0.390252
[epoch13, step2973]: loss 0.497446
[epoch13, step2974]: loss 0.548525
[epoch13, step2975]: loss 0.504102
[epoch13, step2976]: loss 0.581617
[epoch13, step2977]: loss 0.359955
[epoch13, step2978]: loss 0.446197
[epoch13, step2979]: loss 0.628717
[epoch13, step2980]: loss 0.562951
[epoch13, step2981]: loss 0.778277
[epoch13, step2982]: loss 0.645070
[epoch13, step2983]: loss 0.654976
[epoch13, step2984]: loss 0.614206
[epoch13, step2985]: loss 0.628406
[epoch13, step2986]: loss 0.473949
[epoch13, step2987]: loss 0.550821
[epoch13, step2988]: loss 0.631954
[epoch13, step2989]: loss 0.710388
[epoch13, step2990]: loss 0.705516
[epoch13, step2991]: loss 0.445929
[epoch13, step2992]: loss 0.665814
[epoch13, step2993]: loss 0.721337
[epoch13, step2994]: loss 0.640439
[epoch13, step2995]: loss 0.496363
[epoch13, step2996]: loss 0.510202
[epoch13, step2997]: loss 0.582440
[epoch13, step2998]: loss 0.706918
[epoch13, step2999]: loss 0.339276
[epoch13, step3000]: loss 0.545464
[epoch13, step3001]: loss 0.376406
[epoch13, step3002]: loss 0.565380
[epoch13, step3003]: loss 0.586093
[epoch13, step3004]: loss 0.683200
[epoch13, step3005]: loss 0.525569
[epoch13, step3006]: loss 0.548013
[epoch13, step3007]: loss 0.759978
[epoch13, step3008]: loss 0.358217
[epoch13, step3009]: loss 0.613197
[epoch13, step3010]: loss 0.645093
[epoch13, step3011]: loss 0.770918
[epoch13, step3012]: loss 0.878272
[epoch13, step3013]: loss 0.591052
[epoch13, step3014]: loss 0.447958
[epoch13, step3015]: loss 0.481780
[epoch13, step3016]: loss 0.446504
[epoch13, step3017]: loss 0.613976
[epoch13, step3018]: loss 0.501664
[epoch13, step3019]: loss 0.532812
[epoch13, step3020]: loss 0.531705
[epoch13, step3021]: loss 0.723643
[epoch13, step3022]: loss 0.527326
[epoch13, step3023]: loss 0.557212
[epoch13, step3024]: loss 0.791161
[epoch13, step3025]: loss 0.569743
[epoch13, step3026]: loss 0.735797
[epoch13, step3027]: loss 0.470479
[epoch13, step3028]: loss 0.567407
[epoch13, step3029]: loss 0.755866
[epoch13, step3030]: loss 0.517551
[epoch13, step3031]: loss 0.610677
[epoch13, step3032]: loss 0.709958
[epoch13, step3033]: loss 0.307894
[epoch13, step3034]: loss 0.437721
[epoch13, step3035]: loss 0.569724
[epoch13, step3036]: loss 0.580338
[epoch13, step3037]: loss 0.637425
[epoch13, step3038]: loss 0.400762
[epoch13, step3039]: loss 0.445998
[epoch13, step3040]: loss 0.348637
[epoch13, step3041]: loss 0.617035
[epoch13, step3042]: loss 0.410721
[epoch13, step3043]: loss 0.591259
[epoch13, step3044]: loss 0.379564
[epoch13, step3045]: loss 0.502413
[epoch13, step3046]: loss 0.545774
[epoch13, step3047]: loss 0.254177
[epoch13, step3048]: loss 0.480665
[epoch13, step3049]: loss 0.600701
[epoch13, step3050]: loss 0.779919
[epoch13, step3051]: loss 0.460141
[epoch13, step3052]: loss 0.747679
[epoch13, step3053]: loss 0.543019
[epoch13, step3054]: loss 0.524654
[epoch13, step3055]: loss 0.409831
[epoch13, step3056]: loss 0.799951
[epoch13, step3057]: loss 0.783262
[epoch13, step3058]: loss 0.681038
[epoch13, step3059]: loss 0.620051
[epoch13, step3060]: loss 0.478408
[epoch13, step3061]: loss 0.432765
[epoch13, step3062]: loss 0.466181
[epoch13, step3063]: loss 0.314925
[epoch13, step3064]: loss 0.833468
[epoch13, step3065]: loss 0.757971
[epoch13, step3066]: loss 0.514875
[epoch13, step3067]: loss 0.723840
[epoch13, step3068]: loss 0.302083
[epoch13, step3069]: loss 0.706741
[epoch13, step3070]: loss 0.363880
[epoch13, step3071]: loss 0.542220
[epoch13, step3072]: loss 0.480635
[epoch13, step3073]: loss 0.500508
[epoch13, step3074]: loss 0.498424
[epoch13, step3075]: loss 0.652607
[epoch13, step3076]: loss 0.615848

[epoch13]: avg loss 0.615848

[epoch14, step1]: loss 0.709684
[epoch14, step2]: loss 0.445446
[epoch14, step3]: loss 0.696696
[epoch14, step4]: loss 0.099566
[epoch14, step5]: loss 0.693591
[epoch14, step6]: loss 0.416634
[epoch14, step7]: loss 0.658621
[epoch14, step8]: loss 0.759600
[epoch14, step9]: loss 0.564443
[epoch14, step10]: loss 0.364185
[epoch14, step11]: loss 0.332795
[epoch14, step12]: loss 0.524943
[epoch14, step13]: loss 0.495109
[epoch14, step14]: loss 0.534019
[epoch14, step15]: loss 0.523887
[epoch14, step16]: loss 0.537429
[epoch14, step17]: loss 0.308333
[epoch14, step18]: loss 0.542976
[epoch14, step19]: loss 0.756070
[epoch14, step20]: loss 0.494777
[epoch14, step21]: loss 0.413151
[epoch14, step22]: loss 0.466116
[epoch14, step23]: loss 0.539125
[epoch14, step24]: loss 0.566184
[epoch14, step25]: loss 0.390646
[epoch14, step26]: loss 0.503384
[epoch14, step27]: loss 0.424230
[epoch14, step28]: loss 0.714868
[epoch14, step29]: loss 0.407800
[epoch14, step30]: loss 0.673466
[epoch14, step31]: loss 0.411826
[epoch14, step32]: loss 0.736286
[epoch14, step33]: loss 0.397635
[epoch14, step34]: loss 0.378283
[epoch14, step35]: loss 0.608153
[epoch14, step36]: loss 0.360830
[epoch14, step37]: loss 0.634957
[epoch14, step38]: loss 0.720054
[epoch14, step39]: loss 0.612777
[epoch14, step40]: loss 0.462702
[epoch14, step41]: loss 0.731142
[epoch14, step42]: loss 0.564695
[epoch14, step43]: loss 0.392771
[epoch14, step44]: loss 0.503208
[epoch14, step45]: loss 0.553229
[epoch14, step46]: loss 0.373343
[epoch14, step47]: loss 0.502511
[epoch14, step48]: loss 0.628079
[epoch14, step49]: loss 0.637114
[epoch14, step50]: loss 0.564093
[epoch14, step51]: loss 0.596642
[epoch14, step52]: loss 0.408762
[epoch14, step53]: loss 0.526129
[epoch14, step54]: loss 0.562538
[epoch14, step55]: loss 0.724398
[epoch14, step56]: loss 0.182477
[epoch14, step57]: loss 0.591481
[epoch14, step58]: loss 0.656389
[epoch14, step59]: loss 0.549825
[epoch14, step60]: loss 0.530316
[epoch14, step61]: loss 0.602031
[epoch14, step62]: loss 0.446691
[epoch14, step63]: loss 0.652358
[epoch14, step64]: loss 0.590315
[epoch14, step65]: loss 0.471401
[epoch14, step66]: loss 0.607932
[epoch14, step67]: loss 0.394593
[epoch14, step68]: loss 0.418442
[epoch14, step69]: loss 0.868469
[epoch14, step70]: loss 0.469552
[epoch14, step71]: loss 0.493178
[epoch14, step72]: loss 0.541340
[epoch14, step73]: loss 0.628581
[epoch14, step74]: loss 0.659168
[epoch14, step75]: loss 0.696851
[epoch14, step76]: loss 0.698298
[epoch14, step77]: loss 0.411771
[epoch14, step78]: loss 0.889984
[epoch14, step79]: loss 0.640052
[epoch14, step80]: loss 0.713694
[epoch14, step81]: loss 0.480822
[epoch14, step82]: loss 0.470814
[epoch14, step83]: loss 0.252829
[epoch14, step84]: loss 0.629070
[epoch14, step85]: loss 0.430069
[epoch14, step86]: loss 0.374038
[epoch14, step87]: loss 0.434965
[epoch14, step88]: loss 0.650474
[epoch14, step89]: loss 0.637041
[epoch14, step90]: loss 0.525354
[epoch14, step91]: loss 0.292046
[epoch14, step92]: loss 0.474030
[epoch14, step93]: loss 0.630321
[epoch14, step94]: loss 0.646844
[epoch14, step95]: loss 0.544025
[epoch14, step96]: loss 0.517213
[epoch14, step97]: loss 0.437354
[epoch14, step98]: loss 0.652894
[epoch14, step99]: loss 0.557022
[epoch14, step100]: loss 0.435399
[epoch14, step101]: loss 0.573317
[epoch14, step102]: loss 0.681550
[epoch14, step103]: loss 0.533055
[epoch14, step104]: loss 0.703461
[epoch14, step105]: loss 0.641299
[epoch14, step106]: loss 0.738516
[epoch14, step107]: loss 0.648323
[epoch14, step108]: loss 0.343960
[epoch14, step109]: loss 0.471523
[epoch14, step110]: loss 0.649785
[epoch14, step111]: loss 0.687629
[epoch14, step112]: loss 0.674144
[epoch14, step113]: loss 0.652539
[epoch14, step114]: loss 0.416963
[epoch14, step115]: loss 0.366262
[epoch14, step116]: loss 0.577641
[epoch14, step117]: loss 0.473248
[epoch14, step118]: loss 0.534441
[epoch14, step119]: loss 0.536772
[epoch14, step120]: loss 0.377727
[epoch14, step121]: loss 0.297504
[epoch14, step122]: loss 0.720513
[epoch14, step123]: loss 0.306251
[epoch14, step124]: loss 0.454805
[epoch14, step125]: loss 0.590663
[epoch14, step126]: loss 0.719174
[epoch14, step127]: loss 0.472679
[epoch14, step128]: loss 0.594876
[epoch14, step129]: loss 0.359281
[epoch14, step130]: loss 0.784218
[epoch14, step131]: loss 0.746344
[epoch14, step132]: loss 0.547609
[epoch14, step133]: loss 0.785919
[epoch14, step134]: loss 0.270778
[epoch14, step135]: loss 0.438437
[epoch14, step136]: loss 0.506396
[epoch14, step137]: loss 0.597893
[epoch14, step138]: loss 0.518091
[epoch14, step139]: loss 0.539580
[epoch14, step140]: loss 0.600058
[epoch14, step141]: loss 0.434290
[epoch14, step142]: loss 0.506457
[epoch14, step143]: loss 0.549676
[epoch14, step144]: loss 0.327178
[epoch14, step145]: loss 0.663902
[epoch14, step146]: loss 0.532388
[epoch14, step147]: loss 0.673108
[epoch14, step148]: loss 0.661319
[epoch14, step149]: loss 0.551966
[epoch14, step150]: loss 0.541575
[epoch14, step151]: loss 0.747964
[epoch14, step152]: loss 0.410553
[epoch14, step153]: loss 0.360237
[epoch14, step154]: loss 0.625810
[epoch14, step155]: loss 0.516127
[epoch14, step156]: loss 0.511982
[epoch14, step157]: loss 0.281855
[epoch14, step158]: loss 0.559088
[epoch14, step159]: loss 0.552133
[epoch14, step160]: loss 0.516166
[epoch14, step161]: loss 0.527732
[epoch14, step162]: loss 0.321880
[epoch14, step163]: loss 0.607253
[epoch14, step164]: loss 0.441779
[epoch14, step165]: loss 0.367834
[epoch14, step166]: loss 0.461631
[epoch14, step167]: loss 0.633341
[epoch14, step168]: loss 0.594118
[epoch14, step169]: loss 0.516931
[epoch14, step170]: loss 0.640571
[epoch14, step171]: loss 0.375488
[epoch14, step172]: loss 0.660177
[epoch14, step173]: loss 0.682218
[epoch14, step174]: loss 0.499645
[epoch14, step175]: loss 0.378136
[epoch14, step176]: loss 0.517632
[epoch14, step177]: loss 0.757657
[epoch14, step178]: loss 0.709759
[epoch14, step179]: loss 0.386631
[epoch14, step180]: loss 0.693788
[epoch14, step181]: loss 0.433789
[epoch14, step182]: loss 0.618889
[epoch14, step183]: loss 0.452270
[epoch14, step184]: loss 0.667110
[epoch14, step185]: loss 0.669525
[epoch14, step186]: loss 0.611962
[epoch14, step187]: loss 0.325945
[epoch14, step188]: loss 0.542451
[epoch14, step189]: loss 0.577038
[epoch14, step190]: loss 0.588610
[epoch14, step191]: loss 0.791952
[epoch14, step192]: loss 0.412085
[epoch14, step193]: loss 0.633487
[epoch14, step194]: loss 0.543878
[epoch14, step195]: loss 0.694849
[epoch14, step196]: loss 0.575412
[epoch14, step197]: loss 0.465791
[epoch14, step198]: loss 0.692031
[epoch14, step199]: loss 0.630480
[epoch14, step200]: loss 0.599588
[epoch14, step201]: loss 0.736703
[epoch14, step202]: loss 0.497961
[epoch14, step203]: loss 0.441873
[epoch14, step204]: loss 0.300311
[epoch14, step205]: loss 0.665479
[epoch14, step206]: loss 0.607306
[epoch14, step207]: loss 0.472233
[epoch14, step208]: loss 0.399711
[epoch14, step209]: loss 0.428958
[epoch14, step210]: loss 0.513575
[epoch14, step211]: loss 0.605529
[epoch14, step212]: loss 0.482537
[epoch14, step213]: loss 0.575775
[epoch14, step214]: loss 0.424922
[epoch14, step215]: loss 0.727199
[epoch14, step216]: loss 0.523202
[epoch14, step217]: loss 0.629488
[epoch14, step218]: loss 0.538433
[epoch14, step219]: loss 0.365176
[epoch14, step220]: loss 0.394635
[epoch14, step221]: loss 0.471286
[epoch14, step222]: loss 0.391701
[epoch14, step223]: loss 0.656537
[epoch14, step224]: loss 0.514231
[epoch14, step225]: loss 0.376248
[epoch14, step226]: loss 0.465368
[epoch14, step227]: loss 0.450042
[epoch14, step228]: loss 0.575339
[epoch14, step229]: loss 0.677598
[epoch14, step230]: loss 0.483429
[epoch14, step231]: loss 0.492781
[epoch14, step232]: loss 0.756063
[epoch14, step233]: loss 0.680713
[epoch14, step234]: loss 0.523284
[epoch14, step235]: loss 0.377404
[epoch14, step236]: loss 0.753892
[epoch14, step237]: loss 0.453244
[epoch14, step238]: loss 0.404453
[epoch14, step239]: loss 0.828053
[epoch14, step240]: loss 0.509930
[epoch14, step241]: loss 0.477019
[epoch14, step242]: loss 0.429555
[epoch14, step243]: loss 0.254077
[epoch14, step244]: loss 0.504222
[epoch14, step245]: loss 0.442217
[epoch14, step246]: loss 0.621732
[epoch14, step247]: loss 0.400045
[epoch14, step248]: loss 0.661603
[epoch14, step249]: loss 0.893602
[epoch14, step250]: loss 0.476524
[epoch14, step251]: loss 0.425275
[epoch14, step252]: loss 0.362366
[epoch14, step253]: loss 0.563978
[epoch14, step254]: loss 0.388369
[epoch14, step255]: loss 0.531537
[epoch14, step256]: loss 0.603771
[epoch14, step257]: loss 0.633065
[epoch14, step258]: loss 0.319748
[epoch14, step259]: loss 0.356369
[epoch14, step260]: loss 0.510869
[epoch14, step261]: loss 0.827048
[epoch14, step262]: loss 0.630912
[epoch14, step263]: loss 0.556566
[epoch14, step264]: loss 0.494317
[epoch14, step265]: loss 0.655305
[epoch14, step266]: loss 0.586455
[epoch14, step267]: loss 0.563647
[epoch14, step268]: loss 0.767515
[epoch14, step269]: loss 0.801368
[epoch14, step270]: loss 0.653476
[epoch14, step271]: loss 0.622178
[epoch14, step272]: loss 0.571228
[epoch14, step273]: loss 0.318186
[epoch14, step274]: loss 0.537012
[epoch14, step275]: loss 0.636231
[epoch14, step276]: loss 0.658190
[epoch14, step277]: loss 0.549987
[epoch14, step278]: loss 0.607204
[epoch14, step279]: loss 0.288727
[epoch14, step280]: loss 0.774027
[epoch14, step281]: loss 0.857148
[epoch14, step282]: loss 0.560168
[epoch14, step283]: loss 0.422332
[epoch14, step284]: loss 0.854166
[epoch14, step285]: loss 0.703973
[epoch14, step286]: loss 0.732199
[epoch14, step287]: loss 0.604781
[epoch14, step288]: loss 0.441368
[epoch14, step289]: loss 0.673890
[epoch14, step290]: loss 0.288274
[epoch14, step291]: loss 0.371057
[epoch14, step292]: loss 0.492699
[epoch14, step293]: loss 0.625104
[epoch14, step294]: loss 0.636265
[epoch14, step295]: loss 0.376607
[epoch14, step296]: loss 0.673599
[epoch14, step297]: loss 0.686087
[epoch14, step298]: loss 0.728790
[epoch14, step299]: loss 0.483794
[epoch14, step300]: loss 0.508758
[epoch14, step301]: loss 0.486044
[epoch14, step302]: loss 0.373200
[epoch14, step303]: loss 0.668068
[epoch14, step304]: loss 0.706329
[epoch14, step305]: loss 0.594620
[epoch14, step306]: loss 0.777776
[epoch14, step307]: loss 0.750625
[epoch14, step308]: loss 0.527831
[epoch14, step309]: loss 0.763903
[epoch14, step310]: loss 0.724582
[epoch14, step311]: loss 0.334206
[epoch14, step312]: loss 0.552917
[epoch14, step313]: loss 0.666974
[epoch14, step314]: loss 0.638847
[epoch14, step315]: loss 0.492062
[epoch14, step316]: loss 0.628834
[epoch14, step317]: loss 0.550664
[epoch14, step318]: loss 0.595633
[epoch14, step319]: loss 0.527084
[epoch14, step320]: loss 0.673481
[epoch14, step321]: loss 0.498533
[epoch14, step322]: loss 0.632497
[epoch14, step323]: loss 0.596771
[epoch14, step324]: loss 0.627768
[epoch14, step325]: loss 0.286766
[epoch14, step326]: loss 0.353382
[epoch14, step327]: loss 0.641878
[epoch14, step328]: loss 0.523281
[epoch14, step329]: loss 0.317699
[epoch14, step330]: loss 0.626694
[epoch14, step331]: loss 0.486857
[epoch14, step332]: loss 0.267339
[epoch14, step333]: loss 0.757152
[epoch14, step334]: loss 0.660972
[epoch14, step335]: loss 0.541771
[epoch14, step336]: loss 0.561480
[epoch14, step337]: loss 0.543180
[epoch14, step338]: loss 0.530414
[epoch14, step339]: loss 0.505077
[epoch14, step340]: loss 0.569032
[epoch14, step341]: loss 0.787692
[epoch14, step342]: loss 0.596392
[epoch14, step343]: loss 0.547606
[epoch14, step344]: loss 0.307122
[epoch14, step345]: loss 0.649963
[epoch14, step346]: loss 0.476913
[epoch14, step347]: loss 0.617502
[epoch14, step348]: loss 0.561242
[epoch14, step349]: loss 0.470392
[epoch14, step350]: loss 0.634830
[epoch14, step351]: loss 0.437068
[epoch14, step352]: loss 0.590752
[epoch14, step353]: loss 0.364099
[epoch14, step354]: loss 0.677720
[epoch14, step355]: loss 0.650733
[epoch14, step356]: loss 0.516353
[epoch14, step357]: loss 0.667313
[epoch14, step358]: loss 0.478923
[epoch14, step359]: loss 0.639610
[epoch14, step360]: loss 0.764006
[epoch14, step361]: loss 0.549309
[epoch14, step362]: loss 0.587731
[epoch14, step363]: loss 0.790707
[epoch14, step364]: loss 0.450979
[epoch14, step365]: loss 0.631864
[epoch14, step366]: loss 0.629936
[epoch14, step367]: loss 0.472784
[epoch14, step368]: loss 0.780719
[epoch14, step369]: loss 0.448315
[epoch14, step370]: loss 0.377436
[epoch14, step371]: loss 0.515249
[epoch14, step372]: loss 0.675520
[epoch14, step373]: loss 0.636814
[epoch14, step374]: loss 0.311439
[epoch14, step375]: loss 0.469790
[epoch14, step376]: loss 0.321545
[epoch14, step377]: loss 0.525382
[epoch14, step378]: loss 0.541457
[epoch14, step379]: loss 0.322801
[epoch14, step380]: loss 0.420350
[epoch14, step381]: loss 0.641156
[epoch14, step382]: loss 0.448910
[epoch14, step383]: loss 0.777788
[epoch14, step384]: loss 0.615378
[epoch14, step385]: loss 0.667504
[epoch14, step386]: loss 0.521550
[epoch14, step387]: loss 0.654469
[epoch14, step388]: loss 0.655732
[epoch14, step389]: loss 0.417944
[epoch14, step390]: loss 0.690396
[epoch14, step391]: loss 0.347012
[epoch14, step392]: loss 0.377623
[epoch14, step393]: loss 0.636636
[epoch14, step394]: loss 0.684495
[epoch14, step395]: loss 0.614859
[epoch14, step396]: loss 0.556845
[epoch14, step397]: loss 0.669823
[epoch14, step398]: loss 0.542295
[epoch14, step399]: loss 0.492606
[epoch14, step400]: loss 0.648696
[epoch14, step401]: loss 0.689883
[epoch14, step402]: loss 0.608398
[epoch14, step403]: loss 0.583295
[epoch14, step404]: loss 0.730471
[epoch14, step405]: loss 0.640846
[epoch14, step406]: loss 0.421722
[epoch14, step407]: loss 0.385637
[epoch14, step408]: loss 0.386638
[epoch14, step409]: loss 0.442940
[epoch14, step410]: loss 0.596156
[epoch14, step411]: loss 0.668421
[epoch14, step412]: loss 0.496438
[epoch14, step413]: loss 0.692751
[epoch14, step414]: loss 0.459496
[epoch14, step415]: loss 0.595080
[epoch14, step416]: loss 0.414773
[epoch14, step417]: loss 0.527167
[epoch14, step418]: loss 0.434479
[epoch14, step419]: loss 0.288662
[epoch14, step420]: loss 0.325263
[epoch14, step421]: loss 0.790673
[epoch14, step422]: loss 0.607345
[epoch14, step423]: loss 0.545582
[epoch14, step424]: loss 0.577724
[epoch14, step425]: loss 0.548442
[epoch14, step426]: loss 0.732638
[epoch14, step427]: loss 0.632007
[epoch14, step428]: loss 0.700083
[epoch14, step429]: loss 0.838286
[epoch14, step430]: loss 0.425775
[epoch14, step431]: loss 0.720731
[epoch14, step432]: loss 0.628050
[epoch14, step433]: loss 0.710716
[epoch14, step434]: loss 0.441249
[epoch14, step435]: loss 0.624317
[epoch14, step436]: loss 0.632499
[epoch14, step437]: loss 0.587333
[epoch14, step438]: loss 0.654400
[epoch14, step439]: loss 0.809093
[epoch14, step440]: loss 0.429598
[epoch14, step441]: loss 0.530087
[epoch14, step442]: loss 0.586620
[epoch14, step443]: loss 0.648261
[epoch14, step444]: loss 0.559957
[epoch14, step445]: loss 0.535384
[epoch14, step446]: loss 0.643292
[epoch14, step447]: loss 0.505158
[epoch14, step448]: loss 0.469036
[epoch14, step449]: loss 0.811484
[epoch14, step450]: loss 0.354284
[epoch14, step451]: loss 0.345936
[epoch14, step452]: loss 0.207513
[epoch14, step453]: loss 0.534594
[epoch14, step454]: loss 0.329196
[epoch14, step455]: loss 0.722639
[epoch14, step456]: loss 0.694771
[epoch14, step457]: loss 0.523492
[epoch14, step458]: loss 0.670090
[epoch14, step459]: loss 0.484962
[epoch14, step460]: loss 0.727728
[epoch14, step461]: loss 0.592193
[epoch14, step462]: loss 0.532533
[epoch14, step463]: loss 0.532524
[epoch14, step464]: loss 0.580253
[epoch14, step465]: loss 0.776654
[epoch14, step466]: loss 0.399291
[epoch14, step467]: loss 0.522740
[epoch14, step468]: loss 0.675717
[epoch14, step469]: loss 0.312067
[epoch14, step470]: loss 0.600184
[epoch14, step471]: loss 0.533745
[epoch14, step472]: loss 0.642170
[epoch14, step473]: loss 0.390698
[epoch14, step474]: loss 0.493272
[epoch14, step475]: loss 0.618010
[epoch14, step476]: loss 0.421830
[epoch14, step477]: loss 0.613442
[epoch14, step478]: loss 0.484291
[epoch14, step479]: loss 0.578419
[epoch14, step480]: loss 0.461287
[epoch14, step481]: loss 0.508452
[epoch14, step482]: loss 0.645242
[epoch14, step483]: loss 0.761877
[epoch14, step484]: loss 0.415377
[epoch14, step485]: loss 0.456456
[epoch14, step486]: loss 0.458653
[epoch14, step487]: loss 0.536819
[epoch14, step488]: loss 0.574784
[epoch14, step489]: loss 0.271102
[epoch14, step490]: loss 0.500445
[epoch14, step491]: loss 0.636802
[epoch14, step492]: loss 0.660438
[epoch14, step493]: loss 0.379076
[epoch14, step494]: loss 0.264487
[epoch14, step495]: loss 0.292626
[epoch14, step496]: loss 0.539869
[epoch14, step497]: loss 0.897431
[epoch14, step498]: loss 0.194294
[epoch14, step499]: loss 0.699778
[epoch14, step500]: loss 0.373360
[epoch14, step501]: loss 0.094732
[epoch14, step502]: loss 0.714430
[epoch14, step503]: loss 0.598155
[epoch14, step504]: loss 0.633889
[epoch14, step505]: loss 0.431782
[epoch14, step506]: loss 0.364372
[epoch14, step507]: loss 0.492726
[epoch14, step508]: loss 0.612516
[epoch14, step509]: loss 0.532595
[epoch14, step510]: loss 0.651167
[epoch14, step511]: loss 0.180047
[epoch14, step512]: loss 0.574531
[epoch14, step513]: loss 0.720862
[epoch14, step514]: loss 0.674705
[epoch14, step515]: loss 0.782456
[epoch14, step516]: loss 0.796608
[epoch14, step517]: loss 0.547282
[epoch14, step518]: loss 0.195113
[epoch14, step519]: loss 0.360358
[epoch14, step520]: loss 0.731149
[epoch14, step521]: loss 0.487402
[epoch14, step522]: loss 0.376031
[epoch14, step523]: loss 0.494310
[epoch14, step524]: loss 0.775547
[epoch14, step525]: loss 0.686263
[epoch14, step526]: loss 0.289494
[epoch14, step527]: loss 0.417151
[epoch14, step528]: loss 0.585759
[epoch14, step529]: loss 0.499246
[epoch14, step530]: loss 0.443611
[epoch14, step531]: loss 0.456771
[epoch14, step532]: loss 0.664119
[epoch14, step533]: loss 0.369695
[epoch14, step534]: loss 0.784537
[epoch14, step535]: loss 0.400766
[epoch14, step536]: loss 0.509709
[epoch14, step537]: loss 0.472568
[epoch14, step538]: loss 0.433179
[epoch14, step539]: loss 0.568534
[epoch14, step540]: loss 0.358514
[epoch14, step541]: loss 0.692158
[epoch14, step542]: loss 0.794818
[epoch14, step543]: loss 0.562795
[epoch14, step544]: loss 0.452350
[epoch14, step545]: loss 0.605816
[epoch14, step546]: loss 0.525161
[epoch14, step547]: loss 0.617045
[epoch14, step548]: loss 0.648359
[epoch14, step549]: loss 0.457095
[epoch14, step550]: loss 0.294524
[epoch14, step551]: loss 0.773573
[epoch14, step552]: loss 0.354134
[epoch14, step553]: loss 0.885064
[epoch14, step554]: loss 0.722751
[epoch14, step555]: loss 0.724663
[epoch14, step556]: loss 0.826906
[epoch14, step557]: loss 0.261411
[epoch14, step558]: loss 0.419285
[epoch14, step559]: loss 0.360003
[epoch14, step560]: loss 0.772025
[epoch14, step561]: loss 0.340767
[epoch14, step562]: loss 0.528405
[epoch14, step563]: loss 0.423974
[epoch14, step564]: loss 0.373673
[epoch14, step565]: loss 0.521084
[epoch14, step566]: loss 0.547976
[epoch14, step567]: loss 0.407444
[epoch14, step568]: loss 0.480129
[epoch14, step569]: loss 0.657452
[epoch14, step570]: loss 0.649534
[epoch14, step571]: loss 0.663801
[epoch14, step572]: loss 0.464359
[epoch14, step573]: loss 0.572450
[epoch14, step574]: loss 0.706451
[epoch14, step575]: loss 0.482260
[epoch14, step576]: loss 0.640696
[epoch14, step577]: loss 0.609764
[epoch14, step578]: loss 0.543310
[epoch14, step579]: loss 0.593390
[epoch14, step580]: loss 0.711123
[epoch14, step581]: loss 0.555982
[epoch14, step582]: loss 0.560306
[epoch14, step583]: loss 0.634208
[epoch14, step584]: loss 0.685831
[epoch14, step585]: loss 0.527377
[epoch14, step586]: loss 0.588622
[epoch14, step587]: loss 0.659264
[epoch14, step588]: loss 0.790430
[epoch14, step589]: loss 0.532439
[epoch14, step590]: loss 0.474382
[epoch14, step591]: loss 0.581774
[epoch14, step592]: loss 0.478409
[epoch14, step593]: loss 0.732236
[epoch14, step594]: loss 0.618899
[epoch14, step595]: loss 0.608419
[epoch14, step596]: loss 0.703438
[epoch14, step597]: loss 0.543095
[epoch14, step598]: loss 0.391732
[epoch14, step599]: loss 0.649284
[epoch14, step600]: loss 0.600603
[epoch14, step601]: loss 0.693107
[epoch14, step602]: loss 0.668965
[epoch14, step603]: loss 0.712069
[epoch14, step604]: loss 0.503819
[epoch14, step605]: loss 0.680483
[epoch14, step606]: loss 0.262426
[epoch14, step607]: loss 0.515105
[epoch14, step608]: loss 0.700453
[epoch14, step609]: loss 0.326518
[epoch14, step610]: loss 0.586944
[epoch14, step611]: loss 0.326523
[epoch14, step612]: loss 0.699798
[epoch14, step613]: loss 0.198759
[epoch14, step614]: loss 0.440602
[epoch14, step615]: loss 0.435003
[epoch14, step616]: loss 0.564095
[epoch14, step617]: loss 0.572310
[epoch14, step618]: loss 0.446704
[epoch14, step619]: loss 0.593305
[epoch14, step620]: loss 0.496275
[epoch14, step621]: loss 0.394894
[epoch14, step622]: loss 0.657960
[epoch14, step623]: loss 0.639517
[epoch14, step624]: loss 0.668262
[epoch14, step625]: loss 0.616739
[epoch14, step626]: loss 0.676653
[epoch14, step627]: loss 0.477784
[epoch14, step628]: loss 0.460209
[epoch14, step629]: loss 0.628495
[epoch14, step630]: loss 0.794164
[epoch14, step631]: loss 0.496636
[epoch14, step632]: loss 0.648681
[epoch14, step633]: loss 0.561093
[epoch14, step634]: loss 0.463321
[epoch14, step635]: loss 0.565553
[epoch14, step636]: loss 0.442466
[epoch14, step637]: loss 0.623307
[epoch14, step638]: loss 0.486180
[epoch14, step639]: loss 0.775496
[epoch14, step640]: loss 0.579048
[epoch14, step641]: loss 0.256270
[epoch14, step642]: loss 0.366972
[epoch14, step643]: loss 0.372343
[epoch14, step644]: loss 0.392652
[epoch14, step645]: loss 0.597024
[epoch14, step646]: loss 0.459132
[epoch14, step647]: loss 0.649083
[epoch14, step648]: loss 0.424603
[epoch14, step649]: loss 0.543923
[epoch14, step650]: loss 0.475546
[epoch14, step651]: loss 0.401158
[epoch14, step652]: loss 0.575370
[epoch14, step653]: loss 0.520756
[epoch14, step654]: loss 0.357438
[epoch14, step655]: loss 0.540535
[epoch14, step656]: loss 0.406077
[epoch14, step657]: loss 0.704863
[epoch14, step658]: loss 0.578785
[epoch14, step659]: loss 0.497559
[epoch14, step660]: loss 0.705408
[epoch14, step661]: loss 0.375123
[epoch14, step662]: loss 0.442274
[epoch14, step663]: loss 0.688343
[epoch14, step664]: loss 0.422833
[epoch14, step665]: loss 0.410258
[epoch14, step666]: loss 0.593981
[epoch14, step667]: loss 0.508219
[epoch14, step668]: loss 0.506068
[epoch14, step669]: loss 0.103393
[epoch14, step670]: loss 0.284546
[epoch14, step671]: loss 0.810767
[epoch14, step672]: loss 0.453868
[epoch14, step673]: loss 0.537429
[epoch14, step674]: loss 0.630911
[epoch14, step675]: loss 0.738102
[epoch14, step676]: loss 0.487888
[epoch14, step677]: loss 0.630485
[epoch14, step678]: loss 0.677778
[epoch14, step679]: loss 0.601775
[epoch14, step680]: loss 0.463034
[epoch14, step681]: loss 0.561218
[epoch14, step682]: loss 0.633394
[epoch14, step683]: loss 0.489163
[epoch14, step684]: loss 0.429627
[epoch14, step685]: loss 0.396867
[epoch14, step686]: loss 0.568462
[epoch14, step687]: loss 0.304362
[epoch14, step688]: loss 0.632434
[epoch14, step689]: loss 0.461701
[epoch14, step690]: loss 0.413806
[epoch14, step691]: loss 0.285722
[epoch14, step692]: loss 0.836496
[epoch14, step693]: loss 0.583549
[epoch14, step694]: loss 0.422193
[epoch14, step695]: loss 0.471892
[epoch14, step696]: loss 0.511201
[epoch14, step697]: loss 0.653616
[epoch14, step698]: loss 0.508935
[epoch14, step699]: loss 0.581399
[epoch14, step700]: loss 0.584681
[epoch14, step701]: loss 0.720022
[epoch14, step702]: loss 0.513059
[epoch14, step703]: loss 0.559543
[epoch14, step704]: loss 0.452151
[epoch14, step705]: loss 0.594271
[epoch14, step706]: loss 0.585378
[epoch14, step707]: loss 0.726060
[epoch14, step708]: loss 0.533957
[epoch14, step709]: loss 0.641981
[epoch14, step710]: loss 0.433753
[epoch14, step711]: loss 0.392316
[epoch14, step712]: loss 0.666414
[epoch14, step713]: loss 0.495323
[epoch14, step714]: loss 0.547124
[epoch14, step715]: loss 0.453055
[epoch14, step716]: loss 0.762455
[epoch14, step717]: loss 0.485076
[epoch14, step718]: loss 0.783528
[epoch14, step719]: loss 0.653375
[epoch14, step720]: loss 0.624209
[epoch14, step721]: loss 0.631866
[epoch14, step722]: loss 0.696618
[epoch14, step723]: loss 0.525327
[epoch14, step724]: loss 0.415260
[epoch14, step725]: loss 0.647233
[epoch14, step726]: loss 0.674845
[epoch14, step727]: loss 0.443156
[epoch14, step728]: loss 0.337576
[epoch14, step729]: loss 0.622785
[epoch14, step730]: loss 0.432711
[epoch14, step731]: loss 0.548037
[epoch14, step732]: loss 0.484132
[epoch14, step733]: loss 0.509601
[epoch14, step734]: loss 0.537829
[epoch14, step735]: loss 0.377365
[epoch14, step736]: loss 0.483902
[epoch14, step737]: loss 0.568734
[epoch14, step738]: loss 0.472928
[epoch14, step739]: loss 0.419237
[epoch14, step740]: loss 0.488326
[epoch14, step741]: loss 0.646060
[epoch14, step742]: loss 0.669553
[epoch14, step743]: loss 0.495970
[epoch14, step744]: loss 0.636641
[epoch14, step745]: loss 0.727163
[epoch14, step746]: loss 0.432410
[epoch14, step747]: loss 0.600180
[epoch14, step748]: loss 0.494985
[epoch14, step749]: loss 0.473562
[epoch14, step750]: loss 0.500850
[epoch14, step751]: loss 0.616922
[epoch14, step752]: loss 0.474326
[epoch14, step753]: loss 0.578504
[epoch14, step754]: loss 0.508768
[epoch14, step755]: loss 0.461851
[epoch14, step756]: loss 0.628174
[epoch14, step757]: loss 0.489371
[epoch14, step758]: loss 0.594351
[epoch14, step759]: loss 0.368718
[epoch14, step760]: loss 0.520356
[epoch14, step761]: loss 0.550004
[epoch14, step762]: loss 0.573794
[epoch14, step763]: loss 0.641676
[epoch14, step764]: loss 0.592144
[epoch14, step765]: loss 0.318511
[epoch14, step766]: loss 0.299801
[epoch14, step767]: loss 0.750837
[epoch14, step768]: loss 0.739891
[epoch14, step769]: loss 0.402240
[epoch14, step770]: loss 0.608388
[epoch14, step771]: loss 0.785763
[epoch14, step772]: loss 0.423119
[epoch14, step773]: loss 0.352340
[epoch14, step774]: loss 0.589595
[epoch14, step775]: loss 0.608511
[epoch14, step776]: loss 0.790023
[epoch14, step777]: loss 0.127694
[epoch14, step778]: loss 0.509724
[epoch14, step779]: loss 0.327620
[epoch14, step780]: loss 0.524767
[epoch14, step781]: loss 0.329749
[epoch14, step782]: loss 0.669732
[epoch14, step783]: loss 0.619681
[epoch14, step784]: loss 0.565922
[epoch14, step785]: loss 0.548849
[epoch14, step786]: loss 0.630040
[epoch14, step787]: loss 0.576078
[epoch14, step788]: loss 0.358990
[epoch14, step789]: loss 0.555940
[epoch14, step790]: loss 0.723754
[epoch14, step791]: loss 0.341060
[epoch14, step792]: loss 0.648553
[epoch14, step793]: loss 0.710627
[epoch14, step794]: loss 0.313656
[epoch14, step795]: loss 0.744715
[epoch14, step796]: loss 0.637795
[epoch14, step797]: loss 0.570713
[epoch14, step798]: loss 0.804791
[epoch14, step799]: loss 0.695285
[epoch14, step800]: loss 0.586112
[epoch14, step801]: loss 0.592767
[epoch14, step802]: loss 0.537513
[epoch14, step803]: loss 0.288110
[epoch14, step804]: loss 0.402068
[epoch14, step805]: loss 0.397412
[epoch14, step806]: loss 0.616008
[epoch14, step807]: loss 0.567540
[epoch14, step808]: loss 0.264129
[epoch14, step809]: loss 0.342779
[epoch14, step810]: loss 0.729639
[epoch14, step811]: loss 0.799371
[epoch14, step812]: loss 0.674767
[epoch14, step813]: loss 0.702922
[epoch14, step814]: loss 0.619811
[epoch14, step815]: loss 0.636633
[epoch14, step816]: loss 0.556301
[epoch14, step817]: loss 0.691508
[epoch14, step818]: loss 0.778801
[epoch14, step819]: loss 0.605236
[epoch14, step820]: loss 0.576906
[epoch14, step821]: loss 0.747158
[epoch14, step822]: loss 0.693590
[epoch14, step823]: loss 0.342926
[epoch14, step824]: loss 0.678600
[epoch14, step825]: loss 0.540362
[epoch14, step826]: loss 0.622820
[epoch14, step827]: loss 0.549829
[epoch14, step828]: loss 0.629113
[epoch14, step829]: loss 0.810072
[epoch14, step830]: loss 0.403656
[epoch14, step831]: loss 0.521464
[epoch14, step832]: loss 0.835030
[epoch14, step833]: loss 0.567504
[epoch14, step834]: loss 0.602058
[epoch14, step835]: loss 0.591277
[epoch14, step836]: loss 0.404101
[epoch14, step837]: loss 0.301932
[epoch14, step838]: loss 0.772980
[epoch14, step839]: loss 0.563657
[epoch14, step840]: loss 0.415390
[epoch14, step841]: loss 0.536372
[epoch14, step842]: loss 0.718108
[epoch14, step843]: loss 0.739756
[epoch14, step844]: loss 0.664687
[epoch14, step845]: loss 0.466236
[epoch14, step846]: loss 0.429011
[epoch14, step847]: loss 0.778793
[epoch14, step848]: loss 0.594400
[epoch14, step849]: loss 0.535659
[epoch14, step850]: loss 0.680020
[epoch14, step851]: loss 0.627721
[epoch14, step852]: loss 0.858113
[epoch14, step853]: loss 0.529306
[epoch14, step854]: loss 0.556979
[epoch14, step855]: loss 0.603129
[epoch14, step856]: loss 0.681002
[epoch14, step857]: loss 0.424640
[epoch14, step858]: loss 0.648581
[epoch14, step859]: loss 0.513860
[epoch14, step860]: loss 0.518140
[epoch14, step861]: loss 0.665607
[epoch14, step862]: loss 0.574601
[epoch14, step863]: loss 0.516009
[epoch14, step864]: loss 0.669583
[epoch14, step865]: loss 0.517565
[epoch14, step866]: loss 0.658399
[epoch14, step867]: loss 0.594908
[epoch14, step868]: loss 0.627283
[epoch14, step869]: loss 0.318824
[epoch14, step870]: loss 0.558717
[epoch14, step871]: loss 0.452536
[epoch14, step872]: loss 0.878839
[epoch14, step873]: loss 0.454585
[epoch14, step874]: loss 0.603108
[epoch14, step875]: loss 0.570533
[epoch14, step876]: loss 0.498580
[epoch14, step877]: loss 0.778612
[epoch14, step878]: loss 0.702157
[epoch14, step879]: loss 0.386018
[epoch14, step880]: loss 0.707572
[epoch14, step881]: loss 0.518184
[epoch14, step882]: loss 0.475715
[epoch14, step883]: loss 0.621158
[epoch14, step884]: loss 0.826488
[epoch14, step885]: loss 0.526898
[epoch14, step886]: loss 0.553337
[epoch14, step887]: loss 0.786929
[epoch14, step888]: loss 0.533531
[epoch14, step889]: loss 0.324056
[epoch14, step890]: loss 0.652848
[epoch14, step891]: loss 0.515133
[epoch14, step892]: loss 0.444084
[epoch14, step893]: loss 0.469240
[epoch14, step894]: loss 0.436330
[epoch14, step895]: loss 0.383519
[epoch14, step896]: loss 0.601879
[epoch14, step897]: loss 0.227025
[epoch14, step898]: loss 0.662031
[epoch14, step899]: loss 0.747726
[epoch14, step900]: loss 0.541693
[epoch14, step901]: loss 0.560906
[epoch14, step902]: loss 0.736153
[epoch14, step903]: loss 0.539029
[epoch14, step904]: loss 0.646279
[epoch14, step905]: loss 0.739067
[epoch14, step906]: loss 0.591554
[epoch14, step907]: loss 0.634749
[epoch14, step908]: loss 0.393762
[epoch14, step909]: loss 0.656211
[epoch14, step910]: loss 0.450069
[epoch14, step911]: loss 0.477505
[epoch14, step912]: loss 0.606823
[epoch14, step913]: loss 0.555414
[epoch14, step914]: loss 0.671977
[epoch14, step915]: loss 0.610213
[epoch14, step916]: loss 0.576732
[epoch14, step917]: loss 0.267039
[epoch14, step918]: loss 0.613914
[epoch14, step919]: loss 0.245320
[epoch14, step920]: loss 0.556748
[epoch14, step921]: loss 0.428367
[epoch14, step922]: loss 0.431491
[epoch14, step923]: loss 0.679112
[epoch14, step924]: loss 0.616060
[epoch14, step925]: loss 0.687951
[epoch14, step926]: loss 0.277975
[epoch14, step927]: loss 0.518090
[epoch14, step928]: loss 0.584016
[epoch14, step929]: loss 0.384971
[epoch14, step930]: loss 0.550937
[epoch14, step931]: loss 0.461843
[epoch14, step932]: loss 0.396267
[epoch14, step933]: loss 0.461901
[epoch14, step934]: loss 0.630018
[epoch14, step935]: loss 0.686031
[epoch14, step936]: loss 0.566634
[epoch14, step937]: loss 0.634977
[epoch14, step938]: loss 0.429527
[epoch14, step939]: loss 0.499103
[epoch14, step940]: loss 0.517154
[epoch14, step941]: loss 0.700715
[epoch14, step942]: loss 0.415952
[epoch14, step943]: loss 0.617182
[epoch14, step944]: loss 0.606310
[epoch14, step945]: loss 0.546284
[epoch14, step946]: loss 0.632841
[epoch14, step947]: loss 0.335719
[epoch14, step948]: loss 0.675829
[epoch14, step949]: loss 0.777379
[epoch14, step950]: loss 0.292451
[epoch14, step951]: loss 0.400437
[epoch14, step952]: loss 0.521432
[epoch14, step953]: loss 0.453533
[epoch14, step954]: loss 0.608509
[epoch14, step955]: loss 0.572250
[epoch14, step956]: loss 0.272413
[epoch14, step957]: loss 0.458401
[epoch14, step958]: loss 0.816889
[epoch14, step959]: loss 0.713507
[epoch14, step960]: loss 0.357834
[epoch14, step961]: loss 0.560444
[epoch14, step962]: loss 0.659438
[epoch14, step963]: loss 0.361254
[epoch14, step964]: loss 0.732751
[epoch14, step965]: loss 0.514109
[epoch14, step966]: loss 0.649293
[epoch14, step967]: loss 0.545147
[epoch14, step968]: loss 0.730706
[epoch14, step969]: loss 0.723566
[epoch14, step970]: loss 0.466571
[epoch14, step971]: loss 0.417342
[epoch14, step972]: loss 0.671711
[epoch14, step973]: loss 0.495695
[epoch14, step974]: loss 0.497261
[epoch14, step975]: loss 0.548966
[epoch14, step976]: loss 0.694173
[epoch14, step977]: loss 0.585995
[epoch14, step978]: loss 0.648655
[epoch14, step979]: loss 0.266322
[epoch14, step980]: loss 0.350875
[epoch14, step981]: loss 0.573832
[epoch14, step982]: loss 0.802613
[epoch14, step983]: loss 0.713564
[epoch14, step984]: loss 0.537825
[epoch14, step985]: loss 0.667396
[epoch14, step986]: loss 0.331363
[epoch14, step987]: loss 0.697736
[epoch14, step988]: loss 0.441057
[epoch14, step989]: loss 0.675353
[epoch14, step990]: loss 0.709312
[epoch14, step991]: loss 0.361932
[epoch14, step992]: loss 0.252485
[epoch14, step993]: loss 0.606633
[epoch14, step994]: loss 0.731542
[epoch14, step995]: loss 0.520281
[epoch14, step996]: loss 0.419783
[epoch14, step997]: loss 0.387327
[epoch14, step998]: loss 0.646584
[epoch14, step999]: loss 0.514175
[epoch14, step1000]: loss 0.685926
[epoch14, step1001]: loss 0.453034
[epoch14, step1002]: loss 0.432802
[epoch14, step1003]: loss 0.587875
[epoch14, step1004]: loss 0.572905
[epoch14, step1005]: loss 0.563565
[epoch14, step1006]: loss 0.444800
[epoch14, step1007]: loss 0.574650
[epoch14, step1008]: loss 0.835255
[epoch14, step1009]: loss 0.662836
[epoch14, step1010]: loss 0.599938
[epoch14, step1011]: loss 0.393482
[epoch14, step1012]: loss 0.549467
[epoch14, step1013]: loss 0.151727
[epoch14, step1014]: loss 0.467854
[epoch14, step1015]: loss 0.617880
[epoch14, step1016]: loss 0.488421
[epoch14, step1017]: loss 0.651657
[epoch14, step1018]: loss 0.417430
[epoch14, step1019]: loss 0.773640
[epoch14, step1020]: loss 0.601570
[epoch14, step1021]: loss 0.571612
[epoch14, step1022]: loss 0.670949
[epoch14, step1023]: loss 0.390057
[epoch14, step1024]: loss 0.598732
[epoch14, step1025]: loss 0.589856
[epoch14, step1026]: loss 0.393163
[epoch14, step1027]: loss 0.312924
[epoch14, step1028]: loss 0.597832
[epoch14, step1029]: loss 0.618552
[epoch14, step1030]: loss 0.438750
[epoch14, step1031]: loss 0.703003
[epoch14, step1032]: loss 0.714935
[epoch14, step1033]: loss 0.576886
[epoch14, step1034]: loss 0.498555
[epoch14, step1035]: loss 0.543181
[epoch14, step1036]: loss 0.217115
[epoch14, step1037]: loss 0.790782
[epoch14, step1038]: loss 0.594971
[epoch14, step1039]: loss 0.627082
[epoch14, step1040]: loss 0.535587
[epoch14, step1041]: loss 0.245886
[epoch14, step1042]: loss 0.448475
[epoch14, step1043]: loss 0.596274
[epoch14, step1044]: loss 0.698203
[epoch14, step1045]: loss 0.760997
[epoch14, step1046]: loss 0.387471
[epoch14, step1047]: loss 0.650315
[epoch14, step1048]: loss 0.424971
[epoch14, step1049]: loss 0.500349
[epoch14, step1050]: loss 0.349667
[epoch14, step1051]: loss 0.686821
[epoch14, step1052]: loss 0.462626
[epoch14, step1053]: loss 0.794898
[epoch14, step1054]: loss 0.563093
[epoch14, step1055]: loss 0.634386
[epoch14, step1056]: loss 0.564188
[epoch14, step1057]: loss 0.620738
[epoch14, step1058]: loss 0.742388
[epoch14, step1059]: loss 0.447086
[epoch14, step1060]: loss 0.525891
[epoch14, step1061]: loss 0.539539
[epoch14, step1062]: loss 0.505178
[epoch14, step1063]: loss 0.568408
[epoch14, step1064]: loss 0.637273
[epoch14, step1065]: loss 0.707501
[epoch14, step1066]: loss 0.572835
[epoch14, step1067]: loss 0.384803
[epoch14, step1068]: loss 0.460962
[epoch14, step1069]: loss 0.531610
[epoch14, step1070]: loss 0.447220
[epoch14, step1071]: loss 0.609125
[epoch14, step1072]: loss 0.561657
[epoch14, step1073]: loss 0.828262
[epoch14, step1074]: loss 0.505968
[epoch14, step1075]: loss 0.353999
[epoch14, step1076]: loss 0.505667
[epoch14, step1077]: loss 0.554227
[epoch14, step1078]: loss 0.604228
[epoch14, step1079]: loss 0.358948
[epoch14, step1080]: loss 0.612715
[epoch14, step1081]: loss 0.658468
[epoch14, step1082]: loss 0.524613
[epoch14, step1083]: loss 0.499136
[epoch14, step1084]: loss 0.413713
[epoch14, step1085]: loss 0.556867
[epoch14, step1086]: loss 0.550897
[epoch14, step1087]: loss 0.364094
[epoch14, step1088]: loss 0.542610
[epoch14, step1089]: loss 0.744811
[epoch14, step1090]: loss 0.380531
[epoch14, step1091]: loss 0.452160
[epoch14, step1092]: loss 0.406859
[epoch14, step1093]: loss 0.554366
[epoch14, step1094]: loss 0.432101
[epoch14, step1095]: loss 0.602682
[epoch14, step1096]: loss 0.456872
[epoch14, step1097]: loss 0.344585
[epoch14, step1098]: loss 0.465440
[epoch14, step1099]: loss 0.520611
[epoch14, step1100]: loss 0.611602
[epoch14, step1101]: loss 0.682535
[epoch14, step1102]: loss 0.621661
[epoch14, step1103]: loss 0.639475
[epoch14, step1104]: loss 0.528648
[epoch14, step1105]: loss 0.295334
[epoch14, step1106]: loss 0.617783
[epoch14, step1107]: loss 0.712944
[epoch14, step1108]: loss 0.551051
[epoch14, step1109]: loss 0.626463
[epoch14, step1110]: loss 0.585216
[epoch14, step1111]: loss 0.416208
[epoch14, step1112]: loss 0.488712
[epoch14, step1113]: loss 0.573616
[epoch14, step1114]: loss 0.492688
[epoch14, step1115]: loss 0.707142
[epoch14, step1116]: loss 0.358560
[epoch14, step1117]: loss 0.473772
[epoch14, step1118]: loss 0.571910
[epoch14, step1119]: loss 0.698452
[epoch14, step1120]: loss 0.454421
[epoch14, step1121]: loss 0.574231
[epoch14, step1122]: loss 0.583826
[epoch14, step1123]: loss 0.582653
[epoch14, step1124]: loss 0.557787
[epoch14, step1125]: loss 0.588582
[epoch14, step1126]: loss 0.551974
[epoch14, step1127]: loss 0.601195
[epoch14, step1128]: loss 0.784408
[epoch14, step1129]: loss 0.612337
[epoch14, step1130]: loss 0.362677
[epoch14, step1131]: loss 0.570618
[epoch14, step1132]: loss 0.446169
[epoch14, step1133]: loss 0.457334
[epoch14, step1134]: loss 0.650934
[epoch14, step1135]: loss 0.307725
[epoch14, step1136]: loss 0.679165
[epoch14, step1137]: loss 0.361841
[epoch14, step1138]: loss 0.402231
[epoch14, step1139]: loss 0.571194
[epoch14, step1140]: loss 0.513448
[epoch14, step1141]: loss 0.250347
[epoch14, step1142]: loss 0.612259
[epoch14, step1143]: loss 0.681995
[epoch14, step1144]: loss 0.732449
[epoch14, step1145]: loss 0.674791
[epoch14, step1146]: loss 0.477166
[epoch14, step1147]: loss 0.381553
[epoch14, step1148]: loss 0.162068
[epoch14, step1149]: loss 0.436035
[epoch14, step1150]: loss 0.562792
[epoch14, step1151]: loss 0.810961
[epoch14, step1152]: loss 0.559457
[epoch14, step1153]: loss 0.387763
[epoch14, step1154]: loss 0.327025
[epoch14, step1155]: loss 0.659695
[epoch14, step1156]: loss 0.539538
[epoch14, step1157]: loss 0.562438
[epoch14, step1158]: loss 0.528798
[epoch14, step1159]: loss 0.320671
[epoch14, step1160]: loss 0.444455
[epoch14, step1161]: loss 0.493684
[epoch14, step1162]: loss 0.684476
[epoch14, step1163]: loss 0.692596
[epoch14, step1164]: loss 0.498452
[epoch14, step1165]: loss 0.706884
[epoch14, step1166]: loss 0.584175
[epoch14, step1167]: loss 0.445879
[epoch14, step1168]: loss 0.533771
[epoch14, step1169]: loss 0.552392
[epoch14, step1170]: loss 0.508301
[epoch14, step1171]: loss 0.560516
[epoch14, step1172]: loss 0.668316
[epoch14, step1173]: loss 0.531664
[epoch14, step1174]: loss 0.646952
[epoch14, step1175]: loss 0.618937
[epoch14, step1176]: loss 0.473960
[epoch14, step1177]: loss 0.522325
[epoch14, step1178]: loss 0.655374
[epoch14, step1179]: loss 0.579600
[epoch14, step1180]: loss 0.433790
[epoch14, step1181]: loss 0.449526
[epoch14, step1182]: loss 0.458646
[epoch14, step1183]: loss 0.714814
[epoch14, step1184]: loss 0.465571
[epoch14, step1185]: loss 0.572305
[epoch14, step1186]: loss 0.524332
[epoch14, step1187]: loss 0.533646
[epoch14, step1188]: loss 0.716981
[epoch14, step1189]: loss 0.697975
[epoch14, step1190]: loss 0.771931
[epoch14, step1191]: loss 0.607821
[epoch14, step1192]: loss 0.665673
[epoch14, step1193]: loss 0.432340
[epoch14, step1194]: loss 0.463334
[epoch14, step1195]: loss 0.262802
[epoch14, step1196]: loss 0.338660
[epoch14, step1197]: loss 0.580082
[epoch14, step1198]: loss 0.764729
[epoch14, step1199]: loss 0.738374
[epoch14, step1200]: loss 0.661065
[epoch14, step1201]: loss 0.682977
[epoch14, step1202]: loss 0.500098
[epoch14, step1203]: loss 0.632682
[epoch14, step1204]: loss 0.793802
[epoch14, step1205]: loss 0.369920
[epoch14, step1206]: loss 0.300965
[epoch14, step1207]: loss 0.681162
[epoch14, step1208]: loss 0.424749
[epoch14, step1209]: loss 0.539594
[epoch14, step1210]: loss 0.619522
[epoch14, step1211]: loss 0.483061
[epoch14, step1212]: loss 0.614919
[epoch14, step1213]: loss 0.493743
[epoch14, step1214]: loss 0.569869
[epoch14, step1215]: loss 0.619275
[epoch14, step1216]: loss 0.718882
[epoch14, step1217]: loss 0.590470
[epoch14, step1218]: loss 0.693150
[epoch14, step1219]: loss 0.357279
[epoch14, step1220]: loss 0.727862
[epoch14, step1221]: loss 0.768578
[epoch14, step1222]: loss 0.403298
[epoch14, step1223]: loss 0.373189
[epoch14, step1224]: loss 0.588160
[epoch14, step1225]: loss 0.748958
[epoch14, step1226]: loss 0.415680
[epoch14, step1227]: loss 0.628028
[epoch14, step1228]: loss 0.394592
[epoch14, step1229]: loss 0.518153
[epoch14, step1230]: loss 0.559526
[epoch14, step1231]: loss 0.431255
[epoch14, step1232]: loss 0.648523
[epoch14, step1233]: loss 0.686739
[epoch14, step1234]: loss 0.471414
[epoch14, step1235]: loss 0.570186
[epoch14, step1236]: loss 0.469316
[epoch14, step1237]: loss 0.550893
[epoch14, step1238]: loss 0.416387
[epoch14, step1239]: loss 0.942739
[epoch14, step1240]: loss 0.606141
[epoch14, step1241]: loss 0.754923
[epoch14, step1242]: loss 0.784535
[epoch14, step1243]: loss 0.614200
[epoch14, step1244]: loss 0.640550
[epoch14, step1245]: loss 0.358159
[epoch14, step1246]: loss 0.580505
[epoch14, step1247]: loss 0.754057
[epoch14, step1248]: loss 0.287256
[epoch14, step1249]: loss 0.655210
[epoch14, step1250]: loss 0.518300
[epoch14, step1251]: loss 0.607576
[epoch14, step1252]: loss 0.593576
[epoch14, step1253]: loss 0.682427
[epoch14, step1254]: loss 0.690504
[epoch14, step1255]: loss 0.607698
[epoch14, step1256]: loss 0.587056
[epoch14, step1257]: loss 0.486795
[epoch14, step1258]: loss 0.696060
[epoch14, step1259]: loss 0.577782
[epoch14, step1260]: loss 0.577758
[epoch14, step1261]: loss 0.431015
[epoch14, step1262]: loss 0.634436
[epoch14, step1263]: loss 0.772882
[epoch14, step1264]: loss 0.333791
[epoch14, step1265]: loss 0.415650
[epoch14, step1266]: loss 0.526352
[epoch14, step1267]: loss 0.639965
[epoch14, step1268]: loss 0.826549
[epoch14, step1269]: loss 0.629897
[epoch14, step1270]: loss 0.574366
[epoch14, step1271]: loss 0.690922
[epoch14, step1272]: loss 0.472545
[epoch14, step1273]: loss 0.484030
[epoch14, step1274]: loss 0.641828
[epoch14, step1275]: loss 0.635111
[epoch14, step1276]: loss 0.391196
[epoch14, step1277]: loss 0.616046
[epoch14, step1278]: loss 0.526390
[epoch14, step1279]: loss 0.430344
[epoch14, step1280]: loss 0.425627
[epoch14, step1281]: loss 0.423891
[epoch14, step1282]: loss 0.524381
[epoch14, step1283]: loss 0.515117
[epoch14, step1284]: loss 0.521440
[epoch14, step1285]: loss 0.518938
[epoch14, step1286]: loss 0.602512
[epoch14, step1287]: loss 0.503278
[epoch14, step1288]: loss 0.675390
[epoch14, step1289]: loss 0.721270
[epoch14, step1290]: loss 0.609338
[epoch14, step1291]: loss 0.821460
[epoch14, step1292]: loss 0.728740
[epoch14, step1293]: loss 0.621211
[epoch14, step1294]: loss 0.459398
[epoch14, step1295]: loss 0.693995
[epoch14, step1296]: loss 0.467123
[epoch14, step1297]: loss 0.423351
[epoch14, step1298]: loss 0.562494
[epoch14, step1299]: loss 0.585559
[epoch14, step1300]: loss 0.290215
[epoch14, step1301]: loss 0.504571
[epoch14, step1302]: loss 0.470817
[epoch14, step1303]: loss 0.807408
[epoch14, step1304]: loss 0.627708
[epoch14, step1305]: loss 0.551367
[epoch14, step1306]: loss 0.831673
[epoch14, step1307]: loss 0.443215
[epoch14, step1308]: loss 0.490266
[epoch14, step1309]: loss 0.641471
[epoch14, step1310]: loss 0.658541
[epoch14, step1311]: loss 0.718637
[epoch14, step1312]: loss 0.403260
[epoch14, step1313]: loss 0.707670
[epoch14, step1314]: loss 0.622446
[epoch14, step1315]: loss 0.633368
[epoch14, step1316]: loss 0.518702
[epoch14, step1317]: loss 0.555466
[epoch14, step1318]: loss 0.697871
[epoch14, step1319]: loss 0.680868
[epoch14, step1320]: loss 0.479663
[epoch14, step1321]: loss 0.535998
[epoch14, step1322]: loss 0.612953
[epoch14, step1323]: loss 0.580079
[epoch14, step1324]: loss 0.563379
[epoch14, step1325]: loss 0.726550
[epoch14, step1326]: loss 0.555217
[epoch14, step1327]: loss 0.403200
[epoch14, step1328]: loss 0.586951
[epoch14, step1329]: loss 0.307734
[epoch14, step1330]: loss 0.775922
[epoch14, step1331]: loss 0.471648
[epoch14, step1332]: loss 0.600443
[epoch14, step1333]: loss 0.602406
[epoch14, step1334]: loss 0.693719
[epoch14, step1335]: loss 0.559355
[epoch14, step1336]: loss 0.434272
[epoch14, step1337]: loss 0.611062
[epoch14, step1338]: loss 0.529727
[epoch14, step1339]: loss 0.658973
[epoch14, step1340]: loss 0.368553
[epoch14, step1341]: loss 0.650236
[epoch14, step1342]: loss 0.394278
[epoch14, step1343]: loss 0.547995
[epoch14, step1344]: loss 0.602188
[epoch14, step1345]: loss 0.376245
[epoch14, step1346]: loss 0.641735
[epoch14, step1347]: loss 0.343011
[epoch14, step1348]: loss 0.522360
[epoch14, step1349]: loss 0.601654
[epoch14, step1350]: loss 0.579443
[epoch14, step1351]: loss 0.602066
[epoch14, step1352]: loss 0.710508
[epoch14, step1353]: loss 0.431337
[epoch14, step1354]: loss 0.453555
[epoch14, step1355]: loss 0.767775
[epoch14, step1356]: loss 0.438701
[epoch14, step1357]: loss 0.576148
[epoch14, step1358]: loss 0.249212
[epoch14, step1359]: loss 0.517646
[epoch14, step1360]: loss 0.333786
[epoch14, step1361]: loss 0.641644
[epoch14, step1362]: loss 0.563570
[epoch14, step1363]: loss 0.308678
[epoch14, step1364]: loss 0.407681
[epoch14, step1365]: loss 0.564592
[epoch14, step1366]: loss 0.713066
[epoch14, step1367]: loss 0.492381
[epoch14, step1368]: loss 0.481096
[epoch14, step1369]: loss 0.565473
[epoch14, step1370]: loss 0.590236
[epoch14, step1371]: loss 0.453711
[epoch14, step1372]: loss 0.464747
[epoch14, step1373]: loss 0.531367
[epoch14, step1374]: loss 0.593811
[epoch14, step1375]: loss 0.199235
[epoch14, step1376]: loss 0.539880
[epoch14, step1377]: loss 0.445349
[epoch14, step1378]: loss 0.659287
[epoch14, step1379]: loss 0.594746
[epoch14, step1380]: loss 0.607538
[epoch14, step1381]: loss 0.715457
[epoch14, step1382]: loss 0.696769
[epoch14, step1383]: loss 0.363840
[epoch14, step1384]: loss 0.435971
[epoch14, step1385]: loss 0.476745
[epoch14, step1386]: loss 0.703758
[epoch14, step1387]: loss 0.296019
[epoch14, step1388]: loss 0.716688
[epoch14, step1389]: loss 0.388962
[epoch14, step1390]: loss 0.313213
[epoch14, step1391]: loss 0.553075
[epoch14, step1392]: loss 0.503210
[epoch14, step1393]: loss 0.458452
[epoch14, step1394]: loss 0.571936
[epoch14, step1395]: loss 0.524766
[epoch14, step1396]: loss 0.595645
[epoch14, step1397]: loss 0.450846
[epoch14, step1398]: loss 0.542585
[epoch14, step1399]: loss 0.561212
[epoch14, step1400]: loss 0.799701
[epoch14, step1401]: loss 0.497280
[epoch14, step1402]: loss 0.494823
[epoch14, step1403]: loss 0.643433
[epoch14, step1404]: loss 0.766964
[epoch14, step1405]: loss 0.765523
[epoch14, step1406]: loss 0.692432
[epoch14, step1407]: loss 0.587815
[epoch14, step1408]: loss 0.569149
[epoch14, step1409]: loss 0.457096
[epoch14, step1410]: loss 0.603619
[epoch14, step1411]: loss 0.497252
[epoch14, step1412]: loss 0.626044
[epoch14, step1413]: loss 0.522990
[epoch14, step1414]: loss 0.215442
[epoch14, step1415]: loss 0.542358
[epoch14, step1416]: loss 0.392184
[epoch14, step1417]: loss 0.566859
[epoch14, step1418]: loss 0.673039
[epoch14, step1419]: loss 0.574776
[epoch14, step1420]: loss 0.611896
[epoch14, step1421]: loss 0.457180
[epoch14, step1422]: loss 0.421549
[epoch14, step1423]: loss 0.559796
[epoch14, step1424]: loss 0.558334
[epoch14, step1425]: loss 0.656657
[epoch14, step1426]: loss 0.455733
[epoch14, step1427]: loss 0.691367
[epoch14, step1428]: loss 0.596676
[epoch14, step1429]: loss 0.358807
[epoch14, step1430]: loss 0.681562
[epoch14, step1431]: loss 0.523291
[epoch14, step1432]: loss 0.676650
[epoch14, step1433]: loss 0.722202
[epoch14, step1434]: loss 0.263351
[epoch14, step1435]: loss 0.311551
[epoch14, step1436]: loss 0.671038
[epoch14, step1437]: loss 0.822923
[epoch14, step1438]: loss 0.514846
[epoch14, step1439]: loss 0.522914
[epoch14, step1440]: loss 0.665511
[epoch14, step1441]: loss 0.586892
[epoch14, step1442]: loss 0.243759
[epoch14, step1443]: loss 0.617397
[epoch14, step1444]: loss 0.735320
[epoch14, step1445]: loss 0.520559
[epoch14, step1446]: loss 0.595534
[epoch14, step1447]: loss 0.505976
[epoch14, step1448]: loss 0.134851
[epoch14, step1449]: loss 0.601244
[epoch14, step1450]: loss 0.910966
[epoch14, step1451]: loss 0.601043
[epoch14, step1452]: loss 0.625788
[epoch14, step1453]: loss 0.620170
[epoch14, step1454]: loss 0.430654
[epoch14, step1455]: loss 0.482292
[epoch14, step1456]: loss 0.685947
[epoch14, step1457]: loss 0.465823
[epoch14, step1458]: loss 0.581406
[epoch14, step1459]: loss 0.546219
[epoch14, step1460]: loss 0.618429
[epoch14, step1461]: loss 0.706894
[epoch14, step1462]: loss 0.495713
[epoch14, step1463]: loss 0.630977
[epoch14, step1464]: loss 0.850862
[epoch14, step1465]: loss 0.561048
[epoch14, step1466]: loss 0.535340
[epoch14, step1467]: loss 0.556339
[epoch14, step1468]: loss 0.616543
[epoch14, step1469]: loss 0.488185
[epoch14, step1470]: loss 0.530196
[epoch14, step1471]: loss 0.376297
[epoch14, step1472]: loss 0.331601
[epoch14, step1473]: loss 0.475060
[epoch14, step1474]: loss 0.699569
[epoch14, step1475]: loss 0.526036
[epoch14, step1476]: loss 0.431071
[epoch14, step1477]: loss 0.411113
[epoch14, step1478]: loss 0.443088
[epoch14, step1479]: loss 0.567323
[epoch14, step1480]: loss 0.787049
[epoch14, step1481]: loss 0.445173
[epoch14, step1482]: loss 0.592504
[epoch14, step1483]: loss 0.653495
[epoch14, step1484]: loss 0.735696
[epoch14, step1485]: loss 0.508861
[epoch14, step1486]: loss 0.545285
[epoch14, step1487]: loss 0.446913
[epoch14, step1488]: loss 0.564269
[epoch14, step1489]: loss 0.417068
[epoch14, step1490]: loss 0.343011
[epoch14, step1491]: loss 0.583340
[epoch14, step1492]: loss 0.618996
[epoch14, step1493]: loss 0.560771
[epoch14, step1494]: loss 0.340688
[epoch14, step1495]: loss 0.318199
[epoch14, step1496]: loss 0.392314
[epoch14, step1497]: loss 0.602437
[epoch14, step1498]: loss 0.703229
[epoch14, step1499]: loss 0.715218
[epoch14, step1500]: loss 0.438394
[epoch14, step1501]: loss 0.717242
[epoch14, step1502]: loss 0.372998
[epoch14, step1503]: loss 0.732387
[epoch14, step1504]: loss 0.614367
[epoch14, step1505]: loss 0.820942
[epoch14, step1506]: loss 0.405509
[epoch14, step1507]: loss 0.739681
[epoch14, step1508]: loss 0.158211
[epoch14, step1509]: loss 0.561802
[epoch14, step1510]: loss 0.619011
[epoch14, step1511]: loss 0.500185
[epoch14, step1512]: loss 0.486101
[epoch14, step1513]: loss 0.599617
[epoch14, step1514]: loss 0.751239
[epoch14, step1515]: loss 0.644855
[epoch14, step1516]: loss 0.553513
[epoch14, step1517]: loss 0.644225
[epoch14, step1518]: loss 0.354828
[epoch14, step1519]: loss 0.550048
[epoch14, step1520]: loss 0.499831
[epoch14, step1521]: loss 0.633996
[epoch14, step1522]: loss 0.401941
[epoch14, step1523]: loss 0.548810
[epoch14, step1524]: loss 0.655096
[epoch14, step1525]: loss 0.407575
[epoch14, step1526]: loss 0.557750
[epoch14, step1527]: loss 0.739281
[epoch14, step1528]: loss 0.520129
[epoch14, step1529]: loss 0.645136
[epoch14, step1530]: loss 0.503656
[epoch14, step1531]: loss 0.587804
[epoch14, step1532]: loss 0.612629
[epoch14, step1533]: loss 0.524970
[epoch14, step1534]: loss 0.632488
[epoch14, step1535]: loss 0.491817
[epoch14, step1536]: loss 0.402094
[epoch14, step1537]: loss 0.759561
[epoch14, step1538]: loss 0.689596
[epoch14, step1539]: loss 0.581536
[epoch14, step1540]: loss 0.732575
[epoch14, step1541]: loss 0.484303
[epoch14, step1542]: loss 0.408169
[epoch14, step1543]: loss 0.282649
[epoch14, step1544]: loss 0.497144
[epoch14, step1545]: loss 0.433307
[epoch14, step1546]: loss 0.491434
[epoch14, step1547]: loss 0.589903
[epoch14, step1548]: loss 0.627815
[epoch14, step1549]: loss 0.467759
[epoch14, step1550]: loss 0.367954
[epoch14, step1551]: loss 0.652830
[epoch14, step1552]: loss 0.487122
[epoch14, step1553]: loss 0.684340
[epoch14, step1554]: loss 0.546751
[epoch14, step1555]: loss 0.675967
[epoch14, step1556]: loss 0.393407
[epoch14, step1557]: loss 0.560260
[epoch14, step1558]: loss 0.304506
[epoch14, step1559]: loss 0.464744
[epoch14, step1560]: loss 0.477308
[epoch14, step1561]: loss 0.776391
[epoch14, step1562]: loss 0.710194
[epoch14, step1563]: loss 0.655321
[epoch14, step1564]: loss 0.399344
[epoch14, step1565]: loss 0.342406
[epoch14, step1566]: loss 0.652070
[epoch14, step1567]: loss 0.284020
[epoch14, step1568]: loss 0.445217
[epoch14, step1569]: loss 0.695678
[epoch14, step1570]: loss 0.645520
[epoch14, step1571]: loss 0.435740
[epoch14, step1572]: loss 0.695278
[epoch14, step1573]: loss 0.630573
[epoch14, step1574]: loss 0.498127
[epoch14, step1575]: loss 0.709903
[epoch14, step1576]: loss 0.582423
[epoch14, step1577]: loss 0.742277
[epoch14, step1578]: loss 0.717967
[epoch14, step1579]: loss 0.674061
[epoch14, step1580]: loss 0.458681
[epoch14, step1581]: loss 0.787287
[epoch14, step1582]: loss 0.486005
[epoch14, step1583]: loss 0.451516
[epoch14, step1584]: loss 0.569626
[epoch14, step1585]: loss 0.622163
[epoch14, step1586]: loss 0.580368
[epoch14, step1587]: loss 0.432255
[epoch14, step1588]: loss 0.593098
[epoch14, step1589]: loss 0.636757
[epoch14, step1590]: loss 0.561253
[epoch14, step1591]: loss 0.812129
[epoch14, step1592]: loss 0.352013
[epoch14, step1593]: loss 0.533660
[epoch14, step1594]: loss 0.546196
[epoch14, step1595]: loss 0.436997
[epoch14, step1596]: loss 0.195025
[epoch14, step1597]: loss 0.463337
[epoch14, step1598]: loss 0.681709
[epoch14, step1599]: loss 0.718280
[epoch14, step1600]: loss 0.648856
[epoch14, step1601]: loss 0.626180
[epoch14, step1602]: loss 0.684695
[epoch14, step1603]: loss 0.630269
[epoch14, step1604]: loss 0.704169
[epoch14, step1605]: loss 0.666024
[epoch14, step1606]: loss 0.415027
[epoch14, step1607]: loss 0.615977
[epoch14, step1608]: loss 0.556969
[epoch14, step1609]: loss 0.609390
[epoch14, step1610]: loss 0.511527
[epoch14, step1611]: loss 0.400359
[epoch14, step1612]: loss 0.293322
[epoch14, step1613]: loss 0.597470
[epoch14, step1614]: loss 0.565735
[epoch14, step1615]: loss 0.610419
[epoch14, step1616]: loss 0.409951
[epoch14, step1617]: loss 0.775452
[epoch14, step1618]: loss 0.449409
[epoch14, step1619]: loss 0.444456
[epoch14, step1620]: loss 0.516129
[epoch14, step1621]: loss 0.657399
[epoch14, step1622]: loss 0.606887
[epoch14, step1623]: loss 0.587331
[epoch14, step1624]: loss 0.750123
[epoch14, step1625]: loss 0.686608
[epoch14, step1626]: loss 0.474679
[epoch14, step1627]: loss 0.339889
[epoch14, step1628]: loss 0.680149
[epoch14, step1629]: loss 0.414321
[epoch14, step1630]: loss 0.701483
[epoch14, step1631]: loss 0.482201
[epoch14, step1632]: loss 0.644035
[epoch14, step1633]: loss 0.345129
[epoch14, step1634]: loss 0.757935
[epoch14, step1635]: loss 0.636209
[epoch14, step1636]: loss 0.557312
[epoch14, step1637]: loss 0.635918
[epoch14, step1638]: loss 0.647301
[epoch14, step1639]: loss 0.763406
[epoch14, step1640]: loss 0.368673
[epoch14, step1641]: loss 0.627414
[epoch14, step1642]: loss 0.484764
[epoch14, step1643]: loss 0.437068
[epoch14, step1644]: loss 0.615477
[epoch14, step1645]: loss 0.634718
[epoch14, step1646]: loss 0.597808
[epoch14, step1647]: loss 0.707042
[epoch14, step1648]: loss 0.461217
[epoch14, step1649]: loss 0.585608
[epoch14, step1650]: loss 0.395866
[epoch14, step1651]: loss 0.723618
[epoch14, step1652]: loss 0.446798
[epoch14, step1653]: loss 0.541412
[epoch14, step1654]: loss 0.561196
[epoch14, step1655]: loss 0.517086
[epoch14, step1656]: loss 0.619159
[epoch14, step1657]: loss 0.418958
[epoch14, step1658]: loss 0.636321
[epoch14, step1659]: loss 0.239824
[epoch14, step1660]: loss 0.729414
[epoch14, step1661]: loss 0.651846
[epoch14, step1662]: loss 0.496047
[epoch14, step1663]: loss 0.347763
[epoch14, step1664]: loss 0.667990
[epoch14, step1665]: loss 0.468423
[epoch14, step1666]: loss 0.461997
[epoch14, step1667]: loss 0.426656
[epoch14, step1668]: loss 0.675567
[epoch14, step1669]: loss 0.568839
[epoch14, step1670]: loss 0.467245
[epoch14, step1671]: loss 0.500830
[epoch14, step1672]: loss 0.444735
[epoch14, step1673]: loss 0.605705
[epoch14, step1674]: loss 0.752157
[epoch14, step1675]: loss 0.414357
[epoch14, step1676]: loss 0.708958
[epoch14, step1677]: loss 0.687986
[epoch14, step1678]: loss 0.495944
[epoch14, step1679]: loss 0.388500
[epoch14, step1680]: loss 0.647329
[epoch14, step1681]: loss 0.643240
[epoch14, step1682]: loss 0.634297
[epoch14, step1683]: loss 0.468033
[epoch14, step1684]: loss 0.661363
[epoch14, step1685]: loss 0.609899
[epoch14, step1686]: loss 0.705932
[epoch14, step1687]: loss 0.291100
[epoch14, step1688]: loss 0.561588
[epoch14, step1689]: loss 0.321520
[epoch14, step1690]: loss 0.499448
[epoch14, step1691]: loss 0.622436
[epoch14, step1692]: loss 0.688945
[epoch14, step1693]: loss 0.629613
[epoch14, step1694]: loss 0.553835
[epoch14, step1695]: loss 0.446916
[epoch14, step1696]: loss 0.301794
[epoch14, step1697]: loss 0.713115
[epoch14, step1698]: loss 0.493683
[epoch14, step1699]: loss 0.630519
[epoch14, step1700]: loss 0.821157
[epoch14, step1701]: loss 0.697646
[epoch14, step1702]: loss 0.550905
[epoch14, step1703]: loss 0.507705
[epoch14, step1704]: loss 0.452227
[epoch14, step1705]: loss 0.614894
[epoch14, step1706]: loss 0.649242
[epoch14, step1707]: loss 0.600762
[epoch14, step1708]: loss 0.753541
[epoch14, step1709]: loss 0.305504
[epoch14, step1710]: loss 0.597078
[epoch14, step1711]: loss 0.365741
[epoch14, step1712]: loss 0.659690
[epoch14, step1713]: loss 0.387814
[epoch14, step1714]: loss 0.665958
[epoch14, step1715]: loss 0.880822
[epoch14, step1716]: loss 0.674223
[epoch14, step1717]: loss 0.346398
[epoch14, step1718]: loss 0.376034
[epoch14, step1719]: loss 0.492089
[epoch14, step1720]: loss 0.476387
[epoch14, step1721]: loss 0.664901
[epoch14, step1722]: loss 0.692538
[epoch14, step1723]: loss 0.404174
[epoch14, step1724]: loss 0.743169
[epoch14, step1725]: loss 0.522913
[epoch14, step1726]: loss 0.406224
[epoch14, step1727]: loss 0.547654
[epoch14, step1728]: loss 0.518660
[epoch14, step1729]: loss 0.349141
[epoch14, step1730]: loss 0.416335
[epoch14, step1731]: loss 0.495043
[epoch14, step1732]: loss 0.451970
[epoch14, step1733]: loss 0.423436
[epoch14, step1734]: loss 0.690748
[epoch14, step1735]: loss 0.546666
[epoch14, step1736]: loss 0.481000
[epoch14, step1737]: loss 0.633622
[epoch14, step1738]: loss 0.435605
[epoch14, step1739]: loss 0.536367
[epoch14, step1740]: loss 0.628792
[epoch14, step1741]: loss 0.582158
[epoch14, step1742]: loss 0.551519
[epoch14, step1743]: loss 0.511874
[epoch14, step1744]: loss 0.495570
[epoch14, step1745]: loss 0.618649
[epoch14, step1746]: loss 0.509604
[epoch14, step1747]: loss 0.697816
[epoch14, step1748]: loss 0.648471
[epoch14, step1749]: loss 0.465828
[epoch14, step1750]: loss 0.870587
[epoch14, step1751]: loss 0.443316
[epoch14, step1752]: loss 0.588603
[epoch14, step1753]: loss 0.752805
[epoch14, step1754]: loss 0.486679
[epoch14, step1755]: loss 0.268295
[epoch14, step1756]: loss 0.304850
[epoch14, step1757]: loss 0.653387
[epoch14, step1758]: loss 0.393958
[epoch14, step1759]: loss 0.416285
[epoch14, step1760]: loss 0.620000
[epoch14, step1761]: loss 0.595367
[epoch14, step1762]: loss 0.562632
[epoch14, step1763]: loss 0.717146
[epoch14, step1764]: loss 0.576543
[epoch14, step1765]: loss 0.427789
[epoch14, step1766]: loss 0.723019
[epoch14, step1767]: loss 0.683602
[epoch14, step1768]: loss 0.699703
[epoch14, step1769]: loss 0.422758
[epoch14, step1770]: loss 0.564519
[epoch14, step1771]: loss 0.625631
[epoch14, step1772]: loss 0.334346
[epoch14, step1773]: loss 0.386464
[epoch14, step1774]: loss 0.612476
[epoch14, step1775]: loss 0.322024
[epoch14, step1776]: loss 0.626389
[epoch14, step1777]: loss 0.652665
[epoch14, step1778]: loss 0.617644
[epoch14, step1779]: loss 0.437918
[epoch14, step1780]: loss 0.455866
[epoch14, step1781]: loss 0.424604
[epoch14, step1782]: loss 0.556281
[epoch14, step1783]: loss 0.701936
[epoch14, step1784]: loss 0.314858
[epoch14, step1785]: loss 0.629072
[epoch14, step1786]: loss 0.741549
[epoch14, step1787]: loss 0.184774
[epoch14, step1788]: loss 0.688210
[epoch14, step1789]: loss 0.588342
[epoch14, step1790]: loss 0.576783
[epoch14, step1791]: loss 0.631984
[epoch14, step1792]: loss 0.363437
[epoch14, step1793]: loss 0.415759
[epoch14, step1794]: loss 0.671464
[epoch14, step1795]: loss 0.274592
[epoch14, step1796]: loss 0.489049
[epoch14, step1797]: loss 0.386144
[epoch14, step1798]: loss 0.502365
[epoch14, step1799]: loss 0.804489
[epoch14, step1800]: loss 0.447683
[epoch14, step1801]: loss 0.733020
[epoch14, step1802]: loss 0.454505
[epoch14, step1803]: loss 0.431894
[epoch14, step1804]: loss 0.559579
[epoch14, step1805]: loss 0.259997
[epoch14, step1806]: loss 0.687023
[epoch14, step1807]: loss 0.267273
[epoch14, step1808]: loss 0.533884
[epoch14, step1809]: loss 0.616616
[epoch14, step1810]: loss 0.450958
[epoch14, step1811]: loss 0.412129
[epoch14, step1812]: loss 0.509851
[epoch14, step1813]: loss 0.589829
[epoch14, step1814]: loss 0.474283
[epoch14, step1815]: loss 0.529216
[epoch14, step1816]: loss 0.580053
[epoch14, step1817]: loss 0.476515
[epoch14, step1818]: loss 0.472267
[epoch14, step1819]: loss 0.459552
[epoch14, step1820]: loss 0.519685
[epoch14, step1821]: loss 0.481083
[epoch14, step1822]: loss 0.676419
[epoch14, step1823]: loss 0.664697
[epoch14, step1824]: loss 0.510854
[epoch14, step1825]: loss 0.558904
[epoch14, step1826]: loss 0.697246
[epoch14, step1827]: loss 0.621213
[epoch14, step1828]: loss 0.613984
[epoch14, step1829]: loss 0.539052
[epoch14, step1830]: loss 0.547290
[epoch14, step1831]: loss 0.473795
[epoch14, step1832]: loss 0.586701
[epoch14, step1833]: loss 0.362071
[epoch14, step1834]: loss 0.577125
[epoch14, step1835]: loss 0.586038
[epoch14, step1836]: loss 0.440464
[epoch14, step1837]: loss 0.282732
[epoch14, step1838]: loss 0.771680
[epoch14, step1839]: loss 0.405147
[epoch14, step1840]: loss 0.447590
[epoch14, step1841]: loss 0.703283
[epoch14, step1842]: loss 0.754328
[epoch14, step1843]: loss 0.798509
[epoch14, step1844]: loss 0.484982
[epoch14, step1845]: loss 0.406079
[epoch14, step1846]: loss 0.550651
[epoch14, step1847]: loss 0.319570
[epoch14, step1848]: loss 0.493270
[epoch14, step1849]: loss 0.619188
[epoch14, step1850]: loss 0.468482
[epoch14, step1851]: loss 0.575514
[epoch14, step1852]: loss 0.613792
[epoch14, step1853]: loss 0.446099
[epoch14, step1854]: loss 0.495850
[epoch14, step1855]: loss 0.567749
[epoch14, step1856]: loss 0.662072
[epoch14, step1857]: loss 0.628221
[epoch14, step1858]: loss 0.538789
[epoch14, step1859]: loss 0.493496
[epoch14, step1860]: loss 0.628441
[epoch14, step1861]: loss 0.317151
[epoch14, step1862]: loss 0.421892
[epoch14, step1863]: loss 0.527166
[epoch14, step1864]: loss 0.550762
[epoch14, step1865]: loss 0.586509
[epoch14, step1866]: loss 0.493195
[epoch14, step1867]: loss 0.416267
[epoch14, step1868]: loss 0.709800
[epoch14, step1869]: loss 0.485732
[epoch14, step1870]: loss 0.676705
[epoch14, step1871]: loss 0.560210
[epoch14, step1872]: loss 0.624237
[epoch14, step1873]: loss 0.315752
[epoch14, step1874]: loss 0.559571
[epoch14, step1875]: loss 0.701497
[epoch14, step1876]: loss 0.404989
[epoch14, step1877]: loss 0.516566
[epoch14, step1878]: loss 0.490583
[epoch14, step1879]: loss 0.811736
[epoch14, step1880]: loss 0.622250
[epoch14, step1881]: loss 0.400122
[epoch14, step1882]: loss 0.299783
[epoch14, step1883]: loss 0.603663
[epoch14, step1884]: loss 0.511348
[epoch14, step1885]: loss 0.202531
[epoch14, step1886]: loss 0.606835
[epoch14, step1887]: loss 0.675753
[epoch14, step1888]: loss 0.668342
[epoch14, step1889]: loss 0.652258
[epoch14, step1890]: loss 0.477814
[epoch14, step1891]: loss 0.675670
[epoch14, step1892]: loss 0.522158
[epoch14, step1893]: loss 0.533396
[epoch14, step1894]: loss 0.378170
[epoch14, step1895]: loss 0.626450
[epoch14, step1896]: loss 0.594872
[epoch14, step1897]: loss 0.491825
[epoch14, step1898]: loss 0.712904
[epoch14, step1899]: loss 0.743787
[epoch14, step1900]: loss 0.582669
[epoch14, step1901]: loss 0.608357
[epoch14, step1902]: loss 0.414497
[epoch14, step1903]: loss 0.621671
[epoch14, step1904]: loss 0.489496
[epoch14, step1905]: loss 0.649428
[epoch14, step1906]: loss 0.714454
[epoch14, step1907]: loss 0.636476
[epoch14, step1908]: loss 0.319946
[epoch14, step1909]: loss 0.717086
[epoch14, step1910]: loss 0.596136
[epoch14, step1911]: loss 0.188237
[epoch14, step1912]: loss 0.605879
[epoch14, step1913]: loss 0.712885
[epoch14, step1914]: loss 0.512570
[epoch14, step1915]: loss 0.615883
[epoch14, step1916]: loss 0.498075
[epoch14, step1917]: loss 0.742056
[epoch14, step1918]: loss 0.814042
[epoch14, step1919]: loss 0.418630
[epoch14, step1920]: loss 0.694167
[epoch14, step1921]: loss 0.625638
[epoch14, step1922]: loss 0.336436
[epoch14, step1923]: loss 0.669312
[epoch14, step1924]: loss 0.599566
[epoch14, step1925]: loss 0.578355
[epoch14, step1926]: loss 0.183804
[epoch14, step1927]: loss 0.746972
[epoch14, step1928]: loss 0.222934
[epoch14, step1929]: loss 0.488262
[epoch14, step1930]: loss 0.493467
[epoch14, step1931]: loss 0.662651
[epoch14, step1932]: loss 0.591316
[epoch14, step1933]: loss 0.482983
[epoch14, step1934]: loss 0.332856
[epoch14, step1935]: loss 0.597365
[epoch14, step1936]: loss 0.574474
[epoch14, step1937]: loss 0.533272
[epoch14, step1938]: loss 0.384253
[epoch14, step1939]: loss 0.642532
[epoch14, step1940]: loss 0.311883
[epoch14, step1941]: loss 0.418958
[epoch14, step1942]: loss 0.766175
[epoch14, step1943]: loss 0.600571
[epoch14, step1944]: loss 0.626961
[epoch14, step1945]: loss 0.580871
[epoch14, step1946]: loss 0.602168
[epoch14, step1947]: loss 0.572931
[epoch14, step1948]: loss 0.471304
[epoch14, step1949]: loss 0.242933
[epoch14, step1950]: loss 0.529261
[epoch14, step1951]: loss 0.570201
[epoch14, step1952]: loss 0.542340
[epoch14, step1953]: loss 0.495405
[epoch14, step1954]: loss 0.439908
[epoch14, step1955]: loss 0.535274
[epoch14, step1956]: loss 0.531450
[epoch14, step1957]: loss 0.495576
[epoch14, step1958]: loss 0.777392
[epoch14, step1959]: loss 0.720593
[epoch14, step1960]: loss 0.637744
[epoch14, step1961]: loss 0.627856
[epoch14, step1962]: loss 0.419616
[epoch14, step1963]: loss 0.518256
[epoch14, step1964]: loss 0.735366
[epoch14, step1965]: loss 0.304652
[epoch14, step1966]: loss 0.553843
[epoch14, step1967]: loss 0.411822
[epoch14, step1968]: loss 0.745011
[epoch14, step1969]: loss 0.556296
[epoch14, step1970]: loss 0.663070
[epoch14, step1971]: loss 0.632718
[epoch14, step1972]: loss 0.667975
[epoch14, step1973]: loss 0.623118
[epoch14, step1974]: loss 0.283914
[epoch14, step1975]: loss 0.566021
[epoch14, step1976]: loss 0.616552
[epoch14, step1977]: loss 0.319233
[epoch14, step1978]: loss 0.406299
[epoch14, step1979]: loss 0.746774
[epoch14, step1980]: loss 0.738822
[epoch14, step1981]: loss 0.633613
[epoch14, step1982]: loss 0.464871
[epoch14, step1983]: loss 0.662780
[epoch14, step1984]: loss 0.703258
[epoch14, step1985]: loss 0.674451
[epoch14, step1986]: loss 0.424800
[epoch14, step1987]: loss 0.486920
[epoch14, step1988]: loss 0.657579
[epoch14, step1989]: loss 0.525124
[epoch14, step1990]: loss 0.688658
[epoch14, step1991]: loss 0.735790
[epoch14, step1992]: loss 0.446920
[epoch14, step1993]: loss 0.744866
[epoch14, step1994]: loss 0.505042
[epoch14, step1995]: loss 0.360825
[epoch14, step1996]: loss 0.544223
[epoch14, step1997]: loss 0.651137
[epoch14, step1998]: loss 0.343468
[epoch14, step1999]: loss 0.256188
[epoch14, step2000]: loss 0.505273
[epoch14, step2001]: loss 0.621274
[epoch14, step2002]: loss 0.284099
[epoch14, step2003]: loss 0.674767
[epoch14, step2004]: loss 0.601473
[epoch14, step2005]: loss 0.411744
[epoch14, step2006]: loss 0.340989
[epoch14, step2007]: loss 0.650639
[epoch14, step2008]: loss 0.702860
[epoch14, step2009]: loss 0.382162
[epoch14, step2010]: loss 0.510585
[epoch14, step2011]: loss 0.476806
[epoch14, step2012]: loss 0.605989
[epoch14, step2013]: loss 0.628204
[epoch14, step2014]: loss 0.476944
[epoch14, step2015]: loss 0.731710
[epoch14, step2016]: loss 0.528698
[epoch14, step2017]: loss 0.617072
[epoch14, step2018]: loss 0.742834
[epoch14, step2019]: loss 0.452187
[epoch14, step2020]: loss 0.457171
[epoch14, step2021]: loss 0.446615
[epoch14, step2022]: loss 0.681269
[epoch14, step2023]: loss 0.622182
[epoch14, step2024]: loss 0.420648
[epoch14, step2025]: loss 0.509426
[epoch14, step2026]: loss 0.733743
[epoch14, step2027]: loss 0.749466
[epoch14, step2028]: loss 0.584992
[epoch14, step2029]: loss 0.563261
[epoch14, step2030]: loss 0.625928
[epoch14, step2031]: loss 0.605753
[epoch14, step2032]: loss 0.745028
[epoch14, step2033]: loss 0.522320
[epoch14, step2034]: loss 0.602028
[epoch14, step2035]: loss 0.806615
[epoch14, step2036]: loss 0.413556
[epoch14, step2037]: loss 0.627980
[epoch14, step2038]: loss 0.708193
[epoch14, step2039]: loss 0.768123
[epoch14, step2040]: loss 0.323492
[epoch14, step2041]: loss 0.714962
[epoch14, step2042]: loss 0.482281
[epoch14, step2043]: loss 0.590584
[epoch14, step2044]: loss 0.441988
[epoch14, step2045]: loss 0.574901
[epoch14, step2046]: loss 0.513914
[epoch14, step2047]: loss 0.552165
[epoch14, step2048]: loss 0.802588
[epoch14, step2049]: loss 0.081198
[epoch14, step2050]: loss 0.710767
[epoch14, step2051]: loss 0.362524
[epoch14, step2052]: loss 0.533076
[epoch14, step2053]: loss 0.537390
[epoch14, step2054]: loss 0.676148
[epoch14, step2055]: loss 0.618853
[epoch14, step2056]: loss 0.549780
[epoch14, step2057]: loss 0.421462
[epoch14, step2058]: loss 0.609322
[epoch14, step2059]: loss 0.718044
[epoch14, step2060]: loss 0.521472
[epoch14, step2061]: loss 0.536326
[epoch14, step2062]: loss 0.477375
[epoch14, step2063]: loss 0.848181
[epoch14, step2064]: loss 0.702743
[epoch14, step2065]: loss 0.520306
[epoch14, step2066]: loss 0.432014
[epoch14, step2067]: loss 0.711486
[epoch14, step2068]: loss 0.643211
[epoch14, step2069]: loss 0.723824
[epoch14, step2070]: loss 0.530741
[epoch14, step2071]: loss 0.526731
[epoch14, step2072]: loss 0.592670
[epoch14, step2073]: loss 0.561422
[epoch14, step2074]: loss 0.495114
[epoch14, step2075]: loss 0.651715
[epoch14, step2076]: loss 0.525671
[epoch14, step2077]: loss 0.623514
[epoch14, step2078]: loss 0.666104
[epoch14, step2079]: loss 0.560181
[epoch14, step2080]: loss 0.368794
[epoch14, step2081]: loss 0.294949
[epoch14, step2082]: loss 0.243023
[epoch14, step2083]: loss 0.584617
[epoch14, step2084]: loss 0.721902
[epoch14, step2085]: loss 0.544179
[epoch14, step2086]: loss 0.566115
[epoch14, step2087]: loss 0.690245
[epoch14, step2088]: loss 0.544123
[epoch14, step2089]: loss 0.571857
[epoch14, step2090]: loss 0.262117
[epoch14, step2091]: loss 0.622925
[epoch14, step2092]: loss 0.707447
[epoch14, step2093]: loss 0.738112
[epoch14, step2094]: loss 0.394867
[epoch14, step2095]: loss 0.569584
[epoch14, step2096]: loss 0.457191
[epoch14, step2097]: loss 0.320005
[epoch14, step2098]: loss 0.459855
[epoch14, step2099]: loss 0.360942
[epoch14, step2100]: loss 0.609850
[epoch14, step2101]: loss 0.520584
[epoch14, step2102]: loss 0.428503
[epoch14, step2103]: loss 0.731644
[epoch14, step2104]: loss 0.589242
[epoch14, step2105]: loss 0.343647
[epoch14, step2106]: loss 0.572857
[epoch14, step2107]: loss 0.525008
[epoch14, step2108]: loss 0.342765
[epoch14, step2109]: loss 0.532830
[epoch14, step2110]: loss 0.558513
[epoch14, step2111]: loss 0.605850
[epoch14, step2112]: loss 0.745120
[epoch14, step2113]: loss 0.498737
[epoch14, step2114]: loss 0.448356
[epoch14, step2115]: loss 0.427596
[epoch14, step2116]: loss 0.550381
[epoch14, step2117]: loss 0.509486
[epoch14, step2118]: loss 0.680591
[epoch14, step2119]: loss 0.518789
[epoch14, step2120]: loss 0.628243
[epoch14, step2121]: loss 0.731285
[epoch14, step2122]: loss 0.382590
[epoch14, step2123]: loss 0.577777
[epoch14, step2124]: loss 0.447361
[epoch14, step2125]: loss 0.412214
[epoch14, step2126]: loss 0.423043
[epoch14, step2127]: loss 0.433595
[epoch14, step2128]: loss 0.424740
[epoch14, step2129]: loss 0.242281
[epoch14, step2130]: loss 0.370054
[epoch14, step2131]: loss 0.405101
[epoch14, step2132]: loss 0.345261
[epoch14, step2133]: loss 0.650006
[epoch14, step2134]: loss 0.516089
[epoch14, step2135]: loss 0.642631
[epoch14, step2136]: loss 0.696117
[epoch14, step2137]: loss 0.646146
[epoch14, step2138]: loss 0.616588
[epoch14, step2139]: loss 0.200337
[epoch14, step2140]: loss 0.524701
[epoch14, step2141]: loss 0.431862
[epoch14, step2142]: loss 0.507034
[epoch14, step2143]: loss 0.528767
[epoch14, step2144]: loss 0.634533
[epoch14, step2145]: loss 0.587386
[epoch14, step2146]: loss 0.363683
[epoch14, step2147]: loss 0.409268
[epoch14, step2148]: loss 0.742503
[epoch14, step2149]: loss 0.374110
[epoch14, step2150]: loss 0.659461
[epoch14, step2151]: loss 0.415698
[epoch14, step2152]: loss 0.139048
[epoch14, step2153]: loss 0.533274
[epoch14, step2154]: loss 0.583182
[epoch14, step2155]: loss 0.712758
[epoch14, step2156]: loss 0.235142
[epoch14, step2157]: loss 0.535025
[epoch14, step2158]: loss 0.667711
[epoch14, step2159]: loss 0.401698
[epoch14, step2160]: loss 0.547888
[epoch14, step2161]: loss 0.661558
[epoch14, step2162]: loss 0.571220
[epoch14, step2163]: loss 0.652167
[epoch14, step2164]: loss 0.629231
[epoch14, step2165]: loss 0.670597
[epoch14, step2166]: loss 0.394257
[epoch14, step2167]: loss 0.680218
[epoch14, step2168]: loss 0.672400
[epoch14, step2169]: loss 0.639656
[epoch14, step2170]: loss 0.620906
[epoch14, step2171]: loss 0.681321
[epoch14, step2172]: loss 0.809005
[epoch14, step2173]: loss 0.519485
[epoch14, step2174]: loss 0.650298
[epoch14, step2175]: loss 0.417219
[epoch14, step2176]: loss 0.721084
[epoch14, step2177]: loss 0.439139
[epoch14, step2178]: loss 0.595436
[epoch14, step2179]: loss 0.463830
[epoch14, step2180]: loss 0.793901
[epoch14, step2181]: loss 0.628203
[epoch14, step2182]: loss 0.603416
[epoch14, step2183]: loss 0.733205
[epoch14, step2184]: loss 0.627279
[epoch14, step2185]: loss 0.393720
[epoch14, step2186]: loss 0.516785
[epoch14, step2187]: loss 0.338762
[epoch14, step2188]: loss 0.575495
[epoch14, step2189]: loss 0.496720
[epoch14, step2190]: loss 0.597748
[epoch14, step2191]: loss 0.667210
[epoch14, step2192]: loss 0.428018
[epoch14, step2193]: loss 0.693961
[epoch14, step2194]: loss 0.435458
[epoch14, step2195]: loss 0.647416
[epoch14, step2196]: loss 0.240206
[epoch14, step2197]: loss 0.715171
[epoch14, step2198]: loss 0.592205
[epoch14, step2199]: loss 0.629281
[epoch14, step2200]: loss 0.309455
[epoch14, step2201]: loss 0.623471
[epoch14, step2202]: loss 0.612142
[epoch14, step2203]: loss 0.546693
[epoch14, step2204]: loss 0.494362
[epoch14, step2205]: loss 0.529056
[epoch14, step2206]: loss 0.453858
[epoch14, step2207]: loss 0.587749
[epoch14, step2208]: loss 0.545602
[epoch14, step2209]: loss 0.697890
[epoch14, step2210]: loss 0.479655
[epoch14, step2211]: loss 0.696142
[epoch14, step2212]: loss 0.730138
[epoch14, step2213]: loss 0.422396
[epoch14, step2214]: loss 0.790835
[epoch14, step2215]: loss 0.490381
[epoch14, step2216]: loss 0.449735
[epoch14, step2217]: loss 0.699429
[epoch14, step2218]: loss 0.569954
[epoch14, step2219]: loss 0.749975
[epoch14, step2220]: loss 0.187250
[epoch14, step2221]: loss 0.597224
[epoch14, step2222]: loss 0.475813
[epoch14, step2223]: loss 0.618984
[epoch14, step2224]: loss 0.633303
[epoch14, step2225]: loss 0.492436
[epoch14, step2226]: loss 0.443340
[epoch14, step2227]: loss 0.470095
[epoch14, step2228]: loss 0.721083
[epoch14, step2229]: loss 0.415154
[epoch14, step2230]: loss 0.675338
[epoch14, step2231]: loss 0.420749
[epoch14, step2232]: loss 0.517487
[epoch14, step2233]: loss 0.607337
[epoch14, step2234]: loss 0.557292
[epoch14, step2235]: loss 0.650505
[epoch14, step2236]: loss 0.566671
[epoch14, step2237]: loss 0.430610
[epoch14, step2238]: loss 0.587070
[epoch14, step2239]: loss 0.580965
[epoch14, step2240]: loss 0.755272
[epoch14, step2241]: loss 0.512394
[epoch14, step2242]: loss 0.495376
[epoch14, step2243]: loss 0.532138
[epoch14, step2244]: loss 0.680651
[epoch14, step2245]: loss 0.473021
[epoch14, step2246]: loss 0.763510
[epoch14, step2247]: loss 0.683031
[epoch14, step2248]: loss 0.617834
[epoch14, step2249]: loss 0.423523
[epoch14, step2250]: loss 0.538058
[epoch14, step2251]: loss 0.471569
[epoch14, step2252]: loss 0.710488
[epoch14, step2253]: loss 0.786290
[epoch14, step2254]: loss 0.699939
[epoch14, step2255]: loss 0.758503
[epoch14, step2256]: loss 0.732127
[epoch14, step2257]: loss 0.755474
[epoch14, step2258]: loss 0.345812
[epoch14, step2259]: loss 0.589787
[epoch14, step2260]: loss 0.779692
[epoch14, step2261]: loss 0.642010
[epoch14, step2262]: loss 0.429143
[epoch14, step2263]: loss 0.346631
[epoch14, step2264]: loss 0.585533
[epoch14, step2265]: loss 0.707260
[epoch14, step2266]: loss 0.437417
[epoch14, step2267]: loss 0.476141
[epoch14, step2268]: loss 0.654799
[epoch14, step2269]: loss 0.370271
[epoch14, step2270]: loss 0.608924
[epoch14, step2271]: loss 0.509026
[epoch14, step2272]: loss 0.501949
[epoch14, step2273]: loss 0.621558
[epoch14, step2274]: loss 0.624071
[epoch14, step2275]: loss 0.437655
[epoch14, step2276]: loss 0.518724
[epoch14, step2277]: loss 0.382828
[epoch14, step2278]: loss 0.311935
[epoch14, step2279]: loss 0.568891
[epoch14, step2280]: loss 0.576441
[epoch14, step2281]: loss 0.572491
[epoch14, step2282]: loss 0.663884
[epoch14, step2283]: loss 0.546684
[epoch14, step2284]: loss 0.574822
[epoch14, step2285]: loss 0.382631
[epoch14, step2286]: loss 0.566438
[epoch14, step2287]: loss 0.407590
[epoch14, step2288]: loss 0.746945
[epoch14, step2289]: loss 0.470973
[epoch14, step2290]: loss 0.500943
[epoch14, step2291]: loss 0.511663
[epoch14, step2292]: loss 0.612814
[epoch14, step2293]: loss 0.607974
[epoch14, step2294]: loss 0.706972
[epoch14, step2295]: loss 0.427002
[epoch14, step2296]: loss 0.605905
[epoch14, step2297]: loss 0.304435
[epoch14, step2298]: loss 0.827776
[epoch14, step2299]: loss 0.552723
[epoch14, step2300]: loss 0.563353
[epoch14, step2301]: loss 0.515471
[epoch14, step2302]: loss 0.571307
[epoch14, step2303]: loss 0.618510
[epoch14, step2304]: loss 0.643921
[epoch14, step2305]: loss 0.371538
[epoch14, step2306]: loss 0.922693
[epoch14, step2307]: loss 0.760461
[epoch14, step2308]: loss 0.478404
[epoch14, step2309]: loss 0.668079
[epoch14, step2310]: loss 0.563696
[epoch14, step2311]: loss 0.809498
[epoch14, step2312]: loss 0.722805
[epoch14, step2313]: loss 0.546517
[epoch14, step2314]: loss 0.401681
[epoch14, step2315]: loss 0.530845
[epoch14, step2316]: loss 0.557461
[epoch14, step2317]: loss 0.677858
[epoch14, step2318]: loss 0.369604
[epoch14, step2319]: loss 0.558827
[epoch14, step2320]: loss 0.498503
[epoch14, step2321]: loss 0.660457
[epoch14, step2322]: loss 0.643378
[epoch14, step2323]: loss 0.676986
[epoch14, step2324]: loss 0.545257
[epoch14, step2325]: loss 0.321156
[epoch14, step2326]: loss 0.707344
[epoch14, step2327]: loss 0.565720
[epoch14, step2328]: loss 0.560339
[epoch14, step2329]: loss 0.612645
[epoch14, step2330]: loss 0.666456
[epoch14, step2331]: loss 0.519348
[epoch14, step2332]: loss 0.801249
[epoch14, step2333]: loss 0.434166
[epoch14, step2334]: loss 0.473943
[epoch14, step2335]: loss 0.780531
[epoch14, step2336]: loss 0.386938
[epoch14, step2337]: loss 0.717805
[epoch14, step2338]: loss 0.747243
[epoch14, step2339]: loss 0.719757
[epoch14, step2340]: loss 0.630017
[epoch14, step2341]: loss 0.626757
[epoch14, step2342]: loss 0.449348
[epoch14, step2343]: loss 0.568424
[epoch14, step2344]: loss 0.568648
[epoch14, step2345]: loss 0.609612
[epoch14, step2346]: loss 0.737179
[epoch14, step2347]: loss 0.165926
[epoch14, step2348]: loss 0.501106
[epoch14, step2349]: loss 0.532620
[epoch14, step2350]: loss 0.594951
[epoch14, step2351]: loss 0.694223
[epoch14, step2352]: loss 0.432596
[epoch14, step2353]: loss 0.642055
[epoch14, step2354]: loss 0.600591
[epoch14, step2355]: loss 0.615693
[epoch14, step2356]: loss 0.508077
[epoch14, step2357]: loss 0.821378
[epoch14, step2358]: loss 0.777066
[epoch14, step2359]: loss 0.527029
[epoch14, step2360]: loss 0.911830
[epoch14, step2361]: loss 0.622285
[epoch14, step2362]: loss 0.526254
[epoch14, step2363]: loss 0.763655
[epoch14, step2364]: loss 0.342415
[epoch14, step2365]: loss 0.615534
[epoch14, step2366]: loss 0.474197
[epoch14, step2367]: loss 0.455663
[epoch14, step2368]: loss 0.759353
[epoch14, step2369]: loss 0.468459
[epoch14, step2370]: loss 0.581764
[epoch14, step2371]: loss 0.312369
[epoch14, step2372]: loss 0.548642
[epoch14, step2373]: loss 0.725731
[epoch14, step2374]: loss 0.560425
[epoch14, step2375]: loss 0.478385
[epoch14, step2376]: loss 0.618194
[epoch14, step2377]: loss 0.491825
[epoch14, step2378]: loss 0.475156
[epoch14, step2379]: loss 0.539666
[epoch14, step2380]: loss 0.577243
[epoch14, step2381]: loss 0.556112
[epoch14, step2382]: loss 0.464622
[epoch14, step2383]: loss 0.598710
[epoch14, step2384]: loss 0.689541
[epoch14, step2385]: loss 0.442133
[epoch14, step2386]: loss 0.485582
[epoch14, step2387]: loss 0.727878
[epoch14, step2388]: loss 0.503824
[epoch14, step2389]: loss 0.568394
[epoch14, step2390]: loss 0.564179
[epoch14, step2391]: loss 0.656861
[epoch14, step2392]: loss 0.696473
[epoch14, step2393]: loss 0.572166
[epoch14, step2394]: loss 0.697758
[epoch14, step2395]: loss 0.628992
[epoch14, step2396]: loss 0.626708
[epoch14, step2397]: loss 0.646456
[epoch14, step2398]: loss 0.312279
[epoch14, step2399]: loss 0.705537
[epoch14, step2400]: loss 0.461992
[epoch14, step2401]: loss 0.476363
[epoch14, step2402]: loss 0.709223
[epoch14, step2403]: loss 0.492014
[epoch14, step2404]: loss 0.786205
[epoch14, step2405]: loss 0.590622
[epoch14, step2406]: loss 0.693113
[epoch14, step2407]: loss 0.456183
[epoch14, step2408]: loss 0.500121
[epoch14, step2409]: loss 0.573010
[epoch14, step2410]: loss 0.630792
[epoch14, step2411]: loss 0.204662
[epoch14, step2412]: loss 0.554278
[epoch14, step2413]: loss 0.401864
[epoch14, step2414]: loss 0.573360
[epoch14, step2415]: loss 0.702868
[epoch14, step2416]: loss 0.553594
[epoch14, step2417]: loss 0.404348
[epoch14, step2418]: loss 0.686589
[epoch14, step2419]: loss 0.404732
[epoch14, step2420]: loss 0.571840
[epoch14, step2421]: loss 0.702019
[epoch14, step2422]: loss 0.353588
[epoch14, step2423]: loss 0.600413
[epoch14, step2424]: loss 0.791400
[epoch14, step2425]: loss 0.362148
[epoch14, step2426]: loss 0.706458
[epoch14, step2427]: loss 0.428443
[epoch14, step2428]: loss 0.710500
[epoch14, step2429]: loss 0.727483
[epoch14, step2430]: loss 0.706459
[epoch14, step2431]: loss 0.773251
[epoch14, step2432]: loss 0.396710
[epoch14, step2433]: loss 0.354871
[epoch14, step2434]: loss 0.579982
[epoch14, step2435]: loss 0.530860
[epoch14, step2436]: loss 0.382958
[epoch14, step2437]: loss 0.656401
[epoch14, step2438]: loss 0.614201
[epoch14, step2439]: loss 0.302205
[epoch14, step2440]: loss 0.434026
[epoch14, step2441]: loss 0.485055
[epoch14, step2442]: loss 0.526953
[epoch14, step2443]: loss 0.509713
[epoch14, step2444]: loss 0.642468
[epoch14, step2445]: loss 0.641673
[epoch14, step2446]: loss 0.724316
[epoch14, step2447]: loss 0.472408
[epoch14, step2448]: loss 0.615977
[epoch14, step2449]: loss 0.601654
[epoch14, step2450]: loss 0.427588
[epoch14, step2451]: loss 0.675343
[epoch14, step2452]: loss 0.453843
[epoch14, step2453]: loss 0.507669
[epoch14, step2454]: loss 0.337671
[epoch14, step2455]: loss 0.536747
[epoch14, step2456]: loss 0.545623
[epoch14, step2457]: loss 0.576230
[epoch14, step2458]: loss 0.567584
[epoch14, step2459]: loss 0.633929
[epoch14, step2460]: loss 0.582422
[epoch14, step2461]: loss 0.586318
[epoch14, step2462]: loss 0.717240
[epoch14, step2463]: loss 0.554154
[epoch14, step2464]: loss 0.683899
[epoch14, step2465]: loss 0.800501
[epoch14, step2466]: loss 0.676312
[epoch14, step2467]: loss 0.630086
[epoch14, step2468]: loss 0.699983
[epoch14, step2469]: loss 0.477293
[epoch14, step2470]: loss 0.533826
[epoch14, step2471]: loss 0.539803
[epoch14, step2472]: loss 0.577610
[epoch14, step2473]: loss 0.604114
[epoch14, step2474]: loss 0.472448
[epoch14, step2475]: loss 0.349135
[epoch14, step2476]: loss 0.559580
[epoch14, step2477]: loss 0.528617
[epoch14, step2478]: loss 0.456365
[epoch14, step2479]: loss 0.409499
[epoch14, step2480]: loss 0.470095
[epoch14, step2481]: loss 0.624954
[epoch14, step2482]: loss 0.466174
[epoch14, step2483]: loss 0.697707
[epoch14, step2484]: loss 0.662297
[epoch14, step2485]: loss 0.731717
[epoch14, step2486]: loss 0.630944
[epoch14, step2487]: loss 0.631115
[epoch14, step2488]: loss 0.266565
[epoch14, step2489]: loss 0.671791
[epoch14, step2490]: loss 0.569477
[epoch14, step2491]: loss 0.870837
[epoch14, step2492]: loss 0.828330
[epoch14, step2493]: loss 0.421769
[epoch14, step2494]: loss 0.422842
[epoch14, step2495]: loss 0.501707
[epoch14, step2496]: loss 0.648346
[epoch14, step2497]: loss 0.380758
[epoch14, step2498]: loss 0.631860
[epoch14, step2499]: loss 0.516842
[epoch14, step2500]: loss 0.493009
[epoch14, step2501]: loss 0.687277
[epoch14, step2502]: loss 0.318084
[epoch14, step2503]: loss 0.499126
[epoch14, step2504]: loss 0.601687
[epoch14, step2505]: loss 0.520391
[epoch14, step2506]: loss 0.418975
[epoch14, step2507]: loss 0.711391
[epoch14, step2508]: loss 0.711063
[epoch14, step2509]: loss 0.587479
[epoch14, step2510]: loss 0.557930
[epoch14, step2511]: loss 0.609204
[epoch14, step2512]: loss 0.533123
[epoch14, step2513]: loss 0.495686
[epoch14, step2514]: loss 0.169086
[epoch14, step2515]: loss 0.727517
[epoch14, step2516]: loss 0.475302
[epoch14, step2517]: loss 0.483656
[epoch14, step2518]: loss 0.584983
[epoch14, step2519]: loss 0.404603
[epoch14, step2520]: loss 0.500887
[epoch14, step2521]: loss 0.538054
[epoch14, step2522]: loss 0.612951
[epoch14, step2523]: loss 0.650084
[epoch14, step2524]: loss 0.485450
[epoch14, step2525]: loss 0.562021
[epoch14, step2526]: loss 0.713813
[epoch14, step2527]: loss 0.590271
[epoch14, step2528]: loss 0.389713
[epoch14, step2529]: loss 0.424803
[epoch14, step2530]: loss 0.695254
[epoch14, step2531]: loss 0.371032
[epoch14, step2532]: loss 0.438920
[epoch14, step2533]: loss 0.658609
[epoch14, step2534]: loss 0.605892
[epoch14, step2535]: loss 0.536148
[epoch14, step2536]: loss 0.496607
[epoch14, step2537]: loss 0.722998
[epoch14, step2538]: loss 0.745229
[epoch14, step2539]: loss 0.475530
[epoch14, step2540]: loss 0.741604
[epoch14, step2541]: loss 0.476545
[epoch14, step2542]: loss 0.434905
[epoch14, step2543]: loss 0.556416
[epoch14, step2544]: loss 0.711180
[epoch14, step2545]: loss 0.745937
[epoch14, step2546]: loss 0.500369
[epoch14, step2547]: loss 0.417512
[epoch14, step2548]: loss 0.456186
[epoch14, step2549]: loss 0.546138
[epoch14, step2550]: loss 0.723416
[epoch14, step2551]: loss 0.538605
[epoch14, step2552]: loss 0.562680
[epoch14, step2553]: loss 0.750584
[epoch14, step2554]: loss 0.513390
[epoch14, step2555]: loss 0.557155
[epoch14, step2556]: loss 0.401118
[epoch14, step2557]: loss 0.512375
[epoch14, step2558]: loss 0.475715
[epoch14, step2559]: loss 0.659490
[epoch14, step2560]: loss 0.520455
[epoch14, step2561]: loss 0.520343
[epoch14, step2562]: loss 0.572960
[epoch14, step2563]: loss 0.384334
[epoch14, step2564]: loss 0.586113
[epoch14, step2565]: loss 0.553593
[epoch14, step2566]: loss 0.297535
[epoch14, step2567]: loss 0.501170
[epoch14, step2568]: loss 0.455970
[epoch14, step2569]: loss 0.534449
[epoch14, step2570]: loss 0.588570
[epoch14, step2571]: loss 0.549273
[epoch14, step2572]: loss 0.570257
[epoch14, step2573]: loss 0.562474
[epoch14, step2574]: loss 0.343888
[epoch14, step2575]: loss 0.640533
[epoch14, step2576]: loss 0.557263
[epoch14, step2577]: loss 0.503451
[epoch14, step2578]: loss 0.343361
[epoch14, step2579]: loss 0.407990
[epoch14, step2580]: loss 0.511823
[epoch14, step2581]: loss 0.409645
[epoch14, step2582]: loss 0.609462
[epoch14, step2583]: loss 0.541279
[epoch14, step2584]: loss 0.674398
[epoch14, step2585]: loss 0.733828
[epoch14, step2586]: loss 0.382398
[epoch14, step2587]: loss 0.433007
[epoch14, step2588]: loss 0.297499
[epoch14, step2589]: loss 0.656049
[epoch14, step2590]: loss 0.485553
[epoch14, step2591]: loss 0.752602
[epoch14, step2592]: loss 0.591940
[epoch14, step2593]: loss 0.726908
[epoch14, step2594]: loss 0.853980
[epoch14, step2595]: loss 0.585890
[epoch14, step2596]: loss 0.515018
[epoch14, step2597]: loss 0.661207
[epoch14, step2598]: loss 0.588364
[epoch14, step2599]: loss 0.373756
[epoch14, step2600]: loss 0.260085
[epoch14, step2601]: loss 0.674813
[epoch14, step2602]: loss 0.544938
[epoch14, step2603]: loss 0.566546
[epoch14, step2604]: loss 0.403116
[epoch14, step2605]: loss 0.606497
[epoch14, step2606]: loss 0.430593
[epoch14, step2607]: loss 0.237720
[epoch14, step2608]: loss 0.612252
[epoch14, step2609]: loss 0.573067
[epoch14, step2610]: loss 0.438391
[epoch14, step2611]: loss 0.663096
[epoch14, step2612]: loss 0.558012
[epoch14, step2613]: loss 0.746840
[epoch14, step2614]: loss 0.402676
[epoch14, step2615]: loss 0.422986
[epoch14, step2616]: loss 0.640022
[epoch14, step2617]: loss 0.481097
[epoch14, step2618]: loss 0.572298
[epoch14, step2619]: loss 0.713767
[epoch14, step2620]: loss 0.227501
[epoch14, step2621]: loss 0.505417
[epoch14, step2622]: loss 0.481027
[epoch14, step2623]: loss 0.475306
[epoch14, step2624]: loss 0.574382
[epoch14, step2625]: loss 0.497191
[epoch14, step2626]: loss 0.611109
[epoch14, step2627]: loss 0.716925
[epoch14, step2628]: loss 0.550527
[epoch14, step2629]: loss 0.755640
[epoch14, step2630]: loss 0.702330
[epoch14, step2631]: loss 0.592129
[epoch14, step2632]: loss 0.424416
[epoch14, step2633]: loss 0.509059
[epoch14, step2634]: loss 0.284312
[epoch14, step2635]: loss 0.375440
[epoch14, step2636]: loss 0.771464
[epoch14, step2637]: loss 0.512725
[epoch14, step2638]: loss 0.796458
[epoch14, step2639]: loss 0.605369
[epoch14, step2640]: loss 0.477320
[epoch14, step2641]: loss 0.586895
[epoch14, step2642]: loss 0.480995
[epoch14, step2643]: loss 0.596733
[epoch14, step2644]: loss 0.449689
[epoch14, step2645]: loss 0.631200
[epoch14, step2646]: loss 0.649218
[epoch14, step2647]: loss 0.608182
[epoch14, step2648]: loss 0.356248
[epoch14, step2649]: loss 0.662732
[epoch14, step2650]: loss 0.545064
[epoch14, step2651]: loss 0.588617
[epoch14, step2652]: loss 0.535926
[epoch14, step2653]: loss 0.466631
[epoch14, step2654]: loss 0.532992
[epoch14, step2655]: loss 0.604154
[epoch14, step2656]: loss 0.554334
[epoch14, step2657]: loss 0.391631
[epoch14, step2658]: loss 0.322887
[epoch14, step2659]: loss 0.445996
[epoch14, step2660]: loss 0.290397
[epoch14, step2661]: loss 0.691373
[epoch14, step2662]: loss 0.508560
[epoch14, step2663]: loss 0.580718
[epoch14, step2664]: loss 0.578684
[epoch14, step2665]: loss 0.672321
[epoch14, step2666]: loss 0.432482
[epoch14, step2667]: loss 0.469991
[epoch14, step2668]: loss 0.614069
[epoch14, step2669]: loss 0.514014
[epoch14, step2670]: loss 0.471882
[epoch14, step2671]: loss 0.411468
[epoch14, step2672]: loss 0.735235
[epoch14, step2673]: loss 0.393264
[epoch14, step2674]: loss 0.648560
[epoch14, step2675]: loss 0.668902
[epoch14, step2676]: loss 0.572320
[epoch14, step2677]: loss 0.489526
[epoch14, step2678]: loss 0.183756
[epoch14, step2679]: loss 0.418184
[epoch14, step2680]: loss 0.594000
[epoch14, step2681]: loss 0.587812
[epoch14, step2682]: loss 0.418047
[epoch14, step2683]: loss 0.710958
[epoch14, step2684]: loss 0.789350
[epoch14, step2685]: loss 0.587871
[epoch14, step2686]: loss 0.419466
[epoch14, step2687]: loss 0.470399
[epoch14, step2688]: loss 0.750026
[epoch14, step2689]: loss 0.615885
[epoch14, step2690]: loss 0.707762
[epoch14, step2691]: loss 0.728472
[epoch14, step2692]: loss 0.504080
[epoch14, step2693]: loss 0.728213
[epoch14, step2694]: loss 0.782976
[epoch14, step2695]: loss 0.602755
[epoch14, step2696]: loss 0.270757
[epoch14, step2697]: loss 0.633307
[epoch14, step2698]: loss 0.592638
[epoch14, step2699]: loss 0.529331
[epoch14, step2700]: loss 0.540400
[epoch14, step2701]: loss 0.563006
[epoch14, step2702]: loss 0.750911
[epoch14, step2703]: loss 0.774893
[epoch14, step2704]: loss 0.413807
[epoch14, step2705]: loss 0.477857
[epoch14, step2706]: loss 0.625176
[epoch14, step2707]: loss 0.587966
[epoch14, step2708]: loss 0.389854
[epoch14, step2709]: loss 0.488255
[epoch14, step2710]: loss 0.413594
[epoch14, step2711]: loss 0.679410
[epoch14, step2712]: loss 0.630007
[epoch14, step2713]: loss 0.529625
[epoch14, step2714]: loss 0.389221
[epoch14, step2715]: loss 0.575413
[epoch14, step2716]: loss 0.819333
[epoch14, step2717]: loss 0.432220
[epoch14, step2718]: loss 0.620921
[epoch14, step2719]: loss 0.570908
[epoch14, step2720]: loss 0.450043
[epoch14, step2721]: loss 0.528296
[epoch14, step2722]: loss 0.376037
[epoch14, step2723]: loss 0.582948
[epoch14, step2724]: loss 0.583273
[epoch14, step2725]: loss 0.585993
[epoch14, step2726]: loss 0.797576
[epoch14, step2727]: loss 0.445231
[epoch14, step2728]: loss 0.659585
[epoch14, step2729]: loss 0.537683
[epoch14, step2730]: loss 0.544936
[epoch14, step2731]: loss 0.663231
[epoch14, step2732]: loss 0.540519
[epoch14, step2733]: loss 0.552220
[epoch14, step2734]: loss 0.798303
[epoch14, step2735]: loss 0.715390
[epoch14, step2736]: loss 0.658009
[epoch14, step2737]: loss 0.367637
[epoch14, step2738]: loss 0.446288
[epoch14, step2739]: loss 0.620771
[epoch14, step2740]: loss 0.600385
[epoch14, step2741]: loss 0.670258
[epoch14, step2742]: loss 0.643788
[epoch14, step2743]: loss 0.472822
[epoch14, step2744]: loss 0.700520
[epoch14, step2745]: loss 0.406713
[epoch14, step2746]: loss 0.779355
[epoch14, step2747]: loss 0.510860
[epoch14, step2748]: loss 0.410544
[epoch14, step2749]: loss 0.775208
[epoch14, step2750]: loss 0.570817
[epoch14, step2751]: loss 0.516001
[epoch14, step2752]: loss 0.502472
[epoch14, step2753]: loss 0.345831
[epoch14, step2754]: loss 0.400008
[epoch14, step2755]: loss 0.191690
[epoch14, step2756]: loss 0.589377
[epoch14, step2757]: loss 0.584463
[epoch14, step2758]: loss 0.657005
[epoch14, step2759]: loss 0.700358
[epoch14, step2760]: loss 0.640471
[epoch14, step2761]: loss 0.582102
[epoch14, step2762]: loss 0.668756
[epoch14, step2763]: loss 0.507041
[epoch14, step2764]: loss 0.540422
[epoch14, step2765]: loss 0.586649
[epoch14, step2766]: loss 0.588932
[epoch14, step2767]: loss 0.709401
[epoch14, step2768]: loss 0.579861
[epoch14, step2769]: loss 0.492583
[epoch14, step2770]: loss 0.670151
[epoch14, step2771]: loss 0.407426
[epoch14, step2772]: loss 0.288182
[epoch14, step2773]: loss 0.431321
[epoch14, step2774]: loss 0.323638
[epoch14, step2775]: loss 0.766640
[epoch14, step2776]: loss 0.460414
[epoch14, step2777]: loss 0.835596
[epoch14, step2778]: loss 0.770615
[epoch14, step2779]: loss 0.546577
[epoch14, step2780]: loss 0.700831
[epoch14, step2781]: loss 0.668614
[epoch14, step2782]: loss 0.567173
[epoch14, step2783]: loss 0.577560
[epoch14, step2784]: loss 0.503176
[epoch14, step2785]: loss 0.837238
[epoch14, step2786]: loss 0.560160
[epoch14, step2787]: loss 0.511887
[epoch14, step2788]: loss 0.352196
[epoch14, step2789]: loss 0.302828
[epoch14, step2790]: loss 0.592520
[epoch14, step2791]: loss 0.763677
[epoch14, step2792]: loss 0.311967
[epoch14, step2793]: loss 0.661565
[epoch14, step2794]: loss 0.423254
[epoch14, step2795]: loss 0.752478
[epoch14, step2796]: loss 0.391885
[epoch14, step2797]: loss 0.676394
[epoch14, step2798]: loss 0.444071
[epoch14, step2799]: loss 0.635948
[epoch14, step2800]: loss 0.672510
[epoch14, step2801]: loss 0.272208
[epoch14, step2802]: loss 0.611512
[epoch14, step2803]: loss 0.323687
[epoch14, step2804]: loss 0.795912
[epoch14, step2805]: loss 0.563226
[epoch14, step2806]: loss 0.495604
[epoch14, step2807]: loss 0.347607
[epoch14, step2808]: loss 0.546612
[epoch14, step2809]: loss 0.515068
[epoch14, step2810]: loss 0.439828
[epoch14, step2811]: loss 0.287788
[epoch14, step2812]: loss 0.701045
[epoch14, step2813]: loss 0.498617
[epoch14, step2814]: loss 0.165579
[epoch14, step2815]: loss 0.680730
[epoch14, step2816]: loss 0.734355
[epoch14, step2817]: loss 0.304867
[epoch14, step2818]: loss 0.399797
[epoch14, step2819]: loss 0.344546
[epoch14, step2820]: loss 0.550219
[epoch14, step2821]: loss 0.566060
[epoch14, step2822]: loss 0.827042
[epoch14, step2823]: loss 0.527219
[epoch14, step2824]: loss 0.479462
[epoch14, step2825]: loss 0.719061
[epoch14, step2826]: loss 0.744581
[epoch14, step2827]: loss 0.539984
[epoch14, step2828]: loss 0.919759
[epoch14, step2829]: loss 0.565206
[epoch14, step2830]: loss 0.469970
[epoch14, step2831]: loss 0.651917
[epoch14, step2832]: loss 0.618269
[epoch14, step2833]: loss 0.706874
[epoch14, step2834]: loss 0.601421
[epoch14, step2835]: loss 0.602531
[epoch14, step2836]: loss 0.461551
[epoch14, step2837]: loss 0.536909
[epoch14, step2838]: loss 0.634815
[epoch14, step2839]: loss 0.718541
[epoch14, step2840]: loss 0.584166
[epoch14, step2841]: loss 0.283015
[epoch14, step2842]: loss 0.796276
[epoch14, step2843]: loss 0.530980
[epoch14, step2844]: loss 0.559146
[epoch14, step2845]: loss 0.617535
[epoch14, step2846]: loss 0.483586
[epoch14, step2847]: loss 0.540863
[epoch14, step2848]: loss 0.635404
[epoch14, step2849]: loss 0.593508
[epoch14, step2850]: loss 0.657612
[epoch14, step2851]: loss 0.286907
[epoch14, step2852]: loss 0.650171
[epoch14, step2853]: loss 0.384689
[epoch14, step2854]: loss 0.745396
[epoch14, step2855]: loss 0.635328
[epoch14, step2856]: loss 0.531440
[epoch14, step2857]: loss 0.618899
[epoch14, step2858]: loss 0.463346
[epoch14, step2859]: loss 0.597706
[epoch14, step2860]: loss 0.758500
[epoch14, step2861]: loss 0.364156
[epoch14, step2862]: loss 0.574012
[epoch14, step2863]: loss 0.412722
[epoch14, step2864]: loss 0.410022
[epoch14, step2865]: loss 0.501676
[epoch14, step2866]: loss 0.307534
[epoch14, step2867]: loss 0.755751
[epoch14, step2868]: loss 0.554101
[epoch14, step2869]: loss 0.401660
[epoch14, step2870]: loss 0.789398
[epoch14, step2871]: loss 0.650572
[epoch14, step2872]: loss 0.297936
[epoch14, step2873]: loss 0.619533
[epoch14, step2874]: loss 0.748730
[epoch14, step2875]: loss 0.488777
[epoch14, step2876]: loss 0.652851
[epoch14, step2877]: loss 0.209374
[epoch14, step2878]: loss 0.561204
[epoch14, step2879]: loss 0.376566
[epoch14, step2880]: loss 0.330879
[epoch14, step2881]: loss 0.452721
[epoch14, step2882]: loss 0.632654
[epoch14, step2883]: loss 0.570156
[epoch14, step2884]: loss 0.534020
[epoch14, step2885]: loss 0.392081
[epoch14, step2886]: loss 0.814005
[epoch14, step2887]: loss 0.503617
[epoch14, step2888]: loss 0.660047
[epoch14, step2889]: loss 0.205901
[epoch14, step2890]: loss 0.492696
[epoch14, step2891]: loss 0.523735
[epoch14, step2892]: loss 0.424632
[epoch14, step2893]: loss 0.534943
[epoch14, step2894]: loss 0.409701
[epoch14, step2895]: loss 0.552180
[epoch14, step2896]: loss 0.582481
[epoch14, step2897]: loss 0.594324
[epoch14, step2898]: loss 0.481671
[epoch14, step2899]: loss 0.543349
[epoch14, step2900]: loss 0.425322
[epoch14, step2901]: loss 0.298180
[epoch14, step2902]: loss 0.626563
[epoch14, step2903]: loss 0.598431
[epoch14, step2904]: loss 0.589229
[epoch14, step2905]: loss 0.657372
[epoch14, step2906]: loss 0.471218
[epoch14, step2907]: loss 0.426238
[epoch14, step2908]: loss 0.714047
[epoch14, step2909]: loss 0.293278
[epoch14, step2910]: loss 0.385868
[epoch14, step2911]: loss 0.428551
[epoch14, step2912]: loss 0.342117
[epoch14, step2913]: loss 0.482978
[epoch14, step2914]: loss 0.274047
[epoch14, step2915]: loss 0.664448
[epoch14, step2916]: loss 0.219831
[epoch14, step2917]: loss 0.418390
[epoch14, step2918]: loss 0.404130
[epoch14, step2919]: loss 0.448203
[epoch14, step2920]: loss 0.505354
[epoch14, step2921]: loss 0.687886
[epoch14, step2922]: loss 0.557132
[epoch14, step2923]: loss 0.479922
[epoch14, step2924]: loss 0.764123
[epoch14, step2925]: loss 0.537037
[epoch14, step2926]: loss 0.597519
[epoch14, step2927]: loss 0.428348
[epoch14, step2928]: loss 0.614602
[epoch14, step2929]: loss 0.643914
[epoch14, step2930]: loss 0.758089
[epoch14, step2931]: loss 0.540508
[epoch14, step2932]: loss 0.385437
[epoch14, step2933]: loss 0.405927
[epoch14, step2934]: loss 0.715808
[epoch14, step2935]: loss 0.405751
[epoch14, step2936]: loss 0.438111
[epoch14, step2937]: loss 0.708760
[epoch14, step2938]: loss 0.714214
[epoch14, step2939]: loss 0.673454
[epoch14, step2940]: loss 0.630051
[epoch14, step2941]: loss 0.717604
[epoch14, step2942]: loss 0.531038
[epoch14, step2943]: loss 0.555077
[epoch14, step2944]: loss 0.602573
[epoch14, step2945]: loss 0.528830
[epoch14, step2946]: loss 0.519732
[epoch14, step2947]: loss 0.388448
[epoch14, step2948]: loss 0.537755
[epoch14, step2949]: loss 0.681790
[epoch14, step2950]: loss 0.539843
[epoch14, step2951]: loss 0.354435
[epoch14, step2952]: loss 0.763789
[epoch14, step2953]: loss 0.474257
[epoch14, step2954]: loss 0.681068
[epoch14, step2955]: loss 0.549415
[epoch14, step2956]: loss 0.480579
[epoch14, step2957]: loss 0.609386
[epoch14, step2958]: loss 0.479616
[epoch14, step2959]: loss 0.679220
[epoch14, step2960]: loss 0.731556
[epoch14, step2961]: loss 0.643815
[epoch14, step2962]: loss 0.752935
[epoch14, step2963]: loss 0.300459
[epoch14, step2964]: loss 0.596288
[epoch14, step2965]: loss 0.453868
[epoch14, step2966]: loss 0.386772
[epoch14, step2967]: loss 0.559599
[epoch14, step2968]: loss 0.498175
[epoch14, step2969]: loss 0.582369
[epoch14, step2970]: loss 0.478694
[epoch14, step2971]: loss 0.234522
[epoch14, step2972]: loss 0.629198
[epoch14, step2973]: loss 0.550240
[epoch14, step2974]: loss 0.635488
[epoch14, step2975]: loss 0.404674
[epoch14, step2976]: loss 0.223560
[epoch14, step2977]: loss 0.310820
[epoch14, step2978]: loss 0.632021
[epoch14, step2979]: loss 0.455907
[epoch14, step2980]: loss 0.765146
[epoch14, step2981]: loss 0.621470
[epoch14, step2982]: loss 0.403802
[epoch14, step2983]: loss 0.529505
[epoch14, step2984]: loss 0.691902
[epoch14, step2985]: loss 0.539939
[epoch14, step2986]: loss 0.350947
[epoch14, step2987]: loss 0.780975
[epoch14, step2988]: loss 0.485483
[epoch14, step2989]: loss 0.532212
[epoch14, step2990]: loss 0.491486
[epoch14, step2991]: loss 0.653990
[epoch14, step2992]: loss 0.454760
[epoch14, step2993]: loss 0.512522
[epoch14, step2994]: loss 0.402900
[epoch14, step2995]: loss 0.509552
[epoch14, step2996]: loss 0.508158
[epoch14, step2997]: loss 0.379787
[epoch14, step2998]: loss 0.223886
[epoch14, step2999]: loss 0.351205
[epoch14, step3000]: loss 0.105150
[epoch14, step3001]: loss 0.327082
[epoch14, step3002]: loss 0.518098
[epoch14, step3003]: loss 0.655849
[epoch14, step3004]: loss 0.429381
[epoch14, step3005]: loss 0.561000
[epoch14, step3006]: loss 0.690253
[epoch14, step3007]: loss 0.868025
[epoch14, step3008]: loss 0.708264
[epoch14, step3009]: loss 0.549437
[epoch14, step3010]: loss 0.558374
[epoch14, step3011]: loss 0.656488
[epoch14, step3012]: loss 0.442109
[epoch14, step3013]: loss 0.799792
[epoch14, step3014]: loss 0.484800
[epoch14, step3015]: loss 0.568362
[epoch14, step3016]: loss 0.399413
[epoch14, step3017]: loss 0.441344
[epoch14, step3018]: loss 0.710873
[epoch14, step3019]: loss 0.515679
[epoch14, step3020]: loss 0.394357
[epoch14, step3021]: loss 0.750396
[epoch14, step3022]: loss 0.422218
[epoch14, step3023]: loss 0.667736
[epoch14, step3024]: loss 0.312973
[epoch14, step3025]: loss 0.572460
[epoch14, step3026]: loss 0.622259
[epoch14, step3027]: loss 0.674102
[epoch14, step3028]: loss 0.584447
[epoch14, step3029]: loss 0.303023
[epoch14, step3030]: loss 0.301301
[epoch14, step3031]: loss 0.225761
[epoch14, step3032]: loss 0.883479
[epoch14, step3033]: loss 0.496375
[epoch14, step3034]: loss 0.404412
[epoch14, step3035]: loss 0.546277
[epoch14, step3036]: loss 0.492693
[epoch14, step3037]: loss 0.707980
[epoch14, step3038]: loss 0.793590
[epoch14, step3039]: loss 0.423719
[epoch14, step3040]: loss 0.550823
[epoch14, step3041]: loss 0.556279
[epoch14, step3042]: loss 0.300463
[epoch14, step3043]: loss 0.712584
[epoch14, step3044]: loss 0.667718
[epoch14, step3045]: loss 0.372876
[epoch14, step3046]: loss 0.412755
[epoch14, step3047]: loss 0.625120
[epoch14, step3048]: loss 0.596076
[epoch14, step3049]: loss 0.669883
[epoch14, step3050]: loss 0.274458
[epoch14, step3051]: loss 0.422379
[epoch14, step3052]: loss 0.751120
[epoch14, step3053]: loss 0.414955
[epoch14, step3054]: loss 0.500298
[epoch14, step3055]: loss 0.582659
[epoch14, step3056]: loss 0.661250
[epoch14, step3057]: loss 0.704542
[epoch14, step3058]: loss 0.669278
[epoch14, step3059]: loss 0.349304
[epoch14, step3060]: loss 0.476388
[epoch14, step3061]: loss 0.542959
[epoch14, step3062]: loss 0.516182
[epoch14, step3063]: loss 0.645907
[epoch14, step3064]: loss 0.623692
[epoch14, step3065]: loss 0.655386
[epoch14, step3066]: loss 0.514206
[epoch14, step3067]: loss 0.370822
[epoch14, step3068]: loss 0.667497
[epoch14, step3069]: loss 0.498924
[epoch14, step3070]: loss 0.262589
[epoch14, step3071]: loss 0.581949
[epoch14, step3072]: loss 0.505239
[epoch14, step3073]: loss 0.423673
[epoch14, step3074]: loss 0.531976
[epoch14, step3075]: loss 0.591984
[epoch14, step3076]: loss 0.588466

[epoch14]: avg loss 0.588466

[epoch15, step1]: loss 0.399974
[epoch15, step2]: loss 0.766598
[epoch15, step3]: loss 0.583460
[epoch15, step4]: loss 0.583341
[epoch15, step5]: loss 0.302104
[epoch15, step6]: loss 0.472721
[epoch15, step7]: loss 0.374070
[epoch15, step8]: loss 0.593651
[epoch15, step9]: loss 0.565583
[epoch15, step10]: loss 0.584454
[epoch15, step11]: loss 0.618962
[epoch15, step12]: loss 0.333294
[epoch15, step13]: loss 0.552361
[epoch15, step14]: loss 0.568046
[epoch15, step15]: loss 0.577261
[epoch15, step16]: loss 0.587142
[epoch15, step17]: loss 0.173574
[epoch15, step18]: loss 0.385285
[epoch15, step19]: loss 0.695104
[epoch15, step20]: loss 0.713270
[epoch15, step21]: loss 0.573828
[epoch15, step22]: loss 0.440845
[epoch15, step23]: loss 0.577392
[epoch15, step24]: loss 0.594245
[epoch15, step25]: loss 0.557223
[epoch15, step26]: loss 0.394580
[epoch15, step27]: loss 0.516405
[epoch15, step28]: loss 0.486831
[epoch15, step29]: loss 0.416636
[epoch15, step30]: loss 0.249599
[epoch15, step31]: loss 0.707487
[epoch15, step32]: loss 0.585219
[epoch15, step33]: loss 0.252658
[epoch15, step34]: loss 0.692925
[epoch15, step35]: loss 0.429004
[epoch15, step36]: loss 0.669794
[epoch15, step37]: loss 0.542717
[epoch15, step38]: loss 0.609630
[epoch15, step39]: loss 0.681427
[epoch15, step40]: loss 0.527524
[epoch15, step41]: loss 0.504298
[epoch15, step42]: loss 0.565695
[epoch15, step43]: loss 0.407800
[epoch15, step44]: loss 0.650642
[epoch15, step45]: loss 0.428151
[epoch15, step46]: loss 0.540208
[epoch15, step47]: loss 0.566056
[epoch15, step48]: loss 0.384294
[epoch15, step49]: loss 0.670613
[epoch15, step50]: loss 0.522962
[epoch15, step51]: loss 0.385973
[epoch15, step52]: loss 0.808079
[epoch15, step53]: loss 0.686276
[epoch15, step54]: loss 0.492762
[epoch15, step55]: loss 0.585877
[epoch15, step56]: loss 0.489370
[epoch15, step57]: loss 0.488401
[epoch15, step58]: loss 0.713196
[epoch15, step59]: loss 0.494466
[epoch15, step60]: loss 0.429321
[epoch15, step61]: loss 0.208953
[epoch15, step62]: loss 0.497968
[epoch15, step63]: loss 0.488813
[epoch15, step64]: loss 0.350929
[epoch15, step65]: loss 0.587603
[epoch15, step66]: loss 0.601018
[epoch15, step67]: loss 0.805561
[epoch15, step68]: loss 0.582337
[epoch15, step69]: loss 0.568776
[epoch15, step70]: loss 0.700889
[epoch15, step71]: loss 0.714399
[epoch15, step72]: loss 0.479754
[epoch15, step73]: loss 0.272781
[epoch15, step74]: loss 0.588484
[epoch15, step75]: loss 0.439317
[epoch15, step76]: loss 0.787315
[epoch15, step77]: loss 0.498154
[epoch15, step78]: loss 0.786252
[epoch15, step79]: loss 0.549849
[epoch15, step80]: loss 0.313489
[epoch15, step81]: loss 0.631926
[epoch15, step82]: loss 0.663425
[epoch15, step83]: loss 0.567775
[epoch15, step84]: loss 0.530352
[epoch15, step85]: loss 0.133058
[epoch15, step86]: loss 0.317727
[epoch15, step87]: loss 0.644639
[epoch15, step88]: loss 0.301520
[epoch15, step89]: loss 0.797494
[epoch15, step90]: loss 0.500660
[epoch15, step91]: loss 0.565643
[epoch15, step92]: loss 0.663677
[epoch15, step93]: loss 0.675831
[epoch15, step94]: loss 0.325608
[epoch15, step95]: loss 0.574957
[epoch15, step96]: loss 0.763721
[epoch15, step97]: loss 0.637465
[epoch15, step98]: loss 0.557984
[epoch15, step99]: loss 0.473290
[epoch15, step100]: loss 0.492448
[epoch15, step101]: loss 0.618235
[epoch15, step102]: loss 0.393269
[epoch15, step103]: loss 0.533533
[epoch15, step104]: loss 0.683786
[epoch15, step105]: loss 0.634093
[epoch15, step106]: loss 0.606542
[epoch15, step107]: loss 0.415516
[epoch15, step108]: loss 0.683150
[epoch15, step109]: loss 0.785770
[epoch15, step110]: loss 0.743711
[epoch15, step111]: loss 0.568612
[epoch15, step112]: loss 0.654785
[epoch15, step113]: loss 0.741190
[epoch15, step114]: loss 0.647010
[epoch15, step115]: loss 0.568758
[epoch15, step116]: loss 0.552604
[epoch15, step117]: loss 0.528534
[epoch15, step118]: loss 0.603216
[epoch15, step119]: loss 0.520753
[epoch15, step120]: loss 0.728826
[epoch15, step121]: loss 0.496371
[epoch15, step122]: loss 0.564821
[epoch15, step123]: loss 0.258109
[epoch15, step124]: loss 0.598149
[epoch15, step125]: loss 0.595775
[epoch15, step126]: loss 0.649557
[epoch15, step127]: loss 0.339121
[epoch15, step128]: loss 0.526979
[epoch15, step129]: loss 0.392955
[epoch15, step130]: loss 0.592016
[epoch15, step131]: loss 0.507486
[epoch15, step132]: loss 0.587921
[epoch15, step133]: loss 0.612246
[epoch15, step134]: loss 0.649170
[epoch15, step135]: loss 0.405661
[epoch15, step136]: loss 0.488529
[epoch15, step137]: loss 0.454265
[epoch15, step138]: loss 0.453599
[epoch15, step139]: loss 0.553681
[epoch15, step140]: loss 0.677859
[epoch15, step141]: loss 0.462225
[epoch15, step142]: loss 0.646708
[epoch15, step143]: loss 0.392739
[epoch15, step144]: loss 0.442664
[epoch15, step145]: loss 0.388453
[epoch15, step146]: loss 0.559101
[epoch15, step147]: loss 0.683981
[epoch15, step148]: loss 0.541121
[epoch15, step149]: loss 0.541130
[epoch15, step150]: loss 0.323119
[epoch15, step151]: loss 0.622357
[epoch15, step152]: loss 0.549158
[epoch15, step153]: loss 0.530370
[epoch15, step154]: loss 0.413699
[epoch15, step155]: loss 0.536446
[epoch15, step156]: loss 0.736366
[epoch15, step157]: loss 0.565056
[epoch15, step158]: loss 0.506896
[epoch15, step159]: loss 0.526433
[epoch15, step160]: loss 0.576757
[epoch15, step161]: loss 0.304460
[epoch15, step162]: loss 0.219749
[epoch15, step163]: loss 0.470545
[epoch15, step164]: loss 0.456618
[epoch15, step165]: loss 0.479075
[epoch15, step166]: loss 0.474303
[epoch15, step167]: loss 0.795595
[epoch15, step168]: loss 0.604052
[epoch15, step169]: loss 0.795458
[epoch15, step170]: loss 0.535298
[epoch15, step171]: loss 0.808261
[epoch15, step172]: loss 0.532826
[epoch15, step173]: loss 0.755465
[epoch15, step174]: loss 0.327589
[epoch15, step175]: loss 0.548625
[epoch15, step176]: loss 0.411382
[epoch15, step177]: loss 0.617802
[epoch15, step178]: loss 0.523809
[epoch15, step179]: loss 0.515532
[epoch15, step180]: loss 0.536905
[epoch15, step181]: loss 0.836564
[epoch15, step182]: loss 0.550564
[epoch15, step183]: loss 0.499798
[epoch15, step184]: loss 0.462182
[epoch15, step185]: loss 0.609274
[epoch15, step186]: loss 0.512907
[epoch15, step187]: loss 0.508438
[epoch15, step188]: loss 0.582835
[epoch15, step189]: loss 0.610314
[epoch15, step190]: loss 0.621488
[epoch15, step191]: loss 0.574018
[epoch15, step192]: loss 0.563738
[epoch15, step193]: loss 0.414107
[epoch15, step194]: loss 0.591272
[epoch15, step195]: loss 0.429004
[epoch15, step196]: loss 0.569154
[epoch15, step197]: loss 0.481537
[epoch15, step198]: loss 0.514644
[epoch15, step199]: loss 0.746775
[epoch15, step200]: loss 0.606014
[epoch15, step201]: loss 0.521637
[epoch15, step202]: loss 0.700001
[epoch15, step203]: loss 0.548254
[epoch15, step204]: loss 0.612825
[epoch15, step205]: loss 0.366938
[epoch15, step206]: loss 0.518022
[epoch15, step207]: loss 0.650769
[epoch15, step208]: loss 0.312728
[epoch15, step209]: loss 0.611462
[epoch15, step210]: loss 0.695655
[epoch15, step211]: loss 0.464275
[epoch15, step212]: loss 0.515434
[epoch15, step213]: loss 0.440559
[epoch15, step214]: loss 0.653573
[epoch15, step215]: loss 0.685212
[epoch15, step216]: loss 0.645152
[epoch15, step217]: loss 0.635402
[epoch15, step218]: loss 0.520594
[epoch15, step219]: loss 0.511054
[epoch15, step220]: loss 0.699535
[epoch15, step221]: loss 0.739385
[epoch15, step222]: loss 0.550720
[epoch15, step223]: loss 0.810771
[epoch15, step224]: loss 0.713555
[epoch15, step225]: loss 0.358878
[epoch15, step226]: loss 0.327504
[epoch15, step227]: loss 0.626477
[epoch15, step228]: loss 0.596478
[epoch15, step229]: loss 0.309561
[epoch15, step230]: loss 0.459829
[epoch15, step231]: loss 0.639954
[epoch15, step232]: loss 0.586769
[epoch15, step233]: loss 0.596996
[epoch15, step234]: loss 0.625852
[epoch15, step235]: loss 0.400775
[epoch15, step236]: loss 0.631628
[epoch15, step237]: loss 0.847673
[epoch15, step238]: loss 0.662552
[epoch15, step239]: loss 0.535810
[epoch15, step240]: loss 0.385524
[epoch15, step241]: loss 0.635986
[epoch15, step242]: loss 0.824859
[epoch15, step243]: loss 0.544574
[epoch15, step244]: loss 0.703538
[epoch15, step245]: loss 0.636731
[epoch15, step246]: loss 0.560918
[epoch15, step247]: loss 0.444076
[epoch15, step248]: loss 0.493670
[epoch15, step249]: loss 0.537359
[epoch15, step250]: loss 0.280344
[epoch15, step251]: loss 0.769586
[epoch15, step252]: loss 0.266771
[epoch15, step253]: loss 0.627818
[epoch15, step254]: loss 0.673664
[epoch15, step255]: loss 0.555363
[epoch15, step256]: loss 0.556353
[epoch15, step257]: loss 0.557946
[epoch15, step258]: loss 0.214835
[epoch15, step259]: loss 0.316952
[epoch15, step260]: loss 0.567926
[epoch15, step261]: loss 0.723388
[epoch15, step262]: loss 0.345393
[epoch15, step263]: loss 0.770978
[epoch15, step264]: loss 0.441021
[epoch15, step265]: loss 0.758567
[epoch15, step266]: loss 0.629313
[epoch15, step267]: loss 0.707010
[epoch15, step268]: loss 0.713237
[epoch15, step269]: loss 0.563308
[epoch15, step270]: loss 0.778188
[epoch15, step271]: loss 0.591978
[epoch15, step272]: loss 0.699588
[epoch15, step273]: loss 0.319622
[epoch15, step274]: loss 0.489967
[epoch15, step275]: loss 0.770099
[epoch15, step276]: loss 0.164746
[epoch15, step277]: loss 0.562954
[epoch15, step278]: loss 0.405228
[epoch15, step279]: loss 0.422716
[epoch15, step280]: loss 0.422122
[epoch15, step281]: loss 0.515191
[epoch15, step282]: loss 0.745338
[epoch15, step283]: loss 0.646993
[epoch15, step284]: loss 0.544328
[epoch15, step285]: loss 0.684134
[epoch15, step286]: loss 0.602118
[epoch15, step287]: loss 0.342124
[epoch15, step288]: loss 0.314975
[epoch15, step289]: loss 0.276855
[epoch15, step290]: loss 0.628432
[epoch15, step291]: loss 0.554209
[epoch15, step292]: loss 0.599713
[epoch15, step293]: loss 0.307138
[epoch15, step294]: loss 0.496952
[epoch15, step295]: loss 0.822551
[epoch15, step296]: loss 0.671918
[epoch15, step297]: loss 0.489684
[epoch15, step298]: loss 0.393472
[epoch15, step299]: loss 0.510436
[epoch15, step300]: loss 0.556276
[epoch15, step301]: loss 0.494457
[epoch15, step302]: loss 0.467407
[epoch15, step303]: loss 0.306257
[epoch15, step304]: loss 0.595886
[epoch15, step305]: loss 0.721278
[epoch15, step306]: loss 0.385094
[epoch15, step307]: loss 0.790541
[epoch15, step308]: loss 0.387280
[epoch15, step309]: loss 0.405403
[epoch15, step310]: loss 0.363948
[epoch15, step311]: loss 0.537464
[epoch15, step312]: loss 0.680155
[epoch15, step313]: loss 0.580025
[epoch15, step314]: loss 0.702646
[epoch15, step315]: loss 0.378473
[epoch15, step316]: loss 0.574368
[epoch15, step317]: loss 0.570745
[epoch15, step318]: loss 0.392340
[epoch15, step319]: loss 0.586069
[epoch15, step320]: loss 0.557562
[epoch15, step321]: loss 0.596091
[epoch15, step322]: loss 0.391101
[epoch15, step323]: loss 0.577832
[epoch15, step324]: loss 0.576844
[epoch15, step325]: loss 0.304249
[epoch15, step326]: loss 0.711248
[epoch15, step327]: loss 0.653352
[epoch15, step328]: loss 0.801031
[epoch15, step329]: loss 0.422802
[epoch15, step330]: loss 0.575862
[epoch15, step331]: loss 0.495373
[epoch15, step332]: loss 0.396594
[epoch15, step333]: loss 0.693393
[epoch15, step334]: loss 0.470201
[epoch15, step335]: loss 0.731397
[epoch15, step336]: loss 0.563144
[epoch15, step337]: loss 0.493628
[epoch15, step338]: loss 0.780799
[epoch15, step339]: loss 0.665482
[epoch15, step340]: loss 0.754258
[epoch15, step341]: loss 0.583293
[epoch15, step342]: loss 0.512303
[epoch15, step343]: loss 0.705688
[epoch15, step344]: loss 0.654491
[epoch15, step345]: loss 0.398953
[epoch15, step346]: loss 0.463834
[epoch15, step347]: loss 0.428660
[epoch15, step348]: loss 0.613677
[epoch15, step349]: loss 0.418804
[epoch15, step350]: loss 0.329435
[epoch15, step351]: loss 0.594485
[epoch15, step352]: loss 0.595562
[epoch15, step353]: loss 0.624325
[epoch15, step354]: loss 0.349756
[epoch15, step355]: loss 0.715551
[epoch15, step356]: loss 0.198130
[epoch15, step357]: loss 0.127140
[epoch15, step358]: loss 0.634818
[epoch15, step359]: loss 0.457084
[epoch15, step360]: loss 0.546300
[epoch15, step361]: loss 0.502972
[epoch15, step362]: loss 0.457439
[epoch15, step363]: loss 0.264490
[epoch15, step364]: loss 0.569248
[epoch15, step365]: loss 0.282180
[epoch15, step366]: loss 0.529320
[epoch15, step367]: loss 0.452822
[epoch15, step368]: loss 0.306933
[epoch15, step369]: loss 0.666734
[epoch15, step370]: loss 0.740508
[epoch15, step371]: loss 0.602161
[epoch15, step372]: loss 0.599654
[epoch15, step373]: loss 0.545179
[epoch15, step374]: loss 0.678208
[epoch15, step375]: loss 0.370810
[epoch15, step376]: loss 0.293132
[epoch15, step377]: loss 0.675217
[epoch15, step378]: loss 0.422118
[epoch15, step379]: loss 0.678406
[epoch15, step380]: loss 0.679711
[epoch15, step381]: loss 0.748877
[epoch15, step382]: loss 0.580284
[epoch15, step383]: loss 0.310994
[epoch15, step384]: loss 0.535092
[epoch15, step385]: loss 0.435114
[epoch15, step386]: loss 0.507856
[epoch15, step387]: loss 0.629856
[epoch15, step388]: loss 0.508346
[epoch15, step389]: loss 0.807057
[epoch15, step390]: loss 0.390330
[epoch15, step391]: loss 0.733084
[epoch15, step392]: loss 0.285937
[epoch15, step393]: loss 0.617421
[epoch15, step394]: loss 0.690203
[epoch15, step395]: loss 0.693590
[epoch15, step396]: loss 0.581399
[epoch15, step397]: loss 0.467773
[epoch15, step398]: loss 0.557415
[epoch15, step399]: loss 0.393133
[epoch15, step400]: loss 0.588981
[epoch15, step401]: loss 0.640092
[epoch15, step402]: loss 0.454012
[epoch15, step403]: loss 0.623532
[epoch15, step404]: loss 0.718275
[epoch15, step405]: loss 0.451665
[epoch15, step406]: loss 0.797329
[epoch15, step407]: loss 0.556505
[epoch15, step408]: loss 0.321014
[epoch15, step409]: loss 0.603932
[epoch15, step410]: loss 0.480062
[epoch15, step411]: loss 0.416798
[epoch15, step412]: loss 0.632200
[epoch15, step413]: loss 0.402766
[epoch15, step414]: loss 0.754471
[epoch15, step415]: loss 0.698398
[epoch15, step416]: loss 0.453473
[epoch15, step417]: loss 0.424826
[epoch15, step418]: loss 0.360839
[epoch15, step419]: loss 0.440094
[epoch15, step420]: loss 0.660944
[epoch15, step421]: loss 0.365504
[epoch15, step422]: loss 0.600208
[epoch15, step423]: loss 0.819008
[epoch15, step424]: loss 0.386797
[epoch15, step425]: loss 0.692137
[epoch15, step426]: loss 0.523221
[epoch15, step427]: loss 0.460277
[epoch15, step428]: loss 0.656774
[epoch15, step429]: loss 0.330162
[epoch15, step430]: loss 0.477636
[epoch15, step431]: loss 0.633708
[epoch15, step432]: loss 0.685350
[epoch15, step433]: loss 0.836210
[epoch15, step434]: loss 0.606861
[epoch15, step435]: loss 0.523991
[epoch15, step436]: loss 0.460298
[epoch15, step437]: loss 0.444877
[epoch15, step438]: loss 0.409066
[epoch15, step439]: loss 0.357841
[epoch15, step440]: loss 0.755791
[epoch15, step441]: loss 0.634514
[epoch15, step442]: loss 0.830568
[epoch15, step443]: loss 0.779169
[epoch15, step444]: loss 0.665506
[epoch15, step445]: loss 0.390534
[epoch15, step446]: loss 0.492644
[epoch15, step447]: loss 0.716153
[epoch15, step448]: loss 0.484820
[epoch15, step449]: loss 0.361330
[epoch15, step450]: loss 0.561587
[epoch15, step451]: loss 0.457694
[epoch15, step452]: loss 0.562544
[epoch15, step453]: loss 0.578144
[epoch15, step454]: loss 0.708413
[epoch15, step455]: loss 0.718253
[epoch15, step456]: loss 0.614483
[epoch15, step457]: loss 0.525744
[epoch15, step458]: loss 0.588759
[epoch15, step459]: loss 0.705356
[epoch15, step460]: loss 0.231760
[epoch15, step461]: loss 0.562009
[epoch15, step462]: loss 0.693513
[epoch15, step463]: loss 0.562054
[epoch15, step464]: loss 0.548230
[epoch15, step465]: loss 0.537385
[epoch15, step466]: loss 0.697471
[epoch15, step467]: loss 0.501432
[epoch15, step468]: loss 0.809694
[epoch15, step469]: loss 0.445648
[epoch15, step470]: loss 0.719243
[epoch15, step471]: loss 0.591923
[epoch15, step472]: loss 0.555660
[epoch15, step473]: loss 0.465477
[epoch15, step474]: loss 0.573470
[epoch15, step475]: loss 0.479592
[epoch15, step476]: loss 0.513238
[epoch15, step477]: loss 0.422710
[epoch15, step478]: loss 0.390178
[epoch15, step479]: loss 0.423524
[epoch15, step480]: loss 0.841537
[epoch15, step481]: loss 0.652983
[epoch15, step482]: loss 0.456393
[epoch15, step483]: loss 0.366464
[epoch15, step484]: loss 0.725135
[epoch15, step485]: loss 0.811306
[epoch15, step486]: loss 0.743950
[epoch15, step487]: loss 0.540525
[epoch15, step488]: loss 0.469518
[epoch15, step489]: loss 0.492521
[epoch15, step490]: loss 0.368302
[epoch15, step491]: loss 0.535912
[epoch15, step492]: loss 0.642627
[epoch15, step493]: loss 0.608965
[epoch15, step494]: loss 0.673494
[epoch15, step495]: loss 0.585814
[epoch15, step496]: loss 0.547191
[epoch15, step497]: loss 0.557961
[epoch15, step498]: loss 0.832339
[epoch15, step499]: loss 0.672391
[epoch15, step500]: loss 0.521890
[epoch15, step501]: loss 0.278527
[epoch15, step502]: loss 0.620860
[epoch15, step503]: loss 0.335895
[epoch15, step504]: loss 0.665811
[epoch15, step505]: loss 0.507123
[epoch15, step506]: loss 0.668851
[epoch15, step507]: loss 0.597195
[epoch15, step508]: loss 0.521473
[epoch15, step509]: loss 0.818248
[epoch15, step510]: loss 0.562410
[epoch15, step511]: loss 0.658523
[epoch15, step512]: loss 0.520583
[epoch15, step513]: loss 0.579239
[epoch15, step514]: loss 0.653196
[epoch15, step515]: loss 0.770586
[epoch15, step516]: loss 0.500678
[epoch15, step517]: loss 0.715781
[epoch15, step518]: loss 0.611200
[epoch15, step519]: loss 0.686862
[epoch15, step520]: loss 0.529947
[epoch15, step521]: loss 0.768594
[epoch15, step522]: loss 0.235385
[epoch15, step523]: loss 0.303156
[epoch15, step524]: loss 0.660671
[epoch15, step525]: loss 0.678528
[epoch15, step526]: loss 0.520097
[epoch15, step527]: loss 0.422667
[epoch15, step528]: loss 0.524483
[epoch15, step529]: loss 0.409482
[epoch15, step530]: loss 0.702851
[epoch15, step531]: loss 0.708401
[epoch15, step532]: loss 0.745422
[epoch15, step533]: loss 0.611779
[epoch15, step534]: loss 0.535705
[epoch15, step535]: loss 0.682624
[epoch15, step536]: loss 0.375043
[epoch15, step537]: loss 0.474768
[epoch15, step538]: loss 0.435537
[epoch15, step539]: loss 0.534928
[epoch15, step540]: loss 0.386384
[epoch15, step541]: loss 0.397332
[epoch15, step542]: loss 0.726608
[epoch15, step543]: loss 0.578609
[epoch15, step544]: loss 0.502961
[epoch15, step545]: loss 0.408926
[epoch15, step546]: loss 0.577225
[epoch15, step547]: loss 0.352269
[epoch15, step548]: loss 0.528942
[epoch15, step549]: loss 0.606674
[epoch15, step550]: loss 0.480710
[epoch15, step551]: loss 0.401878
[epoch15, step552]: loss 0.543320
[epoch15, step553]: loss 0.474080
[epoch15, step554]: loss 0.375746
[epoch15, step555]: loss 0.424076
[epoch15, step556]: loss 0.311835
[epoch15, step557]: loss 0.480072
[epoch15, step558]: loss 0.488037
[epoch15, step559]: loss 0.567101
[epoch15, step560]: loss 0.519130
[epoch15, step561]: loss 0.479571
[epoch15, step562]: loss 0.289161
[epoch15, step563]: loss 0.875449
[epoch15, step564]: loss 0.626675
[epoch15, step565]: loss 0.693731
[epoch15, step566]: loss 0.847065
[epoch15, step567]: loss 0.645531
[epoch15, step568]: loss 0.536582
[epoch15, step569]: loss 0.617031
[epoch15, step570]: loss 0.516950
[epoch15, step571]: loss 0.361234
[epoch15, step572]: loss 0.557377
[epoch15, step573]: loss 0.724770
[epoch15, step574]: loss 0.509542
[epoch15, step575]: loss 0.794905
[epoch15, step576]: loss 0.676023
[epoch15, step577]: loss 0.541387
[epoch15, step578]: loss 0.606254
[epoch15, step579]: loss 0.677370
[epoch15, step580]: loss 0.262047
[epoch15, step581]: loss 0.462381
[epoch15, step582]: loss 0.494009
[epoch15, step583]: loss 0.628308
[epoch15, step584]: loss 0.567337
[epoch15, step585]: loss 0.577883
[epoch15, step586]: loss 0.389263
[epoch15, step587]: loss 0.486778
[epoch15, step588]: loss 0.837307
[epoch15, step589]: loss 0.861032
[epoch15, step590]: loss 0.627979
[epoch15, step591]: loss 0.652019
[epoch15, step592]: loss 0.390684
[epoch15, step593]: loss 0.550485
[epoch15, step594]: loss 0.362030
[epoch15, step595]: loss 0.669962
[epoch15, step596]: loss 0.524677
[epoch15, step597]: loss 0.592858
[epoch15, step598]: loss 0.584184
[epoch15, step599]: loss 0.642318
[epoch15, step600]: loss 0.368695
[epoch15, step601]: loss 0.729388
[epoch15, step602]: loss 0.641991
[epoch15, step603]: loss 0.755640
[epoch15, step604]: loss 0.395901
[epoch15, step605]: loss 0.631794
[epoch15, step606]: loss 0.554767
[epoch15, step607]: loss 0.515638
[epoch15, step608]: loss 0.588947
[epoch15, step609]: loss 0.389442
[epoch15, step610]: loss 0.631063
[epoch15, step611]: loss 0.616251
[epoch15, step612]: loss 0.499052
[epoch15, step613]: loss 0.270643
[epoch15, step614]: loss 0.668203
[epoch15, step615]: loss 0.383203
[epoch15, step616]: loss 0.562187
[epoch15, step617]: loss 0.543712
[epoch15, step618]: loss 0.613183
[epoch15, step619]: loss 0.561364
[epoch15, step620]: loss 0.497032
[epoch15, step621]: loss 0.275298
[epoch15, step622]: loss 0.272261
[epoch15, step623]: loss 0.608039
[epoch15, step624]: loss 0.382968
[epoch15, step625]: loss 0.508267
[epoch15, step626]: loss 0.594134
[epoch15, step627]: loss 0.143436
[epoch15, step628]: loss 0.541925
[epoch15, step629]: loss 0.474484
[epoch15, step630]: loss 0.631327
[epoch15, step631]: loss 0.566210
[epoch15, step632]: loss 0.650317
[epoch15, step633]: loss 0.632669
[epoch15, step634]: loss 0.505607
[epoch15, step635]: loss 0.379560
[epoch15, step636]: loss 0.592777
[epoch15, step637]: loss 0.680343
[epoch15, step638]: loss 0.419149
[epoch15, step639]: loss 0.655510
[epoch15, step640]: loss 0.687800
[epoch15, step641]: loss 0.692468
[epoch15, step642]: loss 0.816020
[epoch15, step643]: loss 0.432095
[epoch15, step644]: loss 0.656729
[epoch15, step645]: loss 0.291458
[epoch15, step646]: loss 0.623101
[epoch15, step647]: loss 0.615363
[epoch15, step648]: loss 0.473196
[epoch15, step649]: loss 0.571792
[epoch15, step650]: loss 0.814940
[epoch15, step651]: loss 0.593023
[epoch15, step652]: loss 0.406763
[epoch15, step653]: loss 0.428671
[epoch15, step654]: loss 0.664218
[epoch15, step655]: loss 0.524215
[epoch15, step656]: loss 0.411131
[epoch15, step657]: loss 0.623366
[epoch15, step658]: loss 0.565983
[epoch15, step659]: loss 0.656306
[epoch15, step660]: loss 0.671301
[epoch15, step661]: loss 0.591393
[epoch15, step662]: loss 0.679351
[epoch15, step663]: loss 0.494054
[epoch15, step664]: loss 0.372241
[epoch15, step665]: loss 0.785311
[epoch15, step666]: loss 0.632639
[epoch15, step667]: loss 0.456969
[epoch15, step668]: loss 0.696842
[epoch15, step669]: loss 0.673893
[epoch15, step670]: loss 0.379016
[epoch15, step671]: loss 0.413036
[epoch15, step672]: loss 0.609649
[epoch15, step673]: loss 0.273847
[epoch15, step674]: loss 0.439500
[epoch15, step675]: loss 0.545027
[epoch15, step676]: loss 0.732554
[epoch15, step677]: loss 0.413537
[epoch15, step678]: loss 0.528772
[epoch15, step679]: loss 0.784794
[epoch15, step680]: loss 0.361489
[epoch15, step681]: loss 0.161015
[epoch15, step682]: loss 0.570140
[epoch15, step683]: loss 0.742009
[epoch15, step684]: loss 0.610783
[epoch15, step685]: loss 0.556846
[epoch15, step686]: loss 0.524347
[epoch15, step687]: loss 0.623286
[epoch15, step688]: loss 0.370433
[epoch15, step689]: loss 0.443076
[epoch15, step690]: loss 0.658811
[epoch15, step691]: loss 0.552886
[epoch15, step692]: loss 0.641124
[epoch15, step693]: loss 0.449394
[epoch15, step694]: loss 0.371220
[epoch15, step695]: loss 0.325806
[epoch15, step696]: loss 0.499302
[epoch15, step697]: loss 0.594274
[epoch15, step698]: loss 0.493386
[epoch15, step699]: loss 0.355002
[epoch15, step700]: loss 0.668793
[epoch15, step701]: loss 0.726945
[epoch15, step702]: loss 0.702656
[epoch15, step703]: loss 0.300635
[epoch15, step704]: loss 0.564457
[epoch15, step705]: loss 0.455907
[epoch15, step706]: loss 0.589084
[epoch15, step707]: loss 0.370435
[epoch15, step708]: loss 0.620014
[epoch15, step709]: loss 0.535858
[epoch15, step710]: loss 0.662051
[epoch15, step711]: loss 0.487251
[epoch15, step712]: loss 0.675479
[epoch15, step713]: loss 0.535347
[epoch15, step714]: loss 0.611248
[epoch15, step715]: loss 0.203028
[epoch15, step716]: loss 0.457035
[epoch15, step717]: loss 0.266947
[epoch15, step718]: loss 0.668273
[epoch15, step719]: loss 0.740519
[epoch15, step720]: loss 0.735345
[epoch15, step721]: loss 0.167708
[epoch15, step722]: loss 0.283715
[epoch15, step723]: loss 0.672727
[epoch15, step724]: loss 0.715520
[epoch15, step725]: loss 0.816611
[epoch15, step726]: loss 0.758191
[epoch15, step727]: loss 0.539478
[epoch15, step728]: loss 0.680891
[epoch15, step729]: loss 0.400980
[epoch15, step730]: loss 0.683796
[epoch15, step731]: loss 0.529873
[epoch15, step732]: loss 0.693853
[epoch15, step733]: loss 0.701518
[epoch15, step734]: loss 0.485139
[epoch15, step735]: loss 0.467915
[epoch15, step736]: loss 0.671744
[epoch15, step737]: loss 0.733219
[epoch15, step738]: loss 0.402364
[epoch15, step739]: loss 0.865344
[epoch15, step740]: loss 0.336999
[epoch15, step741]: loss 0.631282
[epoch15, step742]: loss 0.359308
[epoch15, step743]: loss 0.466346
[epoch15, step744]: loss 0.430882
[epoch15, step745]: loss 0.290296
[epoch15, step746]: loss 0.649486
[epoch15, step747]: loss 0.466878
[epoch15, step748]: loss 0.706031
[epoch15, step749]: loss 0.378558
[epoch15, step750]: loss 0.721022
[epoch15, step751]: loss 0.418842
[epoch15, step752]: loss 0.692068
[epoch15, step753]: loss 0.584852
[epoch15, step754]: loss 0.630667
[epoch15, step755]: loss 0.647395
[epoch15, step756]: loss 0.538193
[epoch15, step757]: loss 0.512389
[epoch15, step758]: loss 0.490321
[epoch15, step759]: loss 0.767899
[epoch15, step760]: loss 0.631770
[epoch15, step761]: loss 0.458711
[epoch15, step762]: loss 0.750579
[epoch15, step763]: loss 0.726602
[epoch15, step764]: loss 0.520028
[epoch15, step765]: loss 0.616063
[epoch15, step766]: loss 0.635149
[epoch15, step767]: loss 0.575729
[epoch15, step768]: loss 0.461444
[epoch15, step769]: loss 0.447586
[epoch15, step770]: loss 0.711078
[epoch15, step771]: loss 0.468612
[epoch15, step772]: loss 0.741414
[epoch15, step773]: loss 0.414712
[epoch15, step774]: loss 0.513352
[epoch15, step775]: loss 0.642035
[epoch15, step776]: loss 0.504672
[epoch15, step777]: loss 0.530837
[epoch15, step778]: loss 0.412761
[epoch15, step779]: loss 0.611446
[epoch15, step780]: loss 0.675283
[epoch15, step781]: loss 0.449438
[epoch15, step782]: loss 0.425437
[epoch15, step783]: loss 0.448478
[epoch15, step784]: loss 0.586501
[epoch15, step785]: loss 0.608635
[epoch15, step786]: loss 0.515817
[epoch15, step787]: loss 0.312846
[epoch15, step788]: loss 0.855379
[epoch15, step789]: loss 0.448651
[epoch15, step790]: loss 0.409282
[epoch15, step791]: loss 0.654452
[epoch15, step792]: loss 0.534202
[epoch15, step793]: loss 0.630984
[epoch15, step794]: loss 0.470864
[epoch15, step795]: loss 0.566117
[epoch15, step796]: loss 0.615458
[epoch15, step797]: loss 0.783927
[epoch15, step798]: loss 0.653255
[epoch15, step799]: loss 0.645502
[epoch15, step800]: loss 0.314207
[epoch15, step801]: loss 0.327954
[epoch15, step802]: loss 0.461726
[epoch15, step803]: loss 0.481164
[epoch15, step804]: loss 0.548337
[epoch15, step805]: loss 0.584235
[epoch15, step806]: loss 0.693731
[epoch15, step807]: loss 0.595898
[epoch15, step808]: loss 0.701616
[epoch15, step809]: loss 0.346831
[epoch15, step810]: loss 0.253382
[epoch15, step811]: loss 0.513101
[epoch15, step812]: loss 0.609330
[epoch15, step813]: loss 0.566789
[epoch15, step814]: loss 0.617411
[epoch15, step815]: loss 0.662872
[epoch15, step816]: loss 0.615600
[epoch15, step817]: loss 0.542296
[epoch15, step818]: loss 0.584748
[epoch15, step819]: loss 0.696674
[epoch15, step820]: loss 0.454365
[epoch15, step821]: loss 0.515746
[epoch15, step822]: loss 0.869735
[epoch15, step823]: loss 0.672310
[epoch15, step824]: loss 0.519384
[epoch15, step825]: loss 0.479556
[epoch15, step826]: loss 0.400515
[epoch15, step827]: loss 0.669176
[epoch15, step828]: loss 0.330188
[epoch15, step829]: loss 0.679481
[epoch15, step830]: loss 0.351774
[epoch15, step831]: loss 0.504493
[epoch15, step832]: loss 0.163561
[epoch15, step833]: loss 0.492939
[epoch15, step834]: loss 0.290581
[epoch15, step835]: loss 0.299682
[epoch15, step836]: loss 0.582288
[epoch15, step837]: loss 0.613758
[epoch15, step838]: loss 0.345093
[epoch15, step839]: loss 0.495963
[epoch15, step840]: loss 0.592571
[epoch15, step841]: loss 0.690515
[epoch15, step842]: loss 0.535695
[epoch15, step843]: loss 0.496316
[epoch15, step844]: loss 0.622915
[epoch15, step845]: loss 0.507382
[epoch15, step846]: loss 0.483271
[epoch15, step847]: loss 0.388082
[epoch15, step848]: loss 0.688280
[epoch15, step849]: loss 0.457078
[epoch15, step850]: loss 0.611878
[epoch15, step851]: loss 0.498581
[epoch15, step852]: loss 0.558704
[epoch15, step853]: loss 0.487666
[epoch15, step854]: loss 0.636989
[epoch15, step855]: loss 0.588146
[epoch15, step856]: loss 0.588942
[epoch15, step857]: loss 0.311801
[epoch15, step858]: loss 0.644306
[epoch15, step859]: loss 0.719456
[epoch15, step860]: loss 0.343266
[epoch15, step861]: loss 0.467257
[epoch15, step862]: loss 0.528585
[epoch15, step863]: loss 0.453566
[epoch15, step864]: loss 0.568599
[epoch15, step865]: loss 0.517463
[epoch15, step866]: loss 0.476328
[epoch15, step867]: loss 0.399491
[epoch15, step868]: loss 0.365978
[epoch15, step869]: loss 0.595316
[epoch15, step870]: loss 0.630041
[epoch15, step871]: loss 0.640783
[epoch15, step872]: loss 0.586257
[epoch15, step873]: loss 0.712620
[epoch15, step874]: loss 0.494245
[epoch15, step875]: loss 0.620853
[epoch15, step876]: loss 0.623017
[epoch15, step877]: loss 0.536989
[epoch15, step878]: loss 0.836422
[epoch15, step879]: loss 0.654572
[epoch15, step880]: loss 0.411954
[epoch15, step881]: loss 0.540354
[epoch15, step882]: loss 0.658435
[epoch15, step883]: loss 0.503724
[epoch15, step884]: loss 0.607889
[epoch15, step885]: loss 0.662147
[epoch15, step886]: loss 0.744151
[epoch15, step887]: loss 0.412574
[epoch15, step888]: loss 0.453681
[epoch15, step889]: loss 0.762305
[epoch15, step890]: loss 0.734886
[epoch15, step891]: loss 0.439224
[epoch15, step892]: loss 0.434797
[epoch15, step893]: loss 0.290722
[epoch15, step894]: loss 0.285779
[epoch15, step895]: loss 0.499225
[epoch15, step896]: loss 0.538923
[epoch15, step897]: loss 0.821130
[epoch15, step898]: loss 0.649976
[epoch15, step899]: loss 0.604272
[epoch15, step900]: loss 0.430292
[epoch15, step901]: loss 0.583861
[epoch15, step902]: loss 0.392055
[epoch15, step903]: loss 0.703163
[epoch15, step904]: loss 0.436993
[epoch15, step905]: loss 0.407523
[epoch15, step906]: loss 0.381461
[epoch15, step907]: loss 0.587770
[epoch15, step908]: loss 0.351895
[epoch15, step909]: loss 0.686129
[epoch15, step910]: loss 0.360709
[epoch15, step911]: loss 0.561695
[epoch15, step912]: loss 0.483680
[epoch15, step913]: loss 0.505224
[epoch15, step914]: loss 0.341407
[epoch15, step915]: loss 0.632561
[epoch15, step916]: loss 0.562736
[epoch15, step917]: loss 0.581480
[epoch15, step918]: loss 0.547253
[epoch15, step919]: loss 0.489280
[epoch15, step920]: loss 0.630541
[epoch15, step921]: loss 0.553121
[epoch15, step922]: loss 0.229116
[epoch15, step923]: loss 0.368356
[epoch15, step924]: loss 0.633196
[epoch15, step925]: loss 0.331018
[epoch15, step926]: loss 0.684645
[epoch15, step927]: loss 0.455309
[epoch15, step928]: loss 0.543650
[epoch15, step929]: loss 0.584287
[epoch15, step930]: loss 0.621697
[epoch15, step931]: loss 0.581813
[epoch15, step932]: loss 0.630117
[epoch15, step933]: loss 0.626326
[epoch15, step934]: loss 0.759377
[epoch15, step935]: loss 0.484548
[epoch15, step936]: loss 0.348649
[epoch15, step937]: loss 0.715101
[epoch15, step938]: loss 0.341379
[epoch15, step939]: loss 0.640221
[epoch15, step940]: loss 0.634262
[epoch15, step941]: loss 0.655646
[epoch15, step942]: loss 0.721756
[epoch15, step943]: loss 0.757689
[epoch15, step944]: loss 0.428300
[epoch15, step945]: loss 0.583582
[epoch15, step946]: loss 0.379021
[epoch15, step947]: loss 0.722873
[epoch15, step948]: loss 0.726916
[epoch15, step949]: loss 0.594608
[epoch15, step950]: loss 0.880366
[epoch15, step951]: loss 0.545369
[epoch15, step952]: loss 0.521133
[epoch15, step953]: loss 0.571677
[epoch15, step954]: loss 0.450972
[epoch15, step955]: loss 0.464549
[epoch15, step956]: loss 0.790153
[epoch15, step957]: loss 0.478693
[epoch15, step958]: loss 0.207090
[epoch15, step959]: loss 0.489171
[epoch15, step960]: loss 0.574157
[epoch15, step961]: loss 0.579822
[epoch15, step962]: loss 0.308295
[epoch15, step963]: loss 0.386216
[epoch15, step964]: loss 0.576859
[epoch15, step965]: loss 0.483639
[epoch15, step966]: loss 0.534708
[epoch15, step967]: loss 0.686438
[epoch15, step968]: loss 0.614082
[epoch15, step969]: loss 0.417668
[epoch15, step970]: loss 0.532524
[epoch15, step971]: loss 0.332042
[epoch15, step972]: loss 0.534945
[epoch15, step973]: loss 0.463101
[epoch15, step974]: loss 0.396434
[epoch15, step975]: loss 0.333773
[epoch15, step976]: loss 0.648092
[epoch15, step977]: loss 0.497667
[epoch15, step978]: loss 0.425100
[epoch15, step979]: loss 0.483032
[epoch15, step980]: loss 0.668283
[epoch15, step981]: loss 0.711429
[epoch15, step982]: loss 0.532398
[epoch15, step983]: loss 0.587541
[epoch15, step984]: loss 0.306083
[epoch15, step985]: loss 0.719638
[epoch15, step986]: loss 0.647670
[epoch15, step987]: loss 0.291121
[epoch15, step988]: loss 0.868726
[epoch15, step989]: loss 0.530732
[epoch15, step990]: loss 0.749457
[epoch15, step991]: loss 0.625927
[epoch15, step992]: loss 0.591789
[epoch15, step993]: loss 0.632544
[epoch15, step994]: loss 0.636436
[epoch15, step995]: loss 0.602761
[epoch15, step996]: loss 0.841286
[epoch15, step997]: loss 0.760469
[epoch15, step998]: loss 0.468465
[epoch15, step999]: loss 0.557751
[epoch15, step1000]: loss 0.532157
[epoch15, step1001]: loss 0.418430
[epoch15, step1002]: loss 0.792414
[epoch15, step1003]: loss 0.463836
[epoch15, step1004]: loss 0.590670
[epoch15, step1005]: loss 0.671187
[epoch15, step1006]: loss 0.370252
[epoch15, step1007]: loss 0.666632
[epoch15, step1008]: loss 0.464108
[epoch15, step1009]: loss 0.579777
[epoch15, step1010]: loss 0.635887
[epoch15, step1011]: loss 0.463026
[epoch15, step1012]: loss 0.515299
[epoch15, step1013]: loss 0.706545
[epoch15, step1014]: loss 0.343499
[epoch15, step1015]: loss 0.672163
[epoch15, step1016]: loss 0.822817
[epoch15, step1017]: loss 0.726666
[epoch15, step1018]: loss 0.582613
[epoch15, step1019]: loss 0.538343
[epoch15, step1020]: loss 0.339556
[epoch15, step1021]: loss 0.254866
[epoch15, step1022]: loss 0.478438
[epoch15, step1023]: loss 0.539801
[epoch15, step1024]: loss 0.319210
[epoch15, step1025]: loss 0.630846
[epoch15, step1026]: loss 0.522491
[epoch15, step1027]: loss 0.632803
[epoch15, step1028]: loss 0.515849
[epoch15, step1029]: loss 0.590599
[epoch15, step1030]: loss 0.540399
[epoch15, step1031]: loss 0.711389
[epoch15, step1032]: loss 0.802124
[epoch15, step1033]: loss 0.354543
[epoch15, step1034]: loss 0.313518
[epoch15, step1035]: loss 0.678678
[epoch15, step1036]: loss 0.725999
[epoch15, step1037]: loss 0.458133
[epoch15, step1038]: loss 0.478636
[epoch15, step1039]: loss 0.685871
[epoch15, step1040]: loss 0.504705
[epoch15, step1041]: loss 0.350070
[epoch15, step1042]: loss 0.533418
[epoch15, step1043]: loss 0.369448
[epoch15, step1044]: loss 0.696641
[epoch15, step1045]: loss 0.606054
[epoch15, step1046]: loss 0.437057
[epoch15, step1047]: loss 0.409662
[epoch15, step1048]: loss 0.951021
[epoch15, step1049]: loss 0.642007
[epoch15, step1050]: loss 0.769311
[epoch15, step1051]: loss 0.766133
[epoch15, step1052]: loss 0.312678
[epoch15, step1053]: loss 0.565804
[epoch15, step1054]: loss 0.605774
[epoch15, step1055]: loss 0.617392
[epoch15, step1056]: loss 0.786969
[epoch15, step1057]: loss 0.373212
[epoch15, step1058]: loss 0.564010
[epoch15, step1059]: loss 0.566091
[epoch15, step1060]: loss 0.604253
[epoch15, step1061]: loss 0.587300
[epoch15, step1062]: loss 0.608613
[epoch15, step1063]: loss 0.423984
[epoch15, step1064]: loss 0.330534
[epoch15, step1065]: loss 0.331276
[epoch15, step1066]: loss 0.549643
[epoch15, step1067]: loss 0.683473
[epoch15, step1068]: loss 0.692761
[epoch15, step1069]: loss 0.510490
[epoch15, step1070]: loss 0.636140
[epoch15, step1071]: loss 0.486765
[epoch15, step1072]: loss 0.788150
[epoch15, step1073]: loss 0.722495
[epoch15, step1074]: loss 0.686967
[epoch15, step1075]: loss 0.561089
[epoch15, step1076]: loss 0.659411
[epoch15, step1077]: loss 0.254489
[epoch15, step1078]: loss 0.478706
[epoch15, step1079]: loss 0.277696
[epoch15, step1080]: loss 0.615903
[epoch15, step1081]: loss 0.428666
[epoch15, step1082]: loss 0.427343
[epoch15, step1083]: loss 0.424713
[epoch15, step1084]: loss 0.750415
[epoch15, step1085]: loss 0.556118
[epoch15, step1086]: loss 0.828711
[epoch15, step1087]: loss 0.396824
[epoch15, step1088]: loss 0.703041
[epoch15, step1089]: loss 0.642055
[epoch15, step1090]: loss 0.763252
[epoch15, step1091]: loss 0.690792
[epoch15, step1092]: loss 0.476049
[epoch15, step1093]: loss 0.531879
[epoch15, step1094]: loss 0.436957
[epoch15, step1095]: loss 0.433320
[epoch15, step1096]: loss 0.642428
[epoch15, step1097]: loss 0.572678
[epoch15, step1098]: loss 0.522696
[epoch15, step1099]: loss 0.541827
[epoch15, step1100]: loss 0.409722
[epoch15, step1101]: loss 0.829942
[epoch15, step1102]: loss 0.575792
[epoch15, step1103]: loss 0.581839
[epoch15, step1104]: loss 0.842316
[epoch15, step1105]: loss 0.175131
[epoch15, step1106]: loss 0.588844
[epoch15, step1107]: loss 0.497702
[epoch15, step1108]: loss 0.328020
[epoch15, step1109]: loss 0.478168
[epoch15, step1110]: loss 0.559202
[epoch15, step1111]: loss 0.618734
[epoch15, step1112]: loss 0.710268
[epoch15, step1113]: loss 0.614403
[epoch15, step1114]: loss 0.635881
[epoch15, step1115]: loss 0.552702
[epoch15, step1116]: loss 0.245291
[epoch15, step1117]: loss 0.374148
[epoch15, step1118]: loss 0.697023
[epoch15, step1119]: loss 0.616308
[epoch15, step1120]: loss 0.475753
[epoch15, step1121]: loss 0.437466
[epoch15, step1122]: loss 0.545293
[epoch15, step1123]: loss 0.521211
[epoch15, step1124]: loss 0.361439
[epoch15, step1125]: loss 0.320173
[epoch15, step1126]: loss 0.461349
[epoch15, step1127]: loss 0.347174
[epoch15, step1128]: loss 0.503958
[epoch15, step1129]: loss 0.495605
[epoch15, step1130]: loss 0.364796
[epoch15, step1131]: loss 0.624960
[epoch15, step1132]: loss 0.626645
[epoch15, step1133]: loss 0.808180
[epoch15, step1134]: loss 0.526579
[epoch15, step1135]: loss 0.511626
[epoch15, step1136]: loss 0.626888
[epoch15, step1137]: loss 0.726826
[epoch15, step1138]: loss 0.756112
[epoch15, step1139]: loss 0.389124
[epoch15, step1140]: loss 0.472411
[epoch15, step1141]: loss 0.398742
[epoch15, step1142]: loss 0.775967
[epoch15, step1143]: loss 0.543326
[epoch15, step1144]: loss 0.758816
[epoch15, step1145]: loss 0.463394
[epoch15, step1146]: loss 0.766926
[epoch15, step1147]: loss 0.466911
[epoch15, step1148]: loss 0.702846
[epoch15, step1149]: loss 0.718767
[epoch15, step1150]: loss 0.487598
[epoch15, step1151]: loss 0.555506
[epoch15, step1152]: loss 0.695464
[epoch15, step1153]: loss 0.512902
[epoch15, step1154]: loss 0.586628
[epoch15, step1155]: loss 0.573483
[epoch15, step1156]: loss 0.739654
[epoch15, step1157]: loss 0.711901
[epoch15, step1158]: loss 0.708825
[epoch15, step1159]: loss 0.438414
[epoch15, step1160]: loss 0.632356
[epoch15, step1161]: loss 0.291805
[epoch15, step1162]: loss 0.669887
[epoch15, step1163]: loss 0.679017
[epoch15, step1164]: loss 0.354376
[epoch15, step1165]: loss 0.550061
[epoch15, step1166]: loss 0.287816
[epoch15, step1167]: loss 0.310955
[epoch15, step1168]: loss 0.641328
[epoch15, step1169]: loss 0.377228
[epoch15, step1170]: loss 0.452048
[epoch15, step1171]: loss 0.351339
[epoch15, step1172]: loss 0.189953
[epoch15, step1173]: loss 0.636304
[epoch15, step1174]: loss 0.597127
[epoch15, step1175]: loss 0.366992
[epoch15, step1176]: loss 0.715715
[epoch15, step1177]: loss 0.447920
[epoch15, step1178]: loss 0.836375
[epoch15, step1179]: loss 0.610145
[epoch15, step1180]: loss 0.480366
[epoch15, step1181]: loss 0.500151
[epoch15, step1182]: loss 0.379228
[epoch15, step1183]: loss 0.516380
[epoch15, step1184]: loss 0.357746
[epoch15, step1185]: loss 0.633413
[epoch15, step1186]: loss 0.680807
[epoch15, step1187]: loss 0.357520
[epoch15, step1188]: loss 0.606029
[epoch15, step1189]: loss 0.666099
[epoch15, step1190]: loss 0.377712
[epoch15, step1191]: loss 0.534765
[epoch15, step1192]: loss 0.260679
[epoch15, step1193]: loss 0.564505
[epoch15, step1194]: loss 0.400720
[epoch15, step1195]: loss 0.582691
[epoch15, step1196]: loss 0.578860
[epoch15, step1197]: loss 0.580320
[epoch15, step1198]: loss 0.305691
[epoch15, step1199]: loss 0.736954
[epoch15, step1200]: loss 0.545149
[epoch15, step1201]: loss 0.786278
[epoch15, step1202]: loss 0.373633
[epoch15, step1203]: loss 0.524988
[epoch15, step1204]: loss 0.589452
[epoch15, step1205]: loss 0.606646
[epoch15, step1206]: loss 0.541126
[epoch15, step1207]: loss 0.397380
[epoch15, step1208]: loss 0.797440
[epoch15, step1209]: loss 0.560237
[epoch15, step1210]: loss 0.483489
[epoch15, step1211]: loss 0.534831
[epoch15, step1212]: loss 0.480613
[epoch15, step1213]: loss 0.805701
[epoch15, step1214]: loss 0.650068
[epoch15, step1215]: loss 0.396340
[epoch15, step1216]: loss 0.526941
[epoch15, step1217]: loss 0.680098
[epoch15, step1218]: loss 0.467292
[epoch15, step1219]: loss 0.368650
[epoch15, step1220]: loss 0.477734
[epoch15, step1221]: loss 0.569095
[epoch15, step1222]: loss 0.511149
[epoch15, step1223]: loss 0.684312
[epoch15, step1224]: loss 0.409480
[epoch15, step1225]: loss 0.514130
[epoch15, step1226]: loss 0.487071
[epoch15, step1227]: loss 0.439956
[epoch15, step1228]: loss 0.392841
[epoch15, step1229]: loss 0.428396
[epoch15, step1230]: loss 0.669275
[epoch15, step1231]: loss 0.367118
[epoch15, step1232]: loss 0.623554
[epoch15, step1233]: loss 0.546463
[epoch15, step1234]: loss 0.728112
[epoch15, step1235]: loss 0.748640
[epoch15, step1236]: loss 0.581146
[epoch15, step1237]: loss 0.290043
[epoch15, step1238]: loss 0.626525
[epoch15, step1239]: loss 0.454301
[epoch15, step1240]: loss 0.582550
[epoch15, step1241]: loss 0.744265
[epoch15, step1242]: loss 0.679294
[epoch15, step1243]: loss 0.340422
[epoch15, step1244]: loss 0.401302
[epoch15, step1245]: loss 0.503235
[epoch15, step1246]: loss 0.522547
[epoch15, step1247]: loss 0.574730
[epoch15, step1248]: loss 0.479922
[epoch15, step1249]: loss 0.784369
[epoch15, step1250]: loss 0.846428
[epoch15, step1251]: loss 0.491318
[epoch15, step1252]: loss 0.531398
[epoch15, step1253]: loss 0.650077
[epoch15, step1254]: loss 0.329459
[epoch15, step1255]: loss 0.335779
[epoch15, step1256]: loss 0.470453
[epoch15, step1257]: loss 0.521785
[epoch15, step1258]: loss 0.473344
[epoch15, step1259]: loss 0.642381
[epoch15, step1260]: loss 0.603884
[epoch15, step1261]: loss 0.223674
[epoch15, step1262]: loss 0.200104
[epoch15, step1263]: loss 0.698790
[epoch15, step1264]: loss 0.619312
[epoch15, step1265]: loss 0.786948
[epoch15, step1266]: loss 0.571262
[epoch15, step1267]: loss 0.571299
[epoch15, step1268]: loss 0.450219
[epoch15, step1269]: loss 0.536283
[epoch15, step1270]: loss 0.513225
[epoch15, step1271]: loss 0.423602
[epoch15, step1272]: loss 0.807660
[epoch15, step1273]: loss 0.484268
[epoch15, step1274]: loss 0.596261
[epoch15, step1275]: loss 0.643318
[epoch15, step1276]: loss 0.510024
[epoch15, step1277]: loss 0.443241
[epoch15, step1278]: loss 0.554586
[epoch15, step1279]: loss 0.646316
[epoch15, step1280]: loss 0.620424
[epoch15, step1281]: loss 0.549902
[epoch15, step1282]: loss 0.496953
[epoch15, step1283]: loss 0.590430
[epoch15, step1284]: loss 0.607872
[epoch15, step1285]: loss 0.541550
[epoch15, step1286]: loss 0.698243
[epoch15, step1287]: loss 0.192711
[epoch15, step1288]: loss 0.566344
[epoch15, step1289]: loss 0.493313
[epoch15, step1290]: loss 0.271820
[epoch15, step1291]: loss 0.648319
[epoch15, step1292]: loss 0.677449
[epoch15, step1293]: loss 0.393931
[epoch15, step1294]: loss 0.169134
[epoch15, step1295]: loss 0.639554
[epoch15, step1296]: loss 0.504435
[epoch15, step1297]: loss 0.468933
[epoch15, step1298]: loss 0.636796
[epoch15, step1299]: loss 0.478168
[epoch15, step1300]: loss 0.608691
[epoch15, step1301]: loss 0.583953
[epoch15, step1302]: loss 0.357353
[epoch15, step1303]: loss 0.740694
[epoch15, step1304]: loss 0.403139
[epoch15, step1305]: loss 0.425055
[epoch15, step1306]: loss 0.645172
[epoch15, step1307]: loss 0.465836
[epoch15, step1308]: loss 0.504802
[epoch15, step1309]: loss 0.851550
[epoch15, step1310]: loss 0.507435
[epoch15, step1311]: loss 0.459613
[epoch15, step1312]: loss 0.539038
[epoch15, step1313]: loss 0.734807
[epoch15, step1314]: loss 0.447044
[epoch15, step1315]: loss 0.473776
[epoch15, step1316]: loss 0.699529
[epoch15, step1317]: loss 0.413201
[epoch15, step1318]: loss 0.670501
[epoch15, step1319]: loss 0.220997
[epoch15, step1320]: loss 0.819101
[epoch15, step1321]: loss 0.376420
[epoch15, step1322]: loss 0.477205
[epoch15, step1323]: loss 0.549954
[epoch15, step1324]: loss 0.552494
[epoch15, step1325]: loss 0.686193
[epoch15, step1326]: loss 0.550412
[epoch15, step1327]: loss 0.434373
[epoch15, step1328]: loss 0.831052
[epoch15, step1329]: loss 0.130627
[epoch15, step1330]: loss 0.469302
[epoch15, step1331]: loss 0.373797
[epoch15, step1332]: loss 0.652015
[epoch15, step1333]: loss 0.476292
[epoch15, step1334]: loss 0.543357
[epoch15, step1335]: loss 0.765112
[epoch15, step1336]: loss 0.428071
[epoch15, step1337]: loss 0.577007
[epoch15, step1338]: loss 0.704105
[epoch15, step1339]: loss 0.341336
[epoch15, step1340]: loss 0.361348
[epoch15, step1341]: loss 0.705089
[epoch15, step1342]: loss 0.761936
[epoch15, step1343]: loss 0.388867
[epoch15, step1344]: loss 0.742583
[epoch15, step1345]: loss 0.634207
[epoch15, step1346]: loss 0.614112
[epoch15, step1347]: loss 0.658188
[epoch15, step1348]: loss 0.607439
[epoch15, step1349]: loss 0.446481
[epoch15, step1350]: loss 0.412437
[epoch15, step1351]: loss 0.420323
[epoch15, step1352]: loss 0.431283
[epoch15, step1353]: loss 0.570592
[epoch15, step1354]: loss 0.687976
[epoch15, step1355]: loss 0.378112
[epoch15, step1356]: loss 0.672114
[epoch15, step1357]: loss 0.537769
[epoch15, step1358]: loss 0.497734
[epoch15, step1359]: loss 0.619204
[epoch15, step1360]: loss 0.320367
[epoch15, step1361]: loss 0.617177
[epoch15, step1362]: loss 0.274227
[epoch15, step1363]: loss 0.363158
[epoch15, step1364]: loss 0.588937
[epoch15, step1365]: loss 0.737527
[epoch15, step1366]: loss 0.347997
[epoch15, step1367]: loss 0.330985
[epoch15, step1368]: loss 0.569316
[epoch15, step1369]: loss 0.338658
[epoch15, step1370]: loss 0.792578
[epoch15, step1371]: loss 0.151647
[epoch15, step1372]: loss 0.493743
[epoch15, step1373]: loss 0.548730
[epoch15, step1374]: loss 0.538877
[epoch15, step1375]: loss 0.710843
[epoch15, step1376]: loss 0.717463
[epoch15, step1377]: loss 0.342643
[epoch15, step1378]: loss 0.421881
[epoch15, step1379]: loss 0.480825
[epoch15, step1380]: loss 0.472128
[epoch15, step1381]: loss 0.469161
[epoch15, step1382]: loss 0.460701
[epoch15, step1383]: loss 0.711317
[epoch15, step1384]: loss 0.391552
[epoch15, step1385]: loss 0.400421
[epoch15, step1386]: loss 0.532321
[epoch15, step1387]: loss 0.614975
[epoch15, step1388]: loss 0.605750
[epoch15, step1389]: loss 0.609062
[epoch15, step1390]: loss 0.747713
[epoch15, step1391]: loss 0.354418
[epoch15, step1392]: loss 0.608395
[epoch15, step1393]: loss 0.461496
[epoch15, step1394]: loss 0.620851
[epoch15, step1395]: loss 0.796110
[epoch15, step1396]: loss 0.386547
[epoch15, step1397]: loss 0.464762
[epoch15, step1398]: loss 0.580031
[epoch15, step1399]: loss 0.696887
[epoch15, step1400]: loss 0.398792
[epoch15, step1401]: loss 0.836342
[epoch15, step1402]: loss 0.438843
[epoch15, step1403]: loss 0.461772
[epoch15, step1404]: loss 0.350544
[epoch15, step1405]: loss 0.554896
[epoch15, step1406]: loss 0.424269
[epoch15, step1407]: loss 0.768338
[epoch15, step1408]: loss 0.758412
[epoch15, step1409]: loss 0.697087
[epoch15, step1410]: loss 0.522727
[epoch15, step1411]: loss 0.670867
[epoch15, step1412]: loss 0.622569
[epoch15, step1413]: loss 0.564729
[epoch15, step1414]: loss 0.578500
[epoch15, step1415]: loss 0.776253
[epoch15, step1416]: loss 0.564155
[epoch15, step1417]: loss 0.401767
[epoch15, step1418]: loss 0.656717
[epoch15, step1419]: loss 0.524838
[epoch15, step1420]: loss 0.765152
[epoch15, step1421]: loss 0.492063
[epoch15, step1422]: loss 0.739081
[epoch15, step1423]: loss 0.575124
[epoch15, step1424]: loss 0.630706
[epoch15, step1425]: loss 0.388707
[epoch15, step1426]: loss 0.547659
[epoch15, step1427]: loss 0.439520
[epoch15, step1428]: loss 0.375061
[epoch15, step1429]: loss 0.605744
[epoch15, step1430]: loss 0.472488
[epoch15, step1431]: loss 0.684590
[epoch15, step1432]: loss 0.369129
[epoch15, step1433]: loss 0.239695
[epoch15, step1434]: loss 0.423971
[epoch15, step1435]: loss 0.632027
[epoch15, step1436]: loss 0.655548
[epoch15, step1437]: loss 0.623700
[epoch15, step1438]: loss 0.807090
[epoch15, step1439]: loss 0.513277
[epoch15, step1440]: loss 0.427799
[epoch15, step1441]: loss 0.318462
[epoch15, step1442]: loss 0.643562
[epoch15, step1443]: loss 0.519524
[epoch15, step1444]: loss 0.707571
[epoch15, step1445]: loss 0.649499
[epoch15, step1446]: loss 0.425090
[epoch15, step1447]: loss 0.620632
[epoch15, step1448]: loss 0.515422
[epoch15, step1449]: loss 0.558269
[epoch15, step1450]: loss 0.504778
[epoch15, step1451]: loss 0.678758
[epoch15, step1452]: loss 0.683957
[epoch15, step1453]: loss 0.492654
[epoch15, step1454]: loss 0.547612
[epoch15, step1455]: loss 0.697478
[epoch15, step1456]: loss 0.530865
[epoch15, step1457]: loss 0.547150
[epoch15, step1458]: loss 0.493380
[epoch15, step1459]: loss 0.496513
[epoch15, step1460]: loss 0.651023
[epoch15, step1461]: loss 0.507000
[epoch15, step1462]: loss 0.616471
[epoch15, step1463]: loss 0.616679
[epoch15, step1464]: loss 0.674105
[epoch15, step1465]: loss 0.554670
[epoch15, step1466]: loss 0.491079
[epoch15, step1467]: loss 0.154526
[epoch15, step1468]: loss 0.599164
[epoch15, step1469]: loss 0.648161
[epoch15, step1470]: loss 0.576418
[epoch15, step1471]: loss 0.681719
[epoch15, step1472]: loss 0.660073
[epoch15, step1473]: loss 0.579428
[epoch15, step1474]: loss 0.393111
[epoch15, step1475]: loss 0.492171
[epoch15, step1476]: loss 0.358252
[epoch15, step1477]: loss 0.504710
[epoch15, step1478]: loss 0.763337
[epoch15, step1479]: loss 0.722325
[epoch15, step1480]: loss 0.608306
[epoch15, step1481]: loss 0.490345
[epoch15, step1482]: loss 0.590201
[epoch15, step1483]: loss 0.588468
[epoch15, step1484]: loss 0.705582
[epoch15, step1485]: loss 0.476473
[epoch15, step1486]: loss 0.495612
[epoch15, step1487]: loss 0.518346
[epoch15, step1488]: loss 0.624824
[epoch15, step1489]: loss 0.469648
[epoch15, step1490]: loss 0.580494
[epoch15, step1491]: loss 0.620856
[epoch15, step1492]: loss 0.764190
[epoch15, step1493]: loss 0.452056
[epoch15, step1494]: loss 0.510977
[epoch15, step1495]: loss 0.509450
[epoch15, step1496]: loss 0.124408
[epoch15, step1497]: loss 0.826861
[epoch15, step1498]: loss 0.437327
[epoch15, step1499]: loss 0.559682
[epoch15, step1500]: loss 0.384098
[epoch15, step1501]: loss 0.431615
[epoch15, step1502]: loss 0.412920
[epoch15, step1503]: loss 0.505226
[epoch15, step1504]: loss 0.666337
[epoch15, step1505]: loss 0.417797
[epoch15, step1506]: loss 0.530359
[epoch15, step1507]: loss 0.611436
[epoch15, step1508]: loss 0.402618
[epoch15, step1509]: loss 0.371372
[epoch15, step1510]: loss 0.663524
[epoch15, step1511]: loss 0.262528
[epoch15, step1512]: loss 0.640420
[epoch15, step1513]: loss 0.488311
[epoch15, step1514]: loss 0.727066
[epoch15, step1515]: loss 0.400200
[epoch15, step1516]: loss 0.303569
[epoch15, step1517]: loss 0.774730
[epoch15, step1518]: loss 0.608374
[epoch15, step1519]: loss 0.425484
[epoch15, step1520]: loss 0.472776
[epoch15, step1521]: loss 0.512647
[epoch15, step1522]: loss 0.523831
[epoch15, step1523]: loss 0.628662
[epoch15, step1524]: loss 0.794546
[epoch15, step1525]: loss 0.569969
[epoch15, step1526]: loss 0.466495
[epoch15, step1527]: loss 0.583904
[epoch15, step1528]: loss 0.509466
[epoch15, step1529]: loss 0.621652
[epoch15, step1530]: loss 0.798742
[epoch15, step1531]: loss 0.699717
[epoch15, step1532]: loss 0.684773
[epoch15, step1533]: loss 0.657103
[epoch15, step1534]: loss 0.228121
[epoch15, step1535]: loss 0.535240
[epoch15, step1536]: loss 0.439015
[epoch15, step1537]: loss 0.638135
[epoch15, step1538]: loss 0.599272
[epoch15, step1539]: loss 0.552821
[epoch15, step1540]: loss 0.596579
[epoch15, step1541]: loss 0.372176
[epoch15, step1542]: loss 0.495587
[epoch15, step1543]: loss 0.517763
[epoch15, step1544]: loss 0.655072
[epoch15, step1545]: loss 0.427153
[epoch15, step1546]: loss 0.710333
[epoch15, step1547]: loss 0.407551
[epoch15, step1548]: loss 0.525831
[epoch15, step1549]: loss 0.415925
[epoch15, step1550]: loss 0.238302
[epoch15, step1551]: loss 0.610543
[epoch15, step1552]: loss 0.386069
[epoch15, step1553]: loss 0.691701
[epoch15, step1554]: loss 0.640960
[epoch15, step1555]: loss 0.385244
[epoch15, step1556]: loss 0.705227
[epoch15, step1557]: loss 0.563600
[epoch15, step1558]: loss 0.348668
[epoch15, step1559]: loss 0.630306
[epoch15, step1560]: loss 0.663802
[epoch15, step1561]: loss 0.579011
[epoch15, step1562]: loss 0.704307
[epoch15, step1563]: loss 0.335292
[epoch15, step1564]: loss 0.567994
[epoch15, step1565]: loss 0.516067
[epoch15, step1566]: loss 0.455054
[epoch15, step1567]: loss 0.386431
[epoch15, step1568]: loss 0.734003
[epoch15, step1569]: loss 0.347004
[epoch15, step1570]: loss 0.595895
[epoch15, step1571]: loss 0.638993
[epoch15, step1572]: loss 0.496992
[epoch15, step1573]: loss 0.604817
[epoch15, step1574]: loss 0.664390
[epoch15, step1575]: loss 0.275733
[epoch15, step1576]: loss 0.746965
[epoch15, step1577]: loss 0.656741
[epoch15, step1578]: loss 0.619102
[epoch15, step1579]: loss 0.656433
[epoch15, step1580]: loss 0.645030
[epoch15, step1581]: loss 0.677675
[epoch15, step1582]: loss 0.433693
[epoch15, step1583]: loss 0.461585
[epoch15, step1584]: loss 0.767258
[epoch15, step1585]: loss 0.202719
[epoch15, step1586]: loss 0.396929
[epoch15, step1587]: loss 0.440178
[epoch15, step1588]: loss 0.541338
[epoch15, step1589]: loss 0.367311
[epoch15, step1590]: loss 0.611667
[epoch15, step1591]: loss 0.402099
[epoch15, step1592]: loss 0.641256
[epoch15, step1593]: loss 0.578951
[epoch15, step1594]: loss 0.531506
[epoch15, step1595]: loss 0.554128
[epoch15, step1596]: loss 0.547694
[epoch15, step1597]: loss 0.478489
[epoch15, step1598]: loss 0.617168
[epoch15, step1599]: loss 0.533494
[epoch15, step1600]: loss 0.443474
[epoch15, step1601]: loss 0.358595
[epoch15, step1602]: loss 0.444411
[epoch15, step1603]: loss 0.531327
[epoch15, step1604]: loss 0.485779
[epoch15, step1605]: loss 0.523254
[epoch15, step1606]: loss 0.413463
[epoch15, step1607]: loss 0.488763
[epoch15, step1608]: loss 0.676643
[epoch15, step1609]: loss 0.672204
[epoch15, step1610]: loss 0.733500
[epoch15, step1611]: loss 0.493534
[epoch15, step1612]: loss 0.511816
[epoch15, step1613]: loss 0.395454
[epoch15, step1614]: loss 0.580097
[epoch15, step1615]: loss 0.473042
[epoch15, step1616]: loss 0.439716
[epoch15, step1617]: loss 0.564834
[epoch15, step1618]: loss 0.530084
[epoch15, step1619]: loss 0.574320
[epoch15, step1620]: loss 0.604869
[epoch15, step1621]: loss 0.616115
[epoch15, step1622]: loss 0.400257
[epoch15, step1623]: loss 0.673120
[epoch15, step1624]: loss 0.656097
[epoch15, step1625]: loss 0.712008
[epoch15, step1626]: loss 0.680743
[epoch15, step1627]: loss 0.599402
[epoch15, step1628]: loss 0.529411
[epoch15, step1629]: loss 0.606909
[epoch15, step1630]: loss 0.524967
[epoch15, step1631]: loss 0.640461
[epoch15, step1632]: loss 0.635920
[epoch15, step1633]: loss 0.361007
[epoch15, step1634]: loss 0.781170
[epoch15, step1635]: loss 0.528262
[epoch15, step1636]: loss 0.529278
[epoch15, step1637]: loss 0.746436
[epoch15, step1638]: loss 0.477210
[epoch15, step1639]: loss 0.683024
[epoch15, step1640]: loss 0.593777
[epoch15, step1641]: loss 0.664874
[epoch15, step1642]: loss 0.415578
[epoch15, step1643]: loss 0.430446
[epoch15, step1644]: loss 0.460391
[epoch15, step1645]: loss 0.598745
[epoch15, step1646]: loss 0.753664
[epoch15, step1647]: loss 0.719953
[epoch15, step1648]: loss 0.612187
[epoch15, step1649]: loss 0.283220
[epoch15, step1650]: loss 0.333363
[epoch15, step1651]: loss 0.591704
[epoch15, step1652]: loss 0.387140
[epoch15, step1653]: loss 0.436312
[epoch15, step1654]: loss 0.638399
[epoch15, step1655]: loss 0.632481
[epoch15, step1656]: loss 0.671398
[epoch15, step1657]: loss 0.557345
[epoch15, step1658]: loss 0.664906
[epoch15, step1659]: loss 0.480825
[epoch15, step1660]: loss 0.628975
[epoch15, step1661]: loss 0.699900
[epoch15, step1662]: loss 0.571186
[epoch15, step1663]: loss 0.653402
[epoch15, step1664]: loss 0.517380
[epoch15, step1665]: loss 0.712673
[epoch15, step1666]: loss 0.611613
[epoch15, step1667]: loss 0.502381
[epoch15, step1668]: loss 0.488992
[epoch15, step1669]: loss 0.619046
[epoch15, step1670]: loss 0.368957
[epoch15, step1671]: loss 0.475470
[epoch15, step1672]: loss 0.349917
[epoch15, step1673]: loss 0.396066
[epoch15, step1674]: loss 0.430585
[epoch15, step1675]: loss 0.735595
[epoch15, step1676]: loss 0.574953
[epoch15, step1677]: loss 0.445226
[epoch15, step1678]: loss 0.555286
[epoch15, step1679]: loss 0.551790
[epoch15, step1680]: loss 0.334367
[epoch15, step1681]: loss 0.598579
[epoch15, step1682]: loss 0.455037
[epoch15, step1683]: loss 0.563711
[epoch15, step1684]: loss 0.714352
[epoch15, step1685]: loss 0.430322
[epoch15, step1686]: loss 0.524905
[epoch15, step1687]: loss 0.657459
[epoch15, step1688]: loss 0.402042
[epoch15, step1689]: loss 0.530596
[epoch15, step1690]: loss 0.570384
[epoch15, step1691]: loss 0.477784
[epoch15, step1692]: loss 0.408307
[epoch15, step1693]: loss 0.675357
[epoch15, step1694]: loss 0.505518
[epoch15, step1695]: loss 0.419559
[epoch15, step1696]: loss 0.603631
[epoch15, step1697]: loss 0.580641
[epoch15, step1698]: loss 0.331405
[epoch15, step1699]: loss 0.428801
[epoch15, step1700]: loss 0.590595
[epoch15, step1701]: loss 0.736924
[epoch15, step1702]: loss 0.659984
[epoch15, step1703]: loss 0.560185
[epoch15, step1704]: loss 0.620416
[epoch15, step1705]: loss 0.670642
[epoch15, step1706]: loss 0.453300
[epoch15, step1707]: loss 0.406884
[epoch15, step1708]: loss 0.445464
[epoch15, step1709]: loss 0.547303
[epoch15, step1710]: loss 0.573857
[epoch15, step1711]: loss 0.330086
[epoch15, step1712]: loss 0.389273
[epoch15, step1713]: loss 0.546296
[epoch15, step1714]: loss 0.593745
[epoch15, step1715]: loss 0.585664
[epoch15, step1716]: loss 0.565753
[epoch15, step1717]: loss 0.458532
[epoch15, step1718]: loss 0.365189
[epoch15, step1719]: loss 0.723590
[epoch15, step1720]: loss 0.399491
[epoch15, step1721]: loss 0.378158
[epoch15, step1722]: loss 0.500199
[epoch15, step1723]: loss 0.441976
[epoch15, step1724]: loss 0.491875
[epoch15, step1725]: loss 0.482619
[epoch15, step1726]: loss 0.717394
[epoch15, step1727]: loss 0.419246
[epoch15, step1728]: loss 0.486915
[epoch15, step1729]: loss 0.749290
[epoch15, step1730]: loss 0.565256
[epoch15, step1731]: loss 0.524633
[epoch15, step1732]: loss 0.506401
[epoch15, step1733]: loss 0.521091
[epoch15, step1734]: loss 0.474899
[epoch15, step1735]: loss 0.501779
[epoch15, step1736]: loss 0.623688
[epoch15, step1737]: loss 0.437703
[epoch15, step1738]: loss 0.354072
[epoch15, step1739]: loss 0.554367
[epoch15, step1740]: loss 0.515839
[epoch15, step1741]: loss 0.506465
[epoch15, step1742]: loss 0.445918
[epoch15, step1743]: loss 0.429554
[epoch15, step1744]: loss 0.321864
[epoch15, step1745]: loss 0.398212
[epoch15, step1746]: loss 0.427254
[epoch15, step1747]: loss 0.359128
[epoch15, step1748]: loss 0.567991
[epoch15, step1749]: loss 0.389063
[epoch15, step1750]: loss 0.512149
[epoch15, step1751]: loss 0.626691
[epoch15, step1752]: loss 0.354677
[epoch15, step1753]: loss 0.569716
[epoch15, step1754]: loss 0.566295
[epoch15, step1755]: loss 0.737876
[epoch15, step1756]: loss 0.589850
[epoch15, step1757]: loss 0.403330
[epoch15, step1758]: loss 0.720990
[epoch15, step1759]: loss 0.544535
[epoch15, step1760]: loss 0.509178
[epoch15, step1761]: loss 0.733797
[epoch15, step1762]: loss 0.755548
[epoch15, step1763]: loss 0.859881
[epoch15, step1764]: loss 0.542088
[epoch15, step1765]: loss 0.472041
[epoch15, step1766]: loss 0.427659
[epoch15, step1767]: loss 0.378758
[epoch15, step1768]: loss 0.559309
[epoch15, step1769]: loss 0.598378
[epoch15, step1770]: loss 0.712066
[epoch15, step1771]: loss 0.333202
[epoch15, step1772]: loss 0.424977
[epoch15, step1773]: loss 0.676226
[epoch15, step1774]: loss 0.525781
[epoch15, step1775]: loss 0.699675
[epoch15, step1776]: loss 0.362786
[epoch15, step1777]: loss 0.698487
[epoch15, step1778]: loss 0.902785
[epoch15, step1779]: loss 0.671175
[epoch15, step1780]: loss 0.747137
[epoch15, step1781]: loss 0.451692
[epoch15, step1782]: loss 0.387202
[epoch15, step1783]: loss 0.553685
[epoch15, step1784]: loss 0.650705
[epoch15, step1785]: loss 0.383154
[epoch15, step1786]: loss 0.766423
[epoch15, step1787]: loss 0.632168
[epoch15, step1788]: loss 0.665060
[epoch15, step1789]: loss 0.623766
[epoch15, step1790]: loss 0.456567
[epoch15, step1791]: loss 0.579581
[epoch15, step1792]: loss 0.601335
[epoch15, step1793]: loss 0.677731
[epoch15, step1794]: loss 0.683361
[epoch15, step1795]: loss 0.684943
[epoch15, step1796]: loss 0.318997
[epoch15, step1797]: loss 0.682058
[epoch15, step1798]: loss 0.578766
[epoch15, step1799]: loss 0.579511
[epoch15, step1800]: loss 0.793057
[epoch15, step1801]: loss 0.704391
[epoch15, step1802]: loss 0.374358
[epoch15, step1803]: loss 0.675006
[epoch15, step1804]: loss 0.425260
[epoch15, step1805]: loss 0.603811
[epoch15, step1806]: loss 0.584134
[epoch15, step1807]: loss 0.614864
[epoch15, step1808]: loss 0.600441
[epoch15, step1809]: loss 0.457742
[epoch15, step1810]: loss 0.166903
[epoch15, step1811]: loss 0.577947
[epoch15, step1812]: loss 0.424970
[epoch15, step1813]: loss 0.437960
[epoch15, step1814]: loss 0.658846
[epoch15, step1815]: loss 0.590522
[epoch15, step1816]: loss 0.196399
[epoch15, step1817]: loss 0.519667
[epoch15, step1818]: loss 0.376817
[epoch15, step1819]: loss 0.306990
[epoch15, step1820]: loss 0.408082
[epoch15, step1821]: loss 0.628413
[epoch15, step1822]: loss 0.586698
[epoch15, step1823]: loss 0.206645
[epoch15, step1824]: loss 0.722658
[epoch15, step1825]: loss 0.492485
[epoch15, step1826]: loss 0.551841
[epoch15, step1827]: loss 0.537242
[epoch15, step1828]: loss 0.672126
[epoch15, step1829]: loss 0.527013
[epoch15, step1830]: loss 0.638394
[epoch15, step1831]: loss 0.437004
[epoch15, step1832]: loss 0.772434
[epoch15, step1833]: loss 0.566324
[epoch15, step1834]: loss 0.506053
[epoch15, step1835]: loss 0.356580
[epoch15, step1836]: loss 0.504886
[epoch15, step1837]: loss 0.709979
[epoch15, step1838]: loss 0.568344
[epoch15, step1839]: loss 0.780066
[epoch15, step1840]: loss 0.594668
[epoch15, step1841]: loss 0.563787
[epoch15, step1842]: loss 0.365380
[epoch15, step1843]: loss 0.635693
[epoch15, step1844]: loss 0.820015
[epoch15, step1845]: loss 0.442855
[epoch15, step1846]: loss 0.502278
[epoch15, step1847]: loss 0.698228
[epoch15, step1848]: loss 0.330729
[epoch15, step1849]: loss 0.358727
[epoch15, step1850]: loss 0.512430
[epoch15, step1851]: loss 0.668442
[epoch15, step1852]: loss 0.508210
[epoch15, step1853]: loss 0.636757
[epoch15, step1854]: loss 0.288998
[epoch15, step1855]: loss 0.523140
[epoch15, step1856]: loss 0.648292
[epoch15, step1857]: loss 0.477753
[epoch15, step1858]: loss 0.466337
[epoch15, step1859]: loss 0.633397
[epoch15, step1860]: loss 0.386626
[epoch15, step1861]: loss 0.421557
[epoch15, step1862]: loss 0.705256
[epoch15, step1863]: loss 0.533473
[epoch15, step1864]: loss 0.207908
[epoch15, step1865]: loss 0.503095
[epoch15, step1866]: loss 0.415320
[epoch15, step1867]: loss 0.635534
[epoch15, step1868]: loss 0.675537
[epoch15, step1869]: loss 0.510585
[epoch15, step1870]: loss 0.546959
[epoch15, step1871]: loss 0.697242
[epoch15, step1872]: loss 0.319615
[epoch15, step1873]: loss 0.408761
[epoch15, step1874]: loss 0.484947
[epoch15, step1875]: loss 0.569825
[epoch15, step1876]: loss 0.372436
[epoch15, step1877]: loss 0.689447
[epoch15, step1878]: loss 0.713784
[epoch15, step1879]: loss 0.719581
[epoch15, step1880]: loss 0.608565
[epoch15, step1881]: loss 0.563468
[epoch15, step1882]: loss 0.486778
[epoch15, step1883]: loss 0.407755
[epoch15, step1884]: loss 0.414322
[epoch15, step1885]: loss 0.202243
[epoch15, step1886]: loss 0.578748
[epoch15, step1887]: loss 0.778091
[epoch15, step1888]: loss 0.715490
[epoch15, step1889]: loss 0.542684
[epoch15, step1890]: loss 0.643184
[epoch15, step1891]: loss 0.601367
[epoch15, step1892]: loss 0.602893
[epoch15, step1893]: loss 0.606452
[epoch15, step1894]: loss 0.502660
[epoch15, step1895]: loss 0.539613
[epoch15, step1896]: loss 0.392049
[epoch15, step1897]: loss 0.558557
[epoch15, step1898]: loss 0.533054
[epoch15, step1899]: loss 0.510226
[epoch15, step1900]: loss 0.489168
[epoch15, step1901]: loss 0.217856
[epoch15, step1902]: loss 0.595467
[epoch15, step1903]: loss 0.442685
[epoch15, step1904]: loss 0.521055
[epoch15, step1905]: loss 0.553809
[epoch15, step1906]: loss 0.583606
[epoch15, step1907]: loss 0.590824
[epoch15, step1908]: loss 0.633140
[epoch15, step1909]: loss 0.589365
[epoch15, step1910]: loss 0.641584
[epoch15, step1911]: loss 0.655980
[epoch15, step1912]: loss 0.241120
[epoch15, step1913]: loss 0.654430
[epoch15, step1914]: loss 0.289356
[epoch15, step1915]: loss 0.693806
[epoch15, step1916]: loss 0.750364
[epoch15, step1917]: loss 0.472025
[epoch15, step1918]: loss 0.586671
[epoch15, step1919]: loss 0.399759
[epoch15, step1920]: loss 0.586695
[epoch15, step1921]: loss 0.483414
[epoch15, step1922]: loss 0.469840
[epoch15, step1923]: loss 0.472572
[epoch15, step1924]: loss 0.502381
[epoch15, step1925]: loss 0.444708
[epoch15, step1926]: loss 0.428614
[epoch15, step1927]: loss 0.264542
[epoch15, step1928]: loss 0.574896
[epoch15, step1929]: loss 0.677366
[epoch15, step1930]: loss 0.370205
[epoch15, step1931]: loss 0.475994
[epoch15, step1932]: loss 0.422015
[epoch15, step1933]: loss 0.652333
[epoch15, step1934]: loss 0.784135
[epoch15, step1935]: loss 0.467155
[epoch15, step1936]: loss 0.689928
[epoch15, step1937]: loss 0.739389
[epoch15, step1938]: loss 0.353022
[epoch15, step1939]: loss 0.410596
[epoch15, step1940]: loss 0.675415
[epoch15, step1941]: loss 0.616824
[epoch15, step1942]: loss 0.738640
[epoch15, step1943]: loss 0.535884
[epoch15, step1944]: loss 0.381710
[epoch15, step1945]: loss 0.555655
[epoch15, step1946]: loss 0.653231
[epoch15, step1947]: loss 0.408397
[epoch15, step1948]: loss 0.411990
[epoch15, step1949]: loss 0.430695
[epoch15, step1950]: loss 0.636093
[epoch15, step1951]: loss 0.331882
[epoch15, step1952]: loss 0.358138
[epoch15, step1953]: loss 0.589190
[epoch15, step1954]: loss 0.533598
[epoch15, step1955]: loss 0.636660
[epoch15, step1956]: loss 0.499441
[epoch15, step1957]: loss 0.235125
[epoch15, step1958]: loss 0.648178
[epoch15, step1959]: loss 0.598375
[epoch15, step1960]: loss 0.632696
[epoch15, step1961]: loss 0.344774
[epoch15, step1962]: loss 0.641064
[epoch15, step1963]: loss 0.824515
[epoch15, step1964]: loss 0.588436
[epoch15, step1965]: loss 0.623249
[epoch15, step1966]: loss 0.732551
[epoch15, step1967]: loss 0.666802
[epoch15, step1968]: loss 0.324975
[epoch15, step1969]: loss 0.712787
[epoch15, step1970]: loss 0.420395
[epoch15, step1971]: loss 0.213605
[epoch15, step1972]: loss 0.507848
[epoch15, step1973]: loss 0.465290
[epoch15, step1974]: loss 0.546839
[epoch15, step1975]: loss 0.713750
[epoch15, step1976]: loss 0.506175
[epoch15, step1977]: loss 0.731445
[epoch15, step1978]: loss 0.591387
[epoch15, step1979]: loss 0.699773
[epoch15, step1980]: loss 0.552922
[epoch15, step1981]: loss 0.622412
[epoch15, step1982]: loss 0.612404
[epoch15, step1983]: loss 0.627829
[epoch15, step1984]: loss 0.579615
[epoch15, step1985]: loss 0.638001
[epoch15, step1986]: loss 0.809557
[epoch15, step1987]: loss 0.586141
[epoch15, step1988]: loss 0.520107
[epoch15, step1989]: loss 0.602911
[epoch15, step1990]: loss 0.631822
[epoch15, step1991]: loss 0.657569
[epoch15, step1992]: loss 0.670585
[epoch15, step1993]: loss 0.439523
[epoch15, step1994]: loss 0.605257
[epoch15, step1995]: loss 0.716595
[epoch15, step1996]: loss 0.684532
[epoch15, step1997]: loss 0.501632
[epoch15, step1998]: loss 0.625344
[epoch15, step1999]: loss 0.535219
[epoch15, step2000]: loss 0.362372
[epoch15, step2001]: loss 0.371388
[epoch15, step2002]: loss 0.658504
[epoch15, step2003]: loss 0.392148
[epoch15, step2004]: loss 0.615418
[epoch15, step2005]: loss 0.502879
[epoch15, step2006]: loss 0.499784
[epoch15, step2007]: loss 0.609642
[epoch15, step2008]: loss 0.452292
[epoch15, step2009]: loss 0.544444
[epoch15, step2010]: loss 0.345445
[epoch15, step2011]: loss 0.485298
[epoch15, step2012]: loss 0.510088
[epoch15, step2013]: loss 0.551509
[epoch15, step2014]: loss 0.432718
[epoch15, step2015]: loss 0.555269
[epoch15, step2016]: loss 0.740143
[epoch15, step2017]: loss 0.485149
[epoch15, step2018]: loss 0.420514
[epoch15, step2019]: loss 0.555183
[epoch15, step2020]: loss 0.437061
[epoch15, step2021]: loss 0.664515
[epoch15, step2022]: loss 0.509034
[epoch15, step2023]: loss 0.469263
[epoch15, step2024]: loss 0.575119
[epoch15, step2025]: loss 0.495988
[epoch15, step2026]: loss 0.183478
[epoch15, step2027]: loss 0.521009
[epoch15, step2028]: loss 0.472487
[epoch15, step2029]: loss 0.466037
[epoch15, step2030]: loss 0.307507
[epoch15, step2031]: loss 0.466622
[epoch15, step2032]: loss 0.380433
[epoch15, step2033]: loss 0.481969
[epoch15, step2034]: loss 0.351003
[epoch15, step2035]: loss 0.677207
[epoch15, step2036]: loss 0.372993
[epoch15, step2037]: loss 0.488982
[epoch15, step2038]: loss 0.754427
[epoch15, step2039]: loss 0.502000
[epoch15, step2040]: loss 0.480997
[epoch15, step2041]: loss 0.512461
[epoch15, step2042]: loss 0.706931
[epoch15, step2043]: loss 0.360474
[epoch15, step2044]: loss 0.447390
[epoch15, step2045]: loss 0.382144
[epoch15, step2046]: loss 0.382236
[epoch15, step2047]: loss 0.494018
[epoch15, step2048]: loss 0.390741
[epoch15, step2049]: loss 0.695552
[epoch15, step2050]: loss 0.559289
[epoch15, step2051]: loss 0.600241
[epoch15, step2052]: loss 0.673254
[epoch15, step2053]: loss 0.695204
[epoch15, step2054]: loss 0.567502
[epoch15, step2055]: loss 0.537448
[epoch15, step2056]: loss 0.488882
[epoch15, step2057]: loss 0.792882
[epoch15, step2058]: loss 0.640223
[epoch15, step2059]: loss 0.734714
[epoch15, step2060]: loss 0.498586
[epoch15, step2061]: loss 0.325439
[epoch15, step2062]: loss 0.651666
[epoch15, step2063]: loss 0.386073
[epoch15, step2064]: loss 0.560042
[epoch15, step2065]: loss 0.546676
[epoch15, step2066]: loss 0.306104
[epoch15, step2067]: loss 0.695875
[epoch15, step2068]: loss 0.499613
[epoch15, step2069]: loss 0.870948
[epoch15, step2070]: loss 0.790078
[epoch15, step2071]: loss 0.511104
[epoch15, step2072]: loss 0.756646
[epoch15, step2073]: loss 0.474191
[epoch15, step2074]: loss 0.702448
[epoch15, step2075]: loss 0.509172
[epoch15, step2076]: loss 0.708158
[epoch15, step2077]: loss 0.532232
[epoch15, step2078]: loss 0.593975
[epoch15, step2079]: loss 0.485890
[epoch15, step2080]: loss 0.622209
[epoch15, step2081]: loss 0.518122
[epoch15, step2082]: loss 0.691886
[epoch15, step2083]: loss 0.533603
[epoch15, step2084]: loss 0.504976
[epoch15, step2085]: loss 0.552021
[epoch15, step2086]: loss 0.729287
[epoch15, step2087]: loss 0.627695
[epoch15, step2088]: loss 0.756182
[epoch15, step2089]: loss 0.442338
[epoch15, step2090]: loss 0.382602
[epoch15, step2091]: loss 0.533558
[epoch15, step2092]: loss 0.392853
[epoch15, step2093]: loss 0.426487
[epoch15, step2094]: loss 0.394426
[epoch15, step2095]: loss 0.364164
[epoch15, step2096]: loss 0.570859
[epoch15, step2097]: loss 0.630475
[epoch15, step2098]: loss 0.248596
[epoch15, step2099]: loss 0.731122
[epoch15, step2100]: loss 0.556424
[epoch15, step2101]: loss 0.513220
[epoch15, step2102]: loss 0.625595
[epoch15, step2103]: loss 0.669067
[epoch15, step2104]: loss 0.337764
[epoch15, step2105]: loss 0.645425
[epoch15, step2106]: loss 0.588460
[epoch15, step2107]: loss 0.533012
[epoch15, step2108]: loss 0.623261
[epoch15, step2109]: loss 0.409973
[epoch15, step2110]: loss 0.579403
[epoch15, step2111]: loss 0.779349
[epoch15, step2112]: loss 0.716923
[epoch15, step2113]: loss 0.491856
[epoch15, step2114]: loss 0.687554
[epoch15, step2115]: loss 0.603734
[epoch15, step2116]: loss 0.487291
[epoch15, step2117]: loss 0.577260
[epoch15, step2118]: loss 0.459888
[epoch15, step2119]: loss 0.294229
[epoch15, step2120]: loss 0.336483
[epoch15, step2121]: loss 0.620619
[epoch15, step2122]: loss 0.543032
[epoch15, step2123]: loss 0.480892
[epoch15, step2124]: loss 0.691019
[epoch15, step2125]: loss 0.575406
[epoch15, step2126]: loss 0.391396
[epoch15, step2127]: loss 0.683186
[epoch15, step2128]: loss 0.523880
[epoch15, step2129]: loss 0.634510
[epoch15, step2130]: loss 0.535228
[epoch15, step2131]: loss 0.374067
[epoch15, step2132]: loss 0.371333
[epoch15, step2133]: loss 0.264363
[epoch15, step2134]: loss 0.565731
[epoch15, step2135]: loss 0.534471
[epoch15, step2136]: loss 0.689318
[epoch15, step2137]: loss 0.464474
[epoch15, step2138]: loss 0.467445
[epoch15, step2139]: loss 0.534063
[epoch15, step2140]: loss 0.518014
[epoch15, step2141]: loss 0.656627
[epoch15, step2142]: loss 0.297585
[epoch15, step2143]: loss 0.294262
[epoch15, step2144]: loss 0.642327
[epoch15, step2145]: loss 0.563380
[epoch15, step2146]: loss 0.504932
[epoch15, step2147]: loss 0.489003
[epoch15, step2148]: loss 0.591640
[epoch15, step2149]: loss 0.285224
[epoch15, step2150]: loss 0.629084
[epoch15, step2151]: loss 0.346680
[epoch15, step2152]: loss 0.631870
[epoch15, step2153]: loss 0.494928
[epoch15, step2154]: loss 0.768381
[epoch15, step2155]: loss 0.400200
[epoch15, step2156]: loss 0.406416
[epoch15, step2157]: loss 0.527713
[epoch15, step2158]: loss 0.611064
[epoch15, step2159]: loss 0.770388
[epoch15, step2160]: loss 0.648140
[epoch15, step2161]: loss 0.541719
[epoch15, step2162]: loss 0.454557
[epoch15, step2163]: loss 0.718168
[epoch15, step2164]: loss 0.713096
[epoch15, step2165]: loss 0.479288
[epoch15, step2166]: loss 0.863933
[epoch15, step2167]: loss 0.420267
[epoch15, step2168]: loss 0.596095
[epoch15, step2169]: loss 0.498857
[epoch15, step2170]: loss 0.441994
[epoch15, step2171]: loss 0.556372
[epoch15, step2172]: loss 0.291279
[epoch15, step2173]: loss 0.569304
[epoch15, step2174]: loss 0.314081
[epoch15, step2175]: loss 0.091621
[epoch15, step2176]: loss 0.408072
[epoch15, step2177]: loss 0.548664
[epoch15, step2178]: loss 0.540590
[epoch15, step2179]: loss 0.471201
[epoch15, step2180]: loss 0.574439
[epoch15, step2181]: loss 0.592295
[epoch15, step2182]: loss 0.618744
[epoch15, step2183]: loss 0.416051
[epoch15, step2184]: loss 0.579274
[epoch15, step2185]: loss 0.507719
[epoch15, step2186]: loss 0.768196
[epoch15, step2187]: loss 0.547216
[epoch15, step2188]: loss 0.644483
[epoch15, step2189]: loss 0.415867
[epoch15, step2190]: loss 0.546167
[epoch15, step2191]: loss 0.758546
[epoch15, step2192]: loss 0.599806
[epoch15, step2193]: loss 0.198610
[epoch15, step2194]: loss 0.661543
[epoch15, step2195]: loss 0.601633
[epoch15, step2196]: loss 0.390832
[epoch15, step2197]: loss 0.554611
[epoch15, step2198]: loss 0.405910
[epoch15, step2199]: loss 0.699892
[epoch15, step2200]: loss 0.704477
[epoch15, step2201]: loss 0.616567
[epoch15, step2202]: loss 0.580513
[epoch15, step2203]: loss 0.397414
[epoch15, step2204]: loss 0.557255
[epoch15, step2205]: loss 0.647528
[epoch15, step2206]: loss 0.420665
[epoch15, step2207]: loss 0.577415
[epoch15, step2208]: loss 0.579730
[epoch15, step2209]: loss 0.562566
[epoch15, step2210]: loss 0.393844
[epoch15, step2211]: loss 0.391113
[epoch15, step2212]: loss 0.476497
[epoch15, step2213]: loss 0.743684
[epoch15, step2214]: loss 0.608247
[epoch15, step2215]: loss 0.609549
[epoch15, step2216]: loss 0.490424
[epoch15, step2217]: loss 0.406911
[epoch15, step2218]: loss 0.511847
[epoch15, step2219]: loss 0.165739
[epoch15, step2220]: loss 0.590846
[epoch15, step2221]: loss 0.488639
[epoch15, step2222]: loss 0.115796
[epoch15, step2223]: loss 0.854374
[epoch15, step2224]: loss 0.653386
[epoch15, step2225]: loss 0.668855
[epoch15, step2226]: loss 0.597947
[epoch15, step2227]: loss 0.619799
[epoch15, step2228]: loss 0.560247
[epoch15, step2229]: loss 0.567884
[epoch15, step2230]: loss 0.495905
[epoch15, step2231]: loss 0.550182
[epoch15, step2232]: loss 0.649789
[epoch15, step2233]: loss 0.560330
[epoch15, step2234]: loss 0.746093
[epoch15, step2235]: loss 0.613998
[epoch15, step2236]: loss 0.234829
[epoch15, step2237]: loss 0.756784
[epoch15, step2238]: loss 0.529701
[epoch15, step2239]: loss 0.431704
[epoch15, step2240]: loss 0.544505
[epoch15, step2241]: loss 0.660672
[epoch15, step2242]: loss 0.674162
[epoch15, step2243]: loss 0.599667
[epoch15, step2244]: loss 0.515936
[epoch15, step2245]: loss 0.610512
[epoch15, step2246]: loss 0.604698
[epoch15, step2247]: loss 0.486614
[epoch15, step2248]: loss 0.442251
[epoch15, step2249]: loss 0.502572
[epoch15, step2250]: loss 0.497283
[epoch15, step2251]: loss 0.628618
[epoch15, step2252]: loss 0.552559
[epoch15, step2253]: loss 0.654254
[epoch15, step2254]: loss 0.630581
[epoch15, step2255]: loss 0.730128
[epoch15, step2256]: loss 0.607556
[epoch15, step2257]: loss 0.351947
[epoch15, step2258]: loss 0.569638
[epoch15, step2259]: loss 0.602258
[epoch15, step2260]: loss 0.344682
[epoch15, step2261]: loss 0.737452
[epoch15, step2262]: loss 0.346360
[epoch15, step2263]: loss 0.196442
[epoch15, step2264]: loss 0.340938
[epoch15, step2265]: loss 0.406978
[epoch15, step2266]: loss 0.623447
[epoch15, step2267]: loss 0.694352
[epoch15, step2268]: loss 0.726508
[epoch15, step2269]: loss 0.696916
[epoch15, step2270]: loss 0.648327
[epoch15, step2271]: loss 0.745230
[epoch15, step2272]: loss 0.246486
[epoch15, step2273]: loss 0.569942
[epoch15, step2274]: loss 0.505521
[epoch15, step2275]: loss 0.591005
[epoch15, step2276]: loss 0.652550
[epoch15, step2277]: loss 0.580394
[epoch15, step2278]: loss 0.488121
[epoch15, step2279]: loss 0.339864
[epoch15, step2280]: loss 0.700549
[epoch15, step2281]: loss 0.291875
[epoch15, step2282]: loss 0.568910
[epoch15, step2283]: loss 0.448176
[epoch15, step2284]: loss 0.511347
[epoch15, step2285]: loss 0.656036
[epoch15, step2286]: loss 0.629468
[epoch15, step2287]: loss 0.313803
[epoch15, step2288]: loss 0.575761
[epoch15, step2289]: loss 0.352961
[epoch15, step2290]: loss 0.499643
[epoch15, step2291]: loss 0.452216
[epoch15, step2292]: loss 0.634462
[epoch15, step2293]: loss 0.430019
[epoch15, step2294]: loss 0.696985
[epoch15, step2295]: loss 0.785844
[epoch15, step2296]: loss 0.321019
[epoch15, step2297]: loss 0.517343
[epoch15, step2298]: loss 0.313283
[epoch15, step2299]: loss 0.632950
[epoch15, step2300]: loss 0.700888
[epoch15, step2301]: loss 0.468777
[epoch15, step2302]: loss 0.286067
[epoch15, step2303]: loss 0.593351
[epoch15, step2304]: loss 0.115261
[epoch15, step2305]: loss 0.331023
[epoch15, step2306]: loss 0.527609
[epoch15, step2307]: loss 0.660584
[epoch15, step2308]: loss 0.340041
[epoch15, step2309]: loss 0.383969
[epoch15, step2310]: loss 0.413477
[epoch15, step2311]: loss 0.484185
[epoch15, step2312]: loss 0.548238
[epoch15, step2313]: loss 0.408870
[epoch15, step2314]: loss 0.684975
[epoch15, step2315]: loss 0.600708
[epoch15, step2316]: loss 0.162763
[epoch15, step2317]: loss 0.497384
[epoch15, step2318]: loss 0.472862
[epoch15, step2319]: loss 0.426935
[epoch15, step2320]: loss 0.537467
[epoch15, step2321]: loss 0.405755
[epoch15, step2322]: loss 0.444053
[epoch15, step2323]: loss 0.607936
[epoch15, step2324]: loss 0.475076
[epoch15, step2325]: loss 0.667409
[epoch15, step2326]: loss 0.182662
[epoch15, step2327]: loss 0.560377
[epoch15, step2328]: loss 0.609782
[epoch15, step2329]: loss 0.868807
[epoch15, step2330]: loss 0.582827
[epoch15, step2331]: loss 0.716007
[epoch15, step2332]: loss 0.671599
[epoch15, step2333]: loss 0.455419
[epoch15, step2334]: loss 0.643603
[epoch15, step2335]: loss 0.319573
[epoch15, step2336]: loss 0.480496
[epoch15, step2337]: loss 0.446080
[epoch15, step2338]: loss 0.426567
[epoch15, step2339]: loss 0.304188
[epoch15, step2340]: loss 0.337580
[epoch15, step2341]: loss 0.772234
[epoch15, step2342]: loss 0.733476
[epoch15, step2343]: loss 0.603239
[epoch15, step2344]: loss 0.428450
[epoch15, step2345]: loss 0.424805
[epoch15, step2346]: loss 0.641909
[epoch15, step2347]: loss 0.640487
[epoch15, step2348]: loss 0.420577
[epoch15, step2349]: loss 0.457886
[epoch15, step2350]: loss 0.581144
[epoch15, step2351]: loss 0.710070
[epoch15, step2352]: loss 0.407697
[epoch15, step2353]: loss 0.744058
[epoch15, step2354]: loss 0.299028
[epoch15, step2355]: loss 0.413608
[epoch15, step2356]: loss 0.240197
[epoch15, step2357]: loss 0.551508
[epoch15, step2358]: loss 0.640631
[epoch15, step2359]: loss 0.700394
[epoch15, step2360]: loss 0.704089
[epoch15, step2361]: loss 0.738849
[epoch15, step2362]: loss 0.678068
[epoch15, step2363]: loss 0.413201
[epoch15, step2364]: loss 0.587439
[epoch15, step2365]: loss 0.477228
[epoch15, step2366]: loss 0.485274
[epoch15, step2367]: loss 0.606723
[epoch15, step2368]: loss 0.525649
[epoch15, step2369]: loss 0.568073
[epoch15, step2370]: loss 0.582641
[epoch15, step2371]: loss 0.487313
[epoch15, step2372]: loss 0.667621
[epoch15, step2373]: loss 0.544378
[epoch15, step2374]: loss 0.257073
[epoch15, step2375]: loss 0.438556
[epoch15, step2376]: loss 0.764290
[epoch15, step2377]: loss 0.666756
[epoch15, step2378]: loss 0.688201
[epoch15, step2379]: loss 0.697816
[epoch15, step2380]: loss 0.426462
[epoch15, step2381]: loss 0.411289
[epoch15, step2382]: loss 0.496598
[epoch15, step2383]: loss 0.673452
[epoch15, step2384]: loss 0.513554
[epoch15, step2385]: loss 0.606598
[epoch15, step2386]: loss 0.696861
[epoch15, step2387]: loss 0.327121
[epoch15, step2388]: loss 0.544448
[epoch15, step2389]: loss 0.484770
[epoch15, step2390]: loss 0.590012
[epoch15, step2391]: loss 0.441711
[epoch15, step2392]: loss 0.669292
[epoch15, step2393]: loss 0.677157
[epoch15, step2394]: loss 0.774449
[epoch15, step2395]: loss 0.465912
[epoch15, step2396]: loss 0.544144
[epoch15, step2397]: loss 0.420894
[epoch15, step2398]: loss 0.717973
[epoch15, step2399]: loss 0.595957
[epoch15, step2400]: loss 0.732312
[epoch15, step2401]: loss 0.628018
[epoch15, step2402]: loss 0.420675
[epoch15, step2403]: loss 0.659398
[epoch15, step2404]: loss 0.378048
[epoch15, step2405]: loss 0.465406
[epoch15, step2406]: loss 0.494278
[epoch15, step2407]: loss 0.654257
[epoch15, step2408]: loss 0.327441
[epoch15, step2409]: loss 0.716452
[epoch15, step2410]: loss 0.509013
[epoch15, step2411]: loss 0.482149
[epoch15, step2412]: loss 0.669717
[epoch15, step2413]: loss 0.536467
[epoch15, step2414]: loss 0.324638
[epoch15, step2415]: loss 0.558556
[epoch15, step2416]: loss 0.700470
[epoch15, step2417]: loss 0.534403
[epoch15, step2418]: loss 0.478839
[epoch15, step2419]: loss 0.546450
[epoch15, step2420]: loss 0.751542
[epoch15, step2421]: loss 0.635218
[epoch15, step2422]: loss 0.438003
[epoch15, step2423]: loss 0.580002
[epoch15, step2424]: loss 0.449234
[epoch15, step2425]: loss 0.366585
[epoch15, step2426]: loss 0.382371
[epoch15, step2427]: loss 0.551360
[epoch15, step2428]: loss 0.746426
[epoch15, step2429]: loss 0.496212
[epoch15, step2430]: loss 0.441193
[epoch15, step2431]: loss 0.794599
[epoch15, step2432]: loss 0.420211
[epoch15, step2433]: loss 0.652103
[epoch15, step2434]: loss 0.373466
[epoch15, step2435]: loss 0.501517
[epoch15, step2436]: loss 0.575129
[epoch15, step2437]: loss 0.730180
[epoch15, step2438]: loss 0.468613
[epoch15, step2439]: loss 0.608591
[epoch15, step2440]: loss 0.434978
[epoch15, step2441]: loss 0.702880
[epoch15, step2442]: loss 0.654116
[epoch15, step2443]: loss 0.456197
[epoch15, step2444]: loss 0.389203
[epoch15, step2445]: loss 0.563013
[epoch15, step2446]: loss 0.679820
[epoch15, step2447]: loss 0.869746
[epoch15, step2448]: loss 0.621794
[epoch15, step2449]: loss 0.632950
[epoch15, step2450]: loss 0.454957
[epoch15, step2451]: loss 0.295141
[epoch15, step2452]: loss 0.308783
[epoch15, step2453]: loss 0.271645
[epoch15, step2454]: loss 0.701118
[epoch15, step2455]: loss 0.507309
[epoch15, step2456]: loss 0.542684
[epoch15, step2457]: loss 0.552712
[epoch15, step2458]: loss 0.285906
[epoch15, step2459]: loss 0.473564
[epoch15, step2460]: loss 0.463775
[epoch15, step2461]: loss 0.621335
[epoch15, step2462]: loss 0.402940
[epoch15, step2463]: loss 0.783116
[epoch15, step2464]: loss 0.761469
[epoch15, step2465]: loss 0.390267
[epoch15, step2466]: loss 0.409929
[epoch15, step2467]: loss 0.687620
[epoch15, step2468]: loss 0.793050
[epoch15, step2469]: loss 0.423560
[epoch15, step2470]: loss 0.637218
[epoch15, step2471]: loss 0.764310
[epoch15, step2472]: loss 0.451465
[epoch15, step2473]: loss 0.313744
[epoch15, step2474]: loss 0.728565
[epoch15, step2475]: loss 0.517836
[epoch15, step2476]: loss 0.578376
[epoch15, step2477]: loss 0.587999
[epoch15, step2478]: loss 0.482335
[epoch15, step2479]: loss 0.768201
[epoch15, step2480]: loss 0.811765
[epoch15, step2481]: loss 0.540131
[epoch15, step2482]: loss 0.277960
[epoch15, step2483]: loss 0.495107
[epoch15, step2484]: loss 0.770415
[epoch15, step2485]: loss 0.548637
[epoch15, step2486]: loss 0.724026
[epoch15, step2487]: loss 0.826774
[epoch15, step2488]: loss 0.629566
[epoch15, step2489]: loss 0.621037
[epoch15, step2490]: loss 0.474795
[epoch15, step2491]: loss 0.557394
[epoch15, step2492]: loss 0.477834
[epoch15, step2493]: loss 0.556024
[epoch15, step2494]: loss 0.591068
[epoch15, step2495]: loss 0.539474
[epoch15, step2496]: loss 0.638633
[epoch15, step2497]: loss 0.480369
[epoch15, step2498]: loss 0.540536
[epoch15, step2499]: loss 0.636962
[epoch15, step2500]: loss 0.248364
[epoch15, step2501]: loss 0.742757
[epoch15, step2502]: loss 0.562652
[epoch15, step2503]: loss 0.491741
[epoch15, step2504]: loss 0.592496
[epoch15, step2505]: loss 0.551881
[epoch15, step2506]: loss 0.581211
[epoch15, step2507]: loss 0.340819
[epoch15, step2508]: loss 0.601094
[epoch15, step2509]: loss 0.592290
[epoch15, step2510]: loss 0.350980
[epoch15, step2511]: loss 0.576491
[epoch15, step2512]: loss 0.371164
[epoch15, step2513]: loss 0.602060
[epoch15, step2514]: loss 0.389789
[epoch15, step2515]: loss 0.654395
[epoch15, step2516]: loss 0.628444
[epoch15, step2517]: loss 0.462046
[epoch15, step2518]: loss 0.610772
[epoch15, step2519]: loss 0.469007
[epoch15, step2520]: loss 0.391071
[epoch15, step2521]: loss 0.323695
[epoch15, step2522]: loss 0.388968
[epoch15, step2523]: loss 0.639602
[epoch15, step2524]: loss 0.429978
[epoch15, step2525]: loss 0.839487
[epoch15, step2526]: loss 0.515601
[epoch15, step2527]: loss 0.685617
[epoch15, step2528]: loss 0.618108
[epoch15, step2529]: loss 0.576474
[epoch15, step2530]: loss 0.705761
[epoch15, step2531]: loss 0.656036
[epoch15, step2532]: loss 0.646516
[epoch15, step2533]: loss 0.416739
[epoch15, step2534]: loss 0.462511
[epoch15, step2535]: loss 0.739037
[epoch15, step2536]: loss 0.337852
[epoch15, step2537]: loss 0.432621
[epoch15, step2538]: loss 0.762516
[epoch15, step2539]: loss 0.655391
[epoch15, step2540]: loss 0.805679
[epoch15, step2541]: loss 0.619889
[epoch15, step2542]: loss 0.538194
[epoch15, step2543]: loss 0.553591
[epoch15, step2544]: loss 0.497489
[epoch15, step2545]: loss 0.791010
[epoch15, step2546]: loss 0.575187
[epoch15, step2547]: loss 0.561481
[epoch15, step2548]: loss 0.407299
[epoch15, step2549]: loss 0.433588
[epoch15, step2550]: loss 0.416320
[epoch15, step2551]: loss 0.696338
[epoch15, step2552]: loss 0.529545
[epoch15, step2553]: loss 0.586982
[epoch15, step2554]: loss 0.527130
[epoch15, step2555]: loss 0.323626
[epoch15, step2556]: loss 0.627488
[epoch15, step2557]: loss 0.402778
[epoch15, step2558]: loss 0.411905
[epoch15, step2559]: loss 0.530842
[epoch15, step2560]: loss 0.534930
[epoch15, step2561]: loss 0.246161
[epoch15, step2562]: loss 0.540287
[epoch15, step2563]: loss 0.703045
[epoch15, step2564]: loss 0.696905
[epoch15, step2565]: loss 0.784226
[epoch15, step2566]: loss 0.590808
[epoch15, step2567]: loss 0.572375
[epoch15, step2568]: loss 0.566982
[epoch15, step2569]: loss 0.664392
[epoch15, step2570]: loss 0.476260
[epoch15, step2571]: loss 0.619038
[epoch15, step2572]: loss 0.355366
[epoch15, step2573]: loss 0.267768
[epoch15, step2574]: loss 0.746211
[epoch15, step2575]: loss 0.508731
[epoch15, step2576]: loss 0.469683
[epoch15, step2577]: loss 0.400234
[epoch15, step2578]: loss 0.326441
[epoch15, step2579]: loss 0.424907
[epoch15, step2580]: loss 0.382948
[epoch15, step2581]: loss 0.602059
[epoch15, step2582]: loss 0.284289
[epoch15, step2583]: loss 0.546591
[epoch15, step2584]: loss 0.195693
[epoch15, step2585]: loss 0.552426
[epoch15, step2586]: loss 0.411490
[epoch15, step2587]: loss 0.560086
[epoch15, step2588]: loss 0.748904
[epoch15, step2589]: loss 0.839363
[epoch15, step2590]: loss 0.387477
[epoch15, step2591]: loss 0.566983
[epoch15, step2592]: loss 0.335285
[epoch15, step2593]: loss 0.384825
[epoch15, step2594]: loss 0.208936
[epoch15, step2595]: loss 0.378915
[epoch15, step2596]: loss 0.531851
[epoch15, step2597]: loss 0.521841
[epoch15, step2598]: loss 0.677351
[epoch15, step2599]: loss 0.445219
[epoch15, step2600]: loss 0.715350
[epoch15, step2601]: loss 0.620113
[epoch15, step2602]: loss 0.352725
[epoch15, step2603]: loss 0.534825
[epoch15, step2604]: loss 0.579797
[epoch15, step2605]: loss 0.511110
[epoch15, step2606]: loss 0.527383
[epoch15, step2607]: loss 0.224612
[epoch15, step2608]: loss 0.767346
[epoch15, step2609]: loss 0.422598
[epoch15, step2610]: loss 0.334934
[epoch15, step2611]: loss 0.500829
[epoch15, step2612]: loss 0.492210
[epoch15, step2613]: loss 0.394998
[epoch15, step2614]: loss 0.749076
[epoch15, step2615]: loss 0.593739
[epoch15, step2616]: loss 0.410628
[epoch15, step2617]: loss 0.276306
[epoch15, step2618]: loss 0.657780
[epoch15, step2619]: loss 0.423234
[epoch15, step2620]: loss 0.793205
[epoch15, step2621]: loss 0.543420
[epoch15, step2622]: loss 0.426351
[epoch15, step2623]: loss 0.546596
[epoch15, step2624]: loss 0.493157
[epoch15, step2625]: loss 0.720957
[epoch15, step2626]: loss 0.576240
[epoch15, step2627]: loss 0.760257
[epoch15, step2628]: loss 0.390548
[epoch15, step2629]: loss 0.643183
[epoch15, step2630]: loss 0.645147
[epoch15, step2631]: loss 0.686535
[epoch15, step2632]: loss 0.263139
[epoch15, step2633]: loss 0.580215
[epoch15, step2634]: loss 0.715678
[epoch15, step2635]: loss 0.714580
[epoch15, step2636]: loss 0.524533
[epoch15, step2637]: loss 0.550421
[epoch15, step2638]: loss 0.658038
[epoch15, step2639]: loss 0.474542
[epoch15, step2640]: loss 0.578795
[epoch15, step2641]: loss 0.226636
[epoch15, step2642]: loss 0.402004
[epoch15, step2643]: loss 0.664337
[epoch15, step2644]: loss 0.470396
[epoch15, step2645]: loss 0.773159
[epoch15, step2646]: loss 0.765382
[epoch15, step2647]: loss 0.671357
[epoch15, step2648]: loss 0.606494
[epoch15, step2649]: loss 0.801819
[epoch15, step2650]: loss 0.555483
[epoch15, step2651]: loss 0.565126
[epoch15, step2652]: loss 0.585719
[epoch15, step2653]: loss 0.582203
[epoch15, step2654]: loss 0.551296
[epoch15, step2655]: loss 0.631162
[epoch15, step2656]: loss 0.404353
[epoch15, step2657]: loss 0.397595
[epoch15, step2658]: loss 0.615382
[epoch15, step2659]: loss 0.385077
[epoch15, step2660]: loss 0.153581
[epoch15, step2661]: loss 0.674258
[epoch15, step2662]: loss 0.568597
[epoch15, step2663]: loss 0.527632
[epoch15, step2664]: loss 0.727175
[epoch15, step2665]: loss 0.377490
[epoch15, step2666]: loss 0.583176
[epoch15, step2667]: loss 0.646861
[epoch15, step2668]: loss 0.485277
[epoch15, step2669]: loss 0.595461
[epoch15, step2670]: loss 0.491561
[epoch15, step2671]: loss 0.529012
[epoch15, step2672]: loss 0.505079
[epoch15, step2673]: loss 0.550777
[epoch15, step2674]: loss 0.587286
[epoch15, step2675]: loss 0.613798
[epoch15, step2676]: loss 0.382941
[epoch15, step2677]: loss 0.421376
[epoch15, step2678]: loss 0.502746
[epoch15, step2679]: loss 0.334080
[epoch15, step2680]: loss 0.297415
[epoch15, step2681]: loss 0.772923
[epoch15, step2682]: loss 0.572861
[epoch15, step2683]: loss 0.561163
[epoch15, step2684]: loss 0.446266
[epoch15, step2685]: loss 0.556818
[epoch15, step2686]: loss 0.657528
[epoch15, step2687]: loss 0.455419
[epoch15, step2688]: loss 0.462161
[epoch15, step2689]: loss 0.536255
[epoch15, step2690]: loss 0.520858
[epoch15, step2691]: loss 0.495886
[epoch15, step2692]: loss 0.531574
[epoch15, step2693]: loss 0.645700
[epoch15, step2694]: loss 0.629253
[epoch15, step2695]: loss 0.665704
[epoch15, step2696]: loss 0.567998
[epoch15, step2697]: loss 0.458657
[epoch15, step2698]: loss 0.547725
[epoch15, step2699]: loss 0.248305
[epoch15, step2700]: loss 0.377229
[epoch15, step2701]: loss 0.296914
[epoch15, step2702]: loss 0.435952
[epoch15, step2703]: loss 0.542767
[epoch15, step2704]: loss 0.731014
[epoch15, step2705]: loss 0.560386
[epoch15, step2706]: loss 0.850205
[epoch15, step2707]: loss 0.459038
[epoch15, step2708]: loss 0.667261
[epoch15, step2709]: loss 0.801857
[epoch15, step2710]: loss 0.565671
[epoch15, step2711]: loss 0.737062
[epoch15, step2712]: loss 0.557589
[epoch15, step2713]: loss 0.450991
[epoch15, step2714]: loss 0.435468
[epoch15, step2715]: loss 0.710456
[epoch15, step2716]: loss 0.759130
[epoch15, step2717]: loss 0.548030
[epoch15, step2718]: loss 0.484356
[epoch15, step2719]: loss 0.553369
[epoch15, step2720]: loss 0.534137
[epoch15, step2721]: loss 0.240477
[epoch15, step2722]: loss 0.369424
[epoch15, step2723]: loss 0.493430
[epoch15, step2724]: loss 0.475883
[epoch15, step2725]: loss 0.500825
[epoch15, step2726]: loss 0.603953
[epoch15, step2727]: loss 0.546890
[epoch15, step2728]: loss 0.564536
[epoch15, step2729]: loss 0.654708
[epoch15, step2730]: loss 0.629094
[epoch15, step2731]: loss 0.703059
[epoch15, step2732]: loss 0.515799
[epoch15, step2733]: loss 0.640147
[epoch15, step2734]: loss 0.506010
[epoch15, step2735]: loss 0.675396
[epoch15, step2736]: loss 0.528621
[epoch15, step2737]: loss 0.546796
[epoch15, step2738]: loss 0.494211
[epoch15, step2739]: loss 0.455456
[epoch15, step2740]: loss 0.481138
[epoch15, step2741]: loss 0.409106
[epoch15, step2742]: loss 0.595737
[epoch15, step2743]: loss 0.782821
[epoch15, step2744]: loss 0.414909
[epoch15, step2745]: loss 0.700210
[epoch15, step2746]: loss 0.552640
[epoch15, step2747]: loss 0.549632
[epoch15, step2748]: loss 0.313370
[epoch15, step2749]: loss 0.501371
[epoch15, step2750]: loss 0.453184
[epoch15, step2751]: loss 0.814988
[epoch15, step2752]: loss 0.605808
[epoch15, step2753]: loss 0.271124
[epoch15, step2754]: loss 0.587150
[epoch15, step2755]: loss 0.314452
[epoch15, step2756]: loss 0.745887
[epoch15, step2757]: loss 0.629461
[epoch15, step2758]: loss 0.534422
[epoch15, step2759]: loss 0.631315
[epoch15, step2760]: loss 0.255391
[epoch15, step2761]: loss 0.497373
[epoch15, step2762]: loss 0.782405
[epoch15, step2763]: loss 0.393968
[epoch15, step2764]: loss 0.338418
[epoch15, step2765]: loss 0.550172
[epoch15, step2766]: loss 0.342914
[epoch15, step2767]: loss 0.760140
[epoch15, step2768]: loss 0.692150
[epoch15, step2769]: loss 0.564808
[epoch15, step2770]: loss 0.601252
[epoch15, step2771]: loss 0.507458
[epoch15, step2772]: loss 0.726701
[epoch15, step2773]: loss 0.660793
[epoch15, step2774]: loss 0.633833
[epoch15, step2775]: loss 0.625684
[epoch15, step2776]: loss 0.573211
[epoch15, step2777]: loss 0.566034
[epoch15, step2778]: loss 0.646350
[epoch15, step2779]: loss 0.565979
[epoch15, step2780]: loss 0.584514
[epoch15, step2781]: loss 0.281550
[epoch15, step2782]: loss 0.484619
[epoch15, step2783]: loss 0.741208
[epoch15, step2784]: loss 0.678293
[epoch15, step2785]: loss 0.568310
[epoch15, step2786]: loss 0.415489
[epoch15, step2787]: loss 0.532108
[epoch15, step2788]: loss 0.740931
[epoch15, step2789]: loss 0.606239
[epoch15, step2790]: loss 0.460053
[epoch15, step2791]: loss 0.455444
[epoch15, step2792]: loss 0.545968
[epoch15, step2793]: loss 0.490016
[epoch15, step2794]: loss 0.619649
[epoch15, step2795]: loss 0.336353
[epoch15, step2796]: loss 0.580522
[epoch15, step2797]: loss 0.518064
[epoch15, step2798]: loss 0.512153
[epoch15, step2799]: loss 0.666556
[epoch15, step2800]: loss 0.502417
[epoch15, step2801]: loss 0.591011
[epoch15, step2802]: loss 0.538142
[epoch15, step2803]: loss 0.551255
[epoch15, step2804]: loss 0.505462
[epoch15, step2805]: loss 0.660060
[epoch15, step2806]: loss 0.627290
[epoch15, step2807]: loss 0.646521
[epoch15, step2808]: loss 0.542694
[epoch15, step2809]: loss 0.699127
[epoch15, step2810]: loss 0.527085
[epoch15, step2811]: loss 0.635145
[epoch15, step2812]: loss 0.251303
[epoch15, step2813]: loss 0.493885
[epoch15, step2814]: loss 0.416021
[epoch15, step2815]: loss 0.557430
[epoch15, step2816]: loss 0.326076
[epoch15, step2817]: loss 0.541149
[epoch15, step2818]: loss 0.478052
[epoch15, step2819]: loss 0.530833
[epoch15, step2820]: loss 0.435307
[epoch15, step2821]: loss 0.541030
[epoch15, step2822]: loss 0.567391
[epoch15, step2823]: loss 0.400973
[epoch15, step2824]: loss 0.494325
[epoch15, step2825]: loss 0.382038
[epoch15, step2826]: loss 0.296838
[epoch15, step2827]: loss 0.602222
[epoch15, step2828]: loss 0.428179
[epoch15, step2829]: loss 0.639183
[epoch15, step2830]: loss 0.525510
[epoch15, step2831]: loss 0.520155
[epoch15, step2832]: loss 0.653074
[epoch15, step2833]: loss 0.600840
[epoch15, step2834]: loss 0.462589
[epoch15, step2835]: loss 0.618602
[epoch15, step2836]: loss 0.535996
[epoch15, step2837]: loss 0.717585
[epoch15, step2838]: loss 0.571867
[epoch15, step2839]: loss 0.442594
[epoch15, step2840]: loss 0.697187
[epoch15, step2841]: loss 0.623744
[epoch15, step2842]: loss 0.489135
[epoch15, step2843]: loss 0.668207
[epoch15, step2844]: loss 0.767659
[epoch15, step2845]: loss 0.571469
[epoch15, step2846]: loss 0.573756
[epoch15, step2847]: loss 0.431289
[epoch15, step2848]: loss 0.560191
[epoch15, step2849]: loss 0.485711
[epoch15, step2850]: loss 0.538955
[epoch15, step2851]: loss 0.564396
[epoch15, step2852]: loss 0.293538
[epoch15, step2853]: loss 0.563636
[epoch15, step2854]: loss 0.558071
[epoch15, step2855]: loss 0.659301
[epoch15, step2856]: loss 0.484595
[epoch15, step2857]: loss 0.565444
[epoch15, step2858]: loss 0.649164
[epoch15, step2859]: loss 0.512530
[epoch15, step2860]: loss 0.460813
[epoch15, step2861]: loss 0.611095
[epoch15, step2862]: loss 0.547333
[epoch15, step2863]: loss 0.481067
[epoch15, step2864]: loss 0.503068
[epoch15, step2865]: loss 0.477088
[epoch15, step2866]: loss 0.740527
[epoch15, step2867]: loss 0.666101
[epoch15, step2868]: loss 0.638084
[epoch15, step2869]: loss 0.568007
[epoch15, step2870]: loss 0.680918
[epoch15, step2871]: loss 0.650189
[epoch15, step2872]: loss 0.510250
[epoch15, step2873]: loss 0.711746
[epoch15, step2874]: loss 0.553247
[epoch15, step2875]: loss 0.839379
[epoch15, step2876]: loss 0.724583
[epoch15, step2877]: loss 0.475322
[epoch15, step2878]: loss 0.429931
[epoch15, step2879]: loss 0.449385
[epoch15, step2880]: loss 0.576850
[epoch15, step2881]: loss 0.615715
[epoch15, step2882]: loss 0.719100
[epoch15, step2883]: loss 0.395360
[epoch15, step2884]: loss 0.572117
[epoch15, step2885]: loss 0.477155
[epoch15, step2886]: loss 0.496743
[epoch15, step2887]: loss 0.548002
[epoch15, step2888]: loss 0.390276
[epoch15, step2889]: loss 0.325431
[epoch15, step2890]: loss 0.672506
[epoch15, step2891]: loss 0.502535
[epoch15, step2892]: loss 0.576233
[epoch15, step2893]: loss 0.662664
[epoch15, step2894]: loss 0.300760
[epoch15, step2895]: loss 0.533063
[epoch15, step2896]: loss 0.537401
[epoch15, step2897]: loss 0.460083
[epoch15, step2898]: loss 0.460097
[epoch15, step2899]: loss 0.400365
[epoch15, step2900]: loss 0.323710
[epoch15, step2901]: loss 0.611461
[epoch15, step2902]: loss 0.489012
[epoch15, step2903]: loss 0.551644
[epoch15, step2904]: loss 0.718477
[epoch15, step2905]: loss 0.738608
[epoch15, step2906]: loss 0.577232
[epoch15, step2907]: loss 0.471916
[epoch15, step2908]: loss 0.277741
[epoch15, step2909]: loss 0.435001
[epoch15, step2910]: loss 0.552819
[epoch15, step2911]: loss 0.771144
[epoch15, step2912]: loss 0.521681
[epoch15, step2913]: loss 0.704198
[epoch15, step2914]: loss 0.660220
[epoch15, step2915]: loss 0.456655
[epoch15, step2916]: loss 0.666393
[epoch15, step2917]: loss 0.654479
[epoch15, step2918]: loss 0.460689
[epoch15, step2919]: loss 0.508561
[epoch15, step2920]: loss 0.409798
[epoch15, step2921]: loss 0.680172
[epoch15, step2922]: loss 0.579717
[epoch15, step2923]: loss 0.625108
[epoch15, step2924]: loss 0.246471
[epoch15, step2925]: loss 0.411880
[epoch15, step2926]: loss 0.701928
[epoch15, step2927]: loss 0.432631
[epoch15, step2928]: loss 0.699254
[epoch15, step2929]: loss 0.452648
[epoch15, step2930]: loss 0.590259
[epoch15, step2931]: loss 0.622817
[epoch15, step2932]: loss 0.664203
[epoch15, step2933]: loss 0.654477
[epoch15, step2934]: loss 0.553101
[epoch15, step2935]: loss 0.645460
[epoch15, step2936]: loss 0.629293
[epoch15, step2937]: loss 0.663267
[epoch15, step2938]: loss 0.613795
[epoch15, step2939]: loss 0.425783
[epoch15, step2940]: loss 0.691722
[epoch15, step2941]: loss 0.599472
[epoch15, step2942]: loss 0.730754
[epoch15, step2943]: loss 0.488722
[epoch15, step2944]: loss 0.453997
[epoch15, step2945]: loss 0.537009
[epoch15, step2946]: loss 0.473316
[epoch15, step2947]: loss 0.425543
[epoch15, step2948]: loss 0.506165
[epoch15, step2949]: loss 0.609870
[epoch15, step2950]: loss 0.441938
[epoch15, step2951]: loss 0.565630
[epoch15, step2952]: loss 0.637514
[epoch15, step2953]: loss 0.429265
[epoch15, step2954]: loss 0.378445
[epoch15, step2955]: loss 0.646220
[epoch15, step2956]: loss 0.525719
[epoch15, step2957]: loss 0.725150
[epoch15, step2958]: loss 0.501125
[epoch15, step2959]: loss 0.512079
[epoch15, step2960]: loss 0.632968
[epoch15, step2961]: loss 0.455655
[epoch15, step2962]: loss 0.501576
[epoch15, step2963]: loss 0.519017
[epoch15, step2964]: loss 0.379853
[epoch15, step2965]: loss 0.735093
[epoch15, step2966]: loss 0.468358
[epoch15, step2967]: loss 0.733817
[epoch15, step2968]: loss 0.411465
[epoch15, step2969]: loss 0.508037
[epoch15, step2970]: loss 0.646694
[epoch15, step2971]: loss 0.489989
[epoch15, step2972]: loss 0.506796
[epoch15, step2973]: loss 0.305156
[epoch15, step2974]: loss 0.406405
[epoch15, step2975]: loss 0.529684
[epoch15, step2976]: loss 0.519242
[epoch15, step2977]: loss 0.719495
[epoch15, step2978]: loss 0.362093
[epoch15, step2979]: loss 0.758680
[epoch15, step2980]: loss 0.305674
[epoch15, step2981]: loss 0.643414
[epoch15, step2982]: loss 0.600300
[epoch15, step2983]: loss 0.578515
[epoch15, step2984]: loss 0.617111
[epoch15, step2985]: loss 0.708009
[epoch15, step2986]: loss 0.264170
[epoch15, step2987]: loss 0.156654
[epoch15, step2988]: loss 0.494408
[epoch15, step2989]: loss 0.437979
[epoch15, step2990]: loss 0.663410
[epoch15, step2991]: loss 0.832523
[epoch15, step2992]: loss 0.312769
[epoch15, step2993]: loss 0.589583
[epoch15, step2994]: loss 0.675486
[epoch15, step2995]: loss 0.450233
[epoch15, step2996]: loss 0.704285
[epoch15, step2997]: loss 0.134018
[epoch15, step2998]: loss 0.511336
[epoch15, step2999]: loss 0.614033
[epoch15, step3000]: loss 0.742971
[epoch15, step3001]: loss 0.371815
[epoch15, step3002]: loss 0.588633
[epoch15, step3003]: loss 0.540165
[epoch15, step3004]: loss 0.380531
[epoch15, step3005]: loss 0.658173
[epoch15, step3006]: loss 0.334609
[epoch15, step3007]: loss 0.736790
[epoch15, step3008]: loss 0.689114
[epoch15, step3009]: loss 0.474493
[epoch15, step3010]: loss 0.789574
[epoch15, step3011]: loss 0.581320
[epoch15, step3012]: loss 0.569475
[epoch15, step3013]: loss 0.698267
[epoch15, step3014]: loss 0.394103
[epoch15, step3015]: loss 0.597284
[epoch15, step3016]: loss 0.701795
[epoch15, step3017]: loss 0.344721
[epoch15, step3018]: loss 0.546963
[epoch15, step3019]: loss 0.560271
[epoch15, step3020]: loss 0.708316
[epoch15, step3021]: loss 0.560015
[epoch15, step3022]: loss 0.488424
[epoch15, step3023]: loss 0.543578
[epoch15, step3024]: loss 0.477238
[epoch15, step3025]: loss 0.761971
[epoch15, step3026]: loss 0.591415
[epoch15, step3027]: loss 0.603483
[epoch15, step3028]: loss 0.497813
[epoch15, step3029]: loss 0.570730
[epoch15, step3030]: loss 0.533828
[epoch15, step3031]: loss 0.542392
[epoch15, step3032]: loss 0.546940
[epoch15, step3033]: loss 0.649824
[epoch15, step3034]: loss 0.457120
[epoch15, step3035]: loss 0.480833
[epoch15, step3036]: loss 0.556790
[epoch15, step3037]: loss 0.353843
[epoch15, step3038]: loss 0.779257
[epoch15, step3039]: loss 0.669180
[epoch15, step3040]: loss 0.235024
[epoch15, step3041]: loss 0.458924
[epoch15, step3042]: loss 0.539061
[epoch15, step3043]: loss 0.621598
[epoch15, step3044]: loss 0.308107
[epoch15, step3045]: loss 0.353283
[epoch15, step3046]: loss 0.523133
[epoch15, step3047]: loss 0.607537
[epoch15, step3048]: loss 0.327729
[epoch15, step3049]: loss 0.464528
[epoch15, step3050]: loss 0.729726
[epoch15, step3051]: loss 0.309639
[epoch15, step3052]: loss 0.646744
[epoch15, step3053]: loss 0.731960
[epoch15, step3054]: loss 0.467223
[epoch15, step3055]: loss 0.582608
[epoch15, step3056]: loss 0.538238
[epoch15, step3057]: loss 0.566123
[epoch15, step3058]: loss 0.631444
[epoch15, step3059]: loss 0.598799
[epoch15, step3060]: loss 0.547806
[epoch15, step3061]: loss 0.580673
[epoch15, step3062]: loss 0.624541
[epoch15, step3063]: loss 0.686080
[epoch15, step3064]: loss 0.610888
[epoch15, step3065]: loss 0.731366
[epoch15, step3066]: loss 0.672676
[epoch15, step3067]: loss 0.713846
[epoch15, step3068]: loss 0.390090
[epoch15, step3069]: loss 0.647869
[epoch15, step3070]: loss 0.592133
[epoch15, step3071]: loss 0.488051
[epoch15, step3072]: loss 0.534540
[epoch15, step3073]: loss 0.475290
[epoch15, step3074]: loss 0.581443
[epoch15, step3075]: loss 0.443162
[epoch15, step3076]: loss 0.558614

[epoch15]: avg loss 0.558614

[epoch16, step1]: loss 0.556362
[epoch16, step2]: loss 0.468114
[epoch16, step3]: loss 0.578442
[epoch16, step4]: loss 0.766883
[epoch16, step5]: loss 0.369733
[epoch16, step6]: loss 0.578098
[epoch16, step7]: loss 0.382840
[epoch16, step8]: loss 0.372115
[epoch16, step9]: loss 0.360352
[epoch16, step10]: loss 0.601007
[epoch16, step11]: loss 0.539377
[epoch16, step12]: loss 0.340883
[epoch16, step13]: loss 0.567756
[epoch16, step14]: loss 0.625180
[epoch16, step15]: loss 0.788954
[epoch16, step16]: loss 0.471564
[epoch16, step17]: loss 0.492029
[epoch16, step18]: loss 0.657723
[epoch16, step19]: loss 0.593848
[epoch16, step20]: loss 0.523977
[epoch16, step21]: loss 0.424132
[epoch16, step22]: loss 0.595341
[epoch16, step23]: loss 0.610580
[epoch16, step24]: loss 0.734532
[epoch16, step25]: loss 0.415321
[epoch16, step26]: loss 0.587775
[epoch16, step27]: loss 0.508644
[epoch16, step28]: loss 0.506112
[epoch16, step29]: loss 0.432382
[epoch16, step30]: loss 0.623892
[epoch16, step31]: loss 0.473470
[epoch16, step32]: loss 0.610338
[epoch16, step33]: loss 0.757143
[epoch16, step34]: loss 0.516688
[epoch16, step35]: loss 0.462477
[epoch16, step36]: loss 0.702600
[epoch16, step37]: loss 0.660960
[epoch16, step38]: loss 0.623898
[epoch16, step39]: loss 0.600223
[epoch16, step40]: loss 0.760279
[epoch16, step41]: loss 0.473430
[epoch16, step42]: loss 0.762344
[epoch16, step43]: loss 0.443617
[epoch16, step44]: loss 0.648068
[epoch16, step45]: loss 0.644239
[epoch16, step46]: loss 0.486785
[epoch16, step47]: loss 0.572740
[epoch16, step48]: loss 0.516097
[epoch16, step49]: loss 0.641508
[epoch16, step50]: loss 0.488177
[epoch16, step51]: loss 0.472362
[epoch16, step52]: loss 0.524226
[epoch16, step53]: loss 0.672367
[epoch16, step54]: loss 0.546984
[epoch16, step55]: loss 0.424261
[epoch16, step56]: loss 0.600315
[epoch16, step57]: loss 0.607668
[epoch16, step58]: loss 0.442768
[epoch16, step59]: loss 0.464724
[epoch16, step60]: loss 0.426654
[epoch16, step61]: loss 0.624932
[epoch16, step62]: loss 0.622083
[epoch16, step63]: loss 0.746888
[epoch16, step64]: loss 0.412754
[epoch16, step65]: loss 0.559670
[epoch16, step66]: loss 0.551681
[epoch16, step67]: loss 0.503959
[epoch16, step68]: loss 0.528669
[epoch16, step69]: loss 0.577905
[epoch16, step70]: loss 0.352084
[epoch16, step71]: loss 0.375864
[epoch16, step72]: loss 0.593230
[epoch16, step73]: loss 0.684877
[epoch16, step74]: loss 0.487216
[epoch16, step75]: loss 0.647884
[epoch16, step76]: loss 0.436592
[epoch16, step77]: loss 0.507339
[epoch16, step78]: loss 0.301425
[epoch16, step79]: loss 0.624752
[epoch16, step80]: loss 0.685013
[epoch16, step81]: loss 0.433418
[epoch16, step82]: loss 0.592176
[epoch16, step83]: loss 0.690808
[epoch16, step84]: loss 0.763823
[epoch16, step85]: loss 0.542443
[epoch16, step86]: loss 0.439593
[epoch16, step87]: loss 0.637830
[epoch16, step88]: loss 0.612206
[epoch16, step89]: loss 0.621795
[epoch16, step90]: loss 0.836481
[epoch16, step91]: loss 0.579103
[epoch16, step92]: loss 0.509813
[epoch16, step93]: loss 0.140445
[epoch16, step94]: loss 0.442899
[epoch16, step95]: loss 0.603367
[epoch16, step96]: loss 0.546586
[epoch16, step97]: loss 0.784153
[epoch16, step98]: loss 0.314050
[epoch16, step99]: loss 0.382547
[epoch16, step100]: loss 0.453822
[epoch16, step101]: loss 0.678478
[epoch16, step102]: loss 0.547269
[epoch16, step103]: loss 0.515422
[epoch16, step104]: loss 0.756248
[epoch16, step105]: loss 0.380615
[epoch16, step106]: loss 0.757620
[epoch16, step107]: loss 0.561433
[epoch16, step108]: loss 0.407973
[epoch16, step109]: loss 0.825470
[epoch16, step110]: loss 0.528321
[epoch16, step111]: loss 0.150221
[epoch16, step112]: loss 0.284482
[epoch16, step113]: loss 0.652932
[epoch16, step114]: loss 0.623050
[epoch16, step115]: loss 0.706186
[epoch16, step116]: loss 0.464905
[epoch16, step117]: loss 0.697302
[epoch16, step118]: loss 0.644238
[epoch16, step119]: loss 0.789495
[epoch16, step120]: loss 0.838826
[epoch16, step121]: loss 0.681890
[epoch16, step122]: loss 0.606197
[epoch16, step123]: loss 0.773355
[epoch16, step124]: loss 0.444158
[epoch16, step125]: loss 0.648826
[epoch16, step126]: loss 0.620961
[epoch16, step127]: loss 0.253183
[epoch16, step128]: loss 0.613331
[epoch16, step129]: loss 0.583501
[epoch16, step130]: loss 0.438800
[epoch16, step131]: loss 0.467738
[epoch16, step132]: loss 0.537638
[epoch16, step133]: loss 0.604499
[epoch16, step134]: loss 0.555621
[epoch16, step135]: loss 0.607519
[epoch16, step136]: loss 0.411191
[epoch16, step137]: loss 0.280992
[epoch16, step138]: loss 0.637669
[epoch16, step139]: loss 0.337576
[epoch16, step140]: loss 0.588113
[epoch16, step141]: loss 0.542144
[epoch16, step142]: loss 0.601595
[epoch16, step143]: loss 0.522902
[epoch16, step144]: loss 0.632880
[epoch16, step145]: loss 0.475047
[epoch16, step146]: loss 0.447723
[epoch16, step147]: loss 0.638923
[epoch16, step148]: loss 0.560003
[epoch16, step149]: loss 0.343062
[epoch16, step150]: loss 0.573179
[epoch16, step151]: loss 0.439044
[epoch16, step152]: loss 0.515909
[epoch16, step153]: loss 0.624197
[epoch16, step154]: loss 0.575990
[epoch16, step155]: loss 0.428807
[epoch16, step156]: loss 0.583261
[epoch16, step157]: loss 0.429160
[epoch16, step158]: loss 0.570497
[epoch16, step159]: loss 0.500932
[epoch16, step160]: loss 0.565067
[epoch16, step161]: loss 0.404488
[epoch16, step162]: loss 0.566437
[epoch16, step163]: loss 0.691117
[epoch16, step164]: loss 0.558818
[epoch16, step165]: loss 0.448358
[epoch16, step166]: loss 0.675799
[epoch16, step167]: loss 0.569402
[epoch16, step168]: loss 0.636122
[epoch16, step169]: loss 0.541989
[epoch16, step170]: loss 0.650172
[epoch16, step171]: loss 0.555535
[epoch16, step172]: loss 0.522460
[epoch16, step173]: loss 0.545017
[epoch16, step174]: loss 0.706176
[epoch16, step175]: loss 0.450060
[epoch16, step176]: loss 0.621877
[epoch16, step177]: loss 0.705614
[epoch16, step178]: loss 0.367420
[epoch16, step179]: loss 0.706727
[epoch16, step180]: loss 0.493873
[epoch16, step181]: loss 0.707686
[epoch16, step182]: loss 0.598708
[epoch16, step183]: loss 0.677238
[epoch16, step184]: loss 0.356527
[epoch16, step185]: loss 0.513298
[epoch16, step186]: loss 0.537085
[epoch16, step187]: loss 0.747104
[epoch16, step188]: loss 0.588397
[epoch16, step189]: loss 0.351553
[epoch16, step190]: loss 0.502530
[epoch16, step191]: loss 0.561723
[epoch16, step192]: loss 0.627995
[epoch16, step193]: loss 0.452540
[epoch16, step194]: loss 0.634834
[epoch16, step195]: loss 0.717609
[epoch16, step196]: loss 0.548669
[epoch16, step197]: loss 0.444192
[epoch16, step198]: loss 0.440056
[epoch16, step199]: loss 0.608751
[epoch16, step200]: loss 0.487090
[epoch16, step201]: loss 0.589776
[epoch16, step202]: loss 0.664085
[epoch16, step203]: loss 0.437737
[epoch16, step204]: loss 0.606387
[epoch16, step205]: loss 0.486182
[epoch16, step206]: loss 0.417634
[epoch16, step207]: loss 0.381169
[epoch16, step208]: loss 0.590914
[epoch16, step209]: loss 0.585262
[epoch16, step210]: loss 0.329103
[epoch16, step211]: loss 0.666912
[epoch16, step212]: loss 0.686165
[epoch16, step213]: loss 0.427085
[epoch16, step214]: loss 0.352678
[epoch16, step215]: loss 0.510987
[epoch16, step216]: loss 0.707502
[epoch16, step217]: loss 0.463035
[epoch16, step218]: loss 0.427810
[epoch16, step219]: loss 0.574983
[epoch16, step220]: loss 0.624336
[epoch16, step221]: loss 0.583693
[epoch16, step222]: loss 0.429928
[epoch16, step223]: loss 0.610699
[epoch16, step224]: loss 0.693361
[epoch16, step225]: loss 0.573808
[epoch16, step226]: loss 0.491565
[epoch16, step227]: loss 0.495817
[epoch16, step228]: loss 0.755261
[epoch16, step229]: loss 0.275151
[epoch16, step230]: loss 0.605224
[epoch16, step231]: loss 0.474632
[epoch16, step232]: loss 0.454812
[epoch16, step233]: loss 0.634883
[epoch16, step234]: loss 0.488300
[epoch16, step235]: loss 0.447340
[epoch16, step236]: loss 0.397435
[epoch16, step237]: loss 0.483507
[epoch16, step238]: loss 0.640176
[epoch16, step239]: loss 0.723548
[epoch16, step240]: loss 0.360066
[epoch16, step241]: loss 0.812966
[epoch16, step242]: loss 0.671484
[epoch16, step243]: loss 0.644748
[epoch16, step244]: loss 0.656153
[epoch16, step245]: loss 0.613826
[epoch16, step246]: loss 0.691602
[epoch16, step247]: loss 0.516711
[epoch16, step248]: loss 0.295899
[epoch16, step249]: loss 0.116754
[epoch16, step250]: loss 0.548289
[epoch16, step251]: loss 0.592491
[epoch16, step252]: loss 0.216287
[epoch16, step253]: loss 0.729714
[epoch16, step254]: loss 0.461947
[epoch16, step255]: loss 0.579362
[epoch16, step256]: loss 0.562590
[epoch16, step257]: loss 0.819153
[epoch16, step258]: loss 0.400505
[epoch16, step259]: loss 0.404613
[epoch16, step260]: loss 0.479417
[epoch16, step261]: loss 0.629612
[epoch16, step262]: loss 0.375323
[epoch16, step263]: loss 0.477365
[epoch16, step264]: loss 0.629057
[epoch16, step265]: loss 0.542035
[epoch16, step266]: loss 0.539328
[epoch16, step267]: loss 0.734645
[epoch16, step268]: loss 0.534936
[epoch16, step269]: loss 0.522030
[epoch16, step270]: loss 0.436002
[epoch16, step271]: loss 0.620622
[epoch16, step272]: loss 0.588891
[epoch16, step273]: loss 0.641509
[epoch16, step274]: loss 0.580445
[epoch16, step275]: loss 0.453596
[epoch16, step276]: loss 0.515063
[epoch16, step277]: loss 0.520806
[epoch16, step278]: loss 0.790365
[epoch16, step279]: loss 0.702775
[epoch16, step280]: loss 0.536900
[epoch16, step281]: loss 0.736472
[epoch16, step282]: loss 0.620691
[epoch16, step283]: loss 0.400100
[epoch16, step284]: loss 0.440294
[epoch16, step285]: loss 0.789267
[epoch16, step286]: loss 0.700617
[epoch16, step287]: loss 0.516210
[epoch16, step288]: loss 0.280941
[epoch16, step289]: loss 0.424408
[epoch16, step290]: loss 0.550043
[epoch16, step291]: loss 0.395296
[epoch16, step292]: loss 0.643848
[epoch16, step293]: loss 0.585296
[epoch16, step294]: loss 0.557455
[epoch16, step295]: loss 0.414503
[epoch16, step296]: loss 0.531775
[epoch16, step297]: loss 0.521610
[epoch16, step298]: loss 0.577720
[epoch16, step299]: loss 0.621791
[epoch16, step300]: loss 0.397706
[epoch16, step301]: loss 0.437354
[epoch16, step302]: loss 0.615307
[epoch16, step303]: loss 0.738577
[epoch16, step304]: loss 0.687789
[epoch16, step305]: loss 0.415651
[epoch16, step306]: loss 0.505606
[epoch16, step307]: loss 0.808135
[epoch16, step308]: loss 0.793915
[epoch16, step309]: loss 0.380977
[epoch16, step310]: loss 0.642378
[epoch16, step311]: loss 0.609658
[epoch16, step312]: loss 0.405202
[epoch16, step313]: loss 0.344636
[epoch16, step314]: loss 0.426364
[epoch16, step315]: loss 0.705703
[epoch16, step316]: loss 0.629357
[epoch16, step317]: loss 0.548444
[epoch16, step318]: loss 0.387282
[epoch16, step319]: loss 0.584370
[epoch16, step320]: loss 0.696603
[epoch16, step321]: loss 0.651737
[epoch16, step322]: loss 0.212965
[epoch16, step323]: loss 0.730673
[epoch16, step324]: loss 0.518058
[epoch16, step325]: loss 0.620933
[epoch16, step326]: loss 0.763777
[epoch16, step327]: loss 0.623029
[epoch16, step328]: loss 0.526913
[epoch16, step329]: loss 0.582326
[epoch16, step330]: loss 0.346227
[epoch16, step331]: loss 0.402608
[epoch16, step332]: loss 0.691313
[epoch16, step333]: loss 0.687697
[epoch16, step334]: loss 0.611302
[epoch16, step335]: loss 0.564813
[epoch16, step336]: loss 0.700566
[epoch16, step337]: loss 0.472259
[epoch16, step338]: loss 0.314073
[epoch16, step339]: loss 0.487174
[epoch16, step340]: loss 0.500795
[epoch16, step341]: loss 0.764810
[epoch16, step342]: loss 0.591961
[epoch16, step343]: loss 0.559780
[epoch16, step344]: loss 0.600478
[epoch16, step345]: loss 0.423526
[epoch16, step346]: loss 0.690584
[epoch16, step347]: loss 0.437604
[epoch16, step348]: loss 0.547270
[epoch16, step349]: loss 0.368624
[epoch16, step350]: loss 0.504585
[epoch16, step351]: loss 0.678489
[epoch16, step352]: loss 0.394385
[epoch16, step353]: loss 0.570126
[epoch16, step354]: loss 0.720821
[epoch16, step355]: loss 0.581963
[epoch16, step356]: loss 0.634034
[epoch16, step357]: loss 0.441506
[epoch16, step358]: loss 0.726371
[epoch16, step359]: loss 0.474806
[epoch16, step360]: loss 0.568708
[epoch16, step361]: loss 0.595462
[epoch16, step362]: loss 0.573475
[epoch16, step363]: loss 0.636066
[epoch16, step364]: loss 0.490101
[epoch16, step365]: loss 0.686689
[epoch16, step366]: loss 0.637827
[epoch16, step367]: loss 0.482011
[epoch16, step368]: loss 0.511529
[epoch16, step369]: loss 0.524913
[epoch16, step370]: loss 0.580216
[epoch16, step371]: loss 0.522665
[epoch16, step372]: loss 0.519171
[epoch16, step373]: loss 0.479139
[epoch16, step374]: loss 0.713236
[epoch16, step375]: loss 0.511426
[epoch16, step376]: loss 0.463071
[epoch16, step377]: loss 0.608878
[epoch16, step378]: loss 0.721985
[epoch16, step379]: loss 0.638600
[epoch16, step380]: loss 0.476283
[epoch16, step381]: loss 0.504363
[epoch16, step382]: loss 0.567813
[epoch16, step383]: loss 0.162725
[epoch16, step384]: loss 0.583184
[epoch16, step385]: loss 0.315055
[epoch16, step386]: loss 0.771917
[epoch16, step387]: loss 0.677996
[epoch16, step388]: loss 0.687368
[epoch16, step389]: loss 0.608765
[epoch16, step390]: loss 0.699309
[epoch16, step391]: loss 0.583756
[epoch16, step392]: loss 0.288828
[epoch16, step393]: loss 0.611626
[epoch16, step394]: loss 0.608110
[epoch16, step395]: loss 0.575531
[epoch16, step396]: loss 0.535962
[epoch16, step397]: loss 0.425620
[epoch16, step398]: loss 0.714059
[epoch16, step399]: loss 0.433687
[epoch16, step400]: loss 0.630009
[epoch16, step401]: loss 0.321008
[epoch16, step402]: loss 0.467215
[epoch16, step403]: loss 0.601769
[epoch16, step404]: loss 0.195524
[epoch16, step405]: loss 0.616543
[epoch16, step406]: loss 0.449650
[epoch16, step407]: loss 0.629015
[epoch16, step408]: loss 0.705623
[epoch16, step409]: loss 0.465137
[epoch16, step410]: loss 0.669625
[epoch16, step411]: loss 0.662736
[epoch16, step412]: loss 0.757870
[epoch16, step413]: loss 0.653599
[epoch16, step414]: loss 0.446723
[epoch16, step415]: loss 0.517870
[epoch16, step416]: loss 0.338619
[epoch16, step417]: loss 0.524555
[epoch16, step418]: loss 0.661285
[epoch16, step419]: loss 0.517414
[epoch16, step420]: loss 0.480735
[epoch16, step421]: loss 0.303880
[epoch16, step422]: loss 0.588494
[epoch16, step423]: loss 0.539691
[epoch16, step424]: loss 0.317730
[epoch16, step425]: loss 0.191759
[epoch16, step426]: loss 0.733191
[epoch16, step427]: loss 0.545766
[epoch16, step428]: loss 0.511074
[epoch16, step429]: loss 0.411286
[epoch16, step430]: loss 0.516628
[epoch16, step431]: loss 0.570246
[epoch16, step432]: loss 0.743200
[epoch16, step433]: loss 0.620882
[epoch16, step434]: loss 0.688013
[epoch16, step435]: loss 0.594308
[epoch16, step436]: loss 0.472272
[epoch16, step437]: loss 0.505309
[epoch16, step438]: loss 0.754219
[epoch16, step439]: loss 0.509200
[epoch16, step440]: loss 0.545999
[epoch16, step441]: loss 0.583702
[epoch16, step442]: loss 0.561665
[epoch16, step443]: loss 0.412614
[epoch16, step444]: loss 0.422220
[epoch16, step445]: loss 0.751182
[epoch16, step446]: loss 0.679778
[epoch16, step447]: loss 0.402089
[epoch16, step448]: loss 0.518211
[epoch16, step449]: loss 0.468432
[epoch16, step450]: loss 0.598005
[epoch16, step451]: loss 0.531037
[epoch16, step452]: loss 0.704268
[epoch16, step453]: loss 0.683938
[epoch16, step454]: loss 0.643589
[epoch16, step455]: loss 0.704837
[epoch16, step456]: loss 0.279794
[epoch16, step457]: loss 0.422872
[epoch16, step458]: loss 0.395537
[epoch16, step459]: loss 0.555433
[epoch16, step460]: loss 0.250280
[epoch16, step461]: loss 0.557254
[epoch16, step462]: loss 0.748301
[epoch16, step463]: loss 0.434390
[epoch16, step464]: loss 0.345010
[epoch16, step465]: loss 0.663808
[epoch16, step466]: loss 0.533313
[epoch16, step467]: loss 0.415357
[epoch16, step468]: loss 0.597920
[epoch16, step469]: loss 0.529624
[epoch16, step470]: loss 0.654576
[epoch16, step471]: loss 0.461247
[epoch16, step472]: loss 0.305240
[epoch16, step473]: loss 0.608337
[epoch16, step474]: loss 0.434617
[epoch16, step475]: loss 0.734757
[epoch16, step476]: loss 0.294972
[epoch16, step477]: loss 0.632246
[epoch16, step478]: loss 0.586325
[epoch16, step479]: loss 0.432595
[epoch16, step480]: loss 0.502376
[epoch16, step481]: loss 0.503033
[epoch16, step482]: loss 0.573159
[epoch16, step483]: loss 0.732547
[epoch16, step484]: loss 0.581276
[epoch16, step485]: loss 0.565997
[epoch16, step486]: loss 0.559328
[epoch16, step487]: loss 0.392534
[epoch16, step488]: loss 0.666473
[epoch16, step489]: loss 0.676316
[epoch16, step490]: loss 0.470673
[epoch16, step491]: loss 0.439261
[epoch16, step492]: loss 0.548147
[epoch16, step493]: loss 0.732713
[epoch16, step494]: loss 0.474106
[epoch16, step495]: loss 0.536703
[epoch16, step496]: loss 0.524026
[epoch16, step497]: loss 0.386185
[epoch16, step498]: loss 0.336442
[epoch16, step499]: loss 0.713067
[epoch16, step500]: loss 0.463261
[epoch16, step501]: loss 0.791644
[epoch16, step502]: loss 0.279922
[epoch16, step503]: loss 0.385298
[epoch16, step504]: loss 0.556762
[epoch16, step505]: loss 0.531814
[epoch16, step506]: loss 0.493700
[epoch16, step507]: loss 0.559194
[epoch16, step508]: loss 0.594473
[epoch16, step509]: loss 0.548720
[epoch16, step510]: loss 0.515309
[epoch16, step511]: loss 0.445880
[epoch16, step512]: loss 0.476718
[epoch16, step513]: loss 0.383795
[epoch16, step514]: loss 0.568411
[epoch16, step515]: loss 0.501257
[epoch16, step516]: loss 0.483900
[epoch16, step517]: loss 0.668469
[epoch16, step518]: loss 0.883308
[epoch16, step519]: loss 0.715754
[epoch16, step520]: loss 0.763928
[epoch16, step521]: loss 0.746042
[epoch16, step522]: loss 0.776525
[epoch16, step523]: loss 0.491286
[epoch16, step524]: loss 0.442984
[epoch16, step525]: loss 0.429421
[epoch16, step526]: loss 0.705388
[epoch16, step527]: loss 0.559637
[epoch16, step528]: loss 0.537361
[epoch16, step529]: loss 0.586384
[epoch16, step530]: loss 0.635518
[epoch16, step531]: loss 0.622678
[epoch16, step532]: loss 0.510101
[epoch16, step533]: loss 0.437725
[epoch16, step534]: loss 0.682095
[epoch16, step535]: loss 0.493355
[epoch16, step536]: loss 0.633874
[epoch16, step537]: loss 0.356100
[epoch16, step538]: loss 0.406265
[epoch16, step539]: loss 0.298005
[epoch16, step540]: loss 0.627683
[epoch16, step541]: loss 0.650923
[epoch16, step542]: loss 0.645032
[epoch16, step543]: loss 0.720144
[epoch16, step544]: loss 0.719974
[epoch16, step545]: loss 0.556995
[epoch16, step546]: loss 0.394054
[epoch16, step547]: loss 0.558604
[epoch16, step548]: loss 0.638275
[epoch16, step549]: loss 0.436289
[epoch16, step550]: loss 0.380490
[epoch16, step551]: loss 0.328523
[epoch16, step552]: loss 0.655223
[epoch16, step553]: loss 0.422525
[epoch16, step554]: loss 0.594694
[epoch16, step555]: loss 0.366225
[epoch16, step556]: loss 0.719428
[epoch16, step557]: loss 0.598071
[epoch16, step558]: loss 0.731640
[epoch16, step559]: loss 0.218852
[epoch16, step560]: loss 0.583138
[epoch16, step561]: loss 0.620540
[epoch16, step562]: loss 0.355570
[epoch16, step563]: loss 0.581912
[epoch16, step564]: loss 0.547880
[epoch16, step565]: loss 0.574713
[epoch16, step566]: loss 0.407112
[epoch16, step567]: loss 0.446616
[epoch16, step568]: loss 0.406804
[epoch16, step569]: loss 0.460880
[epoch16, step570]: loss 0.450098
[epoch16, step571]: loss 0.541529
[epoch16, step572]: loss 0.497179
[epoch16, step573]: loss 0.545322
[epoch16, step574]: loss 0.637749
[epoch16, step575]: loss 0.432706
[epoch16, step576]: loss 0.808695
[epoch16, step577]: loss 0.575627
[epoch16, step578]: loss 0.496614
[epoch16, step579]: loss 0.568358
[epoch16, step580]: loss 0.606300
[epoch16, step581]: loss 0.757489
[epoch16, step582]: loss 0.270790
[epoch16, step583]: loss 0.717275
[epoch16, step584]: loss 0.418942
[epoch16, step585]: loss 0.567387
[epoch16, step586]: loss 0.346423
[epoch16, step587]: loss 0.573722
[epoch16, step588]: loss 0.481774
[epoch16, step589]: loss 0.607252
[epoch16, step590]: loss 0.667262
[epoch16, step591]: loss 0.265847
[epoch16, step592]: loss 0.607863
[epoch16, step593]: loss 0.182829
[epoch16, step594]: loss 0.521889
[epoch16, step595]: loss 0.745579
[epoch16, step596]: loss 0.308351
[epoch16, step597]: loss 0.521025
[epoch16, step598]: loss 0.569293
[epoch16, step599]: loss 0.602244
[epoch16, step600]: loss 0.593277
[epoch16, step601]: loss 0.631135
[epoch16, step602]: loss 0.633354
[epoch16, step603]: loss 0.390907
[epoch16, step604]: loss 0.553050
[epoch16, step605]: loss 0.508181
[epoch16, step606]: loss 0.574207
[epoch16, step607]: loss 0.597153
[epoch16, step608]: loss 0.648805
[epoch16, step609]: loss 0.497346
[epoch16, step610]: loss 0.510860
[epoch16, step611]: loss 0.337244
[epoch16, step612]: loss 0.675857
[epoch16, step613]: loss 0.564752
[epoch16, step614]: loss 0.592749
[epoch16, step615]: loss 0.156203
[epoch16, step616]: loss 0.665109
[epoch16, step617]: loss 0.169266
[epoch16, step618]: loss 0.534075
[epoch16, step619]: loss 0.501183
[epoch16, step620]: loss 0.580601
[epoch16, step621]: loss 0.603203
[epoch16, step622]: loss 0.746393
[epoch16, step623]: loss 0.591524
[epoch16, step624]: loss 0.604352
[epoch16, step625]: loss 0.608890
[epoch16, step626]: loss 0.759939
[epoch16, step627]: loss 0.524100
[epoch16, step628]: loss 0.684204
[epoch16, step629]: loss 0.614740
[epoch16, step630]: loss 0.534219
[epoch16, step631]: loss 0.380760
[epoch16, step632]: loss 0.725856
[epoch16, step633]: loss 0.364634
[epoch16, step634]: loss 0.527624
[epoch16, step635]: loss 0.563423
[epoch16, step636]: loss 0.656853
[epoch16, step637]: loss 0.543550
[epoch16, step638]: loss 0.731041
[epoch16, step639]: loss 0.617285
[epoch16, step640]: loss 0.551581
[epoch16, step641]: loss 0.502899
[epoch16, step642]: loss 0.633249
[epoch16, step643]: loss 0.569210
[epoch16, step644]: loss 0.340970
[epoch16, step645]: loss 0.469085
[epoch16, step646]: loss 0.459059
[epoch16, step647]: loss 0.521017
[epoch16, step648]: loss 0.711447
[epoch16, step649]: loss 0.576786
[epoch16, step650]: loss 0.477813
[epoch16, step651]: loss 0.485472
[epoch16, step652]: loss 0.664111
[epoch16, step653]: loss 0.528685
[epoch16, step654]: loss 0.464838
[epoch16, step655]: loss 0.430402
[epoch16, step656]: loss 0.637436
[epoch16, step657]: loss 0.559709
[epoch16, step658]: loss 0.607999
[epoch16, step659]: loss 0.457814
[epoch16, step660]: loss 0.564099
[epoch16, step661]: loss 0.454922
[epoch16, step662]: loss 0.625582
[epoch16, step663]: loss 0.270976
[epoch16, step664]: loss 0.711944
[epoch16, step665]: loss 0.531586
[epoch16, step666]: loss 0.423607
[epoch16, step667]: loss 0.360403
[epoch16, step668]: loss 0.658518
[epoch16, step669]: loss 0.784011
[epoch16, step670]: loss 0.608403
[epoch16, step671]: loss 0.416224
[epoch16, step672]: loss 0.644245
[epoch16, step673]: loss 0.682442
[epoch16, step674]: loss 0.630023
[epoch16, step675]: loss 0.509696
[epoch16, step676]: loss 0.302172
[epoch16, step677]: loss 0.549534
[epoch16, step678]: loss 0.410286
[epoch16, step679]: loss 0.589154
[epoch16, step680]: loss 0.620933
[epoch16, step681]: loss 0.357900
[epoch16, step682]: loss 0.313867
[epoch16, step683]: loss 0.500865
[epoch16, step684]: loss 0.702747
[epoch16, step685]: loss 0.476051
[epoch16, step686]: loss 0.505807
[epoch16, step687]: loss 0.450288
[epoch16, step688]: loss 0.501790
[epoch16, step689]: loss 0.565480
[epoch16, step690]: loss 0.603226
[epoch16, step691]: loss 0.604152
[epoch16, step692]: loss 0.512419
[epoch16, step693]: loss 0.657481
[epoch16, step694]: loss 0.426789
[epoch16, step695]: loss 0.693978
[epoch16, step696]: loss 0.615341
[epoch16, step697]: loss 0.694423
[epoch16, step698]: loss 0.545978
[epoch16, step699]: loss 0.295856
[epoch16, step700]: loss 0.524910
[epoch16, step701]: loss 0.516561
[epoch16, step702]: loss 0.494382
[epoch16, step703]: loss 0.277669
[epoch16, step704]: loss 0.495977
[epoch16, step705]: loss 0.622635
[epoch16, step706]: loss 0.609960
[epoch16, step707]: loss 0.739653
[epoch16, step708]: loss 0.660149
[epoch16, step709]: loss 0.533174
[epoch16, step710]: loss 0.333047
[epoch16, step711]: loss 0.396117
[epoch16, step712]: loss 0.636431
[epoch16, step713]: loss 0.687696
[epoch16, step714]: loss 0.599375
[epoch16, step715]: loss 0.540380
[epoch16, step716]: loss 0.418759
[epoch16, step717]: loss 0.540434
[epoch16, step718]: loss 0.549680
[epoch16, step719]: loss 0.584897
[epoch16, step720]: loss 0.597900
[epoch16, step721]: loss 0.383734
[epoch16, step722]: loss 0.375199
[epoch16, step723]: loss 0.395217
[epoch16, step724]: loss 0.323996
[epoch16, step725]: loss 0.452993
[epoch16, step726]: loss 0.614214
[epoch16, step727]: loss 0.525300
[epoch16, step728]: loss 0.624767
[epoch16, step729]: loss 0.681341
[epoch16, step730]: loss 0.388997
[epoch16, step731]: loss 0.762674
[epoch16, step732]: loss 0.478175
[epoch16, step733]: loss 0.431203
[epoch16, step734]: loss 0.531748
[epoch16, step735]: loss 0.355819
[epoch16, step736]: loss 0.631491
[epoch16, step737]: loss 0.762142
[epoch16, step738]: loss 0.607339
[epoch16, step739]: loss 0.615051
[epoch16, step740]: loss 0.521661
[epoch16, step741]: loss 0.452181
[epoch16, step742]: loss 0.593632
[epoch16, step743]: loss 0.472064
[epoch16, step744]: loss 0.618593
[epoch16, step745]: loss 0.611266
[epoch16, step746]: loss 0.203183
[epoch16, step747]: loss 0.505156
[epoch16, step748]: loss 0.595800
[epoch16, step749]: loss 0.642768
[epoch16, step750]: loss 0.766163
[epoch16, step751]: loss 0.526519
[epoch16, step752]: loss 0.355331
[epoch16, step753]: loss 0.495068
[epoch16, step754]: loss 0.603512
[epoch16, step755]: loss 0.269182
[epoch16, step756]: loss 0.676772
[epoch16, step757]: loss 0.378958
[epoch16, step758]: loss 0.388878
[epoch16, step759]: loss 0.382397
[epoch16, step760]: loss 0.634127
[epoch16, step761]: loss 0.484378
[epoch16, step762]: loss 0.611096
[epoch16, step763]: loss 0.496697
[epoch16, step764]: loss 0.739453
[epoch16, step765]: loss 0.680516
[epoch16, step766]: loss 0.590869
[epoch16, step767]: loss 0.492605
[epoch16, step768]: loss 0.370288
[epoch16, step769]: loss 0.610373
[epoch16, step770]: loss 0.840489
[epoch16, step771]: loss 0.757088
[epoch16, step772]: loss 0.644763
[epoch16, step773]: loss 0.437720
[epoch16, step774]: loss 0.621544
[epoch16, step775]: loss 0.893754
[epoch16, step776]: loss 0.646568
[epoch16, step777]: loss 0.402466
[epoch16, step778]: loss 0.299319
[epoch16, step779]: loss 0.350561
[epoch16, step780]: loss 0.312657
[epoch16, step781]: loss 0.488319
[epoch16, step782]: loss 0.369047
[epoch16, step783]: loss 0.769513
[epoch16, step784]: loss 0.681341
[epoch16, step785]: loss 0.520564
[epoch16, step786]: loss 0.576751
[epoch16, step787]: loss 0.488466
[epoch16, step788]: loss 0.339443
[epoch16, step789]: loss 0.634423
[epoch16, step790]: loss 0.485165
[epoch16, step791]: loss 0.476882
[epoch16, step792]: loss 0.538472
[epoch16, step793]: loss 0.286007
[epoch16, step794]: loss 0.489379
[epoch16, step795]: loss 0.628136
[epoch16, step796]: loss 0.389070
[epoch16, step797]: loss 0.476928
[epoch16, step798]: loss 0.567676
[epoch16, step799]: loss 0.512327
[epoch16, step800]: loss 0.525275
[epoch16, step801]: loss 0.517734
[epoch16, step802]: loss 0.665190
[epoch16, step803]: loss 0.432796
[epoch16, step804]: loss 0.638120
[epoch16, step805]: loss 0.611830
[epoch16, step806]: loss 0.236282
[epoch16, step807]: loss 0.485663
[epoch16, step808]: loss 0.123172
[epoch16, step809]: loss 0.511165
[epoch16, step810]: loss 0.341931
[epoch16, step811]: loss 0.674235
[epoch16, step812]: loss 0.538056
[epoch16, step813]: loss 0.506319
[epoch16, step814]: loss 0.421625
[epoch16, step815]: loss 0.653873
[epoch16, step816]: loss 0.606462
[epoch16, step817]: loss 0.631605
[epoch16, step818]: loss 0.593807
[epoch16, step819]: loss 0.551582
[epoch16, step820]: loss 0.375552
[epoch16, step821]: loss 0.777497
[epoch16, step822]: loss 0.739529
[epoch16, step823]: loss 0.703631
[epoch16, step824]: loss 0.713157
[epoch16, step825]: loss 0.400432
[epoch16, step826]: loss 0.501418
[epoch16, step827]: loss 0.421056
[epoch16, step828]: loss 0.452303
[epoch16, step829]: loss 0.453827
[epoch16, step830]: loss 0.398006
[epoch16, step831]: loss 0.663030
[epoch16, step832]: loss 0.630630
[epoch16, step833]: loss 0.573281
[epoch16, step834]: loss 0.531490
[epoch16, step835]: loss 0.502957
[epoch16, step836]: loss 0.575592
[epoch16, step837]: loss 0.449719
[epoch16, step838]: loss 0.809073
[epoch16, step839]: loss 0.476076
[epoch16, step840]: loss 0.719416
[epoch16, step841]: loss 0.571573
[epoch16, step842]: loss 0.583830
[epoch16, step843]: loss 0.567779
[epoch16, step844]: loss 0.691792
[epoch16, step845]: loss 0.553062
[epoch16, step846]: loss 0.331812
[epoch16, step847]: loss 0.401083
[epoch16, step848]: loss 0.621739
[epoch16, step849]: loss 0.414185
[epoch16, step850]: loss 0.384341
[epoch16, step851]: loss 0.682627
[epoch16, step852]: loss 0.410686
[epoch16, step853]: loss 0.562006
[epoch16, step854]: loss 0.455947
[epoch16, step855]: loss 0.516036
[epoch16, step856]: loss 0.341777
[epoch16, step857]: loss 0.799286
[epoch16, step858]: loss 0.681965
[epoch16, step859]: loss 0.727115
[epoch16, step860]: loss 0.414763
[epoch16, step861]: loss 0.665263
[epoch16, step862]: loss 0.407822
[epoch16, step863]: loss 0.694964
[epoch16, step864]: loss 0.555408
[epoch16, step865]: loss 0.730769
[epoch16, step866]: loss 0.379221
[epoch16, step867]: loss 0.779874
[epoch16, step868]: loss 0.397045
[epoch16, step869]: loss 0.658640
[epoch16, step870]: loss 0.644615
[epoch16, step871]: loss 0.411649
[epoch16, step872]: loss 0.487917
[epoch16, step873]: loss 0.555971
[epoch16, step874]: loss 0.565294
[epoch16, step875]: loss 0.728891
[epoch16, step876]: loss 0.601753
[epoch16, step877]: loss 0.580542
[epoch16, step878]: loss 0.372004
[epoch16, step879]: loss 0.281813
[epoch16, step880]: loss 0.094943
[epoch16, step881]: loss 0.547771
[epoch16, step882]: loss 0.630490
[epoch16, step883]: loss 0.572324
[epoch16, step884]: loss 0.474665
[epoch16, step885]: loss 0.667136
[epoch16, step886]: loss 0.499165
[epoch16, step887]: loss 0.600920
[epoch16, step888]: loss 0.658244
[epoch16, step889]: loss 0.543622
[epoch16, step890]: loss 0.601394
[epoch16, step891]: loss 0.607165
[epoch16, step892]: loss 0.733621
[epoch16, step893]: loss 0.510798
[epoch16, step894]: loss 0.548846
[epoch16, step895]: loss 0.468919
[epoch16, step896]: loss 0.445109
[epoch16, step897]: loss 0.531065
[epoch16, step898]: loss 0.560009
[epoch16, step899]: loss 0.670771
[epoch16, step900]: loss 0.404019
[epoch16, step901]: loss 0.757379
[epoch16, step902]: loss 0.571460
[epoch16, step903]: loss 0.542079
[epoch16, step904]: loss 0.531333
[epoch16, step905]: loss 0.472415
[epoch16, step906]: loss 0.493591
[epoch16, step907]: loss 0.688940
[epoch16, step908]: loss 0.535595
[epoch16, step909]: loss 0.696036
[epoch16, step910]: loss 0.663298
[epoch16, step911]: loss 0.332579
[epoch16, step912]: loss 0.496879
[epoch16, step913]: loss 0.595970
[epoch16, step914]: loss 0.667843
[epoch16, step915]: loss 0.416901
[epoch16, step916]: loss 0.922123
[epoch16, step917]: loss 0.583049
[epoch16, step918]: loss 0.580318
[epoch16, step919]: loss 0.625677
[epoch16, step920]: loss 0.650023
[epoch16, step921]: loss 0.676027
[epoch16, step922]: loss 0.410256
[epoch16, step923]: loss 0.258839
[epoch16, step924]: loss 0.623192
[epoch16, step925]: loss 0.584743
[epoch16, step926]: loss 0.655705
[epoch16, step927]: loss 0.603846
[epoch16, step928]: loss 0.689730
[epoch16, step929]: loss 0.707755
[epoch16, step930]: loss 0.502635
[epoch16, step931]: loss 0.646058
[epoch16, step932]: loss 0.299049
[epoch16, step933]: loss 0.424084
[epoch16, step934]: loss 0.581450
[epoch16, step935]: loss 0.309925
[epoch16, step936]: loss 0.504497
[epoch16, step937]: loss 0.477168
[epoch16, step938]: loss 0.809222
[epoch16, step939]: loss 0.499550
[epoch16, step940]: loss 0.541043
[epoch16, step941]: loss 0.434284
[epoch16, step942]: loss 0.502192
[epoch16, step943]: loss 0.490531
[epoch16, step944]: loss 0.809818
[epoch16, step945]: loss 0.536602
[epoch16, step946]: loss 0.696062
[epoch16, step947]: loss 0.516136
[epoch16, step948]: loss 0.717954
[epoch16, step949]: loss 0.532302
[epoch16, step950]: loss 0.472111
[epoch16, step951]: loss 0.484581
[epoch16, step952]: loss 0.474150
[epoch16, step953]: loss 0.337558
[epoch16, step954]: loss 0.296667
[epoch16, step955]: loss 0.680030
[epoch16, step956]: loss 0.423673
[epoch16, step957]: loss 0.544890
[epoch16, step958]: loss 0.399066
[epoch16, step959]: loss 0.492164
[epoch16, step960]: loss 0.467328
[epoch16, step961]: loss 0.645148
[epoch16, step962]: loss 0.490281
[epoch16, step963]: loss 0.494616
[epoch16, step964]: loss 0.655474
[epoch16, step965]: loss 0.457363
[epoch16, step966]: loss 0.591733
[epoch16, step967]: loss 0.736793
[epoch16, step968]: loss 0.332552
[epoch16, step969]: loss 0.487381
[epoch16, step970]: loss 0.341799
[epoch16, step971]: loss 0.574286
[epoch16, step972]: loss 0.418337
[epoch16, step973]: loss 0.646540
[epoch16, step974]: loss 0.475709
[epoch16, step975]: loss 0.450753
[epoch16, step976]: loss 0.186910
[epoch16, step977]: loss 0.317560
[epoch16, step978]: loss 0.414062
[epoch16, step979]: loss 0.455817
[epoch16, step980]: loss 0.621159
[epoch16, step981]: loss 0.506119
[epoch16, step982]: loss 0.604747
[epoch16, step983]: loss 0.431474
[epoch16, step984]: loss 0.641503
[epoch16, step985]: loss 0.659115
[epoch16, step986]: loss 0.385769
[epoch16, step987]: loss 0.804939
[epoch16, step988]: loss 0.642067
[epoch16, step989]: loss 0.711438
[epoch16, step990]: loss 0.522626
[epoch16, step991]: loss 0.656187
[epoch16, step992]: loss 0.517201
[epoch16, step993]: loss 0.440277
[epoch16, step994]: loss 0.730898
[epoch16, step995]: loss 0.585207
[epoch16, step996]: loss 0.653632
[epoch16, step997]: loss 0.676840
[epoch16, step998]: loss 0.537656
[epoch16, step999]: loss 0.726373
[epoch16, step1000]: loss 0.596083
[epoch16, step1001]: loss 0.344513
[epoch16, step1002]: loss 0.587798
[epoch16, step1003]: loss 0.536210
[epoch16, step1004]: loss 0.699745
[epoch16, step1005]: loss 0.602864
[epoch16, step1006]: loss 0.630217
[epoch16, step1007]: loss 0.335240
[epoch16, step1008]: loss 0.303297
[epoch16, step1009]: loss 0.353012
[epoch16, step1010]: loss 0.366975
[epoch16, step1011]: loss 0.429661
[epoch16, step1012]: loss 0.597833
[epoch16, step1013]: loss 0.543004
[epoch16, step1014]: loss 0.410010
[epoch16, step1015]: loss 0.449937
[epoch16, step1016]: loss 0.439594
[epoch16, step1017]: loss 0.548808
[epoch16, step1018]: loss 0.644105
[epoch16, step1019]: loss 0.708328
[epoch16, step1020]: loss 0.679345
[epoch16, step1021]: loss 0.602821
[epoch16, step1022]: loss 0.259991
[epoch16, step1023]: loss 0.475983
[epoch16, step1024]: loss 0.450290
[epoch16, step1025]: loss 0.738387
[epoch16, step1026]: loss 0.517344
[epoch16, step1027]: loss 0.602565
[epoch16, step1028]: loss 0.345333
[epoch16, step1029]: loss 0.666091
[epoch16, step1030]: loss 0.510857
[epoch16, step1031]: loss 0.511546
[epoch16, step1032]: loss 0.590427
[epoch16, step1033]: loss 0.612048
[epoch16, step1034]: loss 0.652609
[epoch16, step1035]: loss 0.561469
[epoch16, step1036]: loss 0.705863
[epoch16, step1037]: loss 0.625566
[epoch16, step1038]: loss 0.475262
[epoch16, step1039]: loss 0.820519
[epoch16, step1040]: loss 0.400755
[epoch16, step1041]: loss 0.612732
[epoch16, step1042]: loss 0.529533
[epoch16, step1043]: loss 0.519505
[epoch16, step1044]: loss 0.407196
[epoch16, step1045]: loss 0.254255
[epoch16, step1046]: loss 0.330091
[epoch16, step1047]: loss 0.583133
[epoch16, step1048]: loss 0.625787
[epoch16, step1049]: loss 0.365982
[epoch16, step1050]: loss 0.778382
[epoch16, step1051]: loss 0.582369
[epoch16, step1052]: loss 0.425154
[epoch16, step1053]: loss 0.424065
[epoch16, step1054]: loss 0.256076
[epoch16, step1055]: loss 0.467018
[epoch16, step1056]: loss 0.590741
[epoch16, step1057]: loss 0.406824
[epoch16, step1058]: loss 0.598031
[epoch16, step1059]: loss 0.454845
[epoch16, step1060]: loss 0.558588
[epoch16, step1061]: loss 0.719390
[epoch16, step1062]: loss 0.368659
[epoch16, step1063]: loss 0.714165
[epoch16, step1064]: loss 0.438276
[epoch16, step1065]: loss 0.484346
[epoch16, step1066]: loss 0.753630
[epoch16, step1067]: loss 0.589552
[epoch16, step1068]: loss 0.559898
[epoch16, step1069]: loss 0.662476
[epoch16, step1070]: loss 0.621575
[epoch16, step1071]: loss 0.647295
[epoch16, step1072]: loss 0.681778
[epoch16, step1073]: loss 0.491291
[epoch16, step1074]: loss 0.475223
[epoch16, step1075]: loss 0.542517
[epoch16, step1076]: loss 0.528672
[epoch16, step1077]: loss 0.709492
[epoch16, step1078]: loss 0.325963
[epoch16, step1079]: loss 0.423056
[epoch16, step1080]: loss 0.546589
[epoch16, step1081]: loss 0.613989
[epoch16, step1082]: loss 0.661165
[epoch16, step1083]: loss 0.713134
[epoch16, step1084]: loss 0.702776
[epoch16, step1085]: loss 0.545718
[epoch16, step1086]: loss 0.664260
[epoch16, step1087]: loss 0.535686
[epoch16, step1088]: loss 0.496534
[epoch16, step1089]: loss 0.619305
[epoch16, step1090]: loss 0.495744
[epoch16, step1091]: loss 0.672651
[epoch16, step1092]: loss 0.595294
[epoch16, step1093]: loss 0.501667
[epoch16, step1094]: loss 0.626608
[epoch16, step1095]: loss 0.455408
[epoch16, step1096]: loss 0.564558
[epoch16, step1097]: loss 0.481360
[epoch16, step1098]: loss 0.560500
[epoch16, step1099]: loss 0.698730
[epoch16, step1100]: loss 0.674353
[epoch16, step1101]: loss 0.394863
[epoch16, step1102]: loss 0.441732
[epoch16, step1103]: loss 0.594315
[epoch16, step1104]: loss 0.504564
[epoch16, step1105]: loss 0.464094
[epoch16, step1106]: loss 0.595898
[epoch16, step1107]: loss 0.391893
[epoch16, step1108]: loss 0.563111
[epoch16, step1109]: loss 0.711375
[epoch16, step1110]: loss 0.505020
[epoch16, step1111]: loss 0.609816
[epoch16, step1112]: loss 0.591066
[epoch16, step1113]: loss 0.606913
[epoch16, step1114]: loss 0.473168
[epoch16, step1115]: loss 0.685569
[epoch16, step1116]: loss 0.371480
[epoch16, step1117]: loss 0.590319
[epoch16, step1118]: loss 0.384314
[epoch16, step1119]: loss 0.601604
[epoch16, step1120]: loss 0.521894
[epoch16, step1121]: loss 0.449637
[epoch16, step1122]: loss 0.515447
[epoch16, step1123]: loss 0.650733
[epoch16, step1124]: loss 0.416357
[epoch16, step1125]: loss 0.489158
[epoch16, step1126]: loss 0.724082
[epoch16, step1127]: loss 0.502867
[epoch16, step1128]: loss 0.190799
[epoch16, step1129]: loss 0.526684
[epoch16, step1130]: loss 0.722105
[epoch16, step1131]: loss 0.514691
[epoch16, step1132]: loss 0.543546
[epoch16, step1133]: loss 0.521939
[epoch16, step1134]: loss 0.647438
[epoch16, step1135]: loss 0.553383
[epoch16, step1136]: loss 0.564854
[epoch16, step1137]: loss 0.652143
[epoch16, step1138]: loss 0.568926
[epoch16, step1139]: loss 0.643485
[epoch16, step1140]: loss 0.453600
[epoch16, step1141]: loss 0.481524
[epoch16, step1142]: loss 0.527988
[epoch16, step1143]: loss 0.515923
[epoch16, step1144]: loss 0.386720
[epoch16, step1145]: loss 0.100920
[epoch16, step1146]: loss 0.624875
[epoch16, step1147]: loss 0.451379
[epoch16, step1148]: loss 0.636726
[epoch16, step1149]: loss 0.454131
[epoch16, step1150]: loss 0.506295
[epoch16, step1151]: loss 0.667603
[epoch16, step1152]: loss 0.428951
[epoch16, step1153]: loss 0.658423
[epoch16, step1154]: loss 0.813242
[epoch16, step1155]: loss 0.111040
[epoch16, step1156]: loss 0.513775
[epoch16, step1157]: loss 0.582302
[epoch16, step1158]: loss 0.548209
[epoch16, step1159]: loss 0.537763
[epoch16, step1160]: loss 0.544501
[epoch16, step1161]: loss 0.176886
[epoch16, step1162]: loss 0.546163
[epoch16, step1163]: loss 0.371109
[epoch16, step1164]: loss 0.599362
[epoch16, step1165]: loss 0.605376
[epoch16, step1166]: loss 0.595868
[epoch16, step1167]: loss 0.370561
[epoch16, step1168]: loss 0.602336
[epoch16, step1169]: loss 0.424986
[epoch16, step1170]: loss 0.462302
[epoch16, step1171]: loss 0.520952
[epoch16, step1172]: loss 0.656633
[epoch16, step1173]: loss 0.443946
[epoch16, step1174]: loss 0.386534
[epoch16, step1175]: loss 0.536324
[epoch16, step1176]: loss 0.712258
[epoch16, step1177]: loss 0.563621
[epoch16, step1178]: loss 0.296785
[epoch16, step1179]: loss 0.581091
[epoch16, step1180]: loss 0.598748
[epoch16, step1181]: loss 0.264668
[epoch16, step1182]: loss 0.523232
[epoch16, step1183]: loss 0.899772
[epoch16, step1184]: loss 0.584071
[epoch16, step1185]: loss 0.603344
[epoch16, step1186]: loss 0.379832
[epoch16, step1187]: loss 0.558222
[epoch16, step1188]: loss 0.580093
[epoch16, step1189]: loss 0.470235
[epoch16, step1190]: loss 0.208228
[epoch16, step1191]: loss 0.620432
[epoch16, step1192]: loss 0.353455
[epoch16, step1193]: loss 0.637126
[epoch16, step1194]: loss 0.640101
[epoch16, step1195]: loss 0.287318
[epoch16, step1196]: loss 0.622608
[epoch16, step1197]: loss 0.536502
[epoch16, step1198]: loss 0.349606
[epoch16, step1199]: loss 0.515436
[epoch16, step1200]: loss 0.384332
[epoch16, step1201]: loss 0.334986
[epoch16, step1202]: loss 0.576645
[epoch16, step1203]: loss 0.586284
[epoch16, step1204]: loss 0.667785
[epoch16, step1205]: loss 0.485694
[epoch16, step1206]: loss 0.746919
[epoch16, step1207]: loss 0.572072
[epoch16, step1208]: loss 0.367839
[epoch16, step1209]: loss 0.431326
[epoch16, step1210]: loss 0.504119
[epoch16, step1211]: loss 0.570298
[epoch16, step1212]: loss 0.679353
[epoch16, step1213]: loss 0.506534
[epoch16, step1214]: loss 0.682432
[epoch16, step1215]: loss 0.549108
[epoch16, step1216]: loss 0.344588
[epoch16, step1217]: loss 0.584452
[epoch16, step1218]: loss 0.571914
[epoch16, step1219]: loss 0.597301
[epoch16, step1220]: loss 0.646913
[epoch16, step1221]: loss 0.720582
[epoch16, step1222]: loss 0.408596
[epoch16, step1223]: loss 0.812374
[epoch16, step1224]: loss 0.784198
[epoch16, step1225]: loss 0.747301
[epoch16, step1226]: loss 0.577157
[epoch16, step1227]: loss 0.478351
[epoch16, step1228]: loss 0.542311
[epoch16, step1229]: loss 0.509268
[epoch16, step1230]: loss 0.563052
[epoch16, step1231]: loss 0.489353
[epoch16, step1232]: loss 0.690671
[epoch16, step1233]: loss 0.586493
[epoch16, step1234]: loss 0.664423
[epoch16, step1235]: loss 0.301695
[epoch16, step1236]: loss 0.436675
[epoch16, step1237]: loss 0.583280
[epoch16, step1238]: loss 0.470829
[epoch16, step1239]: loss 0.240208
[epoch16, step1240]: loss 0.529316
[epoch16, step1241]: loss 0.691642
[epoch16, step1242]: loss 0.759812
[epoch16, step1243]: loss 0.731037
[epoch16, step1244]: loss 0.798793
[epoch16, step1245]: loss 0.787217
[epoch16, step1246]: loss 0.566927
[epoch16, step1247]: loss 0.454382
[epoch16, step1248]: loss 0.335263
[epoch16, step1249]: loss 0.822273
[epoch16, step1250]: loss 0.666654
[epoch16, step1251]: loss 0.237012
[epoch16, step1252]: loss 0.564044
[epoch16, step1253]: loss 0.591242
[epoch16, step1254]: loss 0.560402
[epoch16, step1255]: loss 0.583367
[epoch16, step1256]: loss 0.136797
[epoch16, step1257]: loss 0.792466
[epoch16, step1258]: loss 0.545443
[epoch16, step1259]: loss 0.401405
[epoch16, step1260]: loss 0.413886
[epoch16, step1261]: loss 0.508605
[epoch16, step1262]: loss 0.467527
[epoch16, step1263]: loss 0.522890
[epoch16, step1264]: loss 0.312694
[epoch16, step1265]: loss 0.563400
[epoch16, step1266]: loss 0.548541
[epoch16, step1267]: loss 0.802113
[epoch16, step1268]: loss 0.543201
[epoch16, step1269]: loss 0.650779
[epoch16, step1270]: loss 0.628475
[epoch16, step1271]: loss 0.578141
[epoch16, step1272]: loss 0.486258
[epoch16, step1273]: loss 0.566018
[epoch16, step1274]: loss 0.615019
[epoch16, step1275]: loss 0.535249
[epoch16, step1276]: loss 0.497150
[epoch16, step1277]: loss 0.703915
[epoch16, step1278]: loss 0.594540
[epoch16, step1279]: loss 0.778034
[epoch16, step1280]: loss 0.568978
[epoch16, step1281]: loss 0.302050
[epoch16, step1282]: loss 0.616855
[epoch16, step1283]: loss 0.419569
[epoch16, step1284]: loss 0.637495
[epoch16, step1285]: loss 0.674020
[epoch16, step1286]: loss 0.454769
[epoch16, step1287]: loss 0.667295
[epoch16, step1288]: loss 0.712943
[epoch16, step1289]: loss 0.735764
[epoch16, step1290]: loss 0.444000
[epoch16, step1291]: loss 0.559133
[epoch16, step1292]: loss 0.418353
[epoch16, step1293]: loss 0.749714
[epoch16, step1294]: loss 0.543726
[epoch16, step1295]: loss 0.327296
[epoch16, step1296]: loss 0.677501
[epoch16, step1297]: loss 0.315239
[epoch16, step1298]: loss 0.667729
[epoch16, step1299]: loss 0.477748
[epoch16, step1300]: loss 0.747379
[epoch16, step1301]: loss 0.323711
[epoch16, step1302]: loss 0.514975
[epoch16, step1303]: loss 0.648113
[epoch16, step1304]: loss 0.472413
[epoch16, step1305]: loss 0.456370
[epoch16, step1306]: loss 0.618936
[epoch16, step1307]: loss 0.375791
[epoch16, step1308]: loss 0.367343
[epoch16, step1309]: loss 0.720957
[epoch16, step1310]: loss 0.547960
[epoch16, step1311]: loss 0.770968
[epoch16, step1312]: loss 0.416206
[epoch16, step1313]: loss 0.646694
[epoch16, step1314]: loss 0.892312
[epoch16, step1315]: loss 0.412649
[epoch16, step1316]: loss 0.437794
[epoch16, step1317]: loss 0.772787
[epoch16, step1318]: loss 0.397135
[epoch16, step1319]: loss 0.472799
[epoch16, step1320]: loss 0.416058
[epoch16, step1321]: loss 0.447145
[epoch16, step1322]: loss 0.589644
[epoch16, step1323]: loss 0.520812
[epoch16, step1324]: loss 0.298670
[epoch16, step1325]: loss 0.362919
[epoch16, step1326]: loss 0.470590
[epoch16, step1327]: loss 0.546185
[epoch16, step1328]: loss 0.452163
[epoch16, step1329]: loss 0.507977
[epoch16, step1330]: loss 0.543713
[epoch16, step1331]: loss 0.561815
[epoch16, step1332]: loss 0.603522
[epoch16, step1333]: loss 0.472067
[epoch16, step1334]: loss 0.383418
[epoch16, step1335]: loss 0.375267
[epoch16, step1336]: loss 0.637070
[epoch16, step1337]: loss 0.482437
[epoch16, step1338]: loss 0.587478
[epoch16, step1339]: loss 0.499824
[epoch16, step1340]: loss 0.420573
[epoch16, step1341]: loss 0.643573
[epoch16, step1342]: loss 0.605815
[epoch16, step1343]: loss 0.462191
[epoch16, step1344]: loss 0.644765
[epoch16, step1345]: loss 0.608927
[epoch16, step1346]: loss 0.653662
[epoch16, step1347]: loss 0.330327
[epoch16, step1348]: loss 0.566900
[epoch16, step1349]: loss 0.408009
[epoch16, step1350]: loss 0.656171
[epoch16, step1351]: loss 0.524417
[epoch16, step1352]: loss 0.341910
[epoch16, step1353]: loss 0.465357
[epoch16, step1354]: loss 0.603674
[epoch16, step1355]: loss 0.500840
[epoch16, step1356]: loss 0.485647
[epoch16, step1357]: loss 0.409141
[epoch16, step1358]: loss 0.244123
[epoch16, step1359]: loss 0.341985
[epoch16, step1360]: loss 0.408411
[epoch16, step1361]: loss 0.610785
[epoch16, step1362]: loss 0.756349
[epoch16, step1363]: loss 0.315884
[epoch16, step1364]: loss 0.557803
[epoch16, step1365]: loss 0.597027
[epoch16, step1366]: loss 0.490519
[epoch16, step1367]: loss 0.626299
[epoch16, step1368]: loss 0.814553
[epoch16, step1369]: loss 0.553337
[epoch16, step1370]: loss 0.597782
[epoch16, step1371]: loss 0.659070
[epoch16, step1372]: loss 0.743213
[epoch16, step1373]: loss 0.680045
[epoch16, step1374]: loss 0.711111
[epoch16, step1375]: loss 0.438511
[epoch16, step1376]: loss 0.550735
[epoch16, step1377]: loss 0.566511
[epoch16, step1378]: loss 0.543089
[epoch16, step1379]: loss 0.288994
[epoch16, step1380]: loss 0.629232
[epoch16, step1381]: loss 0.383069
[epoch16, step1382]: loss 0.433853
[epoch16, step1383]: loss 0.511960
[epoch16, step1384]: loss 0.591014
[epoch16, step1385]: loss 0.547830
[epoch16, step1386]: loss 0.567686
[epoch16, step1387]: loss 0.519112
[epoch16, step1388]: loss 0.605027
[epoch16, step1389]: loss 0.598658
[epoch16, step1390]: loss 0.548834
[epoch16, step1391]: loss 0.611072
[epoch16, step1392]: loss 0.493836
[epoch16, step1393]: loss 0.361726
[epoch16, step1394]: loss 0.450914
[epoch16, step1395]: loss 0.581943
[epoch16, step1396]: loss 0.311966
[epoch16, step1397]: loss 0.659809
[epoch16, step1398]: loss 0.459388
[epoch16, step1399]: loss 0.647700
[epoch16, step1400]: loss 0.385087
[epoch16, step1401]: loss 0.736738
[epoch16, step1402]: loss 0.591670
[epoch16, step1403]: loss 0.515110
[epoch16, step1404]: loss 0.258872
[epoch16, step1405]: loss 0.621056
[epoch16, step1406]: loss 0.392717
[epoch16, step1407]: loss 0.384685
[epoch16, step1408]: loss 0.657948
[epoch16, step1409]: loss 0.643393
[epoch16, step1410]: loss 0.388555
[epoch16, step1411]: loss 0.499977
[epoch16, step1412]: loss 0.427510
[epoch16, step1413]: loss 0.539295
[epoch16, step1414]: loss 0.478117
[epoch16, step1415]: loss 0.328236
[epoch16, step1416]: loss 0.848080
[epoch16, step1417]: loss 0.460824
[epoch16, step1418]: loss 0.443938
[epoch16, step1419]: loss 0.448485
[epoch16, step1420]: loss 0.708941
[epoch16, step1421]: loss 0.694626
[epoch16, step1422]: loss 0.552627
[epoch16, step1423]: loss 0.600819
[epoch16, step1424]: loss 0.639286
[epoch16, step1425]: loss 0.687354
[epoch16, step1426]: loss 0.372928
[epoch16, step1427]: loss 0.382293
[epoch16, step1428]: loss 0.534601
[epoch16, step1429]: loss 0.404015
[epoch16, step1430]: loss 0.436105
[epoch16, step1431]: loss 0.480919
[epoch16, step1432]: loss 0.469878
[epoch16, step1433]: loss 0.271129
[epoch16, step1434]: loss 0.678676
[epoch16, step1435]: loss 0.646249
[epoch16, step1436]: loss 0.613982
[epoch16, step1437]: loss 0.555009
[epoch16, step1438]: loss 0.389319
[epoch16, step1439]: loss 0.505088
[epoch16, step1440]: loss 0.753967
[epoch16, step1441]: loss 0.540371
[epoch16, step1442]: loss 0.398878
[epoch16, step1443]: loss 0.607992
[epoch16, step1444]: loss 0.554633
[epoch16, step1445]: loss 0.539455
[epoch16, step1446]: loss 0.723619
[epoch16, step1447]: loss 0.401796
[epoch16, step1448]: loss 0.345211
[epoch16, step1449]: loss 0.633418
[epoch16, step1450]: loss 0.475607
[epoch16, step1451]: loss 0.530392
[epoch16, step1452]: loss 0.416454
[epoch16, step1453]: loss 0.686244
[epoch16, step1454]: loss 0.504245
[epoch16, step1455]: loss 0.693676
[epoch16, step1456]: loss 0.564115
[epoch16, step1457]: loss 0.460833
[epoch16, step1458]: loss 0.273129
[epoch16, step1459]: loss 0.520230
[epoch16, step1460]: loss 0.818905
[epoch16, step1461]: loss 0.488640
[epoch16, step1462]: loss 0.542242
[epoch16, step1463]: loss 0.576096
[epoch16, step1464]: loss 0.663288
[epoch16, step1465]: loss 0.546602
[epoch16, step1466]: loss 0.690469
[epoch16, step1467]: loss 0.626676
[epoch16, step1468]: loss 0.682749
[epoch16, step1469]: loss 0.647755
[epoch16, step1470]: loss 0.515216
[epoch16, step1471]: loss 0.510323
[epoch16, step1472]: loss 0.535927
[epoch16, step1473]: loss 0.499441
[epoch16, step1474]: loss 0.618796
[epoch16, step1475]: loss 0.760510
[epoch16, step1476]: loss 0.490651
[epoch16, step1477]: loss 0.493415
[epoch16, step1478]: loss 0.704834
[epoch16, step1479]: loss 0.261161
[epoch16, step1480]: loss 0.682403
[epoch16, step1481]: loss 0.450246
[epoch16, step1482]: loss 0.475942
[epoch16, step1483]: loss 0.794495
[epoch16, step1484]: loss 0.574816
[epoch16, step1485]: loss 0.509595
[epoch16, step1486]: loss 0.601859
[epoch16, step1487]: loss 0.602331
[epoch16, step1488]: loss 0.415482
[epoch16, step1489]: loss 0.730190
[epoch16, step1490]: loss 0.763271
[epoch16, step1491]: loss 0.740312
[epoch16, step1492]: loss 0.517940
[epoch16, step1493]: loss 0.603523
[epoch16, step1494]: loss 0.268941
[epoch16, step1495]: loss 0.514372
[epoch16, step1496]: loss 0.580024
[epoch16, step1497]: loss 0.471111
[epoch16, step1498]: loss 0.542665
[epoch16, step1499]: loss 0.509829
[epoch16, step1500]: loss 0.607004
[epoch16, step1501]: loss 0.384483
[epoch16, step1502]: loss 0.812770
[epoch16, step1503]: loss 0.477354
[epoch16, step1504]: loss 0.613317
[epoch16, step1505]: loss 0.662191
[epoch16, step1506]: loss 0.509963
[epoch16, step1507]: loss 0.456444
[epoch16, step1508]: loss 0.741832
[epoch16, step1509]: loss 0.628555
[epoch16, step1510]: loss 0.665179
[epoch16, step1511]: loss 0.644191
[epoch16, step1512]: loss 0.541678
[epoch16, step1513]: loss 0.706922
[epoch16, step1514]: loss 0.239802
[epoch16, step1515]: loss 0.559820
[epoch16, step1516]: loss 0.554965
[epoch16, step1517]: loss 0.535711
[epoch16, step1518]: loss 0.796933
[epoch16, step1519]: loss 0.625029
[epoch16, step1520]: loss 0.294388
[epoch16, step1521]: loss 0.633671
[epoch16, step1522]: loss 0.570876
[epoch16, step1523]: loss 0.546905
[epoch16, step1524]: loss 0.497528
[epoch16, step1525]: loss 0.586927
[epoch16, step1526]: loss 0.654558
[epoch16, step1527]: loss 0.557653
[epoch16, step1528]: loss 0.373683
[epoch16, step1529]: loss 0.664647
[epoch16, step1530]: loss 0.616256
[epoch16, step1531]: loss 0.384494
[epoch16, step1532]: loss 0.443179
[epoch16, step1533]: loss 0.191204
[epoch16, step1534]: loss 0.486334
[epoch16, step1535]: loss 0.590975
[epoch16, step1536]: loss 0.725932
[epoch16, step1537]: loss 0.436624
[epoch16, step1538]: loss 0.582800
[epoch16, step1539]: loss 0.311127
[epoch16, step1540]: loss 0.785737
[epoch16, step1541]: loss 0.684723
[epoch16, step1542]: loss 0.581190
[epoch16, step1543]: loss 0.427601
[epoch16, step1544]: loss 0.291736
[epoch16, step1545]: loss 0.470543
[epoch16, step1546]: loss 0.585628
[epoch16, step1547]: loss 0.456369
[epoch16, step1548]: loss 0.722382
[epoch16, step1549]: loss 0.791869
[epoch16, step1550]: loss 0.597235
[epoch16, step1551]: loss 0.430553
[epoch16, step1552]: loss 0.390679
[epoch16, step1553]: loss 0.381802
[epoch16, step1554]: loss 0.630197
[epoch16, step1555]: loss 0.376900
[epoch16, step1556]: loss 0.641893
[epoch16, step1557]: loss 0.493844
[epoch16, step1558]: loss 0.404449
[epoch16, step1559]: loss 0.480259
[epoch16, step1560]: loss 0.785134
[epoch16, step1561]: loss 0.551164
[epoch16, step1562]: loss 0.587137
[epoch16, step1563]: loss 0.527606
[epoch16, step1564]: loss 0.688100
[epoch16, step1565]: loss 0.595008
[epoch16, step1566]: loss 0.283418
[epoch16, step1567]: loss 0.264336
[epoch16, step1568]: loss 0.455579
[epoch16, step1569]: loss 0.696339
[epoch16, step1570]: loss 0.444639
[epoch16, step1571]: loss 0.524558
[epoch16, step1572]: loss 0.328367
[epoch16, step1573]: loss 0.615980
[epoch16, step1574]: loss 0.634708
[epoch16, step1575]: loss 0.726982
[epoch16, step1576]: loss 0.315772
[epoch16, step1577]: loss 0.641049
[epoch16, step1578]: loss 0.469989
[epoch16, step1579]: loss 0.538242
[epoch16, step1580]: loss 0.465147
[epoch16, step1581]: loss 0.492016
[epoch16, step1582]: loss 0.608932
[epoch16, step1583]: loss 0.599818
[epoch16, step1584]: loss 0.597781
[epoch16, step1585]: loss 0.574651
[epoch16, step1586]: loss 0.589180
[epoch16, step1587]: loss 0.470355
[epoch16, step1588]: loss 0.528380
[epoch16, step1589]: loss 0.733079
[epoch16, step1590]: loss 0.484703
[epoch16, step1591]: loss 0.607962
[epoch16, step1592]: loss 0.379597
[epoch16, step1593]: loss 0.622854
[epoch16, step1594]: loss 0.665436
[epoch16, step1595]: loss 0.312084
[epoch16, step1596]: loss 0.692393
[epoch16, step1597]: loss 0.510280
[epoch16, step1598]: loss 0.464857
[epoch16, step1599]: loss 0.425887
[epoch16, step1600]: loss 0.567858
[epoch16, step1601]: loss 0.653305
[epoch16, step1602]: loss 0.424871
[epoch16, step1603]: loss 0.470235
[epoch16, step1604]: loss 0.411388
[epoch16, step1605]: loss 0.644031
[epoch16, step1606]: loss 0.612028
[epoch16, step1607]: loss 0.662764
[epoch16, step1608]: loss 0.587514
[epoch16, step1609]: loss 0.494298
[epoch16, step1610]: loss 0.299960
[epoch16, step1611]: loss 0.417665
[epoch16, step1612]: loss 0.549664
[epoch16, step1613]: loss 0.545088
[epoch16, step1614]: loss 0.587865
[epoch16, step1615]: loss 0.527444
[epoch16, step1616]: loss 0.635506
[epoch16, step1617]: loss 0.593669
[epoch16, step1618]: loss 0.332407
[epoch16, step1619]: loss 0.586840
[epoch16, step1620]: loss 0.902703
[epoch16, step1621]: loss 0.400142
[epoch16, step1622]: loss 0.521765
[epoch16, step1623]: loss 0.776056
[epoch16, step1624]: loss 0.612969
[epoch16, step1625]: loss 0.548691
[epoch16, step1626]: loss 0.179041
[epoch16, step1627]: loss 0.555490
[epoch16, step1628]: loss 0.562207
[epoch16, step1629]: loss 0.527531
[epoch16, step1630]: loss 0.459955
[epoch16, step1631]: loss 0.517353
[epoch16, step1632]: loss 0.294416
[epoch16, step1633]: loss 0.737877
[epoch16, step1634]: loss 0.511810
[epoch16, step1635]: loss 0.689884
[epoch16, step1636]: loss 0.359597
[epoch16, step1637]: loss 0.780012
[epoch16, step1638]: loss 0.488964
[epoch16, step1639]: loss 0.817349
[epoch16, step1640]: loss 0.621465
[epoch16, step1641]: loss 0.781447
[epoch16, step1642]: loss 0.444101
[epoch16, step1643]: loss 0.491525
[epoch16, step1644]: loss 0.140836
[epoch16, step1645]: loss 0.576206
[epoch16, step1646]: loss 0.707895
[epoch16, step1647]: loss 0.651308
[epoch16, step1648]: loss 0.367670
[epoch16, step1649]: loss 0.546356
[epoch16, step1650]: loss 0.570081
[epoch16, step1651]: loss 0.731428
[epoch16, step1652]: loss 0.469408
[epoch16, step1653]: loss 0.581949
[epoch16, step1654]: loss 0.666842
[epoch16, step1655]: loss 0.563781
[epoch16, step1656]: loss 0.386688
[epoch16, step1657]: loss 0.607329
[epoch16, step1658]: loss 0.639011
[epoch16, step1659]: loss 0.701283
[epoch16, step1660]: loss 0.770101
[epoch16, step1661]: loss 0.425242
[epoch16, step1662]: loss 0.604106
[epoch16, step1663]: loss 0.520390
[epoch16, step1664]: loss 0.410360
[epoch16, step1665]: loss 0.462226
[epoch16, step1666]: loss 0.444975
[epoch16, step1667]: loss 0.381506
[epoch16, step1668]: loss 0.350778
[epoch16, step1669]: loss 0.552096
[epoch16, step1670]: loss 0.543820
[epoch16, step1671]: loss 0.600263
[epoch16, step1672]: loss 0.679661
[epoch16, step1673]: loss 0.626532
[epoch16, step1674]: loss 0.548828
[epoch16, step1675]: loss 0.563206
[epoch16, step1676]: loss 0.648233
[epoch16, step1677]: loss 0.388038
[epoch16, step1678]: loss 0.671552
[epoch16, step1679]: loss 0.569412
[epoch16, step1680]: loss 0.449844
[epoch16, step1681]: loss 0.464211
[epoch16, step1682]: loss 0.453323
[epoch16, step1683]: loss 0.400092
[epoch16, step1684]: loss 0.612242
[epoch16, step1685]: loss 0.449037
[epoch16, step1686]: loss 0.313276
[epoch16, step1687]: loss 0.377047
[epoch16, step1688]: loss 0.413600
[epoch16, step1689]: loss 0.475131
[epoch16, step1690]: loss 0.333103
[epoch16, step1691]: loss 0.480363
[epoch16, step1692]: loss 0.431710
[epoch16, step1693]: loss 0.560788
[epoch16, step1694]: loss 0.704499
[epoch16, step1695]: loss 0.692208
[epoch16, step1696]: loss 0.627873
[epoch16, step1697]: loss 0.592967
[epoch16, step1698]: loss 0.622733
[epoch16, step1699]: loss 0.653139
[epoch16, step1700]: loss 0.605475
[epoch16, step1701]: loss 0.737896
[epoch16, step1702]: loss 0.532259
[epoch16, step1703]: loss 0.422341
[epoch16, step1704]: loss 0.615687
[epoch16, step1705]: loss 0.641778
[epoch16, step1706]: loss 0.587341
[epoch16, step1707]: loss 0.478089
[epoch16, step1708]: loss 0.517179
[epoch16, step1709]: loss 0.519237
[epoch16, step1710]: loss 0.460670
[epoch16, step1711]: loss 0.504152
[epoch16, step1712]: loss 0.421255
[epoch16, step1713]: loss 0.360673
[epoch16, step1714]: loss 0.516051
[epoch16, step1715]: loss 0.519983
[epoch16, step1716]: loss 0.625835
[epoch16, step1717]: loss 0.746004
[epoch16, step1718]: loss 0.551448
[epoch16, step1719]: loss 0.674872
[epoch16, step1720]: loss 0.571066
[epoch16, step1721]: loss 0.636381
[epoch16, step1722]: loss 0.284415
[epoch16, step1723]: loss 0.888567
[epoch16, step1724]: loss 0.566442
[epoch16, step1725]: loss 0.477972
[epoch16, step1726]: loss 0.505536
[epoch16, step1727]: loss 0.691949
[epoch16, step1728]: loss 0.453796
[epoch16, step1729]: loss 0.655639
[epoch16, step1730]: loss 0.648873
[epoch16, step1731]: loss 0.445635
[epoch16, step1732]: loss 0.465027
[epoch16, step1733]: loss 0.357865
[epoch16, step1734]: loss 0.682671
[epoch16, step1735]: loss 0.444106
[epoch16, step1736]: loss 0.332086
[epoch16, step1737]: loss 0.475058
[epoch16, step1738]: loss 0.541107
[epoch16, step1739]: loss 0.610324
[epoch16, step1740]: loss 0.498148
[epoch16, step1741]: loss 0.563735
[epoch16, step1742]: loss 0.295137
[epoch16, step1743]: loss 0.547084
[epoch16, step1744]: loss 0.493909
[epoch16, step1745]: loss 0.615296
[epoch16, step1746]: loss 0.414830
[epoch16, step1747]: loss 0.517372
[epoch16, step1748]: loss 0.350058
[epoch16, step1749]: loss 0.429387
[epoch16, step1750]: loss 0.553130
[epoch16, step1751]: loss 0.373149
[epoch16, step1752]: loss 0.525034
[epoch16, step1753]: loss 0.181968
[epoch16, step1754]: loss 0.611441
[epoch16, step1755]: loss 0.501430
[epoch16, step1756]: loss 0.402484
[epoch16, step1757]: loss 0.428594
[epoch16, step1758]: loss 0.611057
[epoch16, step1759]: loss 0.718935
[epoch16, step1760]: loss 0.366871
[epoch16, step1761]: loss 0.687287
[epoch16, step1762]: loss 0.526587
[epoch16, step1763]: loss 0.566482
[epoch16, step1764]: loss 0.691088
[epoch16, step1765]: loss 0.524461
[epoch16, step1766]: loss 0.566565
[epoch16, step1767]: loss 0.631158
[epoch16, step1768]: loss 0.318812
[epoch16, step1769]: loss 0.605468
[epoch16, step1770]: loss 0.542572
[epoch16, step1771]: loss 0.681105
[epoch16, step1772]: loss 0.518192
[epoch16, step1773]: loss 0.645897
[epoch16, step1774]: loss 0.503336
[epoch16, step1775]: loss 0.541199
[epoch16, step1776]: loss 0.706808
[epoch16, step1777]: loss 0.283389
[epoch16, step1778]: loss 0.705375
[epoch16, step1779]: loss 0.574247
[epoch16, step1780]: loss 0.190129
[epoch16, step1781]: loss 0.543204
[epoch16, step1782]: loss 0.658465
[epoch16, step1783]: loss 0.675618
[epoch16, step1784]: loss 0.473536
[epoch16, step1785]: loss 0.698449
[epoch16, step1786]: loss 0.563140
[epoch16, step1787]: loss 0.598948
[epoch16, step1788]: loss 0.552314
[epoch16, step1789]: loss 0.504143
[epoch16, step1790]: loss 0.639509
[epoch16, step1791]: loss 0.608181
[epoch16, step1792]: loss 0.688328
[epoch16, step1793]: loss 0.645857
[epoch16, step1794]: loss 0.745191
[epoch16, step1795]: loss 0.476965
[epoch16, step1796]: loss 0.452561
[epoch16, step1797]: loss 0.546752
[epoch16, step1798]: loss 0.637918
[epoch16, step1799]: loss 0.444244
[epoch16, step1800]: loss 0.558426
[epoch16, step1801]: loss 0.639557
[epoch16, step1802]: loss 0.375400
[epoch16, step1803]: loss 0.399914
[epoch16, step1804]: loss 0.527018
[epoch16, step1805]: loss 0.436097
[epoch16, step1806]: loss 0.730553
[epoch16, step1807]: loss 0.598660
[epoch16, step1808]: loss 0.356665
[epoch16, step1809]: loss 0.388450
[epoch16, step1810]: loss 0.488173
[epoch16, step1811]: loss 0.575462
[epoch16, step1812]: loss 0.644728
[epoch16, step1813]: loss 0.477939
[epoch16, step1814]: loss 0.324645
[epoch16, step1815]: loss 0.655250
[epoch16, step1816]: loss 0.728879
[epoch16, step1817]: loss 0.497402
[epoch16, step1818]: loss 0.577855
[epoch16, step1819]: loss 0.432598
[epoch16, step1820]: loss 0.618098
[epoch16, step1821]: loss 0.669905
[epoch16, step1822]: loss 0.365910
[epoch16, step1823]: loss 0.617256
[epoch16, step1824]: loss 0.879975
[epoch16, step1825]: loss 0.505759
[epoch16, step1826]: loss 0.640710
[epoch16, step1827]: loss 0.398563
[epoch16, step1828]: loss 0.531909
[epoch16, step1829]: loss 0.603053
[epoch16, step1830]: loss 0.544616
[epoch16, step1831]: loss 0.403446
[epoch16, step1832]: loss 0.147824
[epoch16, step1833]: loss 0.305271
[epoch16, step1834]: loss 0.550954
[epoch16, step1835]: loss 0.580879
[epoch16, step1836]: loss 0.613908
[epoch16, step1837]: loss 0.520393
[epoch16, step1838]: loss 0.419571
[epoch16, step1839]: loss 0.284488
[epoch16, step1840]: loss 0.639882
[epoch16, step1841]: loss 0.596884
[epoch16, step1842]: loss 0.658011
[epoch16, step1843]: loss 0.663842
[epoch16, step1844]: loss 0.661117
[epoch16, step1845]: loss 0.552374
[epoch16, step1846]: loss 0.552968
[epoch16, step1847]: loss 0.388330
[epoch16, step1848]: loss 0.326790
[epoch16, step1849]: loss 0.493212
[epoch16, step1850]: loss 0.349859
[epoch16, step1851]: loss 0.441576
[epoch16, step1852]: loss 0.580659
[epoch16, step1853]: loss 0.554944
[epoch16, step1854]: loss 0.489115
[epoch16, step1855]: loss 0.537663
[epoch16, step1856]: loss 0.398249
[epoch16, step1857]: loss 0.710920
[epoch16, step1858]: loss 0.680013
[epoch16, step1859]: loss 0.582236
[epoch16, step1860]: loss 0.717544
[epoch16, step1861]: loss 0.354382
[epoch16, step1862]: loss 0.451306
[epoch16, step1863]: loss 0.719893
[epoch16, step1864]: loss 0.806103
[epoch16, step1865]: loss 0.678560
[epoch16, step1866]: loss 0.710227
[epoch16, step1867]: loss 0.473436
[epoch16, step1868]: loss 0.290155
[epoch16, step1869]: loss 0.547171
[epoch16, step1870]: loss 0.343842
[epoch16, step1871]: loss 0.639167
[epoch16, step1872]: loss 0.144371
[epoch16, step1873]: loss 0.579670
[epoch16, step1874]: loss 0.397519
[epoch16, step1875]: loss 0.708332
[epoch16, step1876]: loss 0.735091
[epoch16, step1877]: loss 0.581664
[epoch16, step1878]: loss 0.352534
[epoch16, step1879]: loss 0.317222
[epoch16, step1880]: loss 0.648929
[epoch16, step1881]: loss 0.660104
[epoch16, step1882]: loss 0.773896
[epoch16, step1883]: loss 0.349113
[epoch16, step1884]: loss 0.469214
[epoch16, step1885]: loss 0.595365
[epoch16, step1886]: loss 0.729605
[epoch16, step1887]: loss 0.488178
[epoch16, step1888]: loss 0.323949
[epoch16, step1889]: loss 0.695332
[epoch16, step1890]: loss 0.384181
[epoch16, step1891]: loss 0.532594
[epoch16, step1892]: loss 0.690196
[epoch16, step1893]: loss 0.611893
[epoch16, step1894]: loss 0.705562
[epoch16, step1895]: loss 0.659426
[epoch16, step1896]: loss 0.685320
[epoch16, step1897]: loss 0.619585
[epoch16, step1898]: loss 0.446295
[epoch16, step1899]: loss 0.341056
[epoch16, step1900]: loss 0.479250
[epoch16, step1901]: loss 0.143639
[epoch16, step1902]: loss 0.647389
[epoch16, step1903]: loss 0.328877
[epoch16, step1904]: loss 0.610710
[epoch16, step1905]: loss 0.404506
[epoch16, step1906]: loss 0.419282
[epoch16, step1907]: loss 0.624591
[epoch16, step1908]: loss 0.611874
[epoch16, step1909]: loss 0.299993
[epoch16, step1910]: loss 0.550127
[epoch16, step1911]: loss 0.286972
[epoch16, step1912]: loss 0.844173
[epoch16, step1913]: loss 0.614756
[epoch16, step1914]: loss 0.621835
[epoch16, step1915]: loss 0.566874
[epoch16, step1916]: loss 0.697201
[epoch16, step1917]: loss 0.263970
[epoch16, step1918]: loss 0.237221
[epoch16, step1919]: loss 0.564269
[epoch16, step1920]: loss 0.392247
[epoch16, step1921]: loss 0.592293
[epoch16, step1922]: loss 0.609388
[epoch16, step1923]: loss 0.611577
[epoch16, step1924]: loss 0.746374
[epoch16, step1925]: loss 0.378545
[epoch16, step1926]: loss 0.504774
[epoch16, step1927]: loss 0.625069
[epoch16, step1928]: loss 0.215937
[epoch16, step1929]: loss 0.485246
[epoch16, step1930]: loss 0.303555
[epoch16, step1931]: loss 0.309044
[epoch16, step1932]: loss 0.642436
[epoch16, step1933]: loss 0.387187
[epoch16, step1934]: loss 0.563915
[epoch16, step1935]: loss 0.321730
[epoch16, step1936]: loss 0.620519
[epoch16, step1937]: loss 0.690382
[epoch16, step1938]: loss 0.451648
[epoch16, step1939]: loss 0.587173
[epoch16, step1940]: loss 0.586231
[epoch16, step1941]: loss 0.740902
[epoch16, step1942]: loss 0.710235
[epoch16, step1943]: loss 0.403610
[epoch16, step1944]: loss 0.498620
[epoch16, step1945]: loss 0.485421
[epoch16, step1946]: loss 0.427923
[epoch16, step1947]: loss 0.443111
[epoch16, step1948]: loss 0.754180
[epoch16, step1949]: loss 0.466193
[epoch16, step1950]: loss 0.261093
[epoch16, step1951]: loss 0.593000
[epoch16, step1952]: loss 0.627264
[epoch16, step1953]: loss 0.459452
[epoch16, step1954]: loss 0.644447
[epoch16, step1955]: loss 0.633389
[epoch16, step1956]: loss 0.492066
[epoch16, step1957]: loss 0.287450
[epoch16, step1958]: loss 0.508344
[epoch16, step1959]: loss 0.516392
[epoch16, step1960]: loss 0.240781
[epoch16, step1961]: loss 0.614093
[epoch16, step1962]: loss 0.821214
[epoch16, step1963]: loss 0.617263
[epoch16, step1964]: loss 0.623810
[epoch16, step1965]: loss 0.295509
[epoch16, step1966]: loss 0.706297
[epoch16, step1967]: loss 0.487742
[epoch16, step1968]: loss 0.681021
[epoch16, step1969]: loss 0.511363
[epoch16, step1970]: loss 0.561573
[epoch16, step1971]: loss 0.503839
[epoch16, step1972]: loss 0.579062
[epoch16, step1973]: loss 0.552164
[epoch16, step1974]: loss 0.494246
[epoch16, step1975]: loss 0.189586
[epoch16, step1976]: loss 0.488057
[epoch16, step1977]: loss 0.567545
[epoch16, step1978]: loss 0.504342
[epoch16, step1979]: loss 0.671476
[epoch16, step1980]: loss 0.437935
[epoch16, step1981]: loss 0.690641
[epoch16, step1982]: loss 0.468232
[epoch16, step1983]: loss 0.599006
[epoch16, step1984]: loss 0.170834
[epoch16, step1985]: loss 0.377405
[epoch16, step1986]: loss 0.636175
[epoch16, step1987]: loss 0.550239
[epoch16, step1988]: loss 0.461986
[epoch16, step1989]: loss 0.326290
[epoch16, step1990]: loss 0.648314
[epoch16, step1991]: loss 0.422535
[epoch16, step1992]: loss 0.482949
[epoch16, step1993]: loss 0.474634
[epoch16, step1994]: loss 0.493571
[epoch16, step1995]: loss 0.687298
[epoch16, step1996]: loss 0.671415
[epoch16, step1997]: loss 0.547233
[epoch16, step1998]: loss 0.520464
[epoch16, step1999]: loss 0.501748
[epoch16, step2000]: loss 0.638067
[epoch16, step2001]: loss 0.306817
[epoch16, step2002]: loss 0.405832
[epoch16, step2003]: loss 0.671379
[epoch16, step2004]: loss 0.278552
[epoch16, step2005]: loss 0.773959
[epoch16, step2006]: loss 0.353905
[epoch16, step2007]: loss 0.713972
[epoch16, step2008]: loss 0.429910
[epoch16, step2009]: loss 0.378992
[epoch16, step2010]: loss 0.473481
[epoch16, step2011]: loss 0.521284
[epoch16, step2012]: loss 0.414484
[epoch16, step2013]: loss 0.535548
[epoch16, step2014]: loss 0.615251
[epoch16, step2015]: loss 0.430218
[epoch16, step2016]: loss 0.305519
[epoch16, step2017]: loss 0.417865
[epoch16, step2018]: loss 0.548749
[epoch16, step2019]: loss 0.317571
[epoch16, step2020]: loss 0.767654
[epoch16, step2021]: loss 0.311022
[epoch16, step2022]: loss 0.543265
[epoch16, step2023]: loss 0.664371
[epoch16, step2024]: loss 0.741155
[epoch16, step2025]: loss 0.107568
[epoch16, step2026]: loss 0.295721
[epoch16, step2027]: loss 0.774706
[epoch16, step2028]: loss 0.548565
[epoch16, step2029]: loss 0.571829
[epoch16, step2030]: loss 0.724212
[epoch16, step2031]: loss 0.462685
[epoch16, step2032]: loss 0.648183
[epoch16, step2033]: loss 0.477393
[epoch16, step2034]: loss 0.762364
[epoch16, step2035]: loss 0.454208
[epoch16, step2036]: loss 0.588993
[epoch16, step2037]: loss 0.382459
[epoch16, step2038]: loss 0.271580
[epoch16, step2039]: loss 0.499494
[epoch16, step2040]: loss 0.505877
[epoch16, step2041]: loss 0.565773
[epoch16, step2042]: loss 0.462252
[epoch16, step2043]: loss 0.466015
[epoch16, step2044]: loss 0.668065
[epoch16, step2045]: loss 0.593814
[epoch16, step2046]: loss 0.617495
[epoch16, step2047]: loss 0.504976
[epoch16, step2048]: loss 0.448242
[epoch16, step2049]: loss 0.523608
[epoch16, step2050]: loss 0.500466
[epoch16, step2051]: loss 0.429104
[epoch16, step2052]: loss 0.522218
[epoch16, step2053]: loss 0.482012
[epoch16, step2054]: loss 0.383499
[epoch16, step2055]: loss 0.679250
[epoch16, step2056]: loss 0.727493
[epoch16, step2057]: loss 0.468204
[epoch16, step2058]: loss 0.465958
[epoch16, step2059]: loss 0.412821
[epoch16, step2060]: loss 0.811895
[epoch16, step2061]: loss 0.481103
[epoch16, step2062]: loss 0.746755
[epoch16, step2063]: loss 0.700656
[epoch16, step2064]: loss 0.642796
[epoch16, step2065]: loss 0.438493
[epoch16, step2066]: loss 0.491311
[epoch16, step2067]: loss 0.434829
[epoch16, step2068]: loss 0.726538
[epoch16, step2069]: loss 0.388765
[epoch16, step2070]: loss 0.237575
[epoch16, step2071]: loss 0.567650
[epoch16, step2072]: loss 0.661817
[epoch16, step2073]: loss 0.538241
[epoch16, step2074]: loss 0.610201
[epoch16, step2075]: loss 0.488764
[epoch16, step2076]: loss 0.619240
[epoch16, step2077]: loss 0.573766
[epoch16, step2078]: loss 0.627812
[epoch16, step2079]: loss 0.483824
[epoch16, step2080]: loss 0.483004
[epoch16, step2081]: loss 0.695684
[epoch16, step2082]: loss 0.627401
[epoch16, step2083]: loss 0.564816
[epoch16, step2084]: loss 0.499918
[epoch16, step2085]: loss 0.647786
[epoch16, step2086]: loss 0.706973
[epoch16, step2087]: loss 0.581816
[epoch16, step2088]: loss 0.601151
[epoch16, step2089]: loss 0.487583
[epoch16, step2090]: loss 0.527117
[epoch16, step2091]: loss 0.682311
[epoch16, step2092]: loss 0.399751
[epoch16, step2093]: loss 0.509579
[epoch16, step2094]: loss 0.789892
[epoch16, step2095]: loss 0.088707
[epoch16, step2096]: loss 0.666524
[epoch16, step2097]: loss 0.637592
[epoch16, step2098]: loss 0.566084
[epoch16, step2099]: loss 0.486080
[epoch16, step2100]: loss 0.840670
[epoch16, step2101]: loss 0.299139
[epoch16, step2102]: loss 0.341290
[epoch16, step2103]: loss 0.521053
[epoch16, step2104]: loss 0.563442
[epoch16, step2105]: loss 0.711677
[epoch16, step2106]: loss 0.488509
[epoch16, step2107]: loss 0.561460
[epoch16, step2108]: loss 0.620888
[epoch16, step2109]: loss 0.616647
[epoch16, step2110]: loss 0.598728
[epoch16, step2111]: loss 0.514035
[epoch16, step2112]: loss 0.642516
[epoch16, step2113]: loss 0.537727
[epoch16, step2114]: loss 0.419118
[epoch16, step2115]: loss 0.650017
[epoch16, step2116]: loss 0.522599
[epoch16, step2117]: loss 0.470670
[epoch16, step2118]: loss 0.604946
[epoch16, step2119]: loss 0.508827
[epoch16, step2120]: loss 0.579881
[epoch16, step2121]: loss 0.453041
[epoch16, step2122]: loss 0.484461
[epoch16, step2123]: loss 0.397976
[epoch16, step2124]: loss 0.677811
[epoch16, step2125]: loss 0.353626
[epoch16, step2126]: loss 0.536370
[epoch16, step2127]: loss 0.620986
[epoch16, step2128]: loss 0.264646
[epoch16, step2129]: loss 0.592215
[epoch16, step2130]: loss 0.726358
[epoch16, step2131]: loss 0.566693
[epoch16, step2132]: loss 0.710712
[epoch16, step2133]: loss 0.693799
[epoch16, step2134]: loss 0.371491
[epoch16, step2135]: loss 0.614128
[epoch16, step2136]: loss 0.553464
[epoch16, step2137]: loss 0.547833
[epoch16, step2138]: loss 0.301721
[epoch16, step2139]: loss 0.479391
[epoch16, step2140]: loss 0.583220
[epoch16, step2141]: loss 0.768193
[epoch16, step2142]: loss 0.727604
[epoch16, step2143]: loss 0.514021
[epoch16, step2144]: loss 0.396846
[epoch16, step2145]: loss 0.517373
[epoch16, step2146]: loss 0.570443
[epoch16, step2147]: loss 0.475496
[epoch16, step2148]: loss 0.552784
[epoch16, step2149]: loss 0.568223
[epoch16, step2150]: loss 0.625439
[epoch16, step2151]: loss 0.667829
[epoch16, step2152]: loss 0.321787
[epoch16, step2153]: loss 0.653821
[epoch16, step2154]: loss 0.546894
[epoch16, step2155]: loss 0.741065
[epoch16, step2156]: loss 0.542459
[epoch16, step2157]: loss 0.500526
[epoch16, step2158]: loss 0.374515
[epoch16, step2159]: loss 0.517594
[epoch16, step2160]: loss 0.498378
[epoch16, step2161]: loss 0.676604
[epoch16, step2162]: loss 0.626721
[epoch16, step2163]: loss 0.698014
[epoch16, step2164]: loss 0.658428
[epoch16, step2165]: loss 0.291158
[epoch16, step2166]: loss 0.646103
[epoch16, step2167]: loss 0.646862
[epoch16, step2168]: loss 0.858297
[epoch16, step2169]: loss 0.453296
[epoch16, step2170]: loss 0.474707
[epoch16, step2171]: loss 0.408811
[epoch16, step2172]: loss 0.489132
[epoch16, step2173]: loss 0.891115
[epoch16, step2174]: loss 0.566271
[epoch16, step2175]: loss 0.495034
[epoch16, step2176]: loss 0.385866
[epoch16, step2177]: loss 0.269895
[epoch16, step2178]: loss 0.563810
[epoch16, step2179]: loss 0.495192
[epoch16, step2180]: loss 0.611253
[epoch16, step2181]: loss 0.666683
[epoch16, step2182]: loss 0.381190
[epoch16, step2183]: loss 0.573125
[epoch16, step2184]: loss 0.468505
[epoch16, step2185]: loss 0.655334
[epoch16, step2186]: loss 0.599443
[epoch16, step2187]: loss 0.485226
[epoch16, step2188]: loss 0.577197
[epoch16, step2189]: loss 0.634374
[epoch16, step2190]: loss 0.574250
[epoch16, step2191]: loss 0.675610
[epoch16, step2192]: loss 0.429191
[epoch16, step2193]: loss 0.481746
[epoch16, step2194]: loss 0.447713
[epoch16, step2195]: loss 0.536289
[epoch16, step2196]: loss 0.332054
[epoch16, step2197]: loss 0.287216
[epoch16, step2198]: loss 0.536249
[epoch16, step2199]: loss 0.530641
[epoch16, step2200]: loss 0.702101
[epoch16, step2201]: loss 0.289571
[epoch16, step2202]: loss 0.662133
[epoch16, step2203]: loss 0.455195
[epoch16, step2204]: loss 0.721330
[epoch16, step2205]: loss 0.498437
[epoch16, step2206]: loss 0.712623
[epoch16, step2207]: loss 0.661856
[epoch16, step2208]: loss 0.351914
[epoch16, step2209]: loss 0.637908
[epoch16, step2210]: loss 0.437585
[epoch16, step2211]: loss 0.391754
[epoch16, step2212]: loss 0.402061
[epoch16, step2213]: loss 0.481694
[epoch16, step2214]: loss 0.587320
[epoch16, step2215]: loss 0.497257
[epoch16, step2216]: loss 0.747465
[epoch16, step2217]: loss 0.695764
[epoch16, step2218]: loss 0.519635
[epoch16, step2219]: loss 0.603181
[epoch16, step2220]: loss 0.517901
[epoch16, step2221]: loss 0.603110
[epoch16, step2222]: loss 0.434827
[epoch16, step2223]: loss 0.274530
[epoch16, step2224]: loss 0.538393
[epoch16, step2225]: loss 0.627476
[epoch16, step2226]: loss 0.726654
[epoch16, step2227]: loss 0.576801
[epoch16, step2228]: loss 0.397164
[epoch16, step2229]: loss 0.687466
[epoch16, step2230]: loss 0.484085
[epoch16, step2231]: loss 0.461316
[epoch16, step2232]: loss 0.437775
[epoch16, step2233]: loss 0.566647
[epoch16, step2234]: loss 0.631006
[epoch16, step2235]: loss 0.652929
[epoch16, step2236]: loss 0.522610
[epoch16, step2237]: loss 0.736641
[epoch16, step2238]: loss 0.410557
[epoch16, step2239]: loss 0.588278
[epoch16, step2240]: loss 0.731412
[epoch16, step2241]: loss 0.505133
[epoch16, step2242]: loss 0.705797
[epoch16, step2243]: loss 0.556512
[epoch16, step2244]: loss 0.453253
[epoch16, step2245]: loss 0.495021
[epoch16, step2246]: loss 0.642009
[epoch16, step2247]: loss 0.334113
[epoch16, step2248]: loss 0.499927
[epoch16, step2249]: loss 0.748673
[epoch16, step2250]: loss 0.632844
[epoch16, step2251]: loss 0.464367
[epoch16, step2252]: loss 0.598915
[epoch16, step2253]: loss 0.507737
[epoch16, step2254]: loss 0.282041
[epoch16, step2255]: loss 0.695174
[epoch16, step2256]: loss 0.434794
[epoch16, step2257]: loss 0.640087
[epoch16, step2258]: loss 0.627109
[epoch16, step2259]: loss 0.737956
[epoch16, step2260]: loss 0.599630
[epoch16, step2261]: loss 0.552374
[epoch16, step2262]: loss 0.690427
[epoch16, step2263]: loss 0.598253
[epoch16, step2264]: loss 0.602071
[epoch16, step2265]: loss 0.439649
[epoch16, step2266]: loss 0.588451
[epoch16, step2267]: loss 0.681677
[epoch16, step2268]: loss 0.592768
[epoch16, step2269]: loss 0.384056
[epoch16, step2270]: loss 0.339684
[epoch16, step2271]: loss 0.415653
[epoch16, step2272]: loss 0.557490
[epoch16, step2273]: loss 0.538732
[epoch16, step2274]: loss 0.105586
[epoch16, step2275]: loss 0.616503
[epoch16, step2276]: loss 0.605673
[epoch16, step2277]: loss 0.503917
[epoch16, step2278]: loss 0.431270
[epoch16, step2279]: loss 0.548656
[epoch16, step2280]: loss 0.690342
[epoch16, step2281]: loss 0.333076
[epoch16, step2282]: loss 0.496409
[epoch16, step2283]: loss 0.604025
[epoch16, step2284]: loss 0.361726
[epoch16, step2285]: loss 0.580254
[epoch16, step2286]: loss 0.467804
[epoch16, step2287]: loss 0.597110
[epoch16, step2288]: loss 0.719700
[epoch16, step2289]: loss 0.348711
[epoch16, step2290]: loss 0.490668
[epoch16, step2291]: loss 0.581878
[epoch16, step2292]: loss 0.580075
[epoch16, step2293]: loss 0.429606
[epoch16, step2294]: loss 0.522896
[epoch16, step2295]: loss 0.489278
[epoch16, step2296]: loss 0.714799
[epoch16, step2297]: loss 0.543942
[epoch16, step2298]: loss 0.608777
[epoch16, step2299]: loss 0.494251
[epoch16, step2300]: loss 0.610866
[epoch16, step2301]: loss 0.532952
[epoch16, step2302]: loss 0.485621
[epoch16, step2303]: loss 0.555213
[epoch16, step2304]: loss 0.402337
[epoch16, step2305]: loss 0.762082
[epoch16, step2306]: loss 0.545049
[epoch16, step2307]: loss 0.620930
[epoch16, step2308]: loss 0.578086
[epoch16, step2309]: loss 0.359071
[epoch16, step2310]: loss 0.412964
[epoch16, step2311]: loss 0.595442
[epoch16, step2312]: loss 0.422542
[epoch16, step2313]: loss 0.490024
[epoch16, step2314]: loss 0.497026
[epoch16, step2315]: loss 0.515461
[epoch16, step2316]: loss 0.668461
[epoch16, step2317]: loss 0.530749
[epoch16, step2318]: loss 0.553951
[epoch16, step2319]: loss 0.428919
[epoch16, step2320]: loss 0.660663
[epoch16, step2321]: loss 0.612116
[epoch16, step2322]: loss 0.399500
[epoch16, step2323]: loss 0.526319
[epoch16, step2324]: loss 0.517983
[epoch16, step2325]: loss 0.682159
[epoch16, step2326]: loss 0.633539
[epoch16, step2327]: loss 0.403093
[epoch16, step2328]: loss 0.601164
[epoch16, step2329]: loss 0.532935
[epoch16, step2330]: loss 0.291382
[epoch16, step2331]: loss 0.420010
[epoch16, step2332]: loss 0.577459
[epoch16, step2333]: loss 0.317807
[epoch16, step2334]: loss 0.293865
[epoch16, step2335]: loss 0.762035
[epoch16, step2336]: loss 0.305450
[epoch16, step2337]: loss 0.688185
[epoch16, step2338]: loss 0.370915
[epoch16, step2339]: loss 0.365572
[epoch16, step2340]: loss 0.758047
[epoch16, step2341]: loss 0.591939
[epoch16, step2342]: loss 0.639883
[epoch16, step2343]: loss 0.311750
[epoch16, step2344]: loss 0.746776
[epoch16, step2345]: loss 0.434001
[epoch16, step2346]: loss 0.738654
[epoch16, step2347]: loss 0.547702
[epoch16, step2348]: loss 0.605098
[epoch16, step2349]: loss 0.240288
[epoch16, step2350]: loss 0.457025
[epoch16, step2351]: loss 0.681235
[epoch16, step2352]: loss 0.639499
[epoch16, step2353]: loss 0.639184
[epoch16, step2354]: loss 0.551431
[epoch16, step2355]: loss 0.652442
[epoch16, step2356]: loss 0.683810
[epoch16, step2357]: loss 0.446261
[epoch16, step2358]: loss 0.483785
[epoch16, step2359]: loss 0.291478
[epoch16, step2360]: loss 0.651259
[epoch16, step2361]: loss 0.640364
[epoch16, step2362]: loss 0.424330
[epoch16, step2363]: loss 0.677588
[epoch16, step2364]: loss 0.449417
[epoch16, step2365]: loss 0.578963
[epoch16, step2366]: loss 0.519138
[epoch16, step2367]: loss 0.625091
[epoch16, step2368]: loss 0.506232
[epoch16, step2369]: loss 0.608083
[epoch16, step2370]: loss 0.629043
[epoch16, step2371]: loss 0.601787
[epoch16, step2372]: loss 0.613746
[epoch16, step2373]: loss 0.522052
[epoch16, step2374]: loss 0.630542
[epoch16, step2375]: loss 0.517074
[epoch16, step2376]: loss 0.270701
[epoch16, step2377]: loss 0.641768
[epoch16, step2378]: loss 0.724555
[epoch16, step2379]: loss 0.617063
[epoch16, step2380]: loss 0.426700
[epoch16, step2381]: loss 0.355905
[epoch16, step2382]: loss 0.619960
[epoch16, step2383]: loss 0.720443
[epoch16, step2384]: loss 0.666867
[epoch16, step2385]: loss 0.519785
[epoch16, step2386]: loss 0.505522
[epoch16, step2387]: loss 0.325045
[epoch16, step2388]: loss 0.763530
[epoch16, step2389]: loss 0.674107
[epoch16, step2390]: loss 0.728375
[epoch16, step2391]: loss 0.775398
[epoch16, step2392]: loss 0.464848
[epoch16, step2393]: loss 0.638401
[epoch16, step2394]: loss 0.303648
[epoch16, step2395]: loss 0.373498
[epoch16, step2396]: loss 0.363372
[epoch16, step2397]: loss 0.572001
[epoch16, step2398]: loss 0.594679
[epoch16, step2399]: loss 0.502033
[epoch16, step2400]: loss 0.582956
[epoch16, step2401]: loss 0.384382
[epoch16, step2402]: loss 0.497074
[epoch16, step2403]: loss 0.375410
[epoch16, step2404]: loss 0.678328
[epoch16, step2405]: loss 0.360590
[epoch16, step2406]: loss 0.422453
[epoch16, step2407]: loss 0.551313
[epoch16, step2408]: loss 0.450867
[epoch16, step2409]: loss 0.481907
[epoch16, step2410]: loss 0.482725
[epoch16, step2411]: loss 0.692361
[epoch16, step2412]: loss 0.679672
[epoch16, step2413]: loss 0.805235
[epoch16, step2414]: loss 0.490496
[epoch16, step2415]: loss 0.643509
[epoch16, step2416]: loss 0.553895
[epoch16, step2417]: loss 0.654520
[epoch16, step2418]: loss 0.493483
[epoch16, step2419]: loss 0.670301
[epoch16, step2420]: loss 0.209343
[epoch16, step2421]: loss 0.678925
[epoch16, step2422]: loss 0.431192
[epoch16, step2423]: loss 0.479012
[epoch16, step2424]: loss 0.497702
[epoch16, step2425]: loss 0.425248
[epoch16, step2426]: loss 0.332875
[epoch16, step2427]: loss 0.409710
[epoch16, step2428]: loss 0.598635
[epoch16, step2429]: loss 0.347498
[epoch16, step2430]: loss 0.456656
[epoch16, step2431]: loss 0.532045
[epoch16, step2432]: loss 0.581232
[epoch16, step2433]: loss 0.538191
[epoch16, step2434]: loss 0.320091
[epoch16, step2435]: loss 0.726146
[epoch16, step2436]: loss 0.467013
[epoch16, step2437]: loss 0.382278
[epoch16, step2438]: loss 0.658656
[epoch16, step2439]: loss 0.797198
[epoch16, step2440]: loss 0.679826
[epoch16, step2441]: loss 0.528050
[epoch16, step2442]: loss 0.580258
[epoch16, step2443]: loss 0.387651
[epoch16, step2444]: loss 0.708207
[epoch16, step2445]: loss 0.675137
[epoch16, step2446]: loss 0.312298
[epoch16, step2447]: loss 0.392794
[epoch16, step2448]: loss 0.662280
[epoch16, step2449]: loss 0.515259
[epoch16, step2450]: loss 0.266900
[epoch16, step2451]: loss 0.577662
[epoch16, step2452]: loss 0.509926
[epoch16, step2453]: loss 0.555847
[epoch16, step2454]: loss 0.677881
[epoch16, step2455]: loss 0.415651
[epoch16, step2456]: loss 0.599816
[epoch16, step2457]: loss 0.446037
[epoch16, step2458]: loss 0.350735
[epoch16, step2459]: loss 0.607782
[epoch16, step2460]: loss 0.562827
[epoch16, step2461]: loss 0.665164
[epoch16, step2462]: loss 0.559699
[epoch16, step2463]: loss 0.608419
[epoch16, step2464]: loss 0.607282
[epoch16, step2465]: loss 0.449608
[epoch16, step2466]: loss 0.763713
[epoch16, step2467]: loss 0.569607
[epoch16, step2468]: loss 0.586125
[epoch16, step2469]: loss 0.461020
[epoch16, step2470]: loss 0.557990
[epoch16, step2471]: loss 0.739480
[epoch16, step2472]: loss 0.507233
[epoch16, step2473]: loss 0.413737
[epoch16, step2474]: loss 0.371010
[epoch16, step2475]: loss 0.584535
[epoch16, step2476]: loss 0.847046
[epoch16, step2477]: loss 0.456110
[epoch16, step2478]: loss 0.480375
[epoch16, step2479]: loss 0.698462
[epoch16, step2480]: loss 0.579477
[epoch16, step2481]: loss 0.542736
[epoch16, step2482]: loss 0.472578
[epoch16, step2483]: loss 0.622422
[epoch16, step2484]: loss 0.277271
[epoch16, step2485]: loss 0.482540
[epoch16, step2486]: loss 0.321815
[epoch16, step2487]: loss 0.308431
[epoch16, step2488]: loss 0.589216
[epoch16, step2489]: loss 0.533949
[epoch16, step2490]: loss 0.696378
[epoch16, step2491]: loss 0.483521
[epoch16, step2492]: loss 0.686691
[epoch16, step2493]: loss 0.315183
[epoch16, step2494]: loss 0.395233
[epoch16, step2495]: loss 0.636331
[epoch16, step2496]: loss 0.421905
[epoch16, step2497]: loss 0.532181
[epoch16, step2498]: loss 0.519834
[epoch16, step2499]: loss 0.560082
[epoch16, step2500]: loss 0.680314
[epoch16, step2501]: loss 0.436264
[epoch16, step2502]: loss 0.775603
[epoch16, step2503]: loss 0.544434
[epoch16, step2504]: loss 0.416635
[epoch16, step2505]: loss 0.551815
[epoch16, step2506]: loss 0.487362
[epoch16, step2507]: loss 0.645103
[epoch16, step2508]: loss 0.656534
[epoch16, step2509]: loss 0.340385
[epoch16, step2510]: loss 0.699920
[epoch16, step2511]: loss 0.565816
[epoch16, step2512]: loss 0.581400
[epoch16, step2513]: loss 0.689943
[epoch16, step2514]: loss 0.404915
[epoch16, step2515]: loss 0.661192
[epoch16, step2516]: loss 0.353309
[epoch16, step2517]: loss 0.658522
[epoch16, step2518]: loss 0.393887
[epoch16, step2519]: loss 0.388604
[epoch16, step2520]: loss 0.464504
[epoch16, step2521]: loss 0.554399
[epoch16, step2522]: loss 0.630442
[epoch16, step2523]: loss 0.508799
[epoch16, step2524]: loss 0.643345
[epoch16, step2525]: loss 0.404268
[epoch16, step2526]: loss 0.589670
[epoch16, step2527]: loss 0.640161
[epoch16, step2528]: loss 0.681130
[epoch16, step2529]: loss 0.476393
[epoch16, step2530]: loss 0.496385
[epoch16, step2531]: loss 0.450840
[epoch16, step2532]: loss 0.522780
[epoch16, step2533]: loss 0.426422
[epoch16, step2534]: loss 0.540834
[epoch16, step2535]: loss 0.378320
[epoch16, step2536]: loss 0.673009
[epoch16, step2537]: loss 0.451276
[epoch16, step2538]: loss 0.464752
[epoch16, step2539]: loss 0.566588
[epoch16, step2540]: loss 0.607344
[epoch16, step2541]: loss 0.138439
[epoch16, step2542]: loss 0.568982
[epoch16, step2543]: loss 0.574750
[epoch16, step2544]: loss 0.645834
[epoch16, step2545]: loss 0.519472
[epoch16, step2546]: loss 0.411021
[epoch16, step2547]: loss 0.395969
[epoch16, step2548]: loss 0.354590
[epoch16, step2549]: loss 0.668852
[epoch16, step2550]: loss 0.733770
[epoch16, step2551]: loss 0.405256
[epoch16, step2552]: loss 0.712886
[epoch16, step2553]: loss 0.406521
[epoch16, step2554]: loss 0.693080
[epoch16, step2555]: loss 0.626258
[epoch16, step2556]: loss 0.615426
[epoch16, step2557]: loss 0.543917
[epoch16, step2558]: loss 0.664211
[epoch16, step2559]: loss 0.509562
[epoch16, step2560]: loss 0.623366
[epoch16, step2561]: loss 0.703607
[epoch16, step2562]: loss 0.746818
[epoch16, step2563]: loss 0.399703
[epoch16, step2564]: loss 0.484283
[epoch16, step2565]: loss 0.363738
[epoch16, step2566]: loss 0.560991
[epoch16, step2567]: loss 0.567048
[epoch16, step2568]: loss 0.268706
[epoch16, step2569]: loss 0.740994
[epoch16, step2570]: loss 0.638627
[epoch16, step2571]: loss 0.541208
[epoch16, step2572]: loss 0.599650
[epoch16, step2573]: loss 0.271386
[epoch16, step2574]: loss 0.526017
[epoch16, step2575]: loss 0.626083
[epoch16, step2576]: loss 0.587591
[epoch16, step2577]: loss 0.535886
[epoch16, step2578]: loss 0.355366
[epoch16, step2579]: loss 0.600837
[epoch16, step2580]: loss 0.601302
[epoch16, step2581]: loss 0.536991
[epoch16, step2582]: loss 0.630751
[epoch16, step2583]: loss 0.761354
[epoch16, step2584]: loss 0.390823
[epoch16, step2585]: loss 0.655182
[epoch16, step2586]: loss 0.434647
[epoch16, step2587]: loss 0.606315
[epoch16, step2588]: loss 0.580826
[epoch16, step2589]: loss 0.538439
[epoch16, step2590]: loss 0.683686
[epoch16, step2591]: loss 0.357094
[epoch16, step2592]: loss 0.545784
[epoch16, step2593]: loss 0.508800
[epoch16, step2594]: loss 0.687777
[epoch16, step2595]: loss 0.684981
[epoch16, step2596]: loss 0.562357
[epoch16, step2597]: loss 0.564010
[epoch16, step2598]: loss 0.580778
[epoch16, step2599]: loss 0.678984
[epoch16, step2600]: loss 0.531831
[epoch16, step2601]: loss 0.593624
[epoch16, step2602]: loss 0.540547
[epoch16, step2603]: loss 0.665658
[epoch16, step2604]: loss 0.602666
[epoch16, step2605]: loss 0.564253
[epoch16, step2606]: loss 0.309085
[epoch16, step2607]: loss 0.462798
[epoch16, step2608]: loss 0.507141
[epoch16, step2609]: loss 0.514020
[epoch16, step2610]: loss 0.619459
[epoch16, step2611]: loss 0.473256
[epoch16, step2612]: loss 0.305887
[epoch16, step2613]: loss 0.506118
[epoch16, step2614]: loss 0.289433
[epoch16, step2615]: loss 0.463035
[epoch16, step2616]: loss 0.611036
[epoch16, step2617]: loss 0.655154
[epoch16, step2618]: loss 0.903793
[epoch16, step2619]: loss 0.489832
[epoch16, step2620]: loss 0.742251
[epoch16, step2621]: loss 0.452383
[epoch16, step2622]: loss 0.619029
[epoch16, step2623]: loss 0.596099
[epoch16, step2624]: loss 0.759301
[epoch16, step2625]: loss 0.661615
[epoch16, step2626]: loss 0.692978
[epoch16, step2627]: loss 0.538603
[epoch16, step2628]: loss 0.704545
[epoch16, step2629]: loss 0.573079
[epoch16, step2630]: loss 0.436940
[epoch16, step2631]: loss 0.426846
[epoch16, step2632]: loss 0.517012
[epoch16, step2633]: loss 0.322798
[epoch16, step2634]: loss 0.612244
[epoch16, step2635]: loss 0.529072
[epoch16, step2636]: loss 0.823751
[epoch16, step2637]: loss 0.329244
[epoch16, step2638]: loss 0.536986
[epoch16, step2639]: loss 0.653046
[epoch16, step2640]: loss 0.324097
[epoch16, step2641]: loss 0.518086
[epoch16, step2642]: loss 0.420757
[epoch16, step2643]: loss 0.392547
[epoch16, step2644]: loss 0.480109
[epoch16, step2645]: loss 0.614629
[epoch16, step2646]: loss 0.527958
[epoch16, step2647]: loss 0.569906
[epoch16, step2648]: loss 0.442081
[epoch16, step2649]: loss 0.473751
[epoch16, step2650]: loss 0.743128
[epoch16, step2651]: loss 0.551101
[epoch16, step2652]: loss 0.763697
[epoch16, step2653]: loss 0.441532
[epoch16, step2654]: loss 0.590000
[epoch16, step2655]: loss 0.357938
[epoch16, step2656]: loss 0.665792
[epoch16, step2657]: loss 0.596390
[epoch16, step2658]: loss 0.480357
[epoch16, step2659]: loss 0.676672
[epoch16, step2660]: loss 0.575687
[epoch16, step2661]: loss 0.471835
[epoch16, step2662]: loss 0.496095
[epoch16, step2663]: loss 0.745006
[epoch16, step2664]: loss 0.762433
[epoch16, step2665]: loss 0.571656
[epoch16, step2666]: loss 0.396774
[epoch16, step2667]: loss 0.607552
[epoch16, step2668]: loss 0.283295
[epoch16, step2669]: loss 0.232682
[epoch16, step2670]: loss 0.482400
[epoch16, step2671]: loss 0.831351
[epoch16, step2672]: loss 0.552591
[epoch16, step2673]: loss 0.611083
[epoch16, step2674]: loss 0.323140
[epoch16, step2675]: loss 0.456135
[epoch16, step2676]: loss 0.333448
[epoch16, step2677]: loss 0.654178
[epoch16, step2678]: loss 0.661000
[epoch16, step2679]: loss 0.466904
[epoch16, step2680]: loss 0.435708
[epoch16, step2681]: loss 0.294400
[epoch16, step2682]: loss 0.429618
[epoch16, step2683]: loss 0.473858
[epoch16, step2684]: loss 0.518564
[epoch16, step2685]: loss 0.495818
[epoch16, step2686]: loss 0.725436
[epoch16, step2687]: loss 0.217808
[epoch16, step2688]: loss 0.270924
[epoch16, step2689]: loss 0.510074
[epoch16, step2690]: loss 0.362399
[epoch16, step2691]: loss 0.370997
[epoch16, step2692]: loss 0.612782
[epoch16, step2693]: loss 0.549962
[epoch16, step2694]: loss 0.288929
[epoch16, step2695]: loss 0.736750
[epoch16, step2696]: loss 0.603269
[epoch16, step2697]: loss 0.569161
[epoch16, step2698]: loss 0.564826
[epoch16, step2699]: loss 0.457974
[epoch16, step2700]: loss 0.476372
[epoch16, step2701]: loss 0.603115
[epoch16, step2702]: loss 0.547245
[epoch16, step2703]: loss 0.392599
[epoch16, step2704]: loss 0.310410
[epoch16, step2705]: loss 0.374682
[epoch16, step2706]: loss 0.486360
[epoch16, step2707]: loss 0.495293
[epoch16, step2708]: loss 0.390630
[epoch16, step2709]: loss 0.692638
[epoch16, step2710]: loss 0.480219
[epoch16, step2711]: loss 0.762662
[epoch16, step2712]: loss 0.443317
[epoch16, step2713]: loss 0.475384
[epoch16, step2714]: loss 0.379221
[epoch16, step2715]: loss 0.395610
[epoch16, step2716]: loss 0.742637
[epoch16, step2717]: loss 0.369950
[epoch16, step2718]: loss 0.550800
[epoch16, step2719]: loss 0.616792
[epoch16, step2720]: loss 0.531380
[epoch16, step2721]: loss 0.467331
[epoch16, step2722]: loss 0.718684
[epoch16, step2723]: loss 0.736778
[epoch16, step2724]: loss 0.488967
[epoch16, step2725]: loss 0.770210
[epoch16, step2726]: loss 0.383039
[epoch16, step2727]: loss 0.567499
[epoch16, step2728]: loss 0.552566
[epoch16, step2729]: loss 0.463550
[epoch16, step2730]: loss 0.440844
[epoch16, step2731]: loss 0.617564
[epoch16, step2732]: loss 0.530833
[epoch16, step2733]: loss 0.388767
[epoch16, step2734]: loss 0.799167
[epoch16, step2735]: loss 0.646631
[epoch16, step2736]: loss 0.527499
[epoch16, step2737]: loss 0.527193
[epoch16, step2738]: loss 0.296511
[epoch16, step2739]: loss 0.438013
[epoch16, step2740]: loss 0.687419
[epoch16, step2741]: loss 0.556185
[epoch16, step2742]: loss 0.510947
[epoch16, step2743]: loss 0.297757
[epoch16, step2744]: loss 0.479841
[epoch16, step2745]: loss 0.600207
[epoch16, step2746]: loss 0.471890
[epoch16, step2747]: loss 0.470217
[epoch16, step2748]: loss 0.532004
[epoch16, step2749]: loss 0.525454
[epoch16, step2750]: loss 0.473316
[epoch16, step2751]: loss 0.568984
[epoch16, step2752]: loss 0.651516
[epoch16, step2753]: loss 0.561034
[epoch16, step2754]: loss 0.454323
[epoch16, step2755]: loss 0.348872
[epoch16, step2756]: loss 0.504611
[epoch16, step2757]: loss 0.312620
[epoch16, step2758]: loss 0.590864
[epoch16, step2759]: loss 0.474818
[epoch16, step2760]: loss 0.617503
[epoch16, step2761]: loss 0.716954
[epoch16, step2762]: loss 0.493179
[epoch16, step2763]: loss 0.449378
[epoch16, step2764]: loss 0.430296
[epoch16, step2765]: loss 0.611799
[epoch16, step2766]: loss 0.372124
[epoch16, step2767]: loss 0.412247
[epoch16, step2768]: loss 0.772314
[epoch16, step2769]: loss 0.694370
[epoch16, step2770]: loss 0.483752
[epoch16, step2771]: loss 0.401622
[epoch16, step2772]: loss 0.493953
[epoch16, step2773]: loss 0.485735
[epoch16, step2774]: loss 0.528892
[epoch16, step2775]: loss 0.526810
[epoch16, step2776]: loss 0.732650
[epoch16, step2777]: loss 0.617895
[epoch16, step2778]: loss 0.576749
[epoch16, step2779]: loss 0.502761
[epoch16, step2780]: loss 0.386482
[epoch16, step2781]: loss 0.548795
[epoch16, step2782]: loss 0.402559
[epoch16, step2783]: loss 0.728884
[epoch16, step2784]: loss 0.687056
[epoch16, step2785]: loss 0.534045
[epoch16, step2786]: loss 0.690126
[epoch16, step2787]: loss 0.275995
[epoch16, step2788]: loss 0.248834
[epoch16, step2789]: loss 0.623996
[epoch16, step2790]: loss 0.633608
[epoch16, step2791]: loss 0.488417
[epoch16, step2792]: loss 0.435299
[epoch16, step2793]: loss 0.486352
[epoch16, step2794]: loss 0.760107
[epoch16, step2795]: loss 0.784389
[epoch16, step2796]: loss 0.740571
[epoch16, step2797]: loss 0.492567
[epoch16, step2798]: loss 0.554006
[epoch16, step2799]: loss 0.406570
[epoch16, step2800]: loss 0.349686
[epoch16, step2801]: loss 0.518965
[epoch16, step2802]: loss 0.759443
[epoch16, step2803]: loss 0.590440
[epoch16, step2804]: loss 0.498608
[epoch16, step2805]: loss 0.514368
[epoch16, step2806]: loss 0.447742
[epoch16, step2807]: loss 0.531813
[epoch16, step2808]: loss 0.748966
[epoch16, step2809]: loss 0.343278
[epoch16, step2810]: loss 0.414327
[epoch16, step2811]: loss 0.316558
[epoch16, step2812]: loss 0.721333
[epoch16, step2813]: loss 0.509718
[epoch16, step2814]: loss 0.516114
[epoch16, step2815]: loss 0.559149
[epoch16, step2816]: loss 0.562231
[epoch16, step2817]: loss 0.571832
[epoch16, step2818]: loss 0.384977
[epoch16, step2819]: loss 0.561147
[epoch16, step2820]: loss 0.580642
[epoch16, step2821]: loss 0.543480
[epoch16, step2822]: loss 0.571812
[epoch16, step2823]: loss 0.582036
[epoch16, step2824]: loss 0.614305
[epoch16, step2825]: loss 0.506500
[epoch16, step2826]: loss 0.826209
[epoch16, step2827]: loss 0.533039
[epoch16, step2828]: loss 0.534329
[epoch16, step2829]: loss 0.694318
[epoch16, step2830]: loss 0.640554
[epoch16, step2831]: loss 0.559501
[epoch16, step2832]: loss 0.654487
[epoch16, step2833]: loss 0.340419
[epoch16, step2834]: loss 0.745287
[epoch16, step2835]: loss 0.459067
[epoch16, step2836]: loss 0.466423
[epoch16, step2837]: loss 0.556545
[epoch16, step2838]: loss 0.615530
[epoch16, step2839]: loss 0.731787
[epoch16, step2840]: loss 0.387206
[epoch16, step2841]: loss 0.804802
[epoch16, step2842]: loss 0.231550
[epoch16, step2843]: loss 0.789944
[epoch16, step2844]: loss 0.447185
[epoch16, step2845]: loss 0.418372
[epoch16, step2846]: loss 0.486637
[epoch16, step2847]: loss 0.313969
[epoch16, step2848]: loss 0.578442
[epoch16, step2849]: loss 0.563376
[epoch16, step2850]: loss 0.463361
[epoch16, step2851]: loss 0.663066
[epoch16, step2852]: loss 0.647270
[epoch16, step2853]: loss 0.323443
[epoch16, step2854]: loss 0.726316
[epoch16, step2855]: loss 0.563752
[epoch16, step2856]: loss 0.465929
[epoch16, step2857]: loss 0.629379
[epoch16, step2858]: loss 0.599125
[epoch16, step2859]: loss 0.519089
[epoch16, step2860]: loss 0.414553
[epoch16, step2861]: loss 0.660778
[epoch16, step2862]: loss 0.437551
[epoch16, step2863]: loss 0.568352
[epoch16, step2864]: loss 0.467201
[epoch16, step2865]: loss 0.306662
[epoch16, step2866]: loss 0.585211
[epoch16, step2867]: loss 0.764263
[epoch16, step2868]: loss 0.823837
[epoch16, step2869]: loss 0.605106
[epoch16, step2870]: loss 0.595301
[epoch16, step2871]: loss 0.579018
[epoch16, step2872]: loss 0.580353
[epoch16, step2873]: loss 0.607412
[epoch16, step2874]: loss 0.522185
[epoch16, step2875]: loss 0.531622
[epoch16, step2876]: loss 0.615808
[epoch16, step2877]: loss 0.637079
[epoch16, step2878]: loss 0.539293
[epoch16, step2879]: loss 0.559482
[epoch16, step2880]: loss 0.614871
[epoch16, step2881]: loss 0.659374
[epoch16, step2882]: loss 0.665600
[epoch16, step2883]: loss 0.457685
[epoch16, step2884]: loss 0.584226
[epoch16, step2885]: loss 0.635328
[epoch16, step2886]: loss 0.299345
[epoch16, step2887]: loss 0.452451
[epoch16, step2888]: loss 0.382953
[epoch16, step2889]: loss 0.378728
[epoch16, step2890]: loss 0.170129
[epoch16, step2891]: loss 0.566874
[epoch16, step2892]: loss 0.721868
[epoch16, step2893]: loss 0.429855
[epoch16, step2894]: loss 0.221282
[epoch16, step2895]: loss 0.468439
[epoch16, step2896]: loss 0.567175
[epoch16, step2897]: loss 0.441727
[epoch16, step2898]: loss 0.377543
[epoch16, step2899]: loss 0.450748
[epoch16, step2900]: loss 0.477895
[epoch16, step2901]: loss 0.508584
[epoch16, step2902]: loss 0.649270
[epoch16, step2903]: loss 0.438032
[epoch16, step2904]: loss 0.558665
[epoch16, step2905]: loss 0.315220
[epoch16, step2906]: loss 0.769088
[epoch16, step2907]: loss 0.252274
[epoch16, step2908]: loss 0.694639
[epoch16, step2909]: loss 0.545793
[epoch16, step2910]: loss 0.614028
[epoch16, step2911]: loss 0.321973
[epoch16, step2912]: loss 0.484170
[epoch16, step2913]: loss 0.754010
[epoch16, step2914]: loss 0.658429
[epoch16, step2915]: loss 0.705288
[epoch16, step2916]: loss 0.728725
[epoch16, step2917]: loss 0.479151
[epoch16, step2918]: loss 0.727860
[epoch16, step2919]: loss 0.655247
[epoch16, step2920]: loss 0.503130
[epoch16, step2921]: loss 0.605550
[epoch16, step2922]: loss 0.766791
[epoch16, step2923]: loss 0.475620
[epoch16, step2924]: loss 0.564862
[epoch16, step2925]: loss 0.626334
[epoch16, step2926]: loss 0.663722
[epoch16, step2927]: loss 0.278966
[epoch16, step2928]: loss 0.522119
[epoch16, step2929]: loss 0.852233
[epoch16, step2930]: loss 0.537675
[epoch16, step2931]: loss 0.494626
[epoch16, step2932]: loss 0.697952
[epoch16, step2933]: loss 0.623743
[epoch16, step2934]: loss 0.367536
[epoch16, step2935]: loss 0.772571
[epoch16, step2936]: loss 0.525317
[epoch16, step2937]: loss 0.707298
[epoch16, step2938]: loss 0.590209
[epoch16, step2939]: loss 0.261276
[epoch16, step2940]: loss 0.567375
[epoch16, step2941]: loss 0.464900
[epoch16, step2942]: loss 0.390490
[epoch16, step2943]: loss 0.477864
[epoch16, step2944]: loss 0.490004
[epoch16, step2945]: loss 0.348947
[epoch16, step2946]: loss 0.638301
[epoch16, step2947]: loss 0.658028
[epoch16, step2948]: loss 0.528490
[epoch16, step2949]: loss 0.364563
[epoch16, step2950]: loss 0.690660
[epoch16, step2951]: loss 0.614158
[epoch16, step2952]: loss 0.625984
[epoch16, step2953]: loss 0.676319
[epoch16, step2954]: loss 0.553812
[epoch16, step2955]: loss 0.527963
[epoch16, step2956]: loss 0.583817
[epoch16, step2957]: loss 0.598288
[epoch16, step2958]: loss 0.464467
[epoch16, step2959]: loss 0.379004
[epoch16, step2960]: loss 0.727812
[epoch16, step2961]: loss 0.719152
[epoch16, step2962]: loss 0.528847
[epoch16, step2963]: loss 0.584794
[epoch16, step2964]: loss 0.402271
[epoch16, step2965]: loss 0.697084
[epoch16, step2966]: loss 0.482951
[epoch16, step2967]: loss 0.153049
[epoch16, step2968]: loss 0.569493
[epoch16, step2969]: loss 0.534326
[epoch16, step2970]: loss 0.478507
[epoch16, step2971]: loss 0.502342
[epoch16, step2972]: loss 0.518862
[epoch16, step2973]: loss 0.777504
[epoch16, step2974]: loss 0.514110
[epoch16, step2975]: loss 0.454827
[epoch16, step2976]: loss 0.554514
[epoch16, step2977]: loss 0.783817
[epoch16, step2978]: loss 0.481362
[epoch16, step2979]: loss 0.443951
[epoch16, step2980]: loss 0.672724
[epoch16, step2981]: loss 0.673932
[epoch16, step2982]: loss 0.312933
[epoch16, step2983]: loss 0.207030
[epoch16, step2984]: loss 0.394818
[epoch16, step2985]: loss 0.441930
[epoch16, step2986]: loss 0.701089
[epoch16, step2987]: loss 0.519791
[epoch16, step2988]: loss 0.530824
[epoch16, step2989]: loss 0.458682
[epoch16, step2990]: loss 0.396071
[epoch16, step2991]: loss 0.568441
[epoch16, step2992]: loss 0.693625
[epoch16, step2993]: loss 0.684504
[epoch16, step2994]: loss 0.278498
[epoch16, step2995]: loss 0.671887
[epoch16, step2996]: loss 0.593160
[epoch16, step2997]: loss 0.418787
[epoch16, step2998]: loss 0.752473
[epoch16, step2999]: loss 0.542636
[epoch16, step3000]: loss 0.355031
[epoch16, step3001]: loss 0.749608
[epoch16, step3002]: loss 0.556890
[epoch16, step3003]: loss 0.592601
[epoch16, step3004]: loss 0.568450
[epoch16, step3005]: loss 0.265130
[epoch16, step3006]: loss 0.756279
[epoch16, step3007]: loss 0.640970
[epoch16, step3008]: loss 0.482410
[epoch16, step3009]: loss 0.508595
[epoch16, step3010]: loss 0.432920
[epoch16, step3011]: loss 0.326861
[epoch16, step3012]: loss 0.645909
[epoch16, step3013]: loss 0.732693
[epoch16, step3014]: loss 0.436983
[epoch16, step3015]: loss 0.547047
[epoch16, step3016]: loss 0.643264
[epoch16, step3017]: loss 0.676992
[epoch16, step3018]: loss 0.759461
[epoch16, step3019]: loss 0.562704
[epoch16, step3020]: loss 0.518166
[epoch16, step3021]: loss 0.554642
[epoch16, step3022]: loss 0.772255
[epoch16, step3023]: loss 0.488212
[epoch16, step3024]: loss 0.633527
[epoch16, step3025]: loss 0.790293
[epoch16, step3026]: loss 0.591734
[epoch16, step3027]: loss 0.363462
[epoch16, step3028]: loss 0.391802
[epoch16, step3029]: loss 0.509434
[epoch16, step3030]: loss 0.744041
[epoch16, step3031]: loss 0.303542
[epoch16, step3032]: loss 0.461475
[epoch16, step3033]: loss 0.396433
[epoch16, step3034]: loss 0.601189
[epoch16, step3035]: loss 0.533558
[epoch16, step3036]: loss 0.404443
[epoch16, step3037]: loss 0.611373
[epoch16, step3038]: loss 0.642325
[epoch16, step3039]: loss 0.610668
[epoch16, step3040]: loss 0.448016
[epoch16, step3041]: loss 0.477972
[epoch16, step3042]: loss 0.837889
[epoch16, step3043]: loss 0.677975
[epoch16, step3044]: loss 0.604559
[epoch16, step3045]: loss 0.600886
[epoch16, step3046]: loss 0.595900
[epoch16, step3047]: loss 0.612406
[epoch16, step3048]: loss 0.554614
[epoch16, step3049]: loss 0.563386
[epoch16, step3050]: loss 0.526997
[epoch16, step3051]: loss 0.301877
[epoch16, step3052]: loss 0.346056
[epoch16, step3053]: loss 0.311336
[epoch16, step3054]: loss 0.512319
[epoch16, step3055]: loss 0.475566
[epoch16, step3056]: loss 0.497299
[epoch16, step3057]: loss 0.540964
[epoch16, step3058]: loss 0.424268
[epoch16, step3059]: loss 0.554454
[epoch16, step3060]: loss 0.737005
[epoch16, step3061]: loss 0.183002
[epoch16, step3062]: loss 0.490252
[epoch16, step3063]: loss 0.827996
[epoch16, step3064]: loss 0.490740
[epoch16, step3065]: loss 0.258420
[epoch16, step3066]: loss 0.382963
[epoch16, step3067]: loss 0.660155
[epoch16, step3068]: loss 0.819539
[epoch16, step3069]: loss 0.130762
[epoch16, step3070]: loss 0.602756
[epoch16, step3071]: loss 0.390144
[epoch16, step3072]: loss 0.518442
[epoch16, step3073]: loss 0.599732
[epoch16, step3074]: loss 0.443677
[epoch16, step3075]: loss 0.390341
[epoch16, step3076]: loss 0.625213

[epoch16]: avg loss 0.625213

[epoch17, step1]: loss 0.233993
[epoch17, step2]: loss 0.484826
[epoch17, step3]: loss 0.439241
[epoch17, step4]: loss 0.627289
[epoch17, step5]: loss 0.586394
[epoch17, step6]: loss 0.476826
[epoch17, step7]: loss 0.672906
[epoch17, step8]: loss 0.670439
[epoch17, step9]: loss 0.687333
[epoch17, step10]: loss 0.447720
[epoch17, step11]: loss 0.547389
[epoch17, step12]: loss 0.564857
[epoch17, step13]: loss 0.357049
[epoch17, step14]: loss 0.633039
[epoch17, step15]: loss 0.727978
[epoch17, step16]: loss 0.562972
[epoch17, step17]: loss 0.610039
[epoch17, step18]: loss 0.259958
[epoch17, step19]: loss 0.612069
[epoch17, step20]: loss 0.575363
[epoch17, step21]: loss 0.463099
[epoch17, step22]: loss 0.541342
[epoch17, step23]: loss 0.380472
[epoch17, step24]: loss 0.431949
[epoch17, step25]: loss 0.654600
[epoch17, step26]: loss 0.532476
[epoch17, step27]: loss 0.397556
[epoch17, step28]: loss 0.546698
[epoch17, step29]: loss 0.652568
[epoch17, step30]: loss 0.638220
[epoch17, step31]: loss 0.597378
[epoch17, step32]: loss 0.424626
[epoch17, step33]: loss 0.311244
[epoch17, step34]: loss 0.463899
[epoch17, step35]: loss 0.542929
[epoch17, step36]: loss 0.329200
[epoch17, step37]: loss 0.493451
[epoch17, step38]: loss 0.565021
[epoch17, step39]: loss 0.412479
[epoch17, step40]: loss 0.652269
[epoch17, step41]: loss 0.210051
[epoch17, step42]: loss 0.645935
[epoch17, step43]: loss 0.733913
[epoch17, step44]: loss 0.449036
[epoch17, step45]: loss 0.634554
[epoch17, step46]: loss 0.529536
[epoch17, step47]: loss 0.625393
[epoch17, step48]: loss 0.497490
[epoch17, step49]: loss 0.266911
[epoch17, step50]: loss 0.445302
[epoch17, step51]: loss 0.448247
[epoch17, step52]: loss 0.553712
[epoch17, step53]: loss 0.448567
[epoch17, step54]: loss 0.474781
[epoch17, step55]: loss 0.334080
[epoch17, step56]: loss 0.598587
[epoch17, step57]: loss 0.715536
[epoch17, step58]: loss 0.529179
[epoch17, step59]: loss 0.577516
[epoch17, step60]: loss 0.488425
[epoch17, step61]: loss 0.655436
[epoch17, step62]: loss 0.517981
[epoch17, step63]: loss 0.475132
[epoch17, step64]: loss 0.466474
[epoch17, step65]: loss 0.416192
[epoch17, step66]: loss 0.338860
[epoch17, step67]: loss 0.745015
[epoch17, step68]: loss 0.518930
[epoch17, step69]: loss 0.717206
[epoch17, step70]: loss 0.351879
[epoch17, step71]: loss 0.799941
[epoch17, step72]: loss 0.402915
[epoch17, step73]: loss 0.652945
[epoch17, step74]: loss 0.649082
[epoch17, step75]: loss 0.361226
[epoch17, step76]: loss 0.606137
[epoch17, step77]: loss 0.589920
[epoch17, step78]: loss 0.425912
[epoch17, step79]: loss 0.384520
[epoch17, step80]: loss 0.523353
[epoch17, step81]: loss 0.561793
[epoch17, step82]: loss 0.540130
[epoch17, step83]: loss 0.664576
[epoch17, step84]: loss 0.605546
[epoch17, step85]: loss 0.737594
[epoch17, step86]: loss 0.502492
[epoch17, step87]: loss 0.404454
[epoch17, step88]: loss 0.589961
[epoch17, step89]: loss 0.603017
[epoch17, step90]: loss 0.577243
[epoch17, step91]: loss 0.414244
[epoch17, step92]: loss 0.269325
[epoch17, step93]: loss 0.512041
[epoch17, step94]: loss 0.635214
[epoch17, step95]: loss 0.489594
[epoch17, step96]: loss 0.658367
[epoch17, step97]: loss 0.471711
[epoch17, step98]: loss 0.366832
[epoch17, step99]: loss 0.583235
[epoch17, step100]: loss 0.635722
[epoch17, step101]: loss 0.579576
[epoch17, step102]: loss 0.472493
[epoch17, step103]: loss 0.578880
[epoch17, step104]: loss 0.661105
[epoch17, step105]: loss 0.373851
[epoch17, step106]: loss 0.633504
[epoch17, step107]: loss 0.656028
[epoch17, step108]: loss 0.517614
[epoch17, step109]: loss 0.709008
[epoch17, step110]: loss 0.480090
[epoch17, step111]: loss 0.801169
[epoch17, step112]: loss 0.437281
[epoch17, step113]: loss 0.468744
[epoch17, step114]: loss 0.175666
[epoch17, step115]: loss 0.509290
[epoch17, step116]: loss 0.638750
[epoch17, step117]: loss 0.270261
[epoch17, step118]: loss 0.566075
[epoch17, step119]: loss 0.596841
[epoch17, step120]: loss 0.702378
[epoch17, step121]: loss 0.592915
[epoch17, step122]: loss 0.655074
[epoch17, step123]: loss 0.565795
[epoch17, step124]: loss 0.531107
[epoch17, step125]: loss 0.136917
[epoch17, step126]: loss 0.491430
[epoch17, step127]: loss 0.532278
[epoch17, step128]: loss 0.450515
[epoch17, step129]: loss 0.184836
[epoch17, step130]: loss 0.287469
[epoch17, step131]: loss 0.542792
[epoch17, step132]: loss 0.509963
[epoch17, step133]: loss 0.598847
[epoch17, step134]: loss 0.659136
[epoch17, step135]: loss 0.541985
[epoch17, step136]: loss 0.639378
[epoch17, step137]: loss 0.448815
[epoch17, step138]: loss 0.592317
[epoch17, step139]: loss 0.450656
[epoch17, step140]: loss 0.701885
[epoch17, step141]: loss 0.429418
[epoch17, step142]: loss 0.682451
[epoch17, step143]: loss 0.752257
[epoch17, step144]: loss 0.637358
[epoch17, step145]: loss 0.739261
[epoch17, step146]: loss 0.807209
[epoch17, step147]: loss 0.664159
[epoch17, step148]: loss 0.642087
[epoch17, step149]: loss 0.557155
[epoch17, step150]: loss 0.599294
[epoch17, step151]: loss 0.642624
[epoch17, step152]: loss 0.637183
[epoch17, step153]: loss 0.400054
[epoch17, step154]: loss 0.540559
[epoch17, step155]: loss 0.716205
[epoch17, step156]: loss 0.611035
[epoch17, step157]: loss 0.700513
[epoch17, step158]: loss 0.538956
[epoch17, step159]: loss 0.607618
[epoch17, step160]: loss 0.481826
[epoch17, step161]: loss 0.620746
[epoch17, step162]: loss 0.727469
[epoch17, step163]: loss 0.566976
[epoch17, step164]: loss 0.513445
[epoch17, step165]: loss 0.508326
[epoch17, step166]: loss 0.488105
[epoch17, step167]: loss 0.511680
[epoch17, step168]: loss 0.673236
[epoch17, step169]: loss 0.332469
[epoch17, step170]: loss 0.747538
[epoch17, step171]: loss 0.494406
[epoch17, step172]: loss 0.653592
[epoch17, step173]: loss 0.601486
[epoch17, step174]: loss 0.560941
[epoch17, step175]: loss 0.300153
[epoch17, step176]: loss 0.286574
[epoch17, step177]: loss 0.547712
[epoch17, step178]: loss 0.716402
[epoch17, step179]: loss 0.729852
[epoch17, step180]: loss 0.561032
[epoch17, step181]: loss 0.546336
[epoch17, step182]: loss 0.365156
[epoch17, step183]: loss 0.652831
[epoch17, step184]: loss 0.680136
[epoch17, step185]: loss 0.592268
[epoch17, step186]: loss 0.785573
[epoch17, step187]: loss 0.577312
[epoch17, step188]: loss 0.558818
[epoch17, step189]: loss 0.590318
[epoch17, step190]: loss 0.606329
[epoch17, step191]: loss 0.664183
[epoch17, step192]: loss 0.657659
[epoch17, step193]: loss 0.610232
[epoch17, step194]: loss 0.746497
[epoch17, step195]: loss 0.586425
[epoch17, step196]: loss 0.584124
[epoch17, step197]: loss 0.203292
[epoch17, step198]: loss 0.357124
[epoch17, step199]: loss 0.719971
[epoch17, step200]: loss 0.678997
[epoch17, step201]: loss 0.622146
[epoch17, step202]: loss 0.520764
[epoch17, step203]: loss 0.475978
[epoch17, step204]: loss 0.467399
[epoch17, step205]: loss 0.686694
[epoch17, step206]: loss 0.572743
[epoch17, step207]: loss 0.754625
[epoch17, step208]: loss 0.286837
[epoch17, step209]: loss 0.698000
[epoch17, step210]: loss 0.521933
[epoch17, step211]: loss 0.585480
[epoch17, step212]: loss 0.548021
[epoch17, step213]: loss 0.715694
[epoch17, step214]: loss 0.552212
[epoch17, step215]: loss 0.641661
[epoch17, step216]: loss 0.707563
[epoch17, step217]: loss 0.448580
[epoch17, step218]: loss 0.460240
[epoch17, step219]: loss 0.751971
[epoch17, step220]: loss 0.744089
[epoch17, step221]: loss 0.772121
[epoch17, step222]: loss 0.465096
[epoch17, step223]: loss 0.723230
[epoch17, step224]: loss 0.620834
[epoch17, step225]: loss 0.465392
[epoch17, step226]: loss 0.715526
[epoch17, step227]: loss 0.559777
[epoch17, step228]: loss 0.681038
[epoch17, step229]: loss 0.248014
[epoch17, step230]: loss 0.492293
[epoch17, step231]: loss 0.731215
[epoch17, step232]: loss 0.396855
[epoch17, step233]: loss 0.442316
[epoch17, step234]: loss 0.480841
[epoch17, step235]: loss 0.631723
[epoch17, step236]: loss 0.561571
[epoch17, step237]: loss 0.543393
[epoch17, step238]: loss 0.347450
[epoch17, step239]: loss 0.836365
[epoch17, step240]: loss 0.765822
[epoch17, step241]: loss 0.465172
[epoch17, step242]: loss 0.571400
[epoch17, step243]: loss 0.459128
[epoch17, step244]: loss 0.466526
[epoch17, step245]: loss 0.591687
[epoch17, step246]: loss 0.374796
[epoch17, step247]: loss 0.602639
[epoch17, step248]: loss 0.637670
[epoch17, step249]: loss 0.664237
[epoch17, step250]: loss 0.507655
[epoch17, step251]: loss 0.261919
[epoch17, step252]: loss 0.416070
[epoch17, step253]: loss 0.403328
[epoch17, step254]: loss 0.217709
[epoch17, step255]: loss 0.723833
[epoch17, step256]: loss 0.597420
[epoch17, step257]: loss 0.675211
[epoch17, step258]: loss 0.558162
[epoch17, step259]: loss 0.706776
[epoch17, step260]: loss 0.670861
[epoch17, step261]: loss 0.444845
[epoch17, step262]: loss 0.611652
[epoch17, step263]: loss 0.427492
[epoch17, step264]: loss 0.534128
[epoch17, step265]: loss 0.536179
[epoch17, step266]: loss 0.596653
[epoch17, step267]: loss 0.399577
[epoch17, step268]: loss 0.601655
[epoch17, step269]: loss 0.556153
[epoch17, step270]: loss 0.483386
[epoch17, step271]: loss 0.345461
[epoch17, step272]: loss 0.753959
[epoch17, step273]: loss 0.460642
[epoch17, step274]: loss 0.484341
[epoch17, step275]: loss 0.552793
[epoch17, step276]: loss 0.546709
[epoch17, step277]: loss 0.374485
[epoch17, step278]: loss 0.299668
[epoch17, step279]: loss 0.618389
[epoch17, step280]: loss 0.596610
[epoch17, step281]: loss 0.861786
[epoch17, step282]: loss 0.549890
[epoch17, step283]: loss 0.734744
[epoch17, step284]: loss 0.599202
[epoch17, step285]: loss 0.447261
[epoch17, step286]: loss 0.473044
[epoch17, step287]: loss 0.548963
[epoch17, step288]: loss 0.474712
[epoch17, step289]: loss 0.553917
[epoch17, step290]: loss 0.585650
[epoch17, step291]: loss 0.569948
[epoch17, step292]: loss 0.381363
[epoch17, step293]: loss 0.440357
[epoch17, step294]: loss 0.503318
[epoch17, step295]: loss 0.412034
[epoch17, step296]: loss 0.730423
[epoch17, step297]: loss 0.550688
[epoch17, step298]: loss 0.590438
[epoch17, step299]: loss 0.606757
[epoch17, step300]: loss 0.511261
[epoch17, step301]: loss 0.558744
[epoch17, step302]: loss 0.418435
[epoch17, step303]: loss 0.524221
[epoch17, step304]: loss 0.569319
[epoch17, step305]: loss 0.444369
[epoch17, step306]: loss 0.705061
[epoch17, step307]: loss 0.703156
[epoch17, step308]: loss 0.686984
[epoch17, step309]: loss 0.616053
[epoch17, step310]: loss 0.612453
[epoch17, step311]: loss 0.575088
[epoch17, step312]: loss 0.606876
[epoch17, step313]: loss 0.463598
[epoch17, step314]: loss 0.519100
[epoch17, step315]: loss 0.633566
[epoch17, step316]: loss 0.604292
[epoch17, step317]: loss 0.510618
[epoch17, step318]: loss 0.650124
[epoch17, step319]: loss 0.419153
[epoch17, step320]: loss 0.747930
[epoch17, step321]: loss 0.531039
[epoch17, step322]: loss 0.500028
[epoch17, step323]: loss 0.897606
[epoch17, step324]: loss 0.423194
[epoch17, step325]: loss 0.550968
[epoch17, step326]: loss 0.747334
[epoch17, step327]: loss 0.366956
[epoch17, step328]: loss 0.546401
[epoch17, step329]: loss 0.414071
[epoch17, step330]: loss 0.601030
[epoch17, step331]: loss 0.389651
[epoch17, step332]: loss 0.268442
[epoch17, step333]: loss 0.465756
[epoch17, step334]: loss 0.688139
[epoch17, step335]: loss 0.415636
[epoch17, step336]: loss 0.637143
[epoch17, step337]: loss 0.249428
[epoch17, step338]: loss 0.495288
[epoch17, step339]: loss 0.502080
[epoch17, step340]: loss 0.600546
[epoch17, step341]: loss 0.384178
[epoch17, step342]: loss 0.849809
[epoch17, step343]: loss 0.621199
[epoch17, step344]: loss 0.645170
[epoch17, step345]: loss 0.536760
[epoch17, step346]: loss 0.716977
[epoch17, step347]: loss 0.518162
[epoch17, step348]: loss 0.550360
[epoch17, step349]: loss 0.619070
[epoch17, step350]: loss 0.478456
[epoch17, step351]: loss 0.428038
[epoch17, step352]: loss 0.737850
[epoch17, step353]: loss 0.570151
[epoch17, step354]: loss 0.655860
[epoch17, step355]: loss 0.529147
[epoch17, step356]: loss 0.570357
[epoch17, step357]: loss 0.387906
[epoch17, step358]: loss 0.540982
[epoch17, step359]: loss 0.376780
[epoch17, step360]: loss 0.713841
[epoch17, step361]: loss 0.588368
[epoch17, step362]: loss 0.711696
[epoch17, step363]: loss 0.543134
[epoch17, step364]: loss 0.421891
[epoch17, step365]: loss 0.667061
[epoch17, step366]: loss 0.567474
[epoch17, step367]: loss 0.688011
[epoch17, step368]: loss 0.453113
[epoch17, step369]: loss 0.480490
[epoch17, step370]: loss 0.448029
[epoch17, step371]: loss 0.439739
[epoch17, step372]: loss 0.552019
[epoch17, step373]: loss 0.608605
[epoch17, step374]: loss 0.699005
[epoch17, step375]: loss 0.656287
[epoch17, step376]: loss 0.609538
[epoch17, step377]: loss 0.598216
[epoch17, step378]: loss 0.515481
[epoch17, step379]: loss 0.587809
[epoch17, step380]: loss 0.487868
[epoch17, step381]: loss 0.349864
[epoch17, step382]: loss 0.261666
[epoch17, step383]: loss 0.488609
[epoch17, step384]: loss 0.655177
[epoch17, step385]: loss 0.442075
[epoch17, step386]: loss 0.607319
[epoch17, step387]: loss 0.282446
[epoch17, step388]: loss 0.658065
[epoch17, step389]: loss 0.621280
[epoch17, step390]: loss 0.257459
[epoch17, step391]: loss 0.457050
[epoch17, step392]: loss 0.666543
[epoch17, step393]: loss 0.567956
[epoch17, step394]: loss 0.530374
[epoch17, step395]: loss 0.523658
[epoch17, step396]: loss 0.454965
[epoch17, step397]: loss 0.598543
[epoch17, step398]: loss 0.628922
[epoch17, step399]: loss 0.309749
[epoch17, step400]: loss 0.466263
[epoch17, step401]: loss 0.469279
[epoch17, step402]: loss 0.300267
[epoch17, step403]: loss 0.558666
[epoch17, step404]: loss 0.699510
[epoch17, step405]: loss 0.280914
[epoch17, step406]: loss 0.684956
[epoch17, step407]: loss 0.443042
[epoch17, step408]: loss 0.370541
[epoch17, step409]: loss 0.513741
[epoch17, step410]: loss 0.364344
[epoch17, step411]: loss 0.587096
[epoch17, step412]: loss 0.746785
[epoch17, step413]: loss 0.575177
[epoch17, step414]: loss 0.590887
[epoch17, step415]: loss 0.487083
[epoch17, step416]: loss 0.450191
[epoch17, step417]: loss 0.200206
[epoch17, step418]: loss 0.355408
[epoch17, step419]: loss 0.377186
[epoch17, step420]: loss 0.417322
[epoch17, step421]: loss 0.399526
[epoch17, step422]: loss 0.321474
[epoch17, step423]: loss 0.504388
[epoch17, step424]: loss 0.621075
[epoch17, step425]: loss 0.540426
[epoch17, step426]: loss 0.385493
[epoch17, step427]: loss 0.651871
[epoch17, step428]: loss 0.510000
[epoch17, step429]: loss 0.504915
[epoch17, step430]: loss 0.585659
[epoch17, step431]: loss 0.747085
[epoch17, step432]: loss 0.176591
[epoch17, step433]: loss 0.318334
[epoch17, step434]: loss 0.579390
[epoch17, step435]: loss 0.495224
[epoch17, step436]: loss 0.424199
[epoch17, step437]: loss 0.670068
[epoch17, step438]: loss 0.621879
[epoch17, step439]: loss 0.448854
[epoch17, step440]: loss 0.483457
[epoch17, step441]: loss 0.317312
[epoch17, step442]: loss 0.489262
[epoch17, step443]: loss 0.509746
[epoch17, step444]: loss 0.607320
[epoch17, step445]: loss 0.401356
[epoch17, step446]: loss 0.436690
[epoch17, step447]: loss 0.705759
[epoch17, step448]: loss 0.628987
[epoch17, step449]: loss 0.392132
[epoch17, step450]: loss 0.623335
[epoch17, step451]: loss 0.318596
[epoch17, step452]: loss 0.668876
[epoch17, step453]: loss 0.524304
[epoch17, step454]: loss 0.592248
[epoch17, step455]: loss 0.633139
[epoch17, step456]: loss 0.410626
[epoch17, step457]: loss 0.387861
[epoch17, step458]: loss 0.404844
[epoch17, step459]: loss 0.582358
[epoch17, step460]: loss 0.604215
[epoch17, step461]: loss 0.547692
[epoch17, step462]: loss 0.410620
[epoch17, step463]: loss 0.652243
[epoch17, step464]: loss 0.138155
[epoch17, step465]: loss 0.481468
[epoch17, step466]: loss 0.689490
[epoch17, step467]: loss 0.656411
[epoch17, step468]: loss 0.419057
[epoch17, step469]: loss 0.677775
[epoch17, step470]: loss 0.792916
[epoch17, step471]: loss 0.451356
[epoch17, step472]: loss 0.368550
[epoch17, step473]: loss 0.609090
[epoch17, step474]: loss 0.674845
[epoch17, step475]: loss 0.451333
[epoch17, step476]: loss 0.283897
[epoch17, step477]: loss 0.553737
[epoch17, step478]: loss 0.529931
[epoch17, step479]: loss 0.645489
[epoch17, step480]: loss 0.640203
[epoch17, step481]: loss 0.616056
[epoch17, step482]: loss 0.458722
[epoch17, step483]: loss 0.417729
[epoch17, step484]: loss 0.277643
[epoch17, step485]: loss 0.535397
[epoch17, step486]: loss 0.365564
[epoch17, step487]: loss 0.639205
[epoch17, step488]: loss 0.531422
[epoch17, step489]: loss 0.512957
[epoch17, step490]: loss 0.259821
[epoch17, step491]: loss 0.593660
[epoch17, step492]: loss 0.650912
[epoch17, step493]: loss 0.519720
[epoch17, step494]: loss 0.427135
[epoch17, step495]: loss 0.699359
[epoch17, step496]: loss 0.742796
[epoch17, step497]: loss 0.612861
[epoch17, step498]: loss 0.406857
[epoch17, step499]: loss 0.579735
[epoch17, step500]: loss 0.644543
[epoch17, step501]: loss 0.606629
[epoch17, step502]: loss 0.409292
[epoch17, step503]: loss 0.445962
[epoch17, step504]: loss 0.437728
[epoch17, step505]: loss 0.500710
[epoch17, step506]: loss 0.572195
[epoch17, step507]: loss 0.614067
[epoch17, step508]: loss 0.418986
[epoch17, step509]: loss 0.452582
[epoch17, step510]: loss 0.305329
[epoch17, step511]: loss 0.680933
[epoch17, step512]: loss 0.485415
[epoch17, step513]: loss 0.601555
[epoch17, step514]: loss 0.589479
[epoch17, step515]: loss 0.402634
[epoch17, step516]: loss 0.662742
[epoch17, step517]: loss 0.541926
[epoch17, step518]: loss 0.491651
[epoch17, step519]: loss 0.718725
[epoch17, step520]: loss 0.492059
[epoch17, step521]: loss 0.538379
[epoch17, step522]: loss 0.609183
[epoch17, step523]: loss 0.469487
[epoch17, step524]: loss 0.655547
[epoch17, step525]: loss 0.529555
[epoch17, step526]: loss 0.701443
[epoch17, step527]: loss 0.646066
[epoch17, step528]: loss 0.459878
[epoch17, step529]: loss 0.453621
[epoch17, step530]: loss 0.666365
[epoch17, step531]: loss 0.533193
[epoch17, step532]: loss 0.381909
[epoch17, step533]: loss 0.605335
[epoch17, step534]: loss 0.591512
[epoch17, step535]: loss 0.430775
[epoch17, step536]: loss 0.299961
[epoch17, step537]: loss 0.598357
[epoch17, step538]: loss 0.369150
[epoch17, step539]: loss 0.362765
[epoch17, step540]: loss 0.541255
[epoch17, step541]: loss 0.531410
[epoch17, step542]: loss 0.533908
[epoch17, step543]: loss 0.454868
[epoch17, step544]: loss 0.591679
[epoch17, step545]: loss 0.621402
[epoch17, step546]: loss 0.679647
[epoch17, step547]: loss 0.417581
[epoch17, step548]: loss 0.437603
[epoch17, step549]: loss 0.676325
[epoch17, step550]: loss 0.834006
[epoch17, step551]: loss 0.531488
[epoch17, step552]: loss 0.312417
[epoch17, step553]: loss 0.626338
[epoch17, step554]: loss 0.661716
[epoch17, step555]: loss 0.550502
[epoch17, step556]: loss 0.341989
[epoch17, step557]: loss 0.519243
[epoch17, step558]: loss 0.624848
[epoch17, step559]: loss 0.287753
[epoch17, step560]: loss 0.404666
[epoch17, step561]: loss 0.677574
[epoch17, step562]: loss 0.706578
[epoch17, step563]: loss 0.421727
[epoch17, step564]: loss 0.306065
[epoch17, step565]: loss 0.528077
[epoch17, step566]: loss 0.656308
[epoch17, step567]: loss 0.682698
[epoch17, step568]: loss 0.462862
[epoch17, step569]: loss 0.610049
[epoch17, step570]: loss 0.503665
[epoch17, step571]: loss 0.508469
[epoch17, step572]: loss 0.488891
[epoch17, step573]: loss 0.611581
[epoch17, step574]: loss 0.610517
[epoch17, step575]: loss 0.319654
[epoch17, step576]: loss 0.581014
[epoch17, step577]: loss 0.584953
[epoch17, step578]: loss 0.342149
[epoch17, step579]: loss 0.507409
[epoch17, step580]: loss 0.426331
[epoch17, step581]: loss 0.495573
[epoch17, step582]: loss 0.345786
[epoch17, step583]: loss 0.627986
[epoch17, step584]: loss 0.595855
[epoch17, step585]: loss 0.405338
[epoch17, step586]: loss 0.430635
[epoch17, step587]: loss 0.417312
[epoch17, step588]: loss 0.401435
[epoch17, step589]: loss 0.485947
[epoch17, step590]: loss 0.548329
[epoch17, step591]: loss 0.563284
[epoch17, step592]: loss 0.707162
[epoch17, step593]: loss 0.482829
[epoch17, step594]: loss 0.683919
[epoch17, step595]: loss 0.490070
[epoch17, step596]: loss 0.308997
[epoch17, step597]: loss 0.546701
[epoch17, step598]: loss 0.722396
[epoch17, step599]: loss 0.553352
[epoch17, step600]: loss 0.440918
[epoch17, step601]: loss 0.599403
[epoch17, step602]: loss 0.262714
[epoch17, step603]: loss 0.516391
[epoch17, step604]: loss 0.667241
[epoch17, step605]: loss 0.469426
[epoch17, step606]: loss 0.409264
[epoch17, step607]: loss 0.523411
[epoch17, step608]: loss 0.352203
[epoch17, step609]: loss 0.713858
[epoch17, step610]: loss 0.556524
[epoch17, step611]: loss 0.404761
[epoch17, step612]: loss 0.649710
[epoch17, step613]: loss 0.599621
[epoch17, step614]: loss 0.285751
[epoch17, step615]: loss 0.466703
[epoch17, step616]: loss 0.575429
[epoch17, step617]: loss 0.468209
[epoch17, step618]: loss 0.170946
[epoch17, step619]: loss 0.559992
[epoch17, step620]: loss 0.634699
[epoch17, step621]: loss 0.320688
[epoch17, step622]: loss 0.697326
[epoch17, step623]: loss 0.601000
[epoch17, step624]: loss 0.666797
[epoch17, step625]: loss 0.597228
[epoch17, step626]: loss 0.638611
[epoch17, step627]: loss 0.571184
[epoch17, step628]: loss 0.779630
[epoch17, step629]: loss 0.347077
[epoch17, step630]: loss 0.455001
[epoch17, step631]: loss 0.446897
[epoch17, step632]: loss 0.440366
[epoch17, step633]: loss 0.427463
[epoch17, step634]: loss 0.584066
[epoch17, step635]: loss 0.531239
[epoch17, step636]: loss 0.358771
[epoch17, step637]: loss 0.339687
[epoch17, step638]: loss 0.683682
[epoch17, step639]: loss 0.355826
[epoch17, step640]: loss 0.619674
[epoch17, step641]: loss 0.598838
[epoch17, step642]: loss 0.311158
[epoch17, step643]: loss 0.469521
[epoch17, step644]: loss 0.598851
[epoch17, step645]: loss 0.634977
[epoch17, step646]: loss 0.448504
[epoch17, step647]: loss 0.423606
[epoch17, step648]: loss 0.772570
[epoch17, step649]: loss 0.781572
[epoch17, step650]: loss 0.670242
[epoch17, step651]: loss 0.626117
[epoch17, step652]: loss 0.624756
[epoch17, step653]: loss 0.655695
[epoch17, step654]: loss 0.221602
[epoch17, step655]: loss 0.638123
[epoch17, step656]: loss 0.757713
[epoch17, step657]: loss 0.589821
[epoch17, step658]: loss 0.512679
[epoch17, step659]: loss 0.476167
[epoch17, step660]: loss 0.505027
[epoch17, step661]: loss 0.537703
[epoch17, step662]: loss 0.716088
[epoch17, step663]: loss 0.466971
[epoch17, step664]: loss 0.641176
[epoch17, step665]: loss 0.724101
[epoch17, step666]: loss 0.711715
[epoch17, step667]: loss 0.689153
[epoch17, step668]: loss 0.578916
[epoch17, step669]: loss 0.370561
[epoch17, step670]: loss 0.501879
[epoch17, step671]: loss 0.664019
[epoch17, step672]: loss 0.286333
[epoch17, step673]: loss 0.570668
[epoch17, step674]: loss 0.696605
[epoch17, step675]: loss 0.559711
[epoch17, step676]: loss 0.504140
[epoch17, step677]: loss 0.491256
[epoch17, step678]: loss 0.796719
[epoch17, step679]: loss 0.299022
[epoch17, step680]: loss 0.387188
[epoch17, step681]: loss 0.644105
[epoch17, step682]: loss 0.424907
[epoch17, step683]: loss 0.452604
[epoch17, step684]: loss 0.608086
[epoch17, step685]: loss 0.305541
[epoch17, step686]: loss 0.548066
[epoch17, step687]: loss 0.260884
[epoch17, step688]: loss 0.532643
[epoch17, step689]: loss 0.590382
[epoch17, step690]: loss 0.165083
[epoch17, step691]: loss 0.605123
[epoch17, step692]: loss 0.561930
[epoch17, step693]: loss 0.503028
[epoch17, step694]: loss 0.615647
[epoch17, step695]: loss 0.522362
[epoch17, step696]: loss 0.357425
[epoch17, step697]: loss 0.801973
[epoch17, step698]: loss 0.384430
[epoch17, step699]: loss 0.634009
[epoch17, step700]: loss 0.398454
[epoch17, step701]: loss 0.671198
[epoch17, step702]: loss 0.678567
[epoch17, step703]: loss 0.178865
[epoch17, step704]: loss 0.660630
[epoch17, step705]: loss 0.408069
[epoch17, step706]: loss 0.369696
[epoch17, step707]: loss 0.781164
[epoch17, step708]: loss 0.484443
[epoch17, step709]: loss 0.374149
[epoch17, step710]: loss 0.566067
[epoch17, step711]: loss 0.293701
[epoch17, step712]: loss 0.361281
[epoch17, step713]: loss 0.513748
[epoch17, step714]: loss 0.441269
[epoch17, step715]: loss 1.017734
[epoch17, step716]: loss 0.474892
[epoch17, step717]: loss 0.382475
[epoch17, step718]: loss 0.603013
[epoch17, step719]: loss 0.318007
[epoch17, step720]: loss 0.563962
[epoch17, step721]: loss 0.496096
[epoch17, step722]: loss 0.451943
[epoch17, step723]: loss 0.184495
[epoch17, step724]: loss 0.456456
[epoch17, step725]: loss 0.416688
[epoch17, step726]: loss 0.513475
[epoch17, step727]: loss 0.602414
[epoch17, step728]: loss 0.648589
[epoch17, step729]: loss 0.665167
[epoch17, step730]: loss 0.615821
[epoch17, step731]: loss 0.589430
[epoch17, step732]: loss 0.489552
[epoch17, step733]: loss 0.538879
[epoch17, step734]: loss 0.659286
[epoch17, step735]: loss 0.501610
[epoch17, step736]: loss 0.494199
[epoch17, step737]: loss 0.513483
[epoch17, step738]: loss 0.388891
[epoch17, step739]: loss 0.684401
[epoch17, step740]: loss 0.566692
[epoch17, step741]: loss 0.662425
[epoch17, step742]: loss 0.345976
[epoch17, step743]: loss 0.575085
[epoch17, step744]: loss 0.682218
[epoch17, step745]: loss 0.538097
[epoch17, step746]: loss 0.371396
[epoch17, step747]: loss 0.505560
[epoch17, step748]: loss 0.810176
[epoch17, step749]: loss 0.667845
[epoch17, step750]: loss 0.541920
[epoch17, step751]: loss 0.470451
[epoch17, step752]: loss 0.263910
[epoch17, step753]: loss 0.626754
[epoch17, step754]: loss 0.559317
[epoch17, step755]: loss 0.596332
[epoch17, step756]: loss 0.555971
[epoch17, step757]: loss 0.565405
[epoch17, step758]: loss 0.821300
[epoch17, step759]: loss 0.730247
[epoch17, step760]: loss 0.799563
[epoch17, step761]: loss 0.528718
[epoch17, step762]: loss 0.713396
[epoch17, step763]: loss 0.432621
[epoch17, step764]: loss 0.285363
[epoch17, step765]: loss 0.591124
[epoch17, step766]: loss 0.546879
[epoch17, step767]: loss 0.489132
[epoch17, step768]: loss 0.692715
[epoch17, step769]: loss 0.619209
[epoch17, step770]: loss 0.377153
[epoch17, step771]: loss 0.630557
[epoch17, step772]: loss 0.578765
[epoch17, step773]: loss 0.637557
[epoch17, step774]: loss 0.605557
[epoch17, step775]: loss 0.549821
[epoch17, step776]: loss 0.216506
[epoch17, step777]: loss 0.430849
[epoch17, step778]: loss 0.412920
[epoch17, step779]: loss 0.716020
[epoch17, step780]: loss 0.444355
[epoch17, step781]: loss 0.461124
[epoch17, step782]: loss 0.266631
[epoch17, step783]: loss 0.643698
[epoch17, step784]: loss 0.652662
[epoch17, step785]: loss 0.587218
[epoch17, step786]: loss 0.799563
[epoch17, step787]: loss 0.681013
[epoch17, step788]: loss 0.588655
[epoch17, step789]: loss 0.383515
[epoch17, step790]: loss 0.600467
[epoch17, step791]: loss 0.545509
[epoch17, step792]: loss 0.417035
[epoch17, step793]: loss 0.706053
[epoch17, step794]: loss 0.568577
[epoch17, step795]: loss 0.441712
[epoch17, step796]: loss 0.706215
[epoch17, step797]: loss 0.491875
[epoch17, step798]: loss 0.479098
[epoch17, step799]: loss 0.173629
[epoch17, step800]: loss 0.678114
[epoch17, step801]: loss 0.577145
[epoch17, step802]: loss 0.479866
[epoch17, step803]: loss 0.323595
[epoch17, step804]: loss 0.527500
[epoch17, step805]: loss 0.578038
[epoch17, step806]: loss 0.709885
[epoch17, step807]: loss 0.683510
[epoch17, step808]: loss 0.696015
[epoch17, step809]: loss 0.618255
[epoch17, step810]: loss 0.375905
[epoch17, step811]: loss 0.621985
[epoch17, step812]: loss 0.522607
[epoch17, step813]: loss 0.547085
[epoch17, step814]: loss 0.541496
[epoch17, step815]: loss 0.429280
[epoch17, step816]: loss 0.672530
[epoch17, step817]: loss 0.551984
[epoch17, step818]: loss 0.695839
[epoch17, step819]: loss 0.662623
[epoch17, step820]: loss 0.650411
[epoch17, step821]: loss 0.617474
[epoch17, step822]: loss 0.431862
[epoch17, step823]: loss 0.393844
[epoch17, step824]: loss 0.804797
[epoch17, step825]: loss 0.403763
[epoch17, step826]: loss 0.291968
[epoch17, step827]: loss 0.582352
[epoch17, step828]: loss 0.356936
[epoch17, step829]: loss 0.579104
[epoch17, step830]: loss 0.271459
[epoch17, step831]: loss 0.273930
[epoch17, step832]: loss 0.760309
[epoch17, step833]: loss 0.611064
[epoch17, step834]: loss 0.451865
[epoch17, step835]: loss 0.435601
[epoch17, step836]: loss 0.543936
[epoch17, step837]: loss 0.496092
[epoch17, step838]: loss 0.604849
[epoch17, step839]: loss 0.666545
[epoch17, step840]: loss 0.593799
[epoch17, step841]: loss 0.614849
[epoch17, step842]: loss 0.397847
[epoch17, step843]: loss 0.678007
[epoch17, step844]: loss 0.649036
[epoch17, step845]: loss 0.569590
[epoch17, step846]: loss 0.540781
[epoch17, step847]: loss 0.597286
[epoch17, step848]: loss 0.268104
[epoch17, step849]: loss 0.456987
[epoch17, step850]: loss 0.316564
[epoch17, step851]: loss 0.649039
[epoch17, step852]: loss 0.841972
[epoch17, step853]: loss 0.649879
[epoch17, step854]: loss 0.730466
[epoch17, step855]: loss 0.432259
[epoch17, step856]: loss 0.552260
[epoch17, step857]: loss 0.345620
[epoch17, step858]: loss 0.449126
[epoch17, step859]: loss 0.498310
[epoch17, step860]: loss 0.608674
[epoch17, step861]: loss 0.392796
[epoch17, step862]: loss 0.410701
[epoch17, step863]: loss 0.423494
[epoch17, step864]: loss 0.487461
[epoch17, step865]: loss 0.516726
[epoch17, step866]: loss 0.373033
[epoch17, step867]: loss 0.565489
[epoch17, step868]: loss 0.409345
[epoch17, step869]: loss 0.754047
[epoch17, step870]: loss 0.516029
[epoch17, step871]: loss 0.373070
[epoch17, step872]: loss 0.545180
[epoch17, step873]: loss 0.354359
[epoch17, step874]: loss 0.803098
[epoch17, step875]: loss 0.563380
[epoch17, step876]: loss 0.634929
[epoch17, step877]: loss 0.523388
[epoch17, step878]: loss 0.707315
[epoch17, step879]: loss 0.521926
[epoch17, step880]: loss 0.462812
[epoch17, step881]: loss 0.515620
[epoch17, step882]: loss 0.585694
[epoch17, step883]: loss 0.621357
[epoch17, step884]: loss 0.619510
[epoch17, step885]: loss 0.449610
[epoch17, step886]: loss 0.343759
[epoch17, step887]: loss 0.680706
[epoch17, step888]: loss 0.752011
[epoch17, step889]: loss 0.756472
[epoch17, step890]: loss 0.584066
[epoch17, step891]: loss 0.612008
[epoch17, step892]: loss 0.377726
[epoch17, step893]: loss 0.669391
[epoch17, step894]: loss 0.417875
[epoch17, step895]: loss 0.591003
[epoch17, step896]: loss 0.513031
[epoch17, step897]: loss 0.594148
[epoch17, step898]: loss 0.650222
[epoch17, step899]: loss 0.592367
[epoch17, step900]: loss 0.387655
[epoch17, step901]: loss 0.675491
[epoch17, step902]: loss 0.667325
[epoch17, step903]: loss 0.838815
[epoch17, step904]: loss 0.459724
[epoch17, step905]: loss 0.457430
[epoch17, step906]: loss 0.624538
[epoch17, step907]: loss 0.634220
[epoch17, step908]: loss 0.611040
[epoch17, step909]: loss 0.429577
[epoch17, step910]: loss 0.654472
[epoch17, step911]: loss 0.554008
[epoch17, step912]: loss 0.545192
[epoch17, step913]: loss 0.595914
[epoch17, step914]: loss 0.325519
[epoch17, step915]: loss 0.455271
[epoch17, step916]: loss 0.705818
[epoch17, step917]: loss 0.522671
[epoch17, step918]: loss 0.645029
[epoch17, step919]: loss 0.495278
[epoch17, step920]: loss 0.330321
[epoch17, step921]: loss 0.406128
[epoch17, step922]: loss 0.547004
[epoch17, step923]: loss 0.703715
[epoch17, step924]: loss 0.520182
[epoch17, step925]: loss 0.148442
[epoch17, step926]: loss 0.546042
[epoch17, step927]: loss 0.697353
[epoch17, step928]: loss 0.323815
[epoch17, step929]: loss 0.681397
[epoch17, step930]: loss 0.678605
[epoch17, step931]: loss 0.668390
[epoch17, step932]: loss 0.534638
[epoch17, step933]: loss 0.732105
[epoch17, step934]: loss 0.824766
[epoch17, step935]: loss 0.371202
[epoch17, step936]: loss 0.623293
[epoch17, step937]: loss 0.365331
[epoch17, step938]: loss 0.538604
[epoch17, step939]: loss 0.316165
[epoch17, step940]: loss 0.384039
[epoch17, step941]: loss 0.731661
[epoch17, step942]: loss 0.269016
[epoch17, step943]: loss 0.562042
[epoch17, step944]: loss 0.649911
[epoch17, step945]: loss 0.607069
[epoch17, step946]: loss 0.644045
[epoch17, step947]: loss 0.590793
[epoch17, step948]: loss 0.716639
[epoch17, step949]: loss 0.533379
[epoch17, step950]: loss 0.428115
[epoch17, step951]: loss 0.739587
[epoch17, step952]: loss 0.731489
[epoch17, step953]: loss 0.666213
[epoch17, step954]: loss 0.836171
[epoch17, step955]: loss 0.574185
[epoch17, step956]: loss 0.446438
[epoch17, step957]: loss 0.641533
[epoch17, step958]: loss 0.515523
[epoch17, step959]: loss 0.371285
[epoch17, step960]: loss 0.344256
[epoch17, step961]: loss 0.511485
[epoch17, step962]: loss 0.536354
[epoch17, step963]: loss 0.321106
[epoch17, step964]: loss 0.348828
[epoch17, step965]: loss 0.501474
[epoch17, step966]: loss 0.569367
[epoch17, step967]: loss 0.436467
[epoch17, step968]: loss 0.621599
[epoch17, step969]: loss 0.401181
[epoch17, step970]: loss 0.442081
[epoch17, step971]: loss 0.588750
[epoch17, step972]: loss 0.626534
[epoch17, step973]: loss 0.360989
[epoch17, step974]: loss 0.643975
[epoch17, step975]: loss 0.677969
[epoch17, step976]: loss 0.409727
[epoch17, step977]: loss 0.599022
[epoch17, step978]: loss 0.698020
[epoch17, step979]: loss 0.355391
[epoch17, step980]: loss 0.577551
[epoch17, step981]: loss 0.574860
[epoch17, step982]: loss 0.526964
[epoch17, step983]: loss 0.428726
[epoch17, step984]: loss 0.594660
[epoch17, step985]: loss 0.741994
[epoch17, step986]: loss 0.385360
[epoch17, step987]: loss 0.531644
[epoch17, step988]: loss 0.536640
[epoch17, step989]: loss 0.566980
[epoch17, step990]: loss 0.598832
[epoch17, step991]: loss 0.571091
[epoch17, step992]: loss 0.614172
[epoch17, step993]: loss 0.474184
[epoch17, step994]: loss 0.632900
[epoch17, step995]: loss 0.627753
[epoch17, step996]: loss 0.697909
[epoch17, step997]: loss 0.837778
[epoch17, step998]: loss 0.622658
[epoch17, step999]: loss 0.495524
[epoch17, step1000]: loss 0.412229
[epoch17, step1001]: loss 0.162621
[epoch17, step1002]: loss 0.507165
[epoch17, step1003]: loss 0.584309
[epoch17, step1004]: loss 0.518891
[epoch17, step1005]: loss 0.524324
[epoch17, step1006]: loss 0.559336
[epoch17, step1007]: loss 0.472100
[epoch17, step1008]: loss 0.502648
[epoch17, step1009]: loss 0.471928
[epoch17, step1010]: loss 0.518347
[epoch17, step1011]: loss 0.682863
[epoch17, step1012]: loss 0.421788
[epoch17, step1013]: loss 0.743967
[epoch17, step1014]: loss 0.460841
[epoch17, step1015]: loss 0.146810
[epoch17, step1016]: loss 0.561806
[epoch17, step1017]: loss 0.731561
[epoch17, step1018]: loss 0.537805
[epoch17, step1019]: loss 0.667361
[epoch17, step1020]: loss 0.582837
[epoch17, step1021]: loss 0.450531
[epoch17, step1022]: loss 0.631708
[epoch17, step1023]: loss 0.558081
[epoch17, step1024]: loss 0.666335
[epoch17, step1025]: loss 0.443189
[epoch17, step1026]: loss 0.569017
[epoch17, step1027]: loss 0.685598
[epoch17, step1028]: loss 0.302891
[epoch17, step1029]: loss 0.502439
[epoch17, step1030]: loss 0.464389
[epoch17, step1031]: loss 0.374723
[epoch17, step1032]: loss 0.528624
[epoch17, step1033]: loss 0.470309
[epoch17, step1034]: loss 0.419494
[epoch17, step1035]: loss 0.544380
[epoch17, step1036]: loss 0.779754
[epoch17, step1037]: loss 0.558929
[epoch17, step1038]: loss 0.542383
[epoch17, step1039]: loss 0.360875
[epoch17, step1040]: loss 0.355617
[epoch17, step1041]: loss 0.663407
[epoch17, step1042]: loss 0.673739
[epoch17, step1043]: loss 0.712754
[epoch17, step1044]: loss 0.467503
[epoch17, step1045]: loss 0.533227
[epoch17, step1046]: loss 0.293776
[epoch17, step1047]: loss 0.388299
[epoch17, step1048]: loss 0.505959
[epoch17, step1049]: loss 0.628435
[epoch17, step1050]: loss 0.620641
[epoch17, step1051]: loss 0.345028
[epoch17, step1052]: loss 0.611928
[epoch17, step1053]: loss 0.598405
[epoch17, step1054]: loss 0.548974
[epoch17, step1055]: loss 0.889832
[epoch17, step1056]: loss 0.436527
[epoch17, step1057]: loss 0.572315
[epoch17, step1058]: loss 0.502044
[epoch17, step1059]: loss 0.575094
[epoch17, step1060]: loss 0.373093
[epoch17, step1061]: loss 0.693794
[epoch17, step1062]: loss 0.451027
[epoch17, step1063]: loss 0.665200
[epoch17, step1064]: loss 0.720661
[epoch17, step1065]: loss 0.698238
[epoch17, step1066]: loss 0.613851
[epoch17, step1067]: loss 0.501795
[epoch17, step1068]: loss 0.638666
[epoch17, step1069]: loss 0.552026
[epoch17, step1070]: loss 0.477013
[epoch17, step1071]: loss 0.573968
[epoch17, step1072]: loss 0.298714
[epoch17, step1073]: loss 0.397606
[epoch17, step1074]: loss 0.342508
[epoch17, step1075]: loss 0.652201
[epoch17, step1076]: loss 0.529467
[epoch17, step1077]: loss 0.411375
[epoch17, step1078]: loss 0.647404
[epoch17, step1079]: loss 0.624271
[epoch17, step1080]: loss 0.550217
[epoch17, step1081]: loss 0.641098
[epoch17, step1082]: loss 0.679842
[epoch17, step1083]: loss 0.804713
[epoch17, step1084]: loss 0.385557
[epoch17, step1085]: loss 0.509220
[epoch17, step1086]: loss 0.655684
[epoch17, step1087]: loss 0.389995
[epoch17, step1088]: loss 0.693554
[epoch17, step1089]: loss 0.423958
[epoch17, step1090]: loss 0.458592
[epoch17, step1091]: loss 0.576248
[epoch17, step1092]: loss 0.519578
[epoch17, step1093]: loss 0.592717
[epoch17, step1094]: loss 0.510320
[epoch17, step1095]: loss 0.673253
[epoch17, step1096]: loss 0.288894
[epoch17, step1097]: loss 0.605432
[epoch17, step1098]: loss 0.587372
[epoch17, step1099]: loss 0.253721
[epoch17, step1100]: loss 0.575305
[epoch17, step1101]: loss 0.487959
[epoch17, step1102]: loss 0.608174
[epoch17, step1103]: loss 0.609084
[epoch17, step1104]: loss 0.449710
[epoch17, step1105]: loss 0.545637
[epoch17, step1106]: loss 0.528329
[epoch17, step1107]: loss 0.565941
[epoch17, step1108]: loss 0.402693
[epoch17, step1109]: loss 0.725657
[epoch17, step1110]: loss 0.472624
[epoch17, step1111]: loss 0.434864
[epoch17, step1112]: loss 0.757358
[epoch17, step1113]: loss 0.182358
[epoch17, step1114]: loss 0.497650
[epoch17, step1115]: loss 0.557840
[epoch17, step1116]: loss 0.527897
[epoch17, step1117]: loss 0.547224
[epoch17, step1118]: loss 0.719974
[epoch17, step1119]: loss 0.353010
[epoch17, step1120]: loss 0.510255
[epoch17, step1121]: loss 0.588575
[epoch17, step1122]: loss 0.478937
[epoch17, step1123]: loss 0.540536
[epoch17, step1124]: loss 0.507499
[epoch17, step1125]: loss 0.561463
[epoch17, step1126]: loss 0.442106
[epoch17, step1127]: loss 0.835579
[epoch17, step1128]: loss 0.277761
[epoch17, step1129]: loss 0.499720
[epoch17, step1130]: loss 0.364156
[epoch17, step1131]: loss 0.442697
[epoch17, step1132]: loss 0.535724
[epoch17, step1133]: loss 0.493682
[epoch17, step1134]: loss 0.605283
[epoch17, step1135]: loss 0.569811
[epoch17, step1136]: loss 0.253753
[epoch17, step1137]: loss 0.606844
[epoch17, step1138]: loss 0.538958
[epoch17, step1139]: loss 0.351985
[epoch17, step1140]: loss 0.596616
[epoch17, step1141]: loss 0.571700
[epoch17, step1142]: loss 0.349396
[epoch17, step1143]: loss 0.564268
[epoch17, step1144]: loss 0.496735
[epoch17, step1145]: loss 0.353684
[epoch17, step1146]: loss 0.672716
[epoch17, step1147]: loss 0.497002
[epoch17, step1148]: loss 0.565140
[epoch17, step1149]: loss 0.487359
[epoch17, step1150]: loss 0.444375
[epoch17, step1151]: loss 0.395719
[epoch17, step1152]: loss 0.498492
[epoch17, step1153]: loss 0.507718
[epoch17, step1154]: loss 0.621035
[epoch17, step1155]: loss 0.416372
[epoch17, step1156]: loss 0.625319
[epoch17, step1157]: loss 0.820693
[epoch17, step1158]: loss 0.619609
[epoch17, step1159]: loss 0.458449
[epoch17, step1160]: loss 0.399646
[epoch17, step1161]: loss 0.762970
[epoch17, step1162]: loss 0.694299
[epoch17, step1163]: loss 0.646114
[epoch17, step1164]: loss 0.839493
[epoch17, step1165]: loss 0.508907
[epoch17, step1166]: loss 0.726632
[epoch17, step1167]: loss 0.609074
[epoch17, step1168]: loss 0.582188
[epoch17, step1169]: loss 0.576840
[epoch17, step1170]: loss 0.534529
[epoch17, step1171]: loss 0.422454
[epoch17, step1172]: loss 0.549747
[epoch17, step1173]: loss 0.687645
[epoch17, step1174]: loss 0.721573
[epoch17, step1175]: loss 0.700765
[epoch17, step1176]: loss 0.428428
[epoch17, step1177]: loss 0.602002
[epoch17, step1178]: loss 0.527366
[epoch17, step1179]: loss 0.499762
[epoch17, step1180]: loss 0.697854
[epoch17, step1181]: loss 0.464982
[epoch17, step1182]: loss 0.449980
[epoch17, step1183]: loss 0.442795
[epoch17, step1184]: loss 0.517920
[epoch17, step1185]: loss 0.739173
[epoch17, step1186]: loss 0.463759
[epoch17, step1187]: loss 0.702459
[epoch17, step1188]: loss 0.597631
[epoch17, step1189]: loss 0.533862
[epoch17, step1190]: loss 0.682611
[epoch17, step1191]: loss 0.533823
[epoch17, step1192]: loss 0.512594
[epoch17, step1193]: loss 0.455174
[epoch17, step1194]: loss 0.664833
[epoch17, step1195]: loss 0.749109
[epoch17, step1196]: loss 0.752475
[epoch17, step1197]: loss 0.417305
[epoch17, step1198]: loss 0.611743
[epoch17, step1199]: loss 0.592667
[epoch17, step1200]: loss 0.734357
[epoch17, step1201]: loss 0.773943
[epoch17, step1202]: loss 0.645582
[epoch17, step1203]: loss 0.514269
[epoch17, step1204]: loss 0.219564
[epoch17, step1205]: loss 0.510360
[epoch17, step1206]: loss 0.520258
[epoch17, step1207]: loss 0.453569
[epoch17, step1208]: loss 0.702249
[epoch17, step1209]: loss 0.481839
[epoch17, step1210]: loss 0.321114
[epoch17, step1211]: loss 0.647005
[epoch17, step1212]: loss 0.657086
[epoch17, step1213]: loss 0.689937
[epoch17, step1214]: loss 0.524388
[epoch17, step1215]: loss 0.612944
[epoch17, step1216]: loss 0.544713
[epoch17, step1217]: loss 0.590607
[epoch17, step1218]: loss 0.776600
[epoch17, step1219]: loss 0.494245
[epoch17, step1220]: loss 0.451869
[epoch17, step1221]: loss 0.639107
[epoch17, step1222]: loss 0.424078
[epoch17, step1223]: loss 0.432102
[epoch17, step1224]: loss 0.647404
[epoch17, step1225]: loss 0.231948
[epoch17, step1226]: loss 0.759447
[epoch17, step1227]: loss 0.598682
[epoch17, step1228]: loss 0.577951
[epoch17, step1229]: loss 0.508748
[epoch17, step1230]: loss 0.580685
[epoch17, step1231]: loss 0.621261
[epoch17, step1232]: loss 0.355840
[epoch17, step1233]: loss 0.347716
[epoch17, step1234]: loss 0.319604
[epoch17, step1235]: loss 0.591720
[epoch17, step1236]: loss 0.423031
[epoch17, step1237]: loss 0.528594
[epoch17, step1238]: loss 0.843649
[epoch17, step1239]: loss 0.557402
[epoch17, step1240]: loss 0.723394
[epoch17, step1241]: loss 0.474788
[epoch17, step1242]: loss 0.392459
[epoch17, step1243]: loss 0.452725
[epoch17, step1244]: loss 0.401929
[epoch17, step1245]: loss 0.431922
[epoch17, step1246]: loss 0.475405
[epoch17, step1247]: loss 0.445470
[epoch17, step1248]: loss 0.496882
[epoch17, step1249]: loss 0.697054
[epoch17, step1250]: loss 0.663851
[epoch17, step1251]: loss 0.242617
[epoch17, step1252]: loss 0.610088
[epoch17, step1253]: loss 0.607871
[epoch17, step1254]: loss 0.575391
[epoch17, step1255]: loss 0.651600
[epoch17, step1256]: loss 0.488259
[epoch17, step1257]: loss 0.446292
[epoch17, step1258]: loss 0.531880
[epoch17, step1259]: loss 0.449532
[epoch17, step1260]: loss 0.610901
[epoch17, step1261]: loss 0.449042
[epoch17, step1262]: loss 0.693759
[epoch17, step1263]: loss 0.520915
[epoch17, step1264]: loss 0.550150
[epoch17, step1265]: loss 0.616746
[epoch17, step1266]: loss 0.312525
[epoch17, step1267]: loss 0.444930
[epoch17, step1268]: loss 0.773065
[epoch17, step1269]: loss 0.393141
[epoch17, step1270]: loss 0.411848
[epoch17, step1271]: loss 0.348344
[epoch17, step1272]: loss 0.550817
[epoch17, step1273]: loss 0.382585
[epoch17, step1274]: loss 0.569851
[epoch17, step1275]: loss 0.541120
[epoch17, step1276]: loss 0.718501
[epoch17, step1277]: loss 0.491744
[epoch17, step1278]: loss 0.515749
[epoch17, step1279]: loss 0.602371
[epoch17, step1280]: loss 0.488109
[epoch17, step1281]: loss 0.477705
[epoch17, step1282]: loss 0.421211
[epoch17, step1283]: loss 0.503294
[epoch17, step1284]: loss 0.496973
[epoch17, step1285]: loss 0.623815
[epoch17, step1286]: loss 0.459439
[epoch17, step1287]: loss 0.402181
[epoch17, step1288]: loss 0.356391
[epoch17, step1289]: loss 0.609370
[epoch17, step1290]: loss 0.749241
[epoch17, step1291]: loss 0.525172
[epoch17, step1292]: loss 0.360591
[epoch17, step1293]: loss 0.536109
[epoch17, step1294]: loss 0.390532
[epoch17, step1295]: loss 0.644863
[epoch17, step1296]: loss 0.421714
[epoch17, step1297]: loss 0.479225
[epoch17, step1298]: loss 0.632489
[epoch17, step1299]: loss 0.537106
[epoch17, step1300]: loss 0.330734
[epoch17, step1301]: loss 0.583359
[epoch17, step1302]: loss 0.579116
[epoch17, step1303]: loss 0.607985
[epoch17, step1304]: loss 0.275471
[epoch17, step1305]: loss 0.338991
[epoch17, step1306]: loss 0.642168
[epoch17, step1307]: loss 0.550436
[epoch17, step1308]: loss 0.505061
[epoch17, step1309]: loss 0.608736
[epoch17, step1310]: loss 0.710700
[epoch17, step1311]: loss 0.599054
[epoch17, step1312]: loss 0.242713
[epoch17, step1313]: loss 0.581007
[epoch17, step1314]: loss 0.356449
[epoch17, step1315]: loss 0.695362
[epoch17, step1316]: loss 0.645634
[epoch17, step1317]: loss 0.353391
[epoch17, step1318]: loss 0.632373
[epoch17, step1319]: loss 0.364953
[epoch17, step1320]: loss 0.574085
[epoch17, step1321]: loss 0.506514
[epoch17, step1322]: loss 0.460295
[epoch17, step1323]: loss 0.710721
[epoch17, step1324]: loss 0.561296
[epoch17, step1325]: loss 0.626288
[epoch17, step1326]: loss 0.566305
[epoch17, step1327]: loss 0.653055
[epoch17, step1328]: loss 0.456419
[epoch17, step1329]: loss 0.733645
[epoch17, step1330]: loss 0.612693
[epoch17, step1331]: loss 0.582064
[epoch17, step1332]: loss 0.283884
[epoch17, step1333]: loss 0.737201
[epoch17, step1334]: loss 0.479586
[epoch17, step1335]: loss 0.304298
[epoch17, step1336]: loss 0.583434
[epoch17, step1337]: loss 0.500296
[epoch17, step1338]: loss 0.582496
[epoch17, step1339]: loss 0.659896
[epoch17, step1340]: loss 0.722813
[epoch17, step1341]: loss 0.614119
[epoch17, step1342]: loss 0.447854
[epoch17, step1343]: loss 0.646366
[epoch17, step1344]: loss 0.271284
[epoch17, step1345]: loss 0.522289
[epoch17, step1346]: loss 0.892455
[epoch17, step1347]: loss 0.628628
[epoch17, step1348]: loss 0.279305
[epoch17, step1349]: loss 0.574193
[epoch17, step1350]: loss 0.301773
[epoch17, step1351]: loss 0.558419
[epoch17, step1352]: loss 0.249720
[epoch17, step1353]: loss 0.589028
[epoch17, step1354]: loss 0.545936
[epoch17, step1355]: loss 0.562313
[epoch17, step1356]: loss 0.584538
[epoch17, step1357]: loss 0.496071
[epoch17, step1358]: loss 0.649259
[epoch17, step1359]: loss 0.634850
[epoch17, step1360]: loss 0.430886
[epoch17, step1361]: loss 0.371559
[epoch17, step1362]: loss 0.375021
[epoch17, step1363]: loss 0.565330
[epoch17, step1364]: loss 0.605616
[epoch17, step1365]: loss 0.645513
[epoch17, step1366]: loss 0.533552
[epoch17, step1367]: loss 0.752136
[epoch17, step1368]: loss 0.786150
[epoch17, step1369]: loss 0.534535
[epoch17, step1370]: loss 0.593574
[epoch17, step1371]: loss 0.263668
[epoch17, step1372]: loss 0.654541
[epoch17, step1373]: loss 0.618182
[epoch17, step1374]: loss 0.269253
[epoch17, step1375]: loss 0.569566
[epoch17, step1376]: loss 0.671749
[epoch17, step1377]: loss 0.804793
[epoch17, step1378]: loss 0.608454
[epoch17, step1379]: loss 0.509636
[epoch17, step1380]: loss 0.494385
[epoch17, step1381]: loss 0.413561
[epoch17, step1382]: loss 0.547872
[epoch17, step1383]: loss 0.578627
[epoch17, step1384]: loss 0.606017
[epoch17, step1385]: loss 0.542091
[epoch17, step1386]: loss 0.395723
[epoch17, step1387]: loss 0.582070
[epoch17, step1388]: loss 0.580353
[epoch17, step1389]: loss 0.735564
[epoch17, step1390]: loss 0.533698
[epoch17, step1391]: loss 0.676273
[epoch17, step1392]: loss 0.560633
[epoch17, step1393]: loss 0.624407
[epoch17, step1394]: loss 0.269162
[epoch17, step1395]: loss 0.398466
[epoch17, step1396]: loss 0.577917
[epoch17, step1397]: loss 0.610859
[epoch17, step1398]: loss 0.801607
[epoch17, step1399]: loss 0.531523
[epoch17, step1400]: loss 0.386847
[epoch17, step1401]: loss 0.434408
[epoch17, step1402]: loss 0.690161
[epoch17, step1403]: loss 0.486850
[epoch17, step1404]: loss 0.828398
[epoch17, step1405]: loss 0.272850
[epoch17, step1406]: loss 0.305719
[epoch17, step1407]: loss 0.415146
[epoch17, step1408]: loss 0.617541
[epoch17, step1409]: loss 0.621861
[epoch17, step1410]: loss 0.372526
[epoch17, step1411]: loss 0.670097
[epoch17, step1412]: loss 0.486357
[epoch17, step1413]: loss 0.204312
[epoch17, step1414]: loss 0.371270
[epoch17, step1415]: loss 0.734842
[epoch17, step1416]: loss 0.292339
[epoch17, step1417]: loss 0.384470
[epoch17, step1418]: loss 0.463592
[epoch17, step1419]: loss 0.703117
[epoch17, step1420]: loss 0.446738
[epoch17, step1421]: loss 0.684976
[epoch17, step1422]: loss 0.543834
[epoch17, step1423]: loss 0.707293
[epoch17, step1424]: loss 0.436085
[epoch17, step1425]: loss 0.564706
[epoch17, step1426]: loss 0.606452
[epoch17, step1427]: loss 0.621523
[epoch17, step1428]: loss 0.605403
[epoch17, step1429]: loss 0.405486
[epoch17, step1430]: loss 0.158212
[epoch17, step1431]: loss 0.569487
[epoch17, step1432]: loss 0.457088
[epoch17, step1433]: loss 0.593338
[epoch17, step1434]: loss 0.559097
[epoch17, step1435]: loss 0.555066
[epoch17, step1436]: loss 0.656505
[epoch17, step1437]: loss 0.424735
[epoch17, step1438]: loss 0.482494
[epoch17, step1439]: loss 0.708354
[epoch17, step1440]: loss 0.571871
[epoch17, step1441]: loss 0.727495
[epoch17, step1442]: loss 0.503716
[epoch17, step1443]: loss 0.646785
[epoch17, step1444]: loss 0.791116
[epoch17, step1445]: loss 0.319408
[epoch17, step1446]: loss 0.565298
[epoch17, step1447]: loss 0.397441
[epoch17, step1448]: loss 0.432509
[epoch17, step1449]: loss 0.352176
[epoch17, step1450]: loss 0.478995
[epoch17, step1451]: loss 0.497609
[epoch17, step1452]: loss 0.564435
[epoch17, step1453]: loss 0.698406
[epoch17, step1454]: loss 0.728426
[epoch17, step1455]: loss 0.505771
[epoch17, step1456]: loss 0.572785
[epoch17, step1457]: loss 0.611714
[epoch17, step1458]: loss 0.286620
[epoch17, step1459]: loss 0.342639
[epoch17, step1460]: loss 0.608376
[epoch17, step1461]: loss 0.626983
[epoch17, step1462]: loss 0.290922
[epoch17, step1463]: loss 0.607693
[epoch17, step1464]: loss 0.444405
[epoch17, step1465]: loss 0.500983
[epoch17, step1466]: loss 0.171451
[epoch17, step1467]: loss 0.661455
[epoch17, step1468]: loss 0.331483
[epoch17, step1469]: loss 0.757117
[epoch17, step1470]: loss 0.527726
[epoch17, step1471]: loss 0.643335
[epoch17, step1472]: loss 0.610079
[epoch17, step1473]: loss 0.541490
[epoch17, step1474]: loss 0.681282
[epoch17, step1475]: loss 0.626748
[epoch17, step1476]: loss 0.361897
[epoch17, step1477]: loss 0.321189
[epoch17, step1478]: loss 0.644553
[epoch17, step1479]: loss 0.402581
[epoch17, step1480]: loss 0.565287
[epoch17, step1481]: loss 0.492000
[epoch17, step1482]: loss 0.323925
[epoch17, step1483]: loss 0.354262
[epoch17, step1484]: loss 0.247670
[epoch17, step1485]: loss 0.450931
[epoch17, step1486]: loss 0.623088
[epoch17, step1487]: loss 0.566323
[epoch17, step1488]: loss 0.600692
[epoch17, step1489]: loss 0.581010
[epoch17, step1490]: loss 0.350166
[epoch17, step1491]: loss 0.247578
[epoch17, step1492]: loss 0.483972
[epoch17, step1493]: loss 0.367681
[epoch17, step1494]: loss 0.396715
[epoch17, step1495]: loss 0.589444
[epoch17, step1496]: loss 0.275798
[epoch17, step1497]: loss 0.513821
[epoch17, step1498]: loss 0.308912
[epoch17, step1499]: loss 0.572386
[epoch17, step1500]: loss 0.746036
[epoch17, step1501]: loss 0.549880
[epoch17, step1502]: loss 0.659980
[epoch17, step1503]: loss 0.624129
[epoch17, step1504]: loss 0.377812
[epoch17, step1505]: loss 0.609869
[epoch17, step1506]: loss 0.667390
[epoch17, step1507]: loss 0.490829
[epoch17, step1508]: loss 0.328769
[epoch17, step1509]: loss 0.346572
[epoch17, step1510]: loss 0.405140
[epoch17, step1511]: loss 0.291000
[epoch17, step1512]: loss 0.875217
[epoch17, step1513]: loss 0.436135
[epoch17, step1514]: loss 0.522215
[epoch17, step1515]: loss 0.383253
[epoch17, step1516]: loss 0.518928
[epoch17, step1517]: loss 0.563862
[epoch17, step1518]: loss 0.595292
[epoch17, step1519]: loss 0.245937
[epoch17, step1520]: loss 0.335693
[epoch17, step1521]: loss 0.611796
[epoch17, step1522]: loss 0.465831
[epoch17, step1523]: loss 0.472664
[epoch17, step1524]: loss 0.560322
[epoch17, step1525]: loss 0.540298
[epoch17, step1526]: loss 0.532741
[epoch17, step1527]: loss 0.652531
[epoch17, step1528]: loss 0.803349
[epoch17, step1529]: loss 0.484849
[epoch17, step1530]: loss 0.683724
[epoch17, step1531]: loss 0.640374
[epoch17, step1532]: loss 0.573533
[epoch17, step1533]: loss 0.485254
[epoch17, step1534]: loss 0.646849
[epoch17, step1535]: loss 0.385674
[epoch17, step1536]: loss 0.403688
[epoch17, step1537]: loss 0.464620
[epoch17, step1538]: loss 0.665431
[epoch17, step1539]: loss 0.284649
[epoch17, step1540]: loss 0.399828
[epoch17, step1541]: loss 0.424594
[epoch17, step1542]: loss 0.700838
[epoch17, step1543]: loss 0.322165
[epoch17, step1544]: loss 0.451806
[epoch17, step1545]: loss 0.464570
[epoch17, step1546]: loss 0.558384
[epoch17, step1547]: loss 0.540166
[epoch17, step1548]: loss 0.671418
[epoch17, step1549]: loss 0.787377
[epoch17, step1550]: loss 0.735341
[epoch17, step1551]: loss 0.622768
[epoch17, step1552]: loss 0.522874
[epoch17, step1553]: loss 0.479532
[epoch17, step1554]: loss 0.681308
[epoch17, step1555]: loss 0.392376
[epoch17, step1556]: loss 0.672181
[epoch17, step1557]: loss 0.414274
[epoch17, step1558]: loss 0.582546
[epoch17, step1559]: loss 0.575847
[epoch17, step1560]: loss 0.618577
[epoch17, step1561]: loss 0.484723
[epoch17, step1562]: loss 0.490018
[epoch17, step1563]: loss 0.547542
[epoch17, step1564]: loss 0.575448
[epoch17, step1565]: loss 0.639130
[epoch17, step1566]: loss 0.569987
[epoch17, step1567]: loss 0.606529
[epoch17, step1568]: loss 0.348839
[epoch17, step1569]: loss 0.284667
[epoch17, step1570]: loss 0.273748
[epoch17, step1571]: loss 0.623616
[epoch17, step1572]: loss 0.642356
[epoch17, step1573]: loss 0.537736
[epoch17, step1574]: loss 0.476441
[epoch17, step1575]: loss 0.630536
[epoch17, step1576]: loss 0.773919
[epoch17, step1577]: loss 0.495147
[epoch17, step1578]: loss 0.550397
[epoch17, step1579]: loss 0.675809
[epoch17, step1580]: loss 0.716880
[epoch17, step1581]: loss 0.693358
[epoch17, step1582]: loss 0.384352
[epoch17, step1583]: loss 0.545544
[epoch17, step1584]: loss 0.432122
[epoch17, step1585]: loss 0.406473
[epoch17, step1586]: loss 0.577376
[epoch17, step1587]: loss 0.678077
[epoch17, step1588]: loss 0.231089
[epoch17, step1589]: loss 0.446144
[epoch17, step1590]: loss 0.527786
[epoch17, step1591]: loss 0.377649
[epoch17, step1592]: loss 0.713110
[epoch17, step1593]: loss 0.499928
[epoch17, step1594]: loss 0.145614
[epoch17, step1595]: loss 0.804763
[epoch17, step1596]: loss 0.484577
[epoch17, step1597]: loss 0.546293
[epoch17, step1598]: loss 0.422885
[epoch17, step1599]: loss 0.280603
[epoch17, step1600]: loss 0.529890
[epoch17, step1601]: loss 0.540877
[epoch17, step1602]: loss 0.512268
[epoch17, step1603]: loss 0.554731
[epoch17, step1604]: loss 0.455690
[epoch17, step1605]: loss 0.601751
[epoch17, step1606]: loss 0.472845
[epoch17, step1607]: loss 0.802628
[epoch17, step1608]: loss 0.226288
[epoch17, step1609]: loss 0.630499
[epoch17, step1610]: loss 0.164658
[epoch17, step1611]: loss 0.709426
[epoch17, step1612]: loss 0.707455
[epoch17, step1613]: loss 0.775077
[epoch17, step1614]: loss 0.491281
[epoch17, step1615]: loss 0.462901
[epoch17, step1616]: loss 0.433608
[epoch17, step1617]: loss 0.522564
[epoch17, step1618]: loss 0.769586
[epoch17, step1619]: loss 0.486269
[epoch17, step1620]: loss 0.477676
[epoch17, step1621]: loss 0.485065
[epoch17, step1622]: loss 0.629331
[epoch17, step1623]: loss 0.538917
[epoch17, step1624]: loss 0.609809
[epoch17, step1625]: loss 0.711853
[epoch17, step1626]: loss 0.631617
[epoch17, step1627]: loss 0.424262
[epoch17, step1628]: loss 0.501338
[epoch17, step1629]: loss 0.499240
[epoch17, step1630]: loss 0.466747
[epoch17, step1631]: loss 0.532235
[epoch17, step1632]: loss 0.343875
[epoch17, step1633]: loss 0.432756
[epoch17, step1634]: loss 0.223608
[epoch17, step1635]: loss 0.186990
[epoch17, step1636]: loss 0.594837
[epoch17, step1637]: loss 0.420008
[epoch17, step1638]: loss 0.553200
[epoch17, step1639]: loss 0.415799
[epoch17, step1640]: loss 0.486609
[epoch17, step1641]: loss 0.589293
[epoch17, step1642]: loss 0.746191
[epoch17, step1643]: loss 0.244269
[epoch17, step1644]: loss 0.434441
[epoch17, step1645]: loss 0.464241
[epoch17, step1646]: loss 0.607343
[epoch17, step1647]: loss 0.654887
[epoch17, step1648]: loss 0.647338
[epoch17, step1649]: loss 0.597253
[epoch17, step1650]: loss 0.125665
[epoch17, step1651]: loss 0.494464
[epoch17, step1652]: loss 0.831080
[epoch17, step1653]: loss 0.570810
[epoch17, step1654]: loss 0.700498
[epoch17, step1655]: loss 0.470268
[epoch17, step1656]: loss 0.658597
[epoch17, step1657]: loss 0.510686
[epoch17, step1658]: loss 0.526702
[epoch17, step1659]: loss 0.511869
[epoch17, step1660]: loss 0.583656
[epoch17, step1661]: loss 0.300751
[epoch17, step1662]: loss 0.629550
[epoch17, step1663]: loss 0.644439
[epoch17, step1664]: loss 0.541169
[epoch17, step1665]: loss 0.438142
[epoch17, step1666]: loss 0.239179
[epoch17, step1667]: loss 0.446504
[epoch17, step1668]: loss 0.593781
[epoch17, step1669]: loss 0.367788
[epoch17, step1670]: loss 0.777113
[epoch17, step1671]: loss 0.737336
[epoch17, step1672]: loss 0.584318
[epoch17, step1673]: loss 0.449157
[epoch17, step1674]: loss 0.500427
[epoch17, step1675]: loss 0.512867
[epoch17, step1676]: loss 0.651446
[epoch17, step1677]: loss 0.651277
[epoch17, step1678]: loss 0.474295
[epoch17, step1679]: loss 0.664663
[epoch17, step1680]: loss 0.286453
[epoch17, step1681]: loss 0.513973
[epoch17, step1682]: loss 0.674491
[epoch17, step1683]: loss 0.470830
[epoch17, step1684]: loss 0.434750
[epoch17, step1685]: loss 0.591763
[epoch17, step1686]: loss 0.551725
[epoch17, step1687]: loss 0.505597
[epoch17, step1688]: loss 0.471231
[epoch17, step1689]: loss 0.550761
[epoch17, step1690]: loss 0.574355
[epoch17, step1691]: loss 0.328880
[epoch17, step1692]: loss 0.627145
[epoch17, step1693]: loss 0.445812
[epoch17, step1694]: loss 0.565262
[epoch17, step1695]: loss 0.613383
[epoch17, step1696]: loss 0.743740
[epoch17, step1697]: loss 0.410868
[epoch17, step1698]: loss 0.271258
[epoch17, step1699]: loss 0.449741
[epoch17, step1700]: loss 0.552116
[epoch17, step1701]: loss 0.571169
[epoch17, step1702]: loss 0.448117
[epoch17, step1703]: loss 0.493741
[epoch17, step1704]: loss 0.527832
[epoch17, step1705]: loss 0.529687
[epoch17, step1706]: loss 0.518975
[epoch17, step1707]: loss 0.474919
[epoch17, step1708]: loss 0.615955
[epoch17, step1709]: loss 0.434878
[epoch17, step1710]: loss 0.595311
[epoch17, step1711]: loss 0.607756
[epoch17, step1712]: loss 0.534535
[epoch17, step1713]: loss 0.339532
[epoch17, step1714]: loss 0.464472
[epoch17, step1715]: loss 0.328013
[epoch17, step1716]: loss 0.701261
[epoch17, step1717]: loss 0.658237
[epoch17, step1718]: loss 0.678453
[epoch17, step1719]: loss 0.669979
[epoch17, step1720]: loss 0.549152
[epoch17, step1721]: loss 0.455125
[epoch17, step1722]: loss 0.716999
[epoch17, step1723]: loss 0.320155
[epoch17, step1724]: loss 0.631458
[epoch17, step1725]: loss 0.644042
[epoch17, step1726]: loss 0.640524
[epoch17, step1727]: loss 0.599667
[epoch17, step1728]: loss 0.371023
[epoch17, step1729]: loss 0.585525
[epoch17, step1730]: loss 0.372011
[epoch17, step1731]: loss 0.515461
[epoch17, step1732]: loss 0.685541
[epoch17, step1733]: loss 0.594794
[epoch17, step1734]: loss 0.487068
[epoch17, step1735]: loss 0.597894
[epoch17, step1736]: loss 0.488729
[epoch17, step1737]: loss 0.637110
[epoch17, step1738]: loss 0.472052
[epoch17, step1739]: loss 0.625681
[epoch17, step1740]: loss 0.637257
[epoch17, step1741]: loss 0.741765
[epoch17, step1742]: loss 0.683211
[epoch17, step1743]: loss 0.359990
[epoch17, step1744]: loss 0.304098
[epoch17, step1745]: loss 0.516711
[epoch17, step1746]: loss 0.496033
[epoch17, step1747]: loss 0.487253
[epoch17, step1748]: loss 0.511157
[epoch17, step1749]: loss 0.803582
[epoch17, step1750]: loss 0.272930
[epoch17, step1751]: loss 0.620805
[epoch17, step1752]: loss 0.684057
[epoch17, step1753]: loss 0.717766
[epoch17, step1754]: loss 0.352763
[epoch17, step1755]: loss 0.672903
[epoch17, step1756]: loss 0.640319
[epoch17, step1757]: loss 0.535221
[epoch17, step1758]: loss 0.395676
[epoch17, step1759]: loss 0.517048
[epoch17, step1760]: loss 0.399128
[epoch17, step1761]: loss 0.713951
[epoch17, step1762]: loss 0.644095
[epoch17, step1763]: loss 0.465851
[epoch17, step1764]: loss 0.508552
[epoch17, step1765]: loss 0.811175
[epoch17, step1766]: loss 0.539621
[epoch17, step1767]: loss 0.776841
[epoch17, step1768]: loss 0.605691
[epoch17, step1769]: loss 0.308742
[epoch17, step1770]: loss 0.653512
[epoch17, step1771]: loss 0.483568
[epoch17, step1772]: loss 0.654806
[epoch17, step1773]: loss 0.515963
[epoch17, step1774]: loss 0.283931
[epoch17, step1775]: loss 0.788233
[epoch17, step1776]: loss 0.781273
[epoch17, step1777]: loss 0.593191
[epoch17, step1778]: loss 0.632117
[epoch17, step1779]: loss 0.412600
[epoch17, step1780]: loss 0.682344
[epoch17, step1781]: loss 0.608594
[epoch17, step1782]: loss 0.647154
[epoch17, step1783]: loss 0.492875
[epoch17, step1784]: loss 0.541934
[epoch17, step1785]: loss 0.391145
[epoch17, step1786]: loss 0.629807
[epoch17, step1787]: loss 0.687678
[epoch17, step1788]: loss 0.574181
[epoch17, step1789]: loss 0.665793
[epoch17, step1790]: loss 0.520065
[epoch17, step1791]: loss 0.548984
[epoch17, step1792]: loss 0.554183
[epoch17, step1793]: loss 0.668920
[epoch17, step1794]: loss 0.605271
[epoch17, step1795]: loss 0.596861
[epoch17, step1796]: loss 0.674920
[epoch17, step1797]: loss 0.470468
[epoch17, step1798]: loss 0.664826
[epoch17, step1799]: loss 0.469233
[epoch17, step1800]: loss 0.460190
[epoch17, step1801]: loss 0.647605
[epoch17, step1802]: loss 0.584620
[epoch17, step1803]: loss 0.594679
[epoch17, step1804]: loss 0.341200
[epoch17, step1805]: loss 0.522927
[epoch17, step1806]: loss 0.557036
[epoch17, step1807]: loss 0.436366
[epoch17, step1808]: loss 0.644985
[epoch17, step1809]: loss 0.441403
[epoch17, step1810]: loss 0.304349
[epoch17, step1811]: loss 0.603130
[epoch17, step1812]: loss 0.674162
[epoch17, step1813]: loss 0.591586
[epoch17, step1814]: loss 0.455411
[epoch17, step1815]: loss 0.500225
[epoch17, step1816]: loss 0.675872
[epoch17, step1817]: loss 0.753354
[epoch17, step1818]: loss 0.646423
[epoch17, step1819]: loss 0.571263
[epoch17, step1820]: loss 0.714164
[epoch17, step1821]: loss 0.758048
[epoch17, step1822]: loss 0.458804
[epoch17, step1823]: loss 0.536426
[epoch17, step1824]: loss 0.274427
[epoch17, step1825]: loss 0.631355
[epoch17, step1826]: loss 0.593151
[epoch17, step1827]: loss 0.702481
[epoch17, step1828]: loss 0.471058
[epoch17, step1829]: loss 0.463873
[epoch17, step1830]: loss 0.664628
[epoch17, step1831]: loss 0.356496
[epoch17, step1832]: loss 0.435477
[epoch17, step1833]: loss 0.499746
[epoch17, step1834]: loss 0.467153
[epoch17, step1835]: loss 0.817026
[epoch17, step1836]: loss 0.598154
[epoch17, step1837]: loss 0.450397
[epoch17, step1838]: loss 0.670997
[epoch17, step1839]: loss 0.662779
[epoch17, step1840]: loss 0.396143
[epoch17, step1841]: loss 0.348827
[epoch17, step1842]: loss 0.271593
[epoch17, step1843]: loss 0.515551
[epoch17, step1844]: loss 0.352743
[epoch17, step1845]: loss 0.492190
[epoch17, step1846]: loss 0.539531
[epoch17, step1847]: loss 0.652301
[epoch17, step1848]: loss 0.299293
[epoch17, step1849]: loss 0.654471
[epoch17, step1850]: loss 0.622234
[epoch17, step1851]: loss 0.686311
[epoch17, step1852]: loss 0.272218
[epoch17, step1853]: loss 0.715474
[epoch17, step1854]: loss 0.454835
[epoch17, step1855]: loss 0.305571
[epoch17, step1856]: loss 0.626782
[epoch17, step1857]: loss 0.612481
[epoch17, step1858]: loss 0.554634
[epoch17, step1859]: loss 0.387066
[epoch17, step1860]: loss 0.549850
[epoch17, step1861]: loss 0.623360
[epoch17, step1862]: loss 0.505037
[epoch17, step1863]: loss 0.652442
[epoch17, step1864]: loss 0.405952
[epoch17, step1865]: loss 0.449313
[epoch17, step1866]: loss 0.552640
[epoch17, step1867]: loss 0.652453
[epoch17, step1868]: loss 0.324170
[epoch17, step1869]: loss 0.753867
[epoch17, step1870]: loss 0.546839
[epoch17, step1871]: loss 0.579647
[epoch17, step1872]: loss 0.663912
[epoch17, step1873]: loss 0.766179
[epoch17, step1874]: loss 0.642201
[epoch17, step1875]: loss 0.638541
[epoch17, step1876]: loss 0.499539
[epoch17, step1877]: loss 0.655823
[epoch17, step1878]: loss 0.372744
[epoch17, step1879]: loss 0.667876
[epoch17, step1880]: loss 0.473572
[epoch17, step1881]: loss 0.387226
[epoch17, step1882]: loss 0.504373
[epoch17, step1883]: loss 0.420539
[epoch17, step1884]: loss 0.552925
[epoch17, step1885]: loss 0.272590
[epoch17, step1886]: loss 0.624697
[epoch17, step1887]: loss 0.326819
[epoch17, step1888]: loss 0.627930
[epoch17, step1889]: loss 0.534595
[epoch17, step1890]: loss 0.247122
[epoch17, step1891]: loss 0.556910
[epoch17, step1892]: loss 0.474452
[epoch17, step1893]: loss 0.662550
[epoch17, step1894]: loss 0.639131
[epoch17, step1895]: loss 0.485990
[epoch17, step1896]: loss 0.648549
[epoch17, step1897]: loss 0.365895
[epoch17, step1898]: loss 0.525619
[epoch17, step1899]: loss 0.505514
[epoch17, step1900]: loss 0.732113
[epoch17, step1901]: loss 0.685801
[epoch17, step1902]: loss 0.561032
[epoch17, step1903]: loss 0.593319
[epoch17, step1904]: loss 0.498388
[epoch17, step1905]: loss 0.345537
[epoch17, step1906]: loss 0.614442
[epoch17, step1907]: loss 0.733313
[epoch17, step1908]: loss 0.452050
[epoch17, step1909]: loss 0.598326
[epoch17, step1910]: loss 0.689618
[epoch17, step1911]: loss 0.499285
[epoch17, step1912]: loss 0.583989
[epoch17, step1913]: loss 0.589756
[epoch17, step1914]: loss 0.576991
[epoch17, step1915]: loss 0.573787
[epoch17, step1916]: loss 0.663620
[epoch17, step1917]: loss 0.477300
[epoch17, step1918]: loss 0.627841
[epoch17, step1919]: loss 0.689613
[epoch17, step1920]: loss 0.359634
[epoch17, step1921]: loss 0.626908
[epoch17, step1922]: loss 0.488772
[epoch17, step1923]: loss 0.456249
[epoch17, step1924]: loss 0.609855
[epoch17, step1925]: loss 0.433320
[epoch17, step1926]: loss 0.506072
[epoch17, step1927]: loss 0.670547
[epoch17, step1928]: loss 0.376896
[epoch17, step1929]: loss 0.420005
[epoch17, step1930]: loss 0.421239
[epoch17, step1931]: loss 0.520461
[epoch17, step1932]: loss 0.363481
[epoch17, step1933]: loss 0.697469
[epoch17, step1934]: loss 0.657069
[epoch17, step1935]: loss 0.159208
[epoch17, step1936]: loss 0.710541
[epoch17, step1937]: loss 0.697761
[epoch17, step1938]: loss 0.542344
[epoch17, step1939]: loss 0.431861
[epoch17, step1940]: loss 0.540308
[epoch17, step1941]: loss 0.613178
[epoch17, step1942]: loss 0.557546
[epoch17, step1943]: loss 0.205121
[epoch17, step1944]: loss 0.567185
[epoch17, step1945]: loss 0.697562
[epoch17, step1946]: loss 0.540415
[epoch17, step1947]: loss 0.721043
[epoch17, step1948]: loss 0.675517
[epoch17, step1949]: loss 0.396600
[epoch17, step1950]: loss 0.610267
[epoch17, step1951]: loss 0.511116
[epoch17, step1952]: loss 0.617551
[epoch17, step1953]: loss 0.708276
[epoch17, step1954]: loss 0.578239
[epoch17, step1955]: loss 0.602120
[epoch17, step1956]: loss 0.227703
[epoch17, step1957]: loss 0.537078
[epoch17, step1958]: loss 0.408229
[epoch17, step1959]: loss 0.482012
[epoch17, step1960]: loss 0.507763
[epoch17, step1961]: loss 0.553733
[epoch17, step1962]: loss 0.590207
[epoch17, step1963]: loss 0.584718
[epoch17, step1964]: loss 0.583168
[epoch17, step1965]: loss 0.291447
[epoch17, step1966]: loss 0.490141
[epoch17, step1967]: loss 0.433764
[epoch17, step1968]: loss 0.426996
[epoch17, step1969]: loss 0.615695
[epoch17, step1970]: loss 0.522596
[epoch17, step1971]: loss 0.733322
[epoch17, step1972]: loss 0.277470
[epoch17, step1973]: loss 0.669967
[epoch17, step1974]: loss 0.589781
[epoch17, step1975]: loss 0.565142
[epoch17, step1976]: loss 0.470252
[epoch17, step1977]: loss 0.440320
[epoch17, step1978]: loss 0.672511
[epoch17, step1979]: loss 0.500242
[epoch17, step1980]: loss 0.729699
[epoch17, step1981]: loss 0.738787
[epoch17, step1982]: loss 0.495781
[epoch17, step1983]: loss 0.482322
[epoch17, step1984]: loss 0.489190
[epoch17, step1985]: loss 0.497903
[epoch17, step1986]: loss 0.503852
[epoch17, step1987]: loss 0.446281
[epoch17, step1988]: loss 0.537964
[epoch17, step1989]: loss 0.740809
[epoch17, step1990]: loss 0.452325
[epoch17, step1991]: loss 0.736234
[epoch17, step1992]: loss 0.424367
[epoch17, step1993]: loss 0.688834
[epoch17, step1994]: loss 0.524741
[epoch17, step1995]: loss 0.663184
[epoch17, step1996]: loss 0.500652
[epoch17, step1997]: loss 0.486834
[epoch17, step1998]: loss 0.708929
[epoch17, step1999]: loss 0.718163
[epoch17, step2000]: loss 0.356297
[epoch17, step2001]: loss 0.347004
[epoch17, step2002]: loss 0.419631
[epoch17, step2003]: loss 0.436323
[epoch17, step2004]: loss 0.736596
[epoch17, step2005]: loss 0.708798
[epoch17, step2006]: loss 0.308874
[epoch17, step2007]: loss 0.550801
[epoch17, step2008]: loss 0.757961
[epoch17, step2009]: loss 0.649407
[epoch17, step2010]: loss 0.503785
[epoch17, step2011]: loss 0.787946
[epoch17, step2012]: loss 0.584884
[epoch17, step2013]: loss 0.381756
[epoch17, step2014]: loss 0.622466
[epoch17, step2015]: loss 0.551916
[epoch17, step2016]: loss 0.626220
[epoch17, step2017]: loss 0.375327
[epoch17, step2018]: loss 0.567088
[epoch17, step2019]: loss 0.158113
[epoch17, step2020]: loss 0.420593
[epoch17, step2021]: loss 0.561017
[epoch17, step2022]: loss 0.641891
[epoch17, step2023]: loss 0.521676
[epoch17, step2024]: loss 0.766133
[epoch17, step2025]: loss 0.490309
[epoch17, step2026]: loss 0.278586
[epoch17, step2027]: loss 0.381871
[epoch17, step2028]: loss 0.635842
[epoch17, step2029]: loss 0.711794
[epoch17, step2030]: loss 0.541494
[epoch17, step2031]: loss 0.648461
[epoch17, step2032]: loss 0.551756
[epoch17, step2033]: loss 0.529780
[epoch17, step2034]: loss 0.417626
[epoch17, step2035]: loss 0.324106
[epoch17, step2036]: loss 0.610920
[epoch17, step2037]: loss 0.580805
[epoch17, step2038]: loss 0.548925
[epoch17, step2039]: loss 0.739861
[epoch17, step2040]: loss 0.469783
[epoch17, step2041]: loss 0.536546
[epoch17, step2042]: loss 0.708011
[epoch17, step2043]: loss 0.557441
[epoch17, step2044]: loss 0.614589
[epoch17, step2045]: loss 0.646821
[epoch17, step2046]: loss 0.634309
[epoch17, step2047]: loss 0.360064
[epoch17, step2048]: loss 0.298777
[epoch17, step2049]: loss 0.613240
[epoch17, step2050]: loss 0.395448
[epoch17, step2051]: loss 0.772114
[epoch17, step2052]: loss 0.567089
[epoch17, step2053]: loss 0.541602
[epoch17, step2054]: loss 0.282289
[epoch17, step2055]: loss 0.406548
[epoch17, step2056]: loss 0.483371
[epoch17, step2057]: loss 0.662387
[epoch17, step2058]: loss 0.684159
[epoch17, step2059]: loss 0.360816
[epoch17, step2060]: loss 0.620341
[epoch17, step2061]: loss 0.597392
[epoch17, step2062]: loss 0.681416
[epoch17, step2063]: loss 0.234417
[epoch17, step2064]: loss 0.759847
[epoch17, step2065]: loss 0.517149
[epoch17, step2066]: loss 0.494622
[epoch17, step2067]: loss 0.590935
[epoch17, step2068]: loss 0.633677
[epoch17, step2069]: loss 0.545371
[epoch17, step2070]: loss 0.453108
[epoch17, step2071]: loss 0.480078
[epoch17, step2072]: loss 0.701032
[epoch17, step2073]: loss 0.574997
[epoch17, step2074]: loss 0.495584
[epoch17, step2075]: loss 0.394131
[epoch17, step2076]: loss 0.370609
[epoch17, step2077]: loss 0.565565
[epoch17, step2078]: loss 0.453851
[epoch17, step2079]: loss 0.446431
[epoch17, step2080]: loss 0.519660
[epoch17, step2081]: loss 0.610356
[epoch17, step2082]: loss 0.621597
[epoch17, step2083]: loss 0.578844
[epoch17, step2084]: loss 0.617068
[epoch17, step2085]: loss 0.613677
[epoch17, step2086]: loss 0.630599
[epoch17, step2087]: loss 0.326556
[epoch17, step2088]: loss 0.755455
[epoch17, step2089]: loss 0.391627
[epoch17, step2090]: loss 0.411822
[epoch17, step2091]: loss 0.486841
[epoch17, step2092]: loss 0.413380
[epoch17, step2093]: loss 0.492129
[epoch17, step2094]: loss 0.507313
[epoch17, step2095]: loss 0.677345
[epoch17, step2096]: loss 0.472085
[epoch17, step2097]: loss 0.477259
[epoch17, step2098]: loss 0.389949
[epoch17, step2099]: loss 0.392710
[epoch17, step2100]: loss 0.710274
[epoch17, step2101]: loss 0.458665
[epoch17, step2102]: loss 0.178356
[epoch17, step2103]: loss 0.657902
[epoch17, step2104]: loss 0.591652
[epoch17, step2105]: loss 0.609719
[epoch17, step2106]: loss 0.782001
[epoch17, step2107]: loss 0.604263
[epoch17, step2108]: loss 0.415421
[epoch17, step2109]: loss 0.295261
[epoch17, step2110]: loss 0.703215
[epoch17, step2111]: loss 0.480204
[epoch17, step2112]: loss 0.393582
[epoch17, step2113]: loss 0.533289
[epoch17, step2114]: loss 0.584721
[epoch17, step2115]: loss 0.426603
[epoch17, step2116]: loss 0.573370
[epoch17, step2117]: loss 0.379815
[epoch17, step2118]: loss 0.577230
[epoch17, step2119]: loss 0.335026
[epoch17, step2120]: loss 0.487571
[epoch17, step2121]: loss 0.547041
[epoch17, step2122]: loss 0.627182
[epoch17, step2123]: loss 0.368565
[epoch17, step2124]: loss 0.327261
[epoch17, step2125]: loss 0.676293
[epoch17, step2126]: loss 0.416736
[epoch17, step2127]: loss 0.633757
[epoch17, step2128]: loss 0.817276
[epoch17, step2129]: loss 0.714759
[epoch17, step2130]: loss 0.381370
[epoch17, step2131]: loss 0.604655
[epoch17, step2132]: loss 0.453325
[epoch17, step2133]: loss 0.435039
[epoch17, step2134]: loss 0.674058
[epoch17, step2135]: loss 0.567446
[epoch17, step2136]: loss 0.615171
[epoch17, step2137]: loss 0.539728
[epoch17, step2138]: loss 0.534188
[epoch17, step2139]: loss 0.631881
[epoch17, step2140]: loss 0.426813
[epoch17, step2141]: loss 0.792774
[epoch17, step2142]: loss 0.644407
[epoch17, step2143]: loss 0.506783
[epoch17, step2144]: loss 0.566747
[epoch17, step2145]: loss 0.421434
[epoch17, step2146]: loss 0.259560
[epoch17, step2147]: loss 0.638479
[epoch17, step2148]: loss 0.653830
[epoch17, step2149]: loss 0.603757
[epoch17, step2150]: loss 0.611797
[epoch17, step2151]: loss 0.584083
[epoch17, step2152]: loss 0.328402
[epoch17, step2153]: loss 0.437691
[epoch17, step2154]: loss 0.665887
[epoch17, step2155]: loss 0.566986
[epoch17, step2156]: loss 0.429344
[epoch17, step2157]: loss 0.633748
[epoch17, step2158]: loss 0.509298
[epoch17, step2159]: loss 0.606395
[epoch17, step2160]: loss 0.608855
[epoch17, step2161]: loss 0.589090
[epoch17, step2162]: loss 0.484206
[epoch17, step2163]: loss 0.540928
[epoch17, step2164]: loss 0.693098
[epoch17, step2165]: loss 0.287120
[epoch17, step2166]: loss 0.480818
[epoch17, step2167]: loss 0.531278
[epoch17, step2168]: loss 0.640162
[epoch17, step2169]: loss 0.453679
[epoch17, step2170]: loss 0.575294
[epoch17, step2171]: loss 0.253713
[epoch17, step2172]: loss 0.574188
[epoch17, step2173]: loss 0.377545
[epoch17, step2174]: loss 0.579748
[epoch17, step2175]: loss 0.584669
[epoch17, step2176]: loss 0.418372
[epoch17, step2177]: loss 0.595711
[epoch17, step2178]: loss 0.666482
[epoch17, step2179]: loss 0.368057
[epoch17, step2180]: loss 0.385148
[epoch17, step2181]: loss 0.562824
[epoch17, step2182]: loss 0.711793
[epoch17, step2183]: loss 0.494356
[epoch17, step2184]: loss 0.415635
[epoch17, step2185]: loss 0.528836
[epoch17, step2186]: loss 0.557959
[epoch17, step2187]: loss 0.453932
[epoch17, step2188]: loss 0.344564
[epoch17, step2189]: loss 0.523894
[epoch17, step2190]: loss 0.672241
[epoch17, step2191]: loss 0.357234
[epoch17, step2192]: loss 0.603763
[epoch17, step2193]: loss 0.529389
[epoch17, step2194]: loss 0.759245
[epoch17, step2195]: loss 0.431648
[epoch17, step2196]: loss 0.642654
[epoch17, step2197]: loss 0.603414
[epoch17, step2198]: loss 0.142074
[epoch17, step2199]: loss 0.575463
[epoch17, step2200]: loss 0.485568
[epoch17, step2201]: loss 0.453565
[epoch17, step2202]: loss 0.549309
[epoch17, step2203]: loss 0.559054
[epoch17, step2204]: loss 0.564210
[epoch17, step2205]: loss 0.545617
[epoch17, step2206]: loss 0.626802
[epoch17, step2207]: loss 0.490449
[epoch17, step2208]: loss 0.368953
[epoch17, step2209]: loss 0.449839
[epoch17, step2210]: loss 0.552521
[epoch17, step2211]: loss 0.387738
[epoch17, step2212]: loss 0.518083
[epoch17, step2213]: loss 0.705834
[epoch17, step2214]: loss 0.530996
[epoch17, step2215]: loss 0.630093
[epoch17, step2216]: loss 0.452177
[epoch17, step2217]: loss 0.528931
[epoch17, step2218]: loss 0.399703
[epoch17, step2219]: loss 0.625350
[epoch17, step2220]: loss 0.667051
[epoch17, step2221]: loss 0.611923
[epoch17, step2222]: loss 0.442868
[epoch17, step2223]: loss 0.501047
[epoch17, step2224]: loss 0.652376
[epoch17, step2225]: loss 0.693324
[epoch17, step2226]: loss 0.412044
[epoch17, step2227]: loss 0.406774
[epoch17, step2228]: loss 0.201685
[epoch17, step2229]: loss 0.665742
[epoch17, step2230]: loss 0.642506
[epoch17, step2231]: loss 0.604207
[epoch17, step2232]: loss 0.536192
[epoch17, step2233]: loss 0.722564
[epoch17, step2234]: loss 0.486427
[epoch17, step2235]: loss 0.704715
[epoch17, step2236]: loss 0.348792
[epoch17, step2237]: loss 0.701455
[epoch17, step2238]: loss 0.684942
[epoch17, step2239]: loss 0.467674
[epoch17, step2240]: loss 0.593248
[epoch17, step2241]: loss 0.582796
[epoch17, step2242]: loss 0.578768
[epoch17, step2243]: loss 0.491162
[epoch17, step2244]: loss 0.322568
[epoch17, step2245]: loss 0.541171
[epoch17, step2246]: loss 0.219770
[epoch17, step2247]: loss 0.462132
[epoch17, step2248]: loss 0.357937
[epoch17, step2249]: loss 0.415317
[epoch17, step2250]: loss 0.846457
[epoch17, step2251]: loss 0.344619
[epoch17, step2252]: loss 0.512669
[epoch17, step2253]: loss 0.408706
[epoch17, step2254]: loss 0.486866
[epoch17, step2255]: loss 0.656701
[epoch17, step2256]: loss 0.662323
[epoch17, step2257]: loss 0.439424
[epoch17, step2258]: loss 0.544891
[epoch17, step2259]: loss 0.443856
[epoch17, step2260]: loss 0.585322
[epoch17, step2261]: loss 0.640181
[epoch17, step2262]: loss 0.704111
[epoch17, step2263]: loss 0.499680
[epoch17, step2264]: loss 0.567009
[epoch17, step2265]: loss 0.543649
[epoch17, step2266]: loss 0.575216
[epoch17, step2267]: loss 0.584320
[epoch17, step2268]: loss 0.376401
[epoch17, step2269]: loss 0.595717
[epoch17, step2270]: loss 0.523854
[epoch17, step2271]: loss 0.461172
[epoch17, step2272]: loss 0.569862
[epoch17, step2273]: loss 0.704207
[epoch17, step2274]: loss 0.629960
[epoch17, step2275]: loss 0.608622
[epoch17, step2276]: loss 0.702391
[epoch17, step2277]: loss 0.582735
[epoch17, step2278]: loss 0.536179
[epoch17, step2279]: loss 0.501944
[epoch17, step2280]: loss 0.570128
[epoch17, step2281]: loss 0.698533
[epoch17, step2282]: loss 0.611479
[epoch17, step2283]: loss 0.542910
[epoch17, step2284]: loss 0.545670
[epoch17, step2285]: loss 0.699272
[epoch17, step2286]: loss 0.581329
[epoch17, step2287]: loss 0.634417
[epoch17, step2288]: loss 0.252238
[epoch17, step2289]: loss 0.743316
[epoch17, step2290]: loss 0.420901
[epoch17, step2291]: loss 0.570368
[epoch17, step2292]: loss 0.380072
[epoch17, step2293]: loss 0.435262
[epoch17, step2294]: loss 0.434530
[epoch17, step2295]: loss 0.460298
[epoch17, step2296]: loss 0.447432
[epoch17, step2297]: loss 0.520653
[epoch17, step2298]: loss 0.534752
[epoch17, step2299]: loss 0.712317
[epoch17, step2300]: loss 0.653694
[epoch17, step2301]: loss 0.284289
[epoch17, step2302]: loss 0.579087
[epoch17, step2303]: loss 0.338976
[epoch17, step2304]: loss 0.560825
[epoch17, step2305]: loss 0.407616
[epoch17, step2306]: loss 0.524407
[epoch17, step2307]: loss 0.494432
[epoch17, step2308]: loss 0.378548
[epoch17, step2309]: loss 0.520040
[epoch17, step2310]: loss 0.373144
[epoch17, step2311]: loss 0.425573
[epoch17, step2312]: loss 0.636161
[epoch17, step2313]: loss 0.630195
[epoch17, step2314]: loss 0.611206
[epoch17, step2315]: loss 0.514110
[epoch17, step2316]: loss 0.796416
[epoch17, step2317]: loss 0.679849
[epoch17, step2318]: loss 0.588179
[epoch17, step2319]: loss 0.465829
[epoch17, step2320]: loss 0.559766
[epoch17, step2321]: loss 0.526388
[epoch17, step2322]: loss 0.667991
[epoch17, step2323]: loss 0.671551
[epoch17, step2324]: loss 0.477402
[epoch17, step2325]: loss 0.288153
[epoch17, step2326]: loss 0.553217
[epoch17, step2327]: loss 0.432752
[epoch17, step2328]: loss 0.438847
[epoch17, step2329]: loss 0.734832
[epoch17, step2330]: loss 0.723697
[epoch17, step2331]: loss 0.553204
[epoch17, step2332]: loss 0.555186
[epoch17, step2333]: loss 0.538018
[epoch17, step2334]: loss 0.533867
[epoch17, step2335]: loss 0.609447
[epoch17, step2336]: loss 0.654628
[epoch17, step2337]: loss 0.338767
[epoch17, step2338]: loss 0.595150
[epoch17, step2339]: loss 0.543123
[epoch17, step2340]: loss 0.559389
[epoch17, step2341]: loss 0.571570
[epoch17, step2342]: loss 0.487401
[epoch17, step2343]: loss 0.406472
[epoch17, step2344]: loss 0.442356
[epoch17, step2345]: loss 0.568449
[epoch17, step2346]: loss 0.492531
[epoch17, step2347]: loss 0.468706
[epoch17, step2348]: loss 0.680605
[epoch17, step2349]: loss 0.444681
[epoch17, step2350]: loss 0.608253
[epoch17, step2351]: loss 0.417504
[epoch17, step2352]: loss 0.592059
[epoch17, step2353]: loss 0.626732
[epoch17, step2354]: loss 0.595179
[epoch17, step2355]: loss 0.399288
[epoch17, step2356]: loss 0.568762
[epoch17, step2357]: loss 0.555861
[epoch17, step2358]: loss 0.655898
[epoch17, step2359]: loss 0.636955
[epoch17, step2360]: loss 0.679829
[epoch17, step2361]: loss 0.693417
[epoch17, step2362]: loss 0.441032
[epoch17, step2363]: loss 0.506832
[epoch17, step2364]: loss 0.450158
[epoch17, step2365]: loss 0.655831
[epoch17, step2366]: loss 0.525442
[epoch17, step2367]: loss 0.692617
[epoch17, step2368]: loss 0.507840
[epoch17, step2369]: loss 0.710176
[epoch17, step2370]: loss 0.522584
[epoch17, step2371]: loss 0.696823
[epoch17, step2372]: loss 0.467257
[epoch17, step2373]: loss 0.430549
[epoch17, step2374]: loss 0.685521
[epoch17, step2375]: loss 0.337776
[epoch17, step2376]: loss 0.614075
[epoch17, step2377]: loss 0.493027
[epoch17, step2378]: loss 0.796735
[epoch17, step2379]: loss 0.340844
[epoch17, step2380]: loss 0.413502
[epoch17, step2381]: loss 0.649613
[epoch17, step2382]: loss 0.607120
[epoch17, step2383]: loss 0.550548
[epoch17, step2384]: loss 0.706442
[epoch17, step2385]: loss 0.730810
[epoch17, step2386]: loss 0.538534
[epoch17, step2387]: loss 0.527308
[epoch17, step2388]: loss 0.272737
[epoch17, step2389]: loss 0.660231
[epoch17, step2390]: loss 0.624176
[epoch17, step2391]: loss 0.509384
[epoch17, step2392]: loss 0.350030
[epoch17, step2393]: loss 0.312373
[epoch17, step2394]: loss 0.592834
[epoch17, step2395]: loss 0.535670
[epoch17, step2396]: loss 0.487727
[epoch17, step2397]: loss 0.463704
[epoch17, step2398]: loss 0.650722
[epoch17, step2399]: loss 0.597559
[epoch17, step2400]: loss 0.525784
[epoch17, step2401]: loss 0.490219
[epoch17, step2402]: loss 0.664861
[epoch17, step2403]: loss 0.555768
[epoch17, step2404]: loss 0.523802
[epoch17, step2405]: loss 0.496694
[epoch17, step2406]: loss 0.430228
[epoch17, step2407]: loss 0.522109
[epoch17, step2408]: loss 0.428741
[epoch17, step2409]: loss 0.603490
[epoch17, step2410]: loss 0.567119
[epoch17, step2411]: loss 0.656564
[epoch17, step2412]: loss 0.831314
[epoch17, step2413]: loss 0.485665
[epoch17, step2414]: loss 0.257099
[epoch17, step2415]: loss 0.627169
[epoch17, step2416]: loss 0.573109
[epoch17, step2417]: loss 0.397987
[epoch17, step2418]: loss 0.622431
[epoch17, step2419]: loss 0.618077
[epoch17, step2420]: loss 0.567951
[epoch17, step2421]: loss 0.770020
[epoch17, step2422]: loss 0.471792
[epoch17, step2423]: loss 0.501671
[epoch17, step2424]: loss 0.448712
[epoch17, step2425]: loss 0.736169
[epoch17, step2426]: loss 0.581697
[epoch17, step2427]: loss 0.451219
[epoch17, step2428]: loss 0.550843
[epoch17, step2429]: loss 0.409644
[epoch17, step2430]: loss 0.606115
[epoch17, step2431]: loss 0.717085
[epoch17, step2432]: loss 0.681453
[epoch17, step2433]: loss 0.479849
[epoch17, step2434]: loss 0.514892
[epoch17, step2435]: loss 0.666529
[epoch17, step2436]: loss 0.343740
[epoch17, step2437]: loss 0.460547
[epoch17, step2438]: loss 0.426450
[epoch17, step2439]: loss 0.690210
[epoch17, step2440]: loss 0.583136
[epoch17, step2441]: loss 0.587415
[epoch17, step2442]: loss 0.438527
[epoch17, step2443]: loss 0.581523
[epoch17, step2444]: loss 0.421236
[epoch17, step2445]: loss 0.567578
[epoch17, step2446]: loss 0.426905
[epoch17, step2447]: loss 0.636826
[epoch17, step2448]: loss 0.637830
[epoch17, step2449]: loss 0.550831
[epoch17, step2450]: loss 0.755874
[epoch17, step2451]: loss 0.478559
[epoch17, step2452]: loss 0.523365
[epoch17, step2453]: loss 0.383675
[epoch17, step2454]: loss 0.279429
[epoch17, step2455]: loss 0.162253
[epoch17, step2456]: loss 0.769536
[epoch17, step2457]: loss 0.543444
[epoch17, step2458]: loss 0.659571
[epoch17, step2459]: loss 0.420147
[epoch17, step2460]: loss 0.374686
[epoch17, step2461]: loss 0.368128
[epoch17, step2462]: loss 0.577135
[epoch17, step2463]: loss 0.613074
[epoch17, step2464]: loss 0.559258
[epoch17, step2465]: loss 0.340988
[epoch17, step2466]: loss 0.159748
[epoch17, step2467]: loss 0.214861
[epoch17, step2468]: loss 0.546310
[epoch17, step2469]: loss 0.589632
[epoch17, step2470]: loss 0.542170
[epoch17, step2471]: loss 0.555724
[epoch17, step2472]: loss 0.400072
[epoch17, step2473]: loss 0.552429
[epoch17, step2474]: loss 0.542650
[epoch17, step2475]: loss 0.511302
[epoch17, step2476]: loss 0.487262
[epoch17, step2477]: loss 0.418911
[epoch17, step2478]: loss 0.551560
[epoch17, step2479]: loss 0.735152
[epoch17, step2480]: loss 0.545958
[epoch17, step2481]: loss 0.524373
[epoch17, step2482]: loss 0.395598
[epoch17, step2483]: loss 0.443843
[epoch17, step2484]: loss 0.423149
[epoch17, step2485]: loss 0.598934
[epoch17, step2486]: loss 0.395521
[epoch17, step2487]: loss 0.315772
[epoch17, step2488]: loss 0.544152
[epoch17, step2489]: loss 0.684173
[epoch17, step2490]: loss 0.499491
[epoch17, step2491]: loss 0.413799
[epoch17, step2492]: loss 0.613832
[epoch17, step2493]: loss 0.660983
[epoch17, step2494]: loss 0.173297
[epoch17, step2495]: loss 0.395286
[epoch17, step2496]: loss 0.569436
[epoch17, step2497]: loss 0.802717
[epoch17, step2498]: loss 0.265657
[epoch17, step2499]: loss 0.727744
[epoch17, step2500]: loss 0.693139
[epoch17, step2501]: loss 0.538167
[epoch17, step2502]: loss 0.476437
[epoch17, step2503]: loss 0.472111
[epoch17, step2504]: loss 0.556079
[epoch17, step2505]: loss 0.615421
[epoch17, step2506]: loss 0.564247
[epoch17, step2507]: loss 0.513101
[epoch17, step2508]: loss 0.329757
[epoch17, step2509]: loss 0.706004
[epoch17, step2510]: loss 0.702860
[epoch17, step2511]: loss 0.505113
[epoch17, step2512]: loss 0.657828
[epoch17, step2513]: loss 0.202158
[epoch17, step2514]: loss 0.494991
[epoch17, step2515]: loss 0.505236
[epoch17, step2516]: loss 0.590880
[epoch17, step2517]: loss 0.476043
[epoch17, step2518]: loss 0.520046
[epoch17, step2519]: loss 0.383928
[epoch17, step2520]: loss 0.476759
[epoch17, step2521]: loss 0.445717
[epoch17, step2522]: loss 0.473633
[epoch17, step2523]: loss 0.476261
[epoch17, step2524]: loss 0.643802
[epoch17, step2525]: loss 0.651086
[epoch17, step2526]: loss 0.698165
[epoch17, step2527]: loss 0.638807
[epoch17, step2528]: loss 0.572470
[epoch17, step2529]: loss 0.608522
[epoch17, step2530]: loss 0.573041
[epoch17, step2531]: loss 0.648716
[epoch17, step2532]: loss 0.564066
[epoch17, step2533]: loss 0.446249
[epoch17, step2534]: loss 0.493087
[epoch17, step2535]: loss 0.547241
[epoch17, step2536]: loss 0.429338
[epoch17, step2537]: loss 0.574945
[epoch17, step2538]: loss 0.609451
[epoch17, step2539]: loss 0.394150
[epoch17, step2540]: loss 0.546610
[epoch17, step2541]: loss 0.612954
[epoch17, step2542]: loss 0.653570
[epoch17, step2543]: loss 0.558891
[epoch17, step2544]: loss 0.486555
[epoch17, step2545]: loss 0.621251
[epoch17, step2546]: loss 0.567088
[epoch17, step2547]: loss 0.669940
[epoch17, step2548]: loss 0.771882
[epoch17, step2549]: loss 0.525853
[epoch17, step2550]: loss 0.412576
[epoch17, step2551]: loss 0.560914
[epoch17, step2552]: loss 0.639444
[epoch17, step2553]: loss 0.458902
[epoch17, step2554]: loss 0.477181
[epoch17, step2555]: loss 0.478265
[epoch17, step2556]: loss 0.444239
[epoch17, step2557]: loss 0.185864
[epoch17, step2558]: loss 0.670152
[epoch17, step2559]: loss 0.723640
[epoch17, step2560]: loss 0.406217
[epoch17, step2561]: loss 0.333252
[epoch17, step2562]: loss 0.619671
[epoch17, step2563]: loss 0.407969
[epoch17, step2564]: loss 0.478791
[epoch17, step2565]: loss 0.488376
[epoch17, step2566]: loss 0.409188
[epoch17, step2567]: loss 0.644110
[epoch17, step2568]: loss 0.524176
[epoch17, step2569]: loss 0.716037
[epoch17, step2570]: loss 0.652168
[epoch17, step2571]: loss 0.736219
[epoch17, step2572]: loss 0.518730
[epoch17, step2573]: loss 0.574470
[epoch17, step2574]: loss 0.483655
[epoch17, step2575]: loss 0.462708
[epoch17, step2576]: loss 0.297388
[epoch17, step2577]: loss 0.647367
[epoch17, step2578]: loss 0.689314
[epoch17, step2579]: loss 0.661993
[epoch17, step2580]: loss 0.538196
[epoch17, step2581]: loss 0.478599
[epoch17, step2582]: loss 0.528009
[epoch17, step2583]: loss 0.483333
[epoch17, step2584]: loss 0.343214
[epoch17, step2585]: loss 0.569533
[epoch17, step2586]: loss 0.581929
[epoch17, step2587]: loss 0.599046
[epoch17, step2588]: loss 0.411438
[epoch17, step2589]: loss 0.654427
[epoch17, step2590]: loss 0.768586
[epoch17, step2591]: loss 0.520853
[epoch17, step2592]: loss 0.495509
[epoch17, step2593]: loss 0.494098
[epoch17, step2594]: loss 0.426257
[epoch17, step2595]: loss 0.724870
[epoch17, step2596]: loss 0.497289
[epoch17, step2597]: loss 0.823515
[epoch17, step2598]: loss 0.475874
[epoch17, step2599]: loss 0.634117
[epoch17, step2600]: loss 0.526756
[epoch17, step2601]: loss 0.683017
[epoch17, step2602]: loss 0.620871
[epoch17, step2603]: loss 0.489343
[epoch17, step2604]: loss 0.383018
[epoch17, step2605]: loss 0.410903
[epoch17, step2606]: loss 0.385825
[epoch17, step2607]: loss 0.614389
[epoch17, step2608]: loss 0.509089
[epoch17, step2609]: loss 0.405281
[epoch17, step2610]: loss 0.277831
[epoch17, step2611]: loss 0.387207
[epoch17, step2612]: loss 0.885683
[epoch17, step2613]: loss 0.629081
[epoch17, step2614]: loss 0.722731
[epoch17, step2615]: loss 0.280377
[epoch17, step2616]: loss 0.155412
[epoch17, step2617]: loss 0.789722
[epoch17, step2618]: loss 0.611710
[epoch17, step2619]: loss 0.449166
[epoch17, step2620]: loss 0.471350
[epoch17, step2621]: loss 0.698466
[epoch17, step2622]: loss 0.390983
[epoch17, step2623]: loss 0.596439
[epoch17, step2624]: loss 0.381252
[epoch17, step2625]: loss 0.552148
[epoch17, step2626]: loss 0.784349
[epoch17, step2627]: loss 0.788810
[epoch17, step2628]: loss 0.288878
[epoch17, step2629]: loss 0.690139
[epoch17, step2630]: loss 0.389216
[epoch17, step2631]: loss 0.539174
[epoch17, step2632]: loss 0.456706
[epoch17, step2633]: loss 0.849146
[epoch17, step2634]: loss 0.645286
[epoch17, step2635]: loss 0.427737
[epoch17, step2636]: loss 0.476234
[epoch17, step2637]: loss 0.629859
[epoch17, step2638]: loss 0.670791
[epoch17, step2639]: loss 0.546610
[epoch17, step2640]: loss 0.598443
[epoch17, step2641]: loss 0.495171
[epoch17, step2642]: loss 0.467207
[epoch17, step2643]: loss 0.326243
[epoch17, step2644]: loss 0.368702
[epoch17, step2645]: loss 0.486617
[epoch17, step2646]: loss 0.652777
[epoch17, step2647]: loss 0.573536
[epoch17, step2648]: loss 0.427824
[epoch17, step2649]: loss 0.393045
[epoch17, step2650]: loss 0.593964
[epoch17, step2651]: loss 0.494483
[epoch17, step2652]: loss 0.642257
[epoch17, step2653]: loss 0.635685
[epoch17, step2654]: loss 0.358386
[epoch17, step2655]: loss 0.448028
[epoch17, step2656]: loss 0.506664
[epoch17, step2657]: loss 0.649475
[epoch17, step2658]: loss 0.536906
[epoch17, step2659]: loss 0.446783
[epoch17, step2660]: loss 0.529592
[epoch17, step2661]: loss 0.494687
[epoch17, step2662]: loss 0.414708
[epoch17, step2663]: loss 0.820977
[epoch17, step2664]: loss 0.397031
[epoch17, step2665]: loss 0.577355
[epoch17, step2666]: loss 0.604865
[epoch17, step2667]: loss 0.609679
[epoch17, step2668]: loss 0.511973
[epoch17, step2669]: loss 0.635644
[epoch17, step2670]: loss 0.482656
[epoch17, step2671]: loss 0.571723
[epoch17, step2672]: loss 0.295015
[epoch17, step2673]: loss 0.611196
[epoch17, step2674]: loss 0.725974
[epoch17, step2675]: loss 0.806931
[epoch17, step2676]: loss 0.559088
[epoch17, step2677]: loss 0.544186
[epoch17, step2678]: loss 0.458507
[epoch17, step2679]: loss 0.639526
[epoch17, step2680]: loss 0.528077
[epoch17, step2681]: loss 0.417166
[epoch17, step2682]: loss 0.688203
[epoch17, step2683]: loss 0.495142
[epoch17, step2684]: loss 0.512408
[epoch17, step2685]: loss 0.324106
[epoch17, step2686]: loss 0.397694
[epoch17, step2687]: loss 0.546435
[epoch17, step2688]: loss 0.702307
[epoch17, step2689]: loss 0.557972
[epoch17, step2690]: loss 0.448176
[epoch17, step2691]: loss 0.288569
[epoch17, step2692]: loss 0.662247
[epoch17, step2693]: loss 0.537696
[epoch17, step2694]: loss 0.504889
[epoch17, step2695]: loss 0.321251
[epoch17, step2696]: loss 0.556281
[epoch17, step2697]: loss 0.633993
[epoch17, step2698]: loss 0.541193
[epoch17, step2699]: loss 0.467556
[epoch17, step2700]: loss 0.487650
[epoch17, step2701]: loss 0.430542
[epoch17, step2702]: loss 0.391713
[epoch17, step2703]: loss 0.565400
[epoch17, step2704]: loss 0.478562
[epoch17, step2705]: loss 0.433138
[epoch17, step2706]: loss 0.543454
[epoch17, step2707]: loss 0.570726
[epoch17, step2708]: loss 0.453127
[epoch17, step2709]: loss 0.607693
[epoch17, step2710]: loss 0.417009
[epoch17, step2711]: loss 0.252686
[epoch17, step2712]: loss 0.565084
[epoch17, step2713]: loss 0.595380
[epoch17, step2714]: loss 0.577460
[epoch17, step2715]: loss 0.564654
[epoch17, step2716]: loss 0.361966
[epoch17, step2717]: loss 0.610731
[epoch17, step2718]: loss 0.700044
[epoch17, step2719]: loss 0.358845
[epoch17, step2720]: loss 0.634960
[epoch17, step2721]: loss 0.435049
[epoch17, step2722]: loss 0.460437
[epoch17, step2723]: loss 0.524176
[epoch17, step2724]: loss 0.641592
[epoch17, step2725]: loss 0.270153
[epoch17, step2726]: loss 0.526175
[epoch17, step2727]: loss 0.629932
[epoch17, step2728]: loss 0.668488
[epoch17, step2729]: loss 0.770326
[epoch17, step2730]: loss 0.412533
[epoch17, step2731]: loss 0.705339
[epoch17, step2732]: loss 0.357423
[epoch17, step2733]: loss 0.506325
[epoch17, step2734]: loss 0.631844
[epoch17, step2735]: loss 0.426752
[epoch17, step2736]: loss 0.271666
[epoch17, step2737]: loss 0.473993
[epoch17, step2738]: loss 0.596268
[epoch17, step2739]: loss 0.603588
[epoch17, step2740]: loss 0.411974
[epoch17, step2741]: loss 0.428763
[epoch17, step2742]: loss 0.528827
[epoch17, step2743]: loss 0.563771
[epoch17, step2744]: loss 0.599860
[epoch17, step2745]: loss 0.504766
[epoch17, step2746]: loss 0.399467
[epoch17, step2747]: loss 0.658979
[epoch17, step2748]: loss 0.503299
[epoch17, step2749]: loss 0.461988
[epoch17, step2750]: loss 0.550197
[epoch17, step2751]: loss 0.550480
[epoch17, step2752]: loss 0.576876
[epoch17, step2753]: loss 0.369256
[epoch17, step2754]: loss 0.374598
[epoch17, step2755]: loss 0.565097
[epoch17, step2756]: loss 0.433106
[epoch17, step2757]: loss 0.533288
[epoch17, step2758]: loss 0.248906
[epoch17, step2759]: loss 0.552705
[epoch17, step2760]: loss 0.425240
[epoch17, step2761]: loss 0.753915
[epoch17, step2762]: loss 0.776151
[epoch17, step2763]: loss 0.468259
[epoch17, step2764]: loss 0.315616
[epoch17, step2765]: loss 0.629736
[epoch17, step2766]: loss 0.624314
[epoch17, step2767]: loss 0.421670
[epoch17, step2768]: loss 0.599481
[epoch17, step2769]: loss 0.463648
[epoch17, step2770]: loss 0.591110
[epoch17, step2771]: loss 0.607907
[epoch17, step2772]: loss 0.606619
[epoch17, step2773]: loss 0.599603
[epoch17, step2774]: loss 0.572870
[epoch17, step2775]: loss 0.566187
[epoch17, step2776]: loss 0.726226
[epoch17, step2777]: loss 0.204427
[epoch17, step2778]: loss 0.556492
[epoch17, step2779]: loss 0.398594
[epoch17, step2780]: loss 0.438437
[epoch17, step2781]: loss 0.727174
[epoch17, step2782]: loss 0.455939
[epoch17, step2783]: loss 0.589813
[epoch17, step2784]: loss 0.495576
[epoch17, step2785]: loss 0.639338
[epoch17, step2786]: loss 0.700200
[epoch17, step2787]: loss 0.595898
[epoch17, step2788]: loss 0.604080
[epoch17, step2789]: loss 0.557258
[epoch17, step2790]: loss 0.580272
[epoch17, step2791]: loss 0.537358
[epoch17, step2792]: loss 0.368365
[epoch17, step2793]: loss 0.550364
[epoch17, step2794]: loss 0.490542
[epoch17, step2795]: loss 0.595999
[epoch17, step2796]: loss 0.587186
[epoch17, step2797]: loss 0.555205
[epoch17, step2798]: loss 0.513871
[epoch17, step2799]: loss 0.559642
[epoch17, step2800]: loss 0.562840
[epoch17, step2801]: loss 0.562572
[epoch17, step2802]: loss 0.577455
[epoch17, step2803]: loss 0.406287
[epoch17, step2804]: loss 0.537866
[epoch17, step2805]: loss 0.682235
[epoch17, step2806]: loss 0.439901
[epoch17, step2807]: loss 0.527207
[epoch17, step2808]: loss 0.510931
[epoch17, step2809]: loss 0.476166
[epoch17, step2810]: loss 0.416337
[epoch17, step2811]: loss 0.136116
[epoch17, step2812]: loss 0.294561
[epoch17, step2813]: loss 0.592072
[epoch17, step2814]: loss 0.478695
[epoch17, step2815]: loss 0.509491
[epoch17, step2816]: loss 0.597606
[epoch17, step2817]: loss 0.488590
[epoch17, step2818]: loss 0.491728
[epoch17, step2819]: loss 0.358297
[epoch17, step2820]: loss 0.653601
[epoch17, step2821]: loss 0.466579
[epoch17, step2822]: loss 0.552499
[epoch17, step2823]: loss 0.567222
[epoch17, step2824]: loss 0.428254
[epoch17, step2825]: loss 0.555948
[epoch17, step2826]: loss 0.654583
[epoch17, step2827]: loss 0.701498
[epoch17, step2828]: loss 0.287301
[epoch17, step2829]: loss 0.494094
[epoch17, step2830]: loss 0.440481
[epoch17, step2831]: loss 0.515240
[epoch17, step2832]: loss 0.293371
[epoch17, step2833]: loss 0.279087
[epoch17, step2834]: loss 0.543851
[epoch17, step2835]: loss 0.365261
[epoch17, step2836]: loss 0.553799
[epoch17, step2837]: loss 0.571364
[epoch17, step2838]: loss 0.696884
[epoch17, step2839]: loss 0.189476
[epoch17, step2840]: loss 0.370789
[epoch17, step2841]: loss 0.676372
[epoch17, step2842]: loss 0.148695
[epoch17, step2843]: loss 0.824398
[epoch17, step2844]: loss 0.746573
[epoch17, step2845]: loss 0.538802
[epoch17, step2846]: loss 0.449969
[epoch17, step2847]: loss 0.752705
[epoch17, step2848]: loss 0.590466
[epoch17, step2849]: loss 0.428274
[epoch17, step2850]: loss 0.688862
[epoch17, step2851]: loss 0.576308
[epoch17, step2852]: loss 0.413474
[epoch17, step2853]: loss 0.544825
[epoch17, step2854]: loss 0.599842
[epoch17, step2855]: loss 0.719251
[epoch17, step2856]: loss 0.669882
[epoch17, step2857]: loss 0.540523
[epoch17, step2858]: loss 0.487957
[epoch17, step2859]: loss 0.571911
[epoch17, step2860]: loss 0.247225
[epoch17, step2861]: loss 0.478890
[epoch17, step2862]: loss 0.402915
[epoch17, step2863]: loss 0.598563
[epoch17, step2864]: loss 0.657492
[epoch17, step2865]: loss 0.501937
[epoch17, step2866]: loss 0.669622
[epoch17, step2867]: loss 0.607319
[epoch17, step2868]: loss 0.393849
[epoch17, step2869]: loss 0.446786
[epoch17, step2870]: loss 0.744815
[epoch17, step2871]: loss 0.615342
[epoch17, step2872]: loss 0.741278
[epoch17, step2873]: loss 0.501882
[epoch17, step2874]: loss 0.722426
[epoch17, step2875]: loss 0.520392
[epoch17, step2876]: loss 0.319921
[epoch17, step2877]: loss 0.586112
[epoch17, step2878]: loss 0.488092
[epoch17, step2879]: loss 0.359579
[epoch17, step2880]: loss 0.535583
[epoch17, step2881]: loss 0.663884
[epoch17, step2882]: loss 0.261879
[epoch17, step2883]: loss 0.581298
[epoch17, step2884]: loss 0.641284
[epoch17, step2885]: loss 0.623389
[epoch17, step2886]: loss 0.302247
[epoch17, step2887]: loss 0.614968
[epoch17, step2888]: loss 0.601606
[epoch17, step2889]: loss 0.626310
[epoch17, step2890]: loss 0.488235
[epoch17, step2891]: loss 0.615028
[epoch17, step2892]: loss 0.394648
[epoch17, step2893]: loss 0.361984
[epoch17, step2894]: loss 0.256621
[epoch17, step2895]: loss 0.628383
[epoch17, step2896]: loss 0.752588
[epoch17, step2897]: loss 0.494290
[epoch17, step2898]: loss 0.530756
[epoch17, step2899]: loss 0.551670
[epoch17, step2900]: loss 0.487568
[epoch17, step2901]: loss 0.568658
[epoch17, step2902]: loss 0.312497
[epoch17, step2903]: loss 0.688233
[epoch17, step2904]: loss 0.421867
[epoch17, step2905]: loss 0.758809
[epoch17, step2906]: loss 0.546251
[epoch17, step2907]: loss 0.553712
[epoch17, step2908]: loss 0.330951
[epoch17, step2909]: loss 0.516719
[epoch17, step2910]: loss 0.414534
[epoch17, step2911]: loss 0.672701
[epoch17, step2912]: loss 0.412514
[epoch17, step2913]: loss 0.564236
[epoch17, step2914]: loss 0.390798
[epoch17, step2915]: loss 0.151971
[epoch17, step2916]: loss 0.689447
[epoch17, step2917]: loss 0.590027
[epoch17, step2918]: loss 0.456454
[epoch17, step2919]: loss 0.524973
[epoch17, step2920]: loss 0.646132
[epoch17, step2921]: loss 0.592786
[epoch17, step2922]: loss 0.509489
[epoch17, step2923]: loss 0.575061
[epoch17, step2924]: loss 0.826228
[epoch17, step2925]: loss 0.597902
[epoch17, step2926]: loss 0.525211
[epoch17, step2927]: loss 0.676084
[epoch17, step2928]: loss 0.331196
[epoch17, step2929]: loss 0.393705
[epoch17, step2930]: loss 0.640163
[epoch17, step2931]: loss 0.633196
[epoch17, step2932]: loss 0.641252
[epoch17, step2933]: loss 0.517247
[epoch17, step2934]: loss 0.541988
[epoch17, step2935]: loss 0.733894
[epoch17, step2936]: loss 0.538521
[epoch17, step2937]: loss 0.445022
[epoch17, step2938]: loss 0.651329
[epoch17, step2939]: loss 0.667635
[epoch17, step2940]: loss 0.333144
[epoch17, step2941]: loss 0.695827
[epoch17, step2942]: loss 0.635039
[epoch17, step2943]: loss 0.621174
[epoch17, step2944]: loss 0.156236
[epoch17, step2945]: loss 0.460365
[epoch17, step2946]: loss 0.473949
[epoch17, step2947]: loss 0.190277
[epoch17, step2948]: loss 0.527793
[epoch17, step2949]: loss 0.236249
[epoch17, step2950]: loss 0.485957
[epoch17, step2951]: loss 0.551548
[epoch17, step2952]: loss 0.580325
[epoch17, step2953]: loss 0.798770
[epoch17, step2954]: loss 0.611027
[epoch17, step2955]: loss 0.702728
[epoch17, step2956]: loss 0.508897
[epoch17, step2957]: loss 0.526459
[epoch17, step2958]: loss 0.493774
[epoch17, step2959]: loss 0.760090
[epoch17, step2960]: loss 0.656705
[epoch17, step2961]: loss 0.436940
[epoch17, step2962]: loss 0.735438
[epoch17, step2963]: loss 0.255430
[epoch17, step2964]: loss 0.409546
[epoch17, step2965]: loss 0.667293
[epoch17, step2966]: loss 0.633011
[epoch17, step2967]: loss 0.156018
[epoch17, step2968]: loss 0.355401
[epoch17, step2969]: loss 0.244541
[epoch17, step2970]: loss 0.546831
[epoch17, step2971]: loss 0.287990
[epoch17, step2972]: loss 0.322057
[epoch17, step2973]: loss 0.314582
[epoch17, step2974]: loss 0.528698
[epoch17, step2975]: loss 0.268309
[epoch17, step2976]: loss 0.531664
[epoch17, step2977]: loss 0.392253
[epoch17, step2978]: loss 0.596640
[epoch17, step2979]: loss 0.778920
[epoch17, step2980]: loss 0.644955
[epoch17, step2981]: loss 0.626743
[epoch17, step2982]: loss 0.693673
[epoch17, step2983]: loss 0.665680
[epoch17, step2984]: loss 0.364401
[epoch17, step2985]: loss 0.502956
[epoch17, step2986]: loss 0.536377
[epoch17, step2987]: loss 0.463471
[epoch17, step2988]: loss 0.638578
[epoch17, step2989]: loss 0.618290
[epoch17, step2990]: loss 0.576783
[epoch17, step2991]: loss 0.718317
[epoch17, step2992]: loss 0.424676
[epoch17, step2993]: loss 0.607989
[epoch17, step2994]: loss 0.670149
[epoch17, step2995]: loss 0.390570
[epoch17, step2996]: loss 0.391984
[epoch17, step2997]: loss 0.517265
[epoch17, step2998]: loss 0.361865
[epoch17, step2999]: loss 0.535444
[epoch17, step3000]: loss 0.273572
[epoch17, step3001]: loss 0.562019
[epoch17, step3002]: loss 0.421228
[epoch17, step3003]: loss 0.550872
[epoch17, step3004]: loss 0.516411
[epoch17, step3005]: loss 0.519630
[epoch17, step3006]: loss 0.541377
[epoch17, step3007]: loss 0.408703
[epoch17, step3008]: loss 0.548029
[epoch17, step3009]: loss 0.165081
[epoch17, step3010]: loss 0.830445
[epoch17, step3011]: loss 0.427919
[epoch17, step3012]: loss 0.608626
[epoch17, step3013]: loss 0.571590
[epoch17, step3014]: loss 0.495301
[epoch17, step3015]: loss 0.471095
[epoch17, step3016]: loss 0.164703
[epoch17, step3017]: loss 0.552841
[epoch17, step3018]: loss 0.251700
[epoch17, step3019]: loss 0.437786
[epoch17, step3020]: loss 0.700568
[epoch17, step3021]: loss 0.449565
[epoch17, step3022]: loss 0.437393
[epoch17, step3023]: loss 0.542180
[epoch17, step3024]: loss 0.611441
[epoch17, step3025]: loss 0.302438
[epoch17, step3026]: loss 0.803326
[epoch17, step3027]: loss 0.552382
[epoch17, step3028]: loss 0.689902
[epoch17, step3029]: loss 0.617067
[epoch17, step3030]: loss 0.470979
[epoch17, step3031]: loss 0.388571
[epoch17, step3032]: loss 0.142786
[epoch17, step3033]: loss 0.451361
[epoch17, step3034]: loss 0.709463
[epoch17, step3035]: loss 0.572484
[epoch17, step3036]: loss 0.373114
[epoch17, step3037]: loss 0.632080
[epoch17, step3038]: loss 0.617422
[epoch17, step3039]: loss 0.569067
[epoch17, step3040]: loss 0.362604
[epoch17, step3041]: loss 0.419656
[epoch17, step3042]: loss 0.648976
[epoch17, step3043]: loss 0.528971
[epoch17, step3044]: loss 0.356101
[epoch17, step3045]: loss 0.383811
[epoch17, step3046]: loss 0.424398
[epoch17, step3047]: loss 0.450612
[epoch17, step3048]: loss 0.761844
[epoch17, step3049]: loss 0.511501
[epoch17, step3050]: loss 0.560222
[epoch17, step3051]: loss 0.515594
[epoch17, step3052]: loss 0.669376
[epoch17, step3053]: loss 0.446879
[epoch17, step3054]: loss 0.548983
[epoch17, step3055]: loss 0.657139
[epoch17, step3056]: loss 0.520490
[epoch17, step3057]: loss 0.579403
[epoch17, step3058]: loss 0.679487
[epoch17, step3059]: loss 0.628149
[epoch17, step3060]: loss 0.694077
[epoch17, step3061]: loss 0.638675
[epoch17, step3062]: loss 0.555608
[epoch17, step3063]: loss 0.553916
[epoch17, step3064]: loss 0.626859
[epoch17, step3065]: loss 0.524362
[epoch17, step3066]: loss 0.596919
[epoch17, step3067]: loss 0.421383
[epoch17, step3068]: loss 0.611933
[epoch17, step3069]: loss 0.636927
[epoch17, step3070]: loss 0.601639
[epoch17, step3071]: loss 0.333702
[epoch17, step3072]: loss 0.457129
[epoch17, step3073]: loss 0.109225
[epoch17, step3074]: loss 0.579910
[epoch17, step3075]: loss 0.489207
[epoch17, step3076]: loss 0.593219

[epoch17]: avg loss 0.593219

[epoch18, step1]: loss 0.743353
[epoch18, step2]: loss 0.603406
[epoch18, step3]: loss 0.455718
[epoch18, step4]: loss 0.699302
[epoch18, step5]: loss 0.427890
[epoch18, step6]: loss 0.378223
[epoch18, step7]: loss 0.393776
[epoch18, step8]: loss 0.428440
[epoch18, step9]: loss 0.351069
[epoch18, step10]: loss 0.617593
[epoch18, step11]: loss 0.360613
[epoch18, step12]: loss 0.659510
[epoch18, step13]: loss 0.180790
[epoch18, step14]: loss 0.440735
[epoch18, step15]: loss 0.252635
[epoch18, step16]: loss 0.290888
[epoch18, step17]: loss 0.369891
[epoch18, step18]: loss 0.688006
[epoch18, step19]: loss 0.561941
[epoch18, step20]: loss 0.561803
[epoch18, step21]: loss 0.443245
[epoch18, step22]: loss 0.269110
[epoch18, step23]: loss 0.662510
[epoch18, step24]: loss 0.308197
[epoch18, step25]: loss 0.655192
[epoch18, step26]: loss 0.550220
[epoch18, step27]: loss 0.753673
[epoch18, step28]: loss 0.563257
[epoch18, step29]: loss 0.429687
[epoch18, step30]: loss 0.539597
[epoch18, step31]: loss 0.692780
[epoch18, step32]: loss 0.557451
[epoch18, step33]: loss 0.535980
[epoch18, step34]: loss 0.601632
[epoch18, step35]: loss 0.445093
[epoch18, step36]: loss 0.442553
[epoch18, step37]: loss 0.717823
[epoch18, step38]: loss 0.715885
[epoch18, step39]: loss 0.720452
[epoch18, step40]: loss 0.527541
[epoch18, step41]: loss 0.539204
[epoch18, step42]: loss 0.566983
[epoch18, step43]: loss 0.717036
[epoch18, step44]: loss 0.473109
[epoch18, step45]: loss 0.291656
[epoch18, step46]: loss 0.556658
[epoch18, step47]: loss 0.711663
[epoch18, step48]: loss 0.257070
[epoch18, step49]: loss 0.634718
[epoch18, step50]: loss 0.300437
[epoch18, step51]: loss 0.497588
[epoch18, step52]: loss 0.552904
[epoch18, step53]: loss 0.467844
[epoch18, step54]: loss 0.705454
[epoch18, step55]: loss 0.567582
[epoch18, step56]: loss 0.549838
[epoch18, step57]: loss 0.488185
[epoch18, step58]: loss 0.548531
[epoch18, step59]: loss 0.542844
[epoch18, step60]: loss 0.749317
[epoch18, step61]: loss 0.548962
[epoch18, step62]: loss 0.784315
[epoch18, step63]: loss 0.649044
[epoch18, step64]: loss 0.527779
[epoch18, step65]: loss 0.640876
[epoch18, step66]: loss 0.352200
[epoch18, step67]: loss 0.498733
[epoch18, step68]: loss 0.642041
[epoch18, step69]: loss 0.566440
[epoch18, step70]: loss 0.375304
[epoch18, step71]: loss 0.404545
[epoch18, step72]: loss 0.146752
[epoch18, step73]: loss 0.401077
[epoch18, step74]: loss 0.501768
[epoch18, step75]: loss 0.302349
[epoch18, step76]: loss 0.409851
[epoch18, step77]: loss 0.567238
[epoch18, step78]: loss 0.532862
[epoch18, step79]: loss 0.683564
[epoch18, step80]: loss 0.476964
[epoch18, step81]: loss 0.715901
[epoch18, step82]: loss 0.484573
[epoch18, step83]: loss 0.857227
[epoch18, step84]: loss 0.481752
[epoch18, step85]: loss 0.651978
[epoch18, step86]: loss 0.491024
[epoch18, step87]: loss 0.534223
[epoch18, step88]: loss 0.469642
[epoch18, step89]: loss 0.694801
[epoch18, step90]: loss 0.313042
[epoch18, step91]: loss 0.331519
[epoch18, step92]: loss 0.530619
[epoch18, step93]: loss 0.600727
[epoch18, step94]: loss 0.566733
[epoch18, step95]: loss 0.509426
[epoch18, step96]: loss 0.433307
[epoch18, step97]: loss 0.705892
[epoch18, step98]: loss 0.450039
[epoch18, step99]: loss 0.583770
[epoch18, step100]: loss 0.453777
[epoch18, step101]: loss 0.759917
[epoch18, step102]: loss 0.710506
[epoch18, step103]: loss 0.412601
[epoch18, step104]: loss 0.424788
[epoch18, step105]: loss 0.534798
[epoch18, step106]: loss 0.586871
[epoch18, step107]: loss 0.409509
[epoch18, step108]: loss 0.751752
[epoch18, step109]: loss 0.338561
[epoch18, step110]: loss 0.683712
[epoch18, step111]: loss 0.715887
[epoch18, step112]: loss 0.525087
[epoch18, step113]: loss 0.658749
[epoch18, step114]: loss 0.648538
[epoch18, step115]: loss 0.641304
[epoch18, step116]: loss 0.540721
[epoch18, step117]: loss 0.628493
[epoch18, step118]: loss 0.614564
[epoch18, step119]: loss 0.445733
[epoch18, step120]: loss 0.402131
[epoch18, step121]: loss 0.518430
[epoch18, step122]: loss 0.523458
[epoch18, step123]: loss 0.398809
[epoch18, step124]: loss 0.272528
[epoch18, step125]: loss 0.543712
[epoch18, step126]: loss 0.690551
[epoch18, step127]: loss 0.534400
[epoch18, step128]: loss 0.516200
[epoch18, step129]: loss 0.496086
[epoch18, step130]: loss 0.523950
[epoch18, step131]: loss 0.449439
[epoch18, step132]: loss 0.493845
[epoch18, step133]: loss 0.340679
[epoch18, step134]: loss 0.678045
[epoch18, step135]: loss 0.505767
[epoch18, step136]: loss 0.705471
[epoch18, step137]: loss 0.622426
[epoch18, step138]: loss 0.464257
[epoch18, step139]: loss 0.676575
[epoch18, step140]: loss 0.766272
[epoch18, step141]: loss 0.434270
[epoch18, step142]: loss 0.530109
[epoch18, step143]: loss 0.577637
[epoch18, step144]: loss 0.565116
[epoch18, step145]: loss 0.397256
[epoch18, step146]: loss 0.518088
[epoch18, step147]: loss 0.433576
[epoch18, step148]: loss 0.370596
[epoch18, step149]: loss 0.610969
[epoch18, step150]: loss 0.523698
[epoch18, step151]: loss 0.213862
[epoch18, step152]: loss 0.545429
[epoch18, step153]: loss 0.569178
[epoch18, step154]: loss 0.554913
[epoch18, step155]: loss 0.664427
[epoch18, step156]: loss 0.583294
[epoch18, step157]: loss 0.570664
[epoch18, step158]: loss 0.491434
[epoch18, step159]: loss 0.672707
[epoch18, step160]: loss 0.386476
[epoch18, step161]: loss 0.596851
[epoch18, step162]: loss 0.294704
[epoch18, step163]: loss 0.419035
[epoch18, step164]: loss 0.558029
[epoch18, step165]: loss 0.445518
[epoch18, step166]: loss 0.627327
[epoch18, step167]: loss 0.644516
[epoch18, step168]: loss 0.367325
[epoch18, step169]: loss 0.461656
[epoch18, step170]: loss 0.685107
[epoch18, step171]: loss 0.556961
[epoch18, step172]: loss 0.785619
[epoch18, step173]: loss 0.483978
[epoch18, step174]: loss 0.433776
[epoch18, step175]: loss 0.475965
[epoch18, step176]: loss 0.596829
[epoch18, step177]: loss 0.442370
[epoch18, step178]: loss 0.574173
[epoch18, step179]: loss 0.292599
[epoch18, step180]: loss 0.417212
[epoch18, step181]: loss 0.467626
[epoch18, step182]: loss 0.503197
[epoch18, step183]: loss 0.335537
[epoch18, step184]: loss 0.297289
[epoch18, step185]: loss 0.590984
[epoch18, step186]: loss 0.572010
[epoch18, step187]: loss 0.525342
[epoch18, step188]: loss 0.449383
[epoch18, step189]: loss 0.270614
[epoch18, step190]: loss 0.668762
[epoch18, step191]: loss 0.528293
[epoch18, step192]: loss 0.704036
[epoch18, step193]: loss 0.532656
[epoch18, step194]: loss 0.395307
[epoch18, step195]: loss 0.392204
[epoch18, step196]: loss 0.827643
[epoch18, step197]: loss 0.512470
[epoch18, step198]: loss 0.461880
[epoch18, step199]: loss 0.753562
[epoch18, step200]: loss 0.683586
[epoch18, step201]: loss 0.643838
[epoch18, step202]: loss 0.182156
[epoch18, step203]: loss 0.494285
[epoch18, step204]: loss 0.398690
[epoch18, step205]: loss 0.539333
[epoch18, step206]: loss 0.486097
[epoch18, step207]: loss 0.377417
[epoch18, step208]: loss 0.638955
[epoch18, step209]: loss 0.415275
[epoch18, step210]: loss 0.467906
[epoch18, step211]: loss 0.594552
[epoch18, step212]: loss 0.718138
[epoch18, step213]: loss 0.531068
[epoch18, step214]: loss 0.461289
[epoch18, step215]: loss 0.496530
[epoch18, step216]: loss 0.557251
[epoch18, step217]: loss 0.538525
[epoch18, step218]: loss 0.685036
[epoch18, step219]: loss 0.656247
[epoch18, step220]: loss 0.516420
[epoch18, step221]: loss 0.361453
[epoch18, step222]: loss 0.551629
[epoch18, step223]: loss 0.625246
[epoch18, step224]: loss 0.459053
[epoch18, step225]: loss 0.337260
[epoch18, step226]: loss 0.589341
[epoch18, step227]: loss 0.387801
[epoch18, step228]: loss 0.499931
[epoch18, step229]: loss 0.766164
[epoch18, step230]: loss 0.449353
[epoch18, step231]: loss 0.489264
[epoch18, step232]: loss 0.756876
[epoch18, step233]: loss 0.622079
[epoch18, step234]: loss 0.247698
[epoch18, step235]: loss 0.595631
[epoch18, step236]: loss 0.388516
[epoch18, step237]: loss 0.487675
[epoch18, step238]: loss 0.319874
[epoch18, step239]: loss 0.668172
[epoch18, step240]: loss 0.658916
[epoch18, step241]: loss 0.699933
[epoch18, step242]: loss 0.571055
[epoch18, step243]: loss 0.652212
[epoch18, step244]: loss 0.579826
[epoch18, step245]: loss 0.522883
[epoch18, step246]: loss 0.511102
[epoch18, step247]: loss 0.654521
[epoch18, step248]: loss 0.617327
[epoch18, step249]: loss 0.580264
[epoch18, step250]: loss 0.392885
[epoch18, step251]: loss 0.532740
[epoch18, step252]: loss 0.509974
[epoch18, step253]: loss 0.688309
[epoch18, step254]: loss 0.516184
[epoch18, step255]: loss 0.481828
[epoch18, step256]: loss 0.694818
[epoch18, step257]: loss 0.597163
[epoch18, step258]: loss 0.585406
[epoch18, step259]: loss 0.449681
[epoch18, step260]: loss 0.467476
[epoch18, step261]: loss 0.417533
[epoch18, step262]: loss 0.474139
[epoch18, step263]: loss 0.216984
[epoch18, step264]: loss 0.547060
[epoch18, step265]: loss 0.536942
[epoch18, step266]: loss 0.379912
[epoch18, step267]: loss 0.555008
[epoch18, step268]: loss 0.334753
[epoch18, step269]: loss 0.741991
[epoch18, step270]: loss 0.772514
[epoch18, step271]: loss 0.494341
[epoch18, step272]: loss 0.736977
[epoch18, step273]: loss 0.497022
[epoch18, step274]: loss 0.591438
[epoch18, step275]: loss 0.556512
[epoch18, step276]: loss 0.626992
[epoch18, step277]: loss 0.365583
[epoch18, step278]: loss 0.623934
[epoch18, step279]: loss 0.586372
[epoch18, step280]: loss 0.522711
[epoch18, step281]: loss 0.410254
[epoch18, step282]: loss 0.406051
[epoch18, step283]: loss 0.489647
[epoch18, step284]: loss 0.606446
[epoch18, step285]: loss 0.566016
[epoch18, step286]: loss 0.481324
[epoch18, step287]: loss 0.500256
[epoch18, step288]: loss 0.412926
[epoch18, step289]: loss 0.658806
[epoch18, step290]: loss 0.550676
[epoch18, step291]: loss 0.614022
[epoch18, step292]: loss 0.646593
[epoch18, step293]: loss 0.590693
[epoch18, step294]: loss 0.591738
[epoch18, step295]: loss 0.623133
[epoch18, step296]: loss 0.456008
[epoch18, step297]: loss 0.702300
[epoch18, step298]: loss 0.329343
[epoch18, step299]: loss 0.370524
[epoch18, step300]: loss 0.557389
[epoch18, step301]: loss 0.457098
[epoch18, step302]: loss 0.497241
[epoch18, step303]: loss 0.524766
[epoch18, step304]: loss 0.634965
[epoch18, step305]: loss 0.482132
[epoch18, step306]: loss 0.531009
[epoch18, step307]: loss 0.279094
[epoch18, step308]: loss 0.732987
[epoch18, step309]: loss 0.386623
[epoch18, step310]: loss 0.653360
[epoch18, step311]: loss 0.371633
[epoch18, step312]: loss 0.719748
[epoch18, step313]: loss 0.747278
[epoch18, step314]: loss 0.623841
[epoch18, step315]: loss 0.605228
[epoch18, step316]: loss 0.647089
[epoch18, step317]: loss 0.590256
[epoch18, step318]: loss 0.671806
[epoch18, step319]: loss 0.556800
[epoch18, step320]: loss 0.577047
[epoch18, step321]: loss 0.554302
[epoch18, step322]: loss 0.468757
[epoch18, step323]: loss 0.633428
[epoch18, step324]: loss 0.648495
[epoch18, step325]: loss 0.584293
[epoch18, step326]: loss 0.391124
[epoch18, step327]: loss 0.262492
[epoch18, step328]: loss 0.793046
[epoch18, step329]: loss 0.357940
[epoch18, step330]: loss 0.653039
[epoch18, step331]: loss 0.528101
[epoch18, step332]: loss 0.612978
[epoch18, step333]: loss 0.543734
[epoch18, step334]: loss 0.492096
[epoch18, step335]: loss 0.522116
[epoch18, step336]: loss 0.449046
[epoch18, step337]: loss 0.535710
[epoch18, step338]: loss 0.516731
[epoch18, step339]: loss 0.433710
[epoch18, step340]: loss 0.537260
[epoch18, step341]: loss 0.521314
[epoch18, step342]: loss 0.677236
[epoch18, step343]: loss 0.701296
[epoch18, step344]: loss 0.428712
[epoch18, step345]: loss 0.596350
[epoch18, step346]: loss 0.681745
[epoch18, step347]: loss 0.249013
[epoch18, step348]: loss 0.539090
[epoch18, step349]: loss 0.568969
[epoch18, step350]: loss 0.504738
[epoch18, step351]: loss 0.275386
[epoch18, step352]: loss 0.377667
[epoch18, step353]: loss 0.746098
[epoch18, step354]: loss 0.499684
[epoch18, step355]: loss 0.424098
[epoch18, step356]: loss 0.577726
[epoch18, step357]: loss 0.568629
[epoch18, step358]: loss 0.575315
[epoch18, step359]: loss 0.718455
[epoch18, step360]: loss 0.423818
[epoch18, step361]: loss 0.551445
[epoch18, step362]: loss 0.701910
[epoch18, step363]: loss 0.452095
[epoch18, step364]: loss 0.366702
[epoch18, step365]: loss 0.794511
[epoch18, step366]: loss 0.664558
[epoch18, step367]: loss 0.340253
[epoch18, step368]: loss 0.469500
[epoch18, step369]: loss 0.579718
[epoch18, step370]: loss 0.386779
[epoch18, step371]: loss 0.464942
[epoch18, step372]: loss 0.578649
[epoch18, step373]: loss 0.497513
[epoch18, step374]: loss 0.406277
[epoch18, step375]: loss 0.592870
[epoch18, step376]: loss 0.581619
[epoch18, step377]: loss 0.603120
[epoch18, step378]: loss 0.523233
[epoch18, step379]: loss 0.594973
[epoch18, step380]: loss 0.788761
[epoch18, step381]: loss 0.408820
[epoch18, step382]: loss 0.445162
[epoch18, step383]: loss 0.671446
[epoch18, step384]: loss 0.612466
[epoch18, step385]: loss 0.648959
[epoch18, step386]: loss 0.530640
[epoch18, step387]: loss 0.208742
[epoch18, step388]: loss 0.750924
[epoch18, step389]: loss 0.578060
[epoch18, step390]: loss 0.280437
[epoch18, step391]: loss 0.560916
[epoch18, step392]: loss 0.494770
[epoch18, step393]: loss 0.531486
[epoch18, step394]: loss 0.756301
[epoch18, step395]: loss 0.252920
[epoch18, step396]: loss 0.617227
[epoch18, step397]: loss 0.549689
[epoch18, step398]: loss 0.719255
[epoch18, step399]: loss 0.608160
[epoch18, step400]: loss 0.124471
[epoch18, step401]: loss 0.375217
[epoch18, step402]: loss 0.609501
[epoch18, step403]: loss 0.217609
[epoch18, step404]: loss 0.694458
[epoch18, step405]: loss 0.438443
[epoch18, step406]: loss 0.577118
[epoch18, step407]: loss 0.552775
[epoch18, step408]: loss 0.427949
[epoch18, step409]: loss 0.751760
[epoch18, step410]: loss 0.749633
[epoch18, step411]: loss 0.559061
[epoch18, step412]: loss 0.635684
[epoch18, step413]: loss 0.341897
[epoch18, step414]: loss 0.699083
[epoch18, step415]: loss 0.312233
[epoch18, step416]: loss 0.586071
[epoch18, step417]: loss 0.560157
[epoch18, step418]: loss 0.516934
[epoch18, step419]: loss 0.508545
[epoch18, step420]: loss 0.663038
[epoch18, step421]: loss 0.699165
[epoch18, step422]: loss 0.833938
[epoch18, step423]: loss 0.805208
[epoch18, step424]: loss 0.636611
[epoch18, step425]: loss 0.556037
[epoch18, step426]: loss 0.298094
[epoch18, step427]: loss 0.785153
[epoch18, step428]: loss 0.408794
[epoch18, step429]: loss 0.471053
[epoch18, step430]: loss 0.616367
[epoch18, step431]: loss 0.402998
[epoch18, step432]: loss 0.789420
[epoch18, step433]: loss 0.563745
[epoch18, step434]: loss 0.378633
[epoch18, step435]: loss 0.372694
[epoch18, step436]: loss 0.500961
[epoch18, step437]: loss 0.376507
[epoch18, step438]: loss 0.377379
[epoch18, step439]: loss 0.632836
[epoch18, step440]: loss 0.317262
[epoch18, step441]: loss 0.662643
[epoch18, step442]: loss 0.469333
[epoch18, step443]: loss 0.420267
[epoch18, step444]: loss 0.608690
[epoch18, step445]: loss 0.473325
[epoch18, step446]: loss 0.542545
[epoch18, step447]: loss 0.574118
[epoch18, step448]: loss 0.485609
[epoch18, step449]: loss 0.630782
[epoch18, step450]: loss 0.510893
[epoch18, step451]: loss 0.582748
[epoch18, step452]: loss 0.542650
[epoch18, step453]: loss 0.468568
[epoch18, step454]: loss 0.636674
[epoch18, step455]: loss 0.495956
[epoch18, step456]: loss 0.647104
[epoch18, step457]: loss 0.672400
[epoch18, step458]: loss 0.428608
[epoch18, step459]: loss 0.613657
[epoch18, step460]: loss 0.766671
[epoch18, step461]: loss 0.321713
[epoch18, step462]: loss 0.654467
[epoch18, step463]: loss 0.570712
[epoch18, step464]: loss 0.468714
[epoch18, step465]: loss 0.734319
[epoch18, step466]: loss 0.647218
[epoch18, step467]: loss 0.533036
[epoch18, step468]: loss 0.488630
[epoch18, step469]: loss 0.554570
[epoch18, step470]: loss 0.330531
[epoch18, step471]: loss 0.567804
[epoch18, step472]: loss 0.362277
[epoch18, step473]: loss 0.614677
[epoch18, step474]: loss 0.482746
[epoch18, step475]: loss 0.514885
[epoch18, step476]: loss 0.184956
[epoch18, step477]: loss 0.616290
[epoch18, step478]: loss 0.506717
[epoch18, step479]: loss 0.500577
[epoch18, step480]: loss 0.572184
[epoch18, step481]: loss 0.194770
[epoch18, step482]: loss 0.435296
[epoch18, step483]: loss 0.498803
[epoch18, step484]: loss 0.341825
[epoch18, step485]: loss 0.644030
[epoch18, step486]: loss 0.733912
[epoch18, step487]: loss 0.546321
[epoch18, step488]: loss 0.648972
[epoch18, step489]: loss 0.357580
[epoch18, step490]: loss 0.579779
[epoch18, step491]: loss 0.660598
[epoch18, step492]: loss 0.624617
[epoch18, step493]: loss 0.598536
[epoch18, step494]: loss 0.426971
[epoch18, step495]: loss 0.599042
[epoch18, step496]: loss 0.562170
[epoch18, step497]: loss 0.369604
[epoch18, step498]: loss 0.309675
[epoch18, step499]: loss 0.497312
[epoch18, step500]: loss 0.543107
[epoch18, step501]: loss 0.578239
[epoch18, step502]: loss 0.585729
[epoch18, step503]: loss 0.532744
[epoch18, step504]: loss 0.361589
[epoch18, step505]: loss 0.529024
[epoch18, step506]: loss 0.649544
[epoch18, step507]: loss 0.738608
[epoch18, step508]: loss 0.406393
[epoch18, step509]: loss 0.606296
[epoch18, step510]: loss 0.594672
[epoch18, step511]: loss 0.417604
[epoch18, step512]: loss 0.688673
[epoch18, step513]: loss 0.368532
[epoch18, step514]: loss 0.335069
[epoch18, step515]: loss 0.443032
[epoch18, step516]: loss 0.723869
[epoch18, step517]: loss 0.764473
[epoch18, step518]: loss 0.414446
[epoch18, step519]: loss 0.437507
[epoch18, step520]: loss 0.638294
[epoch18, step521]: loss 0.610678
[epoch18, step522]: loss 0.568741
[epoch18, step523]: loss 0.309696
[epoch18, step524]: loss 0.558115
[epoch18, step525]: loss 0.752653
[epoch18, step526]: loss 0.386796
[epoch18, step527]: loss 0.353973
[epoch18, step528]: loss 0.545824
[epoch18, step529]: loss 0.366801
[epoch18, step530]: loss 0.611810
[epoch18, step531]: loss 0.481105
[epoch18, step532]: loss 0.455094
[epoch18, step533]: loss 0.451777
[epoch18, step534]: loss 0.574843
[epoch18, step535]: loss 0.701362
[epoch18, step536]: loss 0.663293
[epoch18, step537]: loss 0.541305
[epoch18, step538]: loss 0.546029
[epoch18, step539]: loss 0.634681
[epoch18, step540]: loss 0.283702
[epoch18, step541]: loss 0.483172
[epoch18, step542]: loss 0.434044
[epoch18, step543]: loss 0.281047
[epoch18, step544]: loss 0.382971
[epoch18, step545]: loss 0.543520
[epoch18, step546]: loss 0.349241
[epoch18, step547]: loss 0.374873
[epoch18, step548]: loss 0.570371
[epoch18, step549]: loss 0.503416
[epoch18, step550]: loss 0.664070
[epoch18, step551]: loss 0.715975
[epoch18, step552]: loss 0.496568
[epoch18, step553]: loss 0.541902
[epoch18, step554]: loss 0.489681
[epoch18, step555]: loss 0.543537
[epoch18, step556]: loss 0.602562
[epoch18, step557]: loss 0.756006
[epoch18, step558]: loss 0.565964
[epoch18, step559]: loss 0.731845
[epoch18, step560]: loss 0.743436
[epoch18, step561]: loss 0.550029
[epoch18, step562]: loss 0.623976
[epoch18, step563]: loss 0.662645
[epoch18, step564]: loss 0.825275
[epoch18, step565]: loss 0.501651
[epoch18, step566]: loss 0.560956
[epoch18, step567]: loss 0.575787
[epoch18, step568]: loss 0.712503
[epoch18, step569]: loss 0.566264
[epoch18, step570]: loss 0.374353
[epoch18, step571]: loss 0.583382
[epoch18, step572]: loss 0.446970
[epoch18, step573]: loss 0.378900
[epoch18, step574]: loss 0.407912
[epoch18, step575]: loss 0.528619
[epoch18, step576]: loss 0.604196
[epoch18, step577]: loss 0.540789
[epoch18, step578]: loss 0.397981
[epoch18, step579]: loss 0.729775
[epoch18, step580]: loss 0.774596
[epoch18, step581]: loss 0.709024
[epoch18, step582]: loss 0.509846
[epoch18, step583]: loss 0.362591
[epoch18, step584]: loss 0.660091
[epoch18, step585]: loss 0.825138
[epoch18, step586]: loss 0.471688
[epoch18, step587]: loss 0.483004
[epoch18, step588]: loss 0.259147
[epoch18, step589]: loss 0.566414
[epoch18, step590]: loss 0.702723
[epoch18, step591]: loss 0.348481
[epoch18, step592]: loss 0.617526
[epoch18, step593]: loss 0.615863
[epoch18, step594]: loss 0.556996
[epoch18, step595]: loss 0.701398
[epoch18, step596]: loss 0.310413
[epoch18, step597]: loss 0.647162
[epoch18, step598]: loss 0.730694
[epoch18, step599]: loss 0.507680
[epoch18, step600]: loss 0.506455
[epoch18, step601]: loss 0.649417
[epoch18, step602]: loss 0.589008
[epoch18, step603]: loss 0.471868
[epoch18, step604]: loss 0.450965
[epoch18, step605]: loss 0.487384
[epoch18, step606]: loss 0.640729
[epoch18, step607]: loss 0.564095
[epoch18, step608]: loss 0.564842
[epoch18, step609]: loss 0.683340
[epoch18, step610]: loss 0.383145
[epoch18, step611]: loss 0.370761
[epoch18, step612]: loss 0.299333
[epoch18, step613]: loss 0.548471
[epoch18, step614]: loss 0.287841
[epoch18, step615]: loss 0.498158
[epoch18, step616]: loss 0.586486
[epoch18, step617]: loss 0.360869
[epoch18, step618]: loss 0.177227
[epoch18, step619]: loss 0.661433
[epoch18, step620]: loss 0.552516
[epoch18, step621]: loss 0.657578
[epoch18, step622]: loss 0.471655
[epoch18, step623]: loss 0.524744
[epoch18, step624]: loss 0.438047
[epoch18, step625]: loss 0.362475
[epoch18, step626]: loss 0.460323
[epoch18, step627]: loss 0.621162
[epoch18, step628]: loss 0.319982
[epoch18, step629]: loss 0.545626
[epoch18, step630]: loss 0.341055
[epoch18, step631]: loss 0.324075
[epoch18, step632]: loss 0.487486
[epoch18, step633]: loss 0.715332
[epoch18, step634]: loss 0.710298
[epoch18, step635]: loss 0.495666
[epoch18, step636]: loss 0.578172
[epoch18, step637]: loss 0.494506
[epoch18, step638]: loss 0.415380
[epoch18, step639]: loss 0.562284
[epoch18, step640]: loss 0.595449
[epoch18, step641]: loss 0.378665
[epoch18, step642]: loss 0.388048
[epoch18, step643]: loss 0.348526
[epoch18, step644]: loss 0.575864
[epoch18, step645]: loss 0.682031
[epoch18, step646]: loss 0.405913
[epoch18, step647]: loss 0.464723
[epoch18, step648]: loss 0.483795
[epoch18, step649]: loss 0.525238
[epoch18, step650]: loss 0.731885
[epoch18, step651]: loss 0.657553
[epoch18, step652]: loss 0.386888
[epoch18, step653]: loss 0.320060
[epoch18, step654]: loss 0.506745
[epoch18, step655]: loss 0.477308
[epoch18, step656]: loss 0.225355
[epoch18, step657]: loss 0.572084
[epoch18, step658]: loss 0.411078
[epoch18, step659]: loss 0.603496
[epoch18, step660]: loss 0.532452
[epoch18, step661]: loss 0.706279
[epoch18, step662]: loss 0.498096
[epoch18, step663]: loss 0.330711
[epoch18, step664]: loss 0.451202
[epoch18, step665]: loss 0.816838
[epoch18, step666]: loss 0.397542
[epoch18, step667]: loss 0.568961
[epoch18, step668]: loss 0.617361
[epoch18, step669]: loss 0.806293
[epoch18, step670]: loss 0.601568
[epoch18, step671]: loss 0.583423
[epoch18, step672]: loss 0.728313
[epoch18, step673]: loss 0.567895
[epoch18, step674]: loss 0.425186
[epoch18, step675]: loss 0.564740
[epoch18, step676]: loss 0.522120
[epoch18, step677]: loss 0.311239
[epoch18, step678]: loss 0.483058
[epoch18, step679]: loss 0.506130
[epoch18, step680]: loss 0.674253
[epoch18, step681]: loss 0.467697
[epoch18, step682]: loss 0.407379
[epoch18, step683]: loss 0.433951
[epoch18, step684]: loss 0.361365
[epoch18, step685]: loss 0.403681
[epoch18, step686]: loss 0.564246
[epoch18, step687]: loss 0.699550
[epoch18, step688]: loss 0.308903
[epoch18, step689]: loss 0.539983
[epoch18, step690]: loss 0.506013
[epoch18, step691]: loss 0.529207
[epoch18, step692]: loss 0.464229
[epoch18, step693]: loss 0.463062
[epoch18, step694]: loss 0.667481
[epoch18, step695]: loss 0.548633
[epoch18, step696]: loss 0.549677
[epoch18, step697]: loss 0.663795
[epoch18, step698]: loss 0.413058
[epoch18, step699]: loss 0.475217
[epoch18, step700]: loss 0.533611
[epoch18, step701]: loss 0.587455
[epoch18, step702]: loss 0.411017
[epoch18, step703]: loss 0.612869
[epoch18, step704]: loss 0.497737
[epoch18, step705]: loss 0.533727
[epoch18, step706]: loss 0.454510
[epoch18, step707]: loss 0.243605
[epoch18, step708]: loss 0.659121
[epoch18, step709]: loss 0.571920
[epoch18, step710]: loss 0.522146
[epoch18, step711]: loss 0.699754
[epoch18, step712]: loss 0.531901
[epoch18, step713]: loss 0.508592
[epoch18, step714]: loss 0.396068
[epoch18, step715]: loss 0.345366
[epoch18, step716]: loss 0.537027
[epoch18, step717]: loss 0.387832
[epoch18, step718]: loss 0.574234
[epoch18, step719]: loss 0.592688
[epoch18, step720]: loss 0.590046
[epoch18, step721]: loss 0.291423
[epoch18, step722]: loss 0.518004
[epoch18, step723]: loss 0.583229
[epoch18, step724]: loss 0.610181
[epoch18, step725]: loss 0.618480
[epoch18, step726]: loss 0.558423
[epoch18, step727]: loss 0.488914
[epoch18, step728]: loss 0.524578
[epoch18, step729]: loss 0.661368
[epoch18, step730]: loss 0.514359
[epoch18, step731]: loss 0.276393
[epoch18, step732]: loss 0.428685
[epoch18, step733]: loss 0.582774
[epoch18, step734]: loss 0.216899
[epoch18, step735]: loss 0.794979
[epoch18, step736]: loss 0.702303
[epoch18, step737]: loss 0.287685
[epoch18, step738]: loss 0.450637
[epoch18, step739]: loss 0.674342
[epoch18, step740]: loss 0.722676
[epoch18, step741]: loss 0.561871
[epoch18, step742]: loss 0.587390
[epoch18, step743]: loss 0.530176
[epoch18, step744]: loss 0.532824
[epoch18, step745]: loss 0.461401
[epoch18, step746]: loss 0.585955
[epoch18, step747]: loss 0.538378
[epoch18, step748]: loss 0.518001
[epoch18, step749]: loss 0.717060
[epoch18, step750]: loss 0.504552
[epoch18, step751]: loss 0.706723
[epoch18, step752]: loss 0.633877
[epoch18, step753]: loss 0.335234
[epoch18, step754]: loss 0.682323
[epoch18, step755]: loss 0.571885
[epoch18, step756]: loss 0.630269
[epoch18, step757]: loss 0.372217
[epoch18, step758]: loss 0.444066
[epoch18, step759]: loss 0.425208
[epoch18, step760]: loss 0.574707
[epoch18, step761]: loss 0.634429
[epoch18, step762]: loss 0.176256
[epoch18, step763]: loss 0.393808
[epoch18, step764]: loss 0.459212
[epoch18, step765]: loss 0.609726
[epoch18, step766]: loss 0.702208
[epoch18, step767]: loss 0.408812
[epoch18, step768]: loss 0.511427
[epoch18, step769]: loss 0.278673
[epoch18, step770]: loss 0.653183
[epoch18, step771]: loss 0.348734
[epoch18, step772]: loss 0.545108
[epoch18, step773]: loss 0.628956
[epoch18, step774]: loss 0.499252
[epoch18, step775]: loss 0.440090
[epoch18, step776]: loss 0.547624
[epoch18, step777]: loss 0.758243
[epoch18, step778]: loss 0.762875
[epoch18, step779]: loss 0.530841
[epoch18, step780]: loss 0.637739
[epoch18, step781]: loss 0.768932
[epoch18, step782]: loss 0.582007
[epoch18, step783]: loss 0.455394
[epoch18, step784]: loss 0.581558
[epoch18, step785]: loss 0.521043
[epoch18, step786]: loss 0.510379
[epoch18, step787]: loss 0.255438
[epoch18, step788]: loss 0.558513
[epoch18, step789]: loss 0.473403
[epoch18, step790]: loss 0.513756
[epoch18, step791]: loss 0.728936
[epoch18, step792]: loss 0.652388
[epoch18, step793]: loss 0.491659
[epoch18, step794]: loss 0.589880
[epoch18, step795]: loss 0.439846
[epoch18, step796]: loss 0.500402
[epoch18, step797]: loss 0.711859
[epoch18, step798]: loss 0.648087
[epoch18, step799]: loss 0.734764
[epoch18, step800]: loss 0.444473
[epoch18, step801]: loss 0.426118
[epoch18, step802]: loss 0.487364
[epoch18, step803]: loss 0.505135
[epoch18, step804]: loss 0.645522
[epoch18, step805]: loss 0.560399
[epoch18, step806]: loss 0.703507
[epoch18, step807]: loss 0.366718
[epoch18, step808]: loss 0.654352
[epoch18, step809]: loss 0.408925
[epoch18, step810]: loss 0.596628
[epoch18, step811]: loss 0.434742
[epoch18, step812]: loss 0.332116
[epoch18, step813]: loss 0.605578
[epoch18, step814]: loss 0.495097
[epoch18, step815]: loss 0.601078
[epoch18, step816]: loss 0.588899
[epoch18, step817]: loss 0.189163
[epoch18, step818]: loss 0.720291
[epoch18, step819]: loss 0.457001
[epoch18, step820]: loss 0.593586
[epoch18, step821]: loss 0.547758
[epoch18, step822]: loss 0.577712
[epoch18, step823]: loss 0.734820
[epoch18, step824]: loss 0.738388
[epoch18, step825]: loss 0.504086
[epoch18, step826]: loss 0.247540
[epoch18, step827]: loss 0.128325
[epoch18, step828]: loss 0.447056
[epoch18, step829]: loss 0.356302
[epoch18, step830]: loss 0.598201
[epoch18, step831]: loss 0.570569
[epoch18, step832]: loss 0.488806
[epoch18, step833]: loss 0.520260
[epoch18, step834]: loss 0.759627
[epoch18, step835]: loss 0.292310
[epoch18, step836]: loss 0.546211
[epoch18, step837]: loss 0.599831
[epoch18, step838]: loss 0.483481
[epoch18, step839]: loss 0.505437
[epoch18, step840]: loss 0.393880
[epoch18, step841]: loss 0.797096
[epoch18, step842]: loss 0.571071
[epoch18, step843]: loss 0.542844
[epoch18, step844]: loss 0.223469
[epoch18, step845]: loss 0.445253
[epoch18, step846]: loss 0.568298
[epoch18, step847]: loss 0.698793
[epoch18, step848]: loss 0.412599
[epoch18, step849]: loss 0.687970
[epoch18, step850]: loss 0.478243
[epoch18, step851]: loss 0.437109
[epoch18, step852]: loss 0.584211
[epoch18, step853]: loss 0.518010
[epoch18, step854]: loss 0.521494
[epoch18, step855]: loss 0.490691
[epoch18, step856]: loss 0.502381
[epoch18, step857]: loss 0.447545
[epoch18, step858]: loss 0.410466
[epoch18, step859]: loss 0.559687
[epoch18, step860]: loss 0.514748
[epoch18, step861]: loss 0.675567
[epoch18, step862]: loss 0.476314
[epoch18, step863]: loss 0.454765
[epoch18, step864]: loss 0.588879
[epoch18, step865]: loss 0.300676
[epoch18, step866]: loss 0.508925
[epoch18, step867]: loss 0.626871
[epoch18, step868]: loss 0.798987
[epoch18, step869]: loss 0.326797
[epoch18, step870]: loss 0.775315
[epoch18, step871]: loss 0.441074
[epoch18, step872]: loss 0.634332
[epoch18, step873]: loss 0.477598
[epoch18, step874]: loss 0.358339
[epoch18, step875]: loss 0.628657
[epoch18, step876]: loss 0.587551
[epoch18, step877]: loss 0.650657
[epoch18, step878]: loss 0.406919
[epoch18, step879]: loss 0.633373
[epoch18, step880]: loss 0.450462
[epoch18, step881]: loss 0.589499
[epoch18, step882]: loss 0.635789
[epoch18, step883]: loss 0.497438
[epoch18, step884]: loss 0.562850
[epoch18, step885]: loss 0.558900
[epoch18, step886]: loss 0.488336
[epoch18, step887]: loss 0.430633
[epoch18, step888]: loss 0.416882
[epoch18, step889]: loss 0.439091
[epoch18, step890]: loss 0.615514
[epoch18, step891]: loss 0.741145
[epoch18, step892]: loss 0.347632
[epoch18, step893]: loss 0.227436
[epoch18, step894]: loss 0.582745
[epoch18, step895]: loss 0.483258
[epoch18, step896]: loss 0.587541
[epoch18, step897]: loss 0.425848
[epoch18, step898]: loss 0.486154
[epoch18, step899]: loss 0.585133
[epoch18, step900]: loss 0.686572
[epoch18, step901]: loss 0.639193
[epoch18, step902]: loss 0.340411
[epoch18, step903]: loss 0.244557
[epoch18, step904]: loss 0.494610
[epoch18, step905]: loss 0.672777
[epoch18, step906]: loss 0.401974
[epoch18, step907]: loss 0.639842
[epoch18, step908]: loss 0.729366
[epoch18, step909]: loss 0.384941
[epoch18, step910]: loss 0.541650
[epoch18, step911]: loss 0.414820
[epoch18, step912]: loss 0.738196
[epoch18, step913]: loss 0.335025
[epoch18, step914]: loss 0.751955
[epoch18, step915]: loss 0.741749
[epoch18, step916]: loss 0.669140
[epoch18, step917]: loss 0.576730
[epoch18, step918]: loss 0.455504
[epoch18, step919]: loss 0.632156
[epoch18, step920]: loss 0.401875
[epoch18, step921]: loss 0.645644
[epoch18, step922]: loss 0.456418
[epoch18, step923]: loss 0.633435
[epoch18, step924]: loss 0.602053
[epoch18, step925]: loss 0.532789
[epoch18, step926]: loss 0.448586
[epoch18, step927]: loss 0.623100
[epoch18, step928]: loss 0.521911
[epoch18, step929]: loss 0.362958
[epoch18, step930]: loss 0.649036
[epoch18, step931]: loss 0.443991
[epoch18, step932]: loss 0.512984
[epoch18, step933]: loss 0.527715
[epoch18, step934]: loss 0.387656
[epoch18, step935]: loss 0.479325
[epoch18, step936]: loss 0.606681
[epoch18, step937]: loss 0.435086
[epoch18, step938]: loss 0.447119
[epoch18, step939]: loss 0.720972
[epoch18, step940]: loss 0.395411
[epoch18, step941]: loss 0.649330
[epoch18, step942]: loss 0.492288
[epoch18, step943]: loss 0.747736
[epoch18, step944]: loss 0.487562
[epoch18, step945]: loss 0.688488
[epoch18, step946]: loss 0.668353
[epoch18, step947]: loss 0.722469
[epoch18, step948]: loss 0.604725
[epoch18, step949]: loss 0.392457
[epoch18, step950]: loss 0.603740
[epoch18, step951]: loss 0.479633
[epoch18, step952]: loss 0.420298
[epoch18, step953]: loss 0.640417
[epoch18, step954]: loss 0.488637
[epoch18, step955]: loss 0.619921
[epoch18, step956]: loss 0.771927
[epoch18, step957]: loss 0.679494
[epoch18, step958]: loss 0.474640
[epoch18, step959]: loss 0.267864
[epoch18, step960]: loss 0.606327
[epoch18, step961]: loss 0.675823
[epoch18, step962]: loss 0.590708
[epoch18, step963]: loss 0.367148
[epoch18, step964]: loss 0.698789
[epoch18, step965]: loss 0.415459
[epoch18, step966]: loss 0.455792
[epoch18, step967]: loss 0.484452
[epoch18, step968]: loss 0.634737
[epoch18, step969]: loss 0.267907
[epoch18, step970]: loss 0.489911
[epoch18, step971]: loss 0.831747
[epoch18, step972]: loss 0.557525
[epoch18, step973]: loss 0.588284
[epoch18, step974]: loss 0.623253
[epoch18, step975]: loss 0.310880
[epoch18, step976]: loss 0.721722
[epoch18, step977]: loss 0.505956
[epoch18, step978]: loss 0.743475
[epoch18, step979]: loss 0.613657
[epoch18, step980]: loss 0.748760
[epoch18, step981]: loss 0.279694
[epoch18, step982]: loss 0.509134
[epoch18, step983]: loss 0.378758
[epoch18, step984]: loss 0.582769
[epoch18, step985]: loss 0.758857
[epoch18, step986]: loss 0.641491
[epoch18, step987]: loss 0.347590
[epoch18, step988]: loss 0.411261
[epoch18, step989]: loss 0.330830
[epoch18, step990]: loss 0.611297
[epoch18, step991]: loss 0.627618
[epoch18, step992]: loss 0.402009
[epoch18, step993]: loss 0.549323
[epoch18, step994]: loss 0.487606
[epoch18, step995]: loss 0.777411
[epoch18, step996]: loss 0.606998
[epoch18, step997]: loss 0.709472
[epoch18, step998]: loss 0.590350
[epoch18, step999]: loss 0.547381
[epoch18, step1000]: loss 0.554536
[epoch18, step1001]: loss 0.439623
[epoch18, step1002]: loss 0.628552
[epoch18, step1003]: loss 0.674825
[epoch18, step1004]: loss 0.571712
[epoch18, step1005]: loss 0.347911
[epoch18, step1006]: loss 0.418272
[epoch18, step1007]: loss 0.621288
[epoch18, step1008]: loss 0.591663
[epoch18, step1009]: loss 0.404805
[epoch18, step1010]: loss 0.450540
[epoch18, step1011]: loss 0.529387
[epoch18, step1012]: loss 0.478511
[epoch18, step1013]: loss 0.648667
[epoch18, step1014]: loss 0.634908
[epoch18, step1015]: loss 0.581248
[epoch18, step1016]: loss 0.545651
[epoch18, step1017]: loss 0.522936
[epoch18, step1018]: loss 0.251173
[epoch18, step1019]: loss 0.468984
[epoch18, step1020]: loss 0.469669
[epoch18, step1021]: loss 0.713923
[epoch18, step1022]: loss 0.414965
[epoch18, step1023]: loss 0.234756
[epoch18, step1024]: loss 0.612218
[epoch18, step1025]: loss 0.701410
[epoch18, step1026]: loss 0.186671
[epoch18, step1027]: loss 0.407966
[epoch18, step1028]: loss 0.771393
[epoch18, step1029]: loss 0.617218
[epoch18, step1030]: loss 0.726902
[epoch18, step1031]: loss 0.727672
[epoch18, step1032]: loss 0.294961
[epoch18, step1033]: loss 0.410749
[epoch18, step1034]: loss 0.332049
[epoch18, step1035]: loss 0.486576
[epoch18, step1036]: loss 0.725231
[epoch18, step1037]: loss 0.579369
[epoch18, step1038]: loss 0.454159
[epoch18, step1039]: loss 0.462681
[epoch18, step1040]: loss 0.651097
[epoch18, step1041]: loss 0.178660
[epoch18, step1042]: loss 0.610207
[epoch18, step1043]: loss 0.477276
[epoch18, step1044]: loss 0.539900
[epoch18, step1045]: loss 0.398361
[epoch18, step1046]: loss 0.688738
[epoch18, step1047]: loss 0.595921
[epoch18, step1048]: loss 0.423143
[epoch18, step1049]: loss 0.617248
[epoch18, step1050]: loss 0.138691
[epoch18, step1051]: loss 0.454909
[epoch18, step1052]: loss 0.616528
[epoch18, step1053]: loss 0.428201
[epoch18, step1054]: loss 0.530607
[epoch18, step1055]: loss 0.517377
[epoch18, step1056]: loss 0.494433
[epoch18, step1057]: loss 0.378468
[epoch18, step1058]: loss 0.453901
[epoch18, step1059]: loss 0.537743
[epoch18, step1060]: loss 0.608204
[epoch18, step1061]: loss 0.694821
[epoch18, step1062]: loss 0.332456
[epoch18, step1063]: loss 0.602383
[epoch18, step1064]: loss 0.504772
[epoch18, step1065]: loss 0.609071
[epoch18, step1066]: loss 0.759636
[epoch18, step1067]: loss 0.297972
[epoch18, step1068]: loss 0.577329
[epoch18, step1069]: loss 0.556096
[epoch18, step1070]: loss 0.527535
[epoch18, step1071]: loss 0.767946
[epoch18, step1072]: loss 0.327701
[epoch18, step1073]: loss 0.627087
[epoch18, step1074]: loss 0.673855
[epoch18, step1075]: loss 0.450543
[epoch18, step1076]: loss 0.663727
[epoch18, step1077]: loss 0.394980
[epoch18, step1078]: loss 0.441824
[epoch18, step1079]: loss 0.676253
[epoch18, step1080]: loss 0.468343
[epoch18, step1081]: loss 0.251072
[epoch18, step1082]: loss 0.558241
[epoch18, step1083]: loss 0.260916
[epoch18, step1084]: loss 0.732516
[epoch18, step1085]: loss 0.631824
[epoch18, step1086]: loss 0.511297
[epoch18, step1087]: loss 0.578889
[epoch18, step1088]: loss 0.601611
[epoch18, step1089]: loss 0.505322
[epoch18, step1090]: loss 0.136719
[epoch18, step1091]: loss 0.604453
[epoch18, step1092]: loss 0.678107
[epoch18, step1093]: loss 0.449361
[epoch18, step1094]: loss 0.715687
[epoch18, step1095]: loss 0.589656
[epoch18, step1096]: loss 0.274032
[epoch18, step1097]: loss 0.642348
[epoch18, step1098]: loss 0.594270
[epoch18, step1099]: loss 0.482040
[epoch18, step1100]: loss 0.405248
[epoch18, step1101]: loss 0.412444
[epoch18, step1102]: loss 0.643405
[epoch18, step1103]: loss 0.659933
[epoch18, step1104]: loss 0.547893
[epoch18, step1105]: loss 0.480880
[epoch18, step1106]: loss 0.537880
[epoch18, step1107]: loss 0.667657
[epoch18, step1108]: loss 0.389434
[epoch18, step1109]: loss 0.455013
[epoch18, step1110]: loss 0.317471
[epoch18, step1111]: loss 0.397572
[epoch18, step1112]: loss 0.674447
[epoch18, step1113]: loss 0.596784
[epoch18, step1114]: loss 0.319538
[epoch18, step1115]: loss 0.701432
[epoch18, step1116]: loss 0.733925
[epoch18, step1117]: loss 0.520274
[epoch18, step1118]: loss 0.255987
[epoch18, step1119]: loss 0.380977
[epoch18, step1120]: loss 0.449481
[epoch18, step1121]: loss 0.422445
[epoch18, step1122]: loss 0.448466
[epoch18, step1123]: loss 0.563310
[epoch18, step1124]: loss 0.464590
[epoch18, step1125]: loss 0.613627
[epoch18, step1126]: loss 0.480116
[epoch18, step1127]: loss 0.369629
[epoch18, step1128]: loss 0.573750
[epoch18, step1129]: loss 0.577634
[epoch18, step1130]: loss 0.577920
[epoch18, step1131]: loss 0.379388
[epoch18, step1132]: loss 0.639786
[epoch18, step1133]: loss 0.290671
[epoch18, step1134]: loss 0.437449
[epoch18, step1135]: loss 0.370265
[epoch18, step1136]: loss 0.515507
[epoch18, step1137]: loss 0.623799
[epoch18, step1138]: loss 0.464526
[epoch18, step1139]: loss 0.671136
[epoch18, step1140]: loss 0.632298
[epoch18, step1141]: loss 0.288841
[epoch18, step1142]: loss 0.459234
[epoch18, step1143]: loss 0.547647
[epoch18, step1144]: loss 0.711915
[epoch18, step1145]: loss 0.605003
[epoch18, step1146]: loss 0.593017
[epoch18, step1147]: loss 0.573288
[epoch18, step1148]: loss 0.528748
[epoch18, step1149]: loss 0.854660
[epoch18, step1150]: loss 0.442686
[epoch18, step1151]: loss 0.596314
[epoch18, step1152]: loss 0.244394
[epoch18, step1153]: loss 0.441360
[epoch18, step1154]: loss 0.250421
[epoch18, step1155]: loss 0.152822
[epoch18, step1156]: loss 0.412944
[epoch18, step1157]: loss 0.260509
[epoch18, step1158]: loss 0.689263
[epoch18, step1159]: loss 0.378654
[epoch18, step1160]: loss 0.459521
[epoch18, step1161]: loss 0.476385
[epoch18, step1162]: loss 0.764801
[epoch18, step1163]: loss 0.618546
[epoch18, step1164]: loss 0.622224
[epoch18, step1165]: loss 0.513417
[epoch18, step1166]: loss 0.481826
[epoch18, step1167]: loss 0.334637
[epoch18, step1168]: loss 0.378555
[epoch18, step1169]: loss 0.649497
[epoch18, step1170]: loss 0.646171
[epoch18, step1171]: loss 0.258980
[epoch18, step1172]: loss 0.490712
[epoch18, step1173]: loss 0.408228
[epoch18, step1174]: loss 0.773466
[epoch18, step1175]: loss 0.254473
[epoch18, step1176]: loss 0.686789
[epoch18, step1177]: loss 0.295792
[epoch18, step1178]: loss 0.622492
[epoch18, step1179]: loss 0.544729
[epoch18, step1180]: loss 0.871041
[epoch18, step1181]: loss 0.407189
[epoch18, step1182]: loss 0.591075
[epoch18, step1183]: loss 0.652534
[epoch18, step1184]: loss 0.535898
[epoch18, step1185]: loss 0.754151
[epoch18, step1186]: loss 0.630933
[epoch18, step1187]: loss 0.624297
[epoch18, step1188]: loss 0.242044
[epoch18, step1189]: loss 0.335097
[epoch18, step1190]: loss 0.575965
[epoch18, step1191]: loss 0.470086
[epoch18, step1192]: loss 0.392780
[epoch18, step1193]: loss 0.171207
[epoch18, step1194]: loss 0.602490
[epoch18, step1195]: loss 0.439962
[epoch18, step1196]: loss 0.759464
[epoch18, step1197]: loss 0.691149
[epoch18, step1198]: loss 0.610299
[epoch18, step1199]: loss 0.248090
[epoch18, step1200]: loss 0.316634
[epoch18, step1201]: loss 0.516225
[epoch18, step1202]: loss 0.619411
[epoch18, step1203]: loss 0.610739
[epoch18, step1204]: loss 0.453836
[epoch18, step1205]: loss 0.624630
[epoch18, step1206]: loss 0.325017
[epoch18, step1207]: loss 0.432003
[epoch18, step1208]: loss 0.652465
[epoch18, step1209]: loss 0.826998
[epoch18, step1210]: loss 0.366660
[epoch18, step1211]: loss 0.344320
[epoch18, step1212]: loss 0.685036
[epoch18, step1213]: loss 0.574142
[epoch18, step1214]: loss 0.623785
[epoch18, step1215]: loss 0.239998
[epoch18, step1216]: loss 0.709950
[epoch18, step1217]: loss 0.605984
[epoch18, step1218]: loss 0.650288
[epoch18, step1219]: loss 0.585020
[epoch18, step1220]: loss 0.759723
[epoch18, step1221]: loss 0.436096
[epoch18, step1222]: loss 0.421256
[epoch18, step1223]: loss 0.567974
[epoch18, step1224]: loss 0.533490
[epoch18, step1225]: loss 0.551196
[epoch18, step1226]: loss 0.458613
[epoch18, step1227]: loss 0.360051
[epoch18, step1228]: loss 0.795548
[epoch18, step1229]: loss 0.644721
[epoch18, step1230]: loss 0.428401
[epoch18, step1231]: loss 0.497558
[epoch18, step1232]: loss 0.370645
[epoch18, step1233]: loss 0.689455
[epoch18, step1234]: loss 0.648588
[epoch18, step1235]: loss 0.554306
[epoch18, step1236]: loss 0.636618
[epoch18, step1237]: loss 0.582181
[epoch18, step1238]: loss 0.640217
[epoch18, step1239]: loss 0.560272
[epoch18, step1240]: loss 0.647202
[epoch18, step1241]: loss 0.441171
[epoch18, step1242]: loss 0.638204
[epoch18, step1243]: loss 0.481315
[epoch18, step1244]: loss 0.530423
[epoch18, step1245]: loss 0.479769
[epoch18, step1246]: loss 0.302410
[epoch18, step1247]: loss 0.630829
[epoch18, step1248]: loss 0.272121
[epoch18, step1249]: loss 0.510281
[epoch18, step1250]: loss 0.724809
[epoch18, step1251]: loss 0.433811
[epoch18, step1252]: loss 0.587162
[epoch18, step1253]: loss 0.648777
[epoch18, step1254]: loss 0.437704
[epoch18, step1255]: loss 0.448629
[epoch18, step1256]: loss 0.461448
[epoch18, step1257]: loss 0.531752
[epoch18, step1258]: loss 0.776658
[epoch18, step1259]: loss 0.401930
[epoch18, step1260]: loss 0.480842
[epoch18, step1261]: loss 0.488184
[epoch18, step1262]: loss 0.412204
[epoch18, step1263]: loss 0.495355
[epoch18, step1264]: loss 0.688788
[epoch18, step1265]: loss 0.609939
[epoch18, step1266]: loss 0.640802
[epoch18, step1267]: loss 0.655537
[epoch18, step1268]: loss 0.513564
[epoch18, step1269]: loss 0.163932
[epoch18, step1270]: loss 0.754423
[epoch18, step1271]: loss 0.454135
[epoch18, step1272]: loss 0.653017
[epoch18, step1273]: loss 0.335960
[epoch18, step1274]: loss 0.587193
[epoch18, step1275]: loss 0.663382
[epoch18, step1276]: loss 0.462750
[epoch18, step1277]: loss 0.292291
[epoch18, step1278]: loss 0.541181
[epoch18, step1279]: loss 0.320914
[epoch18, step1280]: loss 0.485637
[epoch18, step1281]: loss 0.565688
[epoch18, step1282]: loss 0.395968
[epoch18, step1283]: loss 0.311306
[epoch18, step1284]: loss 0.639230
[epoch18, step1285]: loss 0.548307
[epoch18, step1286]: loss 0.578727
[epoch18, step1287]: loss 0.472701
[epoch18, step1288]: loss 0.513315
[epoch18, step1289]: loss 0.672934
[epoch18, step1290]: loss 0.625154
[epoch18, step1291]: loss 0.625438
[epoch18, step1292]: loss 0.352680
[epoch18, step1293]: loss 0.567987
[epoch18, step1294]: loss 0.599540
[epoch18, step1295]: loss 0.726743
[epoch18, step1296]: loss 0.474331
[epoch18, step1297]: loss 0.721669
[epoch18, step1298]: loss 0.740418
[epoch18, step1299]: loss 0.155515
[epoch18, step1300]: loss 0.639094
[epoch18, step1301]: loss 0.543058
[epoch18, step1302]: loss 0.473630
[epoch18, step1303]: loss 0.789590
[epoch18, step1304]: loss 0.292595
[epoch18, step1305]: loss 0.796160
[epoch18, step1306]: loss 0.741092
[epoch18, step1307]: loss 0.590622
[epoch18, step1308]: loss 0.536867
[epoch18, step1309]: loss 0.367406
[epoch18, step1310]: loss 0.523534
[epoch18, step1311]: loss 0.897448
[epoch18, step1312]: loss 0.467312
[epoch18, step1313]: loss 0.542983
[epoch18, step1314]: loss 0.590891
[epoch18, step1315]: loss 0.680635
[epoch18, step1316]: loss 0.607406
[epoch18, step1317]: loss 0.541367
[epoch18, step1318]: loss 0.629822
[epoch18, step1319]: loss 0.516958
[epoch18, step1320]: loss 0.559534
[epoch18, step1321]: loss 0.526546
[epoch18, step1322]: loss 0.524023
[epoch18, step1323]: loss 0.519957
[epoch18, step1324]: loss 0.353322
[epoch18, step1325]: loss 0.503130
[epoch18, step1326]: loss 0.395203
[epoch18, step1327]: loss 0.610916
[epoch18, step1328]: loss 0.669644
[epoch18, step1329]: loss 0.746447
[epoch18, step1330]: loss 0.741715
[epoch18, step1331]: loss 0.417052
[epoch18, step1332]: loss 0.560850
[epoch18, step1333]: loss 0.416371
[epoch18, step1334]: loss 0.531722
[epoch18, step1335]: loss 0.573049
[epoch18, step1336]: loss 0.519742
[epoch18, step1337]: loss 0.576379
[epoch18, step1338]: loss 0.610486
[epoch18, step1339]: loss 0.409212
[epoch18, step1340]: loss 0.679466
[epoch18, step1341]: loss 0.617299
[epoch18, step1342]: loss 0.480425
[epoch18, step1343]: loss 0.408750
[epoch18, step1344]: loss 0.539478
[epoch18, step1345]: loss 0.231274
[epoch18, step1346]: loss 0.632655
[epoch18, step1347]: loss 0.613505
[epoch18, step1348]: loss 0.723582
[epoch18, step1349]: loss 0.505052
[epoch18, step1350]: loss 0.659216
[epoch18, step1351]: loss 0.451021
[epoch18, step1352]: loss 0.636648
[epoch18, step1353]: loss 0.495442
[epoch18, step1354]: loss 0.466157
[epoch18, step1355]: loss 0.360919
[epoch18, step1356]: loss 0.497032
[epoch18, step1357]: loss 0.535432
[epoch18, step1358]: loss 0.393043
[epoch18, step1359]: loss 0.401537
[epoch18, step1360]: loss 0.500581
[epoch18, step1361]: loss 0.719203
[epoch18, step1362]: loss 0.513238
[epoch18, step1363]: loss 0.495109
[epoch18, step1364]: loss 0.492741
[epoch18, step1365]: loss 0.682567
[epoch18, step1366]: loss 0.513652
[epoch18, step1367]: loss 0.518582
[epoch18, step1368]: loss 0.593114
[epoch18, step1369]: loss 0.601014
[epoch18, step1370]: loss 0.486887
[epoch18, step1371]: loss 0.240441
[epoch18, step1372]: loss 0.392454
[epoch18, step1373]: loss 0.421284
[epoch18, step1374]: loss 0.347840
[epoch18, step1375]: loss 0.128867
[epoch18, step1376]: loss 0.428980
[epoch18, step1377]: loss 0.429857
[epoch18, step1378]: loss 0.499895
[epoch18, step1379]: loss 0.249189
[epoch18, step1380]: loss 0.558878
[epoch18, step1381]: loss 0.463975
[epoch18, step1382]: loss 0.584788
[epoch18, step1383]: loss 0.570478
[epoch18, step1384]: loss 0.746382
[epoch18, step1385]: loss 0.562992
[epoch18, step1386]: loss 0.360753
[epoch18, step1387]: loss 0.529858
[epoch18, step1388]: loss 0.288569
[epoch18, step1389]: loss 0.559084
[epoch18, step1390]: loss 0.593895
[epoch18, step1391]: loss 0.675557
[epoch18, step1392]: loss 0.302522
[epoch18, step1393]: loss 0.592998
[epoch18, step1394]: loss 0.498495
[epoch18, step1395]: loss 0.641570
[epoch18, step1396]: loss 0.677864
[epoch18, step1397]: loss 0.469563
[epoch18, step1398]: loss 0.371742
[epoch18, step1399]: loss 0.623199
[epoch18, step1400]: loss 0.696115
[epoch18, step1401]: loss 0.696326
[epoch18, step1402]: loss 0.560560
[epoch18, step1403]: loss 0.406753
[epoch18, step1404]: loss 0.636121
[epoch18, step1405]: loss 0.272542
[epoch18, step1406]: loss 0.695841
[epoch18, step1407]: loss 0.469100
[epoch18, step1408]: loss 0.457873
[epoch18, step1409]: loss 0.451475
[epoch18, step1410]: loss 0.439650
[epoch18, step1411]: loss 0.325883
[epoch18, step1412]: loss 0.456369
[epoch18, step1413]: loss 0.449529
[epoch18, step1414]: loss 0.670052
[epoch18, step1415]: loss 0.556150
[epoch18, step1416]: loss 0.702006
[epoch18, step1417]: loss 0.520007
[epoch18, step1418]: loss 0.494252
[epoch18, step1419]: loss 0.533507
[epoch18, step1420]: loss 0.640456
[epoch18, step1421]: loss 0.345298
[epoch18, step1422]: loss 0.406343
[epoch18, step1423]: loss 0.726357
[epoch18, step1424]: loss 0.805976
[epoch18, step1425]: loss 0.555065
[epoch18, step1426]: loss 0.565761
[epoch18, step1427]: loss 0.582260
[epoch18, step1428]: loss 0.566064
[epoch18, step1429]: loss 0.671883
[epoch18, step1430]: loss 0.721091
[epoch18, step1431]: loss 0.299204
[epoch18, step1432]: loss 0.569576
[epoch18, step1433]: loss 0.350744
[epoch18, step1434]: loss 0.527723
[epoch18, step1435]: loss 0.405768
[epoch18, step1436]: loss 0.517595
[epoch18, step1437]: loss 0.553699
[epoch18, step1438]: loss 0.699626
[epoch18, step1439]: loss 0.587966
[epoch18, step1440]: loss 0.461265
[epoch18, step1441]: loss 0.636197
[epoch18, step1442]: loss 0.478056
[epoch18, step1443]: loss 0.605074
[epoch18, step1444]: loss 0.453863
[epoch18, step1445]: loss 0.586673
[epoch18, step1446]: loss 0.523688
[epoch18, step1447]: loss 0.636449
[epoch18, step1448]: loss 0.591507
[epoch18, step1449]: loss 0.448173
[epoch18, step1450]: loss 0.543996
[epoch18, step1451]: loss 0.506377
[epoch18, step1452]: loss 0.668880
[epoch18, step1453]: loss 0.533602
[epoch18, step1454]: loss 0.436428
[epoch18, step1455]: loss 0.598581
[epoch18, step1456]: loss 0.650341
[epoch18, step1457]: loss 0.389074
[epoch18, step1458]: loss 0.479423
[epoch18, step1459]: loss 0.736963
[epoch18, step1460]: loss 0.602799
[epoch18, step1461]: loss 0.452121
[epoch18, step1462]: loss 0.556530
[epoch18, step1463]: loss 0.367072
[epoch18, step1464]: loss 0.716519
[epoch18, step1465]: loss 0.557493
[epoch18, step1466]: loss 0.714773
[epoch18, step1467]: loss 0.588323
[epoch18, step1468]: loss 0.605710
[epoch18, step1469]: loss 0.555824
[epoch18, step1470]: loss 0.209211
[epoch18, step1471]: loss 0.620871
[epoch18, step1472]: loss 0.427976
[epoch18, step1473]: loss 0.478197
[epoch18, step1474]: loss 0.601578
[epoch18, step1475]: loss 0.661431
[epoch18, step1476]: loss 0.585027
[epoch18, step1477]: loss 0.547213
[epoch18, step1478]: loss 0.639297
[epoch18, step1479]: loss 0.426856
[epoch18, step1480]: loss 0.564318
[epoch18, step1481]: loss 0.636286
[epoch18, step1482]: loss 0.521480
[epoch18, step1483]: loss 0.640070
[epoch18, step1484]: loss 0.624306
[epoch18, step1485]: loss 0.492884
[epoch18, step1486]: loss 0.351856
[epoch18, step1487]: loss 0.733602
[epoch18, step1488]: loss 0.692238
[epoch18, step1489]: loss 0.668511
[epoch18, step1490]: loss 0.530477
[epoch18, step1491]: loss 0.336801
[epoch18, step1492]: loss 0.664752
[epoch18, step1493]: loss 0.382822
[epoch18, step1494]: loss 0.356170
[epoch18, step1495]: loss 0.361958
[epoch18, step1496]: loss 0.554218
[epoch18, step1497]: loss 0.571160
[epoch18, step1498]: loss 0.363421
[epoch18, step1499]: loss 0.536405
[epoch18, step1500]: loss 0.147715
[epoch18, step1501]: loss 0.342346
[epoch18, step1502]: loss 0.445271
[epoch18, step1503]: loss 0.639797
[epoch18, step1504]: loss 0.491100
[epoch18, step1505]: loss 0.284267
[epoch18, step1506]: loss 0.388205
[epoch18, step1507]: loss 0.543416
[epoch18, step1508]: loss 0.660993
[epoch18, step1509]: loss 0.739904
[epoch18, step1510]: loss 0.442998
[epoch18, step1511]: loss 0.618218
[epoch18, step1512]: loss 0.481313
[epoch18, step1513]: loss 0.443877
[epoch18, step1514]: loss 0.405392
[epoch18, step1515]: loss 0.395052
[epoch18, step1516]: loss 0.533652
[epoch18, step1517]: loss 0.739404
[epoch18, step1518]: loss 0.397920
[epoch18, step1519]: loss 0.577892
[epoch18, step1520]: loss 0.087424
[epoch18, step1521]: loss 0.490904
[epoch18, step1522]: loss 0.543015
[epoch18, step1523]: loss 0.532963
[epoch18, step1524]: loss 0.559626
[epoch18, step1525]: loss 0.595588
[epoch18, step1526]: loss 0.497977
[epoch18, step1527]: loss 0.440205
[epoch18, step1528]: loss 0.400458
[epoch18, step1529]: loss 0.531237
[epoch18, step1530]: loss 0.462101
[epoch18, step1531]: loss 0.650670
[epoch18, step1532]: loss 0.655964
[epoch18, step1533]: loss 0.693771
[epoch18, step1534]: loss 0.533078
[epoch18, step1535]: loss 0.650810
[epoch18, step1536]: loss 0.511083
[epoch18, step1537]: loss 0.508412
[epoch18, step1538]: loss 0.642564
[epoch18, step1539]: loss 0.661626
[epoch18, step1540]: loss 0.371464
[epoch18, step1541]: loss 0.547016
[epoch18, step1542]: loss 0.442119
[epoch18, step1543]: loss 0.610086
[epoch18, step1544]: loss 0.467754
[epoch18, step1545]: loss 0.611028
[epoch18, step1546]: loss 0.552585
[epoch18, step1547]: loss 0.657153
[epoch18, step1548]: loss 0.552167
[epoch18, step1549]: loss 0.720200
[epoch18, step1550]: loss 0.631047
[epoch18, step1551]: loss 0.354485
[epoch18, step1552]: loss 0.454411
[epoch18, step1553]: loss 0.387447
[epoch18, step1554]: loss 0.669529
[epoch18, step1555]: loss 0.535609
[epoch18, step1556]: loss 0.576124
[epoch18, step1557]: loss 0.622067
[epoch18, step1558]: loss 0.543298
[epoch18, step1559]: loss 0.558567
[epoch18, step1560]: loss 0.633622
[epoch18, step1561]: loss 0.603829
[epoch18, step1562]: loss 0.641104
[epoch18, step1563]: loss 0.844390
[epoch18, step1564]: loss 0.416991
[epoch18, step1565]: loss 0.406717
[epoch18, step1566]: loss 0.696405
[epoch18, step1567]: loss 0.386592
[epoch18, step1568]: loss 0.479292
[epoch18, step1569]: loss 0.658291
[epoch18, step1570]: loss 0.666830
[epoch18, step1571]: loss 0.562424
[epoch18, step1572]: loss 0.690896
[epoch18, step1573]: loss 0.555884
[epoch18, step1574]: loss 0.693051
[epoch18, step1575]: loss 0.683697
[epoch18, step1576]: loss 0.370079
[epoch18, step1577]: loss 0.628713
[epoch18, step1578]: loss 0.568645
[epoch18, step1579]: loss 0.424080
[epoch18, step1580]: loss 0.710220
[epoch18, step1581]: loss 0.723513
[epoch18, step1582]: loss 0.633090
[epoch18, step1583]: loss 0.613810
[epoch18, step1584]: loss 0.732770
[epoch18, step1585]: loss 0.358466
[epoch18, step1586]: loss 0.613632
[epoch18, step1587]: loss 0.522354
[epoch18, step1588]: loss 0.553976
[epoch18, step1589]: loss 0.613298
[epoch18, step1590]: loss 0.688104
[epoch18, step1591]: loss 0.568733
[epoch18, step1592]: loss 0.458780
[epoch18, step1593]: loss 0.595641
[epoch18, step1594]: loss 0.439795
[epoch18, step1595]: loss 0.523061
[epoch18, step1596]: loss 0.677773
[epoch18, step1597]: loss 0.621124
[epoch18, step1598]: loss 0.438187
[epoch18, step1599]: loss 0.297024
[epoch18, step1600]: loss 0.435620
[epoch18, step1601]: loss 0.481514
[epoch18, step1602]: loss 0.381429
[epoch18, step1603]: loss 0.711004
[epoch18, step1604]: loss 0.726882
[epoch18, step1605]: loss 0.265800
[epoch18, step1606]: loss 0.564372
[epoch18, step1607]: loss 0.509616
[epoch18, step1608]: loss 0.363900
[epoch18, step1609]: loss 0.353316
[epoch18, step1610]: loss 0.731581
[epoch18, step1611]: loss 0.546876
[epoch18, step1612]: loss 0.571205
[epoch18, step1613]: loss 0.576838
[epoch18, step1614]: loss 0.465333
[epoch18, step1615]: loss 0.456026
[epoch18, step1616]: loss 0.470539
[epoch18, step1617]: loss 0.693765
[epoch18, step1618]: loss 0.603715
[epoch18, step1619]: loss 0.400855
[epoch18, step1620]: loss 0.550899
[epoch18, step1621]: loss 0.407671
[epoch18, step1622]: loss 0.152804
[epoch18, step1623]: loss 0.351131
[epoch18, step1624]: loss 0.647265
[epoch18, step1625]: loss 0.622520
[epoch18, step1626]: loss 0.567736
[epoch18, step1627]: loss 0.696000
[epoch18, step1628]: loss 0.538703
[epoch18, step1629]: loss 0.714568
[epoch18, step1630]: loss 0.518911
[epoch18, step1631]: loss 0.450123
[epoch18, step1632]: loss 0.723108
[epoch18, step1633]: loss 0.373023
[epoch18, step1634]: loss 0.613607
[epoch18, step1635]: loss 0.556923
[epoch18, step1636]: loss 0.531338
[epoch18, step1637]: loss 0.478451
[epoch18, step1638]: loss 0.486298
[epoch18, step1639]: loss 0.747806
[epoch18, step1640]: loss 0.525622
[epoch18, step1641]: loss 0.658171
[epoch18, step1642]: loss 0.527754
[epoch18, step1643]: loss 0.584145
[epoch18, step1644]: loss 0.465701
[epoch18, step1645]: loss 0.678384
[epoch18, step1646]: loss 0.444115
[epoch18, step1647]: loss 0.600832
[epoch18, step1648]: loss 0.538162
[epoch18, step1649]: loss 0.159795
[epoch18, step1650]: loss 0.565182
[epoch18, step1651]: loss 0.398202
[epoch18, step1652]: loss 0.622942
[epoch18, step1653]: loss 0.559714
[epoch18, step1654]: loss 0.565987
[epoch18, step1655]: loss 0.704495
[epoch18, step1656]: loss 0.581711
[epoch18, step1657]: loss 0.667549
[epoch18, step1658]: loss 0.737625
[epoch18, step1659]: loss 0.609138
[epoch18, step1660]: loss 0.554884
[epoch18, step1661]: loss 0.546862
[epoch18, step1662]: loss 0.701591
[epoch18, step1663]: loss 0.534149
[epoch18, step1664]: loss 0.489996
[epoch18, step1665]: loss 0.721132
[epoch18, step1666]: loss 0.524228
[epoch18, step1667]: loss 0.509466
[epoch18, step1668]: loss 0.490421
[epoch18, step1669]: loss 0.472244
[epoch18, step1670]: loss 0.635479
[epoch18, step1671]: loss 0.536382
[epoch18, step1672]: loss 0.612619
[epoch18, step1673]: loss 0.400997
[epoch18, step1674]: loss 0.696102
[epoch18, step1675]: loss 0.617424
[epoch18, step1676]: loss 0.466173
[epoch18, step1677]: loss 0.701990
[epoch18, step1678]: loss 0.487140
[epoch18, step1679]: loss 0.650549
[epoch18, step1680]: loss 0.646582
[epoch18, step1681]: loss 0.718239
[epoch18, step1682]: loss 0.649929
[epoch18, step1683]: loss 0.316756
[epoch18, step1684]: loss 0.573818
[epoch18, step1685]: loss 0.510732
[epoch18, step1686]: loss 0.622265
[epoch18, step1687]: loss 0.469272
[epoch18, step1688]: loss 0.506432
[epoch18, step1689]: loss 0.694462
[epoch18, step1690]: loss 0.396712
[epoch18, step1691]: loss 0.580664
[epoch18, step1692]: loss 0.411163
[epoch18, step1693]: loss 0.588682
[epoch18, step1694]: loss 0.457751
[epoch18, step1695]: loss 0.291483
[epoch18, step1696]: loss 0.481659
[epoch18, step1697]: loss 0.260572
[epoch18, step1698]: loss 0.624191
[epoch18, step1699]: loss 0.810956
[epoch18, step1700]: loss 0.317081
[epoch18, step1701]: loss 0.311405
[epoch18, step1702]: loss 0.507272
[epoch18, step1703]: loss 0.759273
[epoch18, step1704]: loss 0.704931
[epoch18, step1705]: loss 0.621682
[epoch18, step1706]: loss 0.649701
[epoch18, step1707]: loss 0.742989
[epoch18, step1708]: loss 0.653971
[epoch18, step1709]: loss 0.685112
[epoch18, step1710]: loss 0.589330
[epoch18, step1711]: loss 0.538165
[epoch18, step1712]: loss 0.723896
[epoch18, step1713]: loss 0.374358
[epoch18, step1714]: loss 0.630012
[epoch18, step1715]: loss 0.497472
[epoch18, step1716]: loss 0.577836
[epoch18, step1717]: loss 0.555400
[epoch18, step1718]: loss 0.701037
[epoch18, step1719]: loss 0.447604
[epoch18, step1720]: loss 0.388752
[epoch18, step1721]: loss 0.722602
[epoch18, step1722]: loss 0.465606
[epoch18, step1723]: loss 0.128816
[epoch18, step1724]: loss 0.560963
[epoch18, step1725]: loss 0.592522
[epoch18, step1726]: loss 0.363812
[epoch18, step1727]: loss 0.598639
[epoch18, step1728]: loss 0.656844
[epoch18, step1729]: loss 0.442392
[epoch18, step1730]: loss 0.489754
[epoch18, step1731]: loss 0.341986
[epoch18, step1732]: loss 0.530682
[epoch18, step1733]: loss 0.259766
[epoch18, step1734]: loss 0.250464
[epoch18, step1735]: loss 0.516290
[epoch18, step1736]: loss 0.103680
[epoch18, step1737]: loss 0.649254
[epoch18, step1738]: loss 0.326536
[epoch18, step1739]: loss 0.459765
[epoch18, step1740]: loss 0.573644
[epoch18, step1741]: loss 0.702401
[epoch18, step1742]: loss 0.558268
[epoch18, step1743]: loss 0.322695
[epoch18, step1744]: loss 0.358695
[epoch18, step1745]: loss 0.548550
[epoch18, step1746]: loss 0.634476
[epoch18, step1747]: loss 0.305571
[epoch18, step1748]: loss 0.751235
[epoch18, step1749]: loss 0.540765
[epoch18, step1750]: loss 0.461550
[epoch18, step1751]: loss 0.493586
[epoch18, step1752]: loss 0.386445
[epoch18, step1753]: loss 0.571479
[epoch18, step1754]: loss 0.629674
[epoch18, step1755]: loss 0.573933
[epoch18, step1756]: loss 0.355670
[epoch18, step1757]: loss 0.405772
[epoch18, step1758]: loss 0.368611
[epoch18, step1759]: loss 0.713870
[epoch18, step1760]: loss 0.500218
[epoch18, step1761]: loss 0.328988
[epoch18, step1762]: loss 0.400220
[epoch18, step1763]: loss 0.639847
[epoch18, step1764]: loss 0.343582
[epoch18, step1765]: loss 0.712183
[epoch18, step1766]: loss 0.575829
[epoch18, step1767]: loss 0.361998
[epoch18, step1768]: loss 0.478640
[epoch18, step1769]: loss 0.577533
[epoch18, step1770]: loss 0.623444
[epoch18, step1771]: loss 0.689870
[epoch18, step1772]: loss 0.536341
[epoch18, step1773]: loss 0.583027
[epoch18, step1774]: loss 0.381687
[epoch18, step1775]: loss 0.552101
[epoch18, step1776]: loss 0.453370
[epoch18, step1777]: loss 0.323086
[epoch18, step1778]: loss 0.490211
[epoch18, step1779]: loss 0.547559
[epoch18, step1780]: loss 0.422193
[epoch18, step1781]: loss 0.525200
[epoch18, step1782]: loss 0.622801
[epoch18, step1783]: loss 0.651640
[epoch18, step1784]: loss 0.359829
[epoch18, step1785]: loss 0.367619
[epoch18, step1786]: loss 0.579643
[epoch18, step1787]: loss 0.644496
[epoch18, step1788]: loss 0.227338
[epoch18, step1789]: loss 0.666848
[epoch18, step1790]: loss 0.616017
[epoch18, step1791]: loss 0.390347
[epoch18, step1792]: loss 0.554986
[epoch18, step1793]: loss 0.625182
[epoch18, step1794]: loss 0.488468
[epoch18, step1795]: loss 0.569633
[epoch18, step1796]: loss 0.652104
[epoch18, step1797]: loss 0.624787
[epoch18, step1798]: loss 0.583577
[epoch18, step1799]: loss 0.754276
[epoch18, step1800]: loss 0.377930
[epoch18, step1801]: loss 0.398581
[epoch18, step1802]: loss 0.486162
[epoch18, step1803]: loss 0.472325
[epoch18, step1804]: loss 0.468915
[epoch18, step1805]: loss 0.669956
[epoch18, step1806]: loss 0.511485
[epoch18, step1807]: loss 0.325131
[epoch18, step1808]: loss 0.449635
[epoch18, step1809]: loss 0.281686
[epoch18, step1810]: loss 0.699617
[epoch18, step1811]: loss 0.563969
[epoch18, step1812]: loss 0.256122
[epoch18, step1813]: loss 0.537305
[epoch18, step1814]: loss 0.447745
[epoch18, step1815]: loss 0.569112
[epoch18, step1816]: loss 0.288708
[epoch18, step1817]: loss 0.587534
[epoch18, step1818]: loss 0.573025
[epoch18, step1819]: loss 0.575207
[epoch18, step1820]: loss 0.805636
[epoch18, step1821]: loss 0.560076
[epoch18, step1822]: loss 0.306273
[epoch18, step1823]: loss 0.548659
[epoch18, step1824]: loss 0.619716
[epoch18, step1825]: loss 0.517455
[epoch18, step1826]: loss 0.528322
[epoch18, step1827]: loss 0.556562
[epoch18, step1828]: loss 0.558119
[epoch18, step1829]: loss 0.554707
[epoch18, step1830]: loss 0.482260
[epoch18, step1831]: loss 0.545971
[epoch18, step1832]: loss 0.448667
[epoch18, step1833]: loss 0.755413
[epoch18, step1834]: loss 0.434560
[epoch18, step1835]: loss 0.704067
[epoch18, step1836]: loss 0.411940
[epoch18, step1837]: loss 0.577919
[epoch18, step1838]: loss 0.572949
[epoch18, step1839]: loss 0.755018
[epoch18, step1840]: loss 0.568573
[epoch18, step1841]: loss 0.300906
[epoch18, step1842]: loss 0.335061
[epoch18, step1843]: loss 0.599573
[epoch18, step1844]: loss 0.632203
[epoch18, step1845]: loss 0.563938
[epoch18, step1846]: loss 0.513742
[epoch18, step1847]: loss 0.445902
[epoch18, step1848]: loss 0.372450
[epoch18, step1849]: loss 0.419760
[epoch18, step1850]: loss 0.669894
[epoch18, step1851]: loss 0.744378
[epoch18, step1852]: loss 0.301362
[epoch18, step1853]: loss 0.706570
[epoch18, step1854]: loss 0.428557
[epoch18, step1855]: loss 0.583042
[epoch18, step1856]: loss 0.415655
[epoch18, step1857]: loss 0.522102
[epoch18, step1858]: loss 0.494997
[epoch18, step1859]: loss 0.730457
[epoch18, step1860]: loss 0.476487
[epoch18, step1861]: loss 0.743821
[epoch18, step1862]: loss 0.452024
[epoch18, step1863]: loss 0.789933
[epoch18, step1864]: loss 0.656759
[epoch18, step1865]: loss 0.449735
[epoch18, step1866]: loss 0.577883
[epoch18, step1867]: loss 0.295546
[epoch18, step1868]: loss 0.306739
[epoch18, step1869]: loss 0.521888
[epoch18, step1870]: loss 0.506168
[epoch18, step1871]: loss 0.667766
[epoch18, step1872]: loss 0.386868
[epoch18, step1873]: loss 0.617723
[epoch18, step1874]: loss 0.568657
[epoch18, step1875]: loss 0.406018
[epoch18, step1876]: loss 0.392048
[epoch18, step1877]: loss 0.399638
[epoch18, step1878]: loss 0.542471
[epoch18, step1879]: loss 0.279739
[epoch18, step1880]: loss 0.593487
[epoch18, step1881]: loss 0.741168
[epoch18, step1882]: loss 0.698369
[epoch18, step1883]: loss 0.662845
[epoch18, step1884]: loss 0.539750
[epoch18, step1885]: loss 0.766358
[epoch18, step1886]: loss 0.513284
[epoch18, step1887]: loss 0.630338
[epoch18, step1888]: loss 0.556166
[epoch18, step1889]: loss 0.548698
[epoch18, step1890]: loss 0.141577
[epoch18, step1891]: loss 0.594150
[epoch18, step1892]: loss 0.264987
[epoch18, step1893]: loss 0.692265
[epoch18, step1894]: loss 0.193624
[epoch18, step1895]: loss 0.256289
[epoch18, step1896]: loss 0.811641
[epoch18, step1897]: loss 0.284212
[epoch18, step1898]: loss 0.566795
[epoch18, step1899]: loss 0.531873
[epoch18, step1900]: loss 0.622532
[epoch18, step1901]: loss 0.639346
[epoch18, step1902]: loss 0.642927
[epoch18, step1903]: loss 0.498620
[epoch18, step1904]: loss 0.633208
[epoch18, step1905]: loss 0.334061
[epoch18, step1906]: loss 0.649906
[epoch18, step1907]: loss 0.479467
[epoch18, step1908]: loss 0.710742
[epoch18, step1909]: loss 0.551959
[epoch18, step1910]: loss 0.632931
[epoch18, step1911]: loss 0.609388
[epoch18, step1912]: loss 0.509797
[epoch18, step1913]: loss 0.397227
[epoch18, step1914]: loss 0.532655
[epoch18, step1915]: loss 0.457312
[epoch18, step1916]: loss 0.367473
[epoch18, step1917]: loss 0.586159
[epoch18, step1918]: loss 0.565284
[epoch18, step1919]: loss 0.498804
[epoch18, step1920]: loss 0.588905
[epoch18, step1921]: loss 0.659175
[epoch18, step1922]: loss 0.368899
[epoch18, step1923]: loss 0.511244
[epoch18, step1924]: loss 0.534772
[epoch18, step1925]: loss 0.607554
[epoch18, step1926]: loss 0.478514
[epoch18, step1927]: loss 0.344037
[epoch18, step1928]: loss 0.420630
[epoch18, step1929]: loss 0.531538
[epoch18, step1930]: loss 0.662582
[epoch18, step1931]: loss 0.583438
[epoch18, step1932]: loss 0.621894
[epoch18, step1933]: loss 0.914881
[epoch18, step1934]: loss 0.809729
[epoch18, step1935]: loss 0.598201
[epoch18, step1936]: loss 0.553023
[epoch18, step1937]: loss 0.484352
[epoch18, step1938]: loss 0.233874
[epoch18, step1939]: loss 0.503401
[epoch18, step1940]: loss 0.504393
[epoch18, step1941]: loss 0.891268
[epoch18, step1942]: loss 0.644971
[epoch18, step1943]: loss 0.386836
[epoch18, step1944]: loss 0.647168
[epoch18, step1945]: loss 0.606244
[epoch18, step1946]: loss 0.486304
[epoch18, step1947]: loss 0.695932
[epoch18, step1948]: loss 0.427600
[epoch18, step1949]: loss 0.687364
[epoch18, step1950]: loss 0.563054
[epoch18, step1951]: loss 0.607689
[epoch18, step1952]: loss 0.538294
[epoch18, step1953]: loss 0.705915
[epoch18, step1954]: loss 0.403659
[epoch18, step1955]: loss 0.152303
[epoch18, step1956]: loss 0.620733
[epoch18, step1957]: loss 0.580255
[epoch18, step1958]: loss 0.396533
[epoch18, step1959]: loss 0.502938
[epoch18, step1960]: loss 0.689127
[epoch18, step1961]: loss 0.419229
[epoch18, step1962]: loss 0.643730
[epoch18, step1963]: loss 0.445193
[epoch18, step1964]: loss 0.707126
[epoch18, step1965]: loss 0.458954
[epoch18, step1966]: loss 0.694412
[epoch18, step1967]: loss 0.515283
[epoch18, step1968]: loss 0.413729
[epoch18, step1969]: loss 0.634777
[epoch18, step1970]: loss 0.623525
[epoch18, step1971]: loss 0.637310
[epoch18, step1972]: loss 0.653407
[epoch18, step1973]: loss 0.702533
[epoch18, step1974]: loss 0.686215
[epoch18, step1975]: loss 0.545838
[epoch18, step1976]: loss 0.525949
[epoch18, step1977]: loss 0.497158
[epoch18, step1978]: loss 0.378525
[epoch18, step1979]: loss 0.645762
[epoch18, step1980]: loss 0.686516
[epoch18, step1981]: loss 0.559614
[epoch18, step1982]: loss 0.585217
[epoch18, step1983]: loss 0.739813
[epoch18, step1984]: loss 0.543113
[epoch18, step1985]: loss 0.724348
[epoch18, step1986]: loss 0.459216
[epoch18, step1987]: loss 0.641703
[epoch18, step1988]: loss 0.460452
[epoch18, step1989]: loss 0.354251
[epoch18, step1990]: loss 0.570243
[epoch18, step1991]: loss 0.632513
[epoch18, step1992]: loss 0.631612
[epoch18, step1993]: loss 0.642723
[epoch18, step1994]: loss 0.698810
[epoch18, step1995]: loss 0.567745
[epoch18, step1996]: loss 0.590667
[epoch18, step1997]: loss 0.489034
[epoch18, step1998]: loss 0.360267
[epoch18, step1999]: loss 0.381441
[epoch18, step2000]: loss 0.554925
[epoch18, step2001]: loss 0.386327
[epoch18, step2002]: loss 0.419420
[epoch18, step2003]: loss 0.620030
[epoch18, step2004]: loss 0.474303
[epoch18, step2005]: loss 0.733839
[epoch18, step2006]: loss 0.487043
[epoch18, step2007]: loss 0.310109
[epoch18, step2008]: loss 0.393609
[epoch18, step2009]: loss 0.588099
[epoch18, step2010]: loss 0.232811
[epoch18, step2011]: loss 0.565574
[epoch18, step2012]: loss 0.605675
[epoch18, step2013]: loss 0.467805
[epoch18, step2014]: loss 0.601982
[epoch18, step2015]: loss 0.368301
[epoch18, step2016]: loss 0.689431
[epoch18, step2017]: loss 0.473219
[epoch18, step2018]: loss 0.440771
[epoch18, step2019]: loss 0.658529
[epoch18, step2020]: loss 0.614541
[epoch18, step2021]: loss 0.591050
[epoch18, step2022]: loss 0.543697
[epoch18, step2023]: loss 0.581720
[epoch18, step2024]: loss 0.584517
[epoch18, step2025]: loss 0.446624
[epoch18, step2026]: loss 0.462948
[epoch18, step2027]: loss 0.427884
[epoch18, step2028]: loss 0.656151
[epoch18, step2029]: loss 0.156701
[epoch18, step2030]: loss 0.744840
[epoch18, step2031]: loss 0.577384
[epoch18, step2032]: loss 0.697284
[epoch18, step2033]: loss 0.361266
[epoch18, step2034]: loss 0.727710
[epoch18, step2035]: loss 0.278739
[epoch18, step2036]: loss 0.415058
[epoch18, step2037]: loss 0.542146
[epoch18, step2038]: loss 0.570707
[epoch18, step2039]: loss 0.623883
[epoch18, step2040]: loss 0.569426
[epoch18, step2041]: loss 0.431765
[epoch18, step2042]: loss 0.225963
[epoch18, step2043]: loss 0.523916
[epoch18, step2044]: loss 0.498718
[epoch18, step2045]: loss 0.433459
[epoch18, step2046]: loss 0.575495
[epoch18, step2047]: loss 0.832988
[epoch18, step2048]: loss 0.362542
[epoch18, step2049]: loss 0.634566
[epoch18, step2050]: loss 0.656326
[epoch18, step2051]: loss 0.427598
[epoch18, step2052]: loss 0.355265
[epoch18, step2053]: loss 0.520636
[epoch18, step2054]: loss 0.438269
[epoch18, step2055]: loss 0.453173
[epoch18, step2056]: loss 0.427033
[epoch18, step2057]: loss 0.739227
[epoch18, step2058]: loss 0.561541
[epoch18, step2059]: loss 0.605820
[epoch18, step2060]: loss 0.422064
[epoch18, step2061]: loss 0.828879
[epoch18, step2062]: loss 0.578439
[epoch18, step2063]: loss 0.290438
[epoch18, step2064]: loss 0.722895
[epoch18, step2065]: loss 0.612873
[epoch18, step2066]: loss 0.529690
[epoch18, step2067]: loss 0.645576
[epoch18, step2068]: loss 0.768853
[epoch18, step2069]: loss 0.607507
[epoch18, step2070]: loss 0.636131
[epoch18, step2071]: loss 0.627630
[epoch18, step2072]: loss 0.478156
[epoch18, step2073]: loss 0.478335
[epoch18, step2074]: loss 0.629810
[epoch18, step2075]: loss 0.635925
[epoch18, step2076]: loss 0.401624
[epoch18, step2077]: loss 0.583913
[epoch18, step2078]: loss 0.611113
[epoch18, step2079]: loss 0.577293
[epoch18, step2080]: loss 0.323944
[epoch18, step2081]: loss 0.439031
[epoch18, step2082]: loss 0.630664
[epoch18, step2083]: loss 0.431366
[epoch18, step2084]: loss 0.544603
[epoch18, step2085]: loss 0.454811
[epoch18, step2086]: loss 0.622941
[epoch18, step2087]: loss 0.341702
[epoch18, step2088]: loss 0.735910
[epoch18, step2089]: loss 0.264120
[epoch18, step2090]: loss 0.632770
[epoch18, step2091]: loss 0.323499
[epoch18, step2092]: loss 0.463098
[epoch18, step2093]: loss 0.597765
[epoch18, step2094]: loss 0.628208
[epoch18, step2095]: loss 0.657137
[epoch18, step2096]: loss 0.490883
[epoch18, step2097]: loss 0.325516
[epoch18, step2098]: loss 0.544863
[epoch18, step2099]: loss 0.604203
[epoch18, step2100]: loss 0.672737
[epoch18, step2101]: loss 0.542470
[epoch18, step2102]: loss 0.762151
[epoch18, step2103]: loss 0.294231
[epoch18, step2104]: loss 0.370892
[epoch18, step2105]: loss 0.309568
[epoch18, step2106]: loss 0.555372
[epoch18, step2107]: loss 0.461209
[epoch18, step2108]: loss 0.782908
[epoch18, step2109]: loss 0.577359
[epoch18, step2110]: loss 0.374523
[epoch18, step2111]: loss 0.381222
[epoch18, step2112]: loss 0.621967
[epoch18, step2113]: loss 0.459413
[epoch18, step2114]: loss 0.560781
[epoch18, step2115]: loss 0.590958
[epoch18, step2116]: loss 0.260617
[epoch18, step2117]: loss 0.485794
[epoch18, step2118]: loss 0.685803
[epoch18, step2119]: loss 0.259564
[epoch18, step2120]: loss 0.536788
[epoch18, step2121]: loss 0.665398
[epoch18, step2122]: loss 0.353970
[epoch18, step2123]: loss 0.533455
[epoch18, step2124]: loss 0.556892
[epoch18, step2125]: loss 0.732852
[epoch18, step2126]: loss 0.533615
[epoch18, step2127]: loss 0.523368
[epoch18, step2128]: loss 0.427330
[epoch18, step2129]: loss 0.441518
[epoch18, step2130]: loss 0.382686
[epoch18, step2131]: loss 0.443879
[epoch18, step2132]: loss 0.404992
[epoch18, step2133]: loss 0.722768
[epoch18, step2134]: loss 0.491664
[epoch18, step2135]: loss 0.889194
[epoch18, step2136]: loss 0.489506
[epoch18, step2137]: loss 0.643338
[epoch18, step2138]: loss 0.541945
[epoch18, step2139]: loss 0.442460
[epoch18, step2140]: loss 0.491649
[epoch18, step2141]: loss 0.480891
[epoch18, step2142]: loss 0.526483
[epoch18, step2143]: loss 0.586183
[epoch18, step2144]: loss 0.506734
[epoch18, step2145]: loss 0.704553
[epoch18, step2146]: loss 0.644021
[epoch18, step2147]: loss 0.535384
[epoch18, step2148]: loss 0.529469
[epoch18, step2149]: loss 0.587123
[epoch18, step2150]: loss 0.699137
[epoch18, step2151]: loss 0.513223
[epoch18, step2152]: loss 0.442396
[epoch18, step2153]: loss 0.644547
[epoch18, step2154]: loss 0.482121
[epoch18, step2155]: loss 0.410643
[epoch18, step2156]: loss 0.658055
[epoch18, step2157]: loss 0.531591
[epoch18, step2158]: loss 0.649927
[epoch18, step2159]: loss 0.547097
[epoch18, step2160]: loss 0.485330
[epoch18, step2161]: loss 0.359551
[epoch18, step2162]: loss 0.428329
[epoch18, step2163]: loss 0.475728
[epoch18, step2164]: loss 0.614978
[epoch18, step2165]: loss 0.722832
[epoch18, step2166]: loss 0.422289
[epoch18, step2167]: loss 0.635172
[epoch18, step2168]: loss 0.607899
[epoch18, step2169]: loss 0.664285
[epoch18, step2170]: loss 0.599954
[epoch18, step2171]: loss 0.478038
[epoch18, step2172]: loss 0.568400
[epoch18, step2173]: loss 0.491049
[epoch18, step2174]: loss 0.509738
[epoch18, step2175]: loss 0.464145
[epoch18, step2176]: loss 0.625207
[epoch18, step2177]: loss 0.530406
[epoch18, step2178]: loss 0.364383
[epoch18, step2179]: loss 0.582859
[epoch18, step2180]: loss 0.591760
[epoch18, step2181]: loss 0.554048
[epoch18, step2182]: loss 0.494230
[epoch18, step2183]: loss 0.542547
[epoch18, step2184]: loss 0.659846
[epoch18, step2185]: loss 0.675791
[epoch18, step2186]: loss 0.657066
[epoch18, step2187]: loss 0.645795
[epoch18, step2188]: loss 0.559920
[epoch18, step2189]: loss 0.637223
[epoch18, step2190]: loss 0.496613
[epoch18, step2191]: loss 0.831582
[epoch18, step2192]: loss 0.657678
[epoch18, step2193]: loss 0.322280
[epoch18, step2194]: loss 0.541258
[epoch18, step2195]: loss 0.559627
[epoch18, step2196]: loss 0.565544
[epoch18, step2197]: loss 0.650480
[epoch18, step2198]: loss 0.562630
[epoch18, step2199]: loss 0.464321
[epoch18, step2200]: loss 0.683494
[epoch18, step2201]: loss 0.724233
[epoch18, step2202]: loss 0.514912
[epoch18, step2203]: loss 0.594870
[epoch18, step2204]: loss 0.398738
[epoch18, step2205]: loss 0.498519
[epoch18, step2206]: loss 0.702909
[epoch18, step2207]: loss 0.391531
[epoch18, step2208]: loss 0.340679
[epoch18, step2209]: loss 0.591475
[epoch18, step2210]: loss 0.515127
[epoch18, step2211]: loss 0.637873
[epoch18, step2212]: loss 0.279104
[epoch18, step2213]: loss 0.642455
[epoch18, step2214]: loss 0.500486
[epoch18, step2215]: loss 0.528692
[epoch18, step2216]: loss 0.614183
[epoch18, step2217]: loss 0.570151
[epoch18, step2218]: loss 0.719465
[epoch18, step2219]: loss 0.628569
[epoch18, step2220]: loss 0.632954
[epoch18, step2221]: loss 0.301868
[epoch18, step2222]: loss 0.515130
[epoch18, step2223]: loss 0.416684
[epoch18, step2224]: loss 0.619046
[epoch18, step2225]: loss 0.598805
[epoch18, step2226]: loss 0.333265
[epoch18, step2227]: loss 0.559680
[epoch18, step2228]: loss 0.684644
[epoch18, step2229]: loss 0.529653
[epoch18, step2230]: loss 0.567753
[epoch18, step2231]: loss 0.509610
[epoch18, step2232]: loss 0.688778
[epoch18, step2233]: loss 0.465255
[epoch18, step2234]: loss 0.642958
[epoch18, step2235]: loss 0.686118
[epoch18, step2236]: loss 0.482193
[epoch18, step2237]: loss 0.503167
[epoch18, step2238]: loss 0.535422
[epoch18, step2239]: loss 0.621904
[epoch18, step2240]: loss 0.334365
[epoch18, step2241]: loss 0.370144
[epoch18, step2242]: loss 0.510294
[epoch18, step2243]: loss 0.697860
[epoch18, step2244]: loss 0.526754
[epoch18, step2245]: loss 0.747888
[epoch18, step2246]: loss 0.463168
[epoch18, step2247]: loss 0.575174
[epoch18, step2248]: loss 0.706378
[epoch18, step2249]: loss 0.470862
[epoch18, step2250]: loss 0.564825
[epoch18, step2251]: loss 0.529002
[epoch18, step2252]: loss 0.435863
[epoch18, step2253]: loss 0.352609
[epoch18, step2254]: loss 0.637063
[epoch18, step2255]: loss 0.711376
[epoch18, step2256]: loss 0.255536
[epoch18, step2257]: loss 0.358569
[epoch18, step2258]: loss 0.620612
[epoch18, step2259]: loss 0.379608
[epoch18, step2260]: loss 0.403760
[epoch18, step2261]: loss 0.514547
[epoch18, step2262]: loss 0.429422
[epoch18, step2263]: loss 0.472239
[epoch18, step2264]: loss 0.519934
[epoch18, step2265]: loss 0.676657
[epoch18, step2266]: loss 0.582219
[epoch18, step2267]: loss 0.715057
[epoch18, step2268]: loss 0.524415
[epoch18, step2269]: loss 0.292138
[epoch18, step2270]: loss 0.447814
[epoch18, step2271]: loss 0.393803
[epoch18, step2272]: loss 0.573687
[epoch18, step2273]: loss 0.380419
[epoch18, step2274]: loss 0.738330
[epoch18, step2275]: loss 0.520602
[epoch18, step2276]: loss 0.394564
[epoch18, step2277]: loss 0.589089
[epoch18, step2278]: loss 0.622162
[epoch18, step2279]: loss 0.551115
[epoch18, step2280]: loss 0.675970
[epoch18, step2281]: loss 0.486205
[epoch18, step2282]: loss 0.458534
[epoch18, step2283]: loss 0.393042
[epoch18, step2284]: loss 0.557247
[epoch18, step2285]: loss 0.602672
[epoch18, step2286]: loss 0.397608
[epoch18, step2287]: loss 0.299306
[epoch18, step2288]: loss 0.417564
[epoch18, step2289]: loss 0.281666
[epoch18, step2290]: loss 0.757857
[epoch18, step2291]: loss 0.455698
[epoch18, step2292]: loss 0.565045
[epoch18, step2293]: loss 0.630896
[epoch18, step2294]: loss 0.661209
[epoch18, step2295]: loss 0.461242
[epoch18, step2296]: loss 0.484931
[epoch18, step2297]: loss 0.291838
[epoch18, step2298]: loss 0.540029
[epoch18, step2299]: loss 0.528080
[epoch18, step2300]: loss 0.604668
[epoch18, step2301]: loss 0.285711
[epoch18, step2302]: loss 0.537548
[epoch18, step2303]: loss 0.294776
[epoch18, step2304]: loss 0.747151
[epoch18, step2305]: loss 0.650001
[epoch18, step2306]: loss 0.439827
[epoch18, step2307]: loss 0.432282
[epoch18, step2308]: loss 0.344600
[epoch18, step2309]: loss 0.354346
[epoch18, step2310]: loss 0.533944
[epoch18, step2311]: loss 0.164911
[epoch18, step2312]: loss 0.722447
[epoch18, step2313]: loss 0.734204
[epoch18, step2314]: loss 0.597717
[epoch18, step2315]: loss 0.490725
[epoch18, step2316]: loss 0.510619
[epoch18, step2317]: loss 0.619406
[epoch18, step2318]: loss 0.729259
[epoch18, step2319]: loss 0.519166
[epoch18, step2320]: loss 0.527325
[epoch18, step2321]: loss 0.423997
[epoch18, step2322]: loss 0.405836
[epoch18, step2323]: loss 0.346656
[epoch18, step2324]: loss 0.603318
[epoch18, step2325]: loss 0.729677
[epoch18, step2326]: loss 0.437667
[epoch18, step2327]: loss 0.672154
[epoch18, step2328]: loss 0.562838
[epoch18, step2329]: loss 0.427961
[epoch18, step2330]: loss 0.508981
[epoch18, step2331]: loss 0.427044
[epoch18, step2332]: loss 0.383902
[epoch18, step2333]: loss 0.561635
[epoch18, step2334]: loss 0.694908
[epoch18, step2335]: loss 0.393482
[epoch18, step2336]: loss 0.507533
[epoch18, step2337]: loss 0.485241
[epoch18, step2338]: loss 0.475180
[epoch18, step2339]: loss 0.279607
[epoch18, step2340]: loss 0.413245
[epoch18, step2341]: loss 0.452021
[epoch18, step2342]: loss 0.584489
[epoch18, step2343]: loss 0.670580
[epoch18, step2344]: loss 0.447700
[epoch18, step2345]: loss 0.557713
[epoch18, step2346]: loss 0.362774
[epoch18, step2347]: loss 0.668569
[epoch18, step2348]: loss 0.712926
[epoch18, step2349]: loss 0.407209
[epoch18, step2350]: loss 0.370642
[epoch18, step2351]: loss 0.370563
[epoch18, step2352]: loss 0.468619
[epoch18, step2353]: loss 0.331497
[epoch18, step2354]: loss 0.647740
[epoch18, step2355]: loss 0.754671
[epoch18, step2356]: loss 0.648216
[epoch18, step2357]: loss 0.706033
[epoch18, step2358]: loss 0.554332
[epoch18, step2359]: loss 0.596252
[epoch18, step2360]: loss 0.594757
[epoch18, step2361]: loss 0.766285
[epoch18, step2362]: loss 0.536237
[epoch18, step2363]: loss 0.478556
[epoch18, step2364]: loss 0.622376
[epoch18, step2365]: loss 0.467115
[epoch18, step2366]: loss 0.407635
[epoch18, step2367]: loss 0.548201
[epoch18, step2368]: loss 0.506399
[epoch18, step2369]: loss 0.553612
[epoch18, step2370]: loss 0.364433
[epoch18, step2371]: loss 0.679706
[epoch18, step2372]: loss 0.532458
[epoch18, step2373]: loss 0.524487
[epoch18, step2374]: loss 0.490177
[epoch18, step2375]: loss 0.489555
[epoch18, step2376]: loss 0.280195
[epoch18, step2377]: loss 0.653192
[epoch18, step2378]: loss 0.524786
[epoch18, step2379]: loss 0.260333
[epoch18, step2380]: loss 0.287519
[epoch18, step2381]: loss 0.612307
[epoch18, step2382]: loss 0.780256
[epoch18, step2383]: loss 0.304744
[epoch18, step2384]: loss 0.673159
[epoch18, step2385]: loss 0.328129
[epoch18, step2386]: loss 0.300386
[epoch18, step2387]: loss 0.696751
[epoch18, step2388]: loss 0.659318
[epoch18, step2389]: loss 0.471783
[epoch18, step2390]: loss 0.664522
[epoch18, step2391]: loss 0.671053
[epoch18, step2392]: loss 0.286629
[epoch18, step2393]: loss 0.778182
[epoch18, step2394]: loss 0.723348
[epoch18, step2395]: loss 0.507063
[epoch18, step2396]: loss 0.519007
[epoch18, step2397]: loss 0.486077
[epoch18, step2398]: loss 0.509579
[epoch18, step2399]: loss 0.668599
[epoch18, step2400]: loss 0.471282
[epoch18, step2401]: loss 0.556792
[epoch18, step2402]: loss 0.702394
[epoch18, step2403]: loss 0.630017
[epoch18, step2404]: loss 0.546851
[epoch18, step2405]: loss 0.623849
[epoch18, step2406]: loss 0.552043
[epoch18, step2407]: loss 0.678745
[epoch18, step2408]: loss 0.644771
[epoch18, step2409]: loss 0.232831
[epoch18, step2410]: loss 0.372952
[epoch18, step2411]: loss 0.619030
[epoch18, step2412]: loss 0.456210
[epoch18, step2413]: loss 0.513519
[epoch18, step2414]: loss 0.587071
[epoch18, step2415]: loss 0.486803
[epoch18, step2416]: loss 0.369576
[epoch18, step2417]: loss 0.696082
[epoch18, step2418]: loss 0.791812
[epoch18, step2419]: loss 0.497238
[epoch18, step2420]: loss 0.502674
[epoch18, step2421]: loss 0.460236
[epoch18, step2422]: loss 0.475684
[epoch18, step2423]: loss 0.499755
[epoch18, step2424]: loss 0.582849
[epoch18, step2425]: loss 0.733878
[epoch18, step2426]: loss 0.516544
[epoch18, step2427]: loss 0.794873
[epoch18, step2428]: loss 0.737845
[epoch18, step2429]: loss 0.253530
[epoch18, step2430]: loss 0.248242
[epoch18, step2431]: loss 0.642078
[epoch18, step2432]: loss 0.610105
[epoch18, step2433]: loss 0.383738
[epoch18, step2434]: loss 0.310305
[epoch18, step2435]: loss 0.553316
[epoch18, step2436]: loss 0.576432
[epoch18, step2437]: loss 0.710176
[epoch18, step2438]: loss 0.413318
[epoch18, step2439]: loss 0.661290
[epoch18, step2440]: loss 0.582397
[epoch18, step2441]: loss 0.389686
[epoch18, step2442]: loss 0.452288
[epoch18, step2443]: loss 0.673207
[epoch18, step2444]: loss 0.414582
[epoch18, step2445]: loss 0.413117
[epoch18, step2446]: loss 0.613135
[epoch18, step2447]: loss 0.511736
[epoch18, step2448]: loss 0.394450
[epoch18, step2449]: loss 0.541312
[epoch18, step2450]: loss 0.629301
[epoch18, step2451]: loss 0.566517
[epoch18, step2452]: loss 0.704293
[epoch18, step2453]: loss 0.450080
[epoch18, step2454]: loss 0.438319
[epoch18, step2455]: loss 0.746281
[epoch18, step2456]: loss 0.535532
[epoch18, step2457]: loss 0.418988
[epoch18, step2458]: loss 0.422012
[epoch18, step2459]: loss 0.731289
[epoch18, step2460]: loss 0.367811
[epoch18, step2461]: loss 0.380384
[epoch18, step2462]: loss 0.666352
[epoch18, step2463]: loss 0.771127
[epoch18, step2464]: loss 0.422015
[epoch18, step2465]: loss 0.582516
[epoch18, step2466]: loss 0.679508
[epoch18, step2467]: loss 0.362096
[epoch18, step2468]: loss 0.419673
[epoch18, step2469]: loss 0.369714
[epoch18, step2470]: loss 0.344197
[epoch18, step2471]: loss 0.344595
[epoch18, step2472]: loss 0.526065
[epoch18, step2473]: loss 0.478783
[epoch18, step2474]: loss 0.533197
[epoch18, step2475]: loss 0.380635
[epoch18, step2476]: loss 0.651903
[epoch18, step2477]: loss 0.496375
[epoch18, step2478]: loss 0.679567
[epoch18, step2479]: loss 0.584039
[epoch18, step2480]: loss 0.659073
[epoch18, step2481]: loss 0.784643
[epoch18, step2482]: loss 0.362223
[epoch18, step2483]: loss 0.349059
[epoch18, step2484]: loss 0.469910
[epoch18, step2485]: loss 0.734171
[epoch18, step2486]: loss 0.735929
[epoch18, step2487]: loss 0.518192
[epoch18, step2488]: loss 0.382295
[epoch18, step2489]: loss 0.556527
[epoch18, step2490]: loss 0.496397
[epoch18, step2491]: loss 0.571854
[epoch18, step2492]: loss 0.439307
[epoch18, step2493]: loss 0.647351
[epoch18, step2494]: loss 0.616027
[epoch18, step2495]: loss 0.373657
[epoch18, step2496]: loss 0.556533
[epoch18, step2497]: loss 0.462351
[epoch18, step2498]: loss 0.473667
[epoch18, step2499]: loss 0.480043
[epoch18, step2500]: loss 0.463332
[epoch18, step2501]: loss 0.207578
[epoch18, step2502]: loss 0.314820
[epoch18, step2503]: loss 0.477333
[epoch18, step2504]: loss 0.733024
[epoch18, step2505]: loss 0.180582
[epoch18, step2506]: loss 0.343794
[epoch18, step2507]: loss 0.549419
[epoch18, step2508]: loss 0.350591
[epoch18, step2509]: loss 0.581567
[epoch18, step2510]: loss 0.619043
[epoch18, step2511]: loss 0.397645
[epoch18, step2512]: loss 0.614114
[epoch18, step2513]: loss 0.748614
[epoch18, step2514]: loss 0.173678
[epoch18, step2515]: loss 0.570019
[epoch18, step2516]: loss 0.544224
[epoch18, step2517]: loss 0.482309
[epoch18, step2518]: loss 0.366733
[epoch18, step2519]: loss 0.755308
[epoch18, step2520]: loss 0.717127
[epoch18, step2521]: loss 0.394587
[epoch18, step2522]: loss 0.468295
[epoch18, step2523]: loss 0.411780
[epoch18, step2524]: loss 0.611211
[epoch18, step2525]: loss 0.887370
[epoch18, step2526]: loss 0.544934
[epoch18, step2527]: loss 0.416702
[epoch18, step2528]: loss 0.533071
[epoch18, step2529]: loss 0.564739
[epoch18, step2530]: loss 0.501925
[epoch18, step2531]: loss 0.595166
[epoch18, step2532]: loss 0.494677
[epoch18, step2533]: loss 0.677453
[epoch18, step2534]: loss 0.132477
[epoch18, step2535]: loss 0.488719
[epoch18, step2536]: loss 0.591877
[epoch18, step2537]: loss 0.473702
[epoch18, step2538]: loss 0.402967
[epoch18, step2539]: loss 0.492514
[epoch18, step2540]: loss 0.636394
[epoch18, step2541]: loss 0.344117
[epoch18, step2542]: loss 0.480797
[epoch18, step2543]: loss 0.580486
[epoch18, step2544]: loss 0.799839
[epoch18, step2545]: loss 0.458171
[epoch18, step2546]: loss 0.332676
[epoch18, step2547]: loss 0.785858
[epoch18, step2548]: loss 0.448321
[epoch18, step2549]: loss 0.684613
[epoch18, step2550]: loss 0.319405
[epoch18, step2551]: loss 0.740255
[epoch18, step2552]: loss 0.637494
[epoch18, step2553]: loss 0.500150
[epoch18, step2554]: loss 0.555123
[epoch18, step2555]: loss 0.539493
[epoch18, step2556]: loss 0.502800
[epoch18, step2557]: loss 0.624560
[epoch18, step2558]: loss 0.312203
[epoch18, step2559]: loss 0.415793
[epoch18, step2560]: loss 0.453977
[epoch18, step2561]: loss 0.619087
[epoch18, step2562]: loss 0.810145
[epoch18, step2563]: loss 0.354472
[epoch18, step2564]: loss 0.459935
[epoch18, step2565]: loss 0.502894
[epoch18, step2566]: loss 0.703917
[epoch18, step2567]: loss 0.564368
[epoch18, step2568]: loss 0.380723
[epoch18, step2569]: loss 0.509288
[epoch18, step2570]: loss 0.398445
[epoch18, step2571]: loss 0.682703
[epoch18, step2572]: loss 0.497001
[epoch18, step2573]: loss 0.376858
[epoch18, step2574]: loss 0.624446
[epoch18, step2575]: loss 0.286350
[epoch18, step2576]: loss 0.394957
[epoch18, step2577]: loss 0.310012
[epoch18, step2578]: loss 0.732341
[epoch18, step2579]: loss 0.502873
[epoch18, step2580]: loss 0.485014
[epoch18, step2581]: loss 0.571459
[epoch18, step2582]: loss 0.350056
[epoch18, step2583]: loss 0.649084
[epoch18, step2584]: loss 0.506333
[epoch18, step2585]: loss 0.643775
[epoch18, step2586]: loss 0.417346
[epoch18, step2587]: loss 0.567376
[epoch18, step2588]: loss 0.530820
[epoch18, step2589]: loss 0.591915
[epoch18, step2590]: loss 0.791244
[epoch18, step2591]: loss 0.403687
[epoch18, step2592]: loss 0.469623
[epoch18, step2593]: loss 0.719436
[epoch18, step2594]: loss 0.541253
[epoch18, step2595]: loss 0.392614
[epoch18, step2596]: loss 0.707713
[epoch18, step2597]: loss 0.406514
[epoch18, step2598]: loss 0.276638
[epoch18, step2599]: loss 0.456835
[epoch18, step2600]: loss 0.620972
[epoch18, step2601]: loss 0.370292
[epoch18, step2602]: loss 0.481184
[epoch18, step2603]: loss 0.520475
[epoch18, step2604]: loss 0.213323
[epoch18, step2605]: loss 0.530051
[epoch18, step2606]: loss 0.350901
[epoch18, step2607]: loss 0.233020
[epoch18, step2608]: loss 0.577301
[epoch18, step2609]: loss 0.191422
[epoch18, step2610]: loss 0.233164
[epoch18, step2611]: loss 0.724293
[epoch18, step2612]: loss 0.508192
[epoch18, step2613]: loss 0.521178
[epoch18, step2614]: loss 0.385757
[epoch18, step2615]: loss 0.654507
[epoch18, step2616]: loss 0.730901
[epoch18, step2617]: loss 0.367472
[epoch18, step2618]: loss 0.696929
[epoch18, step2619]: loss 0.311391
[epoch18, step2620]: loss 0.238131
[epoch18, step2621]: loss 0.764054
[epoch18, step2622]: loss 0.349245
[epoch18, step2623]: loss 0.515144
[epoch18, step2624]: loss 0.361128
[epoch18, step2625]: loss 0.620811
[epoch18, step2626]: loss 0.384438
[epoch18, step2627]: loss 0.677945
[epoch18, step2628]: loss 0.514919
[epoch18, step2629]: loss 0.492054
[epoch18, step2630]: loss 0.575464
[epoch18, step2631]: loss 0.411264
[epoch18, step2632]: loss 0.484756
[epoch18, step2633]: loss 0.677170
[epoch18, step2634]: loss 0.461642
[epoch18, step2635]: loss 0.495138
[epoch18, step2636]: loss 0.479917
[epoch18, step2637]: loss 0.652192
[epoch18, step2638]: loss 0.390396
[epoch18, step2639]: loss 0.466872
[epoch18, step2640]: loss 0.754922
[epoch18, step2641]: loss 0.552941
[epoch18, step2642]: loss 0.394703
[epoch18, step2643]: loss 0.601352
[epoch18, step2644]: loss 0.602491
[epoch18, step2645]: loss 0.449082
[epoch18, step2646]: loss 0.614090
[epoch18, step2647]: loss 0.480819
[epoch18, step2648]: loss 0.634383
[epoch18, step2649]: loss 0.387988
[epoch18, step2650]: loss 0.614243
[epoch18, step2651]: loss 0.415246
[epoch18, step2652]: loss 0.476928
[epoch18, step2653]: loss 0.580253
[epoch18, step2654]: loss 0.672127
[epoch18, step2655]: loss 0.766685
[epoch18, step2656]: loss 0.629634
[epoch18, step2657]: loss 0.602000
[epoch18, step2658]: loss 0.756035
[epoch18, step2659]: loss 0.379505
[epoch18, step2660]: loss 0.652914
[epoch18, step2661]: loss 0.265264
[epoch18, step2662]: loss 0.462832
[epoch18, step2663]: loss 0.510226
[epoch18, step2664]: loss 0.451034
[epoch18, step2665]: loss 0.423645
[epoch18, step2666]: loss 0.549817
[epoch18, step2667]: loss 0.154564
[epoch18, step2668]: loss 0.418989
[epoch18, step2669]: loss 0.320444
[epoch18, step2670]: loss 0.427148
[epoch18, step2671]: loss 0.881380
[epoch18, step2672]: loss 0.423529
[epoch18, step2673]: loss 0.779745
[epoch18, step2674]: loss 0.738268
[epoch18, step2675]: loss 0.374106
[epoch18, step2676]: loss 0.206067
[epoch18, step2677]: loss 0.594579
[epoch18, step2678]: loss 0.557981
[epoch18, step2679]: loss 0.402950
[epoch18, step2680]: loss 0.820929
[epoch18, step2681]: loss 0.677429
[epoch18, step2682]: loss 0.633065
[epoch18, step2683]: loss 0.672050
[epoch18, step2684]: loss 0.496007
[epoch18, step2685]: loss 0.518133
[epoch18, step2686]: loss 0.488089
[epoch18, step2687]: loss 0.514566
[epoch18, step2688]: loss 0.646208
[epoch18, step2689]: loss 0.544096
[epoch18, step2690]: loss 0.651619
[epoch18, step2691]: loss 0.661791
[epoch18, step2692]: loss 0.277066
[epoch18, step2693]: loss 0.493893
[epoch18, step2694]: loss 0.575159
[epoch18, step2695]: loss 0.503118
[epoch18, step2696]: loss 0.369436
[epoch18, step2697]: loss 0.333928
[epoch18, step2698]: loss 0.595937
[epoch18, step2699]: loss 0.749620
[epoch18, step2700]: loss 0.534527
[epoch18, step2701]: loss 0.549289
[epoch18, step2702]: loss 0.345727
[epoch18, step2703]: loss 0.519064
[epoch18, step2704]: loss 0.575460
[epoch18, step2705]: loss 0.256941
[epoch18, step2706]: loss 0.629921
[epoch18, step2707]: loss 0.495397
[epoch18, step2708]: loss 0.560141
[epoch18, step2709]: loss 0.411433
[epoch18, step2710]: loss 0.659520
[epoch18, step2711]: loss 0.590050
[epoch18, step2712]: loss 0.590018
[epoch18, step2713]: loss 0.621642
[epoch18, step2714]: loss 0.616771
[epoch18, step2715]: loss 0.463792
[epoch18, step2716]: loss 0.469140
[epoch18, step2717]: loss 0.343300
[epoch18, step2718]: loss 0.541325
[epoch18, step2719]: loss 0.571099
[epoch18, step2720]: loss 0.713609
[epoch18, step2721]: loss 0.469979
[epoch18, step2722]: loss 0.400736
[epoch18, step2723]: loss 0.322226
[epoch18, step2724]: loss 0.461962
[epoch18, step2725]: loss 0.705316
[epoch18, step2726]: loss 0.437065
[epoch18, step2727]: loss 0.477682
[epoch18, step2728]: loss 0.584528
[epoch18, step2729]: loss 0.267021
[epoch18, step2730]: loss 0.738508
[epoch18, step2731]: loss 0.568597
[epoch18, step2732]: loss 0.460695
[epoch18, step2733]: loss 0.542721
[epoch18, step2734]: loss 0.626177
[epoch18, step2735]: loss 0.593358
[epoch18, step2736]: loss 0.553157
[epoch18, step2737]: loss 0.621143
[epoch18, step2738]: loss 0.345549
[epoch18, step2739]: loss 0.393348
[epoch18, step2740]: loss 0.621762
[epoch18, step2741]: loss 0.520503
[epoch18, step2742]: loss 0.566648
[epoch18, step2743]: loss 0.624663
[epoch18, step2744]: loss 0.270828
[epoch18, step2745]: loss 0.398341
[epoch18, step2746]: loss 0.674164
[epoch18, step2747]: loss 0.545287
[epoch18, step2748]: loss 0.512967
[epoch18, step2749]: loss 0.445495
[epoch18, step2750]: loss 0.509454
[epoch18, step2751]: loss 0.593506
[epoch18, step2752]: loss 0.577194
[epoch18, step2753]: loss 0.393662
[epoch18, step2754]: loss 0.697814
[epoch18, step2755]: loss 0.707979
[epoch18, step2756]: loss 0.583213
[epoch18, step2757]: loss 0.525803
[epoch18, step2758]: loss 0.649371
[epoch18, step2759]: loss 0.613161
[epoch18, step2760]: loss 0.439667
[epoch18, step2761]: loss 0.691779
[epoch18, step2762]: loss 0.179865
[epoch18, step2763]: loss 0.597244
[epoch18, step2764]: loss 0.483196
[epoch18, step2765]: loss 0.503809
[epoch18, step2766]: loss 0.574458
[epoch18, step2767]: loss 0.557074
[epoch18, step2768]: loss 0.503645
[epoch18, step2769]: loss 0.584698
[epoch18, step2770]: loss 0.364618
[epoch18, step2771]: loss 0.543593
[epoch18, step2772]: loss 0.088343
[epoch18, step2773]: loss 0.601223
[epoch18, step2774]: loss 0.328129
[epoch18, step2775]: loss 0.434000
[epoch18, step2776]: loss 0.646988
[epoch18, step2777]: loss 0.609402
[epoch18, step2778]: loss 0.667009
[epoch18, step2779]: loss 0.343664
[epoch18, step2780]: loss 0.573220
[epoch18, step2781]: loss 0.765765
[epoch18, step2782]: loss 0.608587
[epoch18, step2783]: loss 0.543025
[epoch18, step2784]: loss 0.318530
[epoch18, step2785]: loss 0.310148
[epoch18, step2786]: loss 0.421093
[epoch18, step2787]: loss 0.684270
[epoch18, step2788]: loss 0.610398
[epoch18, step2789]: loss 0.558440
[epoch18, step2790]: loss 0.479613
[epoch18, step2791]: loss 0.290831
[epoch18, step2792]: loss 0.400404
[epoch18, step2793]: loss 0.434078
[epoch18, step2794]: loss 0.652667
[epoch18, step2795]: loss 0.483338
[epoch18, step2796]: loss 0.556448
[epoch18, step2797]: loss 0.580020
[epoch18, step2798]: loss 0.793748
[epoch18, step2799]: loss 0.541689
[epoch18, step2800]: loss 0.769375
[epoch18, step2801]: loss 0.587688
[epoch18, step2802]: loss 0.358058
[epoch18, step2803]: loss 0.480724
[epoch18, step2804]: loss 0.596003
[epoch18, step2805]: loss 0.417143
[epoch18, step2806]: loss 0.462708
[epoch18, step2807]: loss 0.519125
[epoch18, step2808]: loss 0.552564
[epoch18, step2809]: loss 0.495503
[epoch18, step2810]: loss 0.476943
[epoch18, step2811]: loss 0.254217
[epoch18, step2812]: loss 0.496302
[epoch18, step2813]: loss 0.397590
[epoch18, step2814]: loss 0.541493
[epoch18, step2815]: loss 0.569474
[epoch18, step2816]: loss 0.524836
[epoch18, step2817]: loss 0.401542
[epoch18, step2818]: loss 0.691043
[epoch18, step2819]: loss 0.669756
[epoch18, step2820]: loss 0.717000
[epoch18, step2821]: loss 0.680506
[epoch18, step2822]: loss 0.457006
[epoch18, step2823]: loss 0.333562
[epoch18, step2824]: loss 0.702612
[epoch18, step2825]: loss 0.634793
[epoch18, step2826]: loss 0.548731
[epoch18, step2827]: loss 0.325490
[epoch18, step2828]: loss 0.531451
[epoch18, step2829]: loss 0.427558
[epoch18, step2830]: loss 0.747431
[epoch18, step2831]: loss 0.503297
[epoch18, step2832]: loss 0.478443
[epoch18, step2833]: loss 0.613085
[epoch18, step2834]: loss 0.305354
[epoch18, step2835]: loss 0.509264
[epoch18, step2836]: loss 0.431189
[epoch18, step2837]: loss 0.572178
[epoch18, step2838]: loss 0.525583
[epoch18, step2839]: loss 0.478294
[epoch18, step2840]: loss 0.508911
[epoch18, step2841]: loss 0.564007
[epoch18, step2842]: loss 0.534413
[epoch18, step2843]: loss 0.568564
[epoch18, step2844]: loss 0.645965
[epoch18, step2845]: loss 0.512496
[epoch18, step2846]: loss 0.541067
[epoch18, step2847]: loss 0.407264
[epoch18, step2848]: loss 0.580938
[epoch18, step2849]: loss 0.476595
[epoch18, step2850]: loss 0.475169
[epoch18, step2851]: loss 0.638661
[epoch18, step2852]: loss 0.403580
[epoch18, step2853]: loss 0.582386
[epoch18, step2854]: loss 0.429900
[epoch18, step2855]: loss 0.695417
[epoch18, step2856]: loss 0.462206
[epoch18, step2857]: loss 0.608612
[epoch18, step2858]: loss 0.716720
[epoch18, step2859]: loss 0.501599
[epoch18, step2860]: loss 0.590121
[epoch18, step2861]: loss 0.286488
[epoch18, step2862]: loss 0.576975
[epoch18, step2863]: loss 0.600580
[epoch18, step2864]: loss 0.614670
[epoch18, step2865]: loss 0.619248
[epoch18, step2866]: loss 0.582004
[epoch18, step2867]: loss 0.471770
[epoch18, step2868]: loss 0.528760
[epoch18, step2869]: loss 0.413552
[epoch18, step2870]: loss 0.606287
[epoch18, step2871]: loss 0.494293
[epoch18, step2872]: loss 0.582909
[epoch18, step2873]: loss 0.846546
[epoch18, step2874]: loss 0.871370
[epoch18, step2875]: loss 0.608198
[epoch18, step2876]: loss 0.535772
[epoch18, step2877]: loss 0.727460
[epoch18, step2878]: loss 0.130155
[epoch18, step2879]: loss 0.390612
[epoch18, step2880]: loss 0.315492
[epoch18, step2881]: loss 0.637832
[epoch18, step2882]: loss 0.421809
[epoch18, step2883]: loss 0.477248
[epoch18, step2884]: loss 0.531883
[epoch18, step2885]: loss 0.383952
[epoch18, step2886]: loss 0.676405
[epoch18, step2887]: loss 0.621132
[epoch18, step2888]: loss 0.498406
[epoch18, step2889]: loss 0.539811
[epoch18, step2890]: loss 0.465500
[epoch18, step2891]: loss 0.294709
[epoch18, step2892]: loss 0.480362
[epoch18, step2893]: loss 0.663216
[epoch18, step2894]: loss 0.643640
[epoch18, step2895]: loss 0.600576
[epoch18, step2896]: loss 0.600389
[epoch18, step2897]: loss 0.575710
[epoch18, step2898]: loss 0.386524
[epoch18, step2899]: loss 0.274671
[epoch18, step2900]: loss 0.304700
[epoch18, step2901]: loss 0.633322
[epoch18, step2902]: loss 0.480768
[epoch18, step2903]: loss 0.593339
[epoch18, step2904]: loss 0.370342
[epoch18, step2905]: loss 0.658389
[epoch18, step2906]: loss 0.302937
[epoch18, step2907]: loss 0.490792
[epoch18, step2908]: loss 0.549962
[epoch18, step2909]: loss 0.751954
[epoch18, step2910]: loss 0.439098
[epoch18, step2911]: loss 0.372001
[epoch18, step2912]: loss 0.587588
[epoch18, step2913]: loss 0.625018
[epoch18, step2914]: loss 0.596300
[epoch18, step2915]: loss 0.503792
[epoch18, step2916]: loss 0.387414
[epoch18, step2917]: loss 0.484390
[epoch18, step2918]: loss 0.517701
[epoch18, step2919]: loss 0.746006
[epoch18, step2920]: loss 0.423033
[epoch18, step2921]: loss 0.466631
[epoch18, step2922]: loss 0.599452
[epoch18, step2923]: loss 0.564970
[epoch18, step2924]: loss 0.508931
[epoch18, step2925]: loss 0.535216
[epoch18, step2926]: loss 0.286929
[epoch18, step2927]: loss 0.297118
[epoch18, step2928]: loss 0.539761
[epoch18, step2929]: loss 0.572289
[epoch18, step2930]: loss 0.476993
[epoch18, step2931]: loss 0.486786
[epoch18, step2932]: loss 0.584516
[epoch18, step2933]: loss 0.624590
[epoch18, step2934]: loss 0.626439
[epoch18, step2935]: loss 0.473190
[epoch18, step2936]: loss 0.458061
[epoch18, step2937]: loss 0.657422
[epoch18, step2938]: loss 0.536728
[epoch18, step2939]: loss 0.610752
[epoch18, step2940]: loss 0.598537
[epoch18, step2941]: loss 0.692887
[epoch18, step2942]: loss 0.555354
[epoch18, step2943]: loss 0.479918
[epoch18, step2944]: loss 0.655389
[epoch18, step2945]: loss 0.418050
[epoch18, step2946]: loss 0.660263
[epoch18, step2947]: loss 0.657357
[epoch18, step2948]: loss 0.745768
[epoch18, step2949]: loss 0.778479
[epoch18, step2950]: loss 0.783560
[epoch18, step2951]: loss 0.591287
[epoch18, step2952]: loss 0.585313
[epoch18, step2953]: loss 0.294455
[epoch18, step2954]: loss 0.562204
[epoch18, step2955]: loss 0.194431
[epoch18, step2956]: loss 0.622833
[epoch18, step2957]: loss 0.680993
[epoch18, step2958]: loss 0.610527
[epoch18, step2959]: loss 0.335012
[epoch18, step2960]: loss 0.615625
[epoch18, step2961]: loss 0.425453
[epoch18, step2962]: loss 0.613657
[epoch18, step2963]: loss 0.450403
[epoch18, step2964]: loss 0.622513
[epoch18, step2965]: loss 0.730391
[epoch18, step2966]: loss 0.613137
[epoch18, step2967]: loss 0.378541
[epoch18, step2968]: loss 0.437023
[epoch18, step2969]: loss 0.484753
[epoch18, step2970]: loss 0.566284
[epoch18, step2971]: loss 0.486023
[epoch18, step2972]: loss 0.401378
[epoch18, step2973]: loss 0.631862
[epoch18, step2974]: loss 0.579665
[epoch18, step2975]: loss 0.403091
[epoch18, step2976]: loss 0.734562
[epoch18, step2977]: loss 0.495722
[epoch18, step2978]: loss 0.296313
[epoch18, step2979]: loss 0.522385
[epoch18, step2980]: loss 0.489222
[epoch18, step2981]: loss 0.824812
[epoch18, step2982]: loss 0.641262
[epoch18, step2983]: loss 0.521534
[epoch18, step2984]: loss 0.748413
[epoch18, step2985]: loss 0.530362
[epoch18, step2986]: loss 0.271387
[epoch18, step2987]: loss 0.441437
[epoch18, step2988]: loss 0.405936
[epoch18, step2989]: loss 0.768810
[epoch18, step2990]: loss 0.378123
[epoch18, step2991]: loss 0.137615
[epoch18, step2992]: loss 0.395610
[epoch18, step2993]: loss 0.447322
[epoch18, step2994]: loss 0.539934
[epoch18, step2995]: loss 0.541653
[epoch18, step2996]: loss 0.549293
[epoch18, step2997]: loss 0.558907
[epoch18, step2998]: loss 0.274921
[epoch18, step2999]: loss 0.480992
[epoch18, step3000]: loss 0.734155
[epoch18, step3001]: loss 0.500283
[epoch18, step3002]: loss 0.566472
[epoch18, step3003]: loss 0.594145
[epoch18, step3004]: loss 0.679306
[epoch18, step3005]: loss 0.647132
[epoch18, step3006]: loss 0.592085
[epoch18, step3007]: loss 0.495366
[epoch18, step3008]: loss 0.458115
[epoch18, step3009]: loss 0.456076
[epoch18, step3010]: loss 0.530642
[epoch18, step3011]: loss 0.717426
[epoch18, step3012]: loss 0.724185
[epoch18, step3013]: loss 0.576865
[epoch18, step3014]: loss 0.717856
[epoch18, step3015]: loss 0.479566
[epoch18, step3016]: loss 0.327528
[epoch18, step3017]: loss 0.336169
[epoch18, step3018]: loss 0.598865
[epoch18, step3019]: loss 0.602051
[epoch18, step3020]: loss 0.439710
[epoch18, step3021]: loss 0.714961
[epoch18, step3022]: loss 0.368700
[epoch18, step3023]: loss 0.792489
[epoch18, step3024]: loss 0.381576
[epoch18, step3025]: loss 0.619710
[epoch18, step3026]: loss 0.551148
[epoch18, step3027]: loss 0.413344
[epoch18, step3028]: loss 0.445796
[epoch18, step3029]: loss 0.490841
[epoch18, step3030]: loss 0.392051
[epoch18, step3031]: loss 0.637974
[epoch18, step3032]: loss 0.463880
[epoch18, step3033]: loss 0.476765
[epoch18, step3034]: loss 0.578379
[epoch18, step3035]: loss 0.644912
[epoch18, step3036]: loss 0.621197
[epoch18, step3037]: loss 0.346690
[epoch18, step3038]: loss 0.427153
[epoch18, step3039]: loss 0.526353
[epoch18, step3040]: loss 0.444100
[epoch18, step3041]: loss 0.794156
[epoch18, step3042]: loss 0.565749
[epoch18, step3043]: loss 0.556372
[epoch18, step3044]: loss 0.545235
[epoch18, step3045]: loss 0.589875
[epoch18, step3046]: loss 0.425259
[epoch18, step3047]: loss 0.478018
[epoch18, step3048]: loss 0.529446
[epoch18, step3049]: loss 0.651211
[epoch18, step3050]: loss 0.585900
[epoch18, step3051]: loss 0.466662
[epoch18, step3052]: loss 0.495064
[epoch18, step3053]: loss 0.426508
[epoch18, step3054]: loss 0.593451
[epoch18, step3055]: loss 0.780201
[epoch18, step3056]: loss 0.190993
[epoch18, step3057]: loss 0.399207
[epoch18, step3058]: loss 0.597385
[epoch18, step3059]: loss 0.486661
[epoch18, step3060]: loss 0.543606
[epoch18, step3061]: loss 0.638724
[epoch18, step3062]: loss 0.298055
[epoch18, step3063]: loss 0.431072
[epoch18, step3064]: loss 0.736647
[epoch18, step3065]: loss 0.837328
[epoch18, step3066]: loss 0.401833
[epoch18, step3067]: loss 0.725060
[epoch18, step3068]: loss 0.531907
[epoch18, step3069]: loss 0.512961
[epoch18, step3070]: loss 0.486134
[epoch18, step3071]: loss 0.608715
[epoch18, step3072]: loss 0.594812
[epoch18, step3073]: loss 0.615089
[epoch18, step3074]: loss 0.776382
[epoch18, step3075]: loss 0.364227
[epoch18, step3076]: loss 0.390067

[epoch18]: avg loss 0.390067

[epoch19, step1]: loss 0.635824
[epoch19, step2]: loss 0.677064
[epoch19, step3]: loss 0.514544
[epoch19, step4]: loss 0.410883
[epoch19, step5]: loss 0.589396
[epoch19, step6]: loss 0.520564
[epoch19, step7]: loss 0.631893
[epoch19, step8]: loss 0.395845
[epoch19, step9]: loss 0.568323
[epoch19, step10]: loss 0.398714
[epoch19, step11]: loss 0.712476
[epoch19, step12]: loss 0.573971
[epoch19, step13]: loss 0.530351
[epoch19, step14]: loss 0.564390
[epoch19, step15]: loss 0.644015
[epoch19, step16]: loss 0.563328
[epoch19, step17]: loss 0.373500
[epoch19, step18]: loss 0.337697
[epoch19, step19]: loss 0.551653
[epoch19, step20]: loss 0.345790
[epoch19, step21]: loss 0.774855
[epoch19, step22]: loss 0.404307
[epoch19, step23]: loss 0.667135
[epoch19, step24]: loss 0.576885
[epoch19, step25]: loss 0.458880
[epoch19, step26]: loss 0.566859
[epoch19, step27]: loss 0.546436
[epoch19, step28]: loss 0.559424
[epoch19, step29]: loss 0.405776
[epoch19, step30]: loss 0.507944
[epoch19, step31]: loss 0.549690
[epoch19, step32]: loss 0.588124
[epoch19, step33]: loss 0.595558
[epoch19, step34]: loss 0.488446
[epoch19, step35]: loss 0.518754
[epoch19, step36]: loss 0.580138
[epoch19, step37]: loss 0.428820
[epoch19, step38]: loss 0.635904
[epoch19, step39]: loss 0.426846
[epoch19, step40]: loss 0.627108
[epoch19, step41]: loss 0.678841
[epoch19, step42]: loss 0.417447
[epoch19, step43]: loss 0.433623
[epoch19, step44]: loss 0.587414
[epoch19, step45]: loss 0.402149
[epoch19, step46]: loss 0.525179
[epoch19, step47]: loss 0.359701
[epoch19, step48]: loss 0.630142
[epoch19, step49]: loss 0.533307
[epoch19, step50]: loss 0.661726
[epoch19, step51]: loss 0.363456
[epoch19, step52]: loss 0.484157
[epoch19, step53]: loss 0.451389
[epoch19, step54]: loss 0.648384
[epoch19, step55]: loss 0.679523
[epoch19, step56]: loss 0.626461
[epoch19, step57]: loss 0.523937
[epoch19, step58]: loss 0.248339
[epoch19, step59]: loss 0.529683
[epoch19, step60]: loss 0.440076
[epoch19, step61]: loss 0.326778
[epoch19, step62]: loss 0.723638
[epoch19, step63]: loss 0.563818
[epoch19, step64]: loss 0.750942
[epoch19, step65]: loss 0.389112
[epoch19, step66]: loss 0.520434
[epoch19, step67]: loss 0.618253
[epoch19, step68]: loss 0.704310
[epoch19, step69]: loss 0.616096
[epoch19, step70]: loss 0.519517
[epoch19, step71]: loss 0.675560
[epoch19, step72]: loss 0.332030
[epoch19, step73]: loss 0.384432
[epoch19, step74]: loss 0.663914
[epoch19, step75]: loss 0.486224
[epoch19, step76]: loss 0.627565
[epoch19, step77]: loss 0.667849
[epoch19, step78]: loss 0.795711
[epoch19, step79]: loss 0.485448
[epoch19, step80]: loss 0.628180
[epoch19, step81]: loss 0.676205
[epoch19, step82]: loss 0.620094
[epoch19, step83]: loss 0.701364
[epoch19, step84]: loss 0.379992
[epoch19, step85]: loss 0.412473
[epoch19, step86]: loss 0.591076
[epoch19, step87]: loss 0.616335
[epoch19, step88]: loss 0.612390
[epoch19, step89]: loss 0.632227
[epoch19, step90]: loss 0.603308
[epoch19, step91]: loss 0.546154
[epoch19, step92]: loss 0.541949
[epoch19, step93]: loss 0.379869
[epoch19, step94]: loss 0.555052
[epoch19, step95]: loss 0.502358
[epoch19, step96]: loss 0.652483
[epoch19, step97]: loss 0.530346
[epoch19, step98]: loss 0.358315
[epoch19, step99]: loss 0.706301
[epoch19, step100]: loss 0.551747
[epoch19, step101]: loss 0.605073
[epoch19, step102]: loss 0.395832
[epoch19, step103]: loss 0.507489
[epoch19, step104]: loss 0.477106
[epoch19, step105]: loss 0.503339
[epoch19, step106]: loss 0.709171
[epoch19, step107]: loss 0.477833
[epoch19, step108]: loss 0.466069
[epoch19, step109]: loss 0.528174
[epoch19, step110]: loss 0.252563
[epoch19, step111]: loss 0.559757
[epoch19, step112]: loss 0.596508
[epoch19, step113]: loss 0.463709
[epoch19, step114]: loss 0.377299
[epoch19, step115]: loss 0.437355
[epoch19, step116]: loss 0.771738
[epoch19, step117]: loss 0.353405
[epoch19, step118]: loss 0.361177
[epoch19, step119]: loss 0.657981
[epoch19, step120]: loss 0.748358
[epoch19, step121]: loss 0.506867
[epoch19, step122]: loss 0.300881
[epoch19, step123]: loss 0.400390
[epoch19, step124]: loss 0.746725
[epoch19, step125]: loss 0.637642
[epoch19, step126]: loss 0.647659
[epoch19, step127]: loss 0.837583
[epoch19, step128]: loss 0.544300
[epoch19, step129]: loss 0.286055
[epoch19, step130]: loss 0.699757
[epoch19, step131]: loss 0.417987
[epoch19, step132]: loss 0.507784
[epoch19, step133]: loss 0.522042
[epoch19, step134]: loss 0.575500
[epoch19, step135]: loss 0.326633
[epoch19, step136]: loss 0.544844
[epoch19, step137]: loss 0.729918
[epoch19, step138]: loss 0.431747
[epoch19, step139]: loss 0.549330
[epoch19, step140]: loss 0.734779
[epoch19, step141]: loss 0.698749
[epoch19, step142]: loss 0.412173
[epoch19, step143]: loss 0.476795
[epoch19, step144]: loss 0.619286
[epoch19, step145]: loss 0.653064
[epoch19, step146]: loss 0.821960
[epoch19, step147]: loss 0.789959
[epoch19, step148]: loss 0.493812
[epoch19, step149]: loss 0.425014
[epoch19, step150]: loss 0.652621
[epoch19, step151]: loss 0.356313
[epoch19, step152]: loss 0.594951
[epoch19, step153]: loss 0.406930
[epoch19, step154]: loss 0.631874
[epoch19, step155]: loss 0.543873
[epoch19, step156]: loss 0.559424
[epoch19, step157]: loss 0.499541
[epoch19, step158]: loss 0.596890
[epoch19, step159]: loss 0.476014
[epoch19, step160]: loss 0.391561
[epoch19, step161]: loss 0.630760
[epoch19, step162]: loss 0.555570
[epoch19, step163]: loss 0.505530
[epoch19, step164]: loss 0.370519
[epoch19, step165]: loss 0.619597
[epoch19, step166]: loss 0.406973
[epoch19, step167]: loss 0.450613
[epoch19, step168]: loss 0.461871
[epoch19, step169]: loss 0.163690
[epoch19, step170]: loss 0.522188
[epoch19, step171]: loss 0.721053
[epoch19, step172]: loss 0.514160
[epoch19, step173]: loss 0.592274
[epoch19, step174]: loss 0.620078
[epoch19, step175]: loss 0.595342
[epoch19, step176]: loss 0.575553
[epoch19, step177]: loss 0.362383
[epoch19, step178]: loss 0.705634
[epoch19, step179]: loss 0.611085
[epoch19, step180]: loss 0.573290
[epoch19, step181]: loss 0.484081
[epoch19, step182]: loss 0.621911
[epoch19, step183]: loss 0.697977
[epoch19, step184]: loss 0.432992
[epoch19, step185]: loss 0.616671
[epoch19, step186]: loss 0.521738
[epoch19, step187]: loss 0.630400
[epoch19, step188]: loss 0.468919
[epoch19, step189]: loss 0.505962
[epoch19, step190]: loss 0.510335
[epoch19, step191]: loss 0.476147
[epoch19, step192]: loss 0.548612
[epoch19, step193]: loss 0.674673
[epoch19, step194]: loss 0.272166
[epoch19, step195]: loss 0.608156
[epoch19, step196]: loss 0.473034
[epoch19, step197]: loss 0.493135
[epoch19, step198]: loss 0.602020
[epoch19, step199]: loss 0.519024
[epoch19, step200]: loss 0.496765
[epoch19, step201]: loss 0.685359
[epoch19, step202]: loss 0.373421
[epoch19, step203]: loss 0.811249
[epoch19, step204]: loss 0.599870
[epoch19, step205]: loss 0.551648
[epoch19, step206]: loss 0.445506
[epoch19, step207]: loss 0.577536
[epoch19, step208]: loss 0.473709
[epoch19, step209]: loss 0.617642
[epoch19, step210]: loss 0.249201
[epoch19, step211]: loss 0.740504
[epoch19, step212]: loss 0.583863
[epoch19, step213]: loss 0.588587
[epoch19, step214]: loss 0.440518
[epoch19, step215]: loss 0.289317
[epoch19, step216]: loss 0.408383
[epoch19, step217]: loss 0.429143
[epoch19, step218]: loss 0.299922
[epoch19, step219]: loss 0.669087
[epoch19, step220]: loss 0.703950
[epoch19, step221]: loss 0.718627
[epoch19, step222]: loss 0.605850
[epoch19, step223]: loss 0.497161
[epoch19, step224]: loss 0.623508
[epoch19, step225]: loss 0.525281
[epoch19, step226]: loss 0.616112
[epoch19, step227]: loss 0.568213
[epoch19, step228]: loss 0.754685
[epoch19, step229]: loss 0.540613
[epoch19, step230]: loss 0.412789
[epoch19, step231]: loss 0.568526
[epoch19, step232]: loss 0.525006
[epoch19, step233]: loss 0.628678
[epoch19, step234]: loss 0.471819
[epoch19, step235]: loss 0.245679
[epoch19, step236]: loss 0.639162
[epoch19, step237]: loss 0.318711
[epoch19, step238]: loss 0.480071
[epoch19, step239]: loss 0.811173
[epoch19, step240]: loss 0.258523
[epoch19, step241]: loss 0.455046
[epoch19, step242]: loss 0.486892
[epoch19, step243]: loss 0.649830
[epoch19, step244]: loss 0.514231
[epoch19, step245]: loss 0.476007
[epoch19, step246]: loss 0.754786
[epoch19, step247]: loss 0.330675
[epoch19, step248]: loss 0.166868
[epoch19, step249]: loss 0.613321
[epoch19, step250]: loss 0.531296
[epoch19, step251]: loss 0.671245
[epoch19, step252]: loss 0.468181
[epoch19, step253]: loss 0.392260
[epoch19, step254]: loss 0.587413
[epoch19, step255]: loss 0.562151
[epoch19, step256]: loss 0.492490
[epoch19, step257]: loss 0.747396
[epoch19, step258]: loss 0.527795
[epoch19, step259]: loss 0.666361
[epoch19, step260]: loss 0.452865
[epoch19, step261]: loss 0.458702
[epoch19, step262]: loss 0.632264
[epoch19, step263]: loss 0.336531
[epoch19, step264]: loss 0.619681
[epoch19, step265]: loss 0.568790
[epoch19, step266]: loss 0.586908
[epoch19, step267]: loss 0.727471
[epoch19, step268]: loss 0.434796
[epoch19, step269]: loss 0.769568
[epoch19, step270]: loss 0.655344
[epoch19, step271]: loss 0.385634
[epoch19, step272]: loss 0.393791
[epoch19, step273]: loss 0.639504
[epoch19, step274]: loss 0.782560
[epoch19, step275]: loss 0.441590
[epoch19, step276]: loss 0.617878
[epoch19, step277]: loss 0.560766
[epoch19, step278]: loss 0.626194
[epoch19, step279]: loss 0.773298
[epoch19, step280]: loss 0.696807
[epoch19, step281]: loss 0.279665
[epoch19, step282]: loss 0.444343
[epoch19, step283]: loss 0.374938
[epoch19, step284]: loss 0.446575
[epoch19, step285]: loss 0.383455
[epoch19, step286]: loss 0.363868
[epoch19, step287]: loss 0.620656
[epoch19, step288]: loss 0.594302
[epoch19, step289]: loss 0.759837
[epoch19, step290]: loss 0.458504
[epoch19, step291]: loss 0.541295
[epoch19, step292]: loss 0.721292
[epoch19, step293]: loss 0.621373
[epoch19, step294]: loss 0.650875
[epoch19, step295]: loss 0.665552
[epoch19, step296]: loss 0.334785
[epoch19, step297]: loss 0.593352
[epoch19, step298]: loss 0.575054
[epoch19, step299]: loss 0.469027
[epoch19, step300]: loss 0.516506
[epoch19, step301]: loss 0.388439
[epoch19, step302]: loss 0.571374
[epoch19, step303]: loss 0.433975
[epoch19, step304]: loss 0.360920
[epoch19, step305]: loss 0.601428
[epoch19, step306]: loss 0.322436
[epoch19, step307]: loss 0.620760
[epoch19, step308]: loss 0.807180
[epoch19, step309]: loss 0.742864
[epoch19, step310]: loss 0.480873
[epoch19, step311]: loss 0.750665
[epoch19, step312]: loss 0.465299
[epoch19, step313]: loss 0.639677
[epoch19, step314]: loss 0.615555
[epoch19, step315]: loss 0.611455
[epoch19, step316]: loss 0.459976
[epoch19, step317]: loss 0.453508
[epoch19, step318]: loss 0.237680
[epoch19, step319]: loss 0.646552
[epoch19, step320]: loss 0.397431
[epoch19, step321]: loss 0.555109
[epoch19, step322]: loss 0.747024
[epoch19, step323]: loss 0.478851
[epoch19, step324]: loss 0.395041
[epoch19, step325]: loss 0.607625
[epoch19, step326]: loss 0.551965
[epoch19, step327]: loss 0.664021
[epoch19, step328]: loss 0.355991
[epoch19, step329]: loss 0.458521
[epoch19, step330]: loss 0.410395
[epoch19, step331]: loss 0.380642
[epoch19, step332]: loss 0.669297
[epoch19, step333]: loss 0.405103
[epoch19, step334]: loss 0.480341
[epoch19, step335]: loss 0.747908
[epoch19, step336]: loss 0.705007
[epoch19, step337]: loss 0.521057
[epoch19, step338]: loss 0.586926
[epoch19, step339]: loss 0.525313
[epoch19, step340]: loss 0.593840
[epoch19, step341]: loss 0.555488
[epoch19, step342]: loss 0.593539
[epoch19, step343]: loss 0.521617
[epoch19, step344]: loss 0.538402
[epoch19, step345]: loss 0.674051
[epoch19, step346]: loss 0.560057
[epoch19, step347]: loss 0.337444
[epoch19, step348]: loss 0.603031
[epoch19, step349]: loss 0.606307
[epoch19, step350]: loss 0.319579
[epoch19, step351]: loss 0.540507
[epoch19, step352]: loss 0.510801
[epoch19, step353]: loss 0.438106
[epoch19, step354]: loss 0.687258
[epoch19, step355]: loss 0.484788
[epoch19, step356]: loss 0.484043
[epoch19, step357]: loss 0.664160
[epoch19, step358]: loss 0.544663
[epoch19, step359]: loss 0.513437
[epoch19, step360]: loss 0.569780
[epoch19, step361]: loss 0.526639
[epoch19, step362]: loss 0.448997
[epoch19, step363]: loss 0.644532
[epoch19, step364]: loss 0.573804
[epoch19, step365]: loss 0.652706
[epoch19, step366]: loss 0.463331
[epoch19, step367]: loss 0.652761
[epoch19, step368]: loss 0.475294
[epoch19, step369]: loss 0.565279
[epoch19, step370]: loss 0.576461
[epoch19, step371]: loss 0.741839
[epoch19, step372]: loss 0.521328
[epoch19, step373]: loss 0.478438
[epoch19, step374]: loss 0.565619
[epoch19, step375]: loss 0.443288
[epoch19, step376]: loss 0.672587
[epoch19, step377]: loss 0.480186
[epoch19, step378]: loss 0.163055
[epoch19, step379]: loss 0.387585
[epoch19, step380]: loss 0.719965
[epoch19, step381]: loss 0.368296
[epoch19, step382]: loss 0.422280
[epoch19, step383]: loss 0.454468
[epoch19, step384]: loss 0.165296
[epoch19, step385]: loss 0.350022
[epoch19, step386]: loss 0.554396
[epoch19, step387]: loss 0.744051
[epoch19, step388]: loss 0.488024
[epoch19, step389]: loss 0.548504
[epoch19, step390]: loss 0.414483
[epoch19, step391]: loss 0.775281
[epoch19, step392]: loss 0.879219
[epoch19, step393]: loss 0.596759
[epoch19, step394]: loss 0.621875
[epoch19, step395]: loss 0.632405
[epoch19, step396]: loss 0.745967
[epoch19, step397]: loss 0.399383
[epoch19, step398]: loss 0.736800
[epoch19, step399]: loss 0.624002
[epoch19, step400]: loss 0.662928
[epoch19, step401]: loss 0.486031
[epoch19, step402]: loss 0.447155
[epoch19, step403]: loss 0.432776
[epoch19, step404]: loss 0.323946
[epoch19, step405]: loss 0.532612
[epoch19, step406]: loss 0.570716
[epoch19, step407]: loss 0.682353
[epoch19, step408]: loss 0.538983
[epoch19, step409]: loss 0.658896
[epoch19, step410]: loss 0.483234
[epoch19, step411]: loss 0.569199
[epoch19, step412]: loss 0.337350
[epoch19, step413]: loss 0.531127
[epoch19, step414]: loss 0.680717
[epoch19, step415]: loss 0.856484
[epoch19, step416]: loss 0.663345
[epoch19, step417]: loss 0.417612
[epoch19, step418]: loss 0.401466
[epoch19, step419]: loss 0.648528
[epoch19, step420]: loss 0.444643
[epoch19, step421]: loss 0.401465
[epoch19, step422]: loss 0.652628
[epoch19, step423]: loss 0.311480
[epoch19, step424]: loss 0.511695
[epoch19, step425]: loss 0.498238
[epoch19, step426]: loss 0.584753
[epoch19, step427]: loss 0.549250
[epoch19, step428]: loss 0.552048
[epoch19, step429]: loss 0.389678
[epoch19, step430]: loss 0.569714
[epoch19, step431]: loss 0.318216
[epoch19, step432]: loss 0.447037
[epoch19, step433]: loss 0.634898
[epoch19, step434]: loss 0.786241
[epoch19, step435]: loss 0.473354
[epoch19, step436]: loss 0.394452
[epoch19, step437]: loss 0.493627
[epoch19, step438]: loss 0.494619
[epoch19, step439]: loss 0.585261
[epoch19, step440]: loss 0.500715
[epoch19, step441]: loss 0.610512
[epoch19, step442]: loss 0.373611
[epoch19, step443]: loss 0.457661
[epoch19, step444]: loss 0.637220
[epoch19, step445]: loss 0.839084
[epoch19, step446]: loss 0.399853
[epoch19, step447]: loss 0.681162
[epoch19, step448]: loss 0.567410
[epoch19, step449]: loss 0.445063
[epoch19, step450]: loss 0.596062
[epoch19, step451]: loss 0.460331
[epoch19, step452]: loss 0.423416
[epoch19, step453]: loss 0.497072
[epoch19, step454]: loss 0.412629
[epoch19, step455]: loss 0.286389
[epoch19, step456]: loss 0.407034
[epoch19, step457]: loss 0.753729
[epoch19, step458]: loss 0.456350
[epoch19, step459]: loss 0.515505
[epoch19, step460]: loss 0.307677
[epoch19, step461]: loss 0.476858
[epoch19, step462]: loss 0.314746
[epoch19, step463]: loss 0.484262
[epoch19, step464]: loss 0.530566
[epoch19, step465]: loss 0.601201
[epoch19, step466]: loss 0.540008
[epoch19, step467]: loss 0.202692
[epoch19, step468]: loss 0.541619
[epoch19, step469]: loss 0.677993
[epoch19, step470]: loss 0.761747
[epoch19, step471]: loss 0.536399
[epoch19, step472]: loss 0.471393
[epoch19, step473]: loss 0.491939
[epoch19, step474]: loss 0.319937
[epoch19, step475]: loss 0.542389
[epoch19, step476]: loss 0.715464
[epoch19, step477]: loss 0.187089
[epoch19, step478]: loss 0.555974
[epoch19, step479]: loss 0.439660
[epoch19, step480]: loss 0.440602
[epoch19, step481]: loss 0.484322
[epoch19, step482]: loss 0.471190
[epoch19, step483]: loss 0.494032
[epoch19, step484]: loss 0.406861
[epoch19, step485]: loss 0.590606
[epoch19, step486]: loss 0.607135
[epoch19, step487]: loss 0.576075
[epoch19, step488]: loss 0.557396
[epoch19, step489]: loss 0.541247
[epoch19, step490]: loss 0.563222
[epoch19, step491]: loss 0.703212
[epoch19, step492]: loss 0.860963
[epoch19, step493]: loss 0.528314
[epoch19, step494]: loss 0.444588
[epoch19, step495]: loss 0.484349
[epoch19, step496]: loss 0.145400
[epoch19, step497]: loss 0.558197
[epoch19, step498]: loss 0.466459
[epoch19, step499]: loss 0.374476
[epoch19, step500]: loss 0.311841
[epoch19, step501]: loss 0.578223
[epoch19, step502]: loss 0.630667
[epoch19, step503]: loss 0.612061
[epoch19, step504]: loss 0.522716
[epoch19, step505]: loss 0.383752
[epoch19, step506]: loss 0.508804
[epoch19, step507]: loss 0.531438
[epoch19, step508]: loss 0.511513
[epoch19, step509]: loss 0.500039
[epoch19, step510]: loss 0.639347
[epoch19, step511]: loss 0.661584
[epoch19, step512]: loss 0.533666
[epoch19, step513]: loss 0.399338
[epoch19, step514]: loss 0.232076
[epoch19, step515]: loss 0.412128
[epoch19, step516]: loss 0.605628
[epoch19, step517]: loss 0.385131
[epoch19, step518]: loss 0.647968
[epoch19, step519]: loss 0.724414
[epoch19, step520]: loss 0.511076
[epoch19, step521]: loss 0.413594
[epoch19, step522]: loss 0.710602
[epoch19, step523]: loss 0.445912
[epoch19, step524]: loss 0.583385
[epoch19, step525]: loss 0.648488
[epoch19, step526]: loss 0.600056
[epoch19, step527]: loss 0.704536
[epoch19, step528]: loss 0.556697
[epoch19, step529]: loss 0.499705
[epoch19, step530]: loss 0.360453
[epoch19, step531]: loss 0.553331
[epoch19, step532]: loss 0.564772
[epoch19, step533]: loss 0.271678
[epoch19, step534]: loss 0.444956
[epoch19, step535]: loss 0.563648
[epoch19, step536]: loss 0.647112
[epoch19, step537]: loss 0.497842
[epoch19, step538]: loss 0.500764
[epoch19, step539]: loss 0.494808
[epoch19, step540]: loss 0.203737
[epoch19, step541]: loss 0.673588
[epoch19, step542]: loss 0.676357
[epoch19, step543]: loss 0.576422
[epoch19, step544]: loss 0.683631
[epoch19, step545]: loss 0.270885
[epoch19, step546]: loss 0.550574
[epoch19, step547]: loss 0.519056
[epoch19, step548]: loss 0.631855
[epoch19, step549]: loss 0.272450
[epoch19, step550]: loss 0.478643
[epoch19, step551]: loss 0.421102
[epoch19, step552]: loss 0.572564
[epoch19, step553]: loss 0.654690
[epoch19, step554]: loss 0.635365
[epoch19, step555]: loss 0.495392
[epoch19, step556]: loss 0.613630
[epoch19, step557]: loss 0.267528
[epoch19, step558]: loss 0.324050
[epoch19, step559]: loss 0.606666
[epoch19, step560]: loss 0.497240
[epoch19, step561]: loss 0.634156
[epoch19, step562]: loss 0.541054
[epoch19, step563]: loss 0.382938
[epoch19, step564]: loss 0.757688
[epoch19, step565]: loss 0.406708
[epoch19, step566]: loss 0.614428
[epoch19, step567]: loss 0.590976
[epoch19, step568]: loss 0.605762
[epoch19, step569]: loss 0.626700
[epoch19, step570]: loss 0.630970
[epoch19, step571]: loss 0.567949
[epoch19, step572]: loss 0.563237
[epoch19, step573]: loss 0.605811
[epoch19, step574]: loss 0.177236
[epoch19, step575]: loss 0.720247
[epoch19, step576]: loss 0.628553
[epoch19, step577]: loss 0.434537
[epoch19, step578]: loss 0.529843
[epoch19, step579]: loss 0.608463
[epoch19, step580]: loss 0.494879
[epoch19, step581]: loss 0.542479
[epoch19, step582]: loss 0.643218
[epoch19, step583]: loss 0.587623
[epoch19, step584]: loss 0.484861
[epoch19, step585]: loss 0.628088
[epoch19, step586]: loss 0.400189
[epoch19, step587]: loss 0.371169
[epoch19, step588]: loss 0.543935
[epoch19, step589]: loss 0.311689
[epoch19, step590]: loss 0.363963
[epoch19, step591]: loss 0.382648
[epoch19, step592]: loss 0.564159
[epoch19, step593]: loss 0.454279
[epoch19, step594]: loss 0.754199
[epoch19, step595]: loss 0.635855
[epoch19, step596]: loss 0.481536
[epoch19, step597]: loss 0.451684
[epoch19, step598]: loss 0.598668
[epoch19, step599]: loss 0.552797
[epoch19, step600]: loss 0.534584
[epoch19, step601]: loss 0.554947
[epoch19, step602]: loss 0.272549
[epoch19, step603]: loss 0.522944
[epoch19, step604]: loss 0.530824
[epoch19, step605]: loss 0.550932
[epoch19, step606]: loss 0.601762
[epoch19, step607]: loss 0.730647
[epoch19, step608]: loss 0.460942
[epoch19, step609]: loss 0.557582
[epoch19, step610]: loss 0.821076
[epoch19, step611]: loss 0.426241
[epoch19, step612]: loss 0.512153
[epoch19, step613]: loss 0.688713
[epoch19, step614]: loss 0.533004
[epoch19, step615]: loss 0.362227
[epoch19, step616]: loss 0.368542
[epoch19, step617]: loss 0.729364
[epoch19, step618]: loss 0.499483
[epoch19, step619]: loss 0.371335
[epoch19, step620]: loss 0.352309
[epoch19, step621]: loss 0.275035
[epoch19, step622]: loss 0.503496
[epoch19, step623]: loss 0.494927
[epoch19, step624]: loss 0.524183
[epoch19, step625]: loss 0.473074
[epoch19, step626]: loss 0.643165
[epoch19, step627]: loss 0.707523
[epoch19, step628]: loss 0.657919
[epoch19, step629]: loss 0.634326
[epoch19, step630]: loss 0.384433
[epoch19, step631]: loss 0.289133
[epoch19, step632]: loss 0.223803
[epoch19, step633]: loss 0.584787
[epoch19, step634]: loss 0.614800
[epoch19, step635]: loss 0.487001
[epoch19, step636]: loss 0.717382
[epoch19, step637]: loss 0.499351
[epoch19, step638]: loss 0.700202
[epoch19, step639]: loss 0.490379
[epoch19, step640]: loss 0.502023
[epoch19, step641]: loss 0.246581
[epoch19, step642]: loss 0.577073
[epoch19, step643]: loss 0.582845
[epoch19, step644]: loss 0.642433
[epoch19, step645]: loss 0.524046
[epoch19, step646]: loss 0.565201
[epoch19, step647]: loss 0.405806
[epoch19, step648]: loss 0.430731
[epoch19, step649]: loss 0.704223
[epoch19, step650]: loss 0.402562
[epoch19, step651]: loss 0.772822
[epoch19, step652]: loss 0.761802
[epoch19, step653]: loss 0.400339
[epoch19, step654]: loss 0.502942
[epoch19, step655]: loss 0.554911
[epoch19, step656]: loss 0.558307
[epoch19, step657]: loss 0.707668
[epoch19, step658]: loss 0.474727
[epoch19, step659]: loss 0.659969
[epoch19, step660]: loss 0.261432
[epoch19, step661]: loss 0.615286
[epoch19, step662]: loss 0.469206
[epoch19, step663]: loss 0.553734
[epoch19, step664]: loss 0.701255
[epoch19, step665]: loss 0.533671
[epoch19, step666]: loss 0.375859
[epoch19, step667]: loss 0.522136
[epoch19, step668]: loss 0.631801
[epoch19, step669]: loss 0.399107
[epoch19, step670]: loss 0.621353
[epoch19, step671]: loss 0.567941
[epoch19, step672]: loss 0.364272
[epoch19, step673]: loss 0.574102
[epoch19, step674]: loss 0.717991
[epoch19, step675]: loss 0.496553
[epoch19, step676]: loss 0.599860
[epoch19, step677]: loss 0.601136
[epoch19, step678]: loss 0.378873
[epoch19, step679]: loss 0.534253
[epoch19, step680]: loss 0.594671
[epoch19, step681]: loss 0.517462
[epoch19, step682]: loss 0.480714
[epoch19, step683]: loss 0.747911
[epoch19, step684]: loss 0.165650
[epoch19, step685]: loss 0.330347
[epoch19, step686]: loss 0.224083
[epoch19, step687]: loss 0.586245
[epoch19, step688]: loss 0.578072
[epoch19, step689]: loss 0.640553
[epoch19, step690]: loss 0.404205
[epoch19, step691]: loss 0.487391
[epoch19, step692]: loss 0.297655
[epoch19, step693]: loss 0.469764
[epoch19, step694]: loss 0.376385
[epoch19, step695]: loss 0.632159
[epoch19, step696]: loss 0.250754
[epoch19, step697]: loss 0.461246
[epoch19, step698]: loss 0.546980
[epoch19, step699]: loss 0.267277
[epoch19, step700]: loss 0.551940
[epoch19, step701]: loss 0.658247
[epoch19, step702]: loss 0.586739
[epoch19, step703]: loss 0.637287
[epoch19, step704]: loss 0.515765
[epoch19, step705]: loss 0.672303
[epoch19, step706]: loss 0.659269
[epoch19, step707]: loss 0.345431
[epoch19, step708]: loss 0.655103
[epoch19, step709]: loss 0.498012
[epoch19, step710]: loss 0.679898
[epoch19, step711]: loss 0.225243
[epoch19, step712]: loss 0.552595
[epoch19, step713]: loss 0.430029
[epoch19, step714]: loss 0.506446
[epoch19, step715]: loss 0.631695
[epoch19, step716]: loss 0.339446
[epoch19, step717]: loss 0.536220
[epoch19, step718]: loss 0.693977
[epoch19, step719]: loss 0.457106
[epoch19, step720]: loss 0.669018
[epoch19, step721]: loss 0.397911
[epoch19, step722]: loss 0.576482
[epoch19, step723]: loss 0.366084
[epoch19, step724]: loss 0.681289
[epoch19, step725]: loss 0.446000
[epoch19, step726]: loss 0.674149
[epoch19, step727]: loss 0.520359
[epoch19, step728]: loss 0.614019
[epoch19, step729]: loss 0.266993
[epoch19, step730]: loss 0.320279
[epoch19, step731]: loss 0.301765
[epoch19, step732]: loss 0.441142
[epoch19, step733]: loss 0.369151
[epoch19, step734]: loss 0.622848
[epoch19, step735]: loss 0.611095
[epoch19, step736]: loss 0.436446
[epoch19, step737]: loss 0.388445
[epoch19, step738]: loss 0.489452
[epoch19, step739]: loss 0.263442
[epoch19, step740]: loss 0.654410
[epoch19, step741]: loss 0.569761
[epoch19, step742]: loss 0.551526
[epoch19, step743]: loss 0.657849
[epoch19, step744]: loss 0.360577
[epoch19, step745]: loss 0.439298
[epoch19, step746]: loss 0.653998
[epoch19, step747]: loss 0.355607
[epoch19, step748]: loss 0.427747
[epoch19, step749]: loss 0.513385
[epoch19, step750]: loss 0.781189
[epoch19, step751]: loss 0.538719
[epoch19, step752]: loss 0.578861
[epoch19, step753]: loss 0.502327
[epoch19, step754]: loss 0.544569
[epoch19, step755]: loss 0.685373
[epoch19, step756]: loss 0.641141
[epoch19, step757]: loss 0.635342
[epoch19, step758]: loss 0.460917
[epoch19, step759]: loss 0.595039
[epoch19, step760]: loss 0.304645
[epoch19, step761]: loss 0.689747
[epoch19, step762]: loss 0.572622
[epoch19, step763]: loss 0.627496
[epoch19, step764]: loss 0.351192
[epoch19, step765]: loss 0.662739
[epoch19, step766]: loss 0.600342
[epoch19, step767]: loss 0.568897
[epoch19, step768]: loss 0.642786
[epoch19, step769]: loss 0.418019
[epoch19, step770]: loss 0.726635
[epoch19, step771]: loss 0.445197
[epoch19, step772]: loss 0.661035
[epoch19, step773]: loss 0.564041
[epoch19, step774]: loss 0.705470
[epoch19, step775]: loss 0.747918
[epoch19, step776]: loss 0.405177
[epoch19, step777]: loss 0.510478
[epoch19, step778]: loss 0.338689
[epoch19, step779]: loss 0.504626
[epoch19, step780]: loss 0.469378
[epoch19, step781]: loss 0.687583
[epoch19, step782]: loss 0.484087
[epoch19, step783]: loss 0.637322
[epoch19, step784]: loss 0.553387
[epoch19, step785]: loss 0.553538
[epoch19, step786]: loss 0.504920
[epoch19, step787]: loss 0.720692
[epoch19, step788]: loss 0.656316
[epoch19, step789]: loss 0.649326
[epoch19, step790]: loss 0.459327
[epoch19, step791]: loss 0.401661
[epoch19, step792]: loss 0.273300
[epoch19, step793]: loss 0.723706
[epoch19, step794]: loss 0.461055
[epoch19, step795]: loss 0.607988
[epoch19, step796]: loss 0.600822
[epoch19, step797]: loss 0.621309
[epoch19, step798]: loss 0.597907
[epoch19, step799]: loss 0.536537
[epoch19, step800]: loss 0.459431
[epoch19, step801]: loss 0.542380
[epoch19, step802]: loss 0.680097
[epoch19, step803]: loss 0.559074
[epoch19, step804]: loss 0.589984
[epoch19, step805]: loss 0.584686
[epoch19, step806]: loss 0.493069
[epoch19, step807]: loss 0.501141
[epoch19, step808]: loss 0.604513
[epoch19, step809]: loss 0.674473
[epoch19, step810]: loss 0.442179
[epoch19, step811]: loss 0.532152
[epoch19, step812]: loss 0.388666
[epoch19, step813]: loss 0.392744
[epoch19, step814]: loss 0.419245
[epoch19, step815]: loss 0.653033
[epoch19, step816]: loss 0.711046
[epoch19, step817]: loss 0.752122
[epoch19, step818]: loss 0.569777
[epoch19, step819]: loss 0.781935
[epoch19, step820]: loss 0.524031
[epoch19, step821]: loss 0.416545
[epoch19, step822]: loss 0.634476
[epoch19, step823]: loss 0.475471
[epoch19, step824]: loss 0.473256
[epoch19, step825]: loss 0.412538
[epoch19, step826]: loss 0.638397
[epoch19, step827]: loss 0.663947
[epoch19, step828]: loss 0.411896
[epoch19, step829]: loss 0.640556
[epoch19, step830]: loss 0.686510
[epoch19, step831]: loss 0.171705
[epoch19, step832]: loss 0.623538
[epoch19, step833]: loss 0.579621
[epoch19, step834]: loss 0.512247
[epoch19, step835]: loss 0.728042
[epoch19, step836]: loss 0.777966
[epoch19, step837]: loss 0.355711
[epoch19, step838]: loss 0.453444
[epoch19, step839]: loss 0.446602
[epoch19, step840]: loss 0.478730
[epoch19, step841]: loss 0.388359
[epoch19, step842]: loss 0.551379
[epoch19, step843]: loss 0.677232
[epoch19, step844]: loss 0.791539
[epoch19, step845]: loss 0.546050
[epoch19, step846]: loss 0.552415
[epoch19, step847]: loss 0.581165
[epoch19, step848]: loss 0.552922
[epoch19, step849]: loss 0.707831
[epoch19, step850]: loss 0.523344
[epoch19, step851]: loss 0.300863
[epoch19, step852]: loss 0.280509
[epoch19, step853]: loss 0.538331
[epoch19, step854]: loss 0.591500
[epoch19, step855]: loss 0.548239
[epoch19, step856]: loss 0.530429
[epoch19, step857]: loss 0.449071
[epoch19, step858]: loss 0.536183
[epoch19, step859]: loss 0.488380
[epoch19, step860]: loss 0.403341
[epoch19, step861]: loss 0.627241
[epoch19, step862]: loss 0.520564
[epoch19, step863]: loss 0.503860
[epoch19, step864]: loss 0.418859
[epoch19, step865]: loss 0.680884
[epoch19, step866]: loss 0.381147
[epoch19, step867]: loss 0.511110
[epoch19, step868]: loss 0.480593
[epoch19, step869]: loss 0.716601
[epoch19, step870]: loss 0.382697
[epoch19, step871]: loss 0.448288
[epoch19, step872]: loss 0.400624
[epoch19, step873]: loss 0.505285
[epoch19, step874]: loss 0.326287
[epoch19, step875]: loss 0.400588
[epoch19, step876]: loss 0.434666
[epoch19, step877]: loss 0.394321
[epoch19, step878]: loss 0.314947
[epoch19, step879]: loss 0.252446
[epoch19, step880]: loss 0.129511
[epoch19, step881]: loss 0.427978
[epoch19, step882]: loss 0.316354
[epoch19, step883]: loss 0.462371
[epoch19, step884]: loss 0.413856
[epoch19, step885]: loss 0.747795
[epoch19, step886]: loss 0.686520
[epoch19, step887]: loss 0.575443
[epoch19, step888]: loss 0.687006
[epoch19, step889]: loss 0.578074
[epoch19, step890]: loss 0.506165
[epoch19, step891]: loss 0.552671
[epoch19, step892]: loss 0.535421
[epoch19, step893]: loss 0.433182
[epoch19, step894]: loss 0.390219
[epoch19, step895]: loss 0.470341
[epoch19, step896]: loss 0.602907
[epoch19, step897]: loss 0.310938
[epoch19, step898]: loss 0.662824
[epoch19, step899]: loss 0.644730
[epoch19, step900]: loss 0.676930
[epoch19, step901]: loss 0.625636
[epoch19, step902]: loss 0.235613
[epoch19, step903]: loss 0.704898
[epoch19, step904]: loss 0.638002
[epoch19, step905]: loss 0.684116
[epoch19, step906]: loss 0.591259
[epoch19, step907]: loss 0.672060
[epoch19, step908]: loss 0.396261
[epoch19, step909]: loss 0.475929
[epoch19, step910]: loss 0.399172
[epoch19, step911]: loss 0.528785
[epoch19, step912]: loss 0.372592
[epoch19, step913]: loss 0.431200
[epoch19, step914]: loss 0.595590
[epoch19, step915]: loss 0.508330
[epoch19, step916]: loss 0.319862
[epoch19, step917]: loss 0.582213
[epoch19, step918]: loss 0.611275
[epoch19, step919]: loss 0.589315
[epoch19, step920]: loss 0.146304
[epoch19, step921]: loss 0.644780
[epoch19, step922]: loss 0.322429
[epoch19, step923]: loss 0.556946
[epoch19, step924]: loss 0.583853
[epoch19, step925]: loss 0.539334
[epoch19, step926]: loss 0.648246
[epoch19, step927]: loss 0.617693
[epoch19, step928]: loss 0.754361
[epoch19, step929]: loss 0.529353
[epoch19, step930]: loss 0.541097
[epoch19, step931]: loss 0.303324
[epoch19, step932]: loss 0.330406
[epoch19, step933]: loss 0.458999
[epoch19, step934]: loss 0.484541
[epoch19, step935]: loss 0.719866
[epoch19, step936]: loss 0.536003
[epoch19, step937]: loss 0.517419
[epoch19, step938]: loss 0.414532
[epoch19, step939]: loss 0.647865
[epoch19, step940]: loss 0.594642
[epoch19, step941]: loss 0.417768
[epoch19, step942]: loss 0.383032
[epoch19, step943]: loss 0.505308
[epoch19, step944]: loss 0.623681
[epoch19, step945]: loss 0.769661
[epoch19, step946]: loss 0.690396
[epoch19, step947]: loss 0.573944
[epoch19, step948]: loss 0.525330
[epoch19, step949]: loss 0.455973
[epoch19, step950]: loss 0.565751
[epoch19, step951]: loss 0.499029
[epoch19, step952]: loss 0.653956
[epoch19, step953]: loss 0.578753
[epoch19, step954]: loss 0.522484
[epoch19, step955]: loss 0.481303
[epoch19, step956]: loss 0.758061
[epoch19, step957]: loss 0.365242
[epoch19, step958]: loss 0.709470
[epoch19, step959]: loss 0.705395
[epoch19, step960]: loss 0.289895
[epoch19, step961]: loss 0.499591
[epoch19, step962]: loss 0.692733
[epoch19, step963]: loss 0.658790
[epoch19, step964]: loss 0.301652
[epoch19, step965]: loss 0.719358
[epoch19, step966]: loss 0.690788
[epoch19, step967]: loss 0.359910
[epoch19, step968]: loss 0.684003
[epoch19, step969]: loss 0.361019
[epoch19, step970]: loss 0.441376
[epoch19, step971]: loss 0.469231
[epoch19, step972]: loss 0.629603
[epoch19, step973]: loss 0.128656
[epoch19, step974]: loss 0.590733
[epoch19, step975]: loss 0.477536
[epoch19, step976]: loss 0.354915
[epoch19, step977]: loss 0.369509
[epoch19, step978]: loss 0.437322
[epoch19, step979]: loss 0.482306
[epoch19, step980]: loss 0.528023
[epoch19, step981]: loss 0.540450
[epoch19, step982]: loss 0.588908
[epoch19, step983]: loss 0.511727
[epoch19, step984]: loss 0.553059
[epoch19, step985]: loss 0.575376
[epoch19, step986]: loss 0.515927
[epoch19, step987]: loss 0.489800
[epoch19, step988]: loss 0.652363
[epoch19, step989]: loss 0.455434
[epoch19, step990]: loss 0.688721
[epoch19, step991]: loss 0.670194
[epoch19, step992]: loss 0.568159
[epoch19, step993]: loss 0.498785
[epoch19, step994]: loss 0.588442
[epoch19, step995]: loss 0.380359
[epoch19, step996]: loss 0.611332
[epoch19, step997]: loss 0.641598
[epoch19, step998]: loss 0.703709
[epoch19, step999]: loss 0.702764
[epoch19, step1000]: loss 0.411498
[epoch19, step1001]: loss 0.636292
[epoch19, step1002]: loss 0.590130
[epoch19, step1003]: loss 0.392956
[epoch19, step1004]: loss 0.626384
[epoch19, step1005]: loss 0.549870
[epoch19, step1006]: loss 0.631321
[epoch19, step1007]: loss 0.589372
[epoch19, step1008]: loss 0.516568
[epoch19, step1009]: loss 0.579344
[epoch19, step1010]: loss 0.581356
[epoch19, step1011]: loss 0.546268
[epoch19, step1012]: loss 0.450282
[epoch19, step1013]: loss 0.769784
[epoch19, step1014]: loss 0.551428
[epoch19, step1015]: loss 0.584596
[epoch19, step1016]: loss 0.597277
[epoch19, step1017]: loss 0.213788
[epoch19, step1018]: loss 0.299659
[epoch19, step1019]: loss 0.442640
[epoch19, step1020]: loss 0.655826
[epoch19, step1021]: loss 0.340764
[epoch19, step1022]: loss 0.514288
[epoch19, step1023]: loss 0.336079
[epoch19, step1024]: loss 0.587170
[epoch19, step1025]: loss 0.340165
[epoch19, step1026]: loss 0.810208
[epoch19, step1027]: loss 0.307155
[epoch19, step1028]: loss 0.321590
[epoch19, step1029]: loss 0.726004
[epoch19, step1030]: loss 0.691023
[epoch19, step1031]: loss 0.754408
[epoch19, step1032]: loss 0.595776
[epoch19, step1033]: loss 0.197661
[epoch19, step1034]: loss 0.740543
[epoch19, step1035]: loss 0.549964
[epoch19, step1036]: loss 0.274240
[epoch19, step1037]: loss 0.320272
[epoch19, step1038]: loss 0.503718
[epoch19, step1039]: loss 0.361158
[epoch19, step1040]: loss 0.522725
[epoch19, step1041]: loss 0.542794
[epoch19, step1042]: loss 0.476143
[epoch19, step1043]: loss 0.565100
[epoch19, step1044]: loss 0.599369
[epoch19, step1045]: loss 0.553131
[epoch19, step1046]: loss 0.591987
[epoch19, step1047]: loss 0.514869
[epoch19, step1048]: loss 0.448152
[epoch19, step1049]: loss 0.541991
[epoch19, step1050]: loss 0.310437
[epoch19, step1051]: loss 0.344521
[epoch19, step1052]: loss 0.672445
[epoch19, step1053]: loss 0.521097
[epoch19, step1054]: loss 0.618210
[epoch19, step1055]: loss 0.564064
[epoch19, step1056]: loss 0.427647
[epoch19, step1057]: loss 0.631447
[epoch19, step1058]: loss 0.514958
[epoch19, step1059]: loss 0.661974
[epoch19, step1060]: loss 0.486896
[epoch19, step1061]: loss 0.564430
[epoch19, step1062]: loss 0.690068
[epoch19, step1063]: loss 0.684660
[epoch19, step1064]: loss 0.564239
[epoch19, step1065]: loss 0.536150
[epoch19, step1066]: loss 0.457949
[epoch19, step1067]: loss 0.275101
[epoch19, step1068]: loss 0.329740
[epoch19, step1069]: loss 0.322263
[epoch19, step1070]: loss 0.560669
[epoch19, step1071]: loss 0.751366
[epoch19, step1072]: loss 0.339573
[epoch19, step1073]: loss 0.331083
[epoch19, step1074]: loss 0.432985
[epoch19, step1075]: loss 0.661783
[epoch19, step1076]: loss 0.459021
[epoch19, step1077]: loss 0.594478
[epoch19, step1078]: loss 0.517810
[epoch19, step1079]: loss 0.627315
[epoch19, step1080]: loss 0.772722
[epoch19, step1081]: loss 0.616610
[epoch19, step1082]: loss 0.327572
[epoch19, step1083]: loss 0.590948
[epoch19, step1084]: loss 0.352019
[epoch19, step1085]: loss 0.630123
[epoch19, step1086]: loss 0.565296
[epoch19, step1087]: loss 0.693321
[epoch19, step1088]: loss 0.488893
[epoch19, step1089]: loss 0.498859
[epoch19, step1090]: loss 0.626733
[epoch19, step1091]: loss 0.553562
[epoch19, step1092]: loss 0.541077
[epoch19, step1093]: loss 0.487370
[epoch19, step1094]: loss 0.558656
[epoch19, step1095]: loss 0.507320
[epoch19, step1096]: loss 0.680854
[epoch19, step1097]: loss 0.462151
[epoch19, step1098]: loss 0.539814
[epoch19, step1099]: loss 0.408216
[epoch19, step1100]: loss 0.523866
[epoch19, step1101]: loss 0.421189
[epoch19, step1102]: loss 0.407642
[epoch19, step1103]: loss 0.561193
[epoch19, step1104]: loss 0.433641
[epoch19, step1105]: loss 0.648752
[epoch19, step1106]: loss 0.428457
[epoch19, step1107]: loss 0.311073
[epoch19, step1108]: loss 0.679456
[epoch19, step1109]: loss 0.547518
[epoch19, step1110]: loss 0.584572
[epoch19, step1111]: loss 0.725116
[epoch19, step1112]: loss 0.437292
[epoch19, step1113]: loss 0.476713
[epoch19, step1114]: loss 0.591593
[epoch19, step1115]: loss 0.499984
[epoch19, step1116]: loss 0.522894
[epoch19, step1117]: loss 0.611103
[epoch19, step1118]: loss 0.633495
[epoch19, step1119]: loss 0.145223
[epoch19, step1120]: loss 0.413970
[epoch19, step1121]: loss 0.667063
[epoch19, step1122]: loss 0.682974
[epoch19, step1123]: loss 0.558503
[epoch19, step1124]: loss 0.765704
[epoch19, step1125]: loss 0.614991
[epoch19, step1126]: loss 0.518586
[epoch19, step1127]: loss 0.638364
[epoch19, step1128]: loss 0.496222
[epoch19, step1129]: loss 0.401253
[epoch19, step1130]: loss 0.509981
[epoch19, step1131]: loss 0.436433
[epoch19, step1132]: loss 0.513725
[epoch19, step1133]: loss 0.131722
[epoch19, step1134]: loss 0.550832
[epoch19, step1135]: loss 0.680446
[epoch19, step1136]: loss 0.489772
[epoch19, step1137]: loss 0.220091
[epoch19, step1138]: loss 0.629205
[epoch19, step1139]: loss 0.551972
[epoch19, step1140]: loss 0.553567
[epoch19, step1141]: loss 0.695295
[epoch19, step1142]: loss 0.363344
[epoch19, step1143]: loss 0.589355
[epoch19, step1144]: loss 0.709866
[epoch19, step1145]: loss 0.556913
[epoch19, step1146]: loss 0.361515
[epoch19, step1147]: loss 0.431755
[epoch19, step1148]: loss 0.635028
[epoch19, step1149]: loss 0.489192
[epoch19, step1150]: loss 0.606304
[epoch19, step1151]: loss 0.471055
[epoch19, step1152]: loss 0.461970
[epoch19, step1153]: loss 0.146050
[epoch19, step1154]: loss 0.428522
[epoch19, step1155]: loss 0.703769
[epoch19, step1156]: loss 0.532909
[epoch19, step1157]: loss 0.804592
[epoch19, step1158]: loss 0.601223
[epoch19, step1159]: loss 0.467310
[epoch19, step1160]: loss 0.455788
[epoch19, step1161]: loss 0.685214
[epoch19, step1162]: loss 0.345552
[epoch19, step1163]: loss 0.486585
[epoch19, step1164]: loss 0.541822
[epoch19, step1165]: loss 0.551869
[epoch19, step1166]: loss 0.406518
[epoch19, step1167]: loss 0.556394
[epoch19, step1168]: loss 0.527932
[epoch19, step1169]: loss 0.652880
[epoch19, step1170]: loss 0.660281
[epoch19, step1171]: loss 0.592723
[epoch19, step1172]: loss 0.586944
[epoch19, step1173]: loss 0.342370
[epoch19, step1174]: loss 0.267032
[epoch19, step1175]: loss 0.513339
[epoch19, step1176]: loss 0.255879
[epoch19, step1177]: loss 0.299190
[epoch19, step1178]: loss 0.517220
[epoch19, step1179]: loss 0.448540
[epoch19, step1180]: loss 0.291768
[epoch19, step1181]: loss 0.449370
[epoch19, step1182]: loss 0.415771
[epoch19, step1183]: loss 0.464527
[epoch19, step1184]: loss 0.412745
[epoch19, step1185]: loss 0.716349
[epoch19, step1186]: loss 0.396116
[epoch19, step1187]: loss 0.241889
[epoch19, step1188]: loss 0.416345
[epoch19, step1189]: loss 0.352829
[epoch19, step1190]: loss 0.541936
[epoch19, step1191]: loss 0.597435
[epoch19, step1192]: loss 0.533667
[epoch19, step1193]: loss 0.603612
[epoch19, step1194]: loss 0.511711
[epoch19, step1195]: loss 0.687462
[epoch19, step1196]: loss 0.657390
[epoch19, step1197]: loss 0.507009
[epoch19, step1198]: loss 0.446458
[epoch19, step1199]: loss 0.519038
[epoch19, step1200]: loss 0.516332
[epoch19, step1201]: loss 0.131721
[epoch19, step1202]: loss 0.724631
[epoch19, step1203]: loss 0.367935
[epoch19, step1204]: loss 0.370126
[epoch19, step1205]: loss 0.753322
[epoch19, step1206]: loss 0.591867
[epoch19, step1207]: loss 0.492129
[epoch19, step1208]: loss 0.406635
[epoch19, step1209]: loss 0.467629
[epoch19, step1210]: loss 0.380445
[epoch19, step1211]: loss 0.781715
[epoch19, step1212]: loss 0.631436
[epoch19, step1213]: loss 0.341319
[epoch19, step1214]: loss 0.678180
[epoch19, step1215]: loss 0.373419
[epoch19, step1216]: loss 0.755397
[epoch19, step1217]: loss 0.372490
[epoch19, step1218]: loss 0.589735
[epoch19, step1219]: loss 0.816611
[epoch19, step1220]: loss 0.644109
[epoch19, step1221]: loss 0.544295
[epoch19, step1222]: loss 0.370090
[epoch19, step1223]: loss 0.499041
[epoch19, step1224]: loss 0.642869
[epoch19, step1225]: loss 0.489527
[epoch19, step1226]: loss 0.542981
[epoch19, step1227]: loss 0.524515
[epoch19, step1228]: loss 0.591117
[epoch19, step1229]: loss 0.614586
[epoch19, step1230]: loss 0.449952
[epoch19, step1231]: loss 0.650648
[epoch19, step1232]: loss 0.669278
[epoch19, step1233]: loss 0.631113
[epoch19, step1234]: loss 0.386136
[epoch19, step1235]: loss 0.552469
[epoch19, step1236]: loss 0.360028
[epoch19, step1237]: loss 0.407760
[epoch19, step1238]: loss 0.487102
[epoch19, step1239]: loss 0.697534
[epoch19, step1240]: loss 0.408775
[epoch19, step1241]: loss 0.440718
[epoch19, step1242]: loss 0.399263
[epoch19, step1243]: loss 0.517789
[epoch19, step1244]: loss 0.719583
[epoch19, step1245]: loss 0.214387
[epoch19, step1246]: loss 0.317510
[epoch19, step1247]: loss 0.575083
[epoch19, step1248]: loss 0.455856
[epoch19, step1249]: loss 0.522966
[epoch19, step1250]: loss 0.523891
[epoch19, step1251]: loss 0.656972
[epoch19, step1252]: loss 0.751953
[epoch19, step1253]: loss 0.424566
[epoch19, step1254]: loss 0.656203
[epoch19, step1255]: loss 0.402386
[epoch19, step1256]: loss 0.734262
[epoch19, step1257]: loss 0.300897
[epoch19, step1258]: loss 0.561173
[epoch19, step1259]: loss 0.598881
[epoch19, step1260]: loss 0.537369
[epoch19, step1261]: loss 0.266968
[epoch19, step1262]: loss 0.763355
[epoch19, step1263]: loss 0.590876
[epoch19, step1264]: loss 0.753098
[epoch19, step1265]: loss 0.459772
[epoch19, step1266]: loss 0.372938
[epoch19, step1267]: loss 0.415163
[epoch19, step1268]: loss 0.566648
[epoch19, step1269]: loss 0.581388
[epoch19, step1270]: loss 0.179271
[epoch19, step1271]: loss 0.517980
[epoch19, step1272]: loss 0.544611
[epoch19, step1273]: loss 0.750975
[epoch19, step1274]: loss 0.667291
[epoch19, step1275]: loss 0.506260
[epoch19, step1276]: loss 0.428313
[epoch19, step1277]: loss 0.595976
[epoch19, step1278]: loss 0.649856
[epoch19, step1279]: loss 0.503931
[epoch19, step1280]: loss 0.806297
[epoch19, step1281]: loss 0.515562
[epoch19, step1282]: loss 0.522078
[epoch19, step1283]: loss 0.578856
[epoch19, step1284]: loss 0.580375
[epoch19, step1285]: loss 0.683320
[epoch19, step1286]: loss 0.458426
[epoch19, step1287]: loss 0.689805
[epoch19, step1288]: loss 0.751968
[epoch19, step1289]: loss 0.466002
[epoch19, step1290]: loss 0.454490
[epoch19, step1291]: loss 0.314038
[epoch19, step1292]: loss 0.281149
[epoch19, step1293]: loss 0.545997
[epoch19, step1294]: loss 0.561040
[epoch19, step1295]: loss 0.553992
[epoch19, step1296]: loss 0.650416
[epoch19, step1297]: loss 0.391845
[epoch19, step1298]: loss 0.440037
[epoch19, step1299]: loss 0.595855
[epoch19, step1300]: loss 0.475977
[epoch19, step1301]: loss 0.543609
[epoch19, step1302]: loss 0.513887
[epoch19, step1303]: loss 0.418537
[epoch19, step1304]: loss 0.535941
[epoch19, step1305]: loss 0.563297
[epoch19, step1306]: loss 0.302916
[epoch19, step1307]: loss 0.408143
[epoch19, step1308]: loss 0.174914
[epoch19, step1309]: loss 0.601197
[epoch19, step1310]: loss 0.814466
[epoch19, step1311]: loss 0.251000
[epoch19, step1312]: loss 0.550056
[epoch19, step1313]: loss 0.190600
[epoch19, step1314]: loss 0.765983
[epoch19, step1315]: loss 0.807306
[epoch19, step1316]: loss 0.663423
[epoch19, step1317]: loss 0.449861
[epoch19, step1318]: loss 0.644598
[epoch19, step1319]: loss 0.253514
[epoch19, step1320]: loss 0.618213
[epoch19, step1321]: loss 0.458994
[epoch19, step1322]: loss 0.613023
[epoch19, step1323]: loss 0.446322
[epoch19, step1324]: loss 0.481732
[epoch19, step1325]: loss 0.386070
[epoch19, step1326]: loss 0.703694
[epoch19, step1327]: loss 0.410876
[epoch19, step1328]: loss 0.514232
[epoch19, step1329]: loss 0.578570
[epoch19, step1330]: loss 0.696572
[epoch19, step1331]: loss 0.576309
[epoch19, step1332]: loss 0.665216
[epoch19, step1333]: loss 0.509743
[epoch19, step1334]: loss 0.525828
[epoch19, step1335]: loss 0.581526
[epoch19, step1336]: loss 0.490072
[epoch19, step1337]: loss 0.511321
[epoch19, step1338]: loss 0.627478
[epoch19, step1339]: loss 0.450976
[epoch19, step1340]: loss 0.691028
[epoch19, step1341]: loss 0.513724
[epoch19, step1342]: loss 0.581996
[epoch19, step1343]: loss 0.548819
[epoch19, step1344]: loss 0.518711
[epoch19, step1345]: loss 0.438167
[epoch19, step1346]: loss 0.619776
[epoch19, step1347]: loss 0.442164
[epoch19, step1348]: loss 0.610686
[epoch19, step1349]: loss 0.581431
[epoch19, step1350]: loss 0.734889
[epoch19, step1351]: loss 0.610215
[epoch19, step1352]: loss 0.697047
[epoch19, step1353]: loss 0.733058
[epoch19, step1354]: loss 0.496278
[epoch19, step1355]: loss 0.538055
[epoch19, step1356]: loss 0.642336
[epoch19, step1357]: loss 0.567119
[epoch19, step1358]: loss 0.576346
[epoch19, step1359]: loss 0.471565
[epoch19, step1360]: loss 0.570995
[epoch19, step1361]: loss 0.599220
[epoch19, step1362]: loss 0.460873
[epoch19, step1363]: loss 0.478221
[epoch19, step1364]: loss 0.681854
[epoch19, step1365]: loss 0.529278
[epoch19, step1366]: loss 0.492473
[epoch19, step1367]: loss 0.549806
[epoch19, step1368]: loss 0.552225
[epoch19, step1369]: loss 0.457420
[epoch19, step1370]: loss 0.503008
[epoch19, step1371]: loss 0.394496
[epoch19, step1372]: loss 0.671214
[epoch19, step1373]: loss 0.596993
[epoch19, step1374]: loss 0.508394
[epoch19, step1375]: loss 0.437597
[epoch19, step1376]: loss 0.573870
[epoch19, step1377]: loss 0.745431
[epoch19, step1378]: loss 0.456220
[epoch19, step1379]: loss 0.685047
[epoch19, step1380]: loss 0.459868
[epoch19, step1381]: loss 0.675710
[epoch19, step1382]: loss 0.473646
[epoch19, step1383]: loss 0.572018
[epoch19, step1384]: loss 0.697517
[epoch19, step1385]: loss 0.498049
[epoch19, step1386]: loss 0.531799
[epoch19, step1387]: loss 0.640395
[epoch19, step1388]: loss 0.266848
[epoch19, step1389]: loss 0.467132
[epoch19, step1390]: loss 0.420336
[epoch19, step1391]: loss 0.538793
[epoch19, step1392]: loss 0.492851
[epoch19, step1393]: loss 0.389978
[epoch19, step1394]: loss 0.395555
[epoch19, step1395]: loss 0.599384
[epoch19, step1396]: loss 0.590423
[epoch19, step1397]: loss 0.476654
[epoch19, step1398]: loss 0.453366
[epoch19, step1399]: loss 0.727388
[epoch19, step1400]: loss 0.604975
[epoch19, step1401]: loss 0.358975
[epoch19, step1402]: loss 0.707853
[epoch19, step1403]: loss 0.353478
[epoch19, step1404]: loss 0.578027
[epoch19, step1405]: loss 0.502094
[epoch19, step1406]: loss 0.470629
[epoch19, step1407]: loss 0.623532
[epoch19, step1408]: loss 0.409357
[epoch19, step1409]: loss 0.606073
[epoch19, step1410]: loss 0.586204
[epoch19, step1411]: loss 0.765073
[epoch19, step1412]: loss 0.569473
[epoch19, step1413]: loss 0.567627
[epoch19, step1414]: loss 0.310497
[epoch19, step1415]: loss 0.382087
[epoch19, step1416]: loss 0.451222
[epoch19, step1417]: loss 0.635242
[epoch19, step1418]: loss 0.482940
[epoch19, step1419]: loss 0.482280
[epoch19, step1420]: loss 0.447411
[epoch19, step1421]: loss 0.537725
[epoch19, step1422]: loss 0.572931
[epoch19, step1423]: loss 0.379108
[epoch19, step1424]: loss 0.808190
[epoch19, step1425]: loss 0.447709
[epoch19, step1426]: loss 0.410682
[epoch19, step1427]: loss 0.345446
[epoch19, step1428]: loss 0.426637
[epoch19, step1429]: loss 0.355608
[epoch19, step1430]: loss 0.682085
[epoch19, step1431]: loss 0.653803
[epoch19, step1432]: loss 0.570958
[epoch19, step1433]: loss 0.491403
[epoch19, step1434]: loss 0.385077
[epoch19, step1435]: loss 0.296154
[epoch19, step1436]: loss 0.364282
[epoch19, step1437]: loss 0.342566
[epoch19, step1438]: loss 0.749168
[epoch19, step1439]: loss 0.576237
[epoch19, step1440]: loss 0.319469
[epoch19, step1441]: loss 0.491502
[epoch19, step1442]: loss 0.511946
[epoch19, step1443]: loss 0.601837
[epoch19, step1444]: loss 0.340831
[epoch19, step1445]: loss 0.535480
[epoch19, step1446]: loss 0.586816
[epoch19, step1447]: loss 0.366728
[epoch19, step1448]: loss 0.221032
[epoch19, step1449]: loss 0.382505
[epoch19, step1450]: loss 0.597301
[epoch19, step1451]: loss 0.446169
[epoch19, step1452]: loss 0.319599
[epoch19, step1453]: loss 0.177338
[epoch19, step1454]: loss 0.555945
[epoch19, step1455]: loss 0.536890
[epoch19, step1456]: loss 0.530606
[epoch19, step1457]: loss 0.726247
[epoch19, step1458]: loss 0.613750
[epoch19, step1459]: loss 0.509699
[epoch19, step1460]: loss 0.516786
[epoch19, step1461]: loss 0.734189
[epoch19, step1462]: loss 0.673144
[epoch19, step1463]: loss 0.739237
[epoch19, step1464]: loss 0.473341
[epoch19, step1465]: loss 0.889121
[epoch19, step1466]: loss 0.476523
[epoch19, step1467]: loss 0.612851
[epoch19, step1468]: loss 0.432032
[epoch19, step1469]: loss 0.871709
[epoch19, step1470]: loss 0.517990
[epoch19, step1471]: loss 0.741820
[epoch19, step1472]: loss 0.575993
[epoch19, step1473]: loss 0.630151
[epoch19, step1474]: loss 0.551385
[epoch19, step1475]: loss 0.607543
[epoch19, step1476]: loss 0.448597
[epoch19, step1477]: loss 0.396494
[epoch19, step1478]: loss 0.545973
[epoch19, step1479]: loss 0.345764
[epoch19, step1480]: loss 0.622591
[epoch19, step1481]: loss 0.554238
[epoch19, step1482]: loss 0.576062
[epoch19, step1483]: loss 0.312441
[epoch19, step1484]: loss 0.406149
[epoch19, step1485]: loss 0.384696
[epoch19, step1486]: loss 0.519921
[epoch19, step1487]: loss 0.714526
[epoch19, step1488]: loss 0.549493
[epoch19, step1489]: loss 0.397554
[epoch19, step1490]: loss 0.281985
[epoch19, step1491]: loss 0.501998
[epoch19, step1492]: loss 0.491439
[epoch19, step1493]: loss 0.453410
[epoch19, step1494]: loss 0.603046
[epoch19, step1495]: loss 0.537846
[epoch19, step1496]: loss 0.610083
[epoch19, step1497]: loss 0.415093
[epoch19, step1498]: loss 0.745033
[epoch19, step1499]: loss 0.426967
[epoch19, step1500]: loss 0.555753
[epoch19, step1501]: loss 0.631971
[epoch19, step1502]: loss 0.319245
[epoch19, step1503]: loss 0.440427
[epoch19, step1504]: loss 0.527085
[epoch19, step1505]: loss 0.774627
[epoch19, step1506]: loss 0.569941
[epoch19, step1507]: loss 0.494280
[epoch19, step1508]: loss 0.467102
[epoch19, step1509]: loss 0.531227
[epoch19, step1510]: loss 0.393090
[epoch19, step1511]: loss 0.687243
[epoch19, step1512]: loss 0.710161
[epoch19, step1513]: loss 0.682205
[epoch19, step1514]: loss 0.613847
[epoch19, step1515]: loss 0.601299
[epoch19, step1516]: loss 0.574639
[epoch19, step1517]: loss 0.812847
[epoch19, step1518]: loss 0.668488
[epoch19, step1519]: loss 0.670054
[epoch19, step1520]: loss 0.595228
[epoch19, step1521]: loss 0.171066
[epoch19, step1522]: loss 0.729445
[epoch19, step1523]: loss 0.568250
[epoch19, step1524]: loss 0.584162
[epoch19, step1525]: loss 0.710168
[epoch19, step1526]: loss 0.578310
[epoch19, step1527]: loss 0.357125
[epoch19, step1528]: loss 0.400654
[epoch19, step1529]: loss 0.707780
[epoch19, step1530]: loss 0.364903
[epoch19, step1531]: loss 0.579453
[epoch19, step1532]: loss 0.264955
[epoch19, step1533]: loss 0.298935
[epoch19, step1534]: loss 0.589485
[epoch19, step1535]: loss 0.443863
[epoch19, step1536]: loss 0.401810
[epoch19, step1537]: loss 0.548961
[epoch19, step1538]: loss 0.392583
[epoch19, step1539]: loss 0.536268
[epoch19, step1540]: loss 0.519381
[epoch19, step1541]: loss 0.531610
[epoch19, step1542]: loss 0.731618
[epoch19, step1543]: loss 0.491700
[epoch19, step1544]: loss 0.588229
[epoch19, step1545]: loss 0.594255
[epoch19, step1546]: loss 0.657506
[epoch19, step1547]: loss 0.291609
[epoch19, step1548]: loss 0.336368
[epoch19, step1549]: loss 0.532711
[epoch19, step1550]: loss 0.489166
[epoch19, step1551]: loss 0.656992
[epoch19, step1552]: loss 0.634443
[epoch19, step1553]: loss 0.780621
[epoch19, step1554]: loss 0.471513
[epoch19, step1555]: loss 0.365321
[epoch19, step1556]: loss 0.417625
[epoch19, step1557]: loss 0.866798
[epoch19, step1558]: loss 0.686018
[epoch19, step1559]: loss 0.459591
[epoch19, step1560]: loss 0.571018
[epoch19, step1561]: loss 0.453643
[epoch19, step1562]: loss 0.665060
[epoch19, step1563]: loss 0.287695
[epoch19, step1564]: loss 0.488136
[epoch19, step1565]: loss 0.497573
[epoch19, step1566]: loss 0.466051
[epoch19, step1567]: loss 0.473789
[epoch19, step1568]: loss 0.404923
[epoch19, step1569]: loss 0.658742
[epoch19, step1570]: loss 0.391760
[epoch19, step1571]: loss 0.799302
[epoch19, step1572]: loss 0.448457
[epoch19, step1573]: loss 0.479631
[epoch19, step1574]: loss 0.398422
[epoch19, step1575]: loss 0.653920
[epoch19, step1576]: loss 0.662613
[epoch19, step1577]: loss 0.551537
[epoch19, step1578]: loss 0.410857
[epoch19, step1579]: loss 0.522334
[epoch19, step1580]: loss 0.483721
[epoch19, step1581]: loss 0.176950
[epoch19, step1582]: loss 0.621090
[epoch19, step1583]: loss 0.701781
[epoch19, step1584]: loss 0.758880
[epoch19, step1585]: loss 0.561114
[epoch19, step1586]: loss 0.350109
[epoch19, step1587]: loss 0.378769
[epoch19, step1588]: loss 0.376743
[epoch19, step1589]: loss 0.355384
[epoch19, step1590]: loss 0.583553
[epoch19, step1591]: loss 0.571976
[epoch19, step1592]: loss 0.356739
[epoch19, step1593]: loss 0.457740
[epoch19, step1594]: loss 0.533073
[epoch19, step1595]: loss 0.370787
[epoch19, step1596]: loss 0.552206
[epoch19, step1597]: loss 0.528039
[epoch19, step1598]: loss 0.336566
[epoch19, step1599]: loss 0.563787
[epoch19, step1600]: loss 0.641596
[epoch19, step1601]: loss 0.281894
[epoch19, step1602]: loss 0.569376
[epoch19, step1603]: loss 0.612911
[epoch19, step1604]: loss 0.508922
[epoch19, step1605]: loss 0.484403
[epoch19, step1606]: loss 0.573608
[epoch19, step1607]: loss 0.640959
[epoch19, step1608]: loss 0.636995
[epoch19, step1609]: loss 0.461887
[epoch19, step1610]: loss 0.751639
[epoch19, step1611]: loss 0.570739
[epoch19, step1612]: loss 0.625738
[epoch19, step1613]: loss 0.400236
[epoch19, step1614]: loss 0.617700
[epoch19, step1615]: loss 0.576591
[epoch19, step1616]: loss 0.342462
[epoch19, step1617]: loss 0.732276
[epoch19, step1618]: loss 0.655405
[epoch19, step1619]: loss 0.472388
[epoch19, step1620]: loss 0.407291
[epoch19, step1621]: loss 0.752262
[epoch19, step1622]: loss 0.731237
[epoch19, step1623]: loss 0.647035
[epoch19, step1624]: loss 0.516921
[epoch19, step1625]: loss 0.581695
[epoch19, step1626]: loss 0.574680
[epoch19, step1627]: loss 0.580804
[epoch19, step1628]: loss 0.794574
[epoch19, step1629]: loss 0.488893
[epoch19, step1630]: loss 0.630984
[epoch19, step1631]: loss 0.352402
[epoch19, step1632]: loss 0.494896
[epoch19, step1633]: loss 0.562509
[epoch19, step1634]: loss 0.393837
[epoch19, step1635]: loss 0.830664
[epoch19, step1636]: loss 0.590348
[epoch19, step1637]: loss 0.480167
[epoch19, step1638]: loss 0.547058
[epoch19, step1639]: loss 0.408528
[epoch19, step1640]: loss 0.533973
[epoch19, step1641]: loss 0.442875
[epoch19, step1642]: loss 0.473147
[epoch19, step1643]: loss 0.671233
[epoch19, step1644]: loss 0.770993
[epoch19, step1645]: loss 0.553196
[epoch19, step1646]: loss 0.658920
[epoch19, step1647]: loss 0.565515
[epoch19, step1648]: loss 0.536437
[epoch19, step1649]: loss 0.587156
[epoch19, step1650]: loss 0.527034
[epoch19, step1651]: loss 0.611963
[epoch19, step1652]: loss 0.537871
[epoch19, step1653]: loss 0.502223
[epoch19, step1654]: loss 0.489749
[epoch19, step1655]: loss 0.174012
[epoch19, step1656]: loss 0.475598
[epoch19, step1657]: loss 0.317622
[epoch19, step1658]: loss 0.616232
[epoch19, step1659]: loss 0.312488
[epoch19, step1660]: loss 0.311641
[epoch19, step1661]: loss 0.425725
[epoch19, step1662]: loss 0.718717
[epoch19, step1663]: loss 0.626245
[epoch19, step1664]: loss 0.425676
[epoch19, step1665]: loss 0.343121
[epoch19, step1666]: loss 0.535846
[epoch19, step1667]: loss 0.484864
[epoch19, step1668]: loss 0.523236
[epoch19, step1669]: loss 0.497257
[epoch19, step1670]: loss 0.603225
[epoch19, step1671]: loss 0.521322
[epoch19, step1672]: loss 0.509928
[epoch19, step1673]: loss 0.584801
[epoch19, step1674]: loss 0.482479
[epoch19, step1675]: loss 0.450606
[epoch19, step1676]: loss 0.591879
[epoch19, step1677]: loss 0.744802
[epoch19, step1678]: loss 0.482338
[epoch19, step1679]: loss 0.483831
[epoch19, step1680]: loss 0.569386
[epoch19, step1681]: loss 0.396288
[epoch19, step1682]: loss 0.618887
[epoch19, step1683]: loss 0.314916
[epoch19, step1684]: loss 0.651628
[epoch19, step1685]: loss 0.528644
[epoch19, step1686]: loss 0.477763
[epoch19, step1687]: loss 0.609275
[epoch19, step1688]: loss 0.403356
[epoch19, step1689]: loss 0.583553
[epoch19, step1690]: loss 0.487164
[epoch19, step1691]: loss 0.315081
[epoch19, step1692]: loss 0.731335
[epoch19, step1693]: loss 0.579494
[epoch19, step1694]: loss 0.540324
[epoch19, step1695]: loss 0.361037
[epoch19, step1696]: loss 0.496999
[epoch19, step1697]: loss 0.606616
[epoch19, step1698]: loss 0.289859
[epoch19, step1699]: loss 0.590032
[epoch19, step1700]: loss 0.386885
[epoch19, step1701]: loss 0.626342
[epoch19, step1702]: loss 0.647996
[epoch19, step1703]: loss 0.294194
[epoch19, step1704]: loss 0.699086
[epoch19, step1705]: loss 0.668862
[epoch19, step1706]: loss 0.335310
[epoch19, step1707]: loss 0.597413
[epoch19, step1708]: loss 0.489681
[epoch19, step1709]: loss 0.481331
[epoch19, step1710]: loss 0.775379
[epoch19, step1711]: loss 0.507703
[epoch19, step1712]: loss 0.683721
[epoch19, step1713]: loss 0.446673
[epoch19, step1714]: loss 0.574974
[epoch19, step1715]: loss 0.408536
[epoch19, step1716]: loss 0.912468
[epoch19, step1717]: loss 0.331526
[epoch19, step1718]: loss 0.294010
[epoch19, step1719]: loss 0.493850
[epoch19, step1720]: loss 0.653096
[epoch19, step1721]: loss 0.563913
[epoch19, step1722]: loss 0.548659
[epoch19, step1723]: loss 0.455404
[epoch19, step1724]: loss 0.510348
[epoch19, step1725]: loss 0.687178
[epoch19, step1726]: loss 0.533772
[epoch19, step1727]: loss 0.338330
[epoch19, step1728]: loss 0.541708
[epoch19, step1729]: loss 0.736350
[epoch19, step1730]: loss 0.588146
[epoch19, step1731]: loss 0.581110
[epoch19, step1732]: loss 0.310699
[epoch19, step1733]: loss 0.644496
[epoch19, step1734]: loss 0.628783
[epoch19, step1735]: loss 0.599815
[epoch19, step1736]: loss 0.519734
[epoch19, step1737]: loss 0.722985
[epoch19, step1738]: loss 0.359803
[epoch19, step1739]: loss 0.381838
[epoch19, step1740]: loss 0.401820
[epoch19, step1741]: loss 0.365841
[epoch19, step1742]: loss 0.501881
[epoch19, step1743]: loss 0.511831
[epoch19, step1744]: loss 0.502787
[epoch19, step1745]: loss 0.740456
[epoch19, step1746]: loss 0.645079
[epoch19, step1747]: loss 0.649444
[epoch19, step1748]: loss 0.648865
[epoch19, step1749]: loss 0.632010
[epoch19, step1750]: loss 0.649905
[epoch19, step1751]: loss 0.516381
[epoch19, step1752]: loss 0.609427
[epoch19, step1753]: loss 0.489200
[epoch19, step1754]: loss 0.589554
[epoch19, step1755]: loss 0.681899
[epoch19, step1756]: loss 0.534354
[epoch19, step1757]: loss 0.738847
[epoch19, step1758]: loss 0.648976
[epoch19, step1759]: loss 0.451357
[epoch19, step1760]: loss 0.255826
[epoch19, step1761]: loss 0.628046
[epoch19, step1762]: loss 0.703969
[epoch19, step1763]: loss 0.406214
[epoch19, step1764]: loss 0.183618
[epoch19, step1765]: loss 0.486461
[epoch19, step1766]: loss 0.545496
[epoch19, step1767]: loss 0.699065
[epoch19, step1768]: loss 0.480841
[epoch19, step1769]: loss 0.487614
[epoch19, step1770]: loss 0.412588
[epoch19, step1771]: loss 0.489697
[epoch19, step1772]: loss 0.426956
[epoch19, step1773]: loss 0.607656
[epoch19, step1774]: loss 0.409766
[epoch19, step1775]: loss 0.481561
[epoch19, step1776]: loss 0.395493
[epoch19, step1777]: loss 0.414573
[epoch19, step1778]: loss 0.708114
[epoch19, step1779]: loss 0.528161
[epoch19, step1780]: loss 0.468795
[epoch19, step1781]: loss 0.553811
[epoch19, step1782]: loss 0.585255
[epoch19, step1783]: loss 0.386256
[epoch19, step1784]: loss 0.573687
[epoch19, step1785]: loss 0.634070
[epoch19, step1786]: loss 0.539114
[epoch19, step1787]: loss 0.706339
[epoch19, step1788]: loss 0.515235
[epoch19, step1789]: loss 0.520092
[epoch19, step1790]: loss 0.630185
[epoch19, step1791]: loss 0.528735
[epoch19, step1792]: loss 0.408989
[epoch19, step1793]: loss 0.564707
[epoch19, step1794]: loss 0.392988
[epoch19, step1795]: loss 0.758309
[epoch19, step1796]: loss 0.610818
[epoch19, step1797]: loss 0.369902
[epoch19, step1798]: loss 0.417067
[epoch19, step1799]: loss 0.529821
[epoch19, step1800]: loss 0.571142
[epoch19, step1801]: loss 0.489612
[epoch19, step1802]: loss 0.412196
[epoch19, step1803]: loss 0.388314
[epoch19, step1804]: loss 0.398098
[epoch19, step1805]: loss 0.446637
[epoch19, step1806]: loss 0.589755
[epoch19, step1807]: loss 0.646423
[epoch19, step1808]: loss 0.500647
[epoch19, step1809]: loss 0.426793
[epoch19, step1810]: loss 0.612019
[epoch19, step1811]: loss 0.363508
[epoch19, step1812]: loss 0.802614
[epoch19, step1813]: loss 0.631261
[epoch19, step1814]: loss 0.592967
[epoch19, step1815]: loss 0.361445
[epoch19, step1816]: loss 0.705755
[epoch19, step1817]: loss 0.687695
[epoch19, step1818]: loss 0.718927
[epoch19, step1819]: loss 0.584337
[epoch19, step1820]: loss 0.397790
[epoch19, step1821]: loss 0.506665
[epoch19, step1822]: loss 0.651483
[epoch19, step1823]: loss 0.559949
[epoch19, step1824]: loss 0.592876
[epoch19, step1825]: loss 0.461165
[epoch19, step1826]: loss 0.441498
[epoch19, step1827]: loss 0.557719
[epoch19, step1828]: loss 0.618122
[epoch19, step1829]: loss 0.600258
[epoch19, step1830]: loss 0.450059
[epoch19, step1831]: loss 0.499960
[epoch19, step1832]: loss 0.380262
[epoch19, step1833]: loss 0.407470
[epoch19, step1834]: loss 0.539292
[epoch19, step1835]: loss 0.548270
[epoch19, step1836]: loss 0.544345
[epoch19, step1837]: loss 0.561764
[epoch19, step1838]: loss 0.353989
[epoch19, step1839]: loss 0.690073
[epoch19, step1840]: loss 0.570765
[epoch19, step1841]: loss 0.670222
[epoch19, step1842]: loss 0.344510
[epoch19, step1843]: loss 0.697105
[epoch19, step1844]: loss 0.626346
[epoch19, step1845]: loss 0.622684
[epoch19, step1846]: loss 0.568674
[epoch19, step1847]: loss 0.345796
[epoch19, step1848]: loss 0.267669
[epoch19, step1849]: loss 0.585812
[epoch19, step1850]: loss 0.343701
[epoch19, step1851]: loss 0.505605
[epoch19, step1852]: loss 0.483527
[epoch19, step1853]: loss 0.634701
[epoch19, step1854]: loss 0.658880
[epoch19, step1855]: loss 0.610174
[epoch19, step1856]: loss 0.460744
[epoch19, step1857]: loss 0.659974
[epoch19, step1858]: loss 0.553627
[epoch19, step1859]: loss 0.545327
[epoch19, step1860]: loss 0.637909
[epoch19, step1861]: loss 0.476158
[epoch19, step1862]: loss 0.581320
[epoch19, step1863]: loss 0.536864
[epoch19, step1864]: loss 0.481801
[epoch19, step1865]: loss 0.339725
[epoch19, step1866]: loss 0.537993
[epoch19, step1867]: loss 0.569171
[epoch19, step1868]: loss 0.608790
[epoch19, step1869]: loss 0.560455
[epoch19, step1870]: loss 0.457667
[epoch19, step1871]: loss 0.563998
[epoch19, step1872]: loss 0.758568
[epoch19, step1873]: loss 0.537571
[epoch19, step1874]: loss 0.496221
[epoch19, step1875]: loss 0.758138
[epoch19, step1876]: loss 0.485630
[epoch19, step1877]: loss 0.662286
[epoch19, step1878]: loss 0.679925
[epoch19, step1879]: loss 0.403669
[epoch19, step1880]: loss 0.490837
[epoch19, step1881]: loss 0.330397
[epoch19, step1882]: loss 0.591162
[epoch19, step1883]: loss 0.449918
[epoch19, step1884]: loss 0.415065
[epoch19, step1885]: loss 0.623182
[epoch19, step1886]: loss 0.449315
[epoch19, step1887]: loss 0.723257
[epoch19, step1888]: loss 0.486641
[epoch19, step1889]: loss 0.420390
[epoch19, step1890]: loss 0.658752
[epoch19, step1891]: loss 0.440785
[epoch19, step1892]: loss 0.493121
[epoch19, step1893]: loss 0.529741
[epoch19, step1894]: loss 0.424773
[epoch19, step1895]: loss 0.415546
[epoch19, step1896]: loss 0.392147
[epoch19, step1897]: loss 0.499807
[epoch19, step1898]: loss 0.481934
[epoch19, step1899]: loss 0.206476
[epoch19, step1900]: loss 0.406434
[epoch19, step1901]: loss 0.390546
[epoch19, step1902]: loss 0.630376
[epoch19, step1903]: loss 0.263372
[epoch19, step1904]: loss 0.738730
[epoch19, step1905]: loss 0.512954
[epoch19, step1906]: loss 0.633150
[epoch19, step1907]: loss 0.774680
[epoch19, step1908]: loss 0.456533
[epoch19, step1909]: loss 0.487316
[epoch19, step1910]: loss 0.614421
[epoch19, step1911]: loss 0.503838
[epoch19, step1912]: loss 0.507237
[epoch19, step1913]: loss 0.317047
[epoch19, step1914]: loss 0.679575
[epoch19, step1915]: loss 0.537387
[epoch19, step1916]: loss 0.516139
[epoch19, step1917]: loss 0.685346
[epoch19, step1918]: loss 0.653686
[epoch19, step1919]: loss 0.294490
[epoch19, step1920]: loss 0.302036
[epoch19, step1921]: loss 0.578619
[epoch19, step1922]: loss 0.650124
[epoch19, step1923]: loss 0.583248
[epoch19, step1924]: loss 0.473559
[epoch19, step1925]: loss 0.565741
[epoch19, step1926]: loss 0.545388
[epoch19, step1927]: loss 0.190866
[epoch19, step1928]: loss 0.621522
[epoch19, step1929]: loss 0.297147
[epoch19, step1930]: loss 0.295519
[epoch19, step1931]: loss 0.464400
[epoch19, step1932]: loss 0.440825
[epoch19, step1933]: loss 0.634410
[epoch19, step1934]: loss 0.499683
[epoch19, step1935]: loss 0.519364
[epoch19, step1936]: loss 0.571316
[epoch19, step1937]: loss 0.396020
[epoch19, step1938]: loss 0.428141
[epoch19, step1939]: loss 0.760780
[epoch19, step1940]: loss 0.534690
[epoch19, step1941]: loss 0.402485
[epoch19, step1942]: loss 0.575028
[epoch19, step1943]: loss 0.376796
[epoch19, step1944]: loss 0.387721
[epoch19, step1945]: loss 0.412610
[epoch19, step1946]: loss 0.613678
[epoch19, step1947]: loss 0.562495
[epoch19, step1948]: loss 0.381306
[epoch19, step1949]: loss 0.642117
[epoch19, step1950]: loss 0.476510
[epoch19, step1951]: loss 0.622432
[epoch19, step1952]: loss 0.629064
[epoch19, step1953]: loss 0.721790
[epoch19, step1954]: loss 0.283092
[epoch19, step1955]: loss 0.595204
[epoch19, step1956]: loss 0.384761
[epoch19, step1957]: loss 0.653897
[epoch19, step1958]: loss 0.430638
[epoch19, step1959]: loss 0.647058
[epoch19, step1960]: loss 0.528533
[epoch19, step1961]: loss 0.657727
[epoch19, step1962]: loss 0.492924
[epoch19, step1963]: loss 0.235158
[epoch19, step1964]: loss 0.452805
[epoch19, step1965]: loss 0.659001
[epoch19, step1966]: loss 0.429313
[epoch19, step1967]: loss 0.435349
[epoch19, step1968]: loss 0.641584
[epoch19, step1969]: loss 0.760547
[epoch19, step1970]: loss 0.660596
[epoch19, step1971]: loss 0.306017
[epoch19, step1972]: loss 0.255537
[epoch19, step1973]: loss 0.656538
[epoch19, step1974]: loss 0.426642
[epoch19, step1975]: loss 0.338143
[epoch19, step1976]: loss 0.467892
[epoch19, step1977]: loss 0.155496
[epoch19, step1978]: loss 0.497123
[epoch19, step1979]: loss 0.563184
[epoch19, step1980]: loss 0.625449
[epoch19, step1981]: loss 0.571535
[epoch19, step1982]: loss 0.618884
[epoch19, step1983]: loss 0.336651
[epoch19, step1984]: loss 0.575563
[epoch19, step1985]: loss 0.544393
[epoch19, step1986]: loss 0.658333
[epoch19, step1987]: loss 0.459417
[epoch19, step1988]: loss 0.597988
[epoch19, step1989]: loss 0.724814
[epoch19, step1990]: loss 0.659688
[epoch19, step1991]: loss 0.568640
[epoch19, step1992]: loss 0.425677
[epoch19, step1993]: loss 0.723740
[epoch19, step1994]: loss 0.615947
[epoch19, step1995]: loss 0.380683
[epoch19, step1996]: loss 0.653988
[epoch19, step1997]: loss 0.320729
[epoch19, step1998]: loss 0.627298
[epoch19, step1999]: loss 0.465194
[epoch19, step2000]: loss 0.412655
[epoch19, step2001]: loss 0.618534
[epoch19, step2002]: loss 0.544753
[epoch19, step2003]: loss 0.632530
[epoch19, step2004]: loss 0.612479
[epoch19, step2005]: loss 0.424615
[epoch19, step2006]: loss 0.585947
[epoch19, step2007]: loss 0.576992
[epoch19, step2008]: loss 0.470012
[epoch19, step2009]: loss 0.359135
[epoch19, step2010]: loss 0.479826
[epoch19, step2011]: loss 0.613997
[epoch19, step2012]: loss 0.497045
[epoch19, step2013]: loss 0.433157
[epoch19, step2014]: loss 0.472861
[epoch19, step2015]: loss 0.539649
[epoch19, step2016]: loss 0.665384
[epoch19, step2017]: loss 0.432074
[epoch19, step2018]: loss 0.581266
[epoch19, step2019]: loss 0.524018
[epoch19, step2020]: loss 0.628894
[epoch19, step2021]: loss 0.539829
[epoch19, step2022]: loss 0.516150
[epoch19, step2023]: loss 0.611606
[epoch19, step2024]: loss 0.544610
[epoch19, step2025]: loss 0.440207
[epoch19, step2026]: loss 0.393864
[epoch19, step2027]: loss 0.568631
[epoch19, step2028]: loss 0.684983
[epoch19, step2029]: loss 0.548166
[epoch19, step2030]: loss 0.602900
[epoch19, step2031]: loss 0.572084
[epoch19, step2032]: loss 0.626069
[epoch19, step2033]: loss 0.496185
[epoch19, step2034]: loss 0.423321
[epoch19, step2035]: loss 0.512682
[epoch19, step2036]: loss 0.614176
[epoch19, step2037]: loss 0.637297
[epoch19, step2038]: loss 0.507335
[epoch19, step2039]: loss 0.655503
[epoch19, step2040]: loss 0.368731
[epoch19, step2041]: loss 0.340680
[epoch19, step2042]: loss 0.525695
[epoch19, step2043]: loss 0.426481
[epoch19, step2044]: loss 0.587375
[epoch19, step2045]: loss 0.731985
[epoch19, step2046]: loss 0.368986
[epoch19, step2047]: loss 0.792420
[epoch19, step2048]: loss 0.582407
[epoch19, step2049]: loss 0.421124
[epoch19, step2050]: loss 0.375161
[epoch19, step2051]: loss 0.327027
[epoch19, step2052]: loss 0.695463
[epoch19, step2053]: loss 0.548295
[epoch19, step2054]: loss 0.629670
[epoch19, step2055]: loss 0.560438
[epoch19, step2056]: loss 0.387910
[epoch19, step2057]: loss 0.573048
[epoch19, step2058]: loss 0.479450
[epoch19, step2059]: loss 0.297613
[epoch19, step2060]: loss 0.604131
[epoch19, step2061]: loss 0.357557
[epoch19, step2062]: loss 0.264342
[epoch19, step2063]: loss 0.640661
[epoch19, step2064]: loss 0.456812
[epoch19, step2065]: loss 0.638906
[epoch19, step2066]: loss 0.640475
[epoch19, step2067]: loss 0.280523
[epoch19, step2068]: loss 0.671619
[epoch19, step2069]: loss 0.617914
[epoch19, step2070]: loss 0.428588
[epoch19, step2071]: loss 0.204455
[epoch19, step2072]: loss 0.388005
[epoch19, step2073]: loss 0.342833
[epoch19, step2074]: loss 0.487595
[epoch19, step2075]: loss 0.420145
[epoch19, step2076]: loss 0.354393
[epoch19, step2077]: loss 0.635141
[epoch19, step2078]: loss 0.809665
[epoch19, step2079]: loss 0.496053
[epoch19, step2080]: loss 0.402560
[epoch19, step2081]: loss 0.550285
[epoch19, step2082]: loss 0.339037
[epoch19, step2083]: loss 0.574247
[epoch19, step2084]: loss 0.594891
[epoch19, step2085]: loss 0.566545
[epoch19, step2086]: loss 0.610419
[epoch19, step2087]: loss 0.459233
[epoch19, step2088]: loss 0.636970
[epoch19, step2089]: loss 0.439148
[epoch19, step2090]: loss 0.511541
[epoch19, step2091]: loss 0.722189
[epoch19, step2092]: loss 0.478327
[epoch19, step2093]: loss 0.565077
[epoch19, step2094]: loss 0.526099
[epoch19, step2095]: loss 0.379320
[epoch19, step2096]: loss 0.666511
[epoch19, step2097]: loss 0.607807
[epoch19, step2098]: loss 0.410702
[epoch19, step2099]: loss 0.482464
[epoch19, step2100]: loss 0.644200
[epoch19, step2101]: loss 0.668507
[epoch19, step2102]: loss 0.222211
[epoch19, step2103]: loss 0.454809
[epoch19, step2104]: loss 0.392458
[epoch19, step2105]: loss 0.786018
[epoch19, step2106]: loss 0.362762
[epoch19, step2107]: loss 0.644508
[epoch19, step2108]: loss 0.371280
[epoch19, step2109]: loss 0.305546
[epoch19, step2110]: loss 0.620617
[epoch19, step2111]: loss 0.701523
[epoch19, step2112]: loss 0.589569
[epoch19, step2113]: loss 0.424661
[epoch19, step2114]: loss 0.247101
[epoch19, step2115]: loss 0.550969
[epoch19, step2116]: loss 0.580614
[epoch19, step2117]: loss 0.277050
[epoch19, step2118]: loss 0.545444
[epoch19, step2119]: loss 0.579371
[epoch19, step2120]: loss 0.559071
[epoch19, step2121]: loss 0.840618
[epoch19, step2122]: loss 0.640086
[epoch19, step2123]: loss 0.681231
[epoch19, step2124]: loss 0.434926
[epoch19, step2125]: loss 0.573756
[epoch19, step2126]: loss 0.422636
[epoch19, step2127]: loss 0.677657
[epoch19, step2128]: loss 0.587205
[epoch19, step2129]: loss 0.572024
[epoch19, step2130]: loss 0.459073
[epoch19, step2131]: loss 0.685849
[epoch19, step2132]: loss 0.636350
[epoch19, step2133]: loss 0.351540
[epoch19, step2134]: loss 0.420369
[epoch19, step2135]: loss 0.767689
[epoch19, step2136]: loss 0.322227
[epoch19, step2137]: loss 0.531321
[epoch19, step2138]: loss 0.554050
[epoch19, step2139]: loss 0.575360
[epoch19, step2140]: loss 0.489594
[epoch19, step2141]: loss 0.415089
[epoch19, step2142]: loss 0.510392
[epoch19, step2143]: loss 0.516588
[epoch19, step2144]: loss 0.622752
[epoch19, step2145]: loss 0.297144
[epoch19, step2146]: loss 0.374146
[epoch19, step2147]: loss 0.578128
[epoch19, step2148]: loss 0.561099
[epoch19, step2149]: loss 0.483001
[epoch19, step2150]: loss 0.634014
[epoch19, step2151]: loss 0.464568
[epoch19, step2152]: loss 0.732925
[epoch19, step2153]: loss 0.565087
[epoch19, step2154]: loss 0.574705
[epoch19, step2155]: loss 0.344503
[epoch19, step2156]: loss 0.580152
[epoch19, step2157]: loss 0.638107
[epoch19, step2158]: loss 0.412187
[epoch19, step2159]: loss 0.509596
[epoch19, step2160]: loss 0.766626
[epoch19, step2161]: loss 0.832712
[epoch19, step2162]: loss 0.379205
[epoch19, step2163]: loss 0.470899
[epoch19, step2164]: loss 0.614685
[epoch19, step2165]: loss 0.586723
[epoch19, step2166]: loss 0.223480
[epoch19, step2167]: loss 0.633019
[epoch19, step2168]: loss 0.387629
[epoch19, step2169]: loss 0.489706
[epoch19, step2170]: loss 0.541864
[epoch19, step2171]: loss 0.449266
[epoch19, step2172]: loss 0.628601
[epoch19, step2173]: loss 0.521219
[epoch19, step2174]: loss 0.494590
[epoch19, step2175]: loss 0.428263
[epoch19, step2176]: loss 0.382614
[epoch19, step2177]: loss 0.561928
[epoch19, step2178]: loss 0.657747
[epoch19, step2179]: loss 0.695068
[epoch19, step2180]: loss 0.504392
[epoch19, step2181]: loss 0.564240
[epoch19, step2182]: loss 0.706314
[epoch19, step2183]: loss 0.277090
[epoch19, step2184]: loss 0.481415
[epoch19, step2185]: loss 0.751931
[epoch19, step2186]: loss 0.747668
[epoch19, step2187]: loss 0.244080
[epoch19, step2188]: loss 0.158938
[epoch19, step2189]: loss 0.509122
[epoch19, step2190]: loss 0.609949
[epoch19, step2191]: loss 0.609021
[epoch19, step2192]: loss 0.473854
[epoch19, step2193]: loss 0.704324
[epoch19, step2194]: loss 0.338547
[epoch19, step2195]: loss 0.318015
[epoch19, step2196]: loss 0.682087
[epoch19, step2197]: loss 0.632923
[epoch19, step2198]: loss 0.660785
[epoch19, step2199]: loss 0.347273
[epoch19, step2200]: loss 0.838095
[epoch19, step2201]: loss 0.725093
[epoch19, step2202]: loss 0.404661
[epoch19, step2203]: loss 0.560080
[epoch19, step2204]: loss 0.490791
[epoch19, step2205]: loss 0.512297
[epoch19, step2206]: loss 0.143692
[epoch19, step2207]: loss 0.423071
[epoch19, step2208]: loss 0.524277
[epoch19, step2209]: loss 0.549739
[epoch19, step2210]: loss 0.585842
[epoch19, step2211]: loss 0.433785
[epoch19, step2212]: loss 0.544059
[epoch19, step2213]: loss 0.369430
[epoch19, step2214]: loss 0.456732
[epoch19, step2215]: loss 0.482940
[epoch19, step2216]: loss 0.464478
[epoch19, step2217]: loss 0.544361
[epoch19, step2218]: loss 0.551473
[epoch19, step2219]: loss 0.491952
[epoch19, step2220]: loss 0.582538
[epoch19, step2221]: loss 0.494664
[epoch19, step2222]: loss 0.480791
[epoch19, step2223]: loss 0.350417
[epoch19, step2224]: loss 0.440236
[epoch19, step2225]: loss 0.311071
[epoch19, step2226]: loss 0.533954
[epoch19, step2227]: loss 0.522822
[epoch19, step2228]: loss 0.672192
[epoch19, step2229]: loss 0.530348
[epoch19, step2230]: loss 0.481188
[epoch19, step2231]: loss 0.577286
[epoch19, step2232]: loss 0.602964
[epoch19, step2233]: loss 0.673387
[epoch19, step2234]: loss 0.396683
[epoch19, step2235]: loss 0.621359
[epoch19, step2236]: loss 0.730699
[epoch19, step2237]: loss 0.649139
[epoch19, step2238]: loss 0.351137
[epoch19, step2239]: loss 0.133637
[epoch19, step2240]: loss 0.567846
[epoch19, step2241]: loss 0.568224
[epoch19, step2242]: loss 0.693226
[epoch19, step2243]: loss 0.434882
[epoch19, step2244]: loss 0.689367
[epoch19, step2245]: loss 0.439107
[epoch19, step2246]: loss 0.635884
[epoch19, step2247]: loss 0.635187
[epoch19, step2248]: loss 0.548987
[epoch19, step2249]: loss 0.457999
[epoch19, step2250]: loss 0.659580
[epoch19, step2251]: loss 0.464810
[epoch19, step2252]: loss 0.402394
[epoch19, step2253]: loss 0.687693
[epoch19, step2254]: loss 0.524138
[epoch19, step2255]: loss 0.701668
[epoch19, step2256]: loss 0.593113
[epoch19, step2257]: loss 0.603691
[epoch19, step2258]: loss 0.296664
[epoch19, step2259]: loss 0.678267
[epoch19, step2260]: loss 0.583519
[epoch19, step2261]: loss 0.519097
[epoch19, step2262]: loss 0.642009
[epoch19, step2263]: loss 0.543398
[epoch19, step2264]: loss 0.514866
[epoch19, step2265]: loss 0.521762
[epoch19, step2266]: loss 0.455833
[epoch19, step2267]: loss 0.630481
[epoch19, step2268]: loss 0.794860
[epoch19, step2269]: loss 0.630389
[epoch19, step2270]: loss 0.610178
[epoch19, step2271]: loss 0.587614
[epoch19, step2272]: loss 0.802725
[epoch19, step2273]: loss 0.648140
[epoch19, step2274]: loss 0.797293
[epoch19, step2275]: loss 0.625985
[epoch19, step2276]: loss 0.491478
[epoch19, step2277]: loss 0.537816
[epoch19, step2278]: loss 0.411313
[epoch19, step2279]: loss 0.232572
[epoch19, step2280]: loss 0.574998
[epoch19, step2281]: loss 0.624611
[epoch19, step2282]: loss 0.507955
[epoch19, step2283]: loss 0.659382
[epoch19, step2284]: loss 0.419261
[epoch19, step2285]: loss 0.473590
[epoch19, step2286]: loss 0.185086
[epoch19, step2287]: loss 0.444328
[epoch19, step2288]: loss 0.446811
[epoch19, step2289]: loss 0.517879
[epoch19, step2290]: loss 0.466980
[epoch19, step2291]: loss 0.601911
[epoch19, step2292]: loss 0.601070
[epoch19, step2293]: loss 0.401214
[epoch19, step2294]: loss 0.694130
[epoch19, step2295]: loss 0.352744
[epoch19, step2296]: loss 0.671896
[epoch19, step2297]: loss 0.572640
[epoch19, step2298]: loss 0.575554
[epoch19, step2299]: loss 0.582770
[epoch19, step2300]: loss 0.519444
[epoch19, step2301]: loss 0.690857
[epoch19, step2302]: loss 0.660880
[epoch19, step2303]: loss 0.696460
[epoch19, step2304]: loss 0.559193
[epoch19, step2305]: loss 0.639649
[epoch19, step2306]: loss 0.533202
[epoch19, step2307]: loss 0.429136
[epoch19, step2308]: loss 0.675837
[epoch19, step2309]: loss 0.358608
[epoch19, step2310]: loss 0.640162
[epoch19, step2311]: loss 0.401808
[epoch19, step2312]: loss 0.651440
[epoch19, step2313]: loss 0.685323
[epoch19, step2314]: loss 0.555096
[epoch19, step2315]: loss 0.365898
[epoch19, step2316]: loss 0.603803
[epoch19, step2317]: loss 0.459133
[epoch19, step2318]: loss 0.706793
[epoch19, step2319]: loss 0.259290
[epoch19, step2320]: loss 0.683742
[epoch19, step2321]: loss 0.499459
[epoch19, step2322]: loss 0.515852
[epoch19, step2323]: loss 0.532658
[epoch19, step2324]: loss 0.530073
[epoch19, step2325]: loss 0.338759
[epoch19, step2326]: loss 0.628980
[epoch19, step2327]: loss 0.484666
[epoch19, step2328]: loss 0.331382
[epoch19, step2329]: loss 0.695647
[epoch19, step2330]: loss 0.449979
[epoch19, step2331]: loss 0.507320
[epoch19, step2332]: loss 0.588988
[epoch19, step2333]: loss 0.460713
[epoch19, step2334]: loss 0.324056
[epoch19, step2335]: loss 0.350878
[epoch19, step2336]: loss 0.400469
[epoch19, step2337]: loss 0.549254
[epoch19, step2338]: loss 0.542534
[epoch19, step2339]: loss 0.529492
[epoch19, step2340]: loss 0.713715
[epoch19, step2341]: loss 0.457847
[epoch19, step2342]: loss 0.533756
[epoch19, step2343]: loss 0.313811
[epoch19, step2344]: loss 0.472993
[epoch19, step2345]: loss 0.383014
[epoch19, step2346]: loss 0.505266
[epoch19, step2347]: loss 0.497254
[epoch19, step2348]: loss 0.616469
[epoch19, step2349]: loss 0.379880
[epoch19, step2350]: loss 0.364897
[epoch19, step2351]: loss 0.520314
[epoch19, step2352]: loss 0.308322
[epoch19, step2353]: loss 0.651209
[epoch19, step2354]: loss 0.636985
[epoch19, step2355]: loss 0.581589
[epoch19, step2356]: loss 0.685101
[epoch19, step2357]: loss 0.172409
[epoch19, step2358]: loss 0.590417
[epoch19, step2359]: loss 0.588413
[epoch19, step2360]: loss 0.589073
[epoch19, step2361]: loss 0.558784
[epoch19, step2362]: loss 0.635898
[epoch19, step2363]: loss 0.469709
[epoch19, step2364]: loss 0.548304
[epoch19, step2365]: loss 0.493119
[epoch19, step2366]: loss 0.331913
[epoch19, step2367]: loss 0.376038
[epoch19, step2368]: loss 0.841546
[epoch19, step2369]: loss 0.572896
[epoch19, step2370]: loss 0.655438
[epoch19, step2371]: loss 0.565293
[epoch19, step2372]: loss 0.335700
[epoch19, step2373]: loss 0.348294
[epoch19, step2374]: loss 0.576713
[epoch19, step2375]: loss 0.498482
[epoch19, step2376]: loss 0.426591
[epoch19, step2377]: loss 0.400570
[epoch19, step2378]: loss 0.550388
[epoch19, step2379]: loss 0.587736
[epoch19, step2380]: loss 0.497934
[epoch19, step2381]: loss 0.445275
[epoch19, step2382]: loss 0.536857
[epoch19, step2383]: loss 0.653190
[epoch19, step2384]: loss 0.410923
[epoch19, step2385]: loss 0.488925
[epoch19, step2386]: loss 0.470370
[epoch19, step2387]: loss 0.540331
[epoch19, step2388]: loss 0.227852
[epoch19, step2389]: loss 0.824003
[epoch19, step2390]: loss 0.499549
[epoch19, step2391]: loss 0.541832
[epoch19, step2392]: loss 0.677810
[epoch19, step2393]: loss 0.680880
[epoch19, step2394]: loss 0.626599
[epoch19, step2395]: loss 0.224111
[epoch19, step2396]: loss 0.365846
[epoch19, step2397]: loss 0.553701
[epoch19, step2398]: loss 0.396505
[epoch19, step2399]: loss 0.709823
[epoch19, step2400]: loss 0.522750
[epoch19, step2401]: loss 0.640791
[epoch19, step2402]: loss 0.393406
[epoch19, step2403]: loss 0.572323
[epoch19, step2404]: loss 0.538565
[epoch19, step2405]: loss 0.864730
[epoch19, step2406]: loss 0.397532
[epoch19, step2407]: loss 0.388869
[epoch19, step2408]: loss 0.621740
[epoch19, step2409]: loss 0.657874
[epoch19, step2410]: loss 0.591336
[epoch19, step2411]: loss 0.457698
[epoch19, step2412]: loss 0.665057
[epoch19, step2413]: loss 0.691631
[epoch19, step2414]: loss 0.592560
[epoch19, step2415]: loss 0.614200
[epoch19, step2416]: loss 0.607659
[epoch19, step2417]: loss 0.526035
[epoch19, step2418]: loss 0.465224
[epoch19, step2419]: loss 0.514785
[epoch19, step2420]: loss 0.161800
[epoch19, step2421]: loss 0.244250
[epoch19, step2422]: loss 0.555652
[epoch19, step2423]: loss 0.599283
[epoch19, step2424]: loss 0.645299
[epoch19, step2425]: loss 0.526334
[epoch19, step2426]: loss 0.597947
[epoch19, step2427]: loss 0.347004
[epoch19, step2428]: loss 0.606995
[epoch19, step2429]: loss 0.553011
[epoch19, step2430]: loss 0.364271
[epoch19, step2431]: loss 0.106828
[epoch19, step2432]: loss 0.440213
[epoch19, step2433]: loss 0.621609
[epoch19, step2434]: loss 0.578913
[epoch19, step2435]: loss 0.572089
[epoch19, step2436]: loss 0.700441
[epoch19, step2437]: loss 0.455616
[epoch19, step2438]: loss 0.555637
[epoch19, step2439]: loss 0.431158
[epoch19, step2440]: loss 0.610551
[epoch19, step2441]: loss 0.632332
[epoch19, step2442]: loss 0.482812
[epoch19, step2443]: loss 0.436510
[epoch19, step2444]: loss 0.816807
[epoch19, step2445]: loss 0.463211
[epoch19, step2446]: loss 0.584651
[epoch19, step2447]: loss 0.550585
[epoch19, step2448]: loss 0.565664
[epoch19, step2449]: loss 0.598642
[epoch19, step2450]: loss 0.556485
[epoch19, step2451]: loss 0.624276
[epoch19, step2452]: loss 0.563983
[epoch19, step2453]: loss 0.539532
[epoch19, step2454]: loss 0.643979
[epoch19, step2455]: loss 0.538655
[epoch19, step2456]: loss 0.348263
[epoch19, step2457]: loss 0.501599
[epoch19, step2458]: loss 0.473193
[epoch19, step2459]: loss 0.584146
[epoch19, step2460]: loss 0.506865
[epoch19, step2461]: loss 0.721683
[epoch19, step2462]: loss 0.189552
[epoch19, step2463]: loss 0.552366
[epoch19, step2464]: loss 0.757931
[epoch19, step2465]: loss 0.533244
[epoch19, step2466]: loss 0.633716
[epoch19, step2467]: loss 0.431008
[epoch19, step2468]: loss 0.276330
[epoch19, step2469]: loss 0.727422
[epoch19, step2470]: loss 0.638515
[epoch19, step2471]: loss 0.492442
[epoch19, step2472]: loss 0.392172
[epoch19, step2473]: loss 0.806538
[epoch19, step2474]: loss 0.652625
[epoch19, step2475]: loss 0.405333
[epoch19, step2476]: loss 0.513610
[epoch19, step2477]: loss 0.526798
[epoch19, step2478]: loss 0.400430
[epoch19, step2479]: loss 0.344720
[epoch19, step2480]: loss 0.604271
[epoch19, step2481]: loss 0.512246
[epoch19, step2482]: loss 0.580876
[epoch19, step2483]: loss 0.504736
[epoch19, step2484]: loss 0.475264
[epoch19, step2485]: loss 0.516962
[epoch19, step2486]: loss 0.395087
[epoch19, step2487]: loss 0.372190
[epoch19, step2488]: loss 0.474351
[epoch19, step2489]: loss 0.462107
[epoch19, step2490]: loss 0.691188
[epoch19, step2491]: loss 0.662479
[epoch19, step2492]: loss 0.530222
[epoch19, step2493]: loss 0.715871
[epoch19, step2494]: loss 0.535079
[epoch19, step2495]: loss 0.701751
[epoch19, step2496]: loss 0.125845
[epoch19, step2497]: loss 0.305700
[epoch19, step2498]: loss 0.600261
[epoch19, step2499]: loss 0.474402
[epoch19, step2500]: loss 0.565278
[epoch19, step2501]: loss 0.398891
[epoch19, step2502]: loss 0.423616
[epoch19, step2503]: loss 0.640303
[epoch19, step2504]: loss 0.540260
[epoch19, step2505]: loss 0.682829
[epoch19, step2506]: loss 0.253019
[epoch19, step2507]: loss 0.432057
[epoch19, step2508]: loss 0.516876
[epoch19, step2509]: loss 0.468702
[epoch19, step2510]: loss 0.366625
[epoch19, step2511]: loss 0.802028
[epoch19, step2512]: loss 0.627904
[epoch19, step2513]: loss 0.541453
[epoch19, step2514]: loss 0.575877
[epoch19, step2515]: loss 0.608485
[epoch19, step2516]: loss 0.356340
[epoch19, step2517]: loss 0.406534
[epoch19, step2518]: loss 0.574239
[epoch19, step2519]: loss 0.256646
[epoch19, step2520]: loss 0.515445
[epoch19, step2521]: loss 0.532854
[epoch19, step2522]: loss 0.556655
[epoch19, step2523]: loss 0.526706
[epoch19, step2524]: loss 0.465191
[epoch19, step2525]: loss 0.489704
[epoch19, step2526]: loss 0.543685
[epoch19, step2527]: loss 0.401799
[epoch19, step2528]: loss 0.488718
[epoch19, step2529]: loss 0.338075
[epoch19, step2530]: loss 0.521780
[epoch19, step2531]: loss 0.341501
[epoch19, step2532]: loss 0.658939
[epoch19, step2533]: loss 0.470191
[epoch19, step2534]: loss 0.561592
[epoch19, step2535]: loss 0.507347
[epoch19, step2536]: loss 0.435517
[epoch19, step2537]: loss 0.646161
[epoch19, step2538]: loss 0.684610
[epoch19, step2539]: loss 0.613873
[epoch19, step2540]: loss 0.510943
[epoch19, step2541]: loss 0.495317
[epoch19, step2542]: loss 0.585667
[epoch19, step2543]: loss 0.620819
[epoch19, step2544]: loss 0.467377
[epoch19, step2545]: loss 0.516387
[epoch19, step2546]: loss 0.301855
[epoch19, step2547]: loss 0.512011
[epoch19, step2548]: loss 0.308437
[epoch19, step2549]: loss 0.379615
[epoch19, step2550]: loss 0.412638
[epoch19, step2551]: loss 0.369201
[epoch19, step2552]: loss 0.468447
[epoch19, step2553]: loss 0.796018
[epoch19, step2554]: loss 0.446946
[epoch19, step2555]: loss 0.439847
[epoch19, step2556]: loss 0.515188
[epoch19, step2557]: loss 0.609673
[epoch19, step2558]: loss 0.295512
[epoch19, step2559]: loss 0.590678
[epoch19, step2560]: loss 0.311266
[epoch19, step2561]: loss 0.516437
[epoch19, step2562]: loss 0.305615
[epoch19, step2563]: loss 0.473943
[epoch19, step2564]: loss 0.661323
[epoch19, step2565]: loss 0.715297
[epoch19, step2566]: loss 0.379532
[epoch19, step2567]: loss 0.374086
[epoch19, step2568]: loss 0.650362
[epoch19, step2569]: loss 0.492123
[epoch19, step2570]: loss 0.602850
[epoch19, step2571]: loss 0.843585
[epoch19, step2572]: loss 0.382009
[epoch19, step2573]: loss 0.540109
[epoch19, step2574]: loss 0.388912
[epoch19, step2575]: loss 0.572676
[epoch19, step2576]: loss 0.471756
[epoch19, step2577]: loss 0.666685
[epoch19, step2578]: loss 0.413749
[epoch19, step2579]: loss 0.647324
[epoch19, step2580]: loss 0.653700
[epoch19, step2581]: loss 0.618694
[epoch19, step2582]: loss 0.381951
[epoch19, step2583]: loss 0.547977
[epoch19, step2584]: loss 0.794790
[epoch19, step2585]: loss 0.635061
[epoch19, step2586]: loss 0.629211
[epoch19, step2587]: loss 0.645997
[epoch19, step2588]: loss 0.490926
[epoch19, step2589]: loss 0.346197
[epoch19, step2590]: loss 0.491405
[epoch19, step2591]: loss 0.443984
[epoch19, step2592]: loss 0.366559
[epoch19, step2593]: loss 0.681889
[epoch19, step2594]: loss 0.540913
[epoch19, step2595]: loss 0.728120
[epoch19, step2596]: loss 0.659140
[epoch19, step2597]: loss 0.735160
[epoch19, step2598]: loss 0.484916
[epoch19, step2599]: loss 0.587014
[epoch19, step2600]: loss 0.436629
[epoch19, step2601]: loss 0.352414
[epoch19, step2602]: loss 0.392168
[epoch19, step2603]: loss 0.480289
[epoch19, step2604]: loss 0.474252
[epoch19, step2605]: loss 0.339250
[epoch19, step2606]: loss 0.483245
[epoch19, step2607]: loss 0.553866
[epoch19, step2608]: loss 0.216407
[epoch19, step2609]: loss 0.721057
[epoch19, step2610]: loss 0.675890
[epoch19, step2611]: loss 0.462493
[epoch19, step2612]: loss 0.557558
[epoch19, step2613]: loss 0.588687
[epoch19, step2614]: loss 0.591621
[epoch19, step2615]: loss 0.479019
[epoch19, step2616]: loss 0.613669
[epoch19, step2617]: loss 0.487545
[epoch19, step2618]: loss 0.578285
[epoch19, step2619]: loss 0.698989
[epoch19, step2620]: loss 0.543864
[epoch19, step2621]: loss 0.700088
[epoch19, step2622]: loss 0.410753
[epoch19, step2623]: loss 0.462720
[epoch19, step2624]: loss 0.659296
[epoch19, step2625]: loss 0.483299
[epoch19, step2626]: loss 0.447678
[epoch19, step2627]: loss 0.519582
[epoch19, step2628]: loss 0.586704
[epoch19, step2629]: loss 0.591594
[epoch19, step2630]: loss 0.610928
[epoch19, step2631]: loss 0.301637
[epoch19, step2632]: loss 0.310870
[epoch19, step2633]: loss 0.384080
[epoch19, step2634]: loss 0.730063
[epoch19, step2635]: loss 0.569912
[epoch19, step2636]: loss 0.420276
[epoch19, step2637]: loss 0.527912
[epoch19, step2638]: loss 0.370731
[epoch19, step2639]: loss 0.584206
[epoch19, step2640]: loss 0.495353
[epoch19, step2641]: loss 0.671335
[epoch19, step2642]: loss 0.404944
[epoch19, step2643]: loss 0.767467
[epoch19, step2644]: loss 0.629014
[epoch19, step2645]: loss 0.453668
[epoch19, step2646]: loss 0.512437
[epoch19, step2647]: loss 0.596869
[epoch19, step2648]: loss 0.316743
[epoch19, step2649]: loss 0.562015
[epoch19, step2650]: loss 0.532906
[epoch19, step2651]: loss 0.668032
[epoch19, step2652]: loss 0.514558
[epoch19, step2653]: loss 0.562878
[epoch19, step2654]: loss 0.484476
[epoch19, step2655]: loss 0.657113
[epoch19, step2656]: loss 0.681237
[epoch19, step2657]: loss 0.691234
[epoch19, step2658]: loss 0.461960
[epoch19, step2659]: loss 0.404760
[epoch19, step2660]: loss 0.193396
[epoch19, step2661]: loss 0.429749
[epoch19, step2662]: loss 0.650762
[epoch19, step2663]: loss 0.500146
[epoch19, step2664]: loss 0.724331
[epoch19, step2665]: loss 0.702039
[epoch19, step2666]: loss 0.633803
[epoch19, step2667]: loss 0.597608
[epoch19, step2668]: loss 0.197404
[epoch19, step2669]: loss 0.485294
[epoch19, step2670]: loss 0.610735
[epoch19, step2671]: loss 0.536944
[epoch19, step2672]: loss 0.500606
[epoch19, step2673]: loss 0.343855
[epoch19, step2674]: loss 0.448125
[epoch19, step2675]: loss 0.801273
[epoch19, step2676]: loss 0.438336
[epoch19, step2677]: loss 0.470591
[epoch19, step2678]: loss 0.667046
[epoch19, step2679]: loss 0.658707
[epoch19, step2680]: loss 0.517310
[epoch19, step2681]: loss 0.459263
[epoch19, step2682]: loss 0.731895
[epoch19, step2683]: loss 0.390657
[epoch19, step2684]: loss 0.429235
[epoch19, step2685]: loss 0.406749
[epoch19, step2686]: loss 0.529688
[epoch19, step2687]: loss 0.445140
[epoch19, step2688]: loss 0.132461
[epoch19, step2689]: loss 0.468838
[epoch19, step2690]: loss 0.585924
[epoch19, step2691]: loss 0.173393
[epoch19, step2692]: loss 0.665927
[epoch19, step2693]: loss 0.411530
[epoch19, step2694]: loss 0.349825
[epoch19, step2695]: loss 0.622883
[epoch19, step2696]: loss 0.731271
[epoch19, step2697]: loss 0.388196
[epoch19, step2698]: loss 0.671413
[epoch19, step2699]: loss 0.470430
[epoch19, step2700]: loss 0.421467
[epoch19, step2701]: loss 0.535208
[epoch19, step2702]: loss 0.403043
[epoch19, step2703]: loss 0.484877
[epoch19, step2704]: loss 0.367466
[epoch19, step2705]: loss 0.841668
[epoch19, step2706]: loss 0.455532
[epoch19, step2707]: loss 0.728576
[epoch19, step2708]: loss 0.570076
[epoch19, step2709]: loss 0.219963
[epoch19, step2710]: loss 0.523243
[epoch19, step2711]: loss 0.703259
[epoch19, step2712]: loss 0.673723
[epoch19, step2713]: loss 0.158445
[epoch19, step2714]: loss 0.457066
[epoch19, step2715]: loss 0.689058
[epoch19, step2716]: loss 0.534379
[epoch19, step2717]: loss 0.112188
[epoch19, step2718]: loss 0.277030
[epoch19, step2719]: loss 0.495417
[epoch19, step2720]: loss 0.287562
[epoch19, step2721]: loss 0.546496
[epoch19, step2722]: loss 0.572392
[epoch19, step2723]: loss 0.508661
[epoch19, step2724]: loss 0.657791
[epoch19, step2725]: loss 0.325773
[epoch19, step2726]: loss 0.502379
[epoch19, step2727]: loss 0.427138
[epoch19, step2728]: loss 0.536660
[epoch19, step2729]: loss 0.525372
[epoch19, step2730]: loss 0.718843
[epoch19, step2731]: loss 0.739196
[epoch19, step2732]: loss 0.354112
[epoch19, step2733]: loss 0.629898
[epoch19, step2734]: loss 0.595896
[epoch19, step2735]: loss 0.495105
[epoch19, step2736]: loss 0.415999
[epoch19, step2737]: loss 0.585357
[epoch19, step2738]: loss 0.649499
[epoch19, step2739]: loss 0.386080
[epoch19, step2740]: loss 0.614295
[epoch19, step2741]: loss 0.676104
[epoch19, step2742]: loss 0.557323
[epoch19, step2743]: loss 0.695586
[epoch19, step2744]: loss 0.567662
[epoch19, step2745]: loss 0.335689
[epoch19, step2746]: loss 0.600391
[epoch19, step2747]: loss 0.560481
[epoch19, step2748]: loss 0.510644
[epoch19, step2749]: loss 0.567803
[epoch19, step2750]: loss 0.631828
[epoch19, step2751]: loss 0.553462
[epoch19, step2752]: loss 0.556561
[epoch19, step2753]: loss 0.253533
[epoch19, step2754]: loss 0.594062
[epoch19, step2755]: loss 0.782816
[epoch19, step2756]: loss 0.523019
[epoch19, step2757]: loss 0.432596
[epoch19, step2758]: loss 0.652669
[epoch19, step2759]: loss 0.402120
[epoch19, step2760]: loss 0.446786
[epoch19, step2761]: loss 0.487093
[epoch19, step2762]: loss 0.661957
[epoch19, step2763]: loss 0.554398
[epoch19, step2764]: loss 0.368593
[epoch19, step2765]: loss 0.576814
[epoch19, step2766]: loss 0.776764
[epoch19, step2767]: loss 0.419476
[epoch19, step2768]: loss 0.746900
[epoch19, step2769]: loss 0.631295
[epoch19, step2770]: loss 0.686896
[epoch19, step2771]: loss 0.729440
[epoch19, step2772]: loss 0.509890
[epoch19, step2773]: loss 0.599120
[epoch19, step2774]: loss 0.382287
[epoch19, step2775]: loss 0.618816
[epoch19, step2776]: loss 0.127743
[epoch19, step2777]: loss 0.457426
[epoch19, step2778]: loss 0.582039
[epoch19, step2779]: loss 0.367330
[epoch19, step2780]: loss 0.386291
[epoch19, step2781]: loss 0.517843
[epoch19, step2782]: loss 0.536470
[epoch19, step2783]: loss 0.692892
[epoch19, step2784]: loss 0.766553
[epoch19, step2785]: loss 0.580066
[epoch19, step2786]: loss 0.561691
[epoch19, step2787]: loss 0.676608
[epoch19, step2788]: loss 0.235104
[epoch19, step2789]: loss 0.493820
[epoch19, step2790]: loss 0.607125
[epoch19, step2791]: loss 0.547047
[epoch19, step2792]: loss 0.464732
[epoch19, step2793]: loss 0.550831
[epoch19, step2794]: loss 0.128543
[epoch19, step2795]: loss 0.674953
[epoch19, step2796]: loss 0.346011
[epoch19, step2797]: loss 0.538591
[epoch19, step2798]: loss 0.525427
[epoch19, step2799]: loss 0.646606
[epoch19, step2800]: loss 0.531572
[epoch19, step2801]: loss 0.324239
[epoch19, step2802]: loss 0.313774
[epoch19, step2803]: loss 0.458094
[epoch19, step2804]: loss 0.486931
[epoch19, step2805]: loss 0.404779
[epoch19, step2806]: loss 0.579346
[epoch19, step2807]: loss 0.262212
[epoch19, step2808]: loss 0.483482
[epoch19, step2809]: loss 0.671429
[epoch19, step2810]: loss 0.636503
[epoch19, step2811]: loss 0.642626
[epoch19, step2812]: loss 0.428192
[epoch19, step2813]: loss 0.435442
[epoch19, step2814]: loss 0.520693
[epoch19, step2815]: loss 0.677764
[epoch19, step2816]: loss 0.470912
[epoch19, step2817]: loss 0.498219
[epoch19, step2818]: loss 0.472119
[epoch19, step2819]: loss 0.497178
[epoch19, step2820]: loss 0.624099
[epoch19, step2821]: loss 0.648983
[epoch19, step2822]: loss 0.459528
[epoch19, step2823]: loss 0.708024
[epoch19, step2824]: loss 0.339189
[epoch19, step2825]: loss 0.551013
[epoch19, step2826]: loss 0.417008
[epoch19, step2827]: loss 0.559937
[epoch19, step2828]: loss 0.607272
[epoch19, step2829]: loss 0.646383
[epoch19, step2830]: loss 0.676918
[epoch19, step2831]: loss 0.411686
[epoch19, step2832]: loss 0.386853
[epoch19, step2833]: loss 0.615096
[epoch19, step2834]: loss 0.507656
[epoch19, step2835]: loss 0.587389
[epoch19, step2836]: loss 0.548416
[epoch19, step2837]: loss 0.387962
[epoch19, step2838]: loss 0.354470
[epoch19, step2839]: loss 0.591953
[epoch19, step2840]: loss 0.257542
[epoch19, step2841]: loss 0.536628
[epoch19, step2842]: loss 0.442883
[epoch19, step2843]: loss 0.423927
[epoch19, step2844]: loss 0.473688
[epoch19, step2845]: loss 0.678625
[epoch19, step2846]: loss 0.542915
[epoch19, step2847]: loss 0.530080
[epoch19, step2848]: loss 0.390119
[epoch19, step2849]: loss 0.544312
[epoch19, step2850]: loss 0.505381
[epoch19, step2851]: loss 0.566024
[epoch19, step2852]: loss 0.523465
[epoch19, step2853]: loss 0.616026
[epoch19, step2854]: loss 0.526268
[epoch19, step2855]: loss 0.484796
[epoch19, step2856]: loss 0.514116
[epoch19, step2857]: loss 0.452293
[epoch19, step2858]: loss 0.448278
[epoch19, step2859]: loss 0.440513
[epoch19, step2860]: loss 0.410902
[epoch19, step2861]: loss 0.771777
[epoch19, step2862]: loss 0.707396
[epoch19, step2863]: loss 0.606978
[epoch19, step2864]: loss 0.291467
[epoch19, step2865]: loss 0.658827
[epoch19, step2866]: loss 0.480735
[epoch19, step2867]: loss 0.586110
[epoch19, step2868]: loss 0.658946
[epoch19, step2869]: loss 0.613930
[epoch19, step2870]: loss 0.679511
[epoch19, step2871]: loss 0.534949
[epoch19, step2872]: loss 0.400783
[epoch19, step2873]: loss 0.412566
[epoch19, step2874]: loss 0.497635
[epoch19, step2875]: loss 0.343454
[epoch19, step2876]: loss 0.373664
[epoch19, step2877]: loss 0.571726
[epoch19, step2878]: loss 0.589608
[epoch19, step2879]: loss 0.529740
[epoch19, step2880]: loss 0.627049
[epoch19, step2881]: loss 0.486430
[epoch19, step2882]: loss 0.295260
[epoch19, step2883]: loss 0.679992
[epoch19, step2884]: loss 0.377838
[epoch19, step2885]: loss 0.730446
[epoch19, step2886]: loss 0.487499
[epoch19, step2887]: loss 0.456761
[epoch19, step2888]: loss 0.514260
[epoch19, step2889]: loss 0.476516
[epoch19, step2890]: loss 0.195960
[epoch19, step2891]: loss 0.506839
[epoch19, step2892]: loss 0.452519
[epoch19, step2893]: loss 0.602318
[epoch19, step2894]: loss 0.539071
[epoch19, step2895]: loss 0.553995
[epoch19, step2896]: loss 0.421699
[epoch19, step2897]: loss 0.786234
[epoch19, step2898]: loss 0.367066
[epoch19, step2899]: loss 0.526951
[epoch19, step2900]: loss 0.624884
[epoch19, step2901]: loss 0.529690
[epoch19, step2902]: loss 0.593234
[epoch19, step2903]: loss 0.479149
[epoch19, step2904]: loss 0.418485
[epoch19, step2905]: loss 0.585479
[epoch19, step2906]: loss 0.653504
[epoch19, step2907]: loss 0.627487
[epoch19, step2908]: loss 0.151808
[epoch19, step2909]: loss 0.320135
[epoch19, step2910]: loss 0.575235
[epoch19, step2911]: loss 0.419326
[epoch19, step2912]: loss 0.622419
[epoch19, step2913]: loss 0.727938
[epoch19, step2914]: loss 0.410677
[epoch19, step2915]: loss 0.576695
[epoch19, step2916]: loss 0.661024
[epoch19, step2917]: loss 0.636008
[epoch19, step2918]: loss 0.692585
[epoch19, step2919]: loss 0.625477
[epoch19, step2920]: loss 0.663068
[epoch19, step2921]: loss 0.631860
[epoch19, step2922]: loss 0.444287
[epoch19, step2923]: loss 0.460750
[epoch19, step2924]: loss 0.388932
[epoch19, step2925]: loss 0.601277
[epoch19, step2926]: loss 0.328258
[epoch19, step2927]: loss 0.475940
[epoch19, step2928]: loss 0.427343
[epoch19, step2929]: loss 0.544119
[epoch19, step2930]: loss 0.657239
[epoch19, step2931]: loss 0.446328
[epoch19, step2932]: loss 0.506734
[epoch19, step2933]: loss 0.554397
[epoch19, step2934]: loss 0.655839
[epoch19, step2935]: loss 0.207536
[epoch19, step2936]: loss 0.479353
[epoch19, step2937]: loss 0.525525
[epoch19, step2938]: loss 0.452900
[epoch19, step2939]: loss 0.624763
[epoch19, step2940]: loss 0.650434
[epoch19, step2941]: loss 0.396673
[epoch19, step2942]: loss 0.693643
[epoch19, step2943]: loss 0.718084
[epoch19, step2944]: loss 0.647145
[epoch19, step2945]: loss 0.567042
[epoch19, step2946]: loss 0.556504
[epoch19, step2947]: loss 0.257807
[epoch19, step2948]: loss 0.462655
[epoch19, step2949]: loss 0.373822
[epoch19, step2950]: loss 0.302307
[epoch19, step2951]: loss 0.610290
[epoch19, step2952]: loss 0.671141
[epoch19, step2953]: loss 0.432594
[epoch19, step2954]: loss 0.481514
[epoch19, step2955]: loss 0.416359
[epoch19, step2956]: loss 0.354707
[epoch19, step2957]: loss 0.514686
[epoch19, step2958]: loss 0.670953
[epoch19, step2959]: loss 0.533768
[epoch19, step2960]: loss 0.455632
[epoch19, step2961]: loss 0.740470
[epoch19, step2962]: loss 0.349975
[epoch19, step2963]: loss 0.696400
[epoch19, step2964]: loss 0.529248
[epoch19, step2965]: loss 0.527038
[epoch19, step2966]: loss 0.644196
[epoch19, step2967]: loss 0.600500
[epoch19, step2968]: loss 0.712976
[epoch19, step2969]: loss 0.432310
[epoch19, step2970]: loss 0.555131
[epoch19, step2971]: loss 0.655958
[epoch19, step2972]: loss 0.588246
[epoch19, step2973]: loss 0.332494
[epoch19, step2974]: loss 0.593712
[epoch19, step2975]: loss 0.677782
[epoch19, step2976]: loss 0.775965
[epoch19, step2977]: loss 0.324521
[epoch19, step2978]: loss 0.587818
[epoch19, step2979]: loss 0.153919
[epoch19, step2980]: loss 0.508395
[epoch19, step2981]: loss 0.602630
[epoch19, step2982]: loss 0.621744
[epoch19, step2983]: loss 0.577961
[epoch19, step2984]: loss 0.623981
[epoch19, step2985]: loss 0.096343
[epoch19, step2986]: loss 0.572901
[epoch19, step2987]: loss 0.465635
[epoch19, step2988]: loss 0.711968
[epoch19, step2989]: loss 0.660104
[epoch19, step2990]: loss 0.571103
[epoch19, step2991]: loss 0.385269
[epoch19, step2992]: loss 0.472610
[epoch19, step2993]: loss 0.527112
[epoch19, step2994]: loss 0.475407
[epoch19, step2995]: loss 0.515649
[epoch19, step2996]: loss 0.695809
[epoch19, step2997]: loss 0.492928
[epoch19, step2998]: loss 0.649181
[epoch19, step2999]: loss 0.109999
[epoch19, step3000]: loss 0.615851
[epoch19, step3001]: loss 0.484006
[epoch19, step3002]: loss 0.780201
[epoch19, step3003]: loss 0.422655
[epoch19, step3004]: loss 0.491537
[epoch19, step3005]: loss 0.488249
[epoch19, step3006]: loss 0.547457
[epoch19, step3007]: loss 0.290883
[epoch19, step3008]: loss 0.527772
[epoch19, step3009]: loss 0.608238
[epoch19, step3010]: loss 0.407684
[epoch19, step3011]: loss 0.419686
[epoch19, step3012]: loss 0.740513
[epoch19, step3013]: loss 0.508170
[epoch19, step3014]: loss 0.378326
[epoch19, step3015]: loss 0.638052
[epoch19, step3016]: loss 0.355153
[epoch19, step3017]: loss 0.610450
[epoch19, step3018]: loss 0.567680
[epoch19, step3019]: loss 0.444149
[epoch19, step3020]: loss 0.628690
[epoch19, step3021]: loss 0.599610
[epoch19, step3022]: loss 0.375063
[epoch19, step3023]: loss 0.604530
[epoch19, step3024]: loss 0.136644
[epoch19, step3025]: loss 0.613858
[epoch19, step3026]: loss 0.289712
[epoch19, step3027]: loss 0.620406
[epoch19, step3028]: loss 0.652196
[epoch19, step3029]: loss 0.631793
[epoch19, step3030]: loss 0.263654
[epoch19, step3031]: loss 0.273331
[epoch19, step3032]: loss 0.483544
[epoch19, step3033]: loss 0.593565
[epoch19, step3034]: loss 0.520523
[epoch19, step3035]: loss 0.753808
[epoch19, step3036]: loss 0.483657
[epoch19, step3037]: loss 0.473848
[epoch19, step3038]: loss 0.462162
[epoch19, step3039]: loss 0.328857
[epoch19, step3040]: loss 0.770939
[epoch19, step3041]: loss 0.519531
[epoch19, step3042]: loss 0.642553
[epoch19, step3043]: loss 0.532296
[epoch19, step3044]: loss 0.409700
[epoch19, step3045]: loss 0.613030
[epoch19, step3046]: loss 0.535244
[epoch19, step3047]: loss 0.396470
[epoch19, step3048]: loss 0.477934
[epoch19, step3049]: loss 0.581439
[epoch19, step3050]: loss 0.465211
[epoch19, step3051]: loss 0.420414
[epoch19, step3052]: loss 0.540666
[epoch19, step3053]: loss 0.625502
[epoch19, step3054]: loss 0.305617
[epoch19, step3055]: loss 0.561532
[epoch19, step3056]: loss 0.463321
[epoch19, step3057]: loss 0.708092
[epoch19, step3058]: loss 0.573604
[epoch19, step3059]: loss 0.472664
[epoch19, step3060]: loss 0.565167
[epoch19, step3061]: loss 0.621100
[epoch19, step3062]: loss 0.224387
[epoch19, step3063]: loss 0.460312
[epoch19, step3064]: loss 0.461869
[epoch19, step3065]: loss 0.594501
[epoch19, step3066]: loss 0.604043
[epoch19, step3067]: loss 0.408315
[epoch19, step3068]: loss 0.535461
[epoch19, step3069]: loss 0.173656
[epoch19, step3070]: loss 0.404632
[epoch19, step3071]: loss 0.603056
[epoch19, step3072]: loss 0.465886
[epoch19, step3073]: loss 0.402174
[epoch19, step3074]: loss 0.621073
[epoch19, step3075]: loss 0.738164
[epoch19, step3076]: loss 0.352513

[epoch19]: avg loss 0.352513

[epoch20, step1]: loss 0.401627
[epoch20, step2]: loss 0.504860
[epoch20, step3]: loss 0.383465
[epoch20, step4]: loss 0.637261
[epoch20, step5]: loss 0.554795
[epoch20, step6]: loss 0.599857
[epoch20, step7]: loss 0.486873
[epoch20, step8]: loss 0.663860
[epoch20, step9]: loss 0.585498
[epoch20, step10]: loss 0.639185
[epoch20, step11]: loss 0.444501
[epoch20, step12]: loss 0.383714
[epoch20, step13]: loss 0.548415
[epoch20, step14]: loss 0.649765
[epoch20, step15]: loss 0.575621
[epoch20, step16]: loss 0.792361
[epoch20, step17]: loss 0.568711
[epoch20, step18]: loss 0.586582
[epoch20, step19]: loss 0.629986
[epoch20, step20]: loss 0.871093
[epoch20, step21]: loss 0.524049
[epoch20, step22]: loss 0.664314
[epoch20, step23]: loss 0.679486
[epoch20, step24]: loss 0.392193
[epoch20, step25]: loss 0.437011
[epoch20, step26]: loss 0.479409
[epoch20, step27]: loss 0.496544
[epoch20, step28]: loss 0.483790
[epoch20, step29]: loss 0.569713
[epoch20, step30]: loss 0.779277
[epoch20, step31]: loss 0.365903
[epoch20, step32]: loss 0.493669
[epoch20, step33]: loss 0.529872
[epoch20, step34]: loss 0.523939
[epoch20, step35]: loss 0.424147
[epoch20, step36]: loss 0.734053
[epoch20, step37]: loss 0.578355
[epoch20, step38]: loss 0.654710
[epoch20, step39]: loss 0.614696
[epoch20, step40]: loss 0.581959
[epoch20, step41]: loss 0.549577
[epoch20, step42]: loss 0.700593
[epoch20, step43]: loss 0.677544
[epoch20, step44]: loss 0.274171
[epoch20, step45]: loss 0.727201
[epoch20, step46]: loss 0.344575
[epoch20, step47]: loss 0.491445
[epoch20, step48]: loss 0.349337
[epoch20, step49]: loss 0.678534
[epoch20, step50]: loss 0.518105
[epoch20, step51]: loss 0.364164
[epoch20, step52]: loss 0.368680
[epoch20, step53]: loss 0.477253
[epoch20, step54]: loss 0.625347
[epoch20, step55]: loss 0.558826
[epoch20, step56]: loss 0.312073
[epoch20, step57]: loss 0.398843
[epoch20, step58]: loss 0.527335
[epoch20, step59]: loss 0.356435
[epoch20, step60]: loss 0.661968
[epoch20, step61]: loss 0.454466
[epoch20, step62]: loss 0.552601
[epoch20, step63]: loss 0.352309
[epoch20, step64]: loss 0.285111
[epoch20, step65]: loss 0.392380
[epoch20, step66]: loss 0.324444
[epoch20, step67]: loss 0.483602
[epoch20, step68]: loss 0.735219
[epoch20, step69]: loss 0.429909
[epoch20, step70]: loss 0.496576
[epoch20, step71]: loss 0.532820
[epoch20, step72]: loss 0.426189
[epoch20, step73]: loss 0.544548
[epoch20, step74]: loss 0.432086
[epoch20, step75]: loss 0.708816
[epoch20, step76]: loss 0.549889
[epoch20, step77]: loss 0.440721
[epoch20, step78]: loss 0.111998
[epoch20, step79]: loss 0.216668
[epoch20, step80]: loss 0.699389
[epoch20, step81]: loss 0.448681
[epoch20, step82]: loss 0.475023
[epoch20, step83]: loss 0.791041
[epoch20, step84]: loss 0.682381
[epoch20, step85]: loss 0.499413
[epoch20, step86]: loss 0.522297
[epoch20, step87]: loss 0.481446
[epoch20, step88]: loss 0.669390
[epoch20, step89]: loss 0.669538
[epoch20, step90]: loss 0.463171
[epoch20, step91]: loss 0.593672
[epoch20, step92]: loss 0.598862
[epoch20, step93]: loss 0.748894
[epoch20, step94]: loss 0.561021
[epoch20, step95]: loss 0.474259
[epoch20, step96]: loss 0.406983
[epoch20, step97]: loss 0.452021
[epoch20, step98]: loss 0.564708
[epoch20, step99]: loss 0.666988
[epoch20, step100]: loss 0.322225
[epoch20, step101]: loss 0.544617
[epoch20, step102]: loss 0.638533
[epoch20, step103]: loss 0.398229
[epoch20, step104]: loss 0.419635
[epoch20, step105]: loss 0.676486
[epoch20, step106]: loss 0.499062
[epoch20, step107]: loss 0.481323
[epoch20, step108]: loss 0.448556
[epoch20, step109]: loss 0.576258
[epoch20, step110]: loss 0.445350
[epoch20, step111]: loss 0.679123
[epoch20, step112]: loss 0.580200
[epoch20, step113]: loss 0.639550
[epoch20, step114]: loss 0.295011
[epoch20, step115]: loss 0.494514
[epoch20, step116]: loss 0.683353
[epoch20, step117]: loss 0.657033
[epoch20, step118]: loss 0.655305
[epoch20, step119]: loss 0.435880
[epoch20, step120]: loss 0.692679
[epoch20, step121]: loss 0.655698
[epoch20, step122]: loss 0.700407
[epoch20, step123]: loss 0.442256
[epoch20, step124]: loss 0.406443
[epoch20, step125]: loss 0.480715
[epoch20, step126]: loss 0.444373
[epoch20, step127]: loss 0.698109
[epoch20, step128]: loss 0.440511
[epoch20, step129]: loss 0.393330
[epoch20, step130]: loss 0.498517
[epoch20, step131]: loss 0.690252
[epoch20, step132]: loss 0.388053
[epoch20, step133]: loss 0.473897
[epoch20, step134]: loss 0.480337
[epoch20, step135]: loss 0.621524
[epoch20, step136]: loss 0.438162
[epoch20, step137]: loss 0.595443
[epoch20, step138]: loss 0.331761
[epoch20, step139]: loss 0.524416
[epoch20, step140]: loss 0.587167
[epoch20, step141]: loss 0.461026
[epoch20, step142]: loss 0.530608
[epoch20, step143]: loss 0.216277
[epoch20, step144]: loss 0.437949
[epoch20, step145]: loss 0.597413
[epoch20, step146]: loss 0.582422
[epoch20, step147]: loss 0.576276
[epoch20, step148]: loss 0.365880
[epoch20, step149]: loss 0.550912
[epoch20, step150]: loss 0.562316
[epoch20, step151]: loss 0.535438
[epoch20, step152]: loss 0.294101
[epoch20, step153]: loss 0.543250
[epoch20, step154]: loss 0.834239
[epoch20, step155]: loss 0.681976
[epoch20, step156]: loss 0.561971
[epoch20, step157]: loss 0.531624
[epoch20, step158]: loss 0.638855
[epoch20, step159]: loss 0.410324
[epoch20, step160]: loss 0.468723
[epoch20, step161]: loss 0.445240
[epoch20, step162]: loss 0.657533
[epoch20, step163]: loss 0.480907
[epoch20, step164]: loss 0.420736
[epoch20, step165]: loss 0.319000
[epoch20, step166]: loss 0.581342
[epoch20, step167]: loss 0.477245
[epoch20, step168]: loss 0.468496
[epoch20, step169]: loss 0.263143
[epoch20, step170]: loss 0.653003
[epoch20, step171]: loss 0.588961
[epoch20, step172]: loss 0.402126
[epoch20, step173]: loss 0.580284
[epoch20, step174]: loss 0.365392
[epoch20, step175]: loss 0.652794
[epoch20, step176]: loss 0.352457
[epoch20, step177]: loss 0.365038
[epoch20, step178]: loss 0.770373
[epoch20, step179]: loss 0.475366
[epoch20, step180]: loss 0.404113
[epoch20, step181]: loss 0.469537
[epoch20, step182]: loss 0.642386
[epoch20, step183]: loss 0.580459
[epoch20, step184]: loss 0.332600
[epoch20, step185]: loss 0.463548
[epoch20, step186]: loss 0.403322
[epoch20, step187]: loss 0.649032
[epoch20, step188]: loss 0.558532
[epoch20, step189]: loss 0.507249
[epoch20, step190]: loss 0.403222
[epoch20, step191]: loss 0.564760
[epoch20, step192]: loss 0.570103
[epoch20, step193]: loss 0.673182
[epoch20, step194]: loss 0.525454
[epoch20, step195]: loss 0.778928
[epoch20, step196]: loss 0.556283
[epoch20, step197]: loss 0.564657
[epoch20, step198]: loss 0.892141
[epoch20, step199]: loss 0.583584
[epoch20, step200]: loss 0.371390
[epoch20, step201]: loss 0.503528
[epoch20, step202]: loss 0.526007
[epoch20, step203]: loss 0.723158
[epoch20, step204]: loss 0.398503
[epoch20, step205]: loss 0.412330
[epoch20, step206]: loss 0.596726
[epoch20, step207]: loss 0.717356
[epoch20, step208]: loss 0.571222
[epoch20, step209]: loss 0.645683
[epoch20, step210]: loss 0.536218
[epoch20, step211]: loss 0.679593
[epoch20, step212]: loss 0.576170
[epoch20, step213]: loss 0.488942
[epoch20, step214]: loss 0.545195
[epoch20, step215]: loss 0.598376
[epoch20, step216]: loss 0.346898
[epoch20, step217]: loss 0.490335
[epoch20, step218]: loss 0.545001
[epoch20, step219]: loss 0.259882
[epoch20, step220]: loss 0.605154
[epoch20, step221]: loss 0.635588
[epoch20, step222]: loss 0.413392
[epoch20, step223]: loss 0.542390
[epoch20, step224]: loss 0.570674
[epoch20, step225]: loss 0.473283
[epoch20, step226]: loss 0.496734
[epoch20, step227]: loss 0.380530
[epoch20, step228]: loss 0.618378
[epoch20, step229]: loss 0.500283
[epoch20, step230]: loss 0.497007
[epoch20, step231]: loss 0.297960
[epoch20, step232]: loss 0.616580
[epoch20, step233]: loss 0.400033
[epoch20, step234]: loss 0.470004
[epoch20, step235]: loss 0.549043
[epoch20, step236]: loss 0.687284
[epoch20, step237]: loss 0.504990
[epoch20, step238]: loss 0.443122
[epoch20, step239]: loss 0.330348
[epoch20, step240]: loss 0.521492
[epoch20, step241]: loss 0.372187
[epoch20, step242]: loss 0.768242
[epoch20, step243]: loss 0.540678
[epoch20, step244]: loss 0.597827
[epoch20, step245]: loss 0.727008
[epoch20, step246]: loss 0.656811
[epoch20, step247]: loss 0.446087
[epoch20, step248]: loss 0.661370
[epoch20, step249]: loss 0.461265
[epoch20, step250]: loss 0.490910
[epoch20, step251]: loss 0.665922
[epoch20, step252]: loss 0.280177
[epoch20, step253]: loss 0.438699
[epoch20, step254]: loss 0.530953
[epoch20, step255]: loss 0.394316
[epoch20, step256]: loss 0.317521
[epoch20, step257]: loss 0.401814
[epoch20, step258]: loss 0.603388
[epoch20, step259]: loss 0.652145
[epoch20, step260]: loss 0.736379
[epoch20, step261]: loss 0.463683
[epoch20, step262]: loss 0.486642
[epoch20, step263]: loss 0.730890
[epoch20, step264]: loss 0.606529
[epoch20, step265]: loss 0.655414
[epoch20, step266]: loss 0.470158
[epoch20, step267]: loss 0.339874
[epoch20, step268]: loss 0.443175
[epoch20, step269]: loss 0.567598
[epoch20, step270]: loss 0.685861
[epoch20, step271]: loss 0.615291
[epoch20, step272]: loss 0.594939
[epoch20, step273]: loss 0.372608
[epoch20, step274]: loss 0.646316
[epoch20, step275]: loss 0.594183
[epoch20, step276]: loss 0.675256
[epoch20, step277]: loss 0.483220
[epoch20, step278]: loss 0.588599
[epoch20, step279]: loss 0.416256
[epoch20, step280]: loss 0.617911
[epoch20, step281]: loss 0.360776
[epoch20, step282]: loss 0.608399
[epoch20, step283]: loss 0.476467
[epoch20, step284]: loss 0.544234
[epoch20, step285]: loss 0.715720
[epoch20, step286]: loss 0.722708
[epoch20, step287]: loss 0.220712
[epoch20, step288]: loss 0.755368
[epoch20, step289]: loss 0.522246
[epoch20, step290]: loss 0.268874
[epoch20, step291]: loss 0.440641
[epoch20, step292]: loss 0.583016
[epoch20, step293]: loss 0.384887
[epoch20, step294]: loss 0.459345
[epoch20, step295]: loss 0.553767
[epoch20, step296]: loss 0.667824
[epoch20, step297]: loss 0.779447
[epoch20, step298]: loss 0.580196
[epoch20, step299]: loss 0.344289
[epoch20, step300]: loss 0.641528
[epoch20, step301]: loss 0.504492
[epoch20, step302]: loss 0.293134
[epoch20, step303]: loss 0.336923
[epoch20, step304]: loss 0.500964
[epoch20, step305]: loss 0.521504
[epoch20, step306]: loss 0.697953
[epoch20, step307]: loss 0.260534
[epoch20, step308]: loss 0.745305
[epoch20, step309]: loss 0.448870
[epoch20, step310]: loss 0.566296
[epoch20, step311]: loss 0.395310
[epoch20, step312]: loss 0.671767
[epoch20, step313]: loss 0.685493
[epoch20, step314]: loss 0.440141
[epoch20, step315]: loss 0.460700
[epoch20, step316]: loss 0.342570
[epoch20, step317]: loss 0.459206
[epoch20, step318]: loss 0.257371
[epoch20, step319]: loss 0.834404
[epoch20, step320]: loss 0.459696
[epoch20, step321]: loss 0.519929
[epoch20, step322]: loss 0.493521
[epoch20, step323]: loss 0.762435
[epoch20, step324]: loss 0.539621
[epoch20, step325]: loss 0.316627
[epoch20, step326]: loss 0.585163
[epoch20, step327]: loss 0.674977
[epoch20, step328]: loss 0.447926
[epoch20, step329]: loss 0.646920
[epoch20, step330]: loss 0.485531
[epoch20, step331]: loss 0.310178
[epoch20, step332]: loss 0.486600
[epoch20, step333]: loss 0.594377
[epoch20, step334]: loss 0.463739
[epoch20, step335]: loss 0.483350
[epoch20, step336]: loss 0.599627
[epoch20, step337]: loss 0.526350
[epoch20, step338]: loss 0.510857
[epoch20, step339]: loss 0.269410
[epoch20, step340]: loss 0.642137
[epoch20, step341]: loss 0.618661
[epoch20, step342]: loss 0.412511
[epoch20, step343]: loss 0.345104
[epoch20, step344]: loss 0.670561
[epoch20, step345]: loss 0.654752
[epoch20, step346]: loss 0.619773
[epoch20, step347]: loss 0.505910
[epoch20, step348]: loss 0.539328
[epoch20, step349]: loss 0.588810
[epoch20, step350]: loss 0.382688
[epoch20, step351]: loss 0.493849
[epoch20, step352]: loss 0.620511
[epoch20, step353]: loss 0.695358
[epoch20, step354]: loss 0.566450
[epoch20, step355]: loss 0.576678
[epoch20, step356]: loss 0.614397
[epoch20, step357]: loss 0.617492
[epoch20, step358]: loss 0.257192
[epoch20, step359]: loss 0.429024
[epoch20, step360]: loss 0.267127
[epoch20, step361]: loss 0.664803
[epoch20, step362]: loss 0.567374
[epoch20, step363]: loss 0.587705
[epoch20, step364]: loss 0.541620
[epoch20, step365]: loss 0.469132
[epoch20, step366]: loss 0.528604
[epoch20, step367]: loss 0.523674
[epoch20, step368]: loss 0.530142
[epoch20, step369]: loss 0.266118
[epoch20, step370]: loss 0.573174
[epoch20, step371]: loss 0.763407
[epoch20, step372]: loss 0.638700
[epoch20, step373]: loss 0.752724
[epoch20, step374]: loss 0.563361
[epoch20, step375]: loss 0.558697
[epoch20, step376]: loss 0.299644
[epoch20, step377]: loss 0.449707
[epoch20, step378]: loss 0.496361
[epoch20, step379]: loss 0.647141
[epoch20, step380]: loss 0.742504
[epoch20, step381]: loss 0.443720
[epoch20, step382]: loss 0.577677
[epoch20, step383]: loss 0.688037
[epoch20, step384]: loss 0.592527
[epoch20, step385]: loss 0.542967
[epoch20, step386]: loss 0.398064
[epoch20, step387]: loss 0.408379
[epoch20, step388]: loss 0.365993
[epoch20, step389]: loss 0.396805
[epoch20, step390]: loss 0.381342
[epoch20, step391]: loss 0.526232
[epoch20, step392]: loss 0.568682
[epoch20, step393]: loss 0.322353
[epoch20, step394]: loss 0.486456
[epoch20, step395]: loss 0.574016
[epoch20, step396]: loss 0.731473
[epoch20, step397]: loss 0.408008
[epoch20, step398]: loss 0.540599
[epoch20, step399]: loss 0.453185
[epoch20, step400]: loss 0.516896
[epoch20, step401]: loss 0.573470
[epoch20, step402]: loss 0.620970
[epoch20, step403]: loss 0.707182
[epoch20, step404]: loss 0.608417
[epoch20, step405]: loss 0.413123
[epoch20, step406]: loss 0.641815
[epoch20, step407]: loss 0.624065
[epoch20, step408]: loss 0.570238
[epoch20, step409]: loss 0.566524
[epoch20, step410]: loss 0.559526
[epoch20, step411]: loss 0.614811
[epoch20, step412]: loss 0.643906
[epoch20, step413]: loss 0.623958
[epoch20, step414]: loss 0.358498
[epoch20, step415]: loss 0.284617
[epoch20, step416]: loss 0.581848
[epoch20, step417]: loss 0.397662
[epoch20, step418]: loss 0.570633
[epoch20, step419]: loss 0.680569
[epoch20, step420]: loss 0.344715
[epoch20, step421]: loss 0.420627
[epoch20, step422]: loss 0.448038
[epoch20, step423]: loss 0.523955
[epoch20, step424]: loss 0.600315
[epoch20, step425]: loss 0.600983
[epoch20, step426]: loss 0.666634
[epoch20, step427]: loss 0.541060
[epoch20, step428]: loss 0.500603
[epoch20, step429]: loss 0.605286
[epoch20, step430]: loss 0.411754
[epoch20, step431]: loss 0.513091
[epoch20, step432]: loss 0.442386
[epoch20, step433]: loss 0.525652
[epoch20, step434]: loss 0.140193
[epoch20, step435]: loss 0.437401
[epoch20, step436]: loss 0.653976
[epoch20, step437]: loss 0.291844
[epoch20, step438]: loss 0.317051
[epoch20, step439]: loss 0.597349
[epoch20, step440]: loss 0.435470
[epoch20, step441]: loss 0.374724
[epoch20, step442]: loss 0.466126
[epoch20, step443]: loss 0.553425
[epoch20, step444]: loss 0.349571
[epoch20, step445]: loss 0.646402
[epoch20, step446]: loss 0.655103
[epoch20, step447]: loss 0.342064
[epoch20, step448]: loss 0.655447
[epoch20, step449]: loss 0.545353
[epoch20, step450]: loss 0.468856
[epoch20, step451]: loss 0.231376
[epoch20, step452]: loss 0.601409
[epoch20, step453]: loss 0.554590
[epoch20, step454]: loss 0.508410
[epoch20, step455]: loss 0.740427
[epoch20, step456]: loss 0.511432
[epoch20, step457]: loss 0.437818
[epoch20, step458]: loss 0.612803
[epoch20, step459]: loss 0.720106
[epoch20, step460]: loss 0.308209
[epoch20, step461]: loss 0.687570
[epoch20, step462]: loss 0.512040
[epoch20, step463]: loss 0.655818
[epoch20, step464]: loss 0.454042
[epoch20, step465]: loss 0.655378
[epoch20, step466]: loss 0.494439
[epoch20, step467]: loss 0.430366
[epoch20, step468]: loss 0.408199
[epoch20, step469]: loss 0.764343
[epoch20, step470]: loss 0.413924
[epoch20, step471]: loss 0.436657
[epoch20, step472]: loss 0.508985
[epoch20, step473]: loss 0.339861
[epoch20, step474]: loss 0.691282
[epoch20, step475]: loss 0.532791
[epoch20, step476]: loss 0.459421
[epoch20, step477]: loss 0.418952
[epoch20, step478]: loss 0.605888
[epoch20, step479]: loss 0.730788
[epoch20, step480]: loss 0.366177
[epoch20, step481]: loss 0.338835
[epoch20, step482]: loss 0.561484
[epoch20, step483]: loss 0.401404
[epoch20, step484]: loss 0.424008
[epoch20, step485]: loss 0.324892
[epoch20, step486]: loss 0.686264
[epoch20, step487]: loss 0.425162
[epoch20, step488]: loss 0.486744
[epoch20, step489]: loss 0.548010
[epoch20, step490]: loss 0.557225
[epoch20, step491]: loss 0.532121
[epoch20, step492]: loss 0.608027
[epoch20, step493]: loss 0.549291
[epoch20, step494]: loss 0.378895
[epoch20, step495]: loss 0.533379
[epoch20, step496]: loss 0.707229
[epoch20, step497]: loss 0.474074
[epoch20, step498]: loss 0.581812
[epoch20, step499]: loss 0.401722
[epoch20, step500]: loss 0.585485
[epoch20, step501]: loss 0.542227
[epoch20, step502]: loss 0.282039
[epoch20, step503]: loss 0.642110
[epoch20, step504]: loss 0.732427
[epoch20, step505]: loss 0.423322
[epoch20, step506]: loss 0.287603
[epoch20, step507]: loss 0.646484
[epoch20, step508]: loss 0.619297
[epoch20, step509]: loss 0.467395
[epoch20, step510]: loss 0.464251
[epoch20, step511]: loss 0.594404
[epoch20, step512]: loss 0.444950
[epoch20, step513]: loss 0.371035
[epoch20, step514]: loss 0.659957
[epoch20, step515]: loss 0.359892
[epoch20, step516]: loss 0.544344
[epoch20, step517]: loss 0.602498
[epoch20, step518]: loss 0.887510
[epoch20, step519]: loss 0.541859
[epoch20, step520]: loss 0.618613
[epoch20, step521]: loss 0.657934
[epoch20, step522]: loss 0.311055
[epoch20, step523]: loss 0.493799
[epoch20, step524]: loss 0.520882
[epoch20, step525]: loss 0.427506
[epoch20, step526]: loss 0.588347
[epoch20, step527]: loss 0.857936
[epoch20, step528]: loss 0.638909
[epoch20, step529]: loss 0.468349
[epoch20, step530]: loss 0.712044
[epoch20, step531]: loss 0.263205
[epoch20, step532]: loss 0.498932
[epoch20, step533]: loss 0.457690
[epoch20, step534]: loss 0.358034
[epoch20, step535]: loss 0.601498
[epoch20, step536]: loss 0.474168
[epoch20, step537]: loss 0.402710
[epoch20, step538]: loss 0.684713
[epoch20, step539]: loss 0.575018
[epoch20, step540]: loss 0.638659
[epoch20, step541]: loss 0.541201
[epoch20, step542]: loss 0.258108
[epoch20, step543]: loss 0.586619
[epoch20, step544]: loss 0.464346
[epoch20, step545]: loss 0.707566
[epoch20, step546]: loss 0.517934
[epoch20, step547]: loss 0.572726
[epoch20, step548]: loss 0.571914
[epoch20, step549]: loss 0.663835
[epoch20, step550]: loss 0.667754
[epoch20, step551]: loss 0.373859
[epoch20, step552]: loss 0.740895
[epoch20, step553]: loss 0.550458
[epoch20, step554]: loss 0.480213
[epoch20, step555]: loss 0.416849
[epoch20, step556]: loss 0.504620
[epoch20, step557]: loss 0.519720
[epoch20, step558]: loss 0.661958
[epoch20, step559]: loss 0.627394
[epoch20, step560]: loss 0.554595
[epoch20, step561]: loss 0.471827
[epoch20, step562]: loss 0.429697
[epoch20, step563]: loss 0.391781
[epoch20, step564]: loss 0.503358
[epoch20, step565]: loss 0.484134
[epoch20, step566]: loss 0.494723
[epoch20, step567]: loss 0.604989
[epoch20, step568]: loss 0.474947
[epoch20, step569]: loss 0.660407
[epoch20, step570]: loss 0.540474
[epoch20, step571]: loss 0.701582
[epoch20, step572]: loss 0.281890
[epoch20, step573]: loss 0.635406
[epoch20, step574]: loss 0.446641
[epoch20, step575]: loss 0.216897
[epoch20, step576]: loss 0.387328
[epoch20, step577]: loss 0.592928
[epoch20, step578]: loss 0.359687
[epoch20, step579]: loss 0.398660
[epoch20, step580]: loss 0.596865
[epoch20, step581]: loss 0.335254
[epoch20, step582]: loss 0.539018
[epoch20, step583]: loss 0.480217
[epoch20, step584]: loss 0.633935
[epoch20, step585]: loss 0.150871
[epoch20, step586]: loss 0.390670
[epoch20, step587]: loss 0.451503
[epoch20, step588]: loss 0.449232
[epoch20, step589]: loss 0.714091
[epoch20, step590]: loss 0.696435
[epoch20, step591]: loss 0.453335
[epoch20, step592]: loss 0.631410
[epoch20, step593]: loss 0.264288
[epoch20, step594]: loss 0.576961
[epoch20, step595]: loss 0.311358
[epoch20, step596]: loss 0.608917
[epoch20, step597]: loss 0.598076
[epoch20, step598]: loss 0.545118
[epoch20, step599]: loss 0.391371
[epoch20, step600]: loss 0.481399
[epoch20, step601]: loss 0.523817
[epoch20, step602]: loss 0.598828
[epoch20, step603]: loss 0.385245
[epoch20, step604]: loss 0.225665
[epoch20, step605]: loss 0.530051
[epoch20, step606]: loss 0.483900
[epoch20, step607]: loss 0.516550
[epoch20, step608]: loss 0.602448
[epoch20, step609]: loss 0.706758
[epoch20, step610]: loss 0.565557
[epoch20, step611]: loss 0.655392
[epoch20, step612]: loss 0.485605
[epoch20, step613]: loss 0.610588
[epoch20, step614]: loss 0.421674
[epoch20, step615]: loss 0.596857
[epoch20, step616]: loss 0.677881
[epoch20, step617]: loss 0.676242
[epoch20, step618]: loss 0.478424
[epoch20, step619]: loss 0.452782
[epoch20, step620]: loss 0.576002
[epoch20, step621]: loss 0.636917
[epoch20, step622]: loss 0.423805
[epoch20, step623]: loss 0.396587
[epoch20, step624]: loss 0.602492
[epoch20, step625]: loss 0.662890
[epoch20, step626]: loss 0.604176
[epoch20, step627]: loss 0.315166
[epoch20, step628]: loss 0.491621
[epoch20, step629]: loss 0.239286
[epoch20, step630]: loss 0.471698
[epoch20, step631]: loss 0.701026
[epoch20, step632]: loss 0.461214
[epoch20, step633]: loss 0.717337
[epoch20, step634]: loss 0.448714
[epoch20, step635]: loss 0.362626
[epoch20, step636]: loss 0.781733
[epoch20, step637]: loss 0.693122
[epoch20, step638]: loss 0.597921
[epoch20, step639]: loss 0.385611
[epoch20, step640]: loss 0.592986
[epoch20, step641]: loss 0.637388
[epoch20, step642]: loss 0.456278
[epoch20, step643]: loss 0.813559
[epoch20, step644]: loss 0.550103
[epoch20, step645]: loss 0.378664
[epoch20, step646]: loss 0.677530
[epoch20, step647]: loss 0.610140
[epoch20, step648]: loss 0.396763
[epoch20, step649]: loss 0.771772
[epoch20, step650]: loss 0.544823
[epoch20, step651]: loss 0.586630
[epoch20, step652]: loss 0.630970
[epoch20, step653]: loss 0.684270
[epoch20, step654]: loss 0.515809
[epoch20, step655]: loss 0.715462
[epoch20, step656]: loss 0.663825
[epoch20, step657]: loss 0.675925
[epoch20, step658]: loss 0.395598
[epoch20, step659]: loss 0.547771
[epoch20, step660]: loss 0.702860
[epoch20, step661]: loss 0.736599
[epoch20, step662]: loss 0.473717
[epoch20, step663]: loss 0.622500
[epoch20, step664]: loss 0.554152
[epoch20, step665]: loss 0.421360
[epoch20, step666]: loss 0.545067
[epoch20, step667]: loss 0.603582
[epoch20, step668]: loss 0.496739
[epoch20, step669]: loss 0.676228
[epoch20, step670]: loss 0.514300
[epoch20, step671]: loss 0.378622
[epoch20, step672]: loss 0.526769
[epoch20, step673]: loss 0.585628
[epoch20, step674]: loss 0.474862
[epoch20, step675]: loss 0.380158
[epoch20, step676]: loss 0.418895
[epoch20, step677]: loss 0.573856
[epoch20, step678]: loss 0.252371
[epoch20, step679]: loss 0.583148
[epoch20, step680]: loss 0.743939
[epoch20, step681]: loss 0.512436
[epoch20, step682]: loss 0.282841
[epoch20, step683]: loss 0.585933
[epoch20, step684]: loss 0.315226
[epoch20, step685]: loss 0.776008
[epoch20, step686]: loss 0.654974
[epoch20, step687]: loss 0.573236
[epoch20, step688]: loss 0.589038
[epoch20, step689]: loss 0.257113
[epoch20, step690]: loss 0.727608
[epoch20, step691]: loss 0.592193
[epoch20, step692]: loss 0.206037
[epoch20, step693]: loss 0.663582
[epoch20, step694]: loss 0.695239
[epoch20, step695]: loss 0.489730
[epoch20, step696]: loss 0.720196
[epoch20, step697]: loss 0.625291
[epoch20, step698]: loss 0.428999
[epoch20, step699]: loss 0.471905
[epoch20, step700]: loss 0.482000
[epoch20, step701]: loss 0.314348
[epoch20, step702]: loss 0.682878
[epoch20, step703]: loss 0.444013
[epoch20, step704]: loss 0.661783
[epoch20, step705]: loss 0.701868
[epoch20, step706]: loss 0.412462
[epoch20, step707]: loss 0.661789
[epoch20, step708]: loss 0.518292
[epoch20, step709]: loss 0.319589
[epoch20, step710]: loss 0.339458
[epoch20, step711]: loss 0.534583
[epoch20, step712]: loss 0.514928
[epoch20, step713]: loss 0.492321
[epoch20, step714]: loss 0.374064
[epoch20, step715]: loss 0.413481
[epoch20, step716]: loss 0.435717
[epoch20, step717]: loss 0.542586
[epoch20, step718]: loss 0.490065
[epoch20, step719]: loss 0.146152
[epoch20, step720]: loss 0.578149
[epoch20, step721]: loss 0.521827
[epoch20, step722]: loss 0.585497
[epoch20, step723]: loss 0.351660
[epoch20, step724]: loss 0.601817
[epoch20, step725]: loss 0.814361
[epoch20, step726]: loss 0.600524
[epoch20, step727]: loss 0.504760
[epoch20, step728]: loss 0.529379
[epoch20, step729]: loss 0.637407
[epoch20, step730]: loss 0.721699
[epoch20, step731]: loss 0.665598
[epoch20, step732]: loss 0.720274
[epoch20, step733]: loss 0.472719
[epoch20, step734]: loss 0.777664
[epoch20, step735]: loss 0.648056
[epoch20, step736]: loss 0.681864
[epoch20, step737]: loss 0.599740
[epoch20, step738]: loss 0.706935
[epoch20, step739]: loss 0.554616
[epoch20, step740]: loss 0.703886
[epoch20, step741]: loss 0.316993
[epoch20, step742]: loss 0.598144
[epoch20, step743]: loss 0.715889
[epoch20, step744]: loss 0.715683
[epoch20, step745]: loss 0.356854
[epoch20, step746]: loss 0.535787
[epoch20, step747]: loss 0.466759
[epoch20, step748]: loss 0.484341
[epoch20, step749]: loss 0.427312
[epoch20, step750]: loss 0.463925
[epoch20, step751]: loss 0.880808
[epoch20, step752]: loss 0.505688
[epoch20, step753]: loss 0.312567
[epoch20, step754]: loss 0.398870
[epoch20, step755]: loss 0.458081
[epoch20, step756]: loss 0.546072
[epoch20, step757]: loss 0.519143
[epoch20, step758]: loss 0.582992
[epoch20, step759]: loss 0.413984
[epoch20, step760]: loss 0.405833
[epoch20, step761]: loss 0.268901
[epoch20, step762]: loss 0.374203
[epoch20, step763]: loss 0.388711
[epoch20, step764]: loss 0.714088
[epoch20, step765]: loss 0.616532
[epoch20, step766]: loss 0.448840
[epoch20, step767]: loss 0.522017
[epoch20, step768]: loss 0.576786
[epoch20, step769]: loss 0.403077
[epoch20, step770]: loss 0.713545
[epoch20, step771]: loss 0.524049
[epoch20, step772]: loss 0.463121
[epoch20, step773]: loss 0.583023
[epoch20, step774]: loss 0.372462
[epoch20, step775]: loss 0.367477
[epoch20, step776]: loss 0.523279
[epoch20, step777]: loss 0.486207
[epoch20, step778]: loss 0.433198
[epoch20, step779]: loss 0.488892
[epoch20, step780]: loss 0.505300
[epoch20, step781]: loss 0.342554
[epoch20, step782]: loss 0.756172
[epoch20, step783]: loss 0.483465
[epoch20, step784]: loss 0.542296
[epoch20, step785]: loss 0.637906
[epoch20, step786]: loss 0.577781
[epoch20, step787]: loss 0.283713
[epoch20, step788]: loss 0.576675
[epoch20, step789]: loss 0.641500
[epoch20, step790]: loss 0.589824
[epoch20, step791]: loss 0.371347
[epoch20, step792]: loss 0.394407
[epoch20, step793]: loss 0.595775
[epoch20, step794]: loss 0.353364
[epoch20, step795]: loss 0.401903
[epoch20, step796]: loss 0.514648
[epoch20, step797]: loss 0.626004
[epoch20, step798]: loss 0.554561
[epoch20, step799]: loss 0.371668
[epoch20, step800]: loss 0.380000
[epoch20, step801]: loss 0.654737
[epoch20, step802]: loss 0.614910
[epoch20, step803]: loss 0.504144
[epoch20, step804]: loss 0.277115
[epoch20, step805]: loss 0.567361
[epoch20, step806]: loss 0.309489
[epoch20, step807]: loss 0.536240
[epoch20, step808]: loss 0.605551
[epoch20, step809]: loss 0.581229
[epoch20, step810]: loss 0.505918
[epoch20, step811]: loss 0.387017
[epoch20, step812]: loss 0.415470
[epoch20, step813]: loss 0.635628
[epoch20, step814]: loss 0.588318
[epoch20, step815]: loss 0.518440
[epoch20, step816]: loss 0.197501
[epoch20, step817]: loss 0.593686
[epoch20, step818]: loss 0.318733
[epoch20, step819]: loss 0.466929
[epoch20, step820]: loss 0.643755
[epoch20, step821]: loss 0.314074
[epoch20, step822]: loss 0.563695
[epoch20, step823]: loss 0.575624
[epoch20, step824]: loss 0.464833
[epoch20, step825]: loss 0.610889
[epoch20, step826]: loss 0.396179
[epoch20, step827]: loss 0.762540
[epoch20, step828]: loss 0.607519
[epoch20, step829]: loss 0.619623
[epoch20, step830]: loss 0.663461
[epoch20, step831]: loss 0.474280
[epoch20, step832]: loss 0.660911
[epoch20, step833]: loss 0.724420
[epoch20, step834]: loss 0.486165
[epoch20, step835]: loss 0.679242
[epoch20, step836]: loss 0.635490
[epoch20, step837]: loss 0.540458
[epoch20, step838]: loss 0.398852
[epoch20, step839]: loss 0.439630
[epoch20, step840]: loss 0.384734
[epoch20, step841]: loss 0.598106
[epoch20, step842]: loss 0.653579
[epoch20, step843]: loss 0.540989
[epoch20, step844]: loss 0.320213
[epoch20, step845]: loss 0.271886
[epoch20, step846]: loss 0.309764
[epoch20, step847]: loss 0.608594
[epoch20, step848]: loss 0.574692
[epoch20, step849]: loss 0.591636
[epoch20, step850]: loss 0.667410
[epoch20, step851]: loss 0.414315
[epoch20, step852]: loss 0.200265
[epoch20, step853]: loss 0.640015
[epoch20, step854]: loss 0.510384
[epoch20, step855]: loss 0.365698
[epoch20, step856]: loss 0.526260
[epoch20, step857]: loss 0.640359
[epoch20, step858]: loss 0.375930
[epoch20, step859]: loss 0.526396
[epoch20, step860]: loss 0.495634
[epoch20, step861]: loss 0.585062
[epoch20, step862]: loss 0.515162
[epoch20, step863]: loss 0.644927
[epoch20, step864]: loss 0.456604
[epoch20, step865]: loss 0.549680
[epoch20, step866]: loss 0.561178
[epoch20, step867]: loss 0.421737
[epoch20, step868]: loss 0.518499
[epoch20, step869]: loss 0.680686
[epoch20, step870]: loss 0.385799
[epoch20, step871]: loss 0.358558
[epoch20, step872]: loss 0.447673
[epoch20, step873]: loss 0.305541
[epoch20, step874]: loss 0.609248
[epoch20, step875]: loss 0.416676
[epoch20, step876]: loss 0.489537
[epoch20, step877]: loss 0.671532
[epoch20, step878]: loss 0.477767
[epoch20, step879]: loss 0.463863
[epoch20, step880]: loss 0.404338
[epoch20, step881]: loss 0.576023
[epoch20, step882]: loss 0.694178
[epoch20, step883]: loss 0.640447
[epoch20, step884]: loss 0.557715
[epoch20, step885]: loss 0.353348
[epoch20, step886]: loss 0.456020
[epoch20, step887]: loss 0.751382
[epoch20, step888]: loss 0.399536
[epoch20, step889]: loss 0.502641
[epoch20, step890]: loss 0.464598
[epoch20, step891]: loss 0.643398
[epoch20, step892]: loss 0.619288
[epoch20, step893]: loss 0.479006
[epoch20, step894]: loss 0.288152
[epoch20, step895]: loss 0.560204
[epoch20, step896]: loss 0.285442
[epoch20, step897]: loss 0.676749
[epoch20, step898]: loss 0.535393
[epoch20, step899]: loss 0.750418
[epoch20, step900]: loss 0.718231
[epoch20, step901]: loss 0.588014
[epoch20, step902]: loss 0.593306
[epoch20, step903]: loss 0.478756
[epoch20, step904]: loss 0.496398
[epoch20, step905]: loss 0.564838
[epoch20, step906]: loss 0.705486
[epoch20, step907]: loss 0.519181
[epoch20, step908]: loss 0.649885
[epoch20, step909]: loss 0.404301
[epoch20, step910]: loss 0.762554
[epoch20, step911]: loss 0.427631
[epoch20, step912]: loss 0.630389
[epoch20, step913]: loss 0.382625
[epoch20, step914]: loss 0.537881
[epoch20, step915]: loss 0.380949
[epoch20, step916]: loss 0.450368
[epoch20, step917]: loss 0.451799
[epoch20, step918]: loss 0.261764
[epoch20, step919]: loss 0.697768
[epoch20, step920]: loss 0.611066
[epoch20, step921]: loss 0.358400
[epoch20, step922]: loss 0.552353
[epoch20, step923]: loss 0.430646
[epoch20, step924]: loss 0.546174
[epoch20, step925]: loss 0.492621
[epoch20, step926]: loss 0.449024
[epoch20, step927]: loss 0.627831
[epoch20, step928]: loss 0.493916
[epoch20, step929]: loss 0.611201
[epoch20, step930]: loss 0.634313
[epoch20, step931]: loss 0.594494
[epoch20, step932]: loss 0.533694
[epoch20, step933]: loss 0.770283
[epoch20, step934]: loss 0.740164
[epoch20, step935]: loss 0.717146
[epoch20, step936]: loss 0.418240
[epoch20, step937]: loss 0.540139
[epoch20, step938]: loss 0.518125
[epoch20, step939]: loss 0.408090
[epoch20, step940]: loss 0.426165
[epoch20, step941]: loss 0.421303
[epoch20, step942]: loss 0.687444
[epoch20, step943]: loss 0.354537
[epoch20, step944]: loss 0.651480
[epoch20, step945]: loss 0.836922
[epoch20, step946]: loss 0.452388
[epoch20, step947]: loss 0.619515
[epoch20, step948]: loss 0.731954
[epoch20, step949]: loss 0.529715
[epoch20, step950]: loss 0.502336
[epoch20, step951]: loss 0.681830
[epoch20, step952]: loss 0.527224
[epoch20, step953]: loss 0.663261
[epoch20, step954]: loss 0.487049
[epoch20, step955]: loss 0.589736
[epoch20, step956]: loss 0.400264
[epoch20, step957]: loss 0.471214
[epoch20, step958]: loss 0.449963
[epoch20, step959]: loss 0.719717
[epoch20, step960]: loss 0.725044
[epoch20, step961]: loss 0.404770
[epoch20, step962]: loss 0.518008
[epoch20, step963]: loss 0.553143
[epoch20, step964]: loss 0.646166
[epoch20, step965]: loss 0.402332
[epoch20, step966]: loss 0.583615
[epoch20, step967]: loss 0.414836
[epoch20, step968]: loss 0.455954
[epoch20, step969]: loss 0.566022
[epoch20, step970]: loss 0.664067
[epoch20, step971]: loss 0.595770
[epoch20, step972]: loss 0.676732
[epoch20, step973]: loss 0.467393
[epoch20, step974]: loss 0.176844
[epoch20, step975]: loss 0.625527
[epoch20, step976]: loss 0.699289
[epoch20, step977]: loss 0.547838
[epoch20, step978]: loss 0.376259
[epoch20, step979]: loss 0.504255
[epoch20, step980]: loss 0.536530
[epoch20, step981]: loss 0.497761
[epoch20, step982]: loss 0.626653
[epoch20, step983]: loss 0.770504
[epoch20, step984]: loss 0.606536
[epoch20, step985]: loss 0.425584
[epoch20, step986]: loss 0.657075
[epoch20, step987]: loss 0.590711
[epoch20, step988]: loss 0.736246
[epoch20, step989]: loss 0.666104
[epoch20, step990]: loss 0.623444
[epoch20, step991]: loss 0.543954
[epoch20, step992]: loss 0.266093
[epoch20, step993]: loss 0.730289
[epoch20, step994]: loss 0.300609
[epoch20, step995]: loss 0.510327
[epoch20, step996]: loss 0.302341
[epoch20, step997]: loss 0.697283
[epoch20, step998]: loss 0.666839
[epoch20, step999]: loss 0.779654
[epoch20, step1000]: loss 0.249424
[epoch20, step1001]: loss 0.389977
[epoch20, step1002]: loss 0.410541
[epoch20, step1003]: loss 0.728812
[epoch20, step1004]: loss 0.451666
[epoch20, step1005]: loss 0.338620
[epoch20, step1006]: loss 0.153327
[epoch20, step1007]: loss 0.477059
[epoch20, step1008]: loss 0.596495
[epoch20, step1009]: loss 0.431725
[epoch20, step1010]: loss 0.605050
[epoch20, step1011]: loss 0.686389
[epoch20, step1012]: loss 0.645718
[epoch20, step1013]: loss 0.354953
[epoch20, step1014]: loss 0.292924
[epoch20, step1015]: loss 0.541198
[epoch20, step1016]: loss 0.363744
[epoch20, step1017]: loss 0.638814
[epoch20, step1018]: loss 0.461805
[epoch20, step1019]: loss 0.390265
[epoch20, step1020]: loss 0.314802
[epoch20, step1021]: loss 0.634492
[epoch20, step1022]: loss 0.579951
[epoch20, step1023]: loss 0.378317
[epoch20, step1024]: loss 0.474943
[epoch20, step1025]: loss 0.595324
[epoch20, step1026]: loss 0.638782
[epoch20, step1027]: loss 0.368798
[epoch20, step1028]: loss 0.508152
[epoch20, step1029]: loss 0.556450
[epoch20, step1030]: loss 0.636830
[epoch20, step1031]: loss 0.527797
[epoch20, step1032]: loss 0.480540
[epoch20, step1033]: loss 0.712693
[epoch20, step1034]: loss 0.489808
[epoch20, step1035]: loss 0.316290
[epoch20, step1036]: loss 0.419867
[epoch20, step1037]: loss 0.643495
[epoch20, step1038]: loss 0.574073
[epoch20, step1039]: loss 0.726248
[epoch20, step1040]: loss 0.493847
[epoch20, step1041]: loss 0.477057
[epoch20, step1042]: loss 0.390856
[epoch20, step1043]: loss 0.397918
[epoch20, step1044]: loss 0.531723
[epoch20, step1045]: loss 0.484463
[epoch20, step1046]: loss 0.659079
[epoch20, step1047]: loss 0.644497
[epoch20, step1048]: loss 0.708294
[epoch20, step1049]: loss 0.401085
[epoch20, step1050]: loss 0.526946
[epoch20, step1051]: loss 0.230027
[epoch20, step1052]: loss 0.590464
[epoch20, step1053]: loss 0.583131
[epoch20, step1054]: loss 0.574066
[epoch20, step1055]: loss 0.485194
[epoch20, step1056]: loss 0.446869
[epoch20, step1057]: loss 0.502218
[epoch20, step1058]: loss 0.699429
[epoch20, step1059]: loss 0.288889
[epoch20, step1060]: loss 0.601727
[epoch20, step1061]: loss 0.642379
[epoch20, step1062]: loss 0.418299
[epoch20, step1063]: loss 0.724897
[epoch20, step1064]: loss 0.483606
[epoch20, step1065]: loss 0.447374
[epoch20, step1066]: loss 0.556457
[epoch20, step1067]: loss 0.498434
[epoch20, step1068]: loss 0.614214
[epoch20, step1069]: loss 0.357857
[epoch20, step1070]: loss 0.606776
[epoch20, step1071]: loss 0.174936
[epoch20, step1072]: loss 0.642048
[epoch20, step1073]: loss 0.586047
[epoch20, step1074]: loss 0.484539
[epoch20, step1075]: loss 0.415997
[epoch20, step1076]: loss 0.643453
[epoch20, step1077]: loss 0.671475
[epoch20, step1078]: loss 0.494257
[epoch20, step1079]: loss 0.729318
[epoch20, step1080]: loss 0.650229
[epoch20, step1081]: loss 0.551424
[epoch20, step1082]: loss 0.452253
[epoch20, step1083]: loss 0.713386
[epoch20, step1084]: loss 0.524335
[epoch20, step1085]: loss 0.542384
[epoch20, step1086]: loss 0.495857
[epoch20, step1087]: loss 0.368936
[epoch20, step1088]: loss 0.621711
[epoch20, step1089]: loss 0.446576
[epoch20, step1090]: loss 0.574927
[epoch20, step1091]: loss 0.588679
[epoch20, step1092]: loss 0.662359
[epoch20, step1093]: loss 0.352879
[epoch20, step1094]: loss 0.509840
[epoch20, step1095]: loss 0.319418
[epoch20, step1096]: loss 0.437829
[epoch20, step1097]: loss 0.522203
[epoch20, step1098]: loss 0.731436
[epoch20, step1099]: loss 0.614742
[epoch20, step1100]: loss 0.236967
[epoch20, step1101]: loss 0.288614
[epoch20, step1102]: loss 0.293164
[epoch20, step1103]: loss 0.539123
[epoch20, step1104]: loss 0.630302
[epoch20, step1105]: loss 0.316232
[epoch20, step1106]: loss 0.519188
[epoch20, step1107]: loss 0.527144
[epoch20, step1108]: loss 0.461964
[epoch20, step1109]: loss 0.301964
[epoch20, step1110]: loss 0.598006
[epoch20, step1111]: loss 0.614312
[epoch20, step1112]: loss 0.720366
[epoch20, step1113]: loss 0.432392
[epoch20, step1114]: loss 0.476042
[epoch20, step1115]: loss 0.585864
[epoch20, step1116]: loss 0.690909
[epoch20, step1117]: loss 0.345777
[epoch20, step1118]: loss 0.716635
[epoch20, step1119]: loss 0.546002
[epoch20, step1120]: loss 0.430525
[epoch20, step1121]: loss 0.410208
[epoch20, step1122]: loss 0.356331
[epoch20, step1123]: loss 0.237851
[epoch20, step1124]: loss 0.609943
[epoch20, step1125]: loss 0.480997
[epoch20, step1126]: loss 0.635866
[epoch20, step1127]: loss 0.463727
[epoch20, step1128]: loss 0.427097
[epoch20, step1129]: loss 0.669201
[epoch20, step1130]: loss 0.583383
[epoch20, step1131]: loss 0.339213
[epoch20, step1132]: loss 0.495295
[epoch20, step1133]: loss 0.530083
[epoch20, step1134]: loss 0.507415
[epoch20, step1135]: loss 0.417701
[epoch20, step1136]: loss 0.458614
[epoch20, step1137]: loss 0.667502
[epoch20, step1138]: loss 0.454464
[epoch20, step1139]: loss 0.528858
[epoch20, step1140]: loss 0.531670
[epoch20, step1141]: loss 0.565704
[epoch20, step1142]: loss 0.480473
[epoch20, step1143]: loss 0.381091
[epoch20, step1144]: loss 0.697754
[epoch20, step1145]: loss 0.707435
[epoch20, step1146]: loss 0.744655
[epoch20, step1147]: loss 0.323397
[epoch20, step1148]: loss 0.439611
[epoch20, step1149]: loss 0.694664
[epoch20, step1150]: loss 0.310884
[epoch20, step1151]: loss 0.671994
[epoch20, step1152]: loss 0.580178
[epoch20, step1153]: loss 0.499644
[epoch20, step1154]: loss 0.566343
[epoch20, step1155]: loss 0.798160
[epoch20, step1156]: loss 0.528461
[epoch20, step1157]: loss 0.561176
[epoch20, step1158]: loss 0.482532
[epoch20, step1159]: loss 0.595492
[epoch20, step1160]: loss 0.649025
[epoch20, step1161]: loss 0.499234
[epoch20, step1162]: loss 0.564673
[epoch20, step1163]: loss 0.645967
[epoch20, step1164]: loss 0.621062
[epoch20, step1165]: loss 0.569926
[epoch20, step1166]: loss 0.689990
[epoch20, step1167]: loss 0.346391
[epoch20, step1168]: loss 0.546690
[epoch20, step1169]: loss 0.504434
[epoch20, step1170]: loss 0.624200
[epoch20, step1171]: loss 0.674570
[epoch20, step1172]: loss 0.611987
[epoch20, step1173]: loss 0.685368
[epoch20, step1174]: loss 0.538631
[epoch20, step1175]: loss 0.459649
[epoch20, step1176]: loss 0.524935
[epoch20, step1177]: loss 0.663363
[epoch20, step1178]: loss 0.623182
[epoch20, step1179]: loss 0.525839
[epoch20, step1180]: loss 0.437973
[epoch20, step1181]: loss 0.607471
[epoch20, step1182]: loss 0.567298
[epoch20, step1183]: loss 0.467289
[epoch20, step1184]: loss 0.346224
[epoch20, step1185]: loss 0.449576
[epoch20, step1186]: loss 0.658963
[epoch20, step1187]: loss 0.730662
[epoch20, step1188]: loss 0.615260
[epoch20, step1189]: loss 0.245606
[epoch20, step1190]: loss 0.618476
[epoch20, step1191]: loss 0.303153
[epoch20, step1192]: loss 0.594224
[epoch20, step1193]: loss 0.673457
[epoch20, step1194]: loss 0.697750
[epoch20, step1195]: loss 0.382806
[epoch20, step1196]: loss 0.324421
[epoch20, step1197]: loss 0.612663
[epoch20, step1198]: loss 0.147603
[epoch20, step1199]: loss 0.700211
[epoch20, step1200]: loss 0.446565
[epoch20, step1201]: loss 0.486044
[epoch20, step1202]: loss 0.342921
[epoch20, step1203]: loss 0.585058
[epoch20, step1204]: loss 0.564278
[epoch20, step1205]: loss 0.638158
[epoch20, step1206]: loss 0.507662
[epoch20, step1207]: loss 0.194016
[epoch20, step1208]: loss 0.528765
[epoch20, step1209]: loss 0.506265
[epoch20, step1210]: loss 0.268417
[epoch20, step1211]: loss 0.644533
[epoch20, step1212]: loss 0.583626
[epoch20, step1213]: loss 0.378097
[epoch20, step1214]: loss 0.450551
[epoch20, step1215]: loss 0.452614
[epoch20, step1216]: loss 0.512820
[epoch20, step1217]: loss 0.682543
[epoch20, step1218]: loss 0.452630
[epoch20, step1219]: loss 0.656117
[epoch20, step1220]: loss 0.441771
[epoch20, step1221]: loss 0.384317
[epoch20, step1222]: loss 0.421708
[epoch20, step1223]: loss 0.466827
[epoch20, step1224]: loss 0.547235
[epoch20, step1225]: loss 0.524055
[epoch20, step1226]: loss 0.581251
[epoch20, step1227]: loss 0.647409
[epoch20, step1228]: loss 0.588153
[epoch20, step1229]: loss 0.277035
[epoch20, step1230]: loss 0.507936
[epoch20, step1231]: loss 0.465735
[epoch20, step1232]: loss 0.411926
[epoch20, step1233]: loss 0.357683
[epoch20, step1234]: loss 0.426208
[epoch20, step1235]: loss 0.601991
[epoch20, step1236]: loss 0.349217
[epoch20, step1237]: loss 0.468058
[epoch20, step1238]: loss 0.585992
[epoch20, step1239]: loss 0.609690
[epoch20, step1240]: loss 0.557723
[epoch20, step1241]: loss 0.422888
[epoch20, step1242]: loss 0.574527
[epoch20, step1243]: loss 0.657708
[epoch20, step1244]: loss 0.313138
[epoch20, step1245]: loss 0.526752
[epoch20, step1246]: loss 0.554804
[epoch20, step1247]: loss 0.278526
[epoch20, step1248]: loss 0.476765
[epoch20, step1249]: loss 0.561481
[epoch20, step1250]: loss 0.462574
[epoch20, step1251]: loss 0.593284
[epoch20, step1252]: loss 0.321544
[epoch20, step1253]: loss 0.679850
[epoch20, step1254]: loss 0.371477
[epoch20, step1255]: loss 0.569436
[epoch20, step1256]: loss 0.589107
[epoch20, step1257]: loss 0.628217
[epoch20, step1258]: loss 0.401822
[epoch20, step1259]: loss 0.726785
[epoch20, step1260]: loss 0.556378
[epoch20, step1261]: loss 0.516487
[epoch20, step1262]: loss 0.497951
[epoch20, step1263]: loss 0.606912
[epoch20, step1264]: loss 0.434583
[epoch20, step1265]: loss 0.670808
[epoch20, step1266]: loss 0.497503
[epoch20, step1267]: loss 0.451143
[epoch20, step1268]: loss 0.333065
[epoch20, step1269]: loss 0.344177
[epoch20, step1270]: loss 0.533959
[epoch20, step1271]: loss 0.544356
[epoch20, step1272]: loss 0.365297
[epoch20, step1273]: loss 0.250725
[epoch20, step1274]: loss 0.762211
[epoch20, step1275]: loss 0.568198
[epoch20, step1276]: loss 0.400901
[epoch20, step1277]: loss 0.448209
[epoch20, step1278]: loss 0.712932
[epoch20, step1279]: loss 0.402789
[epoch20, step1280]: loss 0.430243
[epoch20, step1281]: loss 0.447974
[epoch20, step1282]: loss 0.385527
[epoch20, step1283]: loss 0.549593
[epoch20, step1284]: loss 0.512543
[epoch20, step1285]: loss 0.406697
[epoch20, step1286]: loss 0.542339
[epoch20, step1287]: loss 0.705514
[epoch20, step1288]: loss 0.627953
[epoch20, step1289]: loss 0.591451
[epoch20, step1290]: loss 0.559291
[epoch20, step1291]: loss 0.586058
[epoch20, step1292]: loss 0.613727
[epoch20, step1293]: loss 0.652086
[epoch20, step1294]: loss 0.426542
[epoch20, step1295]: loss 0.461244
[epoch20, step1296]: loss 0.718511
[epoch20, step1297]: loss 0.570577
[epoch20, step1298]: loss 0.391825
[epoch20, step1299]: loss 0.568727
[epoch20, step1300]: loss 0.330350
[epoch20, step1301]: loss 0.631271
[epoch20, step1302]: loss 0.218439
[epoch20, step1303]: loss 0.614201
[epoch20, step1304]: loss 0.651378
[epoch20, step1305]: loss 0.664355
[epoch20, step1306]: loss 0.381309
[epoch20, step1307]: loss 0.578790
[epoch20, step1308]: loss 0.605509
[epoch20, step1309]: loss 0.514252
[epoch20, step1310]: loss 0.338568
[epoch20, step1311]: loss 0.623655
[epoch20, step1312]: loss 0.543926
[epoch20, step1313]: loss 0.598701
[epoch20, step1314]: loss 0.607531
[epoch20, step1315]: loss 0.555770
[epoch20, step1316]: loss 0.409359
[epoch20, step1317]: loss 0.534870
[epoch20, step1318]: loss 0.283928
[epoch20, step1319]: loss 0.547359
[epoch20, step1320]: loss 0.432521
[epoch20, step1321]: loss 0.529713
[epoch20, step1322]: loss 0.586976
[epoch20, step1323]: loss 0.504379
[epoch20, step1324]: loss 0.622494
[epoch20, step1325]: loss 0.559497
[epoch20, step1326]: loss 0.604434
[epoch20, step1327]: loss 0.726790
[epoch20, step1328]: loss 0.638988
[epoch20, step1329]: loss 0.644772
[epoch20, step1330]: loss 0.483462
[epoch20, step1331]: loss 0.394297
[epoch20, step1332]: loss 0.545194
[epoch20, step1333]: loss 0.543039
[epoch20, step1334]: loss 0.452451
[epoch20, step1335]: loss 0.683275
[epoch20, step1336]: loss 0.623916
[epoch20, step1337]: loss 0.475833
[epoch20, step1338]: loss 0.467260
[epoch20, step1339]: loss 0.547499
[epoch20, step1340]: loss 0.499063
[epoch20, step1341]: loss 0.613045
[epoch20, step1342]: loss 0.275459
[epoch20, step1343]: loss 0.495783
[epoch20, step1344]: loss 0.582324
[epoch20, step1345]: loss 0.614570
[epoch20, step1346]: loss 0.605558
[epoch20, step1347]: loss 0.242727
[epoch20, step1348]: loss 0.396736
[epoch20, step1349]: loss 0.448843
[epoch20, step1350]: loss 0.579861
[epoch20, step1351]: loss 0.466411
[epoch20, step1352]: loss 0.551192
[epoch20, step1353]: loss 0.548301
[epoch20, step1354]: loss 0.652017
[epoch20, step1355]: loss 0.681485
[epoch20, step1356]: loss 0.420911
[epoch20, step1357]: loss 0.360643
[epoch20, step1358]: loss 0.673106
[epoch20, step1359]: loss 0.489601
[epoch20, step1360]: loss 0.362408
[epoch20, step1361]: loss 0.483009
[epoch20, step1362]: loss 0.793734
[epoch20, step1363]: loss 0.568804
[epoch20, step1364]: loss 0.477982
[epoch20, step1365]: loss 0.618744
[epoch20, step1366]: loss 0.483093
[epoch20, step1367]: loss 0.617695
[epoch20, step1368]: loss 0.476540
[epoch20, step1369]: loss 0.759229
[epoch20, step1370]: loss 0.651854
[epoch20, step1371]: loss 0.736009
[epoch20, step1372]: loss 0.464209
[epoch20, step1373]: loss 0.615864
[epoch20, step1374]: loss 0.338330
[epoch20, step1375]: loss 0.344035
[epoch20, step1376]: loss 0.552482
[epoch20, step1377]: loss 0.770165
[epoch20, step1378]: loss 0.612202
[epoch20, step1379]: loss 0.568752
[epoch20, step1380]: loss 0.479756
[epoch20, step1381]: loss 0.328671
[epoch20, step1382]: loss 0.673597
[epoch20, step1383]: loss 0.675090
[epoch20, step1384]: loss 0.485971
[epoch20, step1385]: loss 0.570265
[epoch20, step1386]: loss 0.402305
[epoch20, step1387]: loss 0.599762
[epoch20, step1388]: loss 0.653067
[epoch20, step1389]: loss 0.458847
[epoch20, step1390]: loss 0.652979
[epoch20, step1391]: loss 0.598849
[epoch20, step1392]: loss 0.408239
[epoch20, step1393]: loss 0.582982
[epoch20, step1394]: loss 0.537285
[epoch20, step1395]: loss 0.271252
[epoch20, step1396]: loss 0.431577
[epoch20, step1397]: loss 0.389606
[epoch20, step1398]: loss 0.502128
[epoch20, step1399]: loss 0.530235
[epoch20, step1400]: loss 0.338684
[epoch20, step1401]: loss 0.499782
[epoch20, step1402]: loss 0.528158
[epoch20, step1403]: loss 0.643609
[epoch20, step1404]: loss 0.367978
[epoch20, step1405]: loss 0.457484
[epoch20, step1406]: loss 0.486056
[epoch20, step1407]: loss 0.774718
[epoch20, step1408]: loss 0.566070
[epoch20, step1409]: loss 0.418906
[epoch20, step1410]: loss 0.466267
[epoch20, step1411]: loss 0.572233
[epoch20, step1412]: loss 0.475083
[epoch20, step1413]: loss 0.704530
[epoch20, step1414]: loss 0.536601
[epoch20, step1415]: loss 0.628083
[epoch20, step1416]: loss 0.420283
[epoch20, step1417]: loss 0.577078
[epoch20, step1418]: loss 0.504800
[epoch20, step1419]: loss 0.375615
[epoch20, step1420]: loss 0.584547
[epoch20, step1421]: loss 0.327552
[epoch20, step1422]: loss 0.677084
[epoch20, step1423]: loss 0.407129
[epoch20, step1424]: loss 0.710644
[epoch20, step1425]: loss 0.481957
[epoch20, step1426]: loss 0.696273
[epoch20, step1427]: loss 0.554951
[epoch20, step1428]: loss 0.560158
[epoch20, step1429]: loss 0.493496
[epoch20, step1430]: loss 0.259650
[epoch20, step1431]: loss 0.475622
[epoch20, step1432]: loss 0.582154
[epoch20, step1433]: loss 0.750479
[epoch20, step1434]: loss 0.580045
[epoch20, step1435]: loss 0.593364
[epoch20, step1436]: loss 0.558222
[epoch20, step1437]: loss 0.368736
[epoch20, step1438]: loss 0.478505
[epoch20, step1439]: loss 0.586631
[epoch20, step1440]: loss 0.637463
[epoch20, step1441]: loss 0.464212
[epoch20, step1442]: loss 0.316250
[epoch20, step1443]: loss 0.431160
[epoch20, step1444]: loss 0.622328
[epoch20, step1445]: loss 0.561334
[epoch20, step1446]: loss 0.590245
[epoch20, step1447]: loss 0.676168
[epoch20, step1448]: loss 0.404072
[epoch20, step1449]: loss 0.711901
[epoch20, step1450]: loss 0.506299
[epoch20, step1451]: loss 0.397228
[epoch20, step1452]: loss 0.701638
[epoch20, step1453]: loss 0.760327
[epoch20, step1454]: loss 0.492382
[epoch20, step1455]: loss 0.723638
[epoch20, step1456]: loss 0.517879
[epoch20, step1457]: loss 0.492575
[epoch20, step1458]: loss 0.639675
[epoch20, step1459]: loss 0.444417
[epoch20, step1460]: loss 0.170898
[epoch20, step1461]: loss 0.396154
[epoch20, step1462]: loss 0.304256
[epoch20, step1463]: loss 0.600575
[epoch20, step1464]: loss 0.530645
[epoch20, step1465]: loss 0.761383
[epoch20, step1466]: loss 0.498920
[epoch20, step1467]: loss 0.398313
[epoch20, step1468]: loss 0.624101
[epoch20, step1469]: loss 0.598414
[epoch20, step1470]: loss 0.651706
[epoch20, step1471]: loss 0.688033
[epoch20, step1472]: loss 0.309430
[epoch20, step1473]: loss 0.582814
[epoch20, step1474]: loss 0.349803
[epoch20, step1475]: loss 0.141213
[epoch20, step1476]: loss 0.658313
[epoch20, step1477]: loss 0.285657
[epoch20, step1478]: loss 0.406351
[epoch20, step1479]: loss 0.378939
[epoch20, step1480]: loss 0.347557
[epoch20, step1481]: loss 0.447481
[epoch20, step1482]: loss 0.210825
[epoch20, step1483]: loss 0.652695
[epoch20, step1484]: loss 0.556049
[epoch20, step1485]: loss 0.609046
[epoch20, step1486]: loss 0.500086
[epoch20, step1487]: loss 0.467659
[epoch20, step1488]: loss 0.580218
[epoch20, step1489]: loss 0.426994
[epoch20, step1490]: loss 0.365597
[epoch20, step1491]: loss 0.572411
[epoch20, step1492]: loss 0.541379
[epoch20, step1493]: loss 0.568748
[epoch20, step1494]: loss 0.500229
[epoch20, step1495]: loss 0.560069
[epoch20, step1496]: loss 0.436081
[epoch20, step1497]: loss 0.393211
[epoch20, step1498]: loss 0.379080
[epoch20, step1499]: loss 0.553174
[epoch20, step1500]: loss 0.567500
[epoch20, step1501]: loss 0.487333
[epoch20, step1502]: loss 0.401689
[epoch20, step1503]: loss 0.579629
[epoch20, step1504]: loss 0.584331
[epoch20, step1505]: loss 0.345849
[epoch20, step1506]: loss 0.514990
[epoch20, step1507]: loss 0.573314
[epoch20, step1508]: loss 0.634215
[epoch20, step1509]: loss 0.347559
[epoch20, step1510]: loss 0.382309
[epoch20, step1511]: loss 0.511248
[epoch20, step1512]: loss 0.531427
[epoch20, step1513]: loss 0.493711
[epoch20, step1514]: loss 0.594944
[epoch20, step1515]: loss 0.489100
[epoch20, step1516]: loss 0.676823
[epoch20, step1517]: loss 0.564341
[epoch20, step1518]: loss 0.293205
[epoch20, step1519]: loss 0.642522
[epoch20, step1520]: loss 0.563769
[epoch20, step1521]: loss 0.652853
[epoch20, step1522]: loss 0.405408
[epoch20, step1523]: loss 0.620586
[epoch20, step1524]: loss 0.630596
[epoch20, step1525]: loss 0.687023
[epoch20, step1526]: loss 0.329401
[epoch20, step1527]: loss 0.388521
[epoch20, step1528]: loss 0.524225
[epoch20, step1529]: loss 0.478222
[epoch20, step1530]: loss 0.487401
[epoch20, step1531]: loss 0.490239
[epoch20, step1532]: loss 0.380337
[epoch20, step1533]: loss 0.399233
[epoch20, step1534]: loss 0.546826
[epoch20, step1535]: loss 0.658770
[epoch20, step1536]: loss 0.556661
[epoch20, step1537]: loss 0.333085
[epoch20, step1538]: loss 0.656773
[epoch20, step1539]: loss 0.679708
[epoch20, step1540]: loss 0.544295
[epoch20, step1541]: loss 0.685018
[epoch20, step1542]: loss 0.668017
[epoch20, step1543]: loss 0.477933
[epoch20, step1544]: loss 0.609832
[epoch20, step1545]: loss 0.639424
[epoch20, step1546]: loss 0.582700
[epoch20, step1547]: loss 0.592880
[epoch20, step1548]: loss 0.314328
[epoch20, step1549]: loss 0.656193
[epoch20, step1550]: loss 0.439567
[epoch20, step1551]: loss 0.608889
[epoch20, step1552]: loss 0.554064
[epoch20, step1553]: loss 0.462457
[epoch20, step1554]: loss 0.475622
[epoch20, step1555]: loss 0.775245
[epoch20, step1556]: loss 0.544097
[epoch20, step1557]: loss 0.554193
[epoch20, step1558]: loss 0.422713
[epoch20, step1559]: loss 0.209737
[epoch20, step1560]: loss 0.580375
[epoch20, step1561]: loss 0.589169
[epoch20, step1562]: loss 0.600034
[epoch20, step1563]: loss 0.512882
[epoch20, step1564]: loss 0.405347
[epoch20, step1565]: loss 0.530105
[epoch20, step1566]: loss 0.353748
[epoch20, step1567]: loss 0.666946
[epoch20, step1568]: loss 0.603972
[epoch20, step1569]: loss 0.390013
[epoch20, step1570]: loss 0.450181
[epoch20, step1571]: loss 0.640878
[epoch20, step1572]: loss 0.543813
[epoch20, step1573]: loss 0.493692
[epoch20, step1574]: loss 0.388124
[epoch20, step1575]: loss 0.525149
[epoch20, step1576]: loss 0.550417
[epoch20, step1577]: loss 0.642311
[epoch20, step1578]: loss 0.440419
[epoch20, step1579]: loss 0.710289
[epoch20, step1580]: loss 0.473568
[epoch20, step1581]: loss 0.374271
[epoch20, step1582]: loss 0.667507
[epoch20, step1583]: loss 0.468513
[epoch20, step1584]: loss 0.524094
[epoch20, step1585]: loss 0.491114
[epoch20, step1586]: loss 0.363919
[epoch20, step1587]: loss 0.633208
[epoch20, step1588]: loss 0.492499
[epoch20, step1589]: loss 0.663661
[epoch20, step1590]: loss 0.464799
[epoch20, step1591]: loss 0.503078
[epoch20, step1592]: loss 0.212867
[epoch20, step1593]: loss 0.486646
[epoch20, step1594]: loss 0.754973
[epoch20, step1595]: loss 0.375258
[epoch20, step1596]: loss 0.641670
[epoch20, step1597]: loss 0.541960
[epoch20, step1598]: loss 0.797743
[epoch20, step1599]: loss 0.528193
[epoch20, step1600]: loss 0.444711
[epoch20, step1601]: loss 0.407518
[epoch20, step1602]: loss 0.701611
[epoch20, step1603]: loss 0.807999
[epoch20, step1604]: loss 0.704073
[epoch20, step1605]: loss 0.094874
[epoch20, step1606]: loss 0.510145
[epoch20, step1607]: loss 0.762408
[epoch20, step1608]: loss 0.412455
[epoch20, step1609]: loss 0.560207
[epoch20, step1610]: loss 0.663373
[epoch20, step1611]: loss 0.562295
[epoch20, step1612]: loss 0.609526
[epoch20, step1613]: loss 0.607076
[epoch20, step1614]: loss 0.748133
[epoch20, step1615]: loss 0.556361
[epoch20, step1616]: loss 0.533309
[epoch20, step1617]: loss 0.406250
[epoch20, step1618]: loss 0.416278
[epoch20, step1619]: loss 0.489565
[epoch20, step1620]: loss 0.271391
[epoch20, step1621]: loss 0.396214
[epoch20, step1622]: loss 0.486604
[epoch20, step1623]: loss 0.277674
[epoch20, step1624]: loss 0.139697
[epoch20, step1625]: loss 0.395040
[epoch20, step1626]: loss 0.495692
[epoch20, step1627]: loss 0.357537
[epoch20, step1628]: loss 0.544432
[epoch20, step1629]: loss 0.522504
[epoch20, step1630]: loss 0.563698
[epoch20, step1631]: loss 0.209434
[epoch20, step1632]: loss 0.408899
[epoch20, step1633]: loss 0.493014
[epoch20, step1634]: loss 0.508738
[epoch20, step1635]: loss 0.497676
[epoch20, step1636]: loss 0.265386
[epoch20, step1637]: loss 0.614440
[epoch20, step1638]: loss 0.710110
[epoch20, step1639]: loss 0.172856
[epoch20, step1640]: loss 0.419888
[epoch20, step1641]: loss 0.514571
[epoch20, step1642]: loss 0.761009
[epoch20, step1643]: loss 0.526997
[epoch20, step1644]: loss 0.461059
[epoch20, step1645]: loss 0.401493
[epoch20, step1646]: loss 0.437439
[epoch20, step1647]: loss 0.315142
[epoch20, step1648]: loss 0.609297
[epoch20, step1649]: loss 0.404918
[epoch20, step1650]: loss 0.621848
[epoch20, step1651]: loss 0.593681
[epoch20, step1652]: loss 0.476743
[epoch20, step1653]: loss 0.718380
[epoch20, step1654]: loss 0.557118
[epoch20, step1655]: loss 0.283368
[epoch20, step1656]: loss 0.515752
[epoch20, step1657]: loss 0.574520
[epoch20, step1658]: loss 0.736462
[epoch20, step1659]: loss 0.602164
[epoch20, step1660]: loss 0.583309
[epoch20, step1661]: loss 0.489298
[epoch20, step1662]: loss 0.406932
[epoch20, step1663]: loss 0.486518
[epoch20, step1664]: loss 0.464305
[epoch20, step1665]: loss 0.440006
[epoch20, step1666]: loss 0.614621
[epoch20, step1667]: loss 0.536218
[epoch20, step1668]: loss 0.554452
[epoch20, step1669]: loss 0.560672
[epoch20, step1670]: loss 0.547876
[epoch20, step1671]: loss 0.651589
[epoch20, step1672]: loss 0.582883
[epoch20, step1673]: loss 0.532369
[epoch20, step1674]: loss 0.441722
[epoch20, step1675]: loss 0.559374
[epoch20, step1676]: loss 0.619296
[epoch20, step1677]: loss 0.653015
[epoch20, step1678]: loss 0.273411
[epoch20, step1679]: loss 0.389535
[epoch20, step1680]: loss 0.570221
[epoch20, step1681]: loss 0.297112
[epoch20, step1682]: loss 0.282398
[epoch20, step1683]: loss 0.396648
[epoch20, step1684]: loss 0.681459
[epoch20, step1685]: loss 0.637576
[epoch20, step1686]: loss 0.553658
[epoch20, step1687]: loss 0.155554
[epoch20, step1688]: loss 0.677385
[epoch20, step1689]: loss 0.527035
[epoch20, step1690]: loss 0.369783
[epoch20, step1691]: loss 0.340330
[epoch20, step1692]: loss 0.572317
[epoch20, step1693]: loss 0.730131
[epoch20, step1694]: loss 0.776407
[epoch20, step1695]: loss 0.564061
[epoch20, step1696]: loss 0.578917
[epoch20, step1697]: loss 0.508576
[epoch20, step1698]: loss 0.640499
[epoch20, step1699]: loss 0.249814
[epoch20, step1700]: loss 0.605911
[epoch20, step1701]: loss 0.548541
[epoch20, step1702]: loss 0.598621
[epoch20, step1703]: loss 0.327405
[epoch20, step1704]: loss 0.670215
[epoch20, step1705]: loss 0.326622
[epoch20, step1706]: loss 0.578177
[epoch20, step1707]: loss 0.568153
[epoch20, step1708]: loss 0.598335
[epoch20, step1709]: loss 0.233449
[epoch20, step1710]: loss 0.635796
[epoch20, step1711]: loss 0.610377
[epoch20, step1712]: loss 0.423364
[epoch20, step1713]: loss 0.444637
[epoch20, step1714]: loss 0.316236
[epoch20, step1715]: loss 0.596527
[epoch20, step1716]: loss 0.565868
[epoch20, step1717]: loss 0.742983
[epoch20, step1718]: loss 0.453861
[epoch20, step1719]: loss 0.451930
[epoch20, step1720]: loss 0.645272
[epoch20, step1721]: loss 0.580427
[epoch20, step1722]: loss 0.296274
[epoch20, step1723]: loss 0.439872
[epoch20, step1724]: loss 0.422806
[epoch20, step1725]: loss 0.478561
[epoch20, step1726]: loss 0.443392
[epoch20, step1727]: loss 0.507624
[epoch20, step1728]: loss 0.345909
[epoch20, step1729]: loss 0.552338
[epoch20, step1730]: loss 0.471299
[epoch20, step1731]: loss 0.484245
[epoch20, step1732]: loss 0.292638
[epoch20, step1733]: loss 0.460956
[epoch20, step1734]: loss 0.534872
[epoch20, step1735]: loss 0.639480
[epoch20, step1736]: loss 0.780201
[epoch20, step1737]: loss 0.389185
[epoch20, step1738]: loss 0.557302
[epoch20, step1739]: loss 0.489092
[epoch20, step1740]: loss 0.540561
[epoch20, step1741]: loss 0.523505
[epoch20, step1742]: loss 0.568026
[epoch20, step1743]: loss 0.208881
[epoch20, step1744]: loss 0.701833
[epoch20, step1745]: loss 0.567264
[epoch20, step1746]: loss 0.771914
[epoch20, step1747]: loss 0.255499
[epoch20, step1748]: loss 0.202729
[epoch20, step1749]: loss 0.667374
[epoch20, step1750]: loss 0.462975
[epoch20, step1751]: loss 0.519548
[epoch20, step1752]: loss 0.511652
[epoch20, step1753]: loss 0.569297
[epoch20, step1754]: loss 0.457010
[epoch20, step1755]: loss 0.643224
[epoch20, step1756]: loss 0.537431
[epoch20, step1757]: loss 0.611554
[epoch20, step1758]: loss 0.490212
[epoch20, step1759]: loss 0.641472
[epoch20, step1760]: loss 0.435779
[epoch20, step1761]: loss 0.421187
[epoch20, step1762]: loss 0.707237
[epoch20, step1763]: loss 0.470585
[epoch20, step1764]: loss 0.536992
[epoch20, step1765]: loss 0.491977
[epoch20, step1766]: loss 0.433713
[epoch20, step1767]: loss 0.404428
[epoch20, step1768]: loss 0.775973
[epoch20, step1769]: loss 0.494617
[epoch20, step1770]: loss 0.771477
[epoch20, step1771]: loss 0.354184
[epoch20, step1772]: loss 0.515154
[epoch20, step1773]: loss 0.473851
[epoch20, step1774]: loss 0.718035
[epoch20, step1775]: loss 0.555015
[epoch20, step1776]: loss 0.287690
[epoch20, step1777]: loss 0.441907
[epoch20, step1778]: loss 0.540344
[epoch20, step1779]: loss 0.542314
[epoch20, step1780]: loss 0.593015
[epoch20, step1781]: loss 0.424963
[epoch20, step1782]: loss 0.629660
[epoch20, step1783]: loss 0.440933
[epoch20, step1784]: loss 0.493522
[epoch20, step1785]: loss 0.600759
[epoch20, step1786]: loss 0.320665
[epoch20, step1787]: loss 0.423169
[epoch20, step1788]: loss 0.557471
[epoch20, step1789]: loss 0.637254
[epoch20, step1790]: loss 0.334650
[epoch20, step1791]: loss 0.705335
[epoch20, step1792]: loss 0.256972
[epoch20, step1793]: loss 0.726405
[epoch20, step1794]: loss 0.454486
[epoch20, step1795]: loss 0.545298
[epoch20, step1796]: loss 0.496804
[epoch20, step1797]: loss 0.716820
[epoch20, step1798]: loss 0.622950
[epoch20, step1799]: loss 0.553260
[epoch20, step1800]: loss 0.400307
[epoch20, step1801]: loss 0.612776
[epoch20, step1802]: loss 0.485419
[epoch20, step1803]: loss 0.668843
[epoch20, step1804]: loss 0.516920
[epoch20, step1805]: loss 0.413155
[epoch20, step1806]: loss 0.540912
[epoch20, step1807]: loss 0.837609
[epoch20, step1808]: loss 0.573461
[epoch20, step1809]: loss 0.752098
[epoch20, step1810]: loss 0.522090
[epoch20, step1811]: loss 0.475835
[epoch20, step1812]: loss 0.582061
[epoch20, step1813]: loss 0.568251
[epoch20, step1814]: loss 0.487174
[epoch20, step1815]: loss 0.670074
[epoch20, step1816]: loss 0.529641
[epoch20, step1817]: loss 0.565392
[epoch20, step1818]: loss 0.735737
[epoch20, step1819]: loss 0.643905
[epoch20, step1820]: loss 0.507962
[epoch20, step1821]: loss 0.674773
[epoch20, step1822]: loss 0.732550
[epoch20, step1823]: loss 0.754612
[epoch20, step1824]: loss 0.341157
[epoch20, step1825]: loss 0.553604
[epoch20, step1826]: loss 0.332879
[epoch20, step1827]: loss 0.148482
[epoch20, step1828]: loss 0.487172
[epoch20, step1829]: loss 0.539801
[epoch20, step1830]: loss 0.564694
[epoch20, step1831]: loss 0.447182
[epoch20, step1832]: loss 0.547372
[epoch20, step1833]: loss 0.472485
[epoch20, step1834]: loss 0.617082
[epoch20, step1835]: loss 0.590048
[epoch20, step1836]: loss 0.571175
[epoch20, step1837]: loss 0.460949
[epoch20, step1838]: loss 0.384063
[epoch20, step1839]: loss 0.593020
[epoch20, step1840]: loss 0.382200
[epoch20, step1841]: loss 0.461523
[epoch20, step1842]: loss 0.649653
[epoch20, step1843]: loss 0.599594
[epoch20, step1844]: loss 0.510735
[epoch20, step1845]: loss 0.727489
[epoch20, step1846]: loss 0.577359
[epoch20, step1847]: loss 0.350302
[epoch20, step1848]: loss 0.495894
[epoch20, step1849]: loss 0.526886
[epoch20, step1850]: loss 0.527815
[epoch20, step1851]: loss 0.727489
[epoch20, step1852]: loss 0.577900
[epoch20, step1853]: loss 0.473398
[epoch20, step1854]: loss 0.679544
[epoch20, step1855]: loss 0.481668
[epoch20, step1856]: loss 0.423935
[epoch20, step1857]: loss 0.535611
[epoch20, step1858]: loss 0.634752
[epoch20, step1859]: loss 0.377574
[epoch20, step1860]: loss 0.638526
[epoch20, step1861]: loss 0.657388
[epoch20, step1862]: loss 0.282995
[epoch20, step1863]: loss 0.603950
[epoch20, step1864]: loss 0.610614
[epoch20, step1865]: loss 0.685945
[epoch20, step1866]: loss 0.572805
[epoch20, step1867]: loss 0.642361
[epoch20, step1868]: loss 0.476438
[epoch20, step1869]: loss 0.402377
[epoch20, step1870]: loss 0.669588
[epoch20, step1871]: loss 0.472238
[epoch20, step1872]: loss 0.516127
[epoch20, step1873]: loss 0.535686
[epoch20, step1874]: loss 0.582548
[epoch20, step1875]: loss 0.570210
[epoch20, step1876]: loss 0.366537
[epoch20, step1877]: loss 0.636035
[epoch20, step1878]: loss 0.579845
[epoch20, step1879]: loss 0.623325
[epoch20, step1880]: loss 0.367286
[epoch20, step1881]: loss 0.516621
[epoch20, step1882]: loss 0.704768
[epoch20, step1883]: loss 0.528215
[epoch20, step1884]: loss 0.487576
[epoch20, step1885]: loss 0.370289
[epoch20, step1886]: loss 0.612454
[epoch20, step1887]: loss 0.485846
[epoch20, step1888]: loss 0.844072
[epoch20, step1889]: loss 0.410786
[epoch20, step1890]: loss 0.382918
[epoch20, step1891]: loss 0.691515
[epoch20, step1892]: loss 0.487962
[epoch20, step1893]: loss 0.388792
[epoch20, step1894]: loss 0.568198
[epoch20, step1895]: loss 0.411223
[epoch20, step1896]: loss 0.468226
[epoch20, step1897]: loss 0.483747
[epoch20, step1898]: loss 0.642370
[epoch20, step1899]: loss 0.409212
[epoch20, step1900]: loss 0.503678
[epoch20, step1901]: loss 0.240028
[epoch20, step1902]: loss 0.369003
[epoch20, step1903]: loss 0.689975
[epoch20, step1904]: loss 0.303953
[epoch20, step1905]: loss 0.610267
[epoch20, step1906]: loss 0.766171
[epoch20, step1907]: loss 0.638863
[epoch20, step1908]: loss 0.471090
[epoch20, step1909]: loss 0.501885
[epoch20, step1910]: loss 0.574660
[epoch20, step1911]: loss 0.547107
[epoch20, step1912]: loss 0.546884
[epoch20, step1913]: loss 0.389883
[epoch20, step1914]: loss 0.552609
[epoch20, step1915]: loss 0.332537
[epoch20, step1916]: loss 0.476905
[epoch20, step1917]: loss 0.495952
[epoch20, step1918]: loss 0.546281
[epoch20, step1919]: loss 0.734421
[epoch20, step1920]: loss 0.601489
[epoch20, step1921]: loss 0.409334
[epoch20, step1922]: loss 0.390521
[epoch20, step1923]: loss 0.651583
[epoch20, step1924]: loss 0.597982
[epoch20, step1925]: loss 0.532129
[epoch20, step1926]: loss 0.635132
[epoch20, step1927]: loss 0.606795
[epoch20, step1928]: loss 0.504243
[epoch20, step1929]: loss 0.648049
[epoch20, step1930]: loss 0.531926
[epoch20, step1931]: loss 0.426083
[epoch20, step1932]: loss 0.536931
[epoch20, step1933]: loss 0.543314
[epoch20, step1934]: loss 0.543495
[epoch20, step1935]: loss 0.594885
[epoch20, step1936]: loss 0.544013
[epoch20, step1937]: loss 0.455940
[epoch20, step1938]: loss 0.477160
[epoch20, step1939]: loss 0.665966
[epoch20, step1940]: loss 0.432187
[epoch20, step1941]: loss 0.469805
[epoch20, step1942]: loss 0.663384
[epoch20, step1943]: loss 0.517717
[epoch20, step1944]: loss 0.563354
[epoch20, step1945]: loss 0.657302
[epoch20, step1946]: loss 0.528788
[epoch20, step1947]: loss 0.634649
[epoch20, step1948]: loss 0.732202
[epoch20, step1949]: loss 0.466344
[epoch20, step1950]: loss 0.626893
[epoch20, step1951]: loss 0.637011
[epoch20, step1952]: loss 0.532616
[epoch20, step1953]: loss 0.428870
[epoch20, step1954]: loss 0.444897
[epoch20, step1955]: loss 0.568715
[epoch20, step1956]: loss 0.457721
[epoch20, step1957]: loss 0.824225
[epoch20, step1958]: loss 0.309584
[epoch20, step1959]: loss 0.703246
[epoch20, step1960]: loss 0.183448
[epoch20, step1961]: loss 0.585004
[epoch20, step1962]: loss 0.392447
[epoch20, step1963]: loss 0.399619
[epoch20, step1964]: loss 0.551891
[epoch20, step1965]: loss 0.553452
[epoch20, step1966]: loss 0.141978
[epoch20, step1967]: loss 0.396993
[epoch20, step1968]: loss 0.539808
[epoch20, step1969]: loss 0.506065
[epoch20, step1970]: loss 0.711317
[epoch20, step1971]: loss 0.542262
[epoch20, step1972]: loss 0.806356
[epoch20, step1973]: loss 0.488835
[epoch20, step1974]: loss 0.638476
[epoch20, step1975]: loss 0.480700
[epoch20, step1976]: loss 0.267002
[epoch20, step1977]: loss 0.280159
[epoch20, step1978]: loss 0.484932
[epoch20, step1979]: loss 0.603274
[epoch20, step1980]: loss 0.521460
[epoch20, step1981]: loss 0.415378
[epoch20, step1982]: loss 0.359145
[epoch20, step1983]: loss 0.570127
[epoch20, step1984]: loss 0.207723
[epoch20, step1985]: loss 0.553557
[epoch20, step1986]: loss 0.563580
[epoch20, step1987]: loss 0.373921
[epoch20, step1988]: loss 0.430654
[epoch20, step1989]: loss 0.755938
[epoch20, step1990]: loss 0.710017
[epoch20, step1991]: loss 0.612821
[epoch20, step1992]: loss 0.553314
[epoch20, step1993]: loss 0.617674
[epoch20, step1994]: loss 0.753211
[epoch20, step1995]: loss 0.685801
[epoch20, step1996]: loss 0.443432
[epoch20, step1997]: loss 0.255941
[epoch20, step1998]: loss 0.469621
[epoch20, step1999]: loss 0.405459
[epoch20, step2000]: loss 0.478057
[epoch20, step2001]: loss 0.569657
[epoch20, step2002]: loss 0.781667
[epoch20, step2003]: loss 0.333176
[epoch20, step2004]: loss 0.450037
[epoch20, step2005]: loss 0.381784
[epoch20, step2006]: loss 0.696943
[epoch20, step2007]: loss 0.640719
[epoch20, step2008]: loss 0.334574
[epoch20, step2009]: loss 0.796000
[epoch20, step2010]: loss 0.424498
[epoch20, step2011]: loss 0.608625
[epoch20, step2012]: loss 0.678523
[epoch20, step2013]: loss 0.208555
[epoch20, step2014]: loss 0.587653
[epoch20, step2015]: loss 0.678094
[epoch20, step2016]: loss 0.763640
[epoch20, step2017]: loss 0.572643
[epoch20, step2018]: loss 0.599418
[epoch20, step2019]: loss 0.508603
[epoch20, step2020]: loss 0.391062
[epoch20, step2021]: loss 0.649231
[epoch20, step2022]: loss 0.504579
[epoch20, step2023]: loss 0.637377
[epoch20, step2024]: loss 0.667006
[epoch20, step2025]: loss 0.313991
[epoch20, step2026]: loss 0.734978
[epoch20, step2027]: loss 0.518826
[epoch20, step2028]: loss 0.580894
[epoch20, step2029]: loss 0.537034
[epoch20, step2030]: loss 0.530947
[epoch20, step2031]: loss 0.401805
[epoch20, step2032]: loss 0.567471
[epoch20, step2033]: loss 0.575615
[epoch20, step2034]: loss 0.464775
[epoch20, step2035]: loss 0.427507
[epoch20, step2036]: loss 0.548220
[epoch20, step2037]: loss 0.295283
[epoch20, step2038]: loss 0.670204
[epoch20, step2039]: loss 0.500382
[epoch20, step2040]: loss 0.412154
[epoch20, step2041]: loss 0.388442
[epoch20, step2042]: loss 0.523971
[epoch20, step2043]: loss 0.587785
[epoch20, step2044]: loss 0.619326
[epoch20, step2045]: loss 0.280657
[epoch20, step2046]: loss 0.579227
[epoch20, step2047]: loss 0.424405
[epoch20, step2048]: loss 0.602607
[epoch20, step2049]: loss 0.566102
[epoch20, step2050]: loss 0.446497
[epoch20, step2051]: loss 0.512361
[epoch20, step2052]: loss 0.558261
[epoch20, step2053]: loss 0.509340
[epoch20, step2054]: loss 0.743670
[epoch20, step2055]: loss 0.471953
[epoch20, step2056]: loss 0.646945
[epoch20, step2057]: loss 0.369450
[epoch20, step2058]: loss 0.507547
[epoch20, step2059]: loss 0.594676
[epoch20, step2060]: loss 0.400465
[epoch20, step2061]: loss 0.863938
[epoch20, step2062]: loss 0.669774
[epoch20, step2063]: loss 0.538944
[epoch20, step2064]: loss 0.630242
[epoch20, step2065]: loss 0.475778
[epoch20, step2066]: loss 0.351872
[epoch20, step2067]: loss 0.488706
[epoch20, step2068]: loss 0.444750
[epoch20, step2069]: loss 0.586351
[epoch20, step2070]: loss 0.517165
[epoch20, step2071]: loss 0.591703
[epoch20, step2072]: loss 0.246370
[epoch20, step2073]: loss 0.333402
[epoch20, step2074]: loss 0.553583
[epoch20, step2075]: loss 0.616937
[epoch20, step2076]: loss 0.424741
[epoch20, step2077]: loss 0.505888
[epoch20, step2078]: loss 0.322707
[epoch20, step2079]: loss 0.375672
[epoch20, step2080]: loss 0.532444
[epoch20, step2081]: loss 0.381153
[epoch20, step2082]: loss 0.517554
[epoch20, step2083]: loss 0.688458
[epoch20, step2084]: loss 0.679799
[epoch20, step2085]: loss 0.673115
[epoch20, step2086]: loss 0.473806
[epoch20, step2087]: loss 0.514169
[epoch20, step2088]: loss 0.590917
[epoch20, step2089]: loss 0.497774
[epoch20, step2090]: loss 0.786484
[epoch20, step2091]: loss 0.553344
[epoch20, step2092]: loss 0.638561
[epoch20, step2093]: loss 0.690223
[epoch20, step2094]: loss 0.780831
[epoch20, step2095]: loss 0.705640
[epoch20, step2096]: loss 0.450900
[epoch20, step2097]: loss 0.598911
[epoch20, step2098]: loss 0.594336
[epoch20, step2099]: loss 0.678003
[epoch20, step2100]: loss 0.519826
[epoch20, step2101]: loss 0.218534
[epoch20, step2102]: loss 0.557932
[epoch20, step2103]: loss 0.739311
[epoch20, step2104]: loss 0.701276
[epoch20, step2105]: loss 0.575032
[epoch20, step2106]: loss 0.513054
[epoch20, step2107]: loss 0.397893
[epoch20, step2108]: loss 0.329750
[epoch20, step2109]: loss 0.648195
[epoch20, step2110]: loss 0.797758
[epoch20, step2111]: loss 0.477932
[epoch20, step2112]: loss 0.670115
[epoch20, step2113]: loss 0.441948
[epoch20, step2114]: loss 0.531629
[epoch20, step2115]: loss 0.456829
[epoch20, step2116]: loss 0.396846
[epoch20, step2117]: loss 0.479673
[epoch20, step2118]: loss 0.687356
[epoch20, step2119]: loss 0.361614
[epoch20, step2120]: loss 0.433764
[epoch20, step2121]: loss 0.377117
[epoch20, step2122]: loss 0.649549
[epoch20, step2123]: loss 0.169275
[epoch20, step2124]: loss 0.412938
[epoch20, step2125]: loss 0.353526
[epoch20, step2126]: loss 0.553320
[epoch20, step2127]: loss 0.463703
[epoch20, step2128]: loss 0.411410
[epoch20, step2129]: loss 0.570972
[epoch20, step2130]: loss 0.528204
[epoch20, step2131]: loss 0.405433
[epoch20, step2132]: loss 0.652245
[epoch20, step2133]: loss 0.612167
[epoch20, step2134]: loss 0.611367
[epoch20, step2135]: loss 0.171029
[epoch20, step2136]: loss 0.557899
[epoch20, step2137]: loss 0.643408
[epoch20, step2138]: loss 0.366772
[epoch20, step2139]: loss 0.578652
[epoch20, step2140]: loss 0.575114
[epoch20, step2141]: loss 0.659000
[epoch20, step2142]: loss 0.378359
[epoch20, step2143]: loss 0.707620
[epoch20, step2144]: loss 0.405233
[epoch20, step2145]: loss 0.368805
[epoch20, step2146]: loss 0.625088
[epoch20, step2147]: loss 0.412121
[epoch20, step2148]: loss 0.518590
[epoch20, step2149]: loss 0.623861
[epoch20, step2150]: loss 0.539191
[epoch20, step2151]: loss 0.475811
[epoch20, step2152]: loss 0.738247
[epoch20, step2153]: loss 0.476385
[epoch20, step2154]: loss 0.542667
[epoch20, step2155]: loss 0.516223
[epoch20, step2156]: loss 0.588940
[epoch20, step2157]: loss 0.493777
[epoch20, step2158]: loss 0.624326
[epoch20, step2159]: loss 0.592148
[epoch20, step2160]: loss 0.674647
[epoch20, step2161]: loss 0.665415
[epoch20, step2162]: loss 0.578167
[epoch20, step2163]: loss 0.373483
[epoch20, step2164]: loss 0.407242
[epoch20, step2165]: loss 0.343814
[epoch20, step2166]: loss 0.540725
[epoch20, step2167]: loss 0.427622
[epoch20, step2168]: loss 0.536913
[epoch20, step2169]: loss 0.549218
[epoch20, step2170]: loss 0.589844
[epoch20, step2171]: loss 0.753635
[epoch20, step2172]: loss 0.386901
[epoch20, step2173]: loss 0.571305
[epoch20, step2174]: loss 0.480966
[epoch20, step2175]: loss 0.522925
[epoch20, step2176]: loss 0.459306
[epoch20, step2177]: loss 0.376597
[epoch20, step2178]: loss 0.562480
[epoch20, step2179]: loss 0.592567
[epoch20, step2180]: loss 0.692403
[epoch20, step2181]: loss 0.563402
[epoch20, step2182]: loss 0.550469
[epoch20, step2183]: loss 0.580182
[epoch20, step2184]: loss 0.612289
[epoch20, step2185]: loss 0.415280
[epoch20, step2186]: loss 0.663649
[epoch20, step2187]: loss 0.480105
[epoch20, step2188]: loss 0.463531
[epoch20, step2189]: loss 0.513721
[epoch20, step2190]: loss 0.542537
[epoch20, step2191]: loss 0.658590
[epoch20, step2192]: loss 0.755324
[epoch20, step2193]: loss 0.496225
[epoch20, step2194]: loss 0.574819
[epoch20, step2195]: loss 0.518154
[epoch20, step2196]: loss 0.628215
[epoch20, step2197]: loss 0.572151
[epoch20, step2198]: loss 0.609372
[epoch20, step2199]: loss 0.485237
[epoch20, step2200]: loss 0.723475
[epoch20, step2201]: loss 0.751435
[epoch20, step2202]: loss 0.395929
[epoch20, step2203]: loss 0.632186
[epoch20, step2204]: loss 0.482658
[epoch20, step2205]: loss 0.496207
[epoch20, step2206]: loss 0.572341
[epoch20, step2207]: loss 0.565335
[epoch20, step2208]: loss 0.483751
[epoch20, step2209]: loss 0.446127
[epoch20, step2210]: loss 0.679620
[epoch20, step2211]: loss 0.549411
[epoch20, step2212]: loss 0.778806
[epoch20, step2213]: loss 0.570826
[epoch20, step2214]: loss 0.650548
[epoch20, step2215]: loss 0.481995
[epoch20, step2216]: loss 0.650689
[epoch20, step2217]: loss 0.623352
[epoch20, step2218]: loss 0.505551
[epoch20, step2219]: loss 0.726827
[epoch20, step2220]: loss 0.092138
[epoch20, step2221]: loss 0.370570
[epoch20, step2222]: loss 0.666116
[epoch20, step2223]: loss 0.415267
[epoch20, step2224]: loss 0.635120
[epoch20, step2225]: loss 0.536240
[epoch20, step2226]: loss 0.486066
[epoch20, step2227]: loss 0.607042
[epoch20, step2228]: loss 0.576098
[epoch20, step2229]: loss 0.505178
[epoch20, step2230]: loss 0.483272
[epoch20, step2231]: loss 0.536396
[epoch20, step2232]: loss 0.531957
[epoch20, step2233]: loss 0.505476
[epoch20, step2234]: loss 0.458867
[epoch20, step2235]: loss 0.560606
[epoch20, step2236]: loss 0.525361
[epoch20, step2237]: loss 0.607051
[epoch20, step2238]: loss 0.543814
[epoch20, step2239]: loss 0.690994
[epoch20, step2240]: loss 0.684849
[epoch20, step2241]: loss 0.683286
[epoch20, step2242]: loss 0.390223
[epoch20, step2243]: loss 0.534824
[epoch20, step2244]: loss 0.501688
[epoch20, step2245]: loss 0.543641
[epoch20, step2246]: loss 0.551751
[epoch20, step2247]: loss 0.257640
[epoch20, step2248]: loss 0.337006
[epoch20, step2249]: loss 0.539650
[epoch20, step2250]: loss 0.713550
[epoch20, step2251]: loss 0.427256
[epoch20, step2252]: loss 0.548700
[epoch20, step2253]: loss 0.560587
[epoch20, step2254]: loss 0.471620
[epoch20, step2255]: loss 0.606839
[epoch20, step2256]: loss 0.599699
[epoch20, step2257]: loss 0.606694
[epoch20, step2258]: loss 0.387901
[epoch20, step2259]: loss 0.339531
[epoch20, step2260]: loss 0.456721
[epoch20, step2261]: loss 0.519016
[epoch20, step2262]: loss 0.557549
[epoch20, step2263]: loss 0.690412
[epoch20, step2264]: loss 0.626801
[epoch20, step2265]: loss 0.277930
[epoch20, step2266]: loss 0.515971
[epoch20, step2267]: loss 0.374364
[epoch20, step2268]: loss 0.259995
[epoch20, step2269]: loss 0.639128
[epoch20, step2270]: loss 0.440395
[epoch20, step2271]: loss 0.451529
[epoch20, step2272]: loss 0.384864
[epoch20, step2273]: loss 0.576778
[epoch20, step2274]: loss 0.478751
[epoch20, step2275]: loss 0.701987
[epoch20, step2276]: loss 0.462290
[epoch20, step2277]: loss 0.590443
[epoch20, step2278]: loss 0.560713
[epoch20, step2279]: loss 0.532202
[epoch20, step2280]: loss 0.582218
[epoch20, step2281]: loss 0.405275
[epoch20, step2282]: loss 0.693005
[epoch20, step2283]: loss 0.427510
[epoch20, step2284]: loss 0.592833
[epoch20, step2285]: loss 0.386361
[epoch20, step2286]: loss 0.634477
[epoch20, step2287]: loss 0.422403
[epoch20, step2288]: loss 0.418068
[epoch20, step2289]: loss 0.282766
[epoch20, step2290]: loss 0.627771
[epoch20, step2291]: loss 0.598927
[epoch20, step2292]: loss 0.572971
[epoch20, step2293]: loss 0.526177
[epoch20, step2294]: loss 0.489757
[epoch20, step2295]: loss 0.715009
[epoch20, step2296]: loss 0.761727
[epoch20, step2297]: loss 0.168198
[epoch20, step2298]: loss 0.406986
[epoch20, step2299]: loss 0.615098
[epoch20, step2300]: loss 0.583083
[epoch20, step2301]: loss 0.662728
[epoch20, step2302]: loss 0.471161
[epoch20, step2303]: loss 0.509886
[epoch20, step2304]: loss 0.564468
[epoch20, step2305]: loss 0.610863
[epoch20, step2306]: loss 0.661019
[epoch20, step2307]: loss 0.430416
[epoch20, step2308]: loss 0.668609
[epoch20, step2309]: loss 0.138362
[epoch20, step2310]: loss 0.659222
[epoch20, step2311]: loss 0.416459
[epoch20, step2312]: loss 0.387731
[epoch20, step2313]: loss 0.559977
[epoch20, step2314]: loss 0.680578
[epoch20, step2315]: loss 0.486896
[epoch20, step2316]: loss 0.144149
[epoch20, step2317]: loss 0.522630
[epoch20, step2318]: loss 0.445687
[epoch20, step2319]: loss 0.480590
[epoch20, step2320]: loss 0.581913
[epoch20, step2321]: loss 0.447309
[epoch20, step2322]: loss 0.569751
[epoch20, step2323]: loss 0.477509
[epoch20, step2324]: loss 0.380529
[epoch20, step2325]: loss 0.824146
[epoch20, step2326]: loss 0.618842
[epoch20, step2327]: loss 0.476849
[epoch20, step2328]: loss 0.456270
[epoch20, step2329]: loss 0.700500
[epoch20, step2330]: loss 0.382507
[epoch20, step2331]: loss 0.532201
[epoch20, step2332]: loss 0.628306
[epoch20, step2333]: loss 0.573678
[epoch20, step2334]: loss 0.387862
[epoch20, step2335]: loss 0.471298
[epoch20, step2336]: loss 0.381218
[epoch20, step2337]: loss 0.689407
[epoch20, step2338]: loss 0.301248
[epoch20, step2339]: loss 0.380191
[epoch20, step2340]: loss 0.444520
[epoch20, step2341]: loss 0.644666
[epoch20, step2342]: loss 0.745650
[epoch20, step2343]: loss 0.295807
[epoch20, step2344]: loss 0.482434
[epoch20, step2345]: loss 0.416329
[epoch20, step2346]: loss 0.364711
[epoch20, step2347]: loss 0.522225
[epoch20, step2348]: loss 0.394551
[epoch20, step2349]: loss 0.592488
[epoch20, step2350]: loss 0.671362
[epoch20, step2351]: loss 0.178416
[epoch20, step2352]: loss 0.284101
[epoch20, step2353]: loss 0.622504
[epoch20, step2354]: loss 0.561361
[epoch20, step2355]: loss 0.418402
[epoch20, step2356]: loss 0.598411
[epoch20, step2357]: loss 0.257363
[epoch20, step2358]: loss 0.753496
[epoch20, step2359]: loss 0.495020
[epoch20, step2360]: loss 0.558086
[epoch20, step2361]: loss 0.562566
[epoch20, step2362]: loss 0.369740
[epoch20, step2363]: loss 0.507500
[epoch20, step2364]: loss 0.251834
[epoch20, step2365]: loss 0.342025
[epoch20, step2366]: loss 0.667670
[epoch20, step2367]: loss 0.301179
[epoch20, step2368]: loss 0.285761
[epoch20, step2369]: loss 0.735292
[epoch20, step2370]: loss 0.638210
[epoch20, step2371]: loss 0.572996
[epoch20, step2372]: loss 0.421472
[epoch20, step2373]: loss 0.571927
[epoch20, step2374]: loss 0.362466
[epoch20, step2375]: loss 0.421422
[epoch20, step2376]: loss 0.594423
[epoch20, step2377]: loss 0.317710
[epoch20, step2378]: loss 0.529662
[epoch20, step2379]: loss 0.538416
[epoch20, step2380]: loss 0.488367
[epoch20, step2381]: loss 0.449632
[epoch20, step2382]: loss 0.682676
[epoch20, step2383]: loss 0.613800
[epoch20, step2384]: loss 0.616194
[epoch20, step2385]: loss 0.263544
[epoch20, step2386]: loss 0.607024
[epoch20, step2387]: loss 0.628092
[epoch20, step2388]: loss 0.679770
[epoch20, step2389]: loss 0.618590
[epoch20, step2390]: loss 0.464802
[epoch20, step2391]: loss 0.442262
[epoch20, step2392]: loss 0.600205
[epoch20, step2393]: loss 0.550812
[epoch20, step2394]: loss 0.400719
[epoch20, step2395]: loss 0.488904
[epoch20, step2396]: loss 0.534711
[epoch20, step2397]: loss 0.482023
[epoch20, step2398]: loss 0.550066
[epoch20, step2399]: loss 0.712795
[epoch20, step2400]: loss 0.533469
[epoch20, step2401]: loss 0.365520
[epoch20, step2402]: loss 0.128631
[epoch20, step2403]: loss 0.369531
[epoch20, step2404]: loss 0.683849
[epoch20, step2405]: loss 0.664220
[epoch20, step2406]: loss 0.571142
[epoch20, step2407]: loss 0.439007
[epoch20, step2408]: loss 0.677892
[epoch20, step2409]: loss 0.394805
[epoch20, step2410]: loss 0.462209
[epoch20, step2411]: loss 0.465382
[epoch20, step2412]: loss 0.368792
[epoch20, step2413]: loss 0.640222
[epoch20, step2414]: loss 0.533880
[epoch20, step2415]: loss 0.417811
[epoch20, step2416]: loss 0.424188
[epoch20, step2417]: loss 0.764596
[epoch20, step2418]: loss 0.541397
[epoch20, step2419]: loss 0.422004
[epoch20, step2420]: loss 0.630235
[epoch20, step2421]: loss 0.611481
[epoch20, step2422]: loss 0.355988
[epoch20, step2423]: loss 0.463998
[epoch20, step2424]: loss 0.595137
[epoch20, step2425]: loss 0.597418
[epoch20, step2426]: loss 0.537971
[epoch20, step2427]: loss 0.169550
[epoch20, step2428]: loss 0.656264
[epoch20, step2429]: loss 0.397106
[epoch20, step2430]: loss 0.719360
[epoch20, step2431]: loss 0.354946
[epoch20, step2432]: loss 0.431768
[epoch20, step2433]: loss 0.639806
[epoch20, step2434]: loss 0.368314
[epoch20, step2435]: loss 0.571871
[epoch20, step2436]: loss 0.573622
[epoch20, step2437]: loss 0.354242
[epoch20, step2438]: loss 0.510347
[epoch20, step2439]: loss 0.439971
[epoch20, step2440]: loss 0.319577
[epoch20, step2441]: loss 0.671842
[epoch20, step2442]: loss 0.752470
[epoch20, step2443]: loss 0.438038
[epoch20, step2444]: loss 0.693046
[epoch20, step2445]: loss 0.572088
[epoch20, step2446]: loss 0.504776
[epoch20, step2447]: loss 0.278938
[epoch20, step2448]: loss 0.182063
[epoch20, step2449]: loss 0.469111
[epoch20, step2450]: loss 0.573891
[epoch20, step2451]: loss 0.474193
[epoch20, step2452]: loss 0.523825
[epoch20, step2453]: loss 0.384490
[epoch20, step2454]: loss 0.581241
[epoch20, step2455]: loss 0.546697
[epoch20, step2456]: loss 0.374683
[epoch20, step2457]: loss 0.679394
[epoch20, step2458]: loss 0.832639
[epoch20, step2459]: loss 0.777171
[epoch20, step2460]: loss 0.520132
[epoch20, step2461]: loss 0.298366
[epoch20, step2462]: loss 0.698785
[epoch20, step2463]: loss 0.359008
[epoch20, step2464]: loss 0.387497
[epoch20, step2465]: loss 0.602638
[epoch20, step2466]: loss 0.381705
[epoch20, step2467]: loss 0.335817
[epoch20, step2468]: loss 0.637963
[epoch20, step2469]: loss 0.561014
[epoch20, step2470]: loss 0.395868
[epoch20, step2471]: loss 0.418825
[epoch20, step2472]: loss 0.525468
[epoch20, step2473]: loss 0.526970
[epoch20, step2474]: loss 0.467744
[epoch20, step2475]: loss 0.376988
[epoch20, step2476]: loss 0.502931
[epoch20, step2477]: loss 0.580942
[epoch20, step2478]: loss 0.306148
[epoch20, step2479]: loss 0.476976
[epoch20, step2480]: loss 0.651742
[epoch20, step2481]: loss 0.578816
[epoch20, step2482]: loss 0.472871
[epoch20, step2483]: loss 0.365269
[epoch20, step2484]: loss 0.599290
[epoch20, step2485]: loss 0.510331
[epoch20, step2486]: loss 0.578418
[epoch20, step2487]: loss 0.728335
[epoch20, step2488]: loss 0.607695
[epoch20, step2489]: loss 0.565295
[epoch20, step2490]: loss 0.487387
[epoch20, step2491]: loss 0.213923
[epoch20, step2492]: loss 0.662565
[epoch20, step2493]: loss 0.718052
[epoch20, step2494]: loss 0.630674
[epoch20, step2495]: loss 0.533811
[epoch20, step2496]: loss 0.505249
[epoch20, step2497]: loss 0.523138
[epoch20, step2498]: loss 0.460531
[epoch20, step2499]: loss 0.612087
[epoch20, step2500]: loss 0.646406
[epoch20, step2501]: loss 0.693745
[epoch20, step2502]: loss 0.363077
[epoch20, step2503]: loss 0.530254
[epoch20, step2504]: loss 0.460947
[epoch20, step2505]: loss 0.662778
[epoch20, step2506]: loss 0.915951
[epoch20, step2507]: loss 0.542824
[epoch20, step2508]: loss 0.593005
[epoch20, step2509]: loss 0.546946
[epoch20, step2510]: loss 0.626592
[epoch20, step2511]: loss 0.655808
[epoch20, step2512]: loss 0.480069
[epoch20, step2513]: loss 0.564229
[epoch20, step2514]: loss 0.724953
[epoch20, step2515]: loss 0.496181
[epoch20, step2516]: loss 0.630819
[epoch20, step2517]: loss 0.229008
[epoch20, step2518]: loss 0.334243
[epoch20, step2519]: loss 0.579021
[epoch20, step2520]: loss 0.390046
[epoch20, step2521]: loss 0.444136
[epoch20, step2522]: loss 0.459621
[epoch20, step2523]: loss 0.578312
[epoch20, step2524]: loss 0.282607
[epoch20, step2525]: loss 0.586307
[epoch20, step2526]: loss 0.511283
[epoch20, step2527]: loss 0.744717
[epoch20, step2528]: loss 0.630294
[epoch20, step2529]: loss 0.637374
[epoch20, step2530]: loss 0.584194
[epoch20, step2531]: loss 0.416260
[epoch20, step2532]: loss 0.524641
[epoch20, step2533]: loss 0.416295
[epoch20, step2534]: loss 0.692200
[epoch20, step2535]: loss 0.519994
[epoch20, step2536]: loss 0.619596
[epoch20, step2537]: loss 0.678651
[epoch20, step2538]: loss 0.757848
[epoch20, step2539]: loss 0.623329
[epoch20, step2540]: loss 0.445167
[epoch20, step2541]: loss 0.493911
[epoch20, step2542]: loss 0.374651
[epoch20, step2543]: loss 0.873615
[epoch20, step2544]: loss 0.490313
[epoch20, step2545]: loss 0.619871
[epoch20, step2546]: loss 0.440397
[epoch20, step2547]: loss 0.499241
[epoch20, step2548]: loss 0.762441
[epoch20, step2549]: loss 0.572292
[epoch20, step2550]: loss 0.381565
[epoch20, step2551]: loss 0.685965
[epoch20, step2552]: loss 0.577054
[epoch20, step2553]: loss 0.375150
[epoch20, step2554]: loss 0.540128
[epoch20, step2555]: loss 0.626557
[epoch20, step2556]: loss 0.618230
[epoch20, step2557]: loss 0.639825
[epoch20, step2558]: loss 0.401890
[epoch20, step2559]: loss 0.490197
[epoch20, step2560]: loss 0.710681
[epoch20, step2561]: loss 0.589778
[epoch20, step2562]: loss 0.392115
[epoch20, step2563]: loss 0.416612
[epoch20, step2564]: loss 0.541154
[epoch20, step2565]: loss 0.733499
[epoch20, step2566]: loss 0.415336
[epoch20, step2567]: loss 0.496115
[epoch20, step2568]: loss 0.415426
[epoch20, step2569]: loss 0.388940
[epoch20, step2570]: loss 0.521885
[epoch20, step2571]: loss 0.600278
[epoch20, step2572]: loss 0.355795
[epoch20, step2573]: loss 0.574117
[epoch20, step2574]: loss 0.387716
[epoch20, step2575]: loss 0.495007
[epoch20, step2576]: loss 0.479184
[epoch20, step2577]: loss 0.352423
[epoch20, step2578]: loss 0.417052
[epoch20, step2579]: loss 0.568443
[epoch20, step2580]: loss 0.622138
[epoch20, step2581]: loss 0.663067
[epoch20, step2582]: loss 0.601946
[epoch20, step2583]: loss 0.546616
[epoch20, step2584]: loss 0.503577
[epoch20, step2585]: loss 0.617784
[epoch20, step2586]: loss 0.152229
[epoch20, step2587]: loss 0.341899
[epoch20, step2588]: loss 0.678596
[epoch20, step2589]: loss 0.595440
[epoch20, step2590]: loss 0.448226
[epoch20, step2591]: loss 0.435530
[epoch20, step2592]: loss 0.459492
[epoch20, step2593]: loss 0.431793
[epoch20, step2594]: loss 0.620135
[epoch20, step2595]: loss 0.482777
[epoch20, step2596]: loss 0.574718
[epoch20, step2597]: loss 0.636575
[epoch20, step2598]: loss 0.760326
[epoch20, step2599]: loss 0.681928
[epoch20, step2600]: loss 0.543327
[epoch20, step2601]: loss 0.287437
[epoch20, step2602]: loss 0.673975
[epoch20, step2603]: loss 0.604179
[epoch20, step2604]: loss 0.271832
[epoch20, step2605]: loss 0.392568
[epoch20, step2606]: loss 0.490618
[epoch20, step2607]: loss 0.619047
[epoch20, step2608]: loss 0.608491
[epoch20, step2609]: loss 0.468595
[epoch20, step2610]: loss 0.513601
[epoch20, step2611]: loss 0.702052
[epoch20, step2612]: loss 0.402352
[epoch20, step2613]: loss 0.384811
[epoch20, step2614]: loss 0.273150
[epoch20, step2615]: loss 0.295052
[epoch20, step2616]: loss 0.576682
[epoch20, step2617]: loss 0.340057
[epoch20, step2618]: loss 0.181828
[epoch20, step2619]: loss 0.528078
[epoch20, step2620]: loss 0.569196
[epoch20, step2621]: loss 0.390514
[epoch20, step2622]: loss 0.434943
[epoch20, step2623]: loss 0.455578
[epoch20, step2624]: loss 0.458401
[epoch20, step2625]: loss 0.364288
[epoch20, step2626]: loss 0.434513
[epoch20, step2627]: loss 0.438463
[epoch20, step2628]: loss 0.770614
[epoch20, step2629]: loss 0.609877
[epoch20, step2630]: loss 0.631450
[epoch20, step2631]: loss 0.543949
[epoch20, step2632]: loss 0.352311
[epoch20, step2633]: loss 0.231827
[epoch20, step2634]: loss 0.703749
[epoch20, step2635]: loss 0.448124
[epoch20, step2636]: loss 0.549097
[epoch20, step2637]: loss 0.678018
[epoch20, step2638]: loss 0.526974
[epoch20, step2639]: loss 0.402652
[epoch20, step2640]: loss 0.465413
[epoch20, step2641]: loss 0.476068
[epoch20, step2642]: loss 0.582039
[epoch20, step2643]: loss 0.589440
[epoch20, step2644]: loss 0.566122
[epoch20, step2645]: loss 0.657388
[epoch20, step2646]: loss 0.516306
[epoch20, step2647]: loss 0.599629
[epoch20, step2648]: loss 0.330690
[epoch20, step2649]: loss 0.513391
[epoch20, step2650]: loss 0.486272
[epoch20, step2651]: loss 0.358829
[epoch20, step2652]: loss 0.373194
[epoch20, step2653]: loss 0.415614
[epoch20, step2654]: loss 0.598639
[epoch20, step2655]: loss 0.558264
[epoch20, step2656]: loss 0.420886
[epoch20, step2657]: loss 0.442937
[epoch20, step2658]: loss 0.448756
[epoch20, step2659]: loss 0.239550
[epoch20, step2660]: loss 0.518722
[epoch20, step2661]: loss 0.662825
[epoch20, step2662]: loss 0.527708
[epoch20, step2663]: loss 0.459617
[epoch20, step2664]: loss 0.341062
[epoch20, step2665]: loss 0.787281
[epoch20, step2666]: loss 0.474608
[epoch20, step2667]: loss 0.564575
[epoch20, step2668]: loss 0.301293
[epoch20, step2669]: loss 0.744166
[epoch20, step2670]: loss 0.440844
[epoch20, step2671]: loss 0.756043
[epoch20, step2672]: loss 0.503555
[epoch20, step2673]: loss 0.670217
[epoch20, step2674]: loss 0.629159
[epoch20, step2675]: loss 0.592119
[epoch20, step2676]: loss 0.801031
[epoch20, step2677]: loss 0.721988
[epoch20, step2678]: loss 0.608544
[epoch20, step2679]: loss 0.536811
[epoch20, step2680]: loss 0.327798
[epoch20, step2681]: loss 0.364358
[epoch20, step2682]: loss 0.371129
[epoch20, step2683]: loss 0.569378
[epoch20, step2684]: loss 0.406875
[epoch20, step2685]: loss 0.617232
[epoch20, step2686]: loss 0.660901
[epoch20, step2687]: loss 0.540349
[epoch20, step2688]: loss 0.470504
[epoch20, step2689]: loss 0.368777
[epoch20, step2690]: loss 0.463691
[epoch20, step2691]: loss 0.757752
[epoch20, step2692]: loss 0.309286
[epoch20, step2693]: loss 0.623397
[epoch20, step2694]: loss 0.670753
[epoch20, step2695]: loss 0.701433
[epoch20, step2696]: loss 0.328321
[epoch20, step2697]: loss 0.285709
[epoch20, step2698]: loss 0.229216
[epoch20, step2699]: loss 0.567532
[epoch20, step2700]: loss 0.634825
[epoch20, step2701]: loss 0.413125
[epoch20, step2702]: loss 0.538530
[epoch20, step2703]: loss 0.417816
[epoch20, step2704]: loss 0.488239
[epoch20, step2705]: loss 0.650990
[epoch20, step2706]: loss 0.491460
[epoch20, step2707]: loss 0.491822
[epoch20, step2708]: loss 0.675349
[epoch20, step2709]: loss 0.613767
[epoch20, step2710]: loss 0.540020
[epoch20, step2711]: loss 0.468513
[epoch20, step2712]: loss 0.661320
[epoch20, step2713]: loss 0.297006
[epoch20, step2714]: loss 0.581970
[epoch20, step2715]: loss 0.555102
[epoch20, step2716]: loss 0.616326
[epoch20, step2717]: loss 0.381376
[epoch20, step2718]: loss 0.551186
[epoch20, step2719]: loss 0.683774
[epoch20, step2720]: loss 0.649653
[epoch20, step2721]: loss 0.461201
[epoch20, step2722]: loss 0.506323
[epoch20, step2723]: loss 0.444914
[epoch20, step2724]: loss 0.678961
[epoch20, step2725]: loss 0.302475
[epoch20, step2726]: loss 0.623298
[epoch20, step2727]: loss 0.379763
[epoch20, step2728]: loss 0.440931
[epoch20, step2729]: loss 0.709039
[epoch20, step2730]: loss 0.496197
[epoch20, step2731]: loss 0.416108
[epoch20, step2732]: loss 0.413654
[epoch20, step2733]: loss 0.494497
[epoch20, step2734]: loss 0.641902
[epoch20, step2735]: loss 0.319110
[epoch20, step2736]: loss 0.691558
[epoch20, step2737]: loss 0.546009
[epoch20, step2738]: loss 0.202345
[epoch20, step2739]: loss 0.536468
[epoch20, step2740]: loss 0.585456
[epoch20, step2741]: loss 0.579363
[epoch20, step2742]: loss 0.522429
[epoch20, step2743]: loss 0.463502
[epoch20, step2744]: loss 0.568114
[epoch20, step2745]: loss 0.642778
[epoch20, step2746]: loss 0.395810
[epoch20, step2747]: loss 0.547021
[epoch20, step2748]: loss 0.570660
[epoch20, step2749]: loss 0.473092
[epoch20, step2750]: loss 0.254668
[epoch20, step2751]: loss 0.568216
[epoch20, step2752]: loss 0.616692
[epoch20, step2753]: loss 0.613741
[epoch20, step2754]: loss 0.636508
[epoch20, step2755]: loss 0.727209
[epoch20, step2756]: loss 0.400426
[epoch20, step2757]: loss 0.130619
[epoch20, step2758]: loss 0.536194
[epoch20, step2759]: loss 0.385231
[epoch20, step2760]: loss 0.575143
[epoch20, step2761]: loss 0.351342
[epoch20, step2762]: loss 0.487933
[epoch20, step2763]: loss 0.616826
[epoch20, step2764]: loss 0.552367
[epoch20, step2765]: loss 0.514391
[epoch20, step2766]: loss 0.468563
[epoch20, step2767]: loss 0.565360
[epoch20, step2768]: loss 0.521924
[epoch20, step2769]: loss 0.375963
[epoch20, step2770]: loss 0.300254
[epoch20, step2771]: loss 0.464732
[epoch20, step2772]: loss 0.400283
[epoch20, step2773]: loss 0.579759
[epoch20, step2774]: loss 0.324755
[epoch20, step2775]: loss 0.523269
[epoch20, step2776]: loss 0.755230
[epoch20, step2777]: loss 0.564110
[epoch20, step2778]: loss 0.628783
[epoch20, step2779]: loss 0.492288
[epoch20, step2780]: loss 0.507556
[epoch20, step2781]: loss 0.533268
[epoch20, step2782]: loss 0.453391
[epoch20, step2783]: loss 0.430385
[epoch20, step2784]: loss 0.640698
[epoch20, step2785]: loss 0.738260
[epoch20, step2786]: loss 0.467537
[epoch20, step2787]: loss 0.292105
[epoch20, step2788]: loss 0.486605
[epoch20, step2789]: loss 0.530633
[epoch20, step2790]: loss 0.536720
[epoch20, step2791]: loss 0.311327
[epoch20, step2792]: loss 0.641238
[epoch20, step2793]: loss 0.611481
[epoch20, step2794]: loss 0.521214
[epoch20, step2795]: loss 0.639439
[epoch20, step2796]: loss 0.646085
[epoch20, step2797]: loss 0.401835
[epoch20, step2798]: loss 0.553441
[epoch20, step2799]: loss 0.472137
[epoch20, step2800]: loss 0.420831
[epoch20, step2801]: loss 0.668854
[epoch20, step2802]: loss 0.573551
[epoch20, step2803]: loss 0.420840
[epoch20, step2804]: loss 0.586713
[epoch20, step2805]: loss 0.362043
[epoch20, step2806]: loss 0.685978
[epoch20, step2807]: loss 0.562293
[epoch20, step2808]: loss 0.571719
[epoch20, step2809]: loss 0.627361
[epoch20, step2810]: loss 0.390775
[epoch20, step2811]: loss 0.509238
[epoch20, step2812]: loss 0.253156
[epoch20, step2813]: loss 0.492748
[epoch20, step2814]: loss 0.528237
[epoch20, step2815]: loss 0.491534
[epoch20, step2816]: loss 0.376741
[epoch20, step2817]: loss 0.399754
[epoch20, step2818]: loss 0.566305
[epoch20, step2819]: loss 0.445954
[epoch20, step2820]: loss 0.471435
[epoch20, step2821]: loss 0.399790
[epoch20, step2822]: loss 0.651752
[epoch20, step2823]: loss 0.665835
[epoch20, step2824]: loss 0.593540
[epoch20, step2825]: loss 0.470710
[epoch20, step2826]: loss 0.563062
[epoch20, step2827]: loss 0.683357
[epoch20, step2828]: loss 0.413269
[epoch20, step2829]: loss 0.783450
[epoch20, step2830]: loss 0.719487
[epoch20, step2831]: loss 0.446791
[epoch20, step2832]: loss 0.627295
[epoch20, step2833]: loss 0.486993
[epoch20, step2834]: loss 0.430048
[epoch20, step2835]: loss 0.536176
[epoch20, step2836]: loss 0.527421
[epoch20, step2837]: loss 0.366332
[epoch20, step2838]: loss 0.503015
[epoch20, step2839]: loss 0.729038
[epoch20, step2840]: loss 0.575730
[epoch20, step2841]: loss 0.454029
[epoch20, step2842]: loss 0.238185
[epoch20, step2843]: loss 0.288406
[epoch20, step2844]: loss 0.414067
[epoch20, step2845]: loss 0.541422
[epoch20, step2846]: loss 0.222900
[epoch20, step2847]: loss 0.728526
[epoch20, step2848]: loss 0.551479
[epoch20, step2849]: loss 0.314500
[epoch20, step2850]: loss 0.493203
[epoch20, step2851]: loss 0.354266
[epoch20, step2852]: loss 0.318887
[epoch20, step2853]: loss 0.621186
[epoch20, step2854]: loss 0.552740
[epoch20, step2855]: loss 0.563217
[epoch20, step2856]: loss 0.746369
[epoch20, step2857]: loss 0.559984
[epoch20, step2858]: loss 0.462255
[epoch20, step2859]: loss 0.667879
[epoch20, step2860]: loss 0.543668
[epoch20, step2861]: loss 0.648449
[epoch20, step2862]: loss 0.344803
[epoch20, step2863]: loss 0.577658
[epoch20, step2864]: loss 0.549872
[epoch20, step2865]: loss 0.359522
[epoch20, step2866]: loss 0.610454
[epoch20, step2867]: loss 0.687846
[epoch20, step2868]: loss 0.691622
[epoch20, step2869]: loss 0.513659
[epoch20, step2870]: loss 0.438874
[epoch20, step2871]: loss 0.380897
[epoch20, step2872]: loss 0.811783
[epoch20, step2873]: loss 0.498625
[epoch20, step2874]: loss 0.568803
[epoch20, step2875]: loss 0.392462
[epoch20, step2876]: loss 0.474593
[epoch20, step2877]: loss 0.312854
[epoch20, step2878]: loss 0.577047
[epoch20, step2879]: loss 0.351374
[epoch20, step2880]: loss 0.302507
[epoch20, step2881]: loss 0.643519
[epoch20, step2882]: loss 0.645242
[epoch20, step2883]: loss 0.536487
[epoch20, step2884]: loss 0.473404
[epoch20, step2885]: loss 0.499629
[epoch20, step2886]: loss 0.483764
[epoch20, step2887]: loss 0.426017
[epoch20, step2888]: loss 0.339752
[epoch20, step2889]: loss 0.382288
[epoch20, step2890]: loss 0.739678
[epoch20, step2891]: loss 0.558291
[epoch20, step2892]: loss 0.558309
[epoch20, step2893]: loss 0.436134
[epoch20, step2894]: loss 0.770409
[epoch20, step2895]: loss 0.606930
[epoch20, step2896]: loss 0.523340
[epoch20, step2897]: loss 0.563595
[epoch20, step2898]: loss 0.457932
[epoch20, step2899]: loss 0.541615
[epoch20, step2900]: loss 0.281506
[epoch20, step2901]: loss 0.593994
[epoch20, step2902]: loss 0.563668
[epoch20, step2903]: loss 0.625853
[epoch20, step2904]: loss 0.324624
[epoch20, step2905]: loss 0.752781
[epoch20, step2906]: loss 0.519223
[epoch20, step2907]: loss 0.303119
[epoch20, step2908]: loss 0.348811
[epoch20, step2909]: loss 0.655772
[epoch20, step2910]: loss 0.593294
[epoch20, step2911]: loss 0.266128
[epoch20, step2912]: loss 0.510203
[epoch20, step2913]: loss 0.451552
[epoch20, step2914]: loss 0.641991
[epoch20, step2915]: loss 0.391627
[epoch20, step2916]: loss 0.657036
[epoch20, step2917]: loss 0.583901
[epoch20, step2918]: loss 0.379543
[epoch20, step2919]: loss 0.339603
[epoch20, step2920]: loss 0.559040
[epoch20, step2921]: loss 0.591116
[epoch20, step2922]: loss 0.281284
[epoch20, step2923]: loss 0.728503
[epoch20, step2924]: loss 0.454969
[epoch20, step2925]: loss 0.509654
[epoch20, step2926]: loss 0.639120
[epoch20, step2927]: loss 0.720336
[epoch20, step2928]: loss 0.334750
[epoch20, step2929]: loss 0.497401
[epoch20, step2930]: loss 0.155559
[epoch20, step2931]: loss 0.561183
[epoch20, step2932]: loss 0.561780
[epoch20, step2933]: loss 0.416717
[epoch20, step2934]: loss 0.184386
[epoch20, step2935]: loss 0.614257
[epoch20, step2936]: loss 0.276542
[epoch20, step2937]: loss 0.761817
[epoch20, step2938]: loss 0.187988
[epoch20, step2939]: loss 0.309927
[epoch20, step2940]: loss 0.640644
[epoch20, step2941]: loss 0.634309
[epoch20, step2942]: loss 0.506726
[epoch20, step2943]: loss 0.623734
[epoch20, step2944]: loss 0.513190
[epoch20, step2945]: loss 0.518210
[epoch20, step2946]: loss 0.604644
[epoch20, step2947]: loss 0.321196
[epoch20, step2948]: loss 0.671514
[epoch20, step2949]: loss 0.367748
[epoch20, step2950]: loss 0.570539
[epoch20, step2951]: loss 0.547819
[epoch20, step2952]: loss 0.567156
[epoch20, step2953]: loss 0.532026
[epoch20, step2954]: loss 0.636993
[epoch20, step2955]: loss 0.583825
[epoch20, step2956]: loss 0.518251
[epoch20, step2957]: loss 0.581419
[epoch20, step2958]: loss 0.390045
[epoch20, step2959]: loss 0.330496
[epoch20, step2960]: loss 0.402080
[epoch20, step2961]: loss 0.588421
[epoch20, step2962]: loss 0.637753
[epoch20, step2963]: loss 0.405525
[epoch20, step2964]: loss 0.268816
[epoch20, step2965]: loss 0.461089
[epoch20, step2966]: loss 0.191376
[epoch20, step2967]: loss 0.491472
[epoch20, step2968]: loss 0.367423
[epoch20, step2969]: loss 0.591044
[epoch20, step2970]: loss 0.566694
[epoch20, step2971]: loss 0.574663
[epoch20, step2972]: loss 0.724681
[epoch20, step2973]: loss 0.672583
[epoch20, step2974]: loss 0.441161
[epoch20, step2975]: loss 0.413524
[epoch20, step2976]: loss 0.497335
[epoch20, step2977]: loss 0.459982
[epoch20, step2978]: loss 0.344763
[epoch20, step2979]: loss 0.633468
[epoch20, step2980]: loss 0.576292
[epoch20, step2981]: loss 0.398363
[epoch20, step2982]: loss 0.370703
[epoch20, step2983]: loss 0.466562
[epoch20, step2984]: loss 0.603383
[epoch20, step2985]: loss 0.755966
[epoch20, step2986]: loss 0.196416
[epoch20, step2987]: loss 0.698663
[epoch20, step2988]: loss 0.444378
[epoch20, step2989]: loss 0.380564
[epoch20, step2990]: loss 0.587661
[epoch20, step2991]: loss 0.711160
[epoch20, step2992]: loss 0.583294
[epoch20, step2993]: loss 0.181734
[epoch20, step2994]: loss 0.334846
[epoch20, step2995]: loss 0.328998
[epoch20, step2996]: loss 0.363367
[epoch20, step2997]: loss 0.622565
[epoch20, step2998]: loss 0.589658
[epoch20, step2999]: loss 0.413178
[epoch20, step3000]: loss 0.351647
[epoch20, step3001]: loss 0.588811
[epoch20, step3002]: loss 0.666658
[epoch20, step3003]: loss 0.446773
[epoch20, step3004]: loss 0.300920
[epoch20, step3005]: loss 0.723995
[epoch20, step3006]: loss 0.738753
[epoch20, step3007]: loss 0.424815
[epoch20, step3008]: loss 0.377657
[epoch20, step3009]: loss 0.515225
[epoch20, step3010]: loss 0.502723
[epoch20, step3011]: loss 0.746371
[epoch20, step3012]: loss 0.578481
[epoch20, step3013]: loss 0.651752
[epoch20, step3014]: loss 0.560952
[epoch20, step3015]: loss 0.561810
[epoch20, step3016]: loss 0.465963
[epoch20, step3017]: loss 0.706833
[epoch20, step3018]: loss 0.285534
[epoch20, step3019]: loss 0.486954
[epoch20, step3020]: loss 0.579352
[epoch20, step3021]: loss 0.705367
[epoch20, step3022]: loss 0.474553
[epoch20, step3023]: loss 0.590518
[epoch20, step3024]: loss 0.425173
[epoch20, step3025]: loss 0.498923
[epoch20, step3026]: loss 0.631499
[epoch20, step3027]: loss 0.308363
[epoch20, step3028]: loss 0.729864
[epoch20, step3029]: loss 0.522560
[epoch20, step3030]: loss 0.377485
[epoch20, step3031]: loss 0.730514
[epoch20, step3032]: loss 0.622041
[epoch20, step3033]: loss 0.550443
[epoch20, step3034]: loss 0.579007
[epoch20, step3035]: loss 0.588140
[epoch20, step3036]: loss 0.579838
[epoch20, step3037]: loss 0.724979
[epoch20, step3038]: loss 0.481672
[epoch20, step3039]: loss 0.338036
[epoch20, step3040]: loss 0.600655
[epoch20, step3041]: loss 0.702888
[epoch20, step3042]: loss 0.468659
[epoch20, step3043]: loss 0.275790
[epoch20, step3044]: loss 0.678931
[epoch20, step3045]: loss 0.627150
[epoch20, step3046]: loss 0.630155
[epoch20, step3047]: loss 0.345460
[epoch20, step3048]: loss 0.578604
[epoch20, step3049]: loss 0.658150
[epoch20, step3050]: loss 0.439140
[epoch20, step3051]: loss 0.448983
[epoch20, step3052]: loss 0.552166
[epoch20, step3053]: loss 0.270766
[epoch20, step3054]: loss 0.535976
[epoch20, step3055]: loss 0.755745
[epoch20, step3056]: loss 0.462907
[epoch20, step3057]: loss 0.680868
[epoch20, step3058]: loss 0.679156
[epoch20, step3059]: loss 0.560235
[epoch20, step3060]: loss 0.560642
[epoch20, step3061]: loss 0.697632
[epoch20, step3062]: loss 0.603838
[epoch20, step3063]: loss 0.481305
[epoch20, step3064]: loss 0.791291
[epoch20, step3065]: loss 0.270527
[epoch20, step3066]: loss 0.603838
[epoch20, step3067]: loss 0.676157
[epoch20, step3068]: loss 0.651444
[epoch20, step3069]: loss 0.586433
[epoch20, step3070]: loss 0.270155
[epoch20, step3071]: loss 0.586957
[epoch20, step3072]: loss 0.717635
[epoch20, step3073]: loss 0.501952
[epoch20, step3074]: loss 0.442689
[epoch20, step3075]: loss 0.394483
[epoch20, step3076]: loss 0.631037

[epoch20]: avg loss 0.631037

[TEST step1]: loss 0.348721
[TEST step2]: loss 0.628512
[TEST step3]: loss 0.519940
[TEST step4]: loss 0.403994
[TEST step5]: loss 0.454125
[TEST step6]: loss 0.595983
[TEST step7]: loss 0.544042
[TEST step8]: loss 0.493535
[TEST step9]: loss 0.494333
[TEST step10]: loss 0.539930
[TEST step11]: loss 0.534579
[TEST step12]: loss 0.301484
[TEST step13]: loss 0.505811
[TEST step14]: loss 0.721446
[TEST step15]: loss 0.478562
[TEST step16]: loss 0.518624
[TEST step17]: loss 0.523699
[TEST step18]: loss 0.400165
[TEST step19]: loss 0.585046
[TEST step20]: loss 0.534690
[TEST step21]: loss 0.637048
[TEST step22]: loss 0.547985
[TEST step23]: loss 0.813643
[TEST step24]: loss 0.505041
[TEST step25]: loss 0.514441
[TEST step26]: loss 0.449739
[TEST step27]: loss 0.414203
[TEST step28]: loss 0.680530
[TEST step29]: loss 0.515392
[TEST step30]: loss 0.595471
[TEST step31]: loss 0.568386
[TEST step32]: loss 0.449558
[TEST step33]: loss 0.354835
[TEST step34]: loss 0.525196
[TEST step35]: loss 0.363308
[TEST step36]: loss 0.486513
[TEST step37]: loss 0.667558
[TEST step38]: loss 0.453491
[TEST step39]: loss 0.595146
[TEST step40]: loss 0.432734
[TEST step41]: loss 0.517901
[TEST step42]: loss 0.439787
[TEST step43]: loss 0.520468
[TEST step44]: loss 0.516886
[TEST step45]: loss 0.330526
[TEST step46]: loss 0.632719
[TEST step47]: loss 0.290387
[TEST step48]: loss 0.869142
[TEST step49]: loss 0.376822
[TEST step50]: loss 0.692638
[TEST step51]: loss 0.304205
[TEST step52]: loss 0.710254
[TEST step53]: loss 0.275854
[TEST step54]: loss 0.673082
[TEST step55]: loss 0.378495
[TEST step56]: loss 0.435190
[TEST step57]: loss 0.607556
[TEST step58]: loss 0.258044
[TEST step59]: loss 0.532977
[TEST step60]: loss 0.441709
[TEST step61]: loss 0.506301
[TEST step62]: loss 0.381297
[TEST step63]: loss 0.296380
[TEST step64]: loss 0.765570
[TEST step65]: loss 0.600469
[TEST step66]: loss 0.592733
[TEST step67]: loss 0.387620
[TEST step68]: loss 0.718378
[TEST step69]: loss 0.577127
[TEST step70]: loss 0.478521
[TEST step71]: loss 0.411378
[TEST step72]: loss 0.675562
[TEST step73]: loss 0.349245
[TEST step74]: loss 0.635216
[TEST step75]: loss 0.646350
[TEST step76]: loss 0.575868
[TEST step77]: loss 0.412276
[TEST step78]: loss 0.504438
[TEST step79]: loss 0.378933
[TEST step80]: loss 0.475321
[TEST step81]: loss 0.480620
[TEST step82]: loss 0.443348
[TEST step83]: loss 0.606505
[TEST step84]: loss 0.506249
[TEST step85]: loss 0.407268
[TEST step86]: loss 0.249228
[TEST step87]: loss 0.619141
[TEST step88]: loss 0.470077
[TEST step89]: loss 0.424484
[TEST step90]: loss 0.568267
[TEST step91]: loss 0.390657
[TEST step92]: loss 0.262537
[TEST step93]: loss 0.559511
[TEST step94]: loss 0.278755
[TEST step95]: loss 0.517865
[TEST step96]: loss 0.544871
[TEST step97]: loss 0.659934
[TEST step98]: loss 0.463475
[TEST step99]: loss 0.392599
[TEST step100]: loss 0.460951
[TEST step101]: loss 0.400929
[TEST step102]: loss 0.701389
[TEST step103]: loss 0.409039
[TEST step104]: loss 0.627147
[TEST step105]: loss 0.327270
[TEST step106]: loss 0.354969
[TEST step107]: loss 0.387810
[TEST step108]: loss 0.446849
[TEST step109]: loss 0.328502
[TEST step110]: loss 0.482621
[TEST step111]: loss 0.526973
[TEST step112]: loss 0.618026
[TEST step113]: loss 0.767418
[TEST step114]: loss 0.473364
[TEST step115]: loss 0.480429
[TEST step116]: loss 0.607843
[TEST step117]: loss 0.463390
[TEST step118]: loss 0.238140
[TEST step119]: loss 0.392715
[TEST step120]: loss 0.401510
[TEST step121]: loss 0.614390
[TEST step122]: loss 0.374639
[TEST step123]: loss 0.437439
[TEST step124]: loss 0.418321
[TEST step125]: loss 0.375993
[TEST step126]: loss 0.437349
[TEST step127]: loss 0.524715
[TEST step128]: loss 0.662811
[TEST step129]: loss 0.456153
[TEST step130]: loss 0.525128
[TEST step131]: loss 0.333427
[TEST step132]: loss 0.553115
[TEST step133]: loss 0.501499
[TEST step134]: loss 0.504885
[TEST step135]: loss 0.519459
[TEST step136]: loss 0.404473
[TEST step137]: loss 0.466840
[TEST step138]: loss 0.625656
[TEST step139]: loss 0.477775
[TEST step140]: loss 0.512580
[TEST step141]: loss 0.485467
[TEST step142]: loss 0.602181
[TEST step143]: loss 0.491797
[TEST step144]: loss 0.267713
[TEST step145]: loss 0.351474
[TEST step146]: loss 0.468594
[TEST step147]: loss 0.529666
[TEST step148]: loss 0.587981
[TEST step149]: loss 0.462343
[TEST step150]: loss 0.604649
[TEST step151]: loss 0.444060
[TEST step152]: loss 0.731566
[TEST step153]: loss 0.397400
[TEST step154]: loss 0.392456
[TEST step155]: loss 0.523437
[TEST step156]: loss 0.616064
[TEST step157]: loss 0.532264
[TEST step158]: loss 0.522891
[TEST step159]: loss 0.460638
[TEST step160]: loss 0.713597
[TEST step161]: loss 0.540578
[TEST step162]: loss 0.429074
[TEST step163]: loss 0.589740
[TEST step164]: loss 0.577488
[TEST step165]: loss 0.464618
[TEST step166]: loss 0.773062
[TEST step167]: loss 0.545264
[TEST step168]: loss 0.485457
[TEST step169]: loss 0.615693
[TEST step170]: loss 0.340264
[TEST step171]: loss 0.479835
[TEST step172]: loss 0.382769
[TEST step173]: loss 0.444884
[TEST step174]: loss 0.530493
[TEST step175]: loss 0.225593
[TEST step176]: loss 0.454835
[TEST step177]: loss 0.475117
[TEST step178]: loss 0.511939
[TEST step179]: loss 0.602315
[TEST step180]: loss 0.834011
[TEST step181]: loss 0.818252
[TEST step182]: loss 0.381141
[TEST step183]: loss 0.440362
[TEST step184]: loss 0.706354
[TEST step185]: loss 0.725147
[TEST step186]: loss 0.452285
[TEST step187]: loss 0.513561
[TEST step188]: loss 0.583974
[TEST step189]: loss 0.572411
[TEST step190]: loss 0.602541
[TEST step191]: loss 0.492851
[TEST step192]: loss 0.657083
[TEST step193]: loss 0.468250
[TEST step194]: loss 0.463220
[TEST step195]: loss 0.662319
[TEST step196]: loss 0.397762
[TEST step197]: loss 0.434109
[TEST step198]: loss 0.690158
[TEST step199]: loss 0.631935
[TEST step200]: loss 0.135210
[TEST step201]: loss 0.542973
[TEST step202]: loss 0.230246
[TEST step203]: loss 0.532916
[TEST step204]: loss 0.335320
[TEST step205]: loss 0.515608
[TEST step206]: loss 0.292184
[TEST step207]: loss 0.505714
[TEST step208]: loss 0.590039
[TEST step209]: loss 0.629854
[TEST step210]: loss 0.467560
[TEST step211]: loss 0.492733
[TEST step212]: loss 0.580519
[TEST step213]: loss 0.417359
[TEST step214]: loss 0.411283
[TEST step215]: loss 0.348772
[TEST step216]: loss 0.311960
[TEST step217]: loss 0.625560
[TEST step218]: loss 0.536453
[TEST step219]: loss 0.543092
[TEST step220]: loss 0.511814
[TEST step221]: loss 0.215216
[TEST step222]: loss 0.564304
[TEST step223]: loss 0.602295
[TEST step224]: loss 0.443794
[TEST step225]: loss 0.411780
[TEST step226]: loss 0.497730
[TEST step227]: loss 0.565058
[TEST step228]: loss 0.723058
[TEST step229]: loss 0.415904
[TEST step230]: loss 0.515205
[TEST step231]: loss 0.398633
[TEST step232]: loss 0.614182
[TEST step233]: loss 0.468880
[TEST step234]: loss 0.377565
[TEST step235]: loss 0.738615
[TEST step236]: loss 0.453789
[TEST step237]: loss 0.483586
[TEST step238]: loss 0.470786
[TEST step239]: loss 0.592909
[TEST step240]: loss 0.560915
[TEST step241]: loss 0.360904
[TEST step242]: loss 0.572165
[TEST step243]: loss 0.462640
[TEST step244]: loss 0.606883
[TEST step245]: loss 0.435278
[TEST step246]: loss 0.528250
[TEST step247]: loss 0.575289
[TEST step248]: loss 0.547462
[TEST step249]: loss 0.498184
[TEST step250]: loss 0.276542
[TEST step251]: loss 0.465002
[TEST step252]: loss 0.835729
[TEST step253]: loss 0.541751
[TEST step254]: loss 0.675503
[TEST step255]: loss 0.574309
[TEST step256]: loss 0.595027
[TEST step257]: loss 0.167596
[TEST step258]: loss 0.715883
[TEST step259]: loss 0.456356
[TEST step260]: loss 0.415351
[TEST step261]: loss 0.664960
[TEST step262]: loss 0.347687
[TEST step263]: loss 0.750432
[TEST step264]: loss 0.693441
[TEST step265]: loss 0.713564
[TEST step266]: loss 0.625510
[TEST step267]: loss 0.564858
[TEST step268]: loss 0.587389
[TEST step269]: loss 0.359484
[TEST step270]: loss 0.348378
[TEST step271]: loss 0.495993
[TEST step272]: loss 0.651352
[TEST step273]: loss 0.262774
[TEST step274]: loss 0.541320
[TEST step275]: loss 0.526087
[TEST step276]: loss 0.527706
[TEST step277]: loss 0.545301
[TEST step278]: loss 0.315729
[TEST step279]: loss 0.647494
[TEST step280]: loss 0.489147
[TEST step281]: loss 0.660048
[TEST step282]: loss 0.413772
[TEST step283]: loss 0.675794
[TEST step284]: loss 0.583294
[TEST step285]: loss 0.568290
[TEST step286]: loss 0.695841
[TEST step287]: loss 0.441653
[TEST step288]: loss 0.454002
[TEST step289]: loss 0.527730
[TEST step290]: loss 0.588367
[TEST step291]: loss 0.604116
[TEST step292]: loss 0.620111
[TEST step293]: loss 0.391998
[TEST step294]: loss 0.479080
[TEST step295]: loss 0.245645
[TEST step296]: loss 0.563436
[TEST step297]: loss 0.602778
[TEST step298]: loss 0.672340
[TEST step299]: loss 0.605032
[TEST step300]: loss 0.568646
[TEST step301]: loss 0.723784
[TEST step302]: loss 0.714588
[TEST step303]: loss 0.702068
[TEST step304]: loss 0.465565
[TEST step305]: loss 0.421800
[TEST step306]: loss 0.499356
[TEST step307]: loss 0.450699
[TEST step308]: loss 0.530312
[TEST step309]: loss 0.635522
[TEST step310]: loss 0.557227
[TEST step311]: loss 0.407875
[TEST step312]: loss 0.602518
[TEST step313]: loss 0.530578
[TEST step314]: loss 0.577635
[TEST step315]: loss 0.483035
[TEST step316]: loss 0.644243
[TEST step317]: loss 0.331373
[TEST step318]: loss 0.598361
[TEST step319]: loss 0.644174
[TEST step320]: loss 0.616626
[TEST step321]: loss 0.238551
[TEST step322]: loss 0.561293
[TEST step323]: loss 0.552521
[TEST step324]: loss 0.521086
[TEST step325]: loss 0.699498
[TEST step326]: loss 0.741582
[TEST step327]: loss 0.453160
[TEST step328]: loss 0.539019
[TEST step329]: loss 0.562430
[TEST step330]: loss 0.559307
[TEST step331]: loss 0.575458
[TEST step332]: loss 0.719830
[TEST step333]: loss 0.757438
[TEST step334]: loss 0.585423
[TEST step335]: loss 0.659056
[TEST step336]: loss 0.408530
[TEST step337]: loss 0.381691
[TEST step338]: loss 0.563567
[TEST step339]: loss 0.454113
[TEST step340]: loss 0.647638
[TEST step341]: loss 0.483243
[TEST step342]: loss 0.682523
[TEST step343]: loss 0.611290
[TEST step344]: loss 0.566664
[TEST step345]: loss 0.671713
[TEST step346]: loss 0.592904
[TEST step347]: loss 0.389598
[TEST step348]: loss 0.619871
[TEST step349]: loss 0.393395
[TEST step350]: loss 0.504424
[TEST step351]: loss 0.556038
[TEST step352]: loss 0.356938
[TEST step353]: loss 0.527742
[TEST step354]: loss 0.738057
[TEST step355]: loss 0.352113
[TEST step356]: loss 0.397541
[TEST step357]: loss 0.580362
[TEST step358]: loss 0.617186
[TEST step359]: loss 0.385774
[TEST step360]: loss 0.539904
[TEST step361]: loss 0.518852
[TEST step362]: loss 0.530504
[TEST step363]: loss 0.804949
[TEST step364]: loss 0.463647
[TEST step365]: loss 0.585407
[TEST step366]: loss 0.508845
[TEST step367]: loss 0.468969
[TEST step368]: loss 0.539518
[TEST step369]: loss 0.532102
[TEST step370]: loss 0.522219
[TEST step371]: loss 0.598647
[TEST step372]: loss 0.414304
[TEST step373]: loss 0.513692
[TEST step374]: loss 0.131216
[TEST step375]: loss 0.523207
[TEST step376]: loss 0.530034
[TEST step377]: loss 0.348408
[TEST step378]: loss 0.446479
[TEST step379]: loss 0.576351
[TEST step380]: loss 0.659770
[TEST step381]: loss 0.504592
[TEST step382]: loss 0.460001
[TEST step383]: loss 0.508102
[TEST step384]: loss 0.541504
[TEST step385]: loss 0.656840
[TEST step386]: loss 0.486241
[TEST step387]: loss 0.385973
[TEST step388]: loss 0.562070
[TEST step389]: loss 0.433055
[TEST step390]: loss 0.712736
[TEST step391]: loss 0.186639
[TEST step392]: loss 0.726794
[TEST step393]: loss 0.545957
[TEST step394]: loss 0.523609
[TEST step395]: loss 0.569970
[TEST step396]: loss 0.601999
[TEST step397]: loss 0.494475
[TEST step398]: loss 0.540238
[TEST step399]: loss 0.622845
[TEST step400]: loss 0.602859
[TEST step401]: loss 0.498340
[TEST step402]: loss 0.443484
[TEST step403]: loss 0.405934
[TEST step404]: loss 0.651024
[TEST step405]: loss 0.537085
[TEST step406]: loss 0.499643
[TEST step407]: loss 0.174306
[TEST step408]: loss 0.619591
[TEST step409]: loss 0.602273
[TEST step410]: loss 0.410741
[TEST step411]: loss 0.438526
[TEST step412]: loss 0.521483
[TEST step413]: loss 0.297131
[TEST step414]: loss 0.325760
[TEST step415]: loss 0.551217
[TEST step416]: loss 0.243740
[TEST step417]: loss 0.765396
[TEST step418]: loss 0.453521
[TEST step419]: loss 0.628519
[TEST step420]: loss 0.302336
[TEST step421]: loss 0.240873
[TEST step422]: loss 0.413618
[TEST step423]: loss 0.536795
[TEST step424]: loss 0.374488
[TEST step425]: loss 0.465858
[TEST step426]: loss 0.549184
[TEST step427]: loss 0.414738
[TEST step428]: loss 0.425753
[TEST step429]: loss 0.406465
[TEST step430]: loss 0.586977
[TEST step431]: loss 0.601307
[TEST step432]: loss 0.270295
[TEST step433]: loss 0.398514
[TEST step434]: loss 0.622749
[TEST step435]: loss 0.739453
[TEST step436]: loss 0.378934
[TEST step437]: loss 0.503368
[TEST step438]: loss 0.484163
[TEST step439]: loss 0.668966
[TEST step440]: loss 0.583047
[TEST step441]: loss 0.557792
[TEST step442]: loss 0.734647
[TEST step443]: loss 0.421343
[TEST step444]: loss 0.494061
[TEST step445]: loss 0.450924
[TEST step446]: loss 0.506020
[TEST step447]: loss 0.152756
[TEST step448]: loss 0.403804
[TEST step449]: loss 0.701741
[TEST step450]: loss 0.549080
[TEST step451]: loss 0.548092
[TEST step452]: loss 0.678655
[TEST step453]: loss 0.709180
[TEST step454]: loss 0.649708
[TEST step455]: loss 0.291261
[TEST step456]: loss 0.756103
[TEST step457]: loss 0.566640
[TEST step458]: loss 0.799074
[TEST step459]: loss 0.571635
[TEST step460]: loss 0.488284
[TEST step461]: loss 0.393314
[TEST step462]: loss 0.442381
[TEST step463]: loss 0.589390
[TEST step464]: loss 0.365036
[TEST step465]: loss 0.619627
[TEST step466]: loss 0.836484
[TEST step467]: loss 0.582518
[TEST step468]: loss 0.357878
[TEST step469]: loss 0.252076
[TEST step470]: loss 0.548349
[TEST step471]: loss 0.607269
[TEST step472]: loss 0.504885
[TEST step473]: loss 0.392914
[TEST step474]: loss 0.311104
[TEST step475]: loss 0.664122
[TEST step476]: loss 0.629378
[TEST step477]: loss 0.263201
[TEST step478]: loss 0.594894
[TEST step479]: loss 0.651449
[TEST step480]: loss 0.615394
[TEST step481]: loss 0.553628
[TEST step482]: loss 0.645569
[TEST step483]: loss 0.672320
[TEST step484]: loss 0.484667
[TEST step485]: loss 0.456459
[TEST step486]: loss 0.436989
[TEST step487]: loss 0.596350
[TEST step488]: loss 0.366311
[TEST step489]: loss 0.391654
[TEST step490]: loss 0.770101
[TEST step491]: loss 0.498586
[TEST step492]: loss 0.561752
[TEST step493]: loss 0.505078
[TEST step494]: loss 0.485894
[TEST step495]: loss 0.630979
[TEST step496]: loss 0.531911
[TEST step497]: loss 0.435236
[TEST step498]: loss 0.416243
[TEST step499]: loss 0.665145
[TEST step500]: loss 0.284262
[TEST step501]: loss 0.526995
[TEST step502]: loss 0.603616
[TEST step503]: loss 0.647396
[TEST step504]: loss 0.563549
[TEST step505]: loss 0.591953
[TEST step506]: loss 0.577785
[TEST step507]: loss 0.655970
[TEST step508]: loss 0.428692
[TEST step509]: loss 0.351274
[TEST step510]: loss 0.424752
[TEST step511]: loss 0.403844
[TEST step512]: loss 0.499032
[TEST step513]: loss 0.279434
[TEST step514]: loss 0.616111
[TEST step515]: loss 0.668909
[TEST step516]: loss 0.533783
[TEST step517]: loss 0.482471
[TEST step518]: loss 0.323486
[TEST step519]: loss 0.556676
[TEST step520]: loss 0.565710
[TEST step521]: loss 0.652675
[TEST step522]: loss 0.486037
[TEST step523]: loss 0.633009
[TEST step524]: loss 0.712680
[TEST step525]: loss 0.604701
[TEST step526]: loss 0.622681
[TEST step527]: loss 0.470721
[TEST step528]: loss 0.636715
[TEST step529]: loss 0.333994
[TEST step530]: loss 0.713957
[TEST step531]: loss 0.356346
[TEST step532]: loss 0.474368
[TEST step533]: loss 0.499860
[TEST step534]: loss 0.454280
[TEST step535]: loss 0.404876
[TEST step536]: loss 0.317773
[TEST step537]: loss 0.424200
[TEST step538]: loss 0.637099
[TEST step539]: loss 0.687703
[TEST step540]: loss 0.388334
[TEST step541]: loss 0.566542
[TEST step542]: loss 0.632545
[TEST step543]: loss 0.637923
[TEST step544]: loss 0.707758
[TEST step545]: loss 0.600140
[TEST step546]: loss 0.389484
[TEST step547]: loss 0.563093
[TEST step548]: loss 0.580675
[TEST step549]: loss 0.522777
[TEST step550]: loss 0.585965
[TEST step551]: loss 0.629603
[TEST step552]: loss 0.635535
[TEST step553]: loss 0.576307
[TEST step554]: loss 0.376866
[TEST step555]: loss 0.506526
[TEST step556]: loss 0.604771
[TEST step557]: loss 0.533847
[TEST step558]: loss 0.502743
[TEST step559]: loss 0.585476
[TEST step560]: loss 0.506970
[TEST step561]: loss 0.513991
[TEST step562]: loss 0.483311
[TEST step563]: loss 0.675510
[TEST step564]: loss 0.617182
[TEST step565]: loss 0.710177
[TEST step566]: loss 0.561508
[TEST step567]: loss 0.423527
[TEST step568]: loss 0.560933
[TEST step569]: loss 0.398726
[TEST step570]: loss 0.701545
[TEST step571]: loss 0.478526
[TEST step572]: loss 0.643974
[TEST step573]: loss 0.250271
[TEST step574]: loss 0.474454
[TEST step575]: loss 0.583867
[TEST step576]: loss 0.576632
[TEST step577]: loss 0.136963
[TEST step578]: loss 0.462117
[TEST step579]: loss 0.474804
[TEST step580]: loss 0.680969
[TEST step581]: loss 0.704071
[TEST step582]: loss 0.617998
[TEST step583]: loss 0.615126
[TEST step584]: loss 0.347346
[TEST step585]: loss 0.539935
[TEST step586]: loss 0.531319
[TEST step587]: loss 0.607732
[TEST step588]: loss 0.468169
[TEST step589]: loss 0.531373
[TEST step590]: loss 0.662150
[TEST step591]: loss 0.716298
[TEST step592]: loss 0.629854
[TEST step593]: loss 0.414982
[TEST step594]: loss 0.385293
[TEST step595]: loss 0.364439
[TEST step596]: loss 0.526172
[TEST step597]: loss 0.667195
[TEST step598]: loss 0.354295
[TEST step599]: loss 0.425511
[TEST step600]: loss 0.465456
[TEST step601]: loss 0.373603
[TEST step602]: loss 0.566667
[TEST step603]: loss 0.470688
[TEST step604]: loss 0.552134
[TEST step605]: loss 0.690159
[TEST step606]: loss 0.237279
[TEST step607]: loss 0.584328
[TEST step608]: loss 0.409825
[TEST step609]: loss 0.490006
[TEST step610]: loss 0.473982
[TEST step611]: loss 0.433987
[TEST step612]: loss 0.270831
[TEST step613]: loss 0.533342
[TEST step614]: loss 0.376973
[TEST step615]: loss 0.577319
[TEST step616]: loss 0.656886
[TEST step617]: loss 0.529528
[TEST step618]: loss 0.280750
[TEST step619]: loss 0.404393
[TEST step620]: loss 0.514041
[TEST step621]: loss 0.492105
[TEST step622]: loss 0.725264
[TEST step623]: loss 0.637097
[TEST step624]: loss 0.542918
[TEST step625]: loss 0.559541
[TEST step626]: loss 0.293750
[TEST step627]: loss 0.536709
[TEST step628]: loss 0.511741
[TEST step629]: loss 0.481438
[TEST step630]: loss 0.474938
[TEST step631]: loss 0.277968
[TEST step632]: loss 0.497138
[TEST step633]: loss 0.219452
[TEST step634]: loss 0.379044
[TEST step635]: loss 0.455613
[TEST step636]: loss 0.477555
[TEST step637]: loss 0.474003
[TEST step638]: loss 0.429648
[TEST step639]: loss 0.604241
[TEST step640]: loss 0.718338
[TEST step641]: loss 0.572326
[TEST step642]: loss 0.746157
[TEST step643]: loss 0.642619
[TEST step644]: loss 0.743456
[TEST step645]: loss 0.254944
[TEST step646]: loss 0.538311
[TEST step647]: loss 0.472017
[TEST step648]: loss 0.520886
[TEST step649]: loss 0.349512
[TEST step650]: loss 0.508596
[TEST step651]: loss 0.553510
[TEST step652]: loss 0.284102
[TEST step653]: loss 0.478766
[TEST step654]: loss 0.594149
[TEST step655]: loss 0.599720
[TEST step656]: loss 0.474263
[TEST step657]: loss 0.401388
[TEST step658]: loss 0.613153
[TEST step659]: loss 0.643328
[TEST step660]: loss 0.590716
[TEST step661]: loss 0.455369
[TEST step662]: loss 0.336078
[TEST step663]: loss 0.490640
[TEST step664]: loss 0.682484
[TEST step665]: loss 0.732357
[TEST step666]: loss 0.456432
[TEST step667]: loss 0.518329
[TEST step668]: loss 0.488016
[TEST step669]: loss 0.623124
[TEST step670]: loss 0.473461
[TEST step671]: loss 0.643764
[TEST step672]: loss 0.390259
[TEST step673]: loss 0.663568
[TEST step674]: loss 0.278815
[TEST step675]: loss 0.496826
[TEST step676]: loss 0.652672
[TEST step677]: loss 0.561646
[TEST step678]: loss 0.480629
[TEST step679]: loss 0.413954
[TEST step680]: loss 0.366457
[TEST step681]: loss 0.609325
[TEST step682]: loss 0.571979
[TEST step683]: loss 0.377088
[TEST step684]: loss 0.614789
[TEST step685]: loss 0.335966
[TEST step686]: loss 0.691601
[TEST step687]: loss 0.372383
[TEST step688]: loss 0.389324
[TEST step689]: loss 0.489331
[TEST step690]: loss 0.544048
[TEST step691]: loss 0.786114
[TEST step692]: loss 0.754441
[TEST step693]: loss 0.641715
[TEST step694]: loss 0.652026
[TEST step695]: loss 0.439736
[TEST step696]: loss 0.460892
[TEST step697]: loss 0.562177
[TEST step698]: loss 0.526598
[TEST step699]: loss 0.433860
[TEST step700]: loss 0.688055
[TEST step701]: loss 0.269484
[TEST step702]: loss 0.322187
[TEST step703]: loss 0.453774
[TEST step704]: loss 0.661208
[TEST step705]: loss 0.549152
[TEST step706]: loss 0.385227
[TEST step707]: loss 0.496581
[TEST step708]: loss 0.411739
[TEST step709]: loss 0.373472
[TEST step710]: loss 0.521656
[TEST step711]: loss 0.447748
[TEST step712]: loss 0.520465
[TEST step713]: loss 0.560276
[TEST step714]: loss 0.414232
[TEST step715]: loss 0.317609
[TEST step716]: loss 0.698792
[TEST step717]: loss 0.600039
[TEST step718]: loss 0.419263
[TEST step719]: loss 0.536764
[TEST step720]: loss 0.506770
[TEST step721]: loss 0.748426
[TEST step722]: loss 0.661507
[TEST step723]: loss 0.418277
[TEST step724]: loss 0.469847
[TEST step725]: loss 0.510590
[TEST step726]: loss 0.626462
[TEST step727]: loss 0.801677
[TEST step728]: loss 0.500108
[TEST step729]: loss 0.261769
[TEST step730]: loss 0.621584
[TEST step731]: loss 0.727552
[TEST step732]: loss 0.622823
[TEST step733]: loss 0.521746
[TEST step734]: loss 0.737523
[TEST step735]: loss 0.659733
[TEST step736]: loss 0.477045
[TEST step737]: loss 0.377500
[TEST step738]: loss 0.484744
[TEST step739]: loss 0.555329
[TEST step740]: loss 0.490460
[TEST step741]: loss 0.385118
[TEST step742]: loss 0.623759
[TEST step743]: loss 0.352961
[TEST step744]: loss 0.737129
[TEST step745]: loss 0.597533
[TEST step746]: loss 0.687774
[TEST step747]: loss 0.632467
[TEST step748]: loss 0.773526
[TEST step749]: loss 0.584998
[TEST step750]: loss 0.543816
[TEST step751]: loss 0.673927
[TEST step752]: loss 0.469571
[TEST step753]: loss 0.475157
[TEST step754]: loss 0.301024
[TEST step755]: loss 0.483892
[TEST step756]: loss 0.401883
[TEST step757]: loss 0.497111
[TEST step758]: loss 0.542486
[TEST step759]: loss 0.606247
[TEST step760]: loss 0.692382
[TEST step761]: loss 0.603810
[TEST step762]: loss 0.488199
[TEST step763]: loss 0.718594
[TEST step764]: loss 0.555597
[TEST step765]: loss 0.666729
[TEST step766]: loss 0.577701
[TEST step767]: loss 0.499285
[TEST step768]: loss 0.513828
[TEST step769]: loss 0.646990

[TEST]: avg loss 0.646990

